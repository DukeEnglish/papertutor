WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue
XingHanLu`*12 ZdeneÀákKasner*13 SivaReddy124
Abstract
üë© Create a task for a Career
üíª say("Do you want to add any
We propose the problem of conversational web Fair on Google calendar description?")
navigation,whereadigitalagentcontrolsaweb
üíª say("Sure!") üë© Yes, please add "Bring
browserandfollowsuserinstructionstosolvereal- multiple copies of my resume"
worldtasksinamulti-turndialoguefashion. To üíª load("calendar.google.com") as the note.
supportthisproblem, weintroduce WEBLINX üíª input( < d i v > , "Bring multiple
‚Äì a large-scale benchmark of 100K interactions copies of my resume")
across 2300 expert demonstrations of conversa-
tionalwebnavigation. Ourbenchmarkcoversa
broad range of patterns on over 150 real-world
websites and can be used to train and evaluate
üíª click( < d i v > )
agents in diverse scenarios. Due to the mag-
nitude of information present, Large Language üíª click( < s p a n > )
Models(LLMs)cannotprocessentirewebpages
inreal-time. Tosolvethisbottleneck,wedesign
aretrieval-inspiredmodelthatefficientlyprunes
HTMLpagesbyrankingrelevantelements. We
usetheselectedelements,alongwithscreenshots üíª input( < i n p u t > , "Career Fair")
and action history, to assess a variety of mod-
elsfortheirabilitytoreplicatehumanbehavior üíª say("Task created. Anything
whennavigatingtheweb. Ourexperimentsspan else I can assist you with?")
from small text-only to proprietary multimodal üë© No. That's all for now.
LLMs. Wefindthatsmallerfinetuneddecoders
surpassthebestzero-shotLLMs(includingGPT-
4V),butalsolargerfinetunedmultimodalmodels
Figure1:Anexampleoftheconversationalwebnavigationtask.
whichwereexplicitlypretrainedonscreenshots.
The instructor (blue) communicates with the navigator (grey)
However,allfinetunedmodelsstruggletogener- usingonlynaturallanguage.Thelattercontrolsthebrowser,having
alizetounseenwebsites. Ourfindingshighlight accesstoscreenshotsandtextualwebsiterepresentation.
the need for large multimodal models that can
generalizetonovelsettings. Ourcode,dataand
modelsareavailableforresearch: https://mcgill- Pinsky,2023),allowingthemtoperformactionsandpro-
nlp.github.io/weblinx. vide more useful responses. However, this capability is
limited: thepluginsmustbedevelopedseparatelyforeach
websiteandmaynotcoverallofawebsite‚Äôsfunctionality.
Thislimitationraisesanimportantresearchquestion: can
1 Introduction weleveragethemodelsbehindthoseassistantstonavigate
websitesdirectlyintheuser‚Äôsbrowser,whileretainingtheir
Proprietary conversational assistants like ChatGPT (Ope-
conversationalcapabilities?
nAI,2022)arecapableofmorethanjustconversing;they
canalsobrowsewebsitesthroughplugins(OpenAI,2023d; Motivatedbythisquestion,wedefinetheproblemofcon-
versationalwebnavigation: giventheinitialuserinstruc-
*Equalcontribution 1MilaQuebecAIInstitute2McGillUni-
tion,anagentmustcompleteareal-worldtaskinsideaweb
versity3InstituteofFormalandAppliedLinguistics,CharlesUni-
browserwhilecommunicatingwiththeuserviamulti-turn
versity 4Facebook CIFAR AI Chair. Correspondence to: Xing
Han Lu` <xing.han.lu@mail.mcgill.ca>, ZdeneÀák Kasner <kas- dialogue. Thisproblemisrelevantinmanyreal-worldsce-
ner@ufal.mff.cuni.cz>,SivaReddy<siva.reddy@mila.quebec>. narios: helpingvisuallyimpairedusersefficientlynavigate
1
4202
beF
8
]LC.sc[
1v03950.2042:viXraWEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table1:WEBLINXisthefirstbenchmarkfeaturingreal-worldwebsiteswithmulti-turndialogue.Thecolumnsindicate:useofmulti-turn
dialogue(Chat),iftasksaregeneralorspecialized(Gener.),awebbrowserisused(Browse),numberofapp/websitedomains(#Dom.),
numberofinstances(#Inst.),averagenumberofHTMLelementsperpage(Avg.#El.),averagenumberofturnsperinstance(Avg.#
Turns).*AITWhas30KuniquepromptswithmultipledemoseachandthebrowsingdataisstrictlyfromAndroiddevices.
Benchmark Chat Gener. Browse #Dom. #Inst. Avg.#El. Avg.#Turns Setting
MiniWob++(Liuetal.,2018) ‚úó ‚úó ‚úó 100 100 28 3.6 Simplified
WebShop(Yaoetal.,2022) ‚úó ‚úó ‚úì 1 12K 38 11.3 E-Commerce
WebArena(Zhouetal.,2023) ‚úó ‚úì ‚úì 6 812 - - Real-world
VWA(Kohetal.,2024) ‚úó ‚úì ‚úì 3 910 - - Real-world
WebVoyager(Heetal.,2024) ‚úó ‚úì ‚úì 15 300 - - Real-world
Mind2Web(Dengetal.,2023) ‚úó ‚úì ‚úì 137 2350 1135 7.3 Real-world
AITW‚àó(Rawlesetal.,2023) ‚úó ‚úì ‚úì 357 30K - 6.5 Android/Apps
RUSS(Xuetal.,2021) ‚úì ‚úó ‚úì 22 80 801 5.4 Helpcenter
META-GUI(Sunetal.,2022) ‚úì ‚úì ‚úó 11 1125 79 4.3 Mobileapps
WEBLINX(ours) ‚úì ‚úì ‚úì 155 2337 1775 43.0 Real-world
websitesthroughachatinterface,enhancingsmartspeakers Ranking(¬ß5.1),whichcompareseachelementinanHTML
anddigitalassistantswithvoice-controlledwebnavigation, pagewiththefullactionhistory.Byusingasimilarity-based
andimprovingtheproductivityofknowledgeworkersbyre- approachtobothlearnandrankelements,wecanleverage
ducinghighlyrepetitivestepswhilestayingincontrol.From compact architectures used in text retrieval. This lets us
aresearchperspective,thisproblemcanbeusedtoassess findthemostrelevantelementsandpruneirrelevantonesto
theabilityofLLMagentstonotonlyfollowself-contained obtainacompactrepresentationoftheDOM.Wecombine
instructions,butalsoengagewiththeirenvironmentthrough itwiththeactionhistory,detailedinstructionandscreenshot
dialogueandgeneralizetounforeseensituations. (inamultimodalcontext)toconstructaninputrepresenta-
tionforLLMs,whichcannowmeaningfullypredictwhich
Toaddressthisproblem,weintroduce WEBLINX1 (¬ß3),
actionstotake. However,evenifapredictedactioniscor-
abenchmarkcontaining2337demonstrationsofconversa-
rect, itmaybeidentifiedasincorrectbyexistingmetrics,
tionalwebnavigationproducedbyhumanexpertsacross
whichcanhappenwhenthereareminordifferencesinan
155real-worldwebsites. Figure1showsademonstration.
agent‚Äôsresponseorwhenanoverlappingelementisselected.
Eachdemonstrationcapturesthefullsequenceofactionsper-
Thus,wedesignasuiteofevaluationmetrics(¬ß4)tailored
formedbyahumannavigatorwheninteractingwiththeuser
forspecifictypesofaction(forinstance,clickingshouldbe
(known as instructor) through a conversational interface.
evaluateddifferentlyfromwhatthenavigatorsays).
Werecordover100Koccurrencesofactionsandutterances,
where each action is associated with a Document Object We examine 19 models based on 8 architectures (¬ß6), in-
Model(DOM)2 tree,browserscreenshots,andframesfrom cluding smaller image-to-text, larger text-only decoders,
demonstration-level video recordings. Table 1 highlights LLMs,andmultimodalmodels(capableofaccessingboth
theuniqueaspectsofWEBLINX.Unlikepreviousworks imageandtext). Amongthem,5areinthezero-shotsetting,
focused on mobile apps or specialized applications, ours andtheremainingarefinetunedusingthetrainingsplitof
isthefirstlarge-scalebenchmarkthatcanbeusedtotrain WEBLINX. We find that even the best zero-shot model,
dialogue-enablednavigationagentsandevaluatetheirgener- GPT-4V(OpenAI,2023a),issurpassedbyfinetunedmodels
alizationcapabilitiestorealisticscenarios,suchasadapting (¬ß6.1). Notably,asmallermodellikeSheared-LLaMA(Xia
tonewwebsites, categories, andgeographies; wealsore- et al., 2023) outperforms the much larger Fuyu (Bavishi
serveasplittoassesstheabilityofagentstointeractwith et al., 2023), which was pretrained with browser screen-
instructorswithoutvisualaccesstothebrowser. shots. However,allmodelsfacechallengesingeneralizing
tonewsettings, suchasunseenwebsitesfromadifferent
Anaivewaytousethisbenchmarkwouldbetogivethefull
geographic location or when the instructor gives instruc-
DOMtreedirectlytoanagentandinstructittopredictthe
tionswithoutseeingthescreen. Thosefindingsprompted
correctaction. AssomeHTMLpagescontainthousandsof
ustoqualitativelylookatthebehaviorofthemodels(¬ß6.2),
elements, fitting them completely within the context of a
wherewefindthatGPT-4Vlackssituationalawarenessand
LLMposesasignificantchallenge;evenifitwaspossible,
can make obvious blunders. However, the best finetuned
existingLLMswouldbeunabletoprocesstheminreal-time.
modelsstillfailinsimplecases, suchasclickingonnon-
Consequently, wedesignamethodcalledDenseMarkup
existinglinksorfailingtochangethelanguageofatrans-
1WebLanguageInterfaceforNavigationthrougheXemplars lationapp. Thus,webelievethatsignificanteffortwillbe
2TreerepresentationofHTMLpageasrenderedinthebrowser. neededtomakeprogressontheproblemofconversational
2WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table2:Demonstration(Demo)splitsfortrainingandevaluation.
AI Tools
603
Train Split Description
1,404
Booking TRAIN DemosusedtotrainmodelsinSection5
983
VALID In-domaindemosforhyperparametersselection
V 1a 4li 0d
Composing
TESTIID In-domaindemostotestin-domaingeneralization
295 TESTOOD AggregationofsplitsforOODevaluation
I 3n 9fo 1. Lookup TESTWEB Unseenwebsitesfromthesamesubcategories
Tes 1t- ,O 6O 92D
Productivity
TESTCAT Newsubcategorieswithinthesamecategories
218 TESTGEO GeographiclocationsnotinTRAIN
Shopping TESTVIS Instructordoesnotseethescreen
330
Test-IID Social Inter.
146 276
Summarizing
286
Figure 2: Distribution of demonstrations in WEBLINX across Inanefforttobuildmorerealisticenvironments,Yaoetal.
categories(Section5.2)andsplits(Table2). Eachcategoryhas (2022)introducedWebShop,ane-commerceenvironment
manysubcategoriesasshowninAppendixA.2. with over 12K human-written task instructions. Models
trainedonWebShopachievedstrongperformance,butstill
relied on clean HTML and simple visual representations
webnavigation,aswediscussinSection7. (Furutaetal.,2023). Instead,weaimtobuildagentsthat
canactonanyreal-worldwebsite,oftenexistinginnoisy
Ourcontributionsaresummarizedasfollows:
anddynamicenvironments.
‚Ä¢ Weintroducethetaskofconversationalwebnaviga-
TheprospectofusingLLMstoactonrealwebsites(Nakano
tionandalarge-scaleexpert-annotatedbenchmarkfor
etal.,2021)hasleadtothedevelopmentofLLM-basednav-
it,namedWEBLINX(¬ß3).
igationservices(Adept,2023;Multi-On,2023;HyperWrite,
‚Ä¢ Weproposeasuiteofaction-specificmetrics,which 2023), whichhassetthestageforacademiccounterparts.
wecombinetoassessoverallmodelperformance(¬ß4). MIND2WEB(Dengetal.,2023)andWebArena(Zhouetal.,
‚Ä¢ WedesignamethodtosimplifyHTMLpages(¬ß5.1), 2023) are large-scale resources for building autonomous
allowingustoevaluateawiderangeofmodels(¬ß5.2). navigationagentslikeSeeAct(Zhengetal.,2024)andWeb-
‚Ä¢ We find that smaller text-only decoders outperform
Voyager(Heetal.,2024). Ontheotherhand,WEBLINX
is a benchmark for building agents that can interact with
multimodalLLMs,butallfinetunedmodelsstruggle
usersinamulti-turndialoguefashion,allowingthemtobe
togeneralizetonovelscenarios(¬ß6).
steeredtowardsprecisegoals.
2 RelatedWork 2.2 WebsiteRepresentations
Efficiently representing real-world websites is a long-
2.1 WebNavigationAgents
standingchallengeinwebunderstanding(Wuetal.,2023),
Previous work predominantly focused on building web includingsubtaskslikewebinformationextraction(Chang
agentsforasingletask. Aprominentworkfortask-driven et al., 2006) and web segmentation (Kiesel et al., 2020).
webnavigationisMiniWoB++(Shietal.,2017;Liuetal., Theapproachesforsimplifyingorcompressingthetextual
2018),asimulatedwebenvironmentwithanextensivelist representation of the website ‚Äì its HTML code or DOM
oftaskprimitives(e.g.,selectvaluefromadropdownordate tree ‚Äì include rule-based algorithms (Zhou et al., 2021),
fromacalendar). Itswell-definedinputspaceandtheflexi- accessibility-treerepresentationsofferedbybrowsers(As-
bilityofitssimulatedenvironmentsleadtoreinforcement soueletal.,2023),graphembeddings(Wangetal.,2022),
learningapproachesreachinghuman-levelperformance(Liu andmodel-basedapproaches(Dengetal.,2022;Lietal.,
etal.,2018;Humphreysetal.,2022). However,theability 2022; Aghajanyan et al., 2022). Previous works for rep-
ofthosemethodstotransfertorealisticsettingshavebeen resenting the visual information of the webpage usually
limited,evenafterintroducingenvironmentextensions(Gur relyonfeatureextraction(Liuetal.,2010;Cormeretal.,
etal.,2021)andsample-efficientmethods(Kimetal.,2023). 2017),closelyfollowingtheresearchongraphicalUIs(Wu
Otherworksalsoexploredgroundinglanguagecommands etal.,2021;Bunianetal.,2021). Weproposeanoveldense
towebelementsandmobileUIs(Pasupatetal.,2018;Li markup retriever which selects relevant DOM elements,
etal.,2020;Burnsetal.,2022),orquestionanswering(QA) andusetheseelementsoptionallycombinedhigh-resolution
bynavigatingWikipedia(Nogueira&Cho,2016). browserscreenshots.
3WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Instructor Navigator Table3:OverviewoftheWEBLINXcoreactionspace.Forfull
Replies setofactions,seeTable6.
Instructs Action Description
click(element) clickonanelement
Views* Controls load(url) loadURLofanewpage
say(text) navigator‚Äôsutterance
submit(element) submitaform
textinput(element,value) typetextintotheelement
Browser
Figure3:Datacollectionsetup(¬ß3).Werecordinteractions(chat
actions a ‚àà A . At each turn t ‚àà {1,...,n}, the state
andbrowseractions)betweenaninstructorandhumannavigator.
*InstructorcanseethescreenexceptinTESTVISsplit. s t containstherepresentationofthewebsite. Eachaction
followsoneofthe5coreintentsdescribedinTable3. The
fulllistofintentsisprovidedinSectionA.6.
2.3 ConversationalInterfaces DataCollection Tocollectthedemonstrations,weworked
withaprofessionaldatalabelingcompany,3 whoenlisted
Using conversational interfaces to complete tasks is the
8expertannotatorsthatreceiveddetailedinstructionsand
basisoftask-orienteddialogue(Chenetal.,2017;Zhang
extensive training to complete our tasks. The annotators
etal.,2020b). End-to-endsolutionshaveshownpromising
workedinpairs:aninstructorinteractswithanavigatorwho
results(Zhangetal.,2020a;Kannetal.,2022),buttheuse
completesthetasksinawebbrowser(seeFigure3). Both
ofLLMsremainsunderscrutiny(HudecÀáek&DusÀáek,2023).
usethechatinterfacetocommunicate,butonlythenavigator
Forreal-worldservices,Dialog2API(Shuetal.,2022)pro-
controlsthebrowser. Wedesignedanapp,browserexten-
posedaninterfaceforinteractingwithAPI-basedservices,
sion,andprocessingpipelinetorecordthedemonstrations,
whereasMETA-GUI(Sunetal.,2022)introducedadataset
whicharesubsequentlyvalidatedbyadifferentannotator
focusedonautomatingactionsinmobileappsratherthan
underthesupervisionoftheoriginalnavigator(detailsin
generalwebsites. Intermsofdialogue-centricwebnaviga-
AppendixA.5).
tion,RUSS(Xuetal.,2021)isthefirstdatasetdesignedto
helpsupportservicesthrough80demonstrationsannotated EvaluationSplits InadditiontoaTRAINsplit,wecreate
withadomain-specificlanguage. WEBLINXextendspre- VALIDandTEST toassessin-domaingeneralization,and
IID
vious dialogue-centric datasets by covering a wide range 4out-of-domainsplitsforvariousscenarios(seeTable2).
of real-world tasks spanning 2337 demonstrations, with
considerablylongerdemonstrationsduetodynamictopic 3.1 Representingactionsandstatesformodeling
switching,asubjectstudiedbyAdlakhaetal.(2022).
Ateachturnt,wehaveaccesstothestates topredictan
t
actiona . Thestateconsistsofthefollowing(ifavailable):
t
3 WEBLINX
‚Ä¢ c : Candidateelementsthatcanbetargetedbya ,
t t
In this section, we introduce WEBLINX, a large-scale
‚Ä¢ d : CurrentDOMtreeofthepage,
t
benchmark for conversational web navigation consisting
‚Ä¢ i : Screenshotofthenavigator‚Äôsbrowser,
of 2337 demonstrations with an average of 43 turns. It t
containsinteractionsbetweenahumanuser(referredtoas ‚Ä¢ u t: Instructor‚Äôsutterance,
instructor)andhumanassistant(navigator)aimingtocom- ‚Ä¢ v : Viewportsize(heightandwidth),
t
pletetasksacross155real-worldwebsitesselectedfrom15
‚Ä¢ h : Interactionhistory,
t
geographicareas. Weclassifythewebsitesinto8categories
and50subcategoriesbasedontheirdomains. Note that a state need not contain all of the above. For
example,atthestartofademonstration,theinstructorand
Statistics ThedatastatisticsaresummarizedinTable1and
navigatormayneedmultipleroundsofdialoguetoproperly
abreakdownbycategoryandsplitisillustratedbyFigure2.
definetheobjective,inwhichcasetheinitialstatesdonot
Additionalstatisticsaboutthedataset,includingthenumber
have DOM trees or screenshots. A model m predicts an
ofdemonstrationsinsplit,canbefoundinAppendixA.1,
actiona foragivenstates basedonaprompttemplatep
t t m
alongwiththelistofcategoriesinAppendixA.2.
whichindicateshowtomakeuseofthecontentsinastate.
DemonstrationFramework Thedemonstrationscapture
Interactionhistory Sinceamodelmhasalimitedinput
real-timeinteractions,whicharerecordedbythenavigator
lengthinpractice,werepresenthistoryhasthesetofpast
controlling the web browser. Each demonstration D =
{s 1,a 1,...,s n,a n} is a sequence of n states s ‚àà S and 3EsyCommerce:esycommerce.com
4WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
five actions (denoted as a r) and five utterances (u r). We scalebytheIM,resultingin IM(a‚Ä≤,a)√óCHRF(a‚Ä≤,a). In
couldnotincludetherepresentationofpaststatessuchas thecaseofloadintent,URLsfollowastructurethatcanbe
elementsorscreenshots. consistentlysegmented,whichleadsustoapplytheF1-score
onsegmentsinsteadofn-grams;wecallthismeasureURLF.
Parsing Action Output An action consists of an intent
WeuseF1torefertoeitherchrFandURLF,dependingon
andargumentandcanbegeneratedbyanagentinatextual
whetheranactioncontainsatextorURLargument.
format. Itmustfollowapre-definedstructure(seeTable3)
thatallowsittobeparsedintoastructuredform,whichcan 4.2 Turn-levelscoreandoverallscore
beexecutedinabrowserusingtoolslikeSelenium.4 We
To allow better comparisons between models, we divide
discussadditionaldetailsinAppendixA.4.
the intents into groups: The element group (EG) con-
tainsclick,textinput,andsubmit,andisevaluatedwith
4 EvaluationFramework
IoU. The text group (TG) encompasses load, say, and
textinput,andisevaluatedwithF1.
In this section, we describe the evaluation metrics (¬ß4.1)
andtheirapplicabilitytospecificgroupsofintents(¬ß4.2). Weassignaturnlevelscorebasedonthefollowing: Ifthe
turninvolvesanactioninEG,thescoreisthesameasIoU,
4.1 Metrics
i.e. score is 0 when the intent is incorrect or the element
Acommonlyusedmetricinpriorworkonwebnavigationis doesn‚Äôtoverlap,itis1whenintentiscorrectandtheelement
tasksuccessrate,whichmeasurestheproportionofdemon- perfectlyoverlaps,anditissomewhereinbetweenforthe
strationswherethemodelreachedthedesiredfinalstate(Shi rest. For TG actions load and say, the score is same as
etal.,2017;Yaoetal.,2022;Dengetal.,2023). However, F1, i.e., scoreis0wheneitherintentisincorrectorthere
thismetricisinappropriateforourbenchmarkbecausethe isnotextoverlap,itis1whenintentiscorrectandthetext
objectiveisnotfullydefinedinthefirstturnorlaterturns; matchesexactly,anditissomewhereinbetweenfortherest.
instead,itevolvesastheconversationproceeds. Weinstead
Fortextinput,theturnscoreisIoU√óF1sinceitcontains
leverageturn-levelautomaticevaluationmetrics,following bothtextandelementarguments. Finally,wecomputethe
establishedapproachesindialoguesystems(Rastogietal., overallscoreusingthemicro-averageofturn-levelscores.
2020; Zhang et al., 2020a). The aim of the metrics is to
provideaheuristicestimateofthesimilaritybetweenthe 5 Methods
predictedactionandthereferenceaction.
Inthissection,wedescribeamethodforselectingcandidate
IntentMatch(IM) Givenpredictiona‚Ä≤ andreferencea,
elements(¬ß5.1)andhowtousethemintextualinput.Weuse
theintentmatchis IM(a‚Ä≤,a) = 1iftheintentsareequal,
thesemethodstobuildmodelsthatcanaccuratelypredict
otherwise IM(a‚Ä≤,a) = 0. This tells us if a model can
actions(¬ß5.2). WereportresultsinSection6andprovide
correctly identify which action to perform, but does not
implementationdetailsinAppendixB.
indicateifthemodelcanpredictthecorrectarguments.
5.1 DenseMarkupRanking(DMR)forCandidate
ElementSimilarityusingIoU Foractionswithelements
SelectionandInputRepresentation
asarguments(click,textinput,submit),wecomputethe
intersection over union (IoU; Jaccard 1912). Given the Tochooseasetofsuitablecandidatesforthemodelinput
areaofaboundingboxB,wehave: (¬ß3.1),weneedacandidateselectionstagethatfiltersthe
full set of elements in the DOM tree. Deng et al. (2023)
B ‚à©B
IM(a‚Ä≤,a)√ó reference predicted proposed to pair each DOM element with the task query
B ‚à™B
reference predicted and input them into a DeBERTa model (He et al., 2021),
Tocomputethearea,weuse(x,y)coordinatesoftheref- whichisfinetunedusingacross-encoderloss(Reimers&
erence and predicted bounding boxes. This formulation Gurevych,2019). Wefoundthismethodtakesonaverage
(1)favorselementswithhighvisualoverlap,(2)penalizes 916mstoselectcandidatesforagiventurn.5 Whenfactoring
predictingelementsmuchsmallerorlargerthanreference innetworklatencyandLLMinference,thiswouldresultin
elementsevenifoneiscompletelycontainedbytheother, poorprocessingtime. Itisthuscrucialthatweuseefficient
and(3)assigns0iftheelementsdonotoverlap. rankingmethodtobuildagentsthatcanoperateinrealtime
andlearnfrominteractionswithusers.
TextSimilarityusingF1 Tomeasurelexicalsimilarityof
textargumentsinsayandtextinput, wecalculatechrF Tosolvethis,weproposeDenseMarkupRanking(DMR),
(Popovic,2015),anF1-scoreforcharactern-grammatches whichis5timesfasterthanthepreviousapproach,atthe
(weusethedefaultsettingofn = 6). SimilartoIoU,we costofslightlylowerrecall. Themethodconsistsof: (1)a
4https://www.selenium.dev/ 5Calculatedonthetrainingset,seeAppendixB.4.1.
5WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
simplifiedelementrepresentationtoreducecomputational Table4:Aggregatedresults(¬ß6)acrossmajormodels(¬ß5),sorted
byparametercount(Size).FollowingmetricsfromSection4,we
overhead; (2)adualencoder-basedapproach(Reimers&
reportresultsofintentmatch(usingIM),elementgroup(IoU),
Gurevych, 2019; Karpukhin et al., 2020); (3) similarity-
text group(F1), and theoverallscore(using micro-average on
based learning between the text representation of s t and turn-level scores). All results are on TESTOOD except the last
a 1:t‚àí1 and corresponding HTML elements. Using this columnwhichisonTESTIID. 4indicatesmodelswithaccessto
method,wefinetuneavariantofMiniLM(Wangetal.,2020). screenshots;everymodelexceptPix2Acthasaccesstotextinputs.
Weformulatethecosine-basedlearningobjective,examine
theinferencespeedimprovements,andevaluatealternatives Intent Element Text OverallScore
inAppendixB.4. Models Size IM IoU F1 TESTOOD TESTIID
Zero-shot
Evenafterourcandidateselection,theinputsequencelength
toamodelcanexceeditslimit,sowetruncatethesequence. Llama-2 13B 43.5 4.9 1.4 5.2 5.6
GPT-3.5T ‚Äì 42.7 9.0 3.5 8.8 10.3
Toreduceinformationlossfromtraditionaltruncation(e.g.,
GPT-4T ‚Äì 41.8 11.2 6.9 11.0 12.2
for large DOM elements and long history), we design a GPT-4V4 ‚Äì 42.3 11.4 6.4 10.9 12.9
strategythatleveragesthehierarchicalnatureoftheinput
Finetuned
todeterminewhichsubsectionshouldbetruncated. Wein-
troduceseveralimprovementstotherepresentationusedin Pix2Act4 1.3B 82.1 9.3 26.6 18.4 23.9
priorworksbyincludingthefullHTMLattributes,viewport S-LLaMA 2.7B 84.7 25.3 29.2 27.6 37.4
MindAct 3B 80.1 17.7 23.4 21.9 25.7
size,XMLPath,andtheboundingboxesofcandidateele-
Flan-T5 3B 81.6 22.1 26.4 25.2 31.1
ments(implementationdetailsinAppendicesB.1andB.2). Fuyu4 8B 80.9 17.8 24.5 22.2 30.9
Llama-2 13B 83.0 25.7 28.7 27.8 37.0
5.2 ModelingActions
GPT-3.5F ‚Äì 78.5 21.1 23.8 23.3 30.8
Uponselectingthemostpromisingcandidatesforagiven
states ,wecancombinethemwiththeremaininginforma-
t
tionins t toconstructarepresentationthatcanbeusedto Proprietarytext-onlyLLMs WereportresultsforGPT-
predict action strings, which can be parsed and executed 3.5Turbo(Brownetal.,2020;Pengetal.,2023),inboth
(¬ß3.1). Tounderstandwhichfactorsmatterforpredictingac- zero-shot (3.5T) and finetuned (3.5F) settings. We also
tions,weexamine19zero-shotandfinetunedmodels(using includezero-shotresultsforGPT-4T(OpenAI,2023b).
theTRAINsplit)withdifferentinputmodalities:image-only,
Image-to-textmodeling WeexplorePix2Act(Shawetal.,
text-only,andboth. Weprovideimplementationdetailsin
2023) an encoder-decoder (Vaswani et al., 2017) purely
AppendixB.6andhyperparametersinAppendixB.7.
finetuned on pixels. It uses a Pix2Struct backbone (Lee
Model Categories We categorize action models by the et al., 2023), which is pretrained on screenshots using a
input modality, since the output is always in a structured VisionTransformerencoder(Dosovitskiyetal.,2021)anda
format(¬ß3.1). Wedefinethefollowingtypes: (1)text-only, textdecoder. Wefollowthebehaviorcloningapproachused
whichreceivesinstructions, prunedDOMtree, candidate byPix2ActbyfinetuningthesamebackboneonWEBLINX.
elementdescriptionandhistory;(2)image-to-text,which
Multimodalmodels WefinetuneFuyu-8B(Bavishietal.,
receivesthescreenshot,instructionsandpastactionsdirectly
2023),abasemodelpretrainedonbrowserscreenshotsby
embeddedintheimage;(3)multimodal,whichreceivesthe
modelingimagesandtextusingaunifiedarchitecture. We
screenshot, instructions, pruned DOM tree, candidate de-
alsoreportzero-shotresultsforthevariantofGPT-4with
scriptionandhistorydirectlyastext. Additionaldiscussions
visioncapabilities(GPT-4V;OpenAI2023a).
arefoundinAppendixB.3.
Text-onlymodels TherecentMindAct(Dengetal.,2023)
6 ExperimentalResults
model is a Flan-T5 (Chung et al., 2022b) model that has
been finetuned on Mind2Web. We further fine-tune it on Inthissection,wereporttheresultsofourexperiments(¬ß5)
WEBLINXusingitsoriginalconfiguration. forgroupsdefinedinSection4.2. Weaggregatetheresults
for11modelsinTable4. InSection6.2,wequalitatively
ToquantifytheimprovementsbroughtbyDMR-basedrepre-
assesstwomajormodels: GPT-4VandLLaMA-2-13B.See
sentation(¬ß5.1),wedirectlyfinetuneFlan-T5checkpoints,
AppendixCforsupplementaryresultsandAppendixDfor
allowing us to control for size and architecture with re-
thedetailedoverview(includingtheremaining8variants).
specttoMindAct. WealsofinetuneLLaMA-2(Touvron
etal.,2023a;b)6andadistilledversion,Sheared-LLaMA
6.1 OverviewofResults
(S-LLaMA;Xiaetal.2023).
Impactofrepresentationfortext-onlymodels InTable4,
6Weusethevariantsfinetunedonchat.
weobservethatMindActtrailsbehindFlan-T5finetuned
6WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table5:Resultsonout-of-domainsplits(¬ß2)forfinetunedLLaMA- example,weobserveinTable5thatLLaMa-13Bachieves
2-13B(¬ß5.2). Amongthesplits, TESTCAT seemstobethehard-
poor results on TEST , indicating that unseen subcate-
est,indicatingthatmodelsstruggleonunseensubcategories(e.g., CAT
gories are more challenging than new websites from the
restaurantappointmentvs.medicalappointment).
samecategories. Forinstance,ifthemodellearnshowto
bookseatsatarestaurant,itcanadapttoadifferentrestau-
Intent Element Text
Splits Overall
IM IoU F1 rantbutwillstruggletobookamedicalappointment.
TESTWEB 82.7 24.2 28.7 27.0
6.2 QualitativeAssessment
TESTCAT 81.0 20.7 26.1 24.3
TESTGEO 78.6 22.0 27.7 25.9
To better understand the performance gap separating the
TESTVIS 85.3 26.1 23.9 25.0
strongestzero-shotandfinetunedmodels,wequalitatively
examinetwomodels,GPT-4VandLLaMA-2-13B,which
respectivelyrepresentthetwoparadigms. Althoughthegap
usingDMR-basedinputrepresentation(¬ß5.1),whencom-
canbepartiallyattributedtoincorrectlypredictedintents
paringthe3B-parametervariants. AlthoughMindActwas
(seeAppendixD),modelscanstillmakepoorpredictions
finetunedforarelatedtask,itwasneverexposedtomulti-
even when the intent is predicted correctly. We focus on
turndialogue. However,Flan-T5wasnevertrainedonany this scenario by assessing actions from 3 intents: click,
navigationactions. Thus,DMR-basedrepresentationplays textinputandsay;foreach,weshowtwoexamplesinFig-
animportantroleinachievingabetterperformanceforthe
ure4. ExtendedassessmentscanbefoundinAppendixC.5.
samearchitectureandmodelsize. Moreover,bothLLaMa-
based models outperform Flan-T5 and MindAct despite Assessingclick Inscenarioswheremodelsselectobjects
Sheared-LLaMabeingsmallerthanFlan-T5. Thiscouldbe throughclicks,wefindthatGPT-4Vchoseanincorrecttab
duetothehighqualitytrainingofLLaMamodelsonalarge (C1),wasunawareithasalreadystartedasub-task(C2),and
number of instruction-following tasks compared to Flan- chosealessoptimaloption(seeAppendixC.5). Although
T5. However,itisintriguingthatSheared-LLaMaperforms those scenarios are correctly addressed by the finetuned
equallywellcomparedtoLLaMA-213B. LLaMA-2,itcanstillfailbyclickingonirrelevantelements
(evenwhenGPT-4Vselectsthecorrectone).
Image-to-textvs. multimodalmodels Wefurtherhigh-
lightthedifferencebetweensmallerimage-to-textandlarger Assessing textinput When looking at examples where
multimodal models by comparing Pix2Act (1.3B param- modelsareselectingandtypingtextinsideinputs,weob-
eters) and Fuyu-8B. Overall, Fuyu outperforms Pix2Act, servethatGPT-4Vtriedtowritethenameofaemailrecipi-
whichcouldbedueitsabilitytoreceivetextasinputand entinsteadofthesubjecttitle(T1),theusernameinsidea
greaterparametercount. However,ittrailsbehindPix2Act passwordfield(T2),typedapassagealreadyinthetarget
forintentmatchingandtextprediction. textbox,andskipthetitlewhendraftingapost. Although
LLaMAsucceededinthefirsttwocases,itmayattemptto
Comparingmultimodalwithchat-basedmodels Weob- clickinsteadoftextinputandalsoomitthetitle.
servethatFuyu-8Bisoutperformedbychat-basedtext-only
LLaMAmodels. Thisshowsthatmultimodalmodelsfine- Assessingsay Forsayactions,GPT-4Vusedadifferent
tuned on screenshots are still behind chat-based models writingstyle(S1),whereasLLaMA-2learnedthewriting
optimizedforinstruction-basedfinetuning. style of the annotators. Additionally, GPT-4V provided
unhelpful responses by sharing irrelevant links (S2) and
Comparison with proprietary models In the zero-shot
refusedtoassisttheinstructorevenwhenitispossible.Even
setting, where models solely rely on the instructions, we
thoughLLaMA-2isfinetuned,itmissedcertainfollow-up
observe that proprietary models (GPT-3.5T and GPT-4T)
questions(suchasasking‚ÄúWhoshouldreceivethis?‚Äù when
outperformtheopen-sourcedLLaMA-2. However, when
askedtowriteanemail).
finetuned,GPT-3.5FisoutperformedbySheared-LLaMA
and LLaMA-2, but the cause is unclear as most hyperpa-
7 Discussion
rametersareinaccessibleforcommercialtraining. Finally,
GPT-4VandGPT-4Tachievesimilarperformance,suggest-
7.1 ExperimentalFindings
ingthatexistingmultimodalmodelsmightnotbeableto
effectivelyusescreenshotsforpredictingactions. Through our experiments (Section 5), we find that larger
multimodalmodelscansurpasssmallerimage-onlymodels
Generalization capabilities When comparing TEST
OOD whenfinetuned,buttheyarestillbehindfinetunedtext-only
withTEST results,weobserveamajordifferenceacross
IID models. WealsofindthatemployinganDMR-basedrepre-
all finetuned models. This highlights a weakness of fine-
sentationleadstobetterperformance(¬ß6.1). Whenevalu-
tunedmodels: althoughtheyperformwellonfamiliarweb-
atedonout-of-domainsplits,theperformanceoftext-only
sites,theywillstruggletogeneralizetounseenwebsites.For
7WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
C1:InstructorwantsNavigatortoopenaspecifictabonaNews C2:Instructorrequeststhelocationonafooddeliverysitetobe
site,i.e.,‚ÄùSportsdayon28May2023at4.15AM‚Äù. settoLasVegas.TheDeliverydetailswindowisalreadyopen.
GPT-4V(R)clicksonanincorrect(3:30AM)tab,eventhough GPT-4V(R)attemptstoexittheDeliverydetailspageandreopen
theinstructorrequestedadifferenttime(4:15AM). it,whichcouldpotentiallyleadtoaloop.
LLaMA-WL(B)clicksonthecorrect4:15AMtab. LLaMA-WL(B)correctlyclicksontheChangebutton.
T1:Composea‚ÄúInvitationtoCollaboration‚Äùemail. T2:OpenGoogletranslateandsigninusingthefollowingcre-
dentials:[email][password]
Reference(B):InvitationtoCollaboration Reference(B):[password]
GPT-4V(R):LeonTales GPT-4V(R):[email]
LLaMA-WL(B):InvitationtoCollaboration LLaMA-WL(B):[password]
S1:Instructorsays‚ÄúLike&Bookmarkthislisting.‚Äù S2:Instructorsays‚ÄúPleasesharethelink.‚Äú
Reference:Alright Reference:Alright.
LLaMA-WL:Alright LLaMA-WL:Okay.
GPT-4V:Understood.Ifyouneedfurtherassistance,justletmeknow. GPT-4V:Hereisthelinktothediscussion:[incorrectlink]
Figure4:ComparisonofGPT-4VandLLaMA-2-13B(finetuned)onpredictingclickactions.Incorrectlypredictedactionsareinred(R),
referenceactionsareinblue(B).Weshowtwoscenariosforclick(C1,C2),textinput(T1,T2)andsay(S1,S2).
decodersareveryclosetosmallervariant;nonetheless,zero- text-only model to draw on a canvas or describe images.
shotmodelsareconsistentlysurpassedbytheirfinetuned Such limitations can be addressed through multimodal-
counterparts. Weconfirm,throughqualitativeassessments specifictechnicalcontributionsinfutureworks.
(¬ß6.2),thateventhebestzero-shotmodelscanmakesimple
andunjustifiederrors. Ourfindingshighlighttheneedto
8 Conclusion
buildmodelsthatcanbettergeneralizetounseenscenarios
ifwewanttobuildagentsthatwillworkintherealworld. WeintroducedWEBLINX,alarge-scaleexpert-builtbench-
mark covering a wide range of demonstrations for con-
7.2 Limitations
versational web navigation on real-world websites. The
frameworkwebuiltaroundthebenchmarkincludesthetask
Ourbenchmarkcontainsonlystaticdemonstrations,which
definition,datarepresentation,andevaluationmetrics. We
means we cannot meaningfully evaluate the behavior of
alsointroducedadensemarkupranker(DMR)toeffectively
modelsonalternativetrajectories. However,thisapproach
summarizewebpages.Weevaluatedfinetunedandzero-shot
letsustrainmodelsonadiversesetofrealwebsitesthatdo
modelswithvariousmodalities,andfoundthatchat-based
notneedtoberecreatedfromscratch.
decodermodelsfinetunedonWEBLINXachievethebestre-
Generalizability Thereareinherentlimitationsofthear- sults,butstillstruggletogeneralizetoout-of-domainsplits.
chitecturesweevaluate. Forexample,wecannotexpecta Webelievethatmulti-turndialoguecanenhanceflexibility
8WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
andsterabilityofagentsforwebnavigation,leadingtotheir procedures. The resulting research can be used to build
wideradoption. systemsthatarerobustagainstmaliciousagents.
Unintended Actions Navigation agents can also cause
Acknowledgments
harm if they misinterpret instructions and perform unin-
tendedactions;forinstance,bookingthewrongflightcould
XHLissupportedbytheNSERCCGSDFellowship. ZKis
resultinsignificantfinancialloss. Forthisreason,weassert
supportedbytheEuropeanResearchCouncil(Grantagree-
thatconversationalwebnavigationmodelsshouldbeused
ment No. 101039303, NG-NLG) and Charles University
underhumansupervision(wheremulti-turndialoguecannot
project SVV 260 698. SR is supported by a Facebook
bedisabled),andthatitshouldonlybedeployedafterex-
CIFAR AI Chair and NSERC Discovery Grant program.
haustivetestingwithpropersafeguards. Ourmodelsshould
TheprojectissupportedpartiallybytheGoogle-Milagrant.
notbedeployedandshouldonlybeusedforresearch.
WethankEsycommerceforprovidingtheirdataannotation
services and actively working with us in order to reach a DataCollection Wehavecollectedourbenchmarkusing
consistent and high quality data collection process. We expertannotators,whowereproperlytrained,familiarized
thankBennoKrojer,ChrisPal,DilekHakkani-Tu¬®r,Gokhan withthetaskandthepurposeoftheproject,andpaidfair
Tur,IsmailHaritaoglu,NicolasChapados,OndÀárejDusÀáek, wagerelativetotheircountryofemployment. Thewebsites
PeterShaw,SaiRajeswar,VaibhavAdlakha,theUIAssist inourdatasetarepubliclyaccessibleandsafe. Anyaccount
teamatServiceNowResearch,andtheMcGillNLPgroup appearing in the dataset was specifically created for the
membersforhelpfuldiscussions. datacollection;therearenoreferencestotheiridentityto
preservetheirprivacy.
ImpactStatement
References
Webnavigationagentshavethepotentialtobecomeapow-
erful technology with large societal impacts. Therefore, Adept. Adept ACT-1 ‚Äì ‚ÄùA machine learning model that
multipleaspectsneedtobetakenintoconsiderationwhen caninteractwitheverythingonyourcomputer.‚Äù. https:
conductingfurtherresearchinthisarea: //www.adept.ai/blog/act-1,2023. Accessed: 2023/08/31.
Automatingvs. ElevatingUsers Amajorriskofautomat-
Adlakha, V., Dhuliawala, S., Suleman, K., de Vries, H.,
ingwebnavigationistheautomationofworktraditionally
andReddy,S. TopiOCQA:Open-domainConversational
performedbyknowledgeworkers;deployinghighlycapable
QuestionAnsweringwithTopicSwitching. Trans.Assoc.
modelscouldleadtojoblosses. However,onemajordiffer-
Comput. Linguistics, 10:468‚Äì483, 2022. URL https://
encebetweenautonomousnavigationandourframework
doi.org/10.1162/tacl a 00471.
is that we require the inclusion of a human instructor to
providethereal-timeinstructionsneededtocompletethe Aghajanyan,A.,Okhonko,D.,Lewis,M.,Joshi,M.,Xu,H.,
task. Thus,conversationalwebnavigation‚Äôsultimatepur- Ghosh,G.,andZettlemoyer,L. HTLM:Hyper-textPre-
poseisnottoautomatethehumancompletely,butautomate trainingandPromptingofLanguageModels.InTheTenth
difficult, repetitive, and error-prone steps so that the user InternationalConferenceonLearningRepresentations,
canfocusonreliablysolvinghigh-levelproblems. ICLR2022,VirtualEvent,April25-29,2022,2022. URL
https://openreview.net/forum?id=P-pPW1nxf1r.
MaliciousUsageandMitigation Aswebnavigationmod-
elsbecomeincreasinglysophisticated,therearerisksthat
Alayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I.,
they will be used for malicious purposes at scale. These
Hasson, Y., Lenc, K., Mensch, A., Millican, K.,
modelscanautomateharmfulactivities,e.g.,forcreating
Reynolds, M., Ring, R., Rutherford, E., Cabi, S.,
spammessagesandimpersonatingindividualsforfraudu-
Han, T., Gong, Z., Samangooei, S., Monteiro, M.,
lentpurposes. Whiletheseactivitiescanalreadybepartially
Menick,J.L.,Borgeaud,S.,Brock,A.,Nematzadeh,A.,
automatedusingopen-sourcetools,7webnavigationagents
Sharifzadeh,S.,Binkowski,M.,Barreira,R.,Vinyals,O.,
couldmakeautomationeasierandmorerobust. However,
Zisserman, A., and Simonyan, K. Flamingo: a Visual
malicious actors can build such models in private using
Language Model for Few-shot Learning. In NeurIPS,
existingcommercialservices,independentofon-goingre-
2022. URLhttp://papers.nips.cc/paper files/paper/2022/
searchonagents. Ontheotherhand,bymakingourmodels
hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-
anddataaccessibletoresearchers,ourworkcanbeusedto
Conference.html.
researchwaystomitigatetheriskofmalicioususage;for
instance,byincorporatingourmodelsaspartofredteaming Assouel,R.,Marty,T.,Caccia,M.,Laradji,I.,Drouin,A.,
Rajeswar, S., Palacios, H., Cappart, Q., Vazquez, D.,
7Forexample,Selenium:https://www.selenium.dev/
Chapados,N.,Gasse,M.,andLacoste,A. Theunsolved
9WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
challenges of LLMs in open-ended web tasks: A case Chen,M.,Tworek,J.,Jun,H.,Yuan,Q.,deOliveiraPinto,
study. InNeurIPS2023FoundationModelsforDecision H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,
Making Workshop, 2023. URL https://openreview.net/ Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov,
forum?id=jt3il4fC5B. M.,Khlaaf,H.,Sastry,G.,Mishkin,P.,Chan,B.,Gray,
S.,Ryder,N.,Pavlov,M.,Power,A.,Kaiser,L.,Bavar-
Bavishi,R.,Elsen,E.,Hawthorne,C.,Nye,M.,Odena,A., ian, M., Winter, C., Tillet, P., Such, F. P., Cummings,
Somani, A., and Tas¬∏ƒ±rlar, S. Fuyu-8B: A Multimodal D., Plappert, M., Chantzis, F., Barnes, E., Herbert-
ArchitectureforAIAgents, October2023. URLhttps: Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak,
//www.adept.ai/blog/fuyu-8b/.
N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saun-
ders, W., Hesse, C., Carr, A. N., Leike, J., Achiam,
Brown,T.B.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,
J., Misra, V., Morikawa, E., Radford, A., Knight, M.,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,
Brundage,M.,Murati,M.,Mayer,K.,Welinder,P.,Mc-
G.,Askell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,
Grew, B., Amodei, D., McCandlish, S., Sutskever, I.,
G., Henighan, T., Child, R., Ramesh, A., Ziegler,
and Zaremba, W. Evaluating Large Language Models
D. M., Wu, J., Winter, C., Hesse, C., Chen, M.,
TrainedonCode. CoRR,abs/2107.03374, 2021. URL
Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
https://arxiv.org/abs/2107.03374.
Berner, C., McCandlish, S., Radford, A., Sutskever,
I., and Amodei, D. Language Models are Few-shot Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Learners. In Larochelle, H., Ranzato, M., Hadsell, R., Fedus,W.,Li,E.,Wang,X.,Dehghani,M.,Brahma,S.,
Balcan, M., and Lin, H. (eds.), Advances in Neural Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X.,
InformationProcessingSystems33: AnnualConference Chowdhery, A., Narang, S., Mishra, G., Yu, A., Zhao,
on Neural Information Processing Systems 2020, V. Y., Huang, Y., Dai, A. M., Yu, H., Petrov, S., Chi,
NeurIPS 2020, December 6-12, 2020, virtual, 2020. E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le,
URL https://proceedings.neurips.cc/paper/2020/hash/ Q.V.,andWei,J.ScalingInstruction-finetunedLanguage
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Models. CoRR, abs/2210.11416, 2022a. URL https:
//doi.org/10.48550/arXiv.2210.11416.
Bunian,S.,Li,K.,Jemmali,C.,Harteveld,C.,Fu,Y.,and
El-Nasr, M. S. VINS: Visual Search for Mobile User Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
InterfaceDesign.InCHI‚Äô21:CHIConferenceonHuman Fedus,W.,Li,E.,Wang,X.,Dehghani,M.,Brahma,S.,
FactorsinComputingSystems,VirtualEvent/Yokohama, Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X.,
Japan,May8-13,2021,pp.423:1‚Äì423:14,2021. URL Chowdhery, A., Narang, S., Mishra, G., Yu, A., Zhao,
https://doi.org/10.1145/3411764.3445762. V. Y., Huang, Y., Dai, A. M., Yu, H., Petrov, S., Chi,
E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le,
Burns, A., Arsan, D., Agrawal, S., Kumar, R., Saenko,
Q.V.,andWei,J.ScalingInstruction-finetunedLanguage
K., and Plummer, B. A. A Dataset for Interactive
Models. CoRR, abs/2210.11416, 2022b. URL https:
Vision-language Navigation with Unknown Command
//doi.org/10.48550/arXiv.2210.11416.
Feasibility. In Computer Vision - ECCV 2022 - 17th
European Conference, Tel Aviv, Israel, October 23-27, Cormer,M.,Mann,R.,Moffatt,K.,andCohen,R. Towards
2022,Proceedings,PartVIII,volume13668ofLecture anImprovedVision-basedWebPageSegmentationAlgo-
Notes in Computer Science, pp. 312‚Äì328, 2022. URL rithm. In201714thConferenceonComputerandRobot
https://doi.org/10.1007/978-3-031-20074-8 18. Vision(CRV),pp.345‚Äì352,2017.
Carroll, J. M. and Rosson, M. B. Usability Engineering. Dao,T. FlashAttention-2: FasterAttentionwithBetterPar-
InTopi,H.andTucker,A.(eds.),ComputingHandbook, allelismandWorkPartitioning. CoRR,abs/2307.08691,
ThirdEdition:InformationSystemsandInformationTech- 2023. URLhttps://doi.org/10.48550/arXiv.2307.08691.
nology,pp.32: 1‚Äì22.CRCPress,2014.
Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Re¬¥, C.
Chang,C.,Kayed,M.,Girgis,M.R.,andShaalan,K.F. A FlashAttention: Fast and Memory-efficient Exact
SurveyofWebInformationExtractionSystems. IEEE Attention with IO-awareness. In NeurIPS, 2022.
Trans.Knowl.DataEng.,18(10):1411‚Äì1428,2006. URL URL http://papers.nips.cc/paper files/paper/2022/
https://doi.org/10.1109/TKDE.2006.152. hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-
Conference.html.
Chen, H., Liu, X., Yin, D., and Tang, J. A Survey on
Dialogue Systems: Recent Advances and New Fron- Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M.,
tiers. SIGKDD Explor., 19(2):25‚Äì35, 2017. URL Le, Q. V., Mao, M. Z., Ranzato, M., Senior, A. W.,
https://doi.org/10.1145/3166054.3166058. Tucker, P. A., Yang, K., and Ng, A. Y. Large Scale
10WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Distributed Deep Networks. In Bartlett, P. L., Pereira, Systems 34: Annual Conference on Neural Infor-
F. C. N., Burges, C. J. C., Bottou, L., and Wein- mation Processing Systems 2021, NeurIPS 2021,
berger, K. Q. (eds.), Advances in Neural Information December 6-14, 2021, virtual, pp. 4157‚Äì4169, 2021.
Processing Systems 25: 26th Annual Conference on URL https://proceedings.neurips.cc/paper/2021/hash/
Neural Information Processing Systems 2012. Pro- 218344619d8fb95d504ccfa11804073f-Abstract.html.
ceedings of a meeting held December 3-6, 2012, Lake
He,H.,Yao,W.,Ma,K.,Yu,W.,Dai,Y.,Zhang,H.,Lan,Z.,
Tahoe, Nevada, United States, pp. 1232‚Äì1240, 2012.
andYu,D. WebVoyager: BuildinganEnd-to-endWeb
URL https://proceedings.neurips.cc/paper/2012/hash/
AgentwithLargeMultimodalModels,2024.
6aca97005c68f1206823815f66102863-Abstract.html.
He,P.,Liu,X.,Gao,J.,andChen,W. Deberta: decoding-
Deng,X.,Shiralkar,P.,Lockard,C.,Huang,B.,andSun,H.
enhancedBertwithDisentangledAttention. In9thInter-
DOM-LM:LearningGeneralizableRepresentationsfor
nationalConferenceonLearningRepresentations,ICLR
HTMLDocuments. CoRR,abs/2201.10608,2022. URL
2021,VirtualEvent,Austria,May3-7,2021,2021. URL
https://arxiv.org/abs/2201.10608.
https://openreview.net/forum?id=XPZIaotutsD.
Deng,X.,Gu,Y.,Zheng,B.,Chen,S.,Stevens,S.,Wang,
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
B.,Sun,H.,andSu,Y. Mind2Web:TowardsaGeneralist
M., Song, D., and Steinhardt, J. Measuring Massive
AgentfortheWeb. CoRR,abs/2306.06070,2023. URL
MultitaskLanguageUnderstanding. In9thInternational
https://doi.org/10.48550/arXiv.2306.06070.
Conference on Learning Representations, ICLR 2021,
VirtualEvent,Austria,May3-7,2021,2021. URLhttps:
Dettmers, T. and Zettlemoyer, L. The case for 4-bit
precision: k-bit Inference Scaling Laws. In Interna-
//openreview.net/forum?id=d7KBjmI3GmQ.
tional Conference on Machine Learning, ICML 2023,
HudecÀáek,V.andDusÀáek,O. AreLLMsAllYouNeedfor
23-29July2023, Honolulu, Hawaii, USA,volume202
Task-orientedDialogue? CoRR,abs/2304.06556,2023.
ofProceedingsofMachineLearningResearch,pp.7750‚Äì
URLhttps://doi.org/10.48550/arXiv.2304.06556.
7774, 2023. URL https://proceedings.mlr.press/v202/
dettmers23a.html. Humphreys, P. C., Raposo, D., Pohlen, T., Thornton, G.,
Chhaparia, R., Muldal, A., Abramson, J., Georgiev,
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
P., Santoro, A., and Lillicrap, T. P. A data-driven ap-
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
proach for learning to control computers. In Chaud-
M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,
huri,K., Jegelka,S., Song,L.,Szepesva¬¥ri,C., Niu,G.,
N. An Image is Worth 16x16 Words: Transformers
andSabato,S.(eds.),InternationalConferenceonMa-
for Image Recognition at Scale. In 9th International
chine Learning, ICML 2022, 17-23 July 2022, Balti-
Conference on Learning Representations, ICLR 2021,
more, Maryland, USA, volume 162 of Proceedings of
VirtualEvent,Austria,May3-7,2021,2021. URLhttps:
MachineLearningResearch,pp.9466‚Äì9482,2022. URL
//openreview.net/forum?id=YicbFdNTTy.
https://proceedings.mlr.press/v162/humphreys22a.html.
Furuta, H., Nachum, O., Lee, K., Matsuo, Y., Gu, S. S.,
HyperWrite. HyperWriteAIPersonalAssistant‚Äì‚ÄùThefirst
andGur,I. MultimodalWebNavigationwithInstruction-
publicly available AI agent that can operate a browser
finetunedFoundationModels. CoRR,abs/2305.11854, likeahuman.‚Äù. https://www.hyperwriteai.com/personal-
2023. URLhttps://doi.org/10.48550/arXiv.2305.11854.
assistant,2023. Accessed: 2023/08/31.
Google. The bfloat16 numerical format ‚Äî Cloud TPU, Jaccard,P. Thedistributionoftheflorainthealpinezone.1.
December2023.URLhttps://cloud.google.com/tpu/docs/ Newphytologist,11(2):37‚Äì50,1912.
bfloat16.
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
Goyal,Y.,Khot,T.,Agrawal,A.,Summers-Stay,D.,Batra, Chaplot,D.S.,deLasCasas,D.,Bressand,F.,Lengyel,
D.,andParikh,D. MakingtheVinVQAMatter: Elevat- G.,Lample,G.,Saulnier,L.,Lavaud,L.R.,Lachaux,M.,
ingtheRoleofImageUnderstandinginVisualQuestion Stock,P.,Scao,T.L.,Lavril,T.,Wang,T.,Lacroix,T.,
Answering. Int.J.Comput.Vis.,127(4):398‚Äì414,2019. andSayed, W.E. Mistral7B. CoRR,abs/2310.06825,
URLhttps://doi.org/10.1007/s11263-018-1116-0. 2023. URLhttps://doi.org/10.48550/arXiv.2310.06825.
Gur, I., Jaques, N., Miao, Y., Choi, J., Tiwari, M., Kann,K.,Ebrahimi,A.,Koh,J.J.,Dudy,S.,andRoncone,
Lee, H., and Faust, A. Environment Generation A.Open-domainDialogueGeneration:WhatWeCanDo,
for Zero-shot Compositional Reinforcement Learn- CannotDo,AndShouldDoNext. InProceedingsofthe
ing. In Advances in Neural Information Processing 4thWorkshoponNLPforConversationalAI,ConvAIat
11WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
ACL2022,Dublin,Ireland,May27,2022,pp.148‚Äì165, Laurenc¬∏on, H., Saulnier, L., Tronchon, L., Bekman, S.,
2022. URLhttps://doi.org/10.18653/v1/2022.nlp4convai- Singh,A.,Lozhkov,A.,Wang,T.,Karamcheti,S.,Rush,
1.13. A.M.,Kiela,D.,Cord,M.,andSanh,V. OBELICS:An
OpenWeb-scaleFilteredDatasetofInterleavedImage-
Karpukhin, V., Oguz, B., Min, S., Lewis, P. S. H., Wu, textDocuments,2023.
L., Edunov, S., Chen, D., and Yih, W. Dense Passage
RetrievalforOpen-domainQuestionAnswering. InWeb- Lee, K., Joshi, M., Turc, I. R., Hu, H., Liu, F., Eisensch-
ber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceed- los, J. M., Khandelwal, U., Shaw, P., Chang, M., and
ings of the 2020 Conference on Empirical Methods in Toutanova, K. Pix2Struct: Screenshot Parsing as Pre-
Natural Language Processing, EMNLP 2020, Online, training for Visual Language Understanding. In Inter-
November 16-20, 2020, pp. 6769‚Äì6781, 2020. URL nationalConferenceonMachineLearning,ICML2023,
https://doi.org/10.18653/v1/2020.emnlp-main.550. 23-29July2023,Honolulu,Hawaii,USA,volume202of
ProceedingsofMachineLearningResearch,pp.18893‚Äì
Kembhavi,A.,Salvato,M.,Kolve,E.,Seo,M.J.,Hajishirzi, 18912,2023. URLhttps://proceedings.mlr.press/v202/
H., and Farhadi, A. A Diagram is Worth a Dozen Im- lee23g.html.
ages. InLeibe,B.,Matas,J.,Sebe,N.,andWelling,M.
Li,J.,Xu,Y.,Cui,L.,andWei,F. MarkupLM:Pre-training
(eds.),ComputerVision-ECCV2016-14thEuropean
ofTextandMarkupLanguageforVisuallyRichDocu-
Conference,Amsterdam,TheNetherlands,October11-
mentUnderstanding. InProceedingsofthe60thAnnual
14,2016,Proceedings,PartIV,volume9908ofLecture
Meeting of the Association for Computational Linguis-
Notes in Computer Science, pp. 235‚Äì251, 2016. URL
tics (Volume 1: Long Papers), ACL 2022, Dublin, Ire-
https://doi.org/10.1007/978-3-319-46493-0 15.
land, May 22-27, 2022, pp. 6078‚Äì6087, 2022. URL
https://doi.org/10.18653/v1/2022.acl-long.420.
Kiesel, J., Kneist, F., Meyer, L., Komlossy, K., Stein, B.,
and Potthast, M. Web Page Segmentation Revisited:
Li,Y.,He,J.,Zhou,X.,Zhang,Y.,andBaldridge,J. Map-
EvaluationFrameworkandDataset. InCIKM‚Äô20: The
pingNaturalLanguageInstructionstoMobileUIAction
29thACMInternationalConferenceonInformationand
Sequences. InProceedingsofthe58thAnnualMeetingof
KnowledgeManagement,VirtualEvent,Ireland,October
theAssociationforComputationalLinguistics,ACL2020,
19-23,2020,pp.3047‚Äì3054,2020. URLhttps://doi.org/
Online, July 5-10, 2020, pp. 8198‚Äì8210, 2020. URL
10.1145/3340531.3412782.
https://doi.org/10.18653/v1/2020.acl-main.729.
Kim,G.,Baldi,P.,andMcAleer,S. LanguageModelscan Li,Z.,Zhang,X.,Zhang,Y.,Long,D.,Xie,P.,andZhang,
Solve Computer Tasks. CoRR, abs/2303.17491, 2023. M. TowardsGeneralTextEmbeddingswithMulti-stage
URLhttps://doi.org/10.48550/arXiv.2303.17491. Contrastive Learning. CoRR, abs/2308.03281, 2023a.
URLhttps://doi.org/10.48550/arXiv.2308.03281.
Kingma, D. P. and Ba, J. Adam: A Method for Stochas-
tic Optimization. In Bengio, Y. and LeCun, Y. (eds.), Li,Z.,Zhang,X.,Zhang,Y.,Long,D.,Xie,P.,andZhang,
3rd International Conference on Learning Represen- M. TowardsGeneralTextEmbeddingswithMulti-stage
tations, ICLR 2015, San Diego, CA, USA, May 7-9, Contrastive Learning. CoRR, abs/2308.03281, 2023b.
2015,ConferenceTrackProceedings,2015. URLhttp: URLhttps://doi.org/10.48550/arXiv.2308.03281.
//arxiv.org/abs/1412.6980.
Liu,E.Z.,Guu,K.,Pasupat,P.,Shi,T.,andLiang,P. Rein-
forcementLearningonWebInterfacesusingWorkflow-
Koh,J.Y.,Lo,R.,Jang,L.,Duvvur,V.,Lim,M.C.,Huang,
guidedExploration. In6thInternationalConferenceon
P.-Y.,Neubig,G.,Zhou,S.,Salakhutdinov,R.,andFried,
Learning Representations, ICLR 2018, Vancouver, BC,
D. VisualWebArena: EvaluatingMultimodalAgentson
Canada,April30-May3,2018,ConferenceTrackPro-
RealisticVisualWebTasks,2024.
ceedings,2018. URLhttps://openreview.net/forum?id=
ryTp3f-0-.
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,
C.H.,Gonzalez,J.,Zhang,H.,andStoica,I. Efficient
Liu,W.,Meng,X.,andMeng,W. ViDE:AVision-based
MemoryManagementforLargeLanguageModelServ-
ApproachforDeepWebDataExtraction. IEEETrans.
ing with PagedAttention. In Flinn, J., Seltzer, M. I.,
Knowl. Data Eng., 22(3):447‚Äì460, 2010. URL https:
Druschel,P.,Kaufmann,A.,andMace,J.(eds.),Proceed-
//doi.org/10.1109/TKDE.2009.109.
ingsofthe29thSymposiumonOperatingSystemsPrin-
ciples, SOSP2023, Koblenz, Germany, October23-26, Loshchilov, I. and Hutter, F. Decoupled Weight Decay
2023,pp.611‚Äì626,2023. URLhttps://doi.org/10.1145/ Regularization. In 7th International Conference on
3600006.3613165. LearningRepresentations,ICLR2019,NewOrleans,LA,
12WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
USA,May6-9,2019,2019. URLhttps://openreview.net/ OpenAI. ChatGPT Plugins. https://openai.com/blog/
forum?id=Bkg6RiCqY7. chatgpt-plugins,2023d. Accessed: 2023/09/03.
Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R.
Pasupat, P., Jiang, T., Liu, E. Z., Guu, K., and Liang, P.
OK-VQA: A Visual Question Answering Benchmark
Mappingnaturallanguagecommandstowebelements.
Requiring External Knowledge. In IEEE Conference
InRiloff,E.,Chiang,D.,Hockenmaier,J.,andTsujii,J.
on Computer Vision and Pattern Recognition, CVPR
(eds.),Proceedingsofthe2018ConferenceonEmpirical
2019, Long Beach, CA, USA, June 16-20, 2019, pp.
MethodsinNaturalLanguageProcessing,Brussels,Bel-
3195‚Äì3204, 2019. URL http://openaccess.thecvf.com/
gium,October31-November4,2018,pp.4970‚Äì4976,
content CVPR 2019/html/Marino OK-VQA A Visual
2018. URLhttps://aclanthology.org/D18-1540/.
Question Answering Benchmark Requiring External
Knowledge CVPR 2019 paper.html.
Peng, A., Wu, M., Kilpatrick, L., and Heidel, S. GPT-
Muennighoff, N., Tazi, N., Magne, L., and Reimers, N. 3.5 Turbo fine-tuning and API updates, August 2023.
MTEB:MassiveTextEmbeddingBenchmark. InVla- URLhttps://openai.com/blog/gpt-3-5-turbo-fine-tuning-
chos,A.andAugenstein,I.(eds.),Proceedingsofthe17th and-api-updates.
ConferenceoftheEuropeanChapteroftheAssociation
Pinsky,Y. Bardcannowconnecttoyourgoogleappsand
forComputationalLinguistics,EACL2023,Dubrovnik,
services,Sep2023. URLhttps://blog.google/products/
Croatia, May 2-6, 2023, pp. 2006‚Äì2029, 2023. URL
bard/google-bard-new-features-update-sept-2023/.
https://doi.org/10.18653/v1/2023.eacl-main.148.
Multi-On. Multi-on‚Äì‚ÄùTheworld‚ÄôsfirstPersonalAIAgent Popovic,M. chrF:charactern-gramF-scoreforautomatic
& Life Copilot‚Äù. https://multion.ai, 2023. Accessed: MTevaluation. InProceedingsoftheTenthWorkshopon
2023/08/31. StatisticalMachineTranslation,WMTatEMNLP2015,
17-18September2015,Lisbon,Portugal,pp.392‚Äì395,
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L.,
2015. URLhttps://doi.org/10.18653/v1/w15-3049.
Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders,
W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G.,
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Button, K., Knight, M., Chess, B., and Schulman, J.
Matena,M.,Zhou,Y.,Li,W.,andLiu,P.J. Exploring
WebGPT:Browser-assistedquestion-answeringwithhu-
theLimitsofTransferLearningwithaUnifiedText-to-
man feedback. CoRR, abs/2112.09332, 2021. URL
textTransformer. J.Mach.Learn.Res.,21:140:1‚Äì140:67,
https://arxiv.org/abs/2112.09332.
2020. URLhttp://jmlr.org/papers/v21/20-074.html.
Nogueira, R. F. and Cho, K. End-to-end Goal-driven
Web Navigation. In Advances in Neural Information Rastogi, A., Zang, X., Sunkara, S., Gupta, R., and Khai-
Processing Systems 29: Annual Conference on Neural tan,P. TowardsScalableMulti-domainConversational
Information Processing Systems 2016, December Agents: TheSchema-guidedDialogueDataset. InThe
5-10, 2016, Barcelona, Spain, pp. 1903‚Äì1911, 2016. Thirty-FourthAAAIConferenceonArtificialIntelligence,
URL https://proceedings.neurips.cc/paper/2016/hash/ AAAI2020,TheThirty-SecondInnovativeApplications
1579779b98ce9edb98dd85606f2c119d-Abstract.html. of Artificial Intelligence Conference, IAAI 2020, The
TenthAAAISymposiumonEducationalAdvancesinAr-
OpenAI. IntroducingTriton: Open-sourceGPUprogram-
tificial Intelligence, EAAI 2020, New York, NY, USA,
ming for neural networks, July 2021. URL https://
February7-12,2020,pp.8689‚Äì8696,2020. URLhttps:
openai.com/research/triton.
//doi.org/10.1609/aaai.v34i05.6394.
OpenAI. Introducing ChatGPT, November 2022. URL
Rawles,C.,Li,A.,Rodriguez,D.,Riva,O.,andLillicrap,
https://openai.com/blog/chatgpt.
T. P. Android in the Wild: A Large-scale Dataset for
OpenAI. GPT-4V(ision) System Card. Technical Re- AndroidDeviceControl. CoRR,abs/2307.10088,2023.
port,2023a. URLhttps://cdn.openai.com/papers/GPTV URLhttps://doi.org/10.48550/arXiv.2307.10088.
System Card.pdf.
Reimers, N.andGurevych, I. Sentence-BERT:Sentence
OpenAI. GPT-4TechnicalReport. CoRR,abs/2303.08774,
EmbeddingsusingSiameseBERT-networks. InProceed-
2023b. URLhttps://doi.org/10.48550/arXiv.2303.08774.
ingsofthe2019ConferenceonEmpiricalMethodsinNat-
OpenAI. New models and developer products an- uralLanguageProcessingandthe9thInternationalJoint
nounced at DevDay, November 2023c. URL ConferenceonNaturalLanguageProcessing(EMNLP-
https://openai.com/blog/new-models-and-developer- IJCNLP),pp.3982‚Äì3992,HongKong,China,November
products-announced-at-devday. 2019. URLhttps://aclanthology.org/D19-1410.
13WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Shaw,P.,Joshi,M.,Cohan,J.,Berant,J.,Pasupat,P.,Hu, Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
H., Khandelwal, U., Lee, K., andToutanova, K. From L.,Gomez,A.N.,Kaiser,L.,andPolosukhin,I.Attention
PixelstoUIActions: LearningtoFollowInstructionsvia is All you Need. In Advances in Neural Information
GraphicalUserInterfaces. CoRR,abs/2306.00245,2023. Processing Systems 30: Annual Conference on Neural
URLhttps://doi.org/10.48550/arXiv.2306.00245. Information Processing Systems 2017, December 4-9,
2017, Long Beach, CA, USA, pp. 5998‚Äì6008, 2017.
Shi,T.,Karpathy,A.,Fan,L.,Hernandez,J.,andLiang,P.
URL https://proceedings.neurips.cc/paper/2017/hash/
WorldofBits: AnOpen-domainPlatformforWeb-based
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
Agents. In Proceedings of the 34th International Con-
ferenceonMachineLearning,ICML2017,Sydney,NSW, Wang, Q., Fang, Y., Ravula, A., Feng, F., Quan, X., and
Australia,6-11August2017,volume70ofProceedings Liu, D. WebFormer: The Web-page Transformer for
of Machine Learning Research, pp. 3135‚Äì3144, 2017. Structure Information Extraction. In WWW ‚Äô22: The
URLhttp://proceedings.mlr.press/v70/shi17a.html. ACMWebConference2022,VirtualEvent,Lyon,France,
April25-29, 2022, pp.3124‚Äì3133, 2022. URLhttps:
Shu,R.,Mansimov,E.,Alkhouli,T.,Pappas,N.,Romeo,
//doi.org/10.1145/3485447.3512032.
S.,Gupta,A.,Mansour,S.,Zhang,Y.,andRoth,D. Dia-
log2API:Task-orientedDialoguewithAPIDescription Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and
andExamplePrograms. CoRR,abs/2212.09946, 2022. Zhou, M. MiniLM: Deep Self-attention Distillation
URLhttps://doi.org/10.48550/arXiv.2212.09946. for Task-agnostic Compression of Pre-trained Trans-
formers. In Larochelle, H., Ranzato, M., Hadsell, R.,
Sun, L., Chen, X., Chen, L., Dai, T., Zhu, Z., and Yu,
Balcan, M., and Lin, H. (eds.), Advances in Neural
K. META-GUI: Towards Multi-modal Conversational
InformationProcessingSystems33: AnnualConference
Agents on Mobile GUI. In Proceedings of the 2022
on Neural Information Processing Systems 2020,
ConferenceonEmpiricalMethodsinNaturalLanguage
NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Processing,EMNLP2022,AbuDhabi,UnitedArabEmi-
URL https://proceedings.neurips.cc/paper/2020/hash/
rates,December7-11,2022,pp.6699‚Äì6712,2022. URL
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
https://doi.org/10.18653/v1/2022.emnlp-main.449.
Wei,J.,Bosma,M.,Zhao,V.Y.,Guu,K.,Yu,A.W.,Lester,
Together. Redpajama: an open dataset for training large
B., Du, N., Dai, A. M., and Le, Q. V. Finetuned Lan-
languagemodels,october2023. URLhttps://github.com/
guage Models are Zero-shot Learners. In The Tenth
togethercomputer/RedPajama-Data.
InternationalConferenceonLearningRepresentations,
Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux, ICLR2022,VirtualEvent,April25-29,2022,2022. URL
M., Lacroix, T., Rozie`re, B., Goyal, N., Hambro, E., https://openreview.net/forum?id=gEZrGCozdqR.
Azhar,F.,Rodriguez,A.,Joulin,A.,Grave,E.,andLam-
Wolf,T.,Debut,L.,Sanh,V.,Chaumond,J.,Delangue,C.,
ple, G. LLaMA: Open and Efficient Foundation Lan-
Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,
guage Models. CoRR, abs/2302.13971, 2023a. URL
andBrew,J. HuggingFace‚ÄôsTransformers: State-of-the-
https://doi.org/10.48550/arXiv.2302.13971.
artNaturalLanguageProcessing. CoRR,abs/1910.03771,
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, 2019. URLhttp://arxiv.org/abs/1910.03771.
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., Bikel, D., Blecher, L., Canton-Ferrer, C., Wu, J., Zhang, X., Nichols, J., and Bigham, J. P. Screen
Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, Parsing: Towards Reverse Engineering of UI Models
J.,Fu,W.,Fuller,B.,Gao,C.,Goswami,V.,Goyal,N., fromScreenshots. InUIST‚Äô21: The34thAnnualACM
SymposiumonUserInterfaceSoftwareandTechnology,
Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas,
M.,Kerkez,V.,Khabsa,M.,Kloumann,I.,Korenev,A.,
VirtualEvent,USA,October10-14,2021,pp.470‚Äì483,
Koura,P.S.,Lachaux,M.,Lavril,T.,Lee,J.,Liskovich, 2021. URLhttps://doi.org/10.1145/3472749.3474763.
D.,Lu,Y.,Mao,Y.,Martinet,X.,Mihaylov,T.,Mishra,
Wu, J., Wang, S., Shen, S., Peng, Y., Nichols, J., and
P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J.,
Bigham, J. P. WebUI: A Dataset for Enhancing Vi-
Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith,
sual UI Understanding with Web Semantics. In Pro-
E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor,
ceedings of the 2023 CHI Conference on Human Fac-
R.,Williams,A.,Kuan,J.X.,Xu,P.,Yan,Z.,Zarov,I.,
tors in Computing Systems, CHI 2023, Hamburg, Ger-
Zhang,Y.,Fan,A.,Kambadur,M.,Narang,S.,Rodriguez,
many,April23-28,2023,pp.286:1‚Äì286:14,2023. URL
A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2:
https://doi.org/10.1145/3544548.3581158.
OpenFoundationandFine-tunedChatModels. CoRR,
abs/2307.09288,2023b. URLhttps://doi.org/10.48550/ Xia,M.,Gao,T.,Zeng,Z.,andChen,D. ShearedLLaMA:
arXiv.2307.09288. AcceleratingLanguageModelPre-trainingviaStructured
14WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Pruning. CoRR, abs/2310.06694, 2023. URL https:// AutonomousAgents.CoRR,abs/2307.13854,2023.URL
doi.org/10.48550/arXiv.2310.06694. https://doi.org/10.48550/arXiv.2307.13854.
Xiao, S., Liu, Z., Zhang, P., and Muennighof, N. C- Zhou, Y., Sheng, Y., Vo, N., Edmonds, N., and Tata, S.
Pack: Packaged Resources To Advance General Chi- SimplifiedDOMTreesforTransferableAttributeExtrac-
neseEmbedding. CoRR,abs/2309.07597,2023a. URL tionfromtheWeb. CoRR,abs/2101.02415,2021. URL
https://doi.org/10.48550/arXiv.2309.07597. https://arxiv.org/abs/2101.02415.
Xiao, S., Liu, Z., Zhang, P., and Muennighof, N. C- Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.
Pack: Packaged Resources To Advance General Chi- MiniGPT-4: Enhancing Vision-language Understand-
neseEmbedding. CoRR,abs/2309.07597,2023b. URL ing with Advanced Large Language Models. CoRR,
https://doi.org/10.48550/arXiv.2309.07597. abs/2304.10592, 2023. URL https://doi.org/10.48550/
arXiv.2304.10592.
Xu, N., Masling, S., Du, M., Campagna, G., Heck, L.,
Landay, J. A., and Lam, M. Grounding Open-domain
Instructions to Automate Web Support Tasks. In Pro-
ceedingsofthe2021ConferenceoftheNorthAmerican
Chapter of the Association for Computational Linguis-
tics: HumanLanguageTechnologies,NAACL-HLT2021,
Online, June 6-11, 2021, pp. 1022‚Äì1032, 2021. URL
https://doi.org/10.18653/v1/2021.naacl-main.80.
Yao, S., Chen, H., Yang, J., and Narasimhan, K. Web-
Shop:TowardsScalableReal-worldWebInteractionwith
Grounded Language Agents. In NeurIPS, 2022. URL
https://arxiv.org/abs/2207.01206.
Zhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C.,
Gao, X., Gao, J., Liu, J., and Dolan, B. DIALOGPT :
Large-scaleGenerativePre-trainingforConversational
ResponseGeneration. InProceedingsofthe58thAnnual
MeetingoftheAssociationforComputationalLinguistics:
System Demonstrations, ACL 2020, Online, July 5-10,
2020,pp.270‚Äì278,2020a.URLhttps://doi.org/10.18653/
v1/2020.acl-demos.30.
Zhang,Z.,Takanobu,R.,Zhu,Q.,Huang,M.,andZhu,X.
Recentadvancesandchallengesintask-orienteddialog
systems. ScienceChinaTechnologicalSciences,63(10):
2011‚Äì2027,2020b.
Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C., Xu,
M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S.,
Desmaison,A.,Balioglu,C.,Damania,P.,Nguyen,B.,
Chauhan,G.,Hao,Y.,Mathews,A.,andLi,S. PyTorch
FSDP:ExperiencesonScalingFullyShardedDataParal-
lel. Proc.VLDBEndow.,16(12):3848‚Äì3860,2023. URL
https://www.vldb.org/pvldb/vol16/p3848-huang.pdf.
Zheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. GPT-
4V(ision)isaGeneralistWebAgent,ifGrounded. CoRR,
abs/2401.01614, 2024. URL https://doi.org/10.48550/
arXiv.2401.01614.
Zhou,S.,Xu,F.F.,Zhu,H.,Zhou,X.,Lo,R.,Sridhar,A.,
Cheng,X.,Bisk,Y.,Fried,D.,Alon,U.,andNeubig,G.
WebArena: ARealisticWebEnvironmentforBuilding
15WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Contents
1 Introduction 1
2 RelatedWork 3
2.1 WebNavigationAgents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 WebsiteRepresentations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.3 ConversationalInterfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3 WEBLINX 4
3.1 Representingactionsandstatesformodeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
4 EvaluationFramework 5
4.1 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4.2 Turn-levelscoreandoverallscore . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
5 Methods 5
5.1 DenseMarkupRanking(DMR)forCandidateSelectionandInputRepresentation . . . . . . . . . . . . . 5
5.2 ModelingActions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
6 ExperimentalResults 6
6.1 OverviewofResults. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
6.2 QualitativeAssessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
7 Discussion 7
7.1 ExperimentalFindings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
7.2 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
8 Conclusion 8
A DatasetDetails 18
A.1 SupplementaryStatistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.2 CategoriesandSubcategories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.3 InputProcessingDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.4 OutputProcessingDetails. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.5 DataCollectionDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.6 ActionsandIntents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A.7 Websitesoverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B ModelingDetails 25
16WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
B.1 OptimalTextRepresentation(OTR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.2 StrategicTruncation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.3 Understandingthecategorizationofpretrainedmodels . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.4 TechnicalAspectsofDenseMarkupRanking(DMR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.4.1 EmpiricalSpeedImprovements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.5 InputTemplates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.5.1 TemplateforPix2Act . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.5.2 Templateforchat-basedmodels(LLaMA,GPT) . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.5.3 Templateforinstruction-basedmodels(Flan,Fuyu,MindAct) . . . . . . . . . . . . . . . . . . . 28
B.6 ModelImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
B.7 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
B.8 InputSamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
B.8.1 SampleinputforMindAct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
B.8.2 Sampleinputforinstruction-basedmodels(Flan,Fuyu). . . . . . . . . . . . . . . . . . . . . . . 31
B.8.3 Sampleinputforchat-basedmodels(LLaMA,GPT) . . . . . . . . . . . . . . . . . . . . . . . . 31
B.9 OutputSample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
C SupplementaryResults 34
C.1 ComparisonofMind2WebrepresentationwithOTR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C.2 Comparisonofimage-onlybaselinewithmultimodalmodels . . . . . . . . . . . . . . . . . . . . . . . . 34
C.3 Assessingimpactofmodelsizefortext-onlydecoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C.4 Generalizationcapabilitiesofevaluatedmodels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
C.5 ExtendedQualitativeAssessment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
D AdditionalResultTables 38
E InstructionsfortheAnnotators 43
17WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Appendix
A DatasetDetails
A.1 SupplementaryStatistics
InSection3,weintroduceWEBLINX.Inthissection,weprovidesupplementarystatisticsforreaderswishingtogaina
deeperunderstandingofthedataset.
InTable7,wereportdemoandturnstatisticsbyintent. Weobservethatsay,clickandloadareheavilyrepresentedacross
demos. However,thelatterhappenslessoftenthanotherintents. Thisisbecausetheuserloadsnewlinksonlywhenthey
movetoanewwebsite,andmanytaskscanbeaccomplishedwithinthesamepage(suchasbookingaflight). Therefore,
thereisnoneedtoloadnewpagesasfrequentlyasotherintents. Additionally,hoverislessrepresentedduetotheremoval
ofunnecessaryhovering, whichcanbeaccidentallyrecordedwhenmovingthecursoracrossnon-targetelementswith
callbacks.
InTable8,wepresentthenumberofdemosforeachsplitandmeannumberofturns. Althoughmostdemosareinthe
rangeof40-50turns,thenumberofdemosintheTEST splitissubstantiallylower. Thiscanbeattributedtothelack
VIS
offollow-upbasedonwhatishappeningonthescreen. Forexample,aninstructorwithvisioncanrequestthenavigator
toapplysomespecificfilters(e.g.,bysaying‚ÄùPleaseapplythefilterforJapanAirlinesundertheAirlinesfilteroption‚Äù),
whereasaninstructorwithoutvisionwouldnothavethisrequestunlesstheyareusingascreen-reader.
Browser
A ‚Üì url Chat
text
B
click(el) hover(el) input(el,str) change(el,str) load(str) scroll(int,int) message
+ -
say(str)
text text
OK
submit(el) copy(el,str) paste(el,str) tabCreate() tabSwitch(tab,tab)tabRemove(tab)
Figure5:Overviewoftheactionsinourbenchmark,including10browseractionsand1chataction.Anargumentofanactioncanbea
string(str),aninteger(int),anelement(el),orabrowsertabid(tab).TheintentsaredescribedinTable6.
Table6:CompletelistofWEBLINXactionspace.
Action Description
say(speaker, text) talkingtoinstructorornavigator
click(element) clickonanelement
click(x,y) clickonthecoordinatesmappingtoanelement
hover(element) hoveroveranelement
hover(x,y) hoveroverthecoordinatesmappingtoanelement
textinput(element, value) typetextintotheelement
change(element, text) changethevalueoftheelementtoanotheroption
load(url) loadtheURLofanewwebpage
submit(element) submittheform
scroll(x,y) scrolltothecoordinates
copy(element, text) copythetextfromtheelement
paste(element, text) pastethetextintotheelement
tabCreate() createanewtab
tabRemove(tabId) removethetab
tabSwitch(tabIdFrom, tabIdTo) switchbetweentabs
18WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table8: Turn-levelstatsbysplit. Activeturnsareusedforeither
Table7:Turn-levelstatsbyintent. finetuningorevaluation.Totalincludesturnsusedinhistory.
Intent #Demos ¬µturns œÉturns Total Split #Demos ¬µturns œÉturns Active Total
say 2337 16.82 5.62 39305 TRAIN 969 44.93 17.37 24418 43538
click 2333 14.52 10.16 33865 VALID 100 40.76 14.51 1717 4076
load 2324 1.59 1.07 3702 TESTIID 100 43.18 16.08 1846 4318
copy 1587 4.08 3.05 6477 TESTCAT 223 45.30 25.43 4979 10102
textInput 1465 3.28 3.06 4799 TESTWEB 211 40.47 18.17 4184 8540
paste 1130 1.89 1.95 2141 TESTVIS 444 36.05 20.09 7725 16006
scroll 1046 3.82 3.00 3999 TESTGEO 290 48.05 18.66 6141 13934
tabswitch 800 3.28 3.65 2621
tabcreate 712 1.71 1.12 1220 Table9:Turn-levelstatsbyuseofAItools(e.g.,ChatGPT)
submit 645 1.40 1.11 904
hover 361 1.55 1.11 560
UsesAI #Demos ¬µturns œÉturns Total
tabremove 309 1.94 1.17 599
change 165 1.95 1.34 322 ‚úó 2057 42.50 19.5 87414
‚úì 280 46.79 16.9 13100
InTable9,wehighlighttheusagefrequencyofAItools,whicharelistedinTable12. Forcertaintasks,suchassummarizing
newsarticles,itismuchmoreconvenienttouseAItools. Sincewefocusonactionsexecuted,modelscanlearngeneral
actionswhendealingwithAItools,evenwhenthetoolsthemselveschanges.
A.2 CategoriesandSubcategories
In Section 3, discuss the use of categories to classify demonstrations. We have in total 8 categories, each with their
own subcategories, which add up to a total of 50 (¬ß11); we assign one category and subcategory to Each of the 155
URLsub-domainassociatedwithademoturn(¬ß12). Sinceademomayleveragemultiplewebsites(e.g. composingand
informationlookup),ademowillhaveoneormoresubcategory. Wegivethefulllistofcategories,subcategories,andthe
numberofdemonstrationsassociatedwitheachinTable11.
In Table 10, we show the breakdown of subcategories for the TEST split (designed to test generalization to new
CAT
subcategories). Wenotethatthesubcategorieswereautomaticallychosentobetheoneswiththefeweroccurrencesacross
demos,allowingtohaveareasonablesplitsize.
Table10:Listofsubcategoriesbasedonsplits.
TESTCAT Spreadsheet,Handmade,Reviews,ComputerVision,Chatbot,Transport,Presentation,Furniture,
ProfessionalNetwork,Books,Tasks,AutomaticTranslation,QuestionAnswering,Encyclopedia,
Recipe,Geography
Others Stay,Stays,Transport,ScientificArticles,OnlineShopping,Tasks,Blog,DiscussionPlatform,
Recipe,Spreadsheet,Email,ResearchDirectory,MusicSharing,Chatbot,Presentation,Grocery,
Delivery,ImageSharing,AutomaticTranslation,VideoSharing,Encyclopedia,NewsArticles,
Forum,Entertainment,Magazine,Medical,Furniture,Educational,Kanban,SocialNetwork,
ImageGeneration,QuestionAnswering,Media,Notetaking,Agency,Government,SocialEvent,
Cooking,InstantMessaging,Finance,Books,Clothing,Restaurant,Calendar,WritingAssistant
Difference Handmade,Reviews,ComputerVision,ProfessionalNetwork,Geography
A.3 InputProcessingDetails
In Section 3.1, we introduce the components of a state s . More formally, we define the input of a model m to be
t
P (s ,a ),consistingofaprocessingfunctionP thatreceivess anda andreturnsarepresentationthatcan
m t 1:t‚àí1 m t 1:t‚àí1
serveasaninputtoamodel. Weprovidedetailsofourmethodbelow.
AdaptingP permodel Foreachmodelm,wetailorthefunctionP toaccommodatefordifferencesinmethodology.
m
Forimage-to-textmodels,wesequentiallyrenderv ,u ,a asheadertextofthescreenshoti (viewportv isincludedso
t r r t t
modelscanlocateboundingboxesofc ). Fortext-onlymodels,weprovided ,v ,u ,c ,a ,whichareformattedwith
t t t r t r
19WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table11:Numberofdemoseachsubcategoryappearsinforeachsplit.Notethatademomighthavemultiplesubcategorieswhenusing
morethanonewebsite(forexample,InformationLookupandComposing). Inthelastcolumn,wealsoincludethenumberofURLs
associatedwitheachsubcategory;theycorrespondtothewebsitesinTable12.
Category Subcategory Total Train Valid ID Vis Geo Cat Web #URLs
AITools Auto.Translation 53 0 0 0 10 0 43 0 4
Chatbot 408 178 19 21 82 42 31 35 3
ComputerVision 13 0 0 0 0 0 13 0 1
ImageGeneration 59 33 7 3 5 0 0 11 4
WritingAssistant 70 44 3 2 11 0 0 10 5
Booking Medical 34 0 0 0 9 25 0 0 3
Restaurant 77 28 6 5 14 24 0 0 6
SocialEvent 14 0 0 0 0 14 0 0 3
Stay 64 44 0 0 5 15 0 0 7
Stays 37 24 0 0 11 0 0 2 3
Transport 757 314 27 31 252 36 61 36 8
Composing Blog 62 34 2 3 15 0 0 8 4
Email 135 86 10 17 16 0 0 6 6
Notetaking 47 31 0 5 11 0 0 0 4
Recipe 20 0 0 0 3 0 17 0 1
Tasks 31 0 0 0 10 0 21 0 2
InformationLookup Agency 46 29 2 3 0 0 0 12 3
Educational 56 28 3 2 8 0 0 15 2
Encyclopedia 97 56 8 7 11 0 1 14 4
Entertainment 36 13 0 0 10 0 0 13 2
Forum 37 12 4 1 9 0 0 11 2
Geography 13 0 0 0 0 0 13 0 1
Government 36 0 0 0 9 27 0 0 2
Media 60 23 2 3 10 0 0 22 2
ResearchDirectory 10 0 0 0 10 0 0 0 2
Productivity Calendar 50 17 3 2 11 3 0 14 2
Finance 59 21 0 0 10 28 0 0 4
Kanban 50 20 2 3 16 0 0 9 3
Presentation 32 0 0 0 6 0 26 0 1
Spreadsheet 27 0 0 0 10 0 17 0 2
Shopping Clothing 93 18 6 4 8 57 0 0 6
Delivery 91 67 4 6 14 0 0 0 7
Furniture 6 0 0 0 5 0 1 0 1
Grocery 38 0 0 0 8 30 0 0 2
Handmade 15 0 0 0 0 0 15 0 1
OnlineShopping 87 51 3 2 31 0 0 0 7
SocialInteraction DiscussionPlatform 32 18 4 1 9 0 0 0 3
ImageSharing 60 30 6 9 0 0 0 15 4
InstantMessaging 32 11 0 0 11 0 0 10 2
MusicSharing 36 14 0 0 9 0 0 13 2
ProfessionalNetwork 14 0 0 0 0 0 14 0 1
QuestionAnswering 20 0 0 0 5 0 15 0 1
SocialNetwork 62 28 4 2 13 14 0 1 4
VideoSharing 20 10 0 0 1 0 0 9 1
Summarizing Books 25 0 0 0 10 0 15 0 2
Cooking 40 13 0 0 11 16 0 0 2
Magazine 49 24 0 1 11 13 0 0 4
NewsArticles 124 75 11 11 15 12 0 0 5
Reviews 13 0 0 0 0 0 13 0 1
ScientificArticles 35 10 4 2 10 0 0 9 2
promptp . Inmultimodalsettings,weincludei inadditiontotheformattedprompt. Templatesandsamplescanbefound
m t
inAppendicesB.5andB.8.
20WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
/ Uploads
Controls
Web Interface
Navigator
Instructs Recording Post-
Replies processing
Browser
Views*
Instructor Dataset
Figure6:Thedatacollectionprocess.Werecordinteractionsbetweenaninstructorandahumannavigator,includingchatandbrowser
actions.*InstructorcanseethescreenexceptinTESTVISsplit.
Candidateselection FollowingDengetal.(2023), weemployaseparatecandidateselectionstageinordertoreduce
thenumberoftheinputelementstointeractwith. Inthecandidateselectionstage,arankingmodelselectsasubsetofk
relevantelementsfromtheDOMtree,whichisthenpresentedtothemodelinamulti-choicesetup;inSection5.1,we
describeanovelapproachtowardscandidateselectiondesignedforreal-timeusecases. Whenthecandidateisselected,
c isreturnedtobeusedinP. Eachcandidatecontainsatag,XPath,boundingbox,attributesandchildrentags,which
t
aredelimitedwithsquarebrackets(e.g.,[[tag]]...[[xpath]]...). Examplesofcandidatesusedinsidepromptscanbe
foundinAppendixB.8.
Restrictinghistoryforinput Toaccommodatethemaximuminputlengthamodelcanreceive,wecanrestricta and
1:t‚àí1
u toselectasubsetwindowofw. Foractions,weselectthelastwinstancesbyeithertheinstructorornavigator. For
1:t‚àí1
instructorutterances,weonlyselectthefirstandlastw‚àí1instances,allowingustokeeptrackoftheinitialrequestwhile
focusingonthelatestupdatestotheinstruction. Forsimplicity,wedenotetherestrictedsetofactionsasa andutterancesas
r
u . SimilartoDengetal.(2023),wechoosew =5,allowingthemodeltoattendrecentactionswithoutgoingovercontext
r
limits.
A.4 OutputProcessingDetails
AlthoughthemodelisfinetunedtogenerateastringintheformatdescribedinSection3,therawoutputisnotconsistently
suitablefordirectexecution,andmaycontainunnecessaryartifacts. WeprocesstheoutputbyusingRegexpatternmatching
tofindthefirstsuitableintentcall,thenparsetheŒ±intokey/valuepairs,whichcanbecomparedwiththegroundtruth
actions.
Mappingcoordinatestoelements Visionmodelswithoutaccesstocandidateelementswillinsteadbeinstructedand
finetunedtochooseanelementbyspecifyingits(x,y)coordinates. Ifthereareoverlappingelementsatacoordinate,we
choosetheelementwiththesmallestareaatthegiven(x,y)coordinates(whichshouldbethetargetoftheinteractiondueto
thepropertiesoftheCSSboxmodel). Technically,theclicktargetstheelementwiththehighestz-index(thedepthaxisin
HTML),butsincewedonothaveaccesstoCSSpropertiesoftheobject,werelyonthedefaultrenderorder.
SegmentingURLsforloadactions Weuseurllib8tofirstsegmenttheURLintoanetworklocation(netloc)andthe
remaininghierarchicalpath(path). Tonormalizethenetloc,weremovetheleadingwwwfromit. Sinceapathisseparated
byaforwardslash(/),weusethischaractertoseparateeachsegmentinthepath. Thefinalresultisalistoftokens,each
representingapartoftheinitialURL.
A.5 DataCollectionDetails
In Table 3, we provide an overview of the data collection process to build the dataset component of WEBLINX. The
overviewoftheprocessisoutlinedinFigure6. Inthissection,wediveintothetechnicalandsupplementarydetailsofthe
8https://docs.python.org/3/library/urllib.parse.html
21WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
process.
WebsiteSelection Weassembledthelistofrecommendedwebsitestobeusedasstartingpoints,buttheannotatorswere
allowedtovisitanywebsitestheydeemedappropriateforthetask(fulllistavailableinSectionA.7). Theannotatorswere
giventhetimetobecomeacquaintedwiththespecificwebsitesbeforerecordingthedemonstrations. Weencouragedthe
annotatorstorecordbothshorter,single-taskdemonstrations,andmorecomplexdemonstrationsconsistingofmultiple
sub-tasks. Thedemonstrationendsoncetheinstructornotifiesthenavigatorthattheywishtoterminatethedemonstration.
RecordingDemonstrations Tocapturethestatesandactionsduringthedemonstration,weimplementedacustomChrome
browserextension. Foreachactioninthebrowser,theextensioncapturedthescreenshotofthepage,theDOMtreeofthe
page,andboundingboxesoftheelementsintheviewport. Theuseractionswerecapturedusingwebeventhandlers9,and
ChrometabCaptureAPI10wasusedtosavethestateofthepageforeachactioninthebackground. Forscreenrecording,
screensharing,andchatinterface,theannotatorsusedZoom11,afreevideomeetingsoftware. Wecombinedthechatwith
thebrowserstatesandactionsinthepostprocessingstage. Finally,theannotatorsvalidatedemonstrationstoensurethereare
nounnecessaryorincorrectlyorderedactions,andthattherearenotypographicerrors.
CuratingDemonstrations Theannotatorsuploadedtherecordeddemonstrationintoourcustomwebinterfacetoperfom
basicqualitychecks. Usingthereviewmode,theannotatorsthenremovedunnecessaryactions(suchashoveringover
elementsnotnecessaryforcompletingthetask),correctedtheorderofactions(whichwasoccasionallyincorrectdueto
asynchronousprocessing),andfixedtypographicalerrors. Wealsoimprovedthealignmentbetweenscreenshotsandactions
byre-aligningthescreenshotsbasedontheirsimilaritytotherespectivevideoframes.12
AnnotatorPay WepaidUS$7.5perhourforthedemonstrationrecordingandUS$5perhourforoverhead(preparation,
upload,andqualityreview),leadingtoanaverageUS$2.58perdemonstration. Therateissubstantiallyhigherthanthe
minimumwageintheregionwherethedataiscollected,butalsoincludesotheroverheadfees.
A.6 ActionsandIntents
The action a has a structure intent(Œ± ,...,Œ± ), where our core intents are: click, load (new page via URL), say
t 1 m
(navigator‚Äôsutterance),submit(e.g.,aform),textinput(e.g.,typingtextinthesearchbar);weshowexamplesofthese
actionsinFigures1and4. ThesetofargumentsŒ± willbedifferentfromeachaction. Commonlyusedargumentsare
theuniqueIDofanelementind andthetextargumentforsayortextinput. Tocomplementtheintentsdescribedin
t
Section3,weshowadiagramofpossibleargumentsforeachintentisprovidedinFigure5,withthefulllistshownin
Table6.
Evaluating intents Among the 13 recorded intent types, we focus on evaluating 5 types: click, load, say, submit,
textinput. Wealsousechangeandscrollaspredictiontargetsduringfinetuningastheyarenecessarytocompletea
demonstration. However,wedonotevaluatethemaschangedoesnotappearineverysplit(seeTable7)andscrollcannot
bereliablyevaluated. Theotherintents(copy, paste, tabswitch, tabcreate, hover, tabremove)areincludedin
thehistoryandtheassociatedstatesareavailablealongsideactiveintents;copy,paste,andhoverdonotaffectthestateof
thewebsite,whereasthetabactionsarenotmandatorytonavigateawebsite,asloadissufficienttogotoanywebsite.
A.7 Websitesoverview
Table12showsallentrypoints(websitewhereademostarts). Wechoosepopularandalsolesserknownsitestoachieve
categoricalandgeographicdiversity. Thewebsitesareeitherspecificallychosenbytheauthorsortheannotators, who
collaborativelyensuredtheyareappropriateforourtasks‚Äìconsequently,wedonotincludeunsafewebsites. Inthecaseof
socialinteractions,wechoosewebsiteswithtermsofuseprohibitingoffensivecontent. Forinstance,https://facebook.com
statesthat‚ÄúWeremovecontentthatcouldcontributetoariskofharmtothephysicalsecurityofpersons. Contentthat
threatenspeoplehasthepotentialtointimidate,excludeorsilenceothersandisn‚ÄôtallowedonFacebook.‚Äù13.
9developer.mozilla.org/en-US/docs/Web/Events
10developer.chrome.com/docs/extensions/reference/tabCapture
11zoom.us
12There-alignmentwasnecessarysincetheChromeAPIallowstocaptureonly1screenshotper500mswhichsometimescaused
delaysinscreenshotcapture.
13https://transparency.fb.com/policies/community-standards/
22WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table12:Websiteoverview
Name Category Subcategory Geography URL
Airbnb Booking Stays International https://www.airbnb.com
Airtable Productivity Spreadsheet International https://airtable.com
Aldi(Australia) Shopping Grocery Australia https://www.aldi.com.au/en/
Aliexpress Shopping OnlineShopping International https://www.aliexpress.com/
AllenAI‚ÄôsCVExplore AITools ComputerVision USA https://vision-explorer.allenai.org/
Amazon Shopping OnlineShopping International https://www.amazon.com
Asana Productivity Kanban International https://asana.com/
ASOS Shopping Clothing International https://www.asos.com/men/
BBCNews Summarizing NewsArticles International https://www.bbc.com/
BingImageCreator AITools ImageGeneration International https://www.bing.com/create
BingTranslator AITools Auto.Translation International https://www.bing.com/translator
Blogger Composing Blog International https://www.blogger.com/
Booking.com Booking Stays International https://www.booking.com
booknbook Booking Restaurant International https://www.booknbook.com/
Brandmark AITools ImageGeneration International https://brandmark.io/
Britannica Info.Lookup Encyclopedia International https://www.britannica.com/
Calculator.netInvestment Productivity Finance International https://www.calculator.net/investment-calculator.html
ChatGPT AITools Chatbot International https://openai.com/
cheaptickets Booking Transport International https://www.cheaptickets.com/
CIAWorldFactbook Info.Lookup Agency USA https://www.cia.gov/the-world-factbook/
CNN Summarizing NewsArticles International https://edition.cnn.com/
CopyAI AITools WritingAssistant International https://www.copy.ai/
DeepL AITools Auto.Translation International https://www.deepl.com
delivery Shopping Delivery USA https://www.delivery.com/
Dictionary Info.Lookup Encyclopedia International https://www.dictionary.com/
Discord SocialInteraction InstantMessaging International https://discord.com
Discourse SocialInteraction DiscussionPlatf. International https://try.discourse.org/
Doordash Shopping Delivery International https://www.doordash.com/
ebay Shopping OnlineShopping International https://www.ebay.com/
Encyclopedia.com Info.Lookup Encyclopedia International https://www.encyclopedia.com/
Etsy Shopping Handmade International https://www.etsy.com/in-en
EuropeanCommission Info.Lookup Government Europe https://europa.eu/
Eventbrite Booking SocialEvent International https://www.eventbrite.com
Eventbrite(AU) Booking SocialEvent Australia https://www.eventbrite.com.au/
expedia Booking Stay International https://www.expedia.com/
Facebook SocialInteraction SocialNetwork International https://www.facebook.com/login/
Fandom Info.Lookup Entertainment International https://www.fandom.com/
Fastmail Composing Email International https://fastmail.com/
Flickr SocialInteraction ImageSharing International https://www.flickr.com/
Frontiers Summarizing ScientificArticles International https://www.frontiersin.org/journals/
Genius SocialInteraction MusicSharing International https://genius.com
Gmail Composing Email International https://mail.google.com/
GMXEmail Composing Email International https://www.gmx.com/
GoogleBard AITools Chatbot International https://bard.google.com/
GoogleCalendar Productivity Calendar International https://calendar.google.com/calendar/
GoogleDocs Composing Notetaking International https://docs.google.com/document
GoogleFlights Booking Transport International https://www.google.com/travel/flights
GoogleKeep Composing Tasks International https://keep.google.com/
GoogleScholar Info.Lookup ResearchDirectory International https://scholar.google.com/
GoogleSheets Productivity Spreadsheet International https://docs.google.com/spreadsheets
GoogleSlides Productivity Presentation International https://docs.google.com/presentation
GoogleTranslate AITools Auto.Translation International https://translate.google.com
Gov.ofCanadaBudgetPlanner Productivity Finance Canada https://itools-ioutils.fcac-acfc.gc.ca/BP-PB/budget-planner-
tool
Grammarly(Paraphrasing) AITools WritingAssistant International https://www.grammarly.com/paraphrasing-tool
grubhub Shopping Delivery International https://www.grubhub.com/
Gutenberg Summarizing Books International https://www.gutenberg.org/
HackerNews SocialInteraction DiscussionPlatf. USA https://news.ycombinator.com/
Hostelworld Booking Stays International https://www.hostelworld.com/
hotels Booking Stay International https://in.hotels.com/
howstuffworks Info.Lookup Educational International https://www.howstuffworks.com/
Ikea Shopping Furniture International https://www.ikea.com/
IMDB Info.Lookup Entertainment International https://www.imdb.com/
Imgur SocialInteraction ImageSharing International https://imgur.com/
Independent.ie(Ireland) Summarizing NewsArticles Ireland https://www.independent.ie/
Instacart Shopping Delivery NorthAmerica https://www.instacart.com/
Instagram SocialInteraction ImageSharing International https://www.instagram.com/
investopedia Info.Lookup Media International https://www.investopedia.com/
Jack‚Äôs50topfoodbloggers Summarizing Cooking International https://jacksfoodblog.com/2020/04/26/50-top-food-
bloggers-of-2020-the-best-recipe-sites-ranked/
jamesonlinebookclub Summarizing Reviews International https://jamesonlinebookclub.com/
kayak Booking Stay International https://www.kayak.co.in/
KhanAcademy Info.Lookup Educational USA https://www.khanacademy.org/
Koo SocialInteraction SocialNetwork India https://www.kooapp.com/feed
Continuedonnextpage
23WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table12:Websiteoverview
Name Category Subcategory Geography URL
LinkedIn SocialInteraction Prof.Network International https://www.linkedin.com/
Loblaws(Canada) Shopping Grocery Canada https://www.loblaws.ca/
Luko.eu Booking Medical Europe https://de.luko.eu/en/advice/guide/best-rated-tierartz-
veterinarians-by-states/
Macy‚Äôs Shopping Clothing USA https://www.macys.com/
MarieClaire Summarizing Magazine International https://www.marieclaire.com/
MarketWatch Productivity Finance USA https://www.marketwatch.com/
Medium Composing Blog International https://medium.com/
Meetup(Glasgow,Scotland) Booking SocialEvent Scotland https://www.meetup.com/find/?eventType=
inPerson&source=EVENTS&location=gb--v2--Glasgow
momondo Booking Transport International https://www.momondo.in/
MyFitnessPal Composing Recipe International https://www.myfitnesspal.com/recipe/calculator
Myntra Shopping Clothing India https://www.myntra.com/
NASA Info.Lookup Agency USA https://www.nasa.gov/
NationalGeographic Summarizing Magazine International https://www.nationalgeographic.com/magazine
NewYorker Summarizing Magazine USA https://www.newyorker.com/
NewZealandGovernment Info.Lookup Government NewZealand https://www.govt.nz/
Nextdoor SocialInteraction DiscussionPlatf. International https://nextdoor.com/
NHS-Findadentist Booking Medical UK https://www.nhs.uk/service-search/find-a-dentist
Nightcafe AITools ImageGeneration International https://creator.nightcafe.studio/
nirvanahq Composing Tasks International https://www.nirvanahq.com
Notion Composing Notetaking International https://www.notion.so/
nytimes Info.Lookup Media USA https://www.nytimes.com/
OntarioVeterinarians Booking Medical Canada https://www.ovma.org/pet-owners/find-a-veterinarian/
OpenStax Summarizing Books International https://openstax.org/subjects
OpenTables Booking Restaurant International https://www.opentable.com
orbitz Booking Transport International https://www.orbitz.com/
Outlook Composing Email International https://outlook.live.com/
Penzu Composing Notetaking International https://penzu.com/
Perplexity AITools Chatbot International https://www.perplexity.ai/
Pinterest SocialInteraction ImageSharing International https://www.pinterest.com
PlosONE Summarizing ScientificArticles International https://plos.org/
Postmates Shopping Delivery USA https://postmates.com/
Proton Composing Email International https://proton.me/mail
Quandoo Booking Restaurant International https://www.quandoo.com/
QuillBot AITools WritingAssistant International https://quillbot.com
Quora SocialInteraction QuestionAnswering International https://quora.com
Reader‚ÄôsDigest(Australia) Summarizing Magazine Australia https://www.readersdigest.com.au/
Reddit Info.Lookup Forum International https://www.reddit.com/
Resy Booking Restaurant International https://resy.com/
ReversoTranslation AITools Auto.Translation International https://www.reverso.net/text-translation
seamless Shopping Delivery USA https://www.seamless.com/
SemanticScholar Info.Lookup ResearchDirectory International https://www.semanticscholar.org/
Simplenote Composing Notetaking International https://simplenote.com/
SingaporeFoodBlogs Summarizing Cooking Singapore https://ordinarypatrons.com/popular-singapore-food-blogs/
skyscanner Booking Transport International https://www.skyscanner.com/
Slack SocialInteraction InstantMessaging International https://slack.com
sncf Booking Transport France https://sncf.com/
Soundcloud SocialInteraction MusicSharing International https://soundcloud.com
Squarespace Composing Blog International https://squarespace.com/
StableDiffusion AITools ImageGeneration International https://huggingface.co/spaces/stabilityai/stable-diffusion
StackExchange Info.Lookup Forum International https://stackexchange.com/
tableagent Booking Restaurant International https://tableagent.com/
target Shopping OnlineShopping International https://www.target.com/
TheGuardian Summarizing NewsArticles International https://www.theguardian.com/
TheMarshallaProject Summarizing NewsArticles USA https://www.themarshallproject.org/
thefork Booking Restaurant Europe https://www.thefork.com/
Todoist Productivity Kanban International https://todoist.com/app/
Tome AITools WritingAssistant International https://tome.app/
Travelocity Booking Stay International https://www.travelocity.com/
Trello Productivity Kanban International https://trello.com/
Trip Booking Transport International https://www.trip.com/
tripadvisor Booking Stay International https://www.tripadvisor.com/
Trivago Booking Stay India https://www.trivago.in/en-IN
Tumblr SocialInteraction SocialNetwork International https://www.tumblr.com/
Twitch SocialInteraction VideoSharing International https://www.twitch.tv
Twitter SocialInteraction SocialNetwork International https://twitter.com
ubereats Shopping Delivery International https://www.ubereats.com/
UNIQLO(Europe) Shopping Clothing Europe https://www.uniqlo.com/eu/en/home
ViaRail Booking Transport Canada https://www.viarail.ca/en
vrbo Booking Stay International https://www.vrbo.com/
walmart Shopping OnlineShopping International https://www.walmart.com/
Wattpat Composing Blog International https://www.wattpad.com/
wayfair Shopping OnlineShopping International https://www.wayfair.com/
WealthsimpleTaxCalculator Productivity Finance Canada https://www.wealthsimple.com/en-ca/tool/tax-calculator
Continuedonnextpage
24WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table12:Websiteoverview
Name Category Subcategory Geography URL
When2meet Productivity Calendar International https://www.when2meet.com/
Wikipedia Info.Lookup Encyclopedia International https://wikipedia.org/
WorldAtlas Info.Lookup Geography International https://www.worldatlas.com/
WorldHealthOrganization Info.Lookup Agency International https://www.who.int/
YahooMail Composing Email International https://mail.yahoo.com/
YouWrite AITools WritingAssistant International https://you.com/write
YouTube SocialInteraction VideoSharing International https://youtube.com
Zalora Shopping Clothing SoutheastAsia https://www.zalora.com/
Zappos Shopping OnlineShopping USA https://www.zappos.com/
Zara(Philippines) Shopping Clothing Philippines https://www.zara.com/ph/en/
B ModelingDetails
B.1 OptimalTextRepresentation(OTR)
SimilartoMind2Web(Dengetal.,2023),weusethetop-10candidatesselectedbyDMR(¬ß5.1)andstartbypruningthe
DOMtreetocontainelementsrelevanttothecandidates. However,wemakethefollowingchanges:
1. HTML:Inadditiontotagsandchildren,weincorporateattributesandvaluesofelementsintheDOMtree.Forexample,
adivelementwithattributesclassmappingtocontainerwouldbeprovidedasdiv class="container"(...),
where... wouldbethechildrenelements.
2. Viewport: Wespecifytheviewportsize,whichcanbeusedbythemodeltocalculatethecoordinatesofthebounding
boxeswithrespecttothescreen.
3. Candidaterepresentation: WeincludetheXMLPathandboundingboxcoordinates,andusetwosquarebracketsto
separatethetwoelements. Weuseatemplate[[xpath]] /html/<...>/<tag> [[bbox]] x=<x> y=<y> width=<w>
height=<h>,where<x>,<y>,<w>,<h>aretheboundingboxcoordinates,and<tag>isthetagofthetargetelement,
with<...>replacedwiththeparents. Furthermore,insteadofmappingeachcandidateitsalphabeticalorder,weprefix
itwithitsuniqueID,allowingthemodeltodirectlyrefertoanelementratherthanhavingtoremapthealphabetical
orderbacktoanelementreference.
4. Truncation: We truncate the final result as described in Section 5.1 and Appendix B.2. We choose limits that
maximizestheinformationincludedinthecontextwhileremainingunderanideallimitthatiscompatiblewithall
modelsconsidered(seeAppendixB.7forhyperparameterdetails).
B.2 StrategicTruncation
InSection5.1,wehighlighttheimportanceofreducingtheinputsequencelength,i.e.,toavoidexceedingthelimitallowed
bymodelsusedinourexperiments. Althoughcertainmodelscanprocesslongersequences,shortersequencesarefaster
toprocess,requireslessmemoryandrequirelowerrunningcostwhenusingproprietaryLLMs. Naivelytruncatingfrom
therightorleftsidecouldleadtomajorinformationloss. Toavoidthis,wesetalimittoeachcomponentoftheinputtext
(d , u , c , a ). Then, wetruncateeachcomponentbasedonthelimitbydecomposingthemintosub-componentsand
t r t r
strategicallytruncatingeachsub-componentsuntilthelimitisreached.
Definition Foragivenlimit(innumberoftokens),ourgoalistotruncateacomponent(oneofd ,u ,a ,c )untilwereach
t r r t
thelimit. Ifacomponentwasalreadyunderthelimit,thenthedifferenceissavedforc ,whichiscomputedlast.
t
Rendering-based reduction Since a component is an object (e.g., d is an element tree), we need to obtain the text
t
representation before being able to estimate the number of tokens. We thus need a rendering function that converts a
componentorsub-componentintotext,whichcanthenbetokenized. Then,wecanestimatethereduction(numberoftokens
totakeaway)inordertoreachthelimit.
Sub-components Eachcomponentiscomposedofsub-components,whichwecanrender,tokenizeandtruncateindividually.
Inthecaseofd ,sincewehaveatreeofelementswheretheattributeshouldbepreserved,weonlycountthevaluesandtext
t
contentassub-components. Forc ,weconsiderthexpath,attributesandchildrentagstobesub-components,protecting
t
thetagandboundingbox,aswellasthekeysinsidethesquarebrackets. Foru ,wesimplyconsidereachutteranceasa
r
sub-component. Fora ,eachactionisconsideredasub-component.
r
Reducingbylength Althoughitissimplertoreduceallsub-componentsequally,thismayleadtoscenarioswhereshort
sub-componentsareheavilypenalizedduetoverylongsub-componentsmakingupmostofthetokencounts. Toavoid
25WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
this, we instead find a threshold such that, by reducing all sub-components above this threshold, the sub-components‚Äô
truncatedlengthssumuptothetargetlimit. Thisthresholdcanbeeasilycomputedbyfirstsortingthesub-components,
theniteratethroughthelengthsuntilthecumulativesumisgreaterthanthelimit,beforefinallyreducingthelengthofthe
sub-componentsuntilthecumulativesumisunderthelimit.
Byapplyingthestepsabove,wecanensurethateachcomponentrespectsalimit,whichwecansetinawaythattheyaddup
toadesiredtotallimit,suchasL=2048.
B.3 Understandingthecategorizationofpretrainedmodels
InSection5.2,wedistinguishthreetypesofmodelsdependingontheirmodality:
Text-OnlyModels Bytext-onlymodels,wedenotetheencoder-decoderordecoder-onlyTransformermodels(Vaswani
etal.,2017)usingtextastheironlyinputmodality(Chungetal.2022a;Touvronetal.2023a;b;Jiangetal.2023,i.a). There
arecertaininherentlimitationstext-onlymodelsusedforwebnavigation,e.g.,theinabilitytoprocessimagesorpagelayouts.
AnotherpracticalchallengeisthelengthoftheHTMLcode,containingpotentiallythousandsofelementstointeractwith.
Image-to-textModels Byimage-to-textmodels,wedenotethemodelswithanimage(i.e.,thescreenshotofthewebsite)
astheironlyinputmodality. Image-to-textmodelsrepresentingwebsitesfromrawpixelshavealongtraditioninweb
navigationresearch,startingwithRLapproachesbasedonconvolutionalnetworks(Humphreysetal.,2022). Inourwork,
wefocusonPix2Act(Shawetal.,2023),anencoder-decodermodelspecializedattextgenerationwhengivenscreenshots
ofbrowsers. ItusesaVisionTransformer-based(Dosovitskiyetal.,2021)encoderandisfinetunedfromthePix2Struct
model(Leeetal.,2023)onwebnavigationtasks,usingonlypixelsasinput. Themainchallengeforimage-to-textmodelsis
theirinabilitytoprocesslongerinputinstructions(sincethetextmustbeembeddedinsidetheimageasheaders),forcingit
torelyonthescreenshot.
Multimodal Models By multimodal models, we denote the models which accept both image and text as their input
modality(Alayracetal.,2022;Laurenc¬∏onetal.,2023;Zhuetal.,2023). Multimodalmodelshavethepotentialtomitigate
the disadvantages of text-only and image-to-text models. However, due to their novelty, their use for web navigation
isunderexploredinresearch. However,therearepubliclyavailablemultimodalmodelscapableofrecognizingbrowser
screenshots(Bavishietal.,2023),buttheyaremainlyofferedasacommercialproducts; inSection5,wedescribeour
experimentswiththepublicvariantofthismodel. Thus,themainchallengeofusingmultimodalmodelsforwebnavigation
isthelackofmodelspretrainedtosimultaneouslyparseHTMLcodeandprocesswebsitescreenshots.
B.4 TechnicalAspectsofDenseMarkupRanking(DMR)
InSection5.1,weintroducetheDenseMarkupRanking(DMR)methodasawaytoefficientlyselectcandidateelementsfor
thedownstreamtask. Inthissection,wetakeacloserlookatthetechnicalaspectsofthemethod.
Definition LetE(x)betheencoderoutputvectorforaninputtextx.Forturnt,wehavethetheprocessedtextrepresentation
ofthestateP (s ),whichweusetoscorecandidateelementc ,whichisrepresentedastext. Wesetthelabely(c )=1
DMR t t,i t,i
whenc isthetargetcandidate,otherwisey(c )=0. Thecosinesimilaritylossisdefinedasthefollowingmean-squared
t,i t,i
error:
L =‚à•y(c )‚àísim (E(P (s )),E(c ))‚à• ,
t t,i cos DMR t t,i 2
wherethecosinesimilarityisdefinedassim (x,y)=(x¬∑y)/(‚à•x‚à•‚à•y‚à•). Duringinference,thecosinesimilarityisusedto
cos
generateascoreforeachinstancerepresentingthesimilaritybetweenP (s )andcandidateatturnt. Thescoreisusedto
DMR t
rankthecandidatesandchoosethetop-kcandidatesfortheactionpredictionstage.
ComputationalEfficiency Forasequencelengthnandamodelembeddingsizee,thecomplexityofself-attentionis
O(n2¬∑e)(Vaswanietal.,2017). Giventhelengthsofastate|s |andacandidate|c |,thecomplexityofacosine-based
t t,i
scoring is O(|P (s )|2 +|c |2) instead of O((|P (s )|+|c |)2) for the cross-encoder approach of Deng et al.
DMR t t,i DMR t t,i
(2023). Thisdifferencemakesamajorimpactwhen|P (s )|and|c |becomelarge. Wealsopurposefullyfinetune
DMR t t,i
encodermodelswithsmallere(Reimers&Gurevych,2019;Lietal.,2023a;Xiaoetal.,2023b).
Selectingrankingmodel Ourtaskcanbeformulatedasatextretrievaltask: wehaveamodel(DMR)thatencodesaquery
P (s )andcompareitwithadocumentc ,resultinginascorethatcanbeusedtorankcandidates. Thus,weexamine
DMR t t,i
variousmodelsthatweretrainedontextretrievaltasks,astheytendtotransferwelltoadjacentretrievaltasks. Asweaimto
achieveahighinferencespeed,wespecificallychoosesmallermodels,allowingustomaximizethecomputationbudgetof
26WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table13:Comparisonofcandidateselectionmethods(DMRandMindAct-RoBERTa)forthecombinedin-domain(ID)andout-of-domain
splits.WereportRecall@10scores.
Model ID TESTVIS TESTGEO TESTCAT TESTWEB TESTOOD
BGE 74.44 60.07 48.82 43.61 47.55 50.01
GTE 73.24 56.91 44.46 42.74 48.39 48.16
MiniLM 74.27 59.73 50.95 44.05 52.75 51.87
DeBERTa 76.86 63.28 52.76 48.43 54.65 54.78
thedownstreamlanguagemodel. Wefirstchooseall-MiniLM-L6-v2,amodeldevelopedbyReimers&Gurevych(2019)
basedontheMiniLMmodel(Wangetal.,2020). Wealsousebge-small-en-v1.5(Xiaoetal.,2023a)andgte-base(Li
etal.,2023b),whicharetwosmallermodelsthatachievecompetitiveresultsontheMTEBbenchmark(Muennighoffetal.,
2023). Thisbenchmarkwasspecificallychosenbecauseitthoroughlyevaluatesretrieversacrossadiverserangeoftasks.
Finetuningandresults Wefinetuneeachofthemodelsabove,aswellasthecross-encoderproposedbyDengetal.(2023)
(usingtheoriginalauthor‚Äôstrainingcode). TheresultsareshowninTable13,wherewereporttherecall@10,ametricthat
evaluateshowoftenthecorrectresultisinthetop-10candidatesretrieved. WeobservethatMiniLMachievesbetteroverall
resultscomparedtootherretrieversandisclosetotheDeBERTacross-encoderfromMindAct,whilebeingsubstantially
morecomputationallyefficient. Basedonthoseimprovements,weusethefinetunedMiniLMmodelasthebackboneofour
DMRmethod. AlldownstreamresultsincludethesamecandidatesproposedbyDMR.
B.4.1 EMPIRICALSPEEDIMPROVEMENTS
Usingthesameenvironment,CPU(AMDEPYC7453)andGPU(RTXA6000),weobservethatDMR-MiniLMtook4545
secondstoprocesstheentiretrainingset,whereasM2W-DeBERTatook22,385seconds. Sincethereare24,418activeturns,
M2W-DeBERTaneededonaverage916mstoselectedcandidatesateveryturn,whereasDMR-MiniLMneeded186ms. It
isimportanttohighlightthatahighlatencyforselectingcandidatecouldrestrictthepotentialreal-timeusecases(especially
withlargerHTMLpages),sincetheselectedcandidatesneedtobesenttothemodelinchargeofgenerationactions;inthe
caseofLLM,theinferencecouldtakeasignificantamountoftime,andmayincludeanetworkoverheadforwebAPIs
likeGPT-4V.Networklatencyisdifficulttoreduceduetovariousexternalfactors,whereasLLMs‚Äôinferencetimecanbe
reducedthroughalgorithmicimprovements,suchasFlashAttention(Daoetal.,2022;Dao,2023),quantization,suchas
4-bitquantization(Dettmers&Zettlemoyer,2023),andhardwareoptimizationatthehardwarelevel(OpenAI,2021;Kwon
etal.,2023,interalia). Ourmethodcanbecombinedwithsuchimprovementstominimizedelaybetweenactionsandavoid
interruptingtheuser‚Äôsflowofthoughts,whichwouldrequirethetotaltimetobeunder1second(Carroll&Rosson,2014).
B.5 InputTemplates
WeprovidethetemplatesforPix2Act‚Äôsheaders(AppendixB.5.1),forchat-basedmodelslikeLLaMA-2andGPT(Ap-
pendixB.5.2),andfortheinstruct-basedmodels(AppendixB.5.3).
B.5.1 TEMPLATEFORPIX2ACT
Viewport(height={{HEIGHT}}, width={{WIDTH}}) ---- Instructor Utterances: {{FIRST UTTERANCE}} ---- {{PAST UTTERANCES x (W-1)}}
Previous Turns: {{PAST ACTIONS}}
B.5.2 TEMPLATEFORCHAT-BASEDMODELS(LLAMA,GPT)
{{HTML REPRESENTATION}}}
Above are the pruned HTML contents of the page.You are an AI assistant with a deep understanding of HTML and you must predict actions
(cid:44)‚Üí based on a user request, which will be executed. Use one of the following, replacing [] with an appropriate value:
(cid:44)‚Üí change(value=[str], uid=[str]) ; click(uid=[str]) ; load(url=[str]) ; say(speaker="navigator", utterance=[str]) ; scroll(x=[int],
(cid:44)‚Üí y=[int]) ; submit(uid=[str]) ;text_input(text=[str], uid=[str]) ;
The user's first and last 4 utterances are: {{PAST UTTERANCES}};
Viewport size: {{HEIGHT}}h x {{WIDTH}}w ;
Only the last {{W}} turns are provided.
Here are the top candidates for this turn: {REPEAT 10 TIMES}
27WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
(uid = ...) [[tag]] ... [[xpath]] ... [[bbox]] x=X y=Y width=W height=H [[attributes]] attr1=val1 ... [[children]] {{TAG}}
{END REPEAT}
{{PAST ACTIONS}}
Please select the best action using the correct format, do not provide any other information or explanation.
B.5.3 TEMPLATEFORINSTRUCTION-BASEDMODELS(FLAN,FUYU,MINDACT)
{{HTML REPRESENTATIONS}}
Above are the pruned HTML contents of the page.You are an AI assistant with a deep understanding of HTML and you must predict actions
(cid:44)‚Üí based on a user request, which will be executed. Use one of the following, replacing [] with an appropriate value:
(cid:44)‚Üí change(value=[str], uid=[str]) ; click(uid=[str]) ; load(url=[str]) ; say(speaker="navigator", utterance=[str]) ; scroll(x=[int],
(cid:44)‚Üí y=[int]) ; submit(uid=[str]) ;text_input(text=[str], uid=[str]) ;
The user's first and last 4 utterances are: {{PAST UTTERANCES}};
Viewport size: {{HEIGHT}}h x {{WIDTH}}w ;
Only the last {{W}} turns are provided.
Here are the top candidates for this turn: {REPEAT 10 TIMES}
(uid=...) [[tag]] ... [[xpath]] ... [[bbox]] x=X y=Y width=W height=H [[attributes]] a=val1 ... [[children]] {{TAG}}
{END REPEAT}
{REPEAT W-1 TIMES}
User: {{PAST ACTION BY USER}}
Assistant: {{PAST ACTION BY ASSISTANT}}
{END REPEAT}
USER: {{LAST ACTION BY USER}} Please select the best action using the correct format, do not provide any other information or
(cid:44)‚Üí explanation.
Assistant:
B.6 ModelImplementation
InSection5,weprovideanoverviewofallmodelsusedinourexperiments. Anin-depthdescriptionofthemodelscanbe
foundbelow. Eachmodelwasfinetunedonceforagivensetofhyperparametersduetothecomputationalcostassociated
witheachexperiment;wealsoconsiderthatnorandominitializationwereintroducedforthetask,andweuseafixedseed
forreproducibility.
MindAct Dengetal.(2023)proposesatwo-stagetext-onlywebnavigationmodelconsistingofthecandidategeneration
andtheactionpredictionstage.Forthecandidategenerationstage,weusedourcustomDMRmodeldescribedinSection5.1.
Fortheactionpredictionstage,wereusetheirhyperparameters,implementtheirtextformattingmethods,andalsostartfrom
theMindActcheckpoints14finetunedfromFlan-T5(Chungetal.,2022a). However,theirproposedmulti-stepelimination
methodrequires13generationstepstoprocessk =50candidates,whichsubstantiallyincreaseslatencyandcomputation
cost. Instead,weusethetopk =10candidatesoutputbyDMR,whichonlyrequiresasinglegenerationstep.
Pix2Act FollowingthebehaviorcloningmethodproposedinPix2Act(Shawetal.,2023),wefinetunethemodelstarting
fromthePix2Structbackbone(Leeetal.,2023)todirectlypredictactiona foragivenP(s ,a ). Themodelusesan
t t 1:t‚àí1
imageencoderandtextdecoderbasedontheVisionTransformer(Dosovitskiyetal.,2021)anditwaspretrainedforparsing
screenshotsintostructuredrepresentations. Weembedthepromptandtextintheheaderareaofthescreenshot,resultingin
asinglescreenshotforeachstate. Sinceitdoesnothaveaccesscandidateelements,wefinetunedthismodeltopredictthex
andycoordinates,whichismappedtothemostrelevantelement(seeSectionA.4),makingtheresultingoutputcomparable
tocandidate-augmentedmodels.
Flan-T5withOTR ForFlan-T5experiments,weusethesamehyperparametersasMindAct,andstartfromtheFlan-T5
checkpoints(Chungetal.,2022b),whichisaT5model(Raffeletal.,2020)basedonFLAN(Weietal.,2022). However,
whereasMindActusestheMind2Webformat,weusetheOTRformatintroducedinthiswork.
LLaMA-2 Whereasallthemodelsaboveusetheencoder-decoderarchitecture,wefurtherexploredecoder-onlyapproaches.
Tothisend,wefinetunethevariantofLLaMA-2(Touvronetal.,2023a;b)with7Band13Bparametersthatwastrained
onhumanfeedbackforchat15. Wechosethismodeldueitsstrongperformanceonawiderangeofbenchmark,including
MMLU(Hendrycksetal.,2021)andHumanEval(Chenetal.,2021). Unlikethebasemodels,wecanleveragetheprior
capabilitiesofthechat-hfvarianttofollowinstructionsthroughturn-basedlanguagemodeling,allowingabetterstart
14Availableat:https://huggingface.co/osunlp/MindAct ActionPrediction flan-t5-xl
15AlsoknownasLLaMA-2-*b-chat-hf
28WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
duringfinetuning. FollowingourFlan-T5experiments,wealsouseOTR.
Sheared-LLAMA AsafasterandsmallerreplacementforLLAMA-2,weexploreSheared-LLAMA(Xiaetal.,2023),
whichprunesLLAMA-2-7Bandcontinuespretrainingon50BtokensfromtheRedPajamadataset(Together,2023). This
allowsittooutperformmodelsofcomparablesizesthatweretrainedfromscratch. UsingOTR,wefinetuneboththe1.3B
and2.7BvariantsonWEBLINX.
GPTTurbo Weexplorethetext-onlyTurbovariantsoftheGPTAPIservicesofferedbyOpenAI16.Inthezero-shotsetting,
weexploreboththeGPT-3.5-Turbo-1106(Brownetal.,2020;Pengetal.,2023)andGPT-4-1106-Preview(OpenAI,
2023b). Additionally,wefinetuneGPT-3.5-Turbo-1106for3epochsthroughthefinetuningservices(Pengetal.,2023),
usingthevalidationsplitforevaluation.
GPT-4V Inadditiontothetext-baseversionofGPT-4Turbo,wefurtherexplorethevariantcapableoftakingimageinputs
(OpenAI,2023c). Apartfromaddingfull-resolutionscreenshots,theinputremainsthesameasthenon-visionvariantof
GPT-4. Sincetheinputsizeisalreadylarge,includefew-shotexampleswoulddramaticallyincreasecostandlatency;for
example,a32-shotinputforagiventurnwouldresultinover30Mpixels(assumingHDresolution)and66kinputtokens,
whereaszero-shotresultsin2Mpixelsand2ktokensinthezero-shotsetting.
Fuyu Wefinetunethe8BparameterversionofFuyu(Bavishietal.,2023),abasemodelreleasedbyAdept.ai17 thatis
designedtojointlymodelimagesandtextinaunifieddecodertransformer-basedarchitecture(Vaswanietal.,2017),relying
onlinearprojectionofimagepatchestoavoidusingseparateimageencoders. Themodelwasnotablypretrainedonhigh
resolutionimages,andiscapableofperformingvarioustasksrequiringvisualreasoning,reportingcompetitiveresultson
VQAv2(Goyaletal.,2019),OKVQA(Marinoetal.,2019)andAI2D(Kembhavietal.,2016). Itisalsocapableoflocating
objectsonrealwebsites,makingitaparticularlysuitablemodelforourtask.
B.7 Hyperparameters
AllmodelspresentedinSection5havethefollowinghyperparameters:
‚Ä¢ Scheduler: Linear
‚Ä¢ MaximumOutputTokens: 256
‚Ä¢ Precision: Brainfloat16,alsoknownasbf16(Deanetal.,2012;Google,2023)
‚Ä¢ Optimizer: AdamW(Loshchilov&Hutter,2019),basedontheAdamoptimizer(Kingma&Ba,2015)
‚Ä¢ Parallelization: FullyShardedDataParallel(FSDP;Zhaoetal.2023)onlyformodelswith7B+parameters.
‚Ä¢ OTRStrategicTruncation(seeSectionB.6): Targetof2048tokens. 700tokensperDOMtree,40tokensperutterance
inu ,50tokensperactionina ,and65tokenspercandidatestring,remaining(approximately248tokens)forthe
r r
prompttemplate.
The remaining hyper-parameters can be found in Table 14, or otherwise follow the default parameters specified in the
transformerslibrary(Wolfetal.,2019).
B.8 InputSamples
SamplesformodelsusingoneofthetemplatesinAppendixB.5isprovided: AppendixB.8.1forMindAct,AppendixB.8.3
forchat-basedmodels,AppendixB.8.2forinstruct-basedmodels,andFigure7forPix2Act.
B.8.1 SAMPLEINPUTFORMINDACT
(html(body(div container(div row(div col hdr-r d-flex(div(a id=0 rc-link(span id=1 textEXPLORE)(i id=2 fa ency-down ))(div rc-flyout
(cid:44)‚Üí ))))) (div (div(div homepage(div ency-loaded(div ency-loaded mask-hero )(h4 id=3The World‚Äôs #1 Online Encyclopedia)(div clear-both
(cid:44)‚Üí hero(div(form id=4(div id=5 js-form-item form-item form-item-keys form-no-label (span field-preffix (input submit button
(cid:44)‚Üí js-form-submit form-submit ) ) (input id=6 search q what do you want to searchbox form-search form-input ) (span field-suffix (i
(cid:44)‚Üí fa ency-close ) ))(div form-actions form-wrapper (input id=7 submit search button js-form-submit form-submit ))))(div clear-both
(cid:44)‚Üí hero footer-copy(a id=8Read more) about our content and why so many people love it.))))))(div adthrive-ad(div)(span id=9
(cid:44)‚Üí adthrive-close√ó))))
You will find above the HTML elements available for the current webpage.
16https://platform.openai.com
17https://www.adept.ai/
29WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table14:Thetraininghyperparametersofallmodels.Wegivethenumberofepochs,thebatchsize(batch),thelearningrate(LR),the
numberofgradientaccumulationsteps(Accum.),thenumberofwarmupsteps(Warm.)andifthemodelusesflashattention(FA2;Dao
etal.2022;Dao2023).*WeusethePix2Struct(Leeetal.,2023)backboneforPix2Actexperiments.(cid:134)Weusethechat-hfvariantof
LLaMA-2models
Model Size Epochs Batch LR Accum. Warm. Vision FA2
Sheared-LLaMA 1.3B 3 4 5¬∑10‚àí5 4 0 ‚úó ‚úì
Sheared-LLaMA 2.7B 3 4 5¬∑10‚àí5 4 0 ‚úó ‚úì
Llama-2(chat-hf) 7B 3 16 5¬∑10‚àí5 1 0 ‚úó ‚úì
Llama-2(chat-hf) 13B 3 6 5¬∑10‚àí5 3 0 ‚úó ‚úì
Fuyu 8B 3 4 5¬∑10‚àí5 4 0 ‚úì ‚úó
Pix2Act* 282M 5 4 2¬∑10‚àí5 8 100 ‚úì ‚úó
Pix2Act* 1.3B 5 1 2¬∑10‚àí5 16 100 ‚úì ‚úó
MindAct 250M 5 16 5¬∑10‚àí5 1 0 ‚úó ‚úó
MindAct 780M 5 16 5¬∑10‚àí5 1 0 ‚úó ‚úó
MindAct 3B 5 2 5¬∑10‚àí5 8 0 ‚úó ‚úó
Flan-T5 250M 5 8 5¬∑10‚àí5 2 0 ‚úó ‚úó
Flan-T5 780M 5 8 5¬∑10‚àí5 2 0 ‚úó ‚úó
Flan-T5 3B 5 2 5¬∑10‚àí5 8 0 ‚úó ‚úó
GPT-3.5(Turbo) ‚Äì 3 ‚Äì ‚Äì ‚Äì ‚Äì ‚úó ‚Äì
Figure7:SampleinputforPix2Act,whichcontainsembeddedheadertextabovethescreenshot
You are an AI assistant tasked with helping a user (aka Instructor) by answering with the action needed to perform a task on a webpage.
Here are the instructor's utterances, truncated to first and last 4 instances preceded by the relative timestamp: [00:05] Hello ;
Only the last 5 actions are available.
Here are the top candidates for this turn: (uid = 67e2a5fb-8b1d-41a0) (input id=6 search q what do you want to searchbox
(uid = fedfb512-949e-42b3) (input id=7 submit search button js-form-submit form-submit )
(uid = c7fbc11c-0949-4ab2) (form id=4(div id=5 js-form-item form-item form-item-keys form-no-label (span field-preffix (input
(uid = 6c7fe1f1-f640-4dce) (span id=1 textEXPLORE)
(uid = 0ffc6f0e-808a-4c2a) (span id=9 adthrive-close√ó)
(uid = 8d8afc84-5b97-477a) (div id=5 js-form-item form-item form-item-keys form-no-label (span field-preffix (input submit
(uid = 1ea51e98-3fcd-4e30) (h4 id=3The World‚Äôs #1 Online Encyclopedia)
(uid = 769785af-485e-4cf1) (a id=0 rc-link(span id=1 textEXPLORE)(i id=2 fa ency-down ))
(uid = e7b7879f-45ae-48a5) (i id=2 fa ency-down )
(uid = bf33a062-fb67-44f0) (a id=8Read more) about our content and why so many
Assistant: action(intent="say", speaker="navigator", utterance="Hi") action(intent="say", speaker="instructor", utterance="Open
(cid:44)‚Üí Encyclopedia website.") action(intent="say", speaker="navigator", utterance="Yes, sure") action(intent="load",
(cid:44)‚Üí url="https://www.encyclopedia.com/") action(intent="say", speaker="instructor", utterance="Search for biotechnology")
30WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
User: Please select the best action using the correct format, do not provide any other information or explanation.
Assistant:
B.8.2 SAMPLEINPUTFORINSTRUCTION-BASEDMODELS(FLAN,FUYU)
(html(body(div class="container"(div class="row"(div class="col hd...tems-center"(div class="hdr...container"(a class="rc-link"
(cid:44)‚Üí onclick="if (!...Flyout()" data-webtasks-id="7697...-4cf1"(span class="text" data-webtasks-id="6c7f...-4dce"EXPLORE)(i class="fa
(cid:44)‚Üí ency-down" data-webtasks-id="e7...-48a5"))(div class="rc-flyout"))))) (div (div class="dialog-off...main-canvas"(div
(cid:44)‚Üí class="homepage"(div style="background-image:...png');" class="ency-loaded"(div class="ency-loaded mask-hero")(h4
(cid:44)‚Üí data-webtasks-id="1ea...d-4e30"The World‚Äôs #1 Online Encyclopedia)(div class="clear-both hero"(div class="ency-hero-search"(form
(cid:44)‚Üí action="https://www..../gsearch" method="get" data-webtasks-id="c7f...-4ab2"(div class="js-...o-label"
(cid:44)‚Üí data-webtasks-id="8d8...97-477a" (span class="field-preffix" (input class="button j... form-submit" type="submit" value="" )
(cid:44)‚Üí (input title="" class="searchbox form-search form-input" placeholder="What do you want to learn today?" type="search" name="q"
(cid:44)‚Üí value="" size="15" maxlength="128" data-webtasks-id="67e2...-41a0" spellcheck="false" (span class="field-suffix" (i class="fa
(cid:44)‚Üí ency-close")))(div class="form-actions...-wrapper" (input class="button j... form-submit" type="submit" value="Search"
(cid:44)‚Üí data-webtasks-id="fedfb...-42b3")))(div class="clear-both hero footer-copy"(a href="/about" data-webtasks-id="bf33...44f0"Read
(cid:44)‚Üí more) about our content and why so many people love it.))))))(div class="adth...ive-sticky" style="min-height: 90px;"
(cid:44)‚Üí closable="true"(div style="border: 0pt none;")(span class="adthrive-close" data-webtasks-id="0ff...-4c2a"√ó))))
Above are the pruned HTML contents of the page.You are an AI assistant with a deep understanding of HTML and you must predict actions
(cid:44)‚Üí based on a user request, which will be executed. Use one of the following, replacing [] with an appropriate value:
(cid:44)‚Üí change(value=[str], uid=[str]) ; click(uid=[str]) ; load(url=[str]) ; say(speaker="navigator", utterance=[str]) ; scroll(x=[int],
(cid:44)‚Üí y=[int]) ; submit(uid=[str]) ;text_input(text=[str], uid=[str]) ;
The user's first and last 4 utterances are: [00:05] Hello ;
Viewport size: 746h x 1536w ;
Only the last 5 turns are provided.
Here are the top candidates for this turn: (uid = 67e2a5fb-8b1d-41a0) [[tag]] input [[xpath]] /html/body/...[1]/input [[bbox]] x=419.6
(cid:44)‚Üí y=461.0 width=477.6 height=89.6 [[attributes]] title='' value=... want to learn today?'
(uid = fedfb512-949e-42b3) [[tag]] input [[xpath]] /html/body/...[2]/input [[bbox]] x=915.6 y=461.0 width=185.6 height=89.6
(cid:44)‚Üí [[attributes]] type='submit'...mit form-submit'
(uid = c7fbc11c-0949-4ab2) [[tag]] form [[xpath]] /html/body...div[3]/form [[bbox]] x=419.6 y=461.0 width=680 height=88 [[attributes]]
(cid:44)‚Üí method='get' data....com/gsearch' [[children]] div div
(uid = 6c7fe1f1-f640-4dce) [[tag]] span [[xpath]] /html/body...]/a/span [[text]] EXPLORE [[bbox]] x=1240.5 y=28.6 width=54.1 height=30
(cid:44)‚Üí [[attributes]] class='text' data...menu-menu'
(uid = 0ffc6f0e-808a-4c2a) [[tag]] span [[xpath]] /html/body/div[5]/span [[text]] √ó [[bbox]] x=1485.9 y=665.6 width=23.3 height=21.6
(cid:44)‚Üí [[attributes]] class='ad...a-4c2a'
(uid = 8d8afc84-5b97-477a) [[tag]] div [[xpath]] /html/body/.../div[1] [[text]] [[bbox]] x=419.6 y=461.0 width=476 height=88
(cid:44)‚Üí [[attributes]] data-webtasks-...no-label' [[children]] span input
(uid = 1ea51e98-3fcd-4e30) [[tag]] h4 [[xpath]] /html/body/...1]/h4 [[text]] The World‚Äôs #1 Online Encyclopedia [[bbox]] x=33 y=163
(cid:44)‚Üí width=1453.2 height=43.2 [[attributes]] data-webtasks-...d-4e30'
(uid = 769785af-485e-4cf1) [[tag]] a [[xpath]] /html/body/...[2]/a [[bbox]] x=1240.5 y=28.6 width=74.1 height=30 [[attributes]]
(cid:44)‚Üí id='r... toggleFlyout()' [[children]] span i
(uid = e7b7879f-45ae-48a5) [[tag]] i [[xpath]] /html/body/...]/a/i [[bbox]] x=1294.6 y=33.6 width=20 height=20 [[attributes]]
(cid:44)‚Üí class='fa...e-48a5'
(uid = bf33a062-fb67-44f0) [[tag]] a [[xpath]] /html/body...4]/p/a [[text]] Read more [[bbox]] x=567.0 y=641.0 width=69.3 height=16
(cid:44)‚Üí [[attributes]] href=...67-44f0'
Assistant: say(speaker="navigator", utterance="Hi")
User: say(speaker="instructor", utterance="Open Encyclopedia website.")
Assistant: say(speaker="navigator", utterance="Yes, sure") load(url="https://www.encyclopedia.com/")
User: say(speaker="instructor", utterance="Search for biotechnology") Please select the best action using the correct format, do not
(cid:44)‚Üí provide any other information or explanation.
Assistant:
B.8.3 SAMPLEINPUTFORCHAT-BASEDMODELS(LLAMA,GPT)
SystemPrompt
(html(body(div class="container"(div class="row"(div class="col hdr-r justify-...flex align-items-center"(div
(cid:44)‚Üí class="hdr-categories-container"(a class="rc-link" onclick="if (!window.__cfRLUn... false; toggleFlyout()"
(cid:44)‚Üí data-webtasks-id="76978...85e-4cf1"(span class="text" data-webtasks-id="6c7fe1...640-4dce"EXPLORE)(i class="fa ency-down"
(cid:44)‚Üí data-webtasks-id="e7b787...5ae-48a5"))(div class="rc-flyout"))))) (div (div class="dialog-off-canvas-main-canvas"(div
(cid:44)‚Üí class="homepage"(div style="background-image: url('/sites...01_3.png');" class="ency-loaded"(div class="ency-loaded
(cid:44)‚Üí mask-hero")(h4 data-webtasks-id="1ea51e...fcd-4e30"The World‚Äôs #1 Online Encyclopedia)(div class="clear-both hero"(div
(cid:44)‚Üí class="ency-hero-search"(form action="https://www.encyclopedia.com/gsearch" method="get"
(cid:44)‚Üí data-webtasks-id="c7fbc11c...49-4ab2"(div class="js-form-item form-...-keys form-no-label" data-webtasks-id="8d8afc8...7-477a"
(cid:44)‚Üí (span class="field-preffix" (input class="button js-form-submit form-submit" type="submit" value="" ) (input title=""
(cid:44)‚Üí class="searchbox form-search form-input" placeholder="What do you want to learn today?" type="search" name="q" value="" size="15"
(cid:44)‚Üí maxlength="128" data-webtasks-id="67e2a5...d-41a0" spellcheck="false" (span class="field-suffix" (i class="fa ency-close")))(div
(cid:44)‚Üí class="form-actions js-form-wrapper form-wrapper" (input class="button js-form-submit form-submit" type="submit" value="Search"
(cid:44)‚Üí data-webtasks-id="fedfb512-...9e-42b3")))(div class="clear-both hero footer-copy"(a href="/about"
(cid:44)‚Üí data-webtasks-id="bf33a0...67-44f0"Read more) about our content and why so many people love it.))))))(div class="adthrive-ad
(cid:44)‚Üí adth...cls adthrive-sticky" style="min-height: 90px;" closable="true"(div style="border: 0pt none;")(span class="adthrive-close"
(cid:44)‚Üí data-webtasks-id="0ffc6f0...8a-4c2a"√ó))))
Above are the pruned HTML contents of the page.You are an AI assistant with a deep understanding of HTML and you must predict actions
(cid:44)‚Üí based on a user request, which will be executed. Use one of the following, replacing [] with an appropriate value:
(cid:44)‚Üí change(value=[str], uid=[str]) ; click(uid=[str]) ; load(url=[str]) ; say(speaker="navigator", utterance=[str]) ; scroll(x=[int],
(cid:44)‚Üí y=[int]) ; submit(uid=[str]) ;text_input(text=[str], uid=[str]) ;
The user's first and last 4 utterances are: [00:05] Hello ;
Viewport size: 746h x 1536w ;
Only the last 5 turns are provided.
31WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Here are the top candidates for this turn: (uid = 67e2a5fb-8b1d-41a0) [[tag]] input [[xpath]] /html/body/div[2...form/div[1]/input
(cid:44)‚Üí [[bbox]] x=419.6 y=461.0 width=477.6 height=89.6 [[attributes]] title='' value='' name='...What do you want to learn today?'
(uid = fedfb512-949e-42b3) [[tag]] input [[xpath]] /html/body/div[2...form/div[2]/input [[bbox]] x=915.6 y=461.0 width=185.6
(cid:44)‚Üí height=89.6 [[attributes]] type='submit' value='Search...-form-submit form-submit'
(uid = c7fbc11c-0949-4ab2) [[tag]] form [[xpath]] /html/body/div[2...2]/div[3]/form [[bbox]] x=419.6 y=461.0 width=680 height=88
(cid:44)‚Üí [[attributes]] method='get' data-web...clopedia.com/gsearch' [[children]] div div
(uid = 6c7fe1f1-f640-4dce) [[tag]] span [[xpath]] /html/body/header/div...div[2]/a/span [[text]] EXPLORE [[bbox]] x=1240.5 y=28.6
(cid:44)‚Üí width=54.1 height=30 [[attributes]] class='text' data-webtasks...-main-menu-menu'
(uid = 0ffc6f0e-808a-4c2a) [[tag]] span [[xpath]] /html/body/div[5]/span [[text]] √ó [[bbox]] x=1485.9 y=665.6 width=23.3 height=21.6
(cid:44)‚Üí [[attributes]] class='adthrive-close...8a-4c2a'
(uid = 8d8afc84-5b97-477a) [[tag]] div [[xpath]] /html/body/div[...3]/form/div[1] [[text]] [[bbox]] x=419.6 y=461.0 width=476
(cid:44)‚Üí height=88 [[attributes]] data-webtasks-id='8...keys form-no-label' [[children]] span input
(uid = 1ea51e98-3fcd-4e30) [[tag]] h4 [[xpath]] /html/body/div[...div/div[1]/h4 [[text]] The World‚Äôs #1 Online Encyclopedia [[bbox]]
(cid:44)‚Üí x=33 y=163 width=1453.2 height=43.2 [[attributes]] data-webtasks-id='1...cd-4e30'
(uid = 769785af-485e-4cf1) [[tag]] a [[xpath]] /html/body/header/div...2]/div[2]/a [[bbox]] x=1240.5 y=28.6 width=74.1 height=30
(cid:44)‚Üí [[attributes]] id='rcLink' class='... false; toggleFlyout()' [[children]] span i
(uid = e7b7879f-45ae-48a5) [[tag]] i [[xpath]] /html/body/header/div...div[2]/a/i [[bbox]] x=1294.6 y=33.6 width=20 height=20
(cid:44)‚Üí [[attributes]] class='fa ency-down...5ae-48a5'
(uid = bf33a062-fb67-44f0) [[tag]] a [[xpath]] /html/body/div[2...div[4]/p/a [[text]] Read more [[bbox]] x=567.0 y=641.0 width=69.3
(cid:44)‚Üí height=16 [[attributes]] href='/about' data-...67-44f0'
Chat
say(speaker="navigator", utterance="Hi")
say(speaker="instructor", utterance="Open Encyclopedia website.")
say(speaker="navigator", utterance="Yes, sure") load(url="https://www.encyclopedia.com/")
say(speaker="instructor", utterance="Search for biotechnology") Please select the best action using the correct format, do not provide
(cid:44)‚Üí any other information or explanation.
Figure8:Samplescreenshotwithtargetactionhighlighted.
B.9 OutputSample
In Table 15, we see the resulting output when given either one of the formatted text inputs (Appendix B.8), and using
Figure8formultimodalmodels.
32WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table15:SampleoutputsformodelsevaluatedinSection6.InputsareshowninAppendixB.8.
GroundTruth click(uid="67e2a5fb-8b1d-41a0")
click(x=607, y=512)
Flan-T5-250M click(uid="67e2a5fb-8b1d-41a0")
Flan-T5-780M click(uid="67e2a5fb-8b1d-41a0")
Flan-T5-3B click(uid="67e2a5fb-8b1d-41a0")
Fuyu-8B click(uid="67e2a5fb-8b1d-41a0")
GPT-3.5T text_input(text="biotechnology", uid="67e2a5fb-8b1d-41a0")
GPT-4T text_input(text="biotechnology", uid="67e2a5fb-8b1d-41a0")
GPT-4V text_input(text="biotechnology", uid="67e2a5fb-8b1d-41a0")
Llama-2-7B click(uid="67e2a5fb-8b1d-41a0")
Llama-2-13B click(uid="67e2a5fb-8b1d-41a0")
MindAct-250M action(uid="67e2a5fb-8b1d-41a0", intent="click")
MindAct-780M action(uid="67e2a5fb-8b1d-41a0", intent="click")
MindAct-3B action(uid="67e2a5fb-8b1d-41a0", intent="click")
Pix2Act-282M click(x=1536, y=27)
Pix2Act-1.3B click(x=716, y=508)
ShearedLLaMA-1.3B click(uid="67e2a5fb-8b1d-41a0")
ShearedLLaMA-2.7B click(uid="67e2a5fb-8b1d-41a0")
33WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
C SupplementaryResults
InSection6,weprovideanoverviewofourresultsontheaverageofout-of-domainsplit.Inthissection,weprovidein-depth
analysisofbothin-domainandout-of-domainresults. Westartbylookingattheimpactofourimprovedtextrepresentation
(OTR)comparedtoMindAct(AppendixC.1),beforemovingontoacomparisonofbaselineimage-to-textmodelswith
largermultimodalmodels(AppendixC.2),followedbyanassessmentofvarioustext-onlydecoders(AppendixC.3).
C.1 ComparisonofMind2WebrepresentationwithOTR
MindACtisapriormethodproposedbyDengetal.(2023)thatonlyreceivestextasinput. WeusetheMindActcheckpoints
andusetheMind2Webdatastructure. TounderstandwhathappensforlargerDOMtreesandlongerhistory,wecompare
it against our optimal text representation introduced in Section 5.2. In Table 16, we observed that Flan-T5 with OTR
outperformsMindActinbothoverallperformanceandwhenlookingatindividualgroups. Wefurtherobservethatthegap
betweenthemodelalsoincreasesforlargermodels,whichleadsustobelievethatacarefulstrategywhenconstructing
P(s ,a )iscrucialaswescaletomoreparameters.
t 1:t‚àí1
Table16:ComparingFlan-T5usingOTRwithMindActusingMind2Webformatting.Reportedonvalidwithmetricsfrom¬ß4.
Models OverallScore Element Text
Micro-Avg IM IoU F1
MindAct-T5-250M 17.78 77.05 19.02 9.87
MindAct-T5-780M 21.39 77.58 22.46 15.32
MindAct-T5-3B 27.86 79.91 24.24 24.79
Flan-T5-250M 21.91 79.27 24.10 11.02
Flan-T5-780M 23.94 80.26 24.90 15.99
Flan-T5-3B 31.97 82.00 31.18 27.81
C.2 Comparisonofimage-onlybaselinewithmultimodalmodels
InSection5.2,weintroducePix2Act,whichonlyusesscreenshotsasinput(embeddingv ,u anda asheadertext). We
t r r
alsoconsiderlargermultimodalmodels(Section5.2)thatcantakethecompleteP thesamewayastext-onlymodels. In
Table17, weobservethatthelargervariantofPix2Actoffersmeaningfulimprovementsoverthebasevariant, butthat
Fuyu-8Boutperformsbothmodelsintheelementgroupandachievessimilarperformanceforthetextgroupandintent
match,resultinginabetteroverallperformance. Ontheotherhand,GPT-4V,whichwasneverfinetunedforthetask,is
consistentlyoutperformedbyFuyu-8BandisalsobehindPix2Actineachscenarioexcepttheelementgroup. Thoseresults
highlightstheimportanceoffinetuningthemodelswheneveritispossible,usingmodelswithgreaternumberofparameters,
andincorporatingmorecompletetextualinformation(includingcandidates).
Table17:Comparingimage-onlybaselineswithmultimodalmodels.Reportedonvalidwithmetricsfrom¬ß4.(*)GPT-4Vistheonly
modelnotfinetuned.
Models OverallScore Element Text
Micro-Avg IM IoU F1
Pix2Act-282M 14.39 79.09 6.70 18.11
Pix2Act-1.3B 24.21 83.40 13.38 31.61
Fuyu-8B 31.60 81.36 26.34 30.99
GPT-4V* 14.26 41.00 14.44 6.06
C.3 Assessingimpactofmodelsizefortext-onlydecoders
Inadditiontodifferencesinarchitectures,wealsoseektounderstandtheroleofmodelsize(intermsofparametercount)
onthetraining. InTable18,weonlyexaminethescenarioofdecoder-onlymodels(LLaMAandGPT)thatsolelytakes
text as input. In the zero-shot setting, we observe that the performance of a model increases as models become larger.
However,forfinetunedmodels,theimprovementsarenotasimportant,sincethelargestvariant(13B)ofLLaMA-2only
surpassesthe2.7Bvariantbyasmallmargin. Whencomparingzero-shotwithfinetuning,itisclearthatthelatteryields
34WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
considerableimprovements,withmodelsassmallas2.7Bsurpassingthebestzero-shotmodel(GPT-4T)onscenarios. In
parallel,eventhoughGPT-3.5TsurpassesLLaMA-2-13Binzero-shotperformance,thefinetunedvariantsofGPT-3.5T
(reportedasGPT-3.5F)trailsbehindeventhesmallestLLaMAmodel. Thiscouldpotentiallybeattributedtonon-optimal
hyperparameters,sinceAPIuserscanonlycontrolthebatchsizeandnumberofepochs18.
Table18:Performanceofdecoder-onlytextmodels,bothzero-shot(above)andfinetuned(below).Reportedonvalidwithmetricsfrom
¬ß4.Weusethechat-hfvariantsofLLaMA-2.
Models OverallScore Element Text
Micro-Avg IM IoU F1
Llama-2-13B 6.07 39.55 5.54 1.62
GPT-3.5T 11.48 41.93 11.67 3.16
GPT-4T 13.75 41.64 13.83 6.58
Sheared-LLaMA-2.7B 35.47 86.14 33.80 34.20
Llama-2-13B 38.03 86.49 36.43 36.54
GPT-3.5F 28.98 79.03 27.42 25.99
C.4 Generalizationcapabilitiesofevaluatedmodels
Atthisstage,wehavevalidatedthatstrategicallytruncatingtextandbettercandidaterepresentationviaOTRachievebetter
resultscomparedtoMindActbaselines(AppendixC.1,largermultimodalmodelslikeFuyu-8BandGPT-4Vofferimportant
improvementsoverpriorapproacheslikePix2Act(AppendixC.2),andchoosinglargertext-onlydecodermodels(LLaMA,
GPT-Turbo)willconsistentlyoutperformsmalleronesinthezero-shotsetting,butdoesnotshowalargeimprovementwhen
finetuned(AppendixC.3). Thoseresultsleadtorelevantquestions: dothosemodelstransfertoout-of-domainsplits(unseen
websites,newsubdomains,differentgeographies,andvisionlessinstructors),andcanwedrawthesameconclusionsin
thosecases?
InTable4,weobserve,inthezero-shotsetting,thatthegapbetweenGPT-4TandGPT-4Vbecomesnarrower(likelydue
to the decrease in performance in the element group). In the finetuned setting, we observe a sharp decrease in overall
performance for all models, which highlights the challenge of applying models on new scenarios. However, we can
reassertthatOTR,multimodalityandfinetuningarenecessarytoachievebetteroverallperformance,andthatdecoder-only
models remain the strongest models we evaluated. However, the gap between Sheared-LLaMA-2.7B and LLama-2-
13Bissubstantiallynarrowerthanonthevalidationsplit,indicatingthatSheared-LLaMAismorerobusttochangesto
the environment. Finally, we see that, even on out-of-domain splits, multimodal models remain behind their text-only
counterpart.
C.5 ExtendedQualitativeAssessment
InSection6.2,wehighlightthemaintakeawaysofourqualitativeassessment. Wecanfindbelowthecompleteassessment,
includingsupplementaryscenarios.
Assessingclick InFigure9,weexaminemultiplescenariosinvolvingGPT-4VandcomparethemagainstLLaMA-2-13B.
Inscenario1,wefoundthatGPT-4Vcanmakemistakebyselectingtheincorrectlinkwhengivenmultiplelinksthatcontain
differenttimeframes(forexample,choosinga3:30AMnewsarticleinsteadof4:15AM).Inscenario2,itmaynotbecapable
ofacknowledgingthatitisalreadyinthesecondstepofperformingatask(e.g.,changingthecurrentlocationofthesite),
andmaytrytorepeatthetaskfromstart(e.g.,re-openthedetailswindowwhenitisalreadyopen). Inscenario3,weseem
itcorrectlypredictsanactionthatisintheorycorrect,butthatislessoptimalthanwhatahumanwouldhavechosen;for
example,itmayopentheloginpageofacommonlyusedwebsite,eventhoughchoosingthehomepagemightallowthe
navigatortousetheappfasterifalreadyloggedin. Ineachofthosescenarios,LLaMAiscapableofselectingthecorrect
option. However,weseeinscenario4thatLLaMA-2-13Bcanalsosometimesfailbyattemptingtoclickonelementsthat
donotaffectthestate(e.g.,atext-onlyheading),whereasGPT-4Vcanmakethecorrectdecisioninthesameexample.
Assessing textinput In Figure 10, we observe that GPT-4 will sometimes attempt to perform illogical actions when
performingtaskslikesendinganemail;itmaywritethenameofarecipientwhentheemailhasalreadybeenspecified,
whereasLLaMAwillcorrectlyinputthesubjectspecifiedbytheinstructor(Scenario1). Additionally,GPT-4canmixup
18Alearningratemultiplieralsoexists,butitisunclearwhatthebaserateandoptimizersare
35WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
S1:Onanewswebsite,InstructorwantsNavigatortoopena S2:Instructorrequeststhelocationonafooddeliverywebsite
specifictabonthepage,i.e.,‚ÄùSportsdayon28May2023at tobesettoLasVegas,Nevada.TheDeliveryDetailspageis
4.15AM‚Äù. alreadyopen.
GPT-4V(R)clicksonanincorrect(3:30AM)tab. GPT-4V(R)attemptstoexittheDeliverydetailspageand
reopenit,whichcouldleadtoaloop.
LLaMA(B)clicksonthecorrect4:15AMtab. LLaMA(B)correctlyclicksontheChangebutton.
S3:InstructorwantsNavigatortocomposeanemail.Naviga- S4:InstructorrequestsNavigatortosendthetopquestionsof
torusesBardforthedraft. theweek.
GPT-4V(R)attemptstoclickdirectlyontheloginpage,which GPT-4V(B)selectsthe‚ÄùWeek‚Äùbutton,whichmatchesthe
islessoptimal. referenceaction.
LLaMA(B)opensthehomepage(correspondstoreference). LLaMA(R)clicksonatext-onlyheading(TopQuestions).
Figure9:ComparisonofGPT-4VandLLaMA-2-13B(finetuned)onpredictingclickactions.Incorrectlypredictedactionsareinred(R),
referenceactionsareinblue(B).Weshow4scenarios(S1-S4).
usernameandpasswordformsonloginpagesbytryingtotypeintheemailaddressgivenbytheinstructorintothepassword
field;ontheotherhand,LLaMAcancorrectlyinputthepassword(S2). Moreover,therearescenarioswherebothstruggleto
leveragethecontexttocompletethesecondstepofamulti-steptask. Forexample,whentheinstructorrequestapassageto
betranslatedintoacertainlanguage(S3),andthefirststep(typinginthepassagetotranslate)hasalreadybeencompleted,
bothmodelswillignorethesecondstep(changingthelanguagetothetarget). Finally,bothmodelsmaystruggletoleverage
informationthatwasgivenmanystepsbefore. Forinstance,iftheinstructorwantstowriteapost,theymaygiventhetitle
earlierinthedemonstration, thenprovidethetextfortheintroductionlateron(S4); inthosecases, bothmodelsfailto
includethetitle.
Assessingsay OnemajordifferencebetweenGPT-4VandLLaMA-2-13Bisthattheformerwillhaveacompletelydifferent
writingstyle,whereasthelattercanlearnthestyleofthenavigatorduringfinetuning. Forexample,thenavigatorsmay
employacknowledgingtermslike‚ÄúAlright‚Äùand‚ÄúSure‚ÄùthatcanbelearnedbyLLaMA-2-13B,whereasGPT-4Vtendstouse
‚ÄúUnderstood‚Äùand‚ÄúAcknowledged‚Äù. Beyondthosesuperficialdifferences,wenoticesomepatternsoffailureinTable19.
First,GPT-4Vmightcomeupwithunhelpfulreplies,suchasincorrectlysharingalinktothecurrentpagewhenrequested
tosharealinktoadifferentpage. Inthesamescenario,itmightsimplyrefusetoassisttheinstructor,evenwhentheaction
isachievable. Finally,GPT-4Vmightgenerateanutterancethatsemanticallydiffersfromthereferenceutterance,butwould
bepragmaticallycorrect. Weshowoneexamplewhere,givenarequesttowriteanemailthatincludesadiscount,thehuman
navigatorwouldaskwhoshouldbetherecipient,whereasGPT-4Vmightaskaboutthedetailsofthediscount;clearly,both
arevalidfollow-upquestions,butitischallengingtoevaluatewithexistingmethods. Inalltheaforementionedcases(except
forthelastone),LLaMA-2-13Bwillprovideashortbutcorrectresponse. Althoughitmayseemlessverbose,wefound
36WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
S1:Composea‚ÄúInvitationtoCollaboration‚Äùemail. S2:OpenGoogletranslateandsigninusingthefollowingcre-
dentials:[email][password]
Reference(B):InvitationtoCollaboration Reference(B):[password]
GPT-4V(R):LeonTales GPT-4V(R):[email]
LLaMA(B):InvitationtoCollaboration LLaMA(B):[password]
S3:TranslateapassageintofrenchusingReverso.Thelanguage S4:CreateapostaboutEffectiveCommunicationwithatitleand
selectionwindowiscurrentlyopen. anintroduction
Reference(B):fr Reference(B):5StrategiesforBuildingStrongerRelationships.
Introduction:Effective...
GPT-4V(R):Thesunbeganitsslow... GPT-4V(R):Introduction:Effective...
LLaMA(R):click(incorrectaction) LLaMA(B):Introduction:Effective...
Figure10:ComparisonofGPT-4VandLLaMA-2-13B(finetuned)onpredictingtextinputactions.ActionspredictedbyGPT-4Varein
red(R),referencesareinblue(B),LLaMA‚Äôsactionisindicatedinparenthesis.Weshow4scenarios(S1-S4).
Table19:ComparisonofLLaMA-2-13B(finetuned)andGPT-4Vonpredictingsayactions.
Request Like&Bookmarkthislisting.
Reference Alright
LLaMA Alright
GPT-4V Understood.Ifyouneedfurtherassistance,justletmeknow.
Request Pleasesharethelink.
Reference Alright.
LLaMA Okay.
GPT-4V Hereisthelinktothediscussion:[incorrectlink]
Request Pleasesharethelinkwithme.
Reference Sure.
LLaMA Sure.
GPT-4V I‚Äômsorry,butIcan‚Äôtassistwiththat.
Request Couldyouhelpmecomposeanemailforadiscountoffer?
Reference Alright.Whoshouldreceivethisemail?
LLaMA Okay.
GPT-4V Wouldyouliketomentionanyspecifictermsorexpirationdatefortheoffer?
thattheyareinrealityalmostasverboseasGPT-4V;themodelsrespectivelyhave,onaverage,58.29(n=1194)and60.41
characters(n=220)whenpredictingasayintentonthevalidationandin-domaintestsets.
37WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
D AdditionalResultTables
TocomplementSection6,weincludethescoresforeachsplit: in-domain(¬ß20),out-of-domainmean(¬ß21),TEST (¬ß22),
CAT
TEST (¬ß23),TEST (¬ß24),andTEST (¬ß25). Wereporttheintentmatch(IM)toidentifywhichmodelsfaildueto
GEO VIS WEB
theirinabilitytopredictthecorrectintent. WealsoincludethegroupedresultsintablesTables26to28.
Table20:Fullin-domaintestresults.Weabbreviatesubmittosbmtandtextinputtoinput.Thefirstsectioncontainszero-shotresults
andthesecondcontainsfinetunedresults.
click sbmt input say input load click load say sbmt input
IoU IoU IoU chrF chrF F1 IM IM IM IM IM
Llama-2-7B 6.19 5.83 4.97 4.33 4.57 29.47 43.23 36.67 32.17 6.90 10.50
Llama-2-13B 9.42 0.00 4.97 1.25 4.82 20.57 75.65 23.33 14.93 0.00 8.84
GPT-3.5T 16.90 9.62 21.68 1.78 16.81 18.90 73.27 23.33 8.79 13.79 40.33
GPT-4T 15.92 3.45 41.33 4.53 37.50 18.90 59.61 30.00 18.24 3.45 75.14
GPT-4V 17.36 6.90 46.64 4.20 35.05 15.57 63.03 16.67 14.76 6.90 71.27
MindAct-250M 25.47 0.00 0.00 14.54 0.00 0.00 92.15 0.00 100.00 0.00 0.00
MindAct-780M 24.37 0.93 19.34 20.26 12.39 10.00 90.33 10.00 100.00 3.45 22.10
MindAct-3B 24.60 24.14 30.44 35.19 21.80 16.67 89.65 20.00 100.00 27.59 49.72
Flan-T5-250M 33.49 0.00 0.00 15.25 0.00 0.00 100.00 0.00 100.00 0.00 0.00
Flan-T5-780M 32.66 0.00 15.52 22.61 12.16 0.00 98.63 0.00 100.00 0.00 23.20
Flan-T5-3B 31.22 48.38 42.00 37.46 34.34 24.47 92.26 30.00 100.00 51.72 56.35
Pix2Act-282M 6.85 0.00 0.00 27.00 0.00 13.33 99.89 16.67 100.00 0.00 0.00
Pix2Act-1.3B 17.94 0.00 0.00 43.78 21.75 42.10 95.56 46.67 100.00 13.79 27.07
Fuyu-8B 26.14 62.21 37.93 41.83 30.18 66.10 93.97 66.67 94.36 75.86 53.04
S-LLaMA-1.3B 32.51 57.59 49.90 42.04 36.61 52.23 95.90 63.33 100.00 75.86 67.40
S-LLaMA-2.7B 34.75 75.86 57.25 45.32 39.30 69.10 95.79 73.33 99.67 75.86 67.40
Llama-2-7B 33.71 82.76 62.98 45.21 43.94 73.43 92.38 76.67 99.83 86.21 69.61
Llama-2-13B 32.25 75.86 64.64 43.53 45.77 77.43 90.44 80.00 100.00 75.86 72.93
GPT-3.5F 26.78 72.41 61.91 36.58 42.40 45.77 84.76 50.00 97.01 72.41 70.17
Table21: Out-of-domaintestresults(average). Weabbreviatesubmittosbmtandtextinputtoinput. Thefirstsectioncontains
zero-shotresultsandthesecondcontainsfinetunedresults.
click sbmt input say input load click load say sbmt input
IoU IoU IoU chrF chrF F1 IM IM IM IM IM
Llama-2-7B 5.11 3.58 2.40 4.25 1.60 18.73 43.08 22.72 34.86 7.47 6.40
Llama-2-13B 9.00 0.40 2.20 1.42 1.81 14.96 75.87 17.53 15.24 1.31 5.33
GPT-3.5T 14.15 4.14 19.95 1.50 15.58 20.78 73.71 24.46 9.04 7.17 33.46
GPT-4T 13.63 2.55 43.11 4.40 34.59 22.83 60.03 29.32 17.46 4.61 68.61
GPT-4V 14.33 3.19 43.72 3.35 33.47 18.21 64.18 21.04 13.55 5.12 65.69
MindAct-250M 18.59 0.00 0.40 14.33 0.18 0.00 89.44 0.00 99.98 0.00 0.52
MindAct-780M 17.08 0.19 20.80 21.25 13.26 8.30 88.16 8.30 100.00 0.69 24.72
MindAct-3B 18.55 13.83 32.73 35.35 19.68 13.85 92.04 19.41 99.97 17.66 39.92
Flan-T5-250M 23.44 0.00 0.00 15.50 0.03 0.00 99.82 0.00 100.00 0.00 0.17
Flan-T5-780M 22.98 0.00 8.13 22.84 5.48 0.00 98.81 0.00 100.00 0.20 12.09
Flan-T5-3B 22.16 31.92 44.57 36.82 31.27 16.22 92.11 22.81 99.97 36.52 52.64
Pix2Act-282M 8.33 0.00 0.00 26.76 1.19 12.91 99.11 18.46 100.00 0.00 1.33
Pix2Act-1.3B 12.82 0.00 0.00 37.78 20.71 21.22 95.52 30.74 100.00 6.10 29.10
Fuyu-8B 18.46 32.69 30.76 33.97 23.03 25.63 92.63 40.93 95.84 43.83 40.92
S-LLaMA-1.3B 23.17 31.02 43.00 37.12 27.87 27.04 94.73 44.12 99.88 41.96 53.35
S-LLaMA-2.7B 24.16 40.86 53.42 38.41 33.54 30.22 94.78 42.85 99.80 44.69 62.41
Llama-2-7B 22.87 50.70 56.64 37.57 36.99 38.46 89.54 55.55 99.79 57.39 65.91
Llama-2-13B 23.13 53.72 56.84 37.66 37.06 42.32 88.17 57.33 99.94 58.28 64.23
GPT-3.5F 18.71 49.39 51.96 31.71 35.60 31.86 83.93 41.50 93.85 51.51 62.63
38WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table22:FullTESTCATsplit(test)results.Weabbreviatesubmittosbmtandtextinputtoinput.Thefirstsectioncontainszero-shot
resultsandthesecondcontainsfinetunedresults.
click sbmt input say input load click load say sbmt input
IoU IoU IoU chrF chrF F1 IM IM IM IM IM
Llama-2-7B 5.45 7.58 0.57 4.06 0.57 17.15 47.89 22.94 39.02 15.15 2.28
Llama-2-13B 10.55 0.00 0.85 1.48 1.01 18.97 77.63 22.02 17.62 3.03 1.71
GPT-3.5T 11.92 0.03 25.20 1.43 15.67 21.57 75.24 24.77 10.14 3.03 33.90
GPT-4T 11.23 0.00 48.03 3.08 35.26 15.61 61.01 23.85 14.97 0.00 67.52
GPT-4V 11.42 1.52 46.01 2.61 33.18 14.07 65.42 17.43 12.87 1.52 65.53
MindAct-250M 15.17 0.00 0.28 12.72 0.55 0.00 83.57 0.00 100.00 0.00 0.57
MindAct-780M 13.62 0.00 27.16 19.78 19.03 9.17 78.36 9.17 100.00 0.00 32.76
MindAct-3B 16.62 27.27 37.34 36.57 24.98 11.17 93.53 15.60 99.93 31.82 40.17
Flan-T5-250M 18.37 0.00 0.00 14.82 0.16 0.00 99.39 0.00 100.00 0.00 0.85
Flan-T5-780M 17.90 0.00 6.01 20.71 3.01 0.00 99.13 0.00 100.00 0.00 7.98
Flan-T5-3B 18.35 34.85 46.44 38.70 31.32 13.46 91.52 19.27 99.93 39.39 51.00
Pix2Act-282M 9.33 0.00 0.00 28.00 3.16 15.52 98.36 19.27 100.00 0.00 3.70
Pix2Act-1.3B 11.80 0.00 0.00 37.21 21.83 15.32 97.60 20.18 100.00 10.61 30.48
Fuyu-8B 15.27 42.52 28.50 34.15 22.85 14.80 94.90 35.78 96.50 48.48 32.48
S-LLaMA-1.3B 18.44 34.85 41.57 38.23 30.14 19.95 95.97 38.53 99.86 43.94 45.87
S-LLaMA-2.7B 20.45 39.48 51.44 37.96 32.84 19.14 95.70 33.03 99.86 42.42 56.41
Llama-2-7B 18.58 42.44 57.08 37.76 36.61 27.01 90.26 46.79 100.00 53.03 61.54
Llama-2-13B 18.12 51.53 57.11 37.00 35.05 31.71 84.98 47.71 100.00 57.58 61.25
GPT-3.5F 15.97 43.94 47.21 29.79 30.27 21.26 85.89 32.11 91.96 45.45 55.27
Table23:FullTESTGEOsplit(test)results.Weabbreviatesubmittosbmtandtextinputtoinput.Thefirstsectioncontainszero-shot
resultsandthesecondcontainsfinetunedresults.
click sbmt input say input load click load say sbmt input
IoU IoU IoU chrF chrF F1 IM IM IM IM IM
Llama-2-7B 4.21 4.00 2.54 4.45 1.58 14.04 43.35 17.11 34.58 7.00 11.61
Llama-2-13B 7.21 2.00 2.27 1.25 1.50 10.62 77.83 13.16 12.93 2.00 8.75
GPT-3.5T 14.58 4.00 14.98 1.90 15.22 19.97 73.55 25.00 10.24 5.00 33.75
GPT-4T 13.20 4.00 36.16 5.78 26.16 25.32 57.30 30.26 21.43 6.00 69.11
GPT-4V 14.56 5.00 36.09 4.07 26.50 17.86 62.16 21.05 16.00 7.00 65.00
MindAct-250M 16.58 0.00 0.54 18.08 0.01 0.00 86.32 0.00 100.00 0.00 0.71
MindAct-780M 14.74 0.00 19.29 30.93 10.39 7.89 90.73 7.89 100.00 0.00 23.04
MindAct-3B 15.68 7.00 30.40 41.64 17.88 14.05 91.04 23.68 99.94 8.00 34.64
Flan-T5-250M 20.41 0.00 0.00 20.10 0.00 0.00 99.86 0.00 100.00 0.00 0.00
Flan-T5-780M 19.77 0.00 2.37 32.25 1.67 0.00 98.91 0.00 100.00 1.00 4.29
Flan-T5-3B 17.92 25.77 41.82 42.03 27.16 13.17 90.32 25.00 99.94 33.00 49.29
Pix2Act-282M 9.05 0.00 0.00 31.90 0.18 14.49 99.93 21.05 100.00 0.00 0.36
Pix2Act-1.3B 8.80 0.00 0.00 42.42 20.91 13.39 92.82 22.37 100.00 0.00 29.82
Fuyu-8B 14.92 22.46 30.36 35.50 18.87 9.87 86.83 27.63 97.82 36.00 45.36
S-LLaMA-1.3B 18.79 26.29 36.46 41.14 22.89 12.50 90.56 32.89 99.78 44.00 48.93
S-LLaMA-2.7B 18.85 32.00 54.14 41.75 31.52 13.71 91.11 30.26 99.72 32.00 66.25
Llama-2-7B 17.73 51.00 52.21 40.42 32.23 21.91 85.63 43.42 99.78 53.00 64.64
Llama-2-13B 19.68 56.00 52.98 41.87 33.52 29.72 86.45 50.00 100.00 58.00 61.07
GPT-3.5F 14.90 45.00 49.71 35.34 35.53 21.14 81.05 34.21 94.57 45.00 59.64
39WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table24:FullTESTVISsplit(test)results.Weabbreviatesubmittosbmtandtextinputtoinput.Thefirstsectioncontainszero-shot
resultsandthesecondcontainsfinetunedresults.
click sbmt input say input load click load say sbmt input
IoU IoU IoU chrF chrF F1 IM IM IM IM IM
Llama-2-7B 4.35 0.01 1.16 4.15 0.87 10.61 38.04 12.86 33.41 3.05 3.85
Llama-2-13B 8.82 0.00 1.15 1.75 0.97 8.19 74.76 11.43 14.70 1.53 4.33
GPT-3.5T 14.93 5.34 16.44 0.53 14.97 10.48 74.03 15.00 4.60 6.11 27.77
GPT-4T 15.04 2.31 44.22 2.96 37.58 16.15 60.67 20.71 13.84 3.05 67.42
GPT-4V 15.26 0.82 44.73 1.66 36.59 13.10 65.03 17.14 8.90 2.29 62.76
MindAct-250M 18.94 0.00 0.16 11.65 0.08 0.00 92.28 0.00 100.00 0.00 0.32
MindAct-780M 17.57 0.00 16.33 15.31 9.80 4.29 89.95 4.29 100.00 0.00 22.47
MindAct-3B 19.36 10.74 29.83 24.41 13.78 9.64 93.01 15.00 100.00 12.98 36.44
Flan-T5-250M 23.12 0.00 0.00 11.70 0.00 0.00 99.89 0.00 100.00 0.00 0.00
Flan-T5-780M 22.77 0.00 6.18 17.04 4.19 0.00 98.84 0.00 100.00 0.00 11.08
Flan-T5-3B 22.86 30.76 42.78 26.32 29.75 11.43 93.92 15.71 99.96 37.40 51.52
Pix2Act-282M 6.86 0.00 0.00 16.60 0.32 6.43 99.68 10.00 100.00 0.00 0.32
Pix2Act-1.3B 12.31 0.00 0.00 25.93 15.44 16.09 96.29 32.86 100.00 6.11 25.52
Fuyu-8B 17.56 22.78 27.10 23.43 18.64 20.12 93.20 37.86 93.64 35.11 36.12
S-LLaMA-1.3B 23.65 24.79 40.19 26.03 20.76 23.71 96.18 47.86 99.85 32.82 51.52
S-LLaMA-2.7B 24.14 35.88 52.27 26.26 30.36 21.55 95.62 37.14 99.81 38.93 59.87
Llama-2-7B 23.23 40.46 58.71 26.72 36.19 34.74 90.65 56.43 99.51 47.33 68.86
Llama-2-13B 23.03 40.46 57.31 27.87 35.02 33.63 88.98 50.71 99.89 47.33 64.04
GPT-3.5F 17.97 43.51 50.27 22.99 32.31 29.91 84.09 39.29 91.32 47.33 59.39
Table25:FullTESTWEBsplit(test)results.Weabbreviatesubmittosbmtandtextinputtoinput.Thefirstsectioncontainszero-shot
resultsandthesecondcontainsfinetunedresults.
click sbmt input say input load click load say sbmt input
IoU IoU IoU chrF chrF F1 IM IM IM IM IM
Llama-2-7B 5.33 0.47 2.78 4.27 0.41 22.38 42.92 24.05 35.12 5.26 3.79
Llama-2-13B 8.98 0.00 1.77 1.36 0.76 16.47 73.48 17.72 16.04 0.00 3.03
GPT-3.5T 12.41 1.71 21.46 1.88 15.21 33.00 72.46 34.18 11.43 7.89 31.57
GPT-4T 12.75 2.97 45.83 5.64 36.43 38.15 61.56 41.77 18.83 10.53 63.89
GPT-4V 13.03 1.71 45.12 4.19 36.06 30.47 65.26 32.91 15.21 7.89 63.89
MindAct-250M 16.79 0.00 1.01 14.68 0.26 0.00 92.90 0.00 99.92 0.00 1.01
MindAct-780M 15.09 0.00 21.86 19.99 14.69 10.13 91.44 10.13 100.00 0.00 23.23
MindAct-3B 16.50 0.00 35.63 38.93 19.97 17.72 92.99 22.78 100.00 7.89 38.64
Flan-T5-250M 21.79 0.00 0.00 15.64 0.00 0.00 99.95 0.00 100.00 0.00 0.00
Flan-T5-780M 21.78 0.00 10.55 21.60 6.36 0.00 98.54 0.00 100.00 0.00 13.89
Flan-T5-3B 20.48 19.84 49.79 39.59 33.80 18.57 92.51 24.05 100.00 21.05 55.05
Pix2Act-282M 9.57 0.00 0.00 30.30 2.27 14.77 97.71 25.32 100.00 0.00 2.27
Pix2Act-1.3B 13.24 0.00 0.00 39.57 23.63 19.20 95.33 31.65 100.00 0.00 32.58
Fuyu-8B 18.44 13.50 29.89 34.95 24.58 17.24 94.26 36.71 96.88 23.68 37.63
S-LLaMA-1.3B 22.46 11.58 46.89 38.14 28.92 26.78 95.04 37.97 99.92 13.16 53.03
S-LLaMA-2.7B 22.61 21.05 51.99 40.77 33.68 27.62 95.67 40.51 99.92 34.21 62.12
Llama-2-7B 21.11 36.84 52.23 37.74 35.99 35.20 88.76 54.43 99.84 47.37 64.90
Llama-2-13B 22.58 44.74 52.14 38.03 35.93 39.13 89.98 58.23 99.84 52.63 61.87
GPT-3.5F 17.91 42.11 50.68 33.88 37.47 41.20 83.89 51.90 94.41 47.37 68.69
40WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table26:ElementGroup(EG),TextGroup(TG)andoverallresultsforTESTIID(left)andTESTOOD(right)splits.Thetopsectioncontains
zero-shotresultsandthebottomcontainsfinetunedresults.
Overall Overall EG TG Overall Overall EG TG
MicroAvg IM IoU F1 MicroAvg IM IoU F1
Llama-2-7B 5.32 33.80 4.01 3.06 4.33 33.92 3.17 2.33
Llama-2-13B 5.61 42.85 5.29 1.97 5.28 43.51 4.93 1.44
GPT-3.5T 10.35 42.42 10.68 3.98 8.89 42.70 9.05 3.56
GPT-4T 12.24 42.69 12.55 7.85 11.05 41.87 11.21 6.97
GPT-4V 12.99 42.47 13.68 7.28 10.98 42.38 11.49 6.43
MindAct-250M 16.88 76.54 18.01 8.46 13.48 74.71 13.24 7.83
MindAct-780M 19.61 78.06 20.12 14.04 16.03 76.31 14.75 13.68
MindAct-3B 25.71 80.99 22.50 24.50 21.90 80.11 17.70 23.43
Flan-T5-250M 20.93 80.28 23.68 9.51 16.18 79.81 16.62 9.27
Flan-T5-780M 23.71 81.91 25.35 16.17 18.56 80.40 17.36 14.47
Flan-T5-3B 31.12 83.48 29.56 29.06 25.25 81.61 22.18 26.41
Pix2Act-282M 12.30 80.50 4.86 17.29 12.47 79.87 5.93 16.58
Pix2Act-1.3B 23.91 83.42 13.15 32.59 18.44 82.12 9.36 26.69
Fuyu-8B 30.92 84.51 25.73 33.66 22.29 80.96 17.85 24.57
S-LLaMA-1.3B 33.99 87.81 32.41 34.68 25.94 84.22 23.11 27.62
S-LLaMA-2.7B 37.43 87.70 35.54 37.66 27.61 84.74 25.33 29.27
Llama-2-7B 38.12 88.08 36.71 38.58 27.51 83.73 25.46 28.91
Llama-2-13B 37.09 87.70 35.92 37.43 27.86 83.07 25.79 28.77
GPT-3.5F 30.89 82.34 30.22 29.62 23.35 78.52 21.19 23.84
Table27:ElementGroup(EG),TextGroup(TG)andoverallresultsforTESTCAT(left)andTESTGEO(right)splits.Thetopsectioncontains
zero-shotresultsandthebottomcontainsfinetunedresults.
Overall Overall EG TG Overall Overall EG TG
MicroAvg IM IoU F1 MicroAvg IM IoU F1
Llama-2-7B 4.57 38.32 3.46 2.18 3.61 33.48 2.60 2.11
Llama-2-13B 6.50 47.52 6.03 1.59 4.03 43.04 3.87 1.09
GPT-3.5T 8.23 45.91 8.42 3.35 8.86 42.09 8.78 3.66
GPT-4T 9.48 42.14 9.90 5.63 10.61 40.86 10.53 6.38
GPT-4V 9.26 43.66 9.80 5.30 10.74 41.33 11.05 5.86
MindAct-250M 11.69 72.93 11.27 6.61 13.15 70.25 11.20 8.93
MindAct-780M 14.36 72.83 12.85 12.37 16.99 74.48 12.39 17.68
MindAct-3B 21.60 81.70 16.59 25.01 21.42 76.00 14.65 24.75
Flan-T5-250M 13.96 81.26 13.61 8.98 15.56 76.63 13.70 11.15
Flan-T5-780M 15.61 81.64 13.86 12.93 18.92 76.58 13.58 18.02
Flan-T5-3B 23.67 81.72 18.99 26.85 23.52 77.46 18.09 26.52
Pix2Act-282M 13.31 81.34 6.95 17.95 13.77 76.96 6.10 18.20
Pix2Act-1.3B 17.44 84.03 8.93 26.64 16.96 77.84 6.07 26.72
Fuyu-8B 20.42 82.29 15.03 24.47 19.53 75.87 14.58 21.45
S-LLaMA-1.3B 23.76 84.72 18.82 28.39 23.48 78.64 18.19 25.82
S-LLaMA-2.7B 25.06 85.08 21.38 28.39 24.62 80.04 20.60 27.50
Llama-2-7B 24.57 83.65 20.86 27.96 24.38 78.78 20.54 26.50
Llama-2-13B 24.27 81.00 20.72 26.12 25.93 78.62 21.97 27.67
GPT-3.5F 20.21 78.07 17.31 21.16 21.94 74.69 17.97 23.91
41WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Table28:ElementGroup(EG),TextGroup(TG)andoverallresultsforTESTVIS(left)andTESTWEB(right)splits.Thetopsectioncontains
zero-shotresultsandbottomcontainsfinetunedresults.
Overall Overall EG TG Overall Overall EG TG
MicroAvg IM IoU F1 MicroAvg IM IoU F1
Llama-2-7B 3.77 30.99 2.50 2.10 4.35 33.03 3.26 2.19
Llama-2-13B 5.06 42.05 4.60 1.34 5.21 42.11 4.86 1.23
GPT-3.5T 8.58 40.14 8.84 2.63 8.42 42.95 8.51 4.16
GPT-4T 11.14 40.38 11.65 6.36 11.77 43.28 11.43 8.62
GPT-4V 10.73 40.45 11.59 5.72 11.20 44.00 11.35 7.96
MindAct-250M 13.16 79.07 13.97 7.26 12.54 74.76 11.75 7.89
MindAct-780M 14.46 79.81 14.97 10.87 14.74 76.36 13.40 13.42
MindAct-3B 19.11 82.98 18.46 17.81 21.64 78.90 16.28 25.07
Flan-T5-250M 15.18 82.71 17.02 7.80 15.25 78.15 15.09 8.93
Flan-T5-780M 17.09 83.09 17.53 11.90 17.48 78.78 16.49 13.33
Flan-T5-3B 22.91 84.92 23.01 21.73 25.06 80.47 21.25 27.91
Pix2Act-282M 9.16 82.81 5.06 11.39 13.79 77.75 6.67 18.07
Pix2Act-1.3B 15.33 84.63 9.27 20.18 18.57 80.69 9.39 27.31
Fuyu-8B 18.63 82.29 16.83 18.73 21.97 79.85 17.10 24.57
S-LLaMA-1.3B 22.80 86.83 23.49 21.32 25.68 83.10 22.66 27.86
S-LLaMA-2.7B 24.12 87.29 25.81 22.79 26.82 83.60 23.31 30.01
Llama-2-7B 24.70 86.56 26.36 23.78 25.80 81.57 22.81 27.74
Llama-2-13B 25.00 85.31 26.09 23.89 27.00 82.72 24.24 28.72
GPT-3.5F 20.46 79.37 20.49 19.36 23.24 78.13 19.95 25.13
42WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
E InstructionsfortheAnnotators
ProjectInformation
Wearecollectingdataforevaluatingautomatedwebnavigationsystems. Thedataconsistsofdemonstrationsofinteractions
betweentheuserandthenavigator.
Ineachdemonstration,theuserandthesystemcooperatetoachievetasksinawebbrowser. Theusercontrolsthesystemvia
naturallanguageinstructions.
HowTo
Ingredients
‚Ä¢ twopeople:
‚Äì Instructor:creative,givinginstructions
‚Äì Navigator:systematic,followinginstructions
‚Ä¢ GoogleChrome
‚Ä¢ Zoom
‚Ä¢ internetconnection
Preparation
Youneedtodothisprocessjustonce:
1. DownloadtheChromeextensionZIPfileandunpacktheextensionfoldertoyourlocalfilesystem.
2. IfyouareusingChromeasyourprimarybrowser,createanewprofilefortheexperiments.
3. InstalltheChromeextensionintherepository:
‚Ä¢ OpenanewGoogleChromewindow.
‚Ä¢ Gotochrome://extensions/
‚Ä¢ Atthetopright,turnonDevelopermode.
‚Ä¢ ClickLoadunpacked.
‚Ä¢ Findandselecttheextensionfolderyouhaveunpackedbefore(makesureyouareinsidethefolder).
‚Ä¢ Clickonthe‚Äúpuzzle‚ÄùiconinthetaskbarwithChromeextensionsandpinthisextension.
4. SetupZoom:
‚Ä¢ OpenZoomandlogin.
‚Ä¢ Gotohttps://zoom.us/profile/setting
‚Ä¢ OntheMeetingtab,turnonAutosavingchats(learnmorehere).
‚Ä¢ OntheRecordingtab:
i. enableLocalRecording
ii. enable‚ÄúHostscangivemeetingparticipantspermissiontorecordlocally‚Äù.
iii. enableautomaticrecordingonalocalcomputer
‚Ä¢ SetupyourZoomnametoInstructororNavigatoraccordingtoyourrole.
Updatingtheextension
Checkregularlyifyouareusinganup-to-dateversionoftheextesion:
‚Ä¢ Thecurrentversioncanbefoundatthetopofthisdocument.
‚Ä¢ Yourversionisatchrome://extensions/nexttotheextensionname.
Ifthereisaneverversionoftheextension,removetheextensionandrepeatpoints1)and3)inthePreparationsection.
43WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Demonstrations
1. NavigatorcallsInstructorviaZoom(Participants‚ÜíInvite)
‚Ä¢ Ensurethatbothhavevideoandmicrophonearedisabled.
2. Afterthecallisaccepted:
‚Ä¢ InstructoropensaZoomchatwindow,
‚Ä¢ Navigator:
‚Äì opensaZoomchatwindow,
‚Äì opensaChromewindow,
‚Äì sharesthescreenwiththeirChromewindow(only),
‚Äì startsrecordingaZoomcallvideo(ignorethewarningaboutaudio).
3. NavigatorclicksontheextensionbuttoninthenavigationbarandselectsNewrecording.
‚Ä¢ AnewtabwillopenwithanoverlayStartingrecordingfor1second(makesurethatitisvisibleontheZoomrecording),
followedbyapromptforwaitingforinstructions.
‚Ä¢ Usetheopenedtab,donotopenanynewtab!
4. InstructorgivesNavigatorinstructionsthroughthechatinterfaceforaccomplishingatask(seeTasksfordetails).
‚Ä¢ InstructorhasnootherwayofcommunicatingwithNavigatorthanthroughthechatinterface.
‚Ä¢ Instructorcangiveintermediateinstructionsoranswersystemquestions.
5. NavigatorperformsactionsinthewebbrowseraccordingtoInstructor‚Äôsinstructions.
‚Ä¢ Navigatorshouldusethechatinterfacetoasktheuserforanymissingdetailsandtoprovideanswersifnecessary.
6. Afterthetaskisfinished,Navigator:
‚Ä¢ clicksontheextensionbutton,selectsSaverecordingandwaituntiltherecordinggetssavedtotheircomputer,
‚Ä¢ stopsthevideorecordingandscreensharing,
‚Ä¢ endsthecall,
‚Ä¢ submitstherecording(seeRecordingfordetails).
Recording
Therecordingissubmittedthroughthewebinterface.
Therecordingconsistsof:
‚Ä¢ a‚Äú<recording id>.zip‚Äùfile,whichisaZIParchivewith:
‚Äì metadata,
‚Äì events,
‚Äì screenshots,
‚Äì HTMLsnapshots,
‚Ä¢ Zoomchathistory‚Äúmeeting saved chat.txt‚Äù,
‚Ä¢ Zoominvitelink
TheZoomrecordingfolderdependsonyourplatform.Thedefaultdirectoriesare:
‚Ä¢ Windows:C:\Users\[Username]\Documents\Zoom
‚Ä¢ Linux:/home/[Username]/Documents/Zoom
‚Ä¢ Mac:/Users/[Username]/Documents/Zoom
Actions
Navigatorcanperformthefollowingactionsinthebrowser:
‚Ä¢ gotoaURLthroughthenavigationbar,
‚Ä¢ clickonanelement,
‚Ä¢ inputtextintoaninputfield,
‚Ä¢ scrollupanddownthepage,
44WEBLINX:Real-WorldWebsiteNavigationwithMulti-TurnDialogue
Theactionswhichshouldnotbeperformed:
‚Ä¢ openinganewtab(itisokifthepageopensatabbyitself),
‚Ä¢ horizontalscrolling,
‚Ä¢ pagesearch(Ctrl+F),
‚Ä¢ keyboardshortcuts,
‚Ä¢ drag&drop(e.g.GoogleMaps)
Tasks
Instructorcangivethesystemanytaskswhichanautomatedwebassistantshouldbeabletohandle.Useyourimagination!
Thetaskscanbeunspecifiedatfirst.Itisthejobofthesystemtoaskforintermediatedetailsthroughoutthetasksdemonstration.
Stopthedemonstrationbeforedoinganyrealactionintheworld:bookingatable,buyingaticket,etc.
Websites
Foryourinspiration,hereisaspreadsheetwiththelistofwebsitesandthetaskcategoriesyoucanusethemfor.
Wehavecreatedasharedaccountforthesewebsiteswhichyoushoulduseincaseyouneedtologin.
Ofcoursefeelfreetouseanyotherwebsites(justdonotfillinanyotherpersonaldetailsthere,preferablyusethesharedaccountas
well).
Tips
Navigator
‚Ä¢ Don‚Äôtdothingstooquickly! Savingtheactions,screenshotsandpagestakestimeandperformingtheactionsinaquick
successioncanintroduceerrorsintherecording,especiallyonheavywebsites.
Watchfortheiconindicatingthatthebrowserisprocessinganaction.
‚Ä¢ Donotperformanyunnecessaryactions(alltheactionswillberecordedandwewanttominimizetheamountofmindless
clickingandscrolling)
‚Ä¢ Waituntilthepagefullyloads.
‚Ä¢ Donotuseautofillfortextfields,alwaystypeeverythingfromscratch.
‚Ä¢ Donotchangethesizeofthebrowserwindowifnotnecessary.
Instructor
‚Ä¢ Be creative: assign tasks starting from very simple (‚Äúsubmit the form‚Äù) to very complex (multi-turn conversation with
changingtopics).
‚Ä¢ Askonlyaboutthingsthatarerelevanttothewebpage.
‚Ä¢ Waituntilthesystemperformstheiractions.
‚Äì However,feelfreetointerruptifsomethingdoesnotseemrightoryouhavechangedyourmind.
‚Ä¢ Finalizeallthetasksrightbeforechangingtheactualstateoftheworld(i.e.orderingproducts,submittingissuesetc.).
Notethattheextensiondoesnotworkinananonymouswindow.Ifyouwanttoclearyourhistoryafterwards,useCtrl+Shift+Delete.
45