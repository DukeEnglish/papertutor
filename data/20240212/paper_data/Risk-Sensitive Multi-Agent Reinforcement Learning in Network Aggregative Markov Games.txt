Risk-Sensitive Multi-Agent Reinforcement Learning in Network
Aggregative Markov Games
HafezGhaemi HamedKebriaei
UniversityofTehran,SchoolofECE UniversityofTehran,SchoolofECE
hafez.ghaemi@ut.ac.ir kebriaei@ut.ac.ir
AlirezaRamezaniMoghaddam MajidNiliAhamdabadi
UniversityofTehran,SchoolofECE UniversityofTehran,SchoolofECE
a.ramezany@ut.ac.ir mnili@ut.ac.ir
ABSTRACT years,especiallyinspecifictypesofMGs,suchaszero-sumMGs
Classicalmulti-agentreinforcementlearning(MARL)assumesrisk [2,36,41,47,63]andMarkovpotentialgames[14,16,22,27,29].
neutralityandcompleteobjectivityforagents.However,insettings However,therisk-neutralRLobjectiveoftenfallsshortwhenrepre-
whereagentsneedtoconsiderormodelhumaneconomicorsocial sentingagentswithdistinctsubjectivepreferences,suchasinternal
preferences,anotionofriskmustbeincorporatedintotheRLop- cognitivebiasesofthemselvesorofotheragents.Thus,toaddress
timizationproblem.ThiswillbeofgreaterimportanceinMARL thesepreferences,agentsintegrateariskmeasureintotheirRL
whereotherhumanornon-humanagentsareinvolved,possibly objective,usheringintotherealmofrisk-sensitivereinforcement
withtheirownrisk-sensitivepolicies.Inthiswork,weconsider learning(RSRL).Ingeneral,theliteratureonrisk-sensitiveMARL
risk-sensitiveandnon-cooperativeMARLwithcumulativeprospect ismoresparsecomparedtosingle-agentRSRL.Themajorityofthe
theory(CPT),anon-convexriskmeasureandageneralizationof worksthatconsiderrisk-sensitivemulti-agentMDPsareconcerned
coherentmeasuresofrisk.CPTiscapableofexplaininglossaver- notwithanRLsettingbuteitherwiththeoreticallyprovingthe
sioninhumansandtheirtendencytooverestimate/underestimate existenceofMarkovperfectNashequilibria,orfindingtheseequi-
small/largeprobabilities.Weproposeadistributedsampling-based libriausingiterativealgorithmsgivencompleteinformationofthe
actor-critic(AC)algorithmwithCPTriskfornetworkaggregative gameinacentralizedsettingforMDPswithspecificconstraints
Markovgames(NAMGs),whichwecallDistributedNestedCPT- [4,17,18,34,64,65].
AC.Underasetofassumptions,weprovetheconvergenceofthe RiskinRLcanbecategorizedintotwomaintypesbasedonthe
algorithmtoasubjectivenotionofMarkovperfectNashequilib- risk-sensitiveobjective,asdelineatedbyPrashanthandFu[37].
riuminNAMGs.Theexperimentalresultsshowthatsubjective Thefirstcategory,explicitrisks,involvesdirectlyincorporating
CPTpoliciesobtainedbyouralgorithmcanbedifferentfromthe theriskmeasureintotheobjectivefunction.Incontrast,implicit
risk-neutralones,andagentswithahigherlossaversionaremore risksareintegratedbyimposingaconstraintontheRLstochastic
inclinedtosociallyisolatethemselvesinanNAMG.1 optimizationproblem.Notably,inpractice,implicitrisk-sensitive
objectivesareoftentransformedintoexplicitobjectives.Thisis
achievedbyformulatingaLagrangianandcomputingitsgradient
KEYWORDS
toemployalgorithmsfoundedonpolicygradient(PG)methods[37].
Multi-AgentReinforcementLearning,Actor-Critic,Aggregative
WithinthespectrumofimplicitriskmeasuresinRLandMDPs,no-
Games,Risk-Sensitivity,CumulativeProspectTheory
tableexamplesincludevarianceasrisk([38,54,55]insingle-agent
ACMReferenceFormat: RSRL,and[43]inrisk-sensitiveMARL),andchanceconstraints
HafezGhaemi,HamedKebriaei,AlirezaRamezaniMoghaddam,andMajid ([10]insingle-agentRSRL).Ontheotherhand,explicitriskmea-
NiliAhamdabadi..Risk-SensitiveMulti-AgentReinforcementLearningin suresencompassentropicriskmeasurespredicatedonexponential
NetworkAggregativeMarkovGames.InProceedingsofPreprint(preprint).
return([6,15,30]insingle-agentRSRLand[32,51]inrisk-sensitive
ACM,NewYork,NY,USA,10pages.
MARL),coherentriskmeasures,andcumulativeprospecttheory
(CPT).
1 INTRODUCTION
Coherentriskmeasures[3,11],suchasthewell-knowncondi-
Markovgame(MG)isacommonframeworkforstudyingmulti- tionalvalueatrisk(CVaR),meansemi-deviation[48],andspectral
agentsystems(MAS),anditisthemaintheoreticalframeworkfor risk[1],arewidelyusedinthefieldsofeconomyandoperations
multi-agentreinforcementlearning(MARL)[26,49].Inclassical research.TheirapplicationhasalsobeenexploredwithinMDPs
MARL,eachagentisassumedtohavearisk-neutralobjective,i.e., asdynamicriskmeasures.Osogamietal.[33]showedthatrisk-
ittriestomaximizeanotionofexpectedreturnwithouttakinginto sensitiveMDPsgovernedbyMarkovcoherentriskmeasurescanbe
accountsubjectivepreferencesofitselforoftheotheragentsinthe classifiedunderthedomainofrobustMDPs.Subsequently,dynamic
MAS.Risk-neutralMARLinMGshasseengreatadvancesinrecent programmingmethodologieshavebeensuggestedforthistypeof
MDPs[7,45].Buildingontheseworks,PG-basedtechniquesand
1Codeavailableathttps://github.com/hafezgh/risk-sensitive-marl-namg
actor-critic(AC)algorithmshavealsobeendevelopedforRSRLwith
preprint,Risk-SensitiveMulti-AgentReinforcementLearninginNetworkAggregative coherentriskmeasures,asdetailedin[9,21,53,56]forsingle-agent
MarkovGames, RSRL,andin[31,42,67]forrisk-sensitiveMARL.
.
4202
beF
8
]GL.sc[
1v60950.2042:viXraCPTBackground. TheconceptofProspectTheory(PT)emerged thenestedstructure,whereintheCPToperatorisappliedtothe
asanalternativemodeltoexpectedutilitytheory,providingamore cumulativereturnaftereachstep(actiontaken)[23â€“25].Animpor-
accuratemodelofhumandecision-makingunderuncertainty[20]. tantadvantageofthisformulationisthatitensurestheexistenceof
ToenhancetheapplicabilityofPT,CumulativeProspectTheory aBellmanoptimalityequation.Recently,Tianetal.[58]extended
(CPT)wassubsequentlyintroduced[60].UnlikePT,CPTapplies thisnestedformulationtoamulti-agentsettingwithagentsthatare
weightingfunctionstocumulativeprobabilities,addressingthem characterizedbyboundedrationalityandoperatingunderquantal
separatelyforpositiveandnegativeoutcomes.Byintegratingthese level-ğ‘˜strategies[62].Restrictingtheirapproachtodeterministic
probabilityweightingfunctionsandanon-linearutilityfunction, policies,theyproposeacentralizedvalueiterationalgorithmto
CPTsuccessfullyillustratesvaryinghumanattitudestowardspo- determineoptimalrisk-sensitivepoliciesgivenacompletemodel
tentialgainsandlossesagainstasubjectivereferencepoint.Central oftheenvironmentandtherewardfunctions.
toCPTistheideathathumanstypicallyexhibitaversiontolosses, Inthesecondformulation,theCPToperatorisappliedsolely
i.e.,theygenerallytakemoreriskswhenfacingpotentialgains totheagentâ€™sfinalcumulativereturnattheendofeveryepisode
andtakefewerriskswhenconfrontedwithpotentiallosses.Addi- [19,39].Contrarytothenestedformulation,thisformulationdoes
tionally,CPTâ€™sframeworkelucidateshumaninclinationstoover- nothaveaBellmanequation.However,itcanbeapproachedfrom
estimatesmallprobabilitiesandunderestimatelargeonesduring astochasticoptimizationperspective,allowingpolicyoptimization
uncertaindecision-making.WhenweconsiderCPTinthecontext throughagradient-basedmethodakintoPGtechniques[19].This
ofeitherstaticordynamicMarkovriskmeasures,itmeetsonly PGmethodhasalsobeenimplementedbyconsideringneuralnet-
twoofthefourrequirementsthatdefineacoherentriskmeasure.A worksforpolicyapproximation[28].Itisimportanttoemphasize
riskmeasure,whenappliedtoarandomvariable(r.v.)representing thattheabsenceoftheBellmanequationinthiscontextnecessi-
potentialoutcomes,isdeemedcoherentifithasthefollowingfour tatespolicyoptimizationexclusivelythroughofflineMonteCarlo
characteristics:convexity,monotonicity,translationinvariance,and samplingconstrainedbyafinitetimehorizon.
positivehomogeneity[3].Amongthese,theCPTriskmeasureonly Todate,nocognitiveresearchhasbeenconductedtoascertain
possessesmonotonicityandpositivehomogeneity,andisneither whichofthetwoCPTRSRLformulationsbestrepresentsthedy-
translationinvariantnorconvex.Thenon-coherentnatureofCPT namicriskbehaviorexhibitedbyhumans,whetherinsingle-agent
makesitmorechallengingtoworkwithmathematically.CPTcan ormulti-agentenvironments.Nonetheless,thefollowingcanbe
beseenasageneralizationofcoherentriskmeasures,i.e.,byappro- saidaboutthetwoformulations:
priateselectionofCPTprobabilityweightingfunctions,onecan
â€¢ ThenestedformulationbenefitsfromthepresenceofaBell-
derivevariouscoherentriskmeasureformulations[19,25].
manequation,enablingtheuseofonlineactor-criticalgo-
Contributions. Inthiswork,weconsiderrisk-sensitiveMARL rithmsandarecursivelydefinedvaluefunction.Thisadvan-
with CPT risk measure in network aggregative Markov games tageisabsentinthenon-nestedformulation,whereitisonly
(NAMGs),andproposeadistributedactor-criticalgorithmtofind plausibletousePGtechniquesusingofflineMonteCarlo
risk-sensitivepoliciesforeachagent.Wederiveapolicygradient sampling.
theoremforCPTMARLbasedonasubjectivesteady-statedistri- â€¢ Inbothformulations,duetothesubstitutionoftheexpecta-
butionoftheMDPfromeachagentâ€™sprespective,andprovidea tionoperatorwiththenon-linearCPToperator,itispossible
sampling-basedapproachtoestimatethevaluefunctionswithas- fortheoptimalpolicytoexhibitnon-deterministiccharac-
ymptoticconsistency.SinceCPTisageneralizationofcoherent teristics[19,24]eveninsingle-agentRL.
riskmeasures,ourPGtheoremgeneralizesthepreviousPGworks â€¢ Thenon-nestedformulationalignswellwithfinite-horizon
forstaticanddynamiccoherentriskmeasures[9,53].Underaset episodictaskswheretheagentisrewardedattheendofeach
ofassumptions,weprovetheconvergenceofouralgorithmtoa episode.However,itsapplicabilityislimitedwhenconsider-
subjectiveandrisk-sensitivenotionofMarkovperfectNashequilib- inginfinite-horizontasks.Conversely,thenestedformula-
rium(MPNE)whichweshowisuniquegiventheaforementioned tionanditsBellmanequationaresuitablefortaskswhere
assumptions.Experimentally,wealsodemonstratethatahigher theagentisrewardedateverytimestep.
lossaversioncanmakeagentsmoreconservativeandincreasetheir â€¢ In scenarios without complete information of the model,
tendencyforsocialisolationinanNAMG. therewardfunction,orthepoliciesofotheragents,both
formulationsnecessitateastrategyforestimatingtheCPT
Remark1. (Application)Apotentialapplicationoftheproposed
valuegiventhatwehaveaccesstoasimulatoroftheMDPor
framework is calculating CPT risk-sensitive policies of human
alargeenoughexperiencedictionary(replaybuffer).Suchan
agentsinreal-worldsettings,suchasdrivingscenariosorfinan-
estimationtechniquetailoredforthenon-nestedformulation
cialmarkets,thatcanbemodeledbyNAMGs.Subsequently,these
hasbeenintroducedbyJieetal.[19].
policiescanservedualpurposes:guidingagentstowardsstrategies
optimizedfortheirindividualpreferencesorfacilitatingsocialor Remark2. (Motivation)Giventheaboveconsiderations,inrisk-
economicchangesintheenvironmenttosteeragentsinadirection sensitiveMARLwithCPT,inasettingwheretheagentsinteract
thatalignswithdesiredoutcomes. inanonlineinfinite-horizonMDPwithlimitedinformationabout
otheragentsâ€™policies,thenestedCPTformulationistheviable
2 RELATEDWORKS optiontoadopt.Duetothepossibilityofnon-deterministicopti-
InthecontextofMarkovriskmeasuresinMDPs,CPTisarticu- malpoliciesforeachagentinMARL,weoptforactor-criticstyle
latedthroughtwodistinctformulations.Thefirstformulationis algorithmsusingparameterizedpolicies.Furthermore,weconsiderNAMGsasourMARLframeworkduetothreereasons.First,be- persontopersonbasedontheirlevelofrisk-aversionandindividual
causetheyareinherentlysuitedtodistributedalgorithms.Second, characteristics.Theconventionalrepresentationsofweightingand
givenasetofassumptions,NAMG,anditsrisk-sensitiveversion utilityfunctionsgivenasetofsubjectiveparametersareplottedin
canbeshowntohaveauniqueMarkovperfectNashequilibrium Figures1and2.
whichouralgorithmconvergesto.Andthird,becauseNAMGsare
asuitableframeworktoshowthetangibleeffectoflossaversionin
human-likeagentsandontheirtendencyforsocialisolationand
conservatism.
3 PRELIMINARIES
3.1 CumulativeProspectTheory
Givenareal-valuedr.v.ğ‘‹ withdistributionP(ğ‘‹),areferencepoint
ğ‘¥0,twomonotonicallynon-decreasingweightingfunctions,ğœ”+ :
[0,1] â†’ [0,1],ğœ”âˆ’ : [0,1] â†’ [0,1],utilityfunctionsğ‘¢+ : R+ â†’
R+,ğ‘¢âˆ’ : Râˆ’ â†’ R+,andgivenappropriateintegrabilityassump-
tions,wecandefinetheCPTvalueusingChoquetintegralsas
âˆ« âˆ
CPT P[ğ‘‹] := ğœ”+(P(ğ‘¢+((ğ‘‹ âˆ’ğ‘¥0)+) >ğ‘¥))ğ‘‘ğ‘¥âˆ’
0 (1) Figure1:ConventionalCPTweightingfunctions;ğœ”+(ğ‘) =
âˆ« âˆ ğœ”âˆ’(P(ğ‘¢âˆ’((ğ‘‹ âˆ’ğ‘¥0)âˆ’) >ğ‘¥))ğ‘‘ğ‘¥., (ğ‘ğ›¾+(1âˆ’ğ‘ ğ‘ğ›¾
)ğ›¾)(1/ğ›¾)
andğœ”âˆ’(ğ‘)= (ğ‘ğ›¿+(1âˆ’ğ‘ ğ‘ğ›¿
)ğ›¿)(1/ğ›¿)
withğ›¾ =ğ›¿ =0.69.
0
wherewedenote(.)+ =ğ‘šğ‘ğ‘¥(0,.)and(.)+ = âˆ’ğ‘šğ‘–ğ‘›(0,.).Fora
discreter.v.,wecandefinetheCPTvaluesimilarlyas
ğ‘›
CPT P[ğ‘‹] :=âˆ‘ï¸ ğœ™+(Pğ‘ƒ(ğ‘‹ =ğ‘¥ ğ‘–))ğ‘¢+(ğ‘¥ ğ‘– âˆ’ğ‘¥0 )
ğ‘–=0
(2a)
âˆ’1
âˆ’âˆ‘ï¸ ğœ™âˆ’(P(ğ‘‹ =ğ‘¥ ğ‘–))ğ‘¢âˆ’(ğ‘¥
ğ‘–
âˆ’ğ‘¥0 ),
âˆ’ğ‘š
ğ‘›
ğœ™+(P(ğ‘‹ =ğ‘¥ ğ‘–))=ğœ”+(cid:169) (cid:173)âˆ‘ï¸ P(ğ‘‹ =ğ‘¥ ğ‘—)(cid:170)
(cid:174)
ğ‘—=ğ‘–
(cid:171) (cid:172) (2b)
ğ‘›
âˆ’ğœ”+(cid:169) (cid:173)âˆ‘ï¸ P(ğ‘‹ =ğ‘¥ ğ‘—))(cid:170) (cid:174),
ğ‘—=ğ‘–+1
(cid:171) (cid:172)
ğ‘–
ğœ™âˆ’(P(ğ‘‹ =ğ‘¥ ğ‘–))=ğœ”âˆ’(cid:169)
(cid:173)
âˆ‘ï¸ P(ğ‘‹ =ğ‘¥ ğ‘—)(cid:170)
(cid:174) Figure2:ConventionalCPTutilityfunctions;Theplotshows
(cid:171)ğ‘—=âˆ’ğ‘š (cid:172) (2c) ğ‘¢+(ğ‘¥) =ğ‘¥ğ›¼ forğ‘¥ â‰¥ 0,andâˆ’ğ‘¢âˆ’(ğ‘¥) =âˆ’ğœ†(âˆ’ğ‘¥)ğ›½)forğ‘¥ < 0,with
âˆ’ğœ”âˆ’(cid:169)
(cid:173)
âˆ‘ï¸ğ‘–âˆ’1
P(ğ‘‹ =ğ‘¥ ğ‘—)(cid:170) (cid:174),
ğ›¼ =ğ›½ =0.65andğœ†=2.6.
ğ‘—=âˆ’ğ‘š
(cid:171) (cid:172)
whereğ‘¥0 servesasareferencepointthatseparatesgainsand
3.2 NetworkAggregativeMarkovGames
losses.Withoutlossofgenerality,weassumeğ‘¥0=0throughoutthis
Throughoutthispaper,weassumethatagentsareinteractingin
paper.ConventionalrepresentationsofCPTweightingfunctions
includeğœ”+(ğ‘) = (ğ‘ğ›¾+(1âˆ’ğ‘ ğ‘ğ›¾
)ğ›¾)(1/ğ›¾)
andğœ”âˆ’(ğ‘) = (ğ‘ğ›¿+(1âˆ’ğ‘ ğ‘ğ›¿
)ğ›¿)(1/ğ›¿)
a inn fie nr ig teo -d hi oc rn ize otw no cr rk itea rg ig or ne .g Aat niv Ne AM Mar Gko wv ig tham ğ‘ew pli at yh ea rsdi is sc ao nun Mte Gd
[60], or ğœ”+(ğ‘) = exp(âˆ’(âˆ’ğ‘™ğ‘›ğ‘)ğ›¾) and ğœ”âˆ’(ğ‘) = exp(âˆ’(âˆ’ğ‘™ğ‘›ğ‘)ğ›¿) denotedbyğ‘€ =(ğ‘†,ğ‘,ğ´,ğ‘…,ğ‘ƒ,ğº,ğ›¾,ğ‘ ğ‘ 0),whereğ‘†isthestatespace,
[40]. Note that by setting ğ›¿ andğ›¾ equal to 1, the definition of ğ´=ğ´1Ã—...Ã—ğ´
ğ‘
isthejointactionspace;ğ‘…:ğ‘†Ã—ğ´Ã—ğ‘† â†’Rğ‘ isa
expectedutilityğ¸ P[ğ‘¢(ğ‘‹)]isrecoveredwhichshowsthatCPTisa jointrewardfunctionboundedin[âˆ’ğ‘… ğ‘šğ‘ğ‘¥,ğ‘… ğ‘šğ‘ğ‘¥]whereğ‘…
ğ‘šğ‘ğ‘¥
>0;
generalizationofexpectedutilitytheory.Furthermore,ğ‘¢+andğ‘¢âˆ’ ğ‘ƒ(.|ğ‘ ,ğ‘) istheMDPtransitionprobabilitydistribution; G(N,E)
areusuallyconcavefunctions(âˆ’ğ‘¢âˆ’ isconvex)toreflectthehigher isagraphwithedgesetEonwhicheachagentinteractswithits
sensitivityofhumanstowardslossescomparedtogains[20].As neighbors;ğ›¾istheMDPâ€™sdiscountfactor;andğ‘ istheinitialstate
ğ‘ 0
aresult,theutilityfunctioncanhaveanalyticalrepresentations distribution.InNAMG,foreachagentğ‘›,therewardfunctionis
ğ‘¢+(ğ‘¥)=ğ‘¥ğ›¼ ifğ‘¥ â‰¥0,andğ‘¢âˆ’(ğ‘¥)=ğœ†(âˆ’ğ‘¥)ğ›½ ifğ‘¥ <0.Theparameters afunctionofitsownactionandanaggregativefunctionofother
ğ›¾,ğ›¿,ğ›¼,ğ›½,andğœ†aresubjectivemodelparametersthatcandifferfrom agentsâ€™actions,Therefore,byobservingtheactionsofneighboringagentsand
ğ‘…ğ‘– (ğ‘ ,ğ‘ğ‘–,ğ‘âˆ’ğ‘– )=ğ‘…ğ‘– (ğ‘ ,ğ‘ğ‘–,ğœğ‘– (ğ‘âˆ’ğ‘– )), (3) calculatingtheaggregativetermğœâˆ’ğ‘–,agentğ‘–cantreatP(ğœâˆ’ğ‘–|ğ‘ )as
wherewehave, aprobabilitydistributionsimilartothetransitionprobabilitiesfor
eachstate.
ğœğ‘– (ğ‘âˆ’ğ‘– )= âˆ‘ï¸ ğœ” ğ‘–ğ‘—ğ‘ğ‘—, (4)
ğ‘—âˆˆN\ğ‘– 4 DISTRIBUTEDNESTEDCPTPOLICY
whereğœ” aretheedgeweightsofthecommunicationgraphG, GRADIENT
withğ‘¤ denotingtheweightoftheedgefrom ğ‘— toğ‘–.Therefore,
ğ‘–ğ‘— Inthissection,wederiveagradientexpressionfortheMarkov
giventhegraph,andbyobservingitsneighborsâ€™actions,agentğ‘–is
dynamicCPTriskmeasureinNAMGs,representedbythegradient
abletocalculateğœğ‘–(ğ‘âˆ’ğ‘–).Figure3showsaschematicofanNAMG.
oftheinitialstateâ€™svaluefunctioninanergodicCPTrisk-sensitive
NAMG,âˆ‡ğ‘‰ ğœ‹ğ‘– ğœƒ(ğ‘ 0).BeforepresentingthePGtheorem,westatethe
followingassumption,
Markov Environment Assumption1. Theweightfunctionsğ‘¤Â±aredoubledifferentiable,
ğ‘!" ğ‘#" ğ‘$" ğ‘%" ğ‘&" andthederivativesğ‘¤ Â±â€² areLipschitzcontinuouswithcommoncon-
ğ‘…$"
ğœ$"
s nt oa tn edtğ¿ b. yF ğ‘¢u â€²rt )h fe or rm ao llre a, gt eh ne tsu .tilityfunctionsğ‘¢Â±aredifferentiable(de-
ğ‘…!" P3 ğ‘…&" Â±
ğœ!" P1 P5 ğœ&" Theaboveassumptionmayseemstrictatfirst.However,con-
ventionalformsoftheCPTutilityfunctions,specificallyğ‘¢+(ğ‘¥)=
ğ‘…#" ğ‘…%" ğ‘¥ğ›¼ andâˆ’ğ‘¢âˆ’(ğ‘¥) = âˆ’ğœ†(âˆ’ğ‘¥)ğ›½,alongwiththeweightingfunctions
P2 P4 ğœ”+(ğ‘) =
ğ‘ğ›¾
andğœ”âˆ’(ğ‘) =
ğ‘ğ›¿
,depicted
ğœ#"
ğœ%"
inFigures(ğ‘ 1ğ›¾ a+ n(1 dâˆ’ 2ğ‘ ,) sğ›¾ a) t( i1 s/ğ›¾ fy) thisassumptio( nğ‘ .ğ›¿+(1âˆ’ğ‘)ğ›¿)(1/ğ›¿)
Theorem1. (NestedCPTPolicyGradient)
Figure3:AnetworkaggregativeMarkovgame GivenAssumption1,thegradientoftheCPTreturnforagentğ‘–,
Previously,invariousdomains,suchasresourceallocation[12],
ğ‘‰ ğœ‹ğ‘– ğœƒ(ğ‘ 0),withrespecttothepolicyparameterğœƒğ‘– is
socialnetworks[66],electricalmicrogrids[57],andpowersystems
[ n1 a3 m], icei nth ete wr osi rn kg al ge- gs rt ea gt ae tn ive etw gao mrk esag hg ar ve eg ba eti ev ne sg tuam diees d( iN nA riG sks -) n, eo ur td ray l- âˆ‡ğ‘‰ ğœ‹ğ‘– ğœƒ(ğ‘ 0)âˆE
ğœ‡ ğ‘ğ‘– ğ‘ğ‘¡(ğ‘ )
(cid:20) âˆ‘ï¸ ğœ•(ğœ‹ğ‘–(ğ‘ğ‘–|ğ‘ )P(ğœğœ•ğœ™
âˆ’ğ‘–|ğ‘ )P(ğ‘ â€²|ğ‘ ,ğ‘))
ğ‘,ğ‘ â€² ğœƒ
setting.Furthermore,mostofthetheoreticalworksinthisdomain
(cid:21)
havefocusedonstudyingconvergencetotheMarkovperfectNash P(ğœâˆ’ğ‘– |ğ‘ )P(ğ‘ â€²|ğ‘ ,ğ‘)(âˆ‡ğœ‹ ğœƒğ‘–(ğ‘ğ‘– |ğ‘ ))ğ‘¢(ğ‘…ğ‘– (ğ‘ ,ğ‘ğ‘–,ğœâˆ’ğ‘–,ğ‘ â€²)+ğ›¾ğ‘‰ ğœ‹ğ‘– ğœƒ(ğ‘ â€²)) ,
orStackelbergequilibriuminsingle-stateordynamicNAGswith
(7)
quadraticcost/rewardfunctionsthatensuretheuniquenessofthe
equilibrium[8,35,46,50].Inthispaper,forthefirsttime,wecon- where,ğœ™andğ‘¢representtheCPTcumulativeweightingandutility
siderrisk-sensitiveNAMGs. functionsoftheagentfrom(2)(superscriptğ‘–isdropped).Thedistri-
butionğœ‡ğ‘– isasubjectivesteady-stateprobabilitydistributionofthe
ğ‘ğ‘ğ‘¡
3.3 CPTRisk-SensitiveMARLObjectivein
NAMGs
MDPinwhichğœ‡ ğ‘ğ‘– ğ‘ğ‘¡(ğ‘ )=
(cid:205)
ğ‘ ğœ‚ âˆˆğ‘†ğ‘ğ‘– ğ‘ ğœ‚ğ‘¡ ğ‘ğ‘–( ğ‘ğ‘  ğ‘¡) (ğ‘ ),whereğœ‚ğ‘– CPT(ğ‘ )isasubjective
Usingthenestedformulation,theobjectiveoftherisk-sensitive measureoftimespentineachstateandcanbeobtainedbysolving
agentğ‘–inanNAMGmax ğ½ğœ‹ğ‘–,ğœ‹âˆ’ğ‘– willbeequivalentto thefollowingsystemoflinearequations,
ğœ‹ğ‘–
m ğœ‹a ğ‘–xğ‘‰ ğœ‹ğ‘– (ğ‘ 0)=m ğœ‹a ğ‘–xCPT ğœ‹(ğ‘ 0,.)Ã—P(.|ğ‘ 0,ğ‘0) (cid:2)ğ‘…ğ‘– (ğ‘  ğœ,ğ‘ ğœ)+... ğœ‚ ğ‘ğ‘– ğ‘ğ‘¡(ğ‘ )=ğ‘0(ğ‘ )+âˆ‘ï¸ ğœ‚ ğ‘ğ‘– ğ‘ğ‘¡(ğ‘ Â¯)âˆ‘ï¸ ğœ™(ğœ‹(ğ‘ğ‘– |ğ‘ Â¯)P(ğœâˆ’ğ‘– |ğ‘ Â¯)P(ğ‘ |ğ‘ Â¯,ğ‘)) ğœ•ğ‘‰ğœ•ğ‘¢
ğ‘–
(ğ‘ ),
+ğ›¾ğœCPT
ğœ‹(ğ‘ ğœ,.)Ã—P(.|ğ‘ ğœ,ğ‘ğœ)
(cid:2)ğ‘…ğ‘– (ğ‘  ğœ,ğ‘ ğœ)+...(cid:3)(cid:3) ğ‘ Â¯âˆˆğ‘† ğ‘âˆˆğ´ ğœ‹ (ğœƒ
8)
=m ğœ‹a ğ‘–xCPT ğœ‹(ğ‘ 0,.)Ã—P(.|ğ‘ 0,ğ‘0) (cid:2)ğ‘…ğ‘– (ğ‘ 0,ğ‘0)+ğ›¾ğ‘‰ ğœ‹ğ‘– (ğ‘ 1)(cid:3), insw tah te er ğ‘ e ,ğ‘ a0 n( dğ‘ ) ğ‘¢d ae nn dot ğœ™es at rh ee thp ero ub ta ilb iti ylit cy umth ua lt at th ive eM wa er igk hov tinc gha fuin ncs tt ia or nts
s
(5)
ofagentğ‘– from(2)(ğ‘¢Â± andğœ™Â± arechosenaccordingtothesignof
tivew lyh .e Ure siğœ‹ ng(ğ‘  ğ‘¡ th,. e) p= roğœ‹ pğ‘– e( rğ‘  tğ‘¡ i, e. s) oÃ— fğœ‹ Nâˆ’ Ağ‘– M(ğ‘  ğ‘¡ G,. s) ,aa nn dd cğ‘ oğ‘¡ n= sid(ğ‘ eğ‘– ğ‘¡ r, inğ‘ gğ‘¡âˆ’ğ‘– P), (ğœre 0âˆ’s ğ‘–p |ğ‘ e 0c )- ğ‘…ğ‘–(ğ‘ Â¯,ğ‘,ğ‘ )+ğ›¾ğ‘‰ ğœ‹ğ‘– ğœƒ(ğ‘ )).
astheprobabilitythatğœ 0âˆ’ğ‘– occursatstateğ‘ 0 foragentğ‘–,wecan
Proof. Consideringagentğ‘–,wedropthesubscriptğ‘–anddenote
rewritetheobjectiveas ğ‘…ğ‘–(ğ‘  ğ‘¡,ğ‘ ğ‘¡,ğ‘  ğ‘¡+1)asğ‘…
ğ‘¡
andğ‘‰ ğœ‹ğ‘– ğœƒ(ğ‘  ğ‘¡)asğ‘‰ ğœ‹ğœƒ(ğ‘  ğ‘¡).Furthermore,ğœ‹ ğœƒ(ğ‘ ,ğ‘)
used below, whereğ‘ is equivalent to (ğ‘ğ‘–,ğ‘âˆ’ğ‘–), is the more gen-
m ğœ‹a ğ‘–xğ‘‰ ğœ‹ğ‘–(ğ‘ 0)=m ğœ‹a ğ‘–xCPT ğœ‹ğ‘–(ğ‘ğ‘– 0|ğ‘ 0)Ã—P(ğœ 0âˆ’ğ‘–|ğ‘ 0)Ã—P(ğ‘ 1|ğ‘ 0,ğ‘0) (cid:2)ğ‘…ğ‘–(ğ‘ 0,ğ‘0)+ğ›¾ğ‘‰ ğœ‹ğ‘–(ğ‘ 1)(cid:3). eralcaseofjointpoliciesinMarkovgames,whichencompasses
(6) ğœ‹ ğœƒ(ğ‘ğ‘–|ğ‘ )P(ğœâˆ’ğ‘–|ğ‘ )inanNAMG.ThegradientoftheCPTrisk-sensitivereturnconsideringitsrecursivedefinitioncanbewrittenas [52],Section9.2),ğœ‚(ğ‘ )canbecalculatedbysolvingthefollowing
âˆ‡ğ‘‰ğœ‹ğœƒ(ğ‘ 0)= systemoflinearequations,
(cid:34) (cid:35)
âˆ‘ï¸
âˆ‡
ğ‘0,ğ‘ 1
(cid:20)ğœ™(ğœ‹(ğ‘ 0,ğ‘0)P(ğ‘ 1|ğ‘ 0,ğ‘0))ğ‘¢(ğ‘…0+ğ‘‰ğœ‹ğœƒ(ğ‘ 1))
ğœ‚ ğ‘ğ‘ğ‘¡(ğ‘ )=ğ‘0(ğ‘ )+
ğ‘ âˆ‘ï¸
Â¯âˆˆğ‘†ğœ‚ ğ‘ğ‘ğ‘¡(ğ‘ Â¯)
ğ‘âˆ‘ï¸
âˆˆğ´ğœ™(ğœ‹(ğ‘ Â¯,ğ‘)P(ğ‘ |ğ‘ Â¯,ğ‘))
ğœ•ğ‘‰ğœ•ğ‘¢
ğœ‹ğœƒ
(ğ‘ ), (13)
âˆ‘ï¸
= âˆ‡ğœ™(ğœ‹(ğ‘ 0,ğ‘0)P(ğ‘ 1|ğ‘ 0,ğ‘0))ğ‘¢(ğ‘…0+ğ‘‰ğœ‹ğœƒ(ğ‘ 1)) where ğ‘0 is the probability distribution of the starting state.
ğ‘0,ğ‘ 1
Therefore,wecanwrite(9)as
(cid:21)
+ğœ™(P(ğ‘ 1|ğ‘ 0,ğ‘0))âˆ‡ğ‘¢(ğ‘…0+ğ‘‰ğœ‹ğœƒ(ğ‘ 1))
âˆ‡ğ‘‰ ğœ‹ğœƒ(ğ‘ 0)=âˆ‘ï¸ ğœ‚ ğ‘ğ‘ğ‘¡(ğ‘ )âˆ‘ï¸ âˆ‡ğœ™(ğœ‹(ğ‘ ,ğ‘)P(ğ‘ â€²|ğ‘ ,ğ‘))
âˆ‘ï¸ (cid:34) ğ‘  ğ‘,ğ‘ â€² (14)
=
ğ‘0,ğ‘ 1
âˆ‡ğœ™(ğœ‹(ğ‘ 0,ğ‘0)P(ğ‘ 1|ğ‘ 0,ğ‘0))ğ‘¢(ğ‘…0+ğ‘‰ğœ‹ğœƒ(ğ‘ 1)) ğ‘¢(ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ‘‰ ğœ‹ğœƒ(ğ‘ â€²)).
+ğœ™(ğœ‹(ğ‘ 0,ğ‘0)P(ğ‘ 1|ğ‘ 0,ğ‘0)) ğœ•ğ‘‰ğœ‹ğœ• ğœƒğ‘¢
(ğ‘ 1) (9)
Asğœ‚ ğ‘ğ‘ğ‘¡(ğ‘ )ispositiveforallğ‘ ,wecandefineğœ‡ ğ‘ğ‘ğ‘¡(ğ‘ )=
(cid:205)
ğ‘ ğœ‚ âˆˆğ‘†ğ‘ğ‘ ğœ‚ğ‘¡ ğ‘( ğ‘ğ‘  ğ‘¡)
(ğ‘ )
(cid:20) (cid:21)(cid:35) asthesubjectivelimiting(steady-state)distributionoftheCPTrisk-
âˆ‘ï¸
âˆ‡ ğœ™(ğœ‹(ğ‘ 1,ğ‘1)P(ğ‘ 2|ğ‘ 1,ğ‘1))ğ‘¢(ğ‘…1+ğ‘‰ğœ‹ğœƒ(ğ‘ 2)) sensitiveMDP,andtherefore,wehave
ğ‘1,ğ‘ 2
(cid:34)
= ğ‘âˆ‘ï¸
0,ğ‘ 1
âˆ‡ğœ™(ğœ‹(ğ‘ 0,ğ‘0)P(ğ‘ 1|ğ‘ 0,ğ‘ ğœ•0 ğ‘¢))ğ‘¢(ğ‘…0+ğ‘‰ğœ‹ğœƒ(ğ‘ 1)) âˆ‡ğ‘‰ ğœ‹ğœƒ(ğ‘ 0)âˆE ğœ‡ğ‘ğ‘ğ‘¡(ğ‘ ) (cid:20) âˆ‘ï¸ ğ‘,ğ‘ â€²âˆ‡ğœ™(ğœ‹(ğ‘ ,ğ‘)P(ğ‘ â€²|ğ‘ ,ğ‘))
(15)
+ğœ™(ğœ‹(ğ‘ 0,ğ‘0)P(ğ‘ 1|ğ‘ 0,ğ‘0))
ğœ•ğ‘‰ğœ‹ğœƒ(ğ‘ 1) ğ‘¢(ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ‘‰
ğœ‹ğœƒ(ğ‘ â€²))(cid:21)
.
(cid:20)
âˆ‘ï¸
âˆ‡ğœ™(ğœ‹(ğ‘ 1,ğ‘1)P(ğ‘ 2|ğ‘ 1,ğ‘1))ğ‘¢(ğ‘…1+ğ‘‰ğœ‹ğœƒ(ğ‘ 2))
Itisinterestingtocompare(15)withthesimilarexpressionin
ğ‘1,ğ‘ 2
(cid:20) (cid:21)
+ğœ™(ğœ‹(ğ‘ 1,ğ‘1)P(ğ‘ 2|ğ‘ 1,ğ‘1))âˆ‡ğ‘¢(ğ‘…1+ğ‘‰ğœ‹ğœƒ(ğ‘ 2))(cid:21)(cid:35)
.
risk-neutralpolicygradienttheorem,E
ğœ‡(ğ‘ )
(cid:205) ğ‘âˆ‡ğœ‹(ğ‘ ,ğ‘)ğ‘„(ğ‘ ,ğ‘) .
Duetothenon-linearCPToperator(comparedtothelinearex-
Wedefine pectation operator), the policy is entangled with the transition
âˆ‘ï¸ ğœ•ğ‘¢ probabilitiesinsidethegradientofthecumulativeweightingfunc-
ğ·ğ‘ƒğ‘Ÿ(ğ‘ 0â†’ğ‘ 1,ğ‘˜=1,ğœ‹ ğœƒ):=
ğ‘0
ğœ™(ğœ‹(ğ‘ 0,ğ‘0)P(ğ‘ 1|ğ‘ 0,ğ‘0))
ğœ•ğ‘‰ğœ‹ğœƒ(ğ‘ 1)
(10)
tion,andtherefore,intherisk-sensitivecase,itisnotpossibleto
defineastand-aloneQ-functionasafunctionofstateandactionto
asthesubjective(distorted)visitationprobabilityofğ‘ 1rightaf-
measurethequalityofanactioninagivenstate.AsnotedbyLin
terğ‘ 0 followingpolicyğœ‹ ğœƒ.Notethatsinceğ‘¢ isanon-decreasing
[23],thiscomplicationhastheconsequencethattheoptimalrisk-
functionwithpositivederivativeseverywhereandğœ™isafunction
sensitivepolicyeveninthesingle-agentsettingcanbestochastic.
thatmaps[0,1]to[0,1],thistermisalwayspositive.Bydefining
Tofurtherexpandtheaboveexpression,wecanusethechainrule
ğ·ğ‘ƒğ‘Ÿ(ğ‘ 0â†’ğ‘ 0,0,ğœ‹ ğœƒ):=1,byrecursion,wecanwritethesubjective
ofcalculusandwrite,
probabilityofvisitingstateğ‘  ğ‘˜+1afterğ‘˜+1steps,startingfromğ‘ 0
andfollowingpolicyğœ‹ as
ğœƒ
(cid:20) âˆ‘ï¸ ğœ•ğœ™
ğ·ğ‘ƒğ‘Ÿ(ğ‘ 0â†’ğ‘  ğ‘˜+1,ğ‘˜+1,ğœ‹ ğœƒ)=
âˆ‡ğ‘‰ ğœ‹ğœƒ(ğ‘ 0)âˆE
ğœ‡ğ‘ğ‘ğ‘¡(ğ‘ )
ğ‘,ğ‘ â€²
ğœ•(ğœ‹ ğœƒ(ğ‘ ,ğ‘)P(ğ‘ â€²|ğ‘ ,ğ‘))
(16)
âˆ‘ï¸
ğ‘ ğ‘˜
ğ·ğ‘ƒğ‘Ÿ(ğ‘ 0â†’ğ‘  ğ‘˜,ğ‘˜,ğœ‹ ğœƒ)ğ·ğ‘ƒğ‘Ÿ(ğ‘  ğ‘˜ â†’ğ‘  ğ‘˜+1,1,ğœ‹ ğœƒ) (11)
P(ğ‘ â€²|ğ‘ ,ğ‘)(âˆ‡ğœ‹ ğœƒ(ğ‘ ,ğ‘))ğ‘¢(ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰
ğœ‹ğœƒ(ğ‘ â€²))(cid:21)
.
Therefore,afterrepeatedunrolling,wecanwrite(9)as
ThisisthegeneralcaseofPGinCPTrisk-sensitiveMARL.Given
âˆ‘ï¸(cid:32)(cid:18) âˆ‘ï¸âˆ (cid:19) theaggregativetermğœâˆ’ğ‘– inNAMGs,wecanrewritethisequation
âˆ‡ğ‘‰ ğœ‹ğœƒ(ğ‘ 0)= ğ·ğ‘ƒğ‘Ÿ(ğ‘ 0â†’ğ‘ ,ğ‘˜,ğœ‹ ğœƒ)
as(7).
ğ‘  ğ‘˜=0
(12) â–¡
(cid:33)
âˆ‘ï¸ âˆ‡ğœ™(ğœ‹(ğ‘ ,ğ‘)P(ğ‘ â€²|ğ‘ ,ğ‘))ğ‘¢(ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ‘‰ ğœ‹ğœƒ(ğ‘ â€²)) .
Wenowprovideanalgorithmtoestimatetheabovegradientus-
ğ‘,ğ‘ â€²
ingsamplesfromasimulatoroftheenvironmentoralargeenough
Similartoarisk-neutralMDP,givenğœ‹ andthestatevaluefunc-
ğœƒ experiencedictionary.Thisapproximationschemewhichislater
tioncorresponding tothis policy,thefunctionğ·ğ‘ƒğ‘Ÿ isan inher-
usedtoalsoestimatethevaluefunctionisAlgorithm1isproposed
ent property of the CPT risk-sensitive MDP (this function can
byJieetal.[19]toestimatetheCPTvalueofanr.v.,ğ‘‹,usingsam-
be compared with the function ğ‘ƒğ‘Ÿ in the proof of risk-neutral
plesfromitsdistribution.ThefollowingAssumption(A2in[19])is
policygradienttheoremin[52],Section13.2).Therefore,welet
neededtoguaranteetheasymptoticconsistencyofthisestimation
ğœ‚ ğ‘ğ‘ğ‘¡(ğ‘ ) := (cid:205) ğ‘˜âˆ =0ğ·ğ‘ƒğ‘Ÿ(ğ‘ 0 â†’ ğ‘ ,ğ‘˜,ğœ‹ ğœƒ),whichcanbeconsidereda
algorithm.
subjective(perceived)measureoftimethattheCPTrisk-sensitive
agentspendsinstateğ‘ whenfollowingpolicyğœ‹ ğœƒ andstartingfrom Assumption2. Theutilityfunctionsğ‘¢+andğ‘¢âˆ’arecontinuousand
stateğ‘ 0.Inasimilarfashionasinrisk-neutralergodicMDPs(see non-decreasingontheirsupportR+andRâˆ’,respectively.Algorithm1CPTValueEstimation
1: R soe rq teu dir ine: asS ca em np dl ie ns gğ‘‹ or1 d,. e.. r, .ğ‘‹ ğ‘› from the distribution of r.v. ğ‘‹, ğ‘‡ ğ‘ğ‘ğ‘¡ğ‘‰ ğœ‹ğœƒ(ğ‘ )=CPT
ğœ‹ğœƒ(.|ğ‘ )Ã—P(.|ğ‘ ,ğ‘)
(cid:2)ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰ ğœ‹ğœƒ(ğ‘ â€²)(cid:3) (17)
2: Let Thefollowingassumptionisneededtoensurethattheoperator
ğœŒË† ğ‘+
ğ‘ğ‘¡
:=âˆ‘ï¸
ğ‘–=ğ‘›
1ğ‘¢+(ğ‘‹
ğ‘–)(cid:18) ğœ”+(cid:18)ğ‘›+ ğ‘›1âˆ’ğ‘–(cid:19) âˆ’ğœ”+(cid:18)ğ‘› ğ‘›âˆ’ğ‘–(cid:19)(cid:19)
,
Ain s( s1 u7 m)i ps ta iosu np 3-n .o Tr hm ec uo tn ilt itr yac ft uio nn ct.
ionsğ‘¢+andğ‘¢âˆ’ areinvertible(de-
ğœŒË† ğ‘âˆ’ ğ‘ğ‘¡ :=âˆ‘ï¸ ğ‘–=ğ‘› 1ğ‘¢âˆ’(ğ‘‹ ğ‘–)(cid:18) ğœ”âˆ’(cid:18) ğ‘›ğ‘–(cid:19) âˆ’ğœ”âˆ’(cid:18)ğ‘–âˆ’ ğ‘›1(cid:19)(cid:19) . n w
âˆ«
0o e ğ›¾t ğ‘e hd ğœ”avb +ey (ğ‘¢ Pğ‘¢ + (+âˆ’ ğ‘‹(1 0a ) <n =d ğ‘¥ğ‘¢ )ğ‘¢ )âˆ’âˆ’âˆ’ ğ‘¢1 ( +â€²0) ()a ğ›¾n ğ‘=d âˆ’0d ğ‘¥.i )Fff ğ‘‘ue ğ‘¥rr te +hn âˆ«et 0ri ğ›¾a , ğ‘b th ğœ”le e âˆ’r( ed (e e Pn x (o i ğ‘‹st te sd >ğ›½b ğ‘¥y âˆˆ )ğ‘¢ )( ğ‘¢+â€² 0
â€²
âˆ’,a 1 (n ) ğ‘¥d )sğ‘¢ ğ‘‘uâ€² âˆ’ ğ‘¥ch) â‰¤, ta hn ğ›½ad ğ‘t
holdsforanyğ‘ >0andanynon-negativereal-valuedr.v.ğ‘‹,whereğ›¾
3: ReturnğœŒË† ğ‘ğ‘ğ‘¡ =ğœŒË† ğ‘+ ğ‘ğ‘¡ âˆ’ğœŒË† ğ‘âˆ’ ğ‘ğ‘¡. isthediscountfactoroftheMDP.
SimilartoAssumptions1and2,theaboveassumptioncanalso
beverifiedtoholdfortypicalanalyticalformsofğœ”Â± andğ‘¢Â± in
The above assumption also holds for conventional forms of
Figures2and1asshownbyLinetal.[24].Underthisassumption,
weightingandutilityfunctionsinFigures1and2[19].GivenAs-
basedonTheorem6in[24],theğ‘‡ğ· operator(17)isasup-norm
sumptions1and2,Proposition4in[19]isverifiedandforagivenr.v.
contractiononaBanachspacedefinedovertheMDPâ€™sstatespace
ğ‘‹,Algorithm1isguaranteedtohaveasymptoticconsistency,i.e.,
thatincludesallpossiblestatevaluefunctionsğ‘‰ .Therefore,for
ğ‘›it ,c ao pn pv re or ag ce hs esto inC fiP nT it[ yğ‘‹ .]asymptoticallyasthenumberofsamples, everyğ‘‰ ğœ‹ğœƒ,ğ‘‰Â¯ ğœ‹ğœƒ âˆˆğµ(ğ‘†),thereexistsğ›¼ âˆˆ (0,1)sucğœ‹ hğœƒ that
Gradientestimation. Tohaveaestimateofthegradientin(7), âˆ¥ğ‘‡ ğ‘ğ‘ğ‘¡ğ‘‰ ğœ‹ğœƒ âˆ’ğ‘‡ ğ‘ğ‘ğ‘¡ğ‘‰Â¯ ğœ‹ğœƒâˆ¥âˆ â‰¤ğ›¼âˆ¥ğ‘‰ ğœ‹ğœƒ âˆ’ğ‘‰Â¯ ğœ‹ğœƒâˆ¥âˆ (18)
weneedestimatesofCPTvaluescorrespondingto
ğœ™(ğœ‹(ğ‘ğ‘–|ğ‘ Â¯)P(ğœâˆ’ğ‘–|ğ‘ Â¯)P(ğ‘ |ğ‘ Â¯,ğ‘))and ğœ•(ğœ‹ğœƒ(ğ‘ğ‘–|ğ‘ )P(ğœğœ•ğœ™
âˆ’ğ‘–|ğ‘ Â¯)P(ğ‘ â€²|ğ‘ ,ğ‘))
(which R coe nm tra ar ck tio3 n. o( fA tp hp eli ğ‘‡ca ğ·bi oli pty ero af toli rn (e 7a )r hf au sn oct nio lyn ba ep ep nro vx aim lida at ti eo dn) foT rh ae
wedenotebyğœ™â€²(cid:0)ğœ‹ ğœƒ(ğ‘ğ‘–|ğ‘ )P(ğœâˆ’ğ‘–|ğ‘ Â¯)P(ğ‘ â€²|ğ‘ ,ğ‘)(cid:1)).NotethatAssump- tabular representation of the state-value function [24]. We also
tion1statesthatğœ”â€² areLipschitzandtherefore,theycanbeusedas assessedthepossibilityofapproximatingthestatevaluefunction
Â±
independentCPTweightingfunctionswithcorrespondingcumula- usinglinearfunctionsforscalinguptheproposedactor-criticalgo-
tiveweightingfunctionsğœ™â€².Giventhetransitionprobabilitiesand rithmtolargeorcontinuousstatespaces.Thetraditionalproofof
Â±
repeateddistributedsamplingofrewardsandtransitionsbyagents convergenceforağ‘‡ğ·criticwithlinearfunctionapproximationre-
fromtheenvironmentortheexperiencedictionaries,thetermin quiresthecontractionofthisoperatorwithrespecttotheğ¿2norm
bracketscorrespondingtoeachstatecanbeestimatedusingAlgo- definedoverthesteady-statedistributionoftheMDP(Lemma4
rithm(1).Furthermore,usingthesesamplesandsolvingalinear inTsitsiklisandVanRoy[59]).However,viacounterexample,it
systemofequationsresultingfrom(13),thesubjectivesteady-state canbeseenthatthispropertydoesnotnecessarilyholdfortheğ‘‡ğ·
distributionğœ‡ ğ‘ğ‘ğ‘¡(ğ‘ )canbefound.Wenotethatthisestimationalgo- operator(7).
rithmismodel-basedandrequirestransitionprobabilities,however,
itdoesnotassumeanyknowledgeoftherewardfunctionorthe Remark4. Thepreviousremarkandthefactthatwewererequired
tolimitourselvestotabularrepresentationsimpliesthatmathemat-
policiesofotheragents,andisthereforeprivacy-preserving.Having
icalpropertiesofCPT-sensitiveMDPsdonotallowthemtobelong
apolicygradienttheoremandacorrespondinggradientapprox-
tothefamilyofrobustMDPs[33]andenjoypropertiessuchas
imationscheme,wecannowdevelopourdistributedactor-critic
linearapproximationforthestatevaluefunctionandaconvenient
algorithm.
gradientestimationschemeaswithcoherentriskmeasures[53].
Itwouldbeinterestingtostudyandlookatthislimitationfrom
5 DISTRIBUTEDNESTEDCPTACTOR-CRITIC
acognitiveperspectiveandtoseewhetherdealingwithdynamic
Algorithm(2)laysoutthepseudocodeforDistributedNestedCPT
CPTrisk-sensitivecontinuouscontroliscognitivelycumbersome
Actor-Critic.Ascanbeseen,thecriticâ€™svaluefunctionisestimated
forhumansinbehavioralexperiments.
usingthesamplingstrategyinAlgorithm1,andweusethesam-
plesfromthesimulatorforbootstrapping(byaddingthemtothe
5.2 ConvergenceoftheActor
experiencedictionaryforlateruse).Althoughsamplingfromthe
simulatorforgradientandvaluefunctionapproximationcanbe
Fornotationalsimplicity,wedenoteğ‘‰ ğœ‹ğ‘– ğœƒ(ğ‘ )byğ‘‰ ğ‘–(ğœƒ,ğ‘ ).Weprove
theconvergenceofourACalgorithmtoasubjectiveMPNEofthe
computationallyintensive,itcanbecomelesssoaswebuildthe
gameifthefollowingassumptionsaresatisfied.
experiencedictionaryanddoawaywiththesimulator.Wenow
provetheasymptoticconvergenceoftheproposedalgorithm.
Assumption4. Foreachagentğ‘–,ğ‘‰ ğ‘–(ğœƒ,ğ‘ )isconvexwithrespectto
ğœƒğ‘–.Also,thegradientisuniformlybounded,i.e.,foreachagentğ‘–there
5.1 ConvergenceoftheCritic existsğœ‰ suchthat,
ğ‘–
Inordertocalculatethestatevaluefunctioncorrespondingtothe
currentpolicy,wedefinethefollowingğ‘‡ğ·(0)CPToperator(note
sup(cid:13) (cid:13)âˆ‡ğœƒğ‘–ğ‘‰ ğ‘–(ğœƒ,ğ‘ )(cid:13) (cid:13)â‰¤ğœ‰ ğ‘–. (19)
ğ‘ âˆˆS
thattheagentâ€™ssuperscriptğ‘›hasbeendropped),Algorithm2DistributedNestedCPTActor-Critic
1: Inputs: shared among agents: Initial state ğ‘ 0, number of samples used for CPT estimation (ğ‘› ğ‘šğ‘ğ‘¥), learning rate sequences
( e{ mğ›¼ pğ‘ğ‘Ÿ t, yğ‘¡} eğ‘¡ xâ‰¥ p0 e, r{ iğ›¼ eğ‘ nğ‘ c,ğ‘¡ e} dğ‘¡ iâ‰¥ c0 ti) o, na an rd yt ğ¸r ğ‘¥an ğ‘s ğ·it ğ‘–i ğ‘o ğ‘¡n ğ‘›.probabilities. Local variables for agentğ‘›: Initialğ‘‰ ğœ‹ğ‘› ğœƒ0, initial policy parameters (ğœƒ 0ğ‘›), and an
2: Foreachagentğ‘›,do:
3: Sampleactionğ‘ğ‘› 0 frompolicyğœ‹ ğœƒ 0ğ‘›(.|ğ‘ 0).
4: ğ‘¡ â†0.
5: Repeat
6: Sampleğ‘ğ‘› ğ‘¡ fromğœ‹ ğœƒğ‘›(.|ğ‘  ğ‘¡).
7: Executeğ‘ğ‘› ğ‘¡ andobsğ‘¡ erveğ‘Ÿ ğ‘¡ğ‘›,ğ‘  ğ‘¡+1,andğœ ğ‘¡âˆ’ğ‘›.
8: Push(ğ‘Ÿ ğ‘¡,ğ‘  ğ‘¡+1,ğœ ğ‘¡âˆ’ğ‘›)toğ¸ğ‘¥ğ‘ğ·ğ‘–ğ‘ğ‘¡ğ‘›(ğ‘  ğ‘¡,ğ‘ğ‘› ğ‘¡,ğœ ğ‘¡âˆ’ğ‘›).
9: Criticvalueestimation:
10: Createemptyarrayğ‘‹ ofsizeğ‘› ğ‘šğ‘ğ‘¥.
11: foreachğ‘– =1,2,...,ğ‘› ğ‘šğ‘ğ‘¥,do
12: Sampleğ‘Ë†ğ‘› ğ‘¡ fromğœ‹ ğœƒğ‘›(.|ğ‘  ğ‘¡)andconstructğœË† ğ‘¡âˆ’ğ‘›byobservingneighbors.
13: Sample(ğ‘ŸË† ğ‘¡ğ‘›,ğ‘ Ë† ğ‘¡+1)frğ‘¡ omğ¸ğ‘¥ğ‘ğ·ğ‘–ğ‘ğ‘¡(ğ‘  ğ‘¡,ğ‘Ë†ğ‘› ğ‘¡,ğœË† ğ‘¡âˆ’ğ‘›)ifitislargeenough,andotherwisefromasimulatoroftheenvironment.
11 54 :: IL fe tt hğ‘‹ eğ‘– s= amğ‘ŸË† ğ‘¡ğ‘› pl+ ecğ›¾ ağ‘‰ mğœ‹ğ‘› ğœƒ e( fğ‘ Ë† rğ‘¡ o+ m1). asimulator,push(ğ‘ŸË† ğ‘¡ğ‘›,ğ‘ Ë† ğ‘¡+1)toğ¸ğ‘¥ğ‘ğ·ğ‘–ğ‘ğ‘¡(ğ‘  ğ‘¡,ğ‘Ë†ğ‘› ğ‘¡,ğœË† ğ‘¡âˆ’ğ‘›)forlateruse.
16: endfor
17: Estimateğ‘‰Ë† ğœ‹ğ‘› ğœƒğ‘¡(ğ‘  ğ‘¡)usingAlgorithm1.
18: Criticstep:
19: CalculatetheTD-error:
ğ›¿
ğ‘¡
:=ğ‘‰Ë† ğœ‹ğ‘› ğœƒğ‘¡(ğ‘  ğ‘¡)âˆ’ğ‘‰ ğœ‹ğ‘› ğœƒğ‘¡(ğ‘  ğ‘¡).
ğ‘‰ ğœ‹ğ‘› ğœƒğ‘¡(ğ‘  ğ‘¡)â†ğ‘‰ ğœ‹ğ‘› ğœƒğ‘¡(ğ‘  ğ‘¡)+ğ›¼ ğ‘ğ‘Ÿ,ğ‘¡ğ›¿ ğ‘¡.
20: Actorstep:
21: Computeâˆ‡ğ‘‰ ğœ‹ğ‘› ğœƒğ‘¡(ğ‘ 0)usingthegradientestimationschemedescribedinSection4.
ğœƒ ğ‘¡ğ‘› +1:=ğœƒ ğ‘¡ğ‘› +ğ›¼ ğ‘ğ‘,ğ‘¡âˆ‡ğ‘‰ ğœ‹ğ‘› ğœƒğ‘¡(ğ‘ 0).
22: ğ‘¡ â†ğ‘¡+1.
23: Untilconvergence
Assumption5. Thepseudo-gradientmappingvaluefunction,âˆ‡ğœƒğ‘‰(ğœƒ,ğ‘ )= UnderAssumptions4and5,basedonTheorem2inRosen[44],
col(cid:0)âˆ‡ğœƒğ‘–ğ‘‰1(ğœƒ,ğ‘ ),...,âˆ‡ğœƒğ‘ğ‘‰ ğ‘(ğœƒ,ğ‘ )(cid:1),isstronglymonotonewithrespect thereexistsauniqueMPNEfortheNAMG.NotethatthisCPT-
toğœƒ,i.e.,foreveryğœƒ,ğœƒâ€² âˆˆÎ˜,ğ‘  âˆˆS,thereexistsğœ‡ >0suchthat sensitive(subjective)MPNEcanbedifferentfromtheMPNEofthe
gamewhentheagentsarerisk-neutral.Considertheparameter
(cid:0)âˆ‡ğœƒğ‘‰(ğœƒ,ğ‘ )âˆ’âˆ‡ğœƒğ‘‰ (cid:0)ğœƒâ€²,ğ‘ (cid:1)(cid:1)âŠ¤(cid:0)ğœƒâˆ’ğœƒâ€²(cid:1) â‰¥ğœ‡(cid:13) (cid:13)ğœƒâˆ’ğœƒâ€²(cid:13) (cid:13)2 .
vectorğœƒâˆ—asthevectorthatconstructsthisuniqueNashpolicy,for
Furthermore,thismappingisLipschitzcontinuous,i.e., whichwehaveâˆ‡ğœƒğ‘–ğ½ ğ‘–(ğœƒâˆ—) = 0forallğ‘–.PerAssumption4andby
(cid:13) (cid:13)âˆ‡ğœƒğ‘‰(ğœƒ,ğ‘ )âˆ’âˆ‡ğœƒğ‘‰ (cid:0)ğœƒâ€²,ğ‘ (cid:1)(cid:13) (cid:13)â‰¤ğ¿ ğ‘(cid:13) (cid:13)ğœƒâˆ’ğœƒâ€²(cid:13) (cid:13). definingÎ”ğœƒ ğ‘›ğ‘¡ =ğœƒ ğ‘›ğ‘¡ âˆ’ğœƒ ğ‘›âˆ—,wehave
Theorem2. (Convergenceoftheactor)GivenAssumptions4and
5andacriticandanactorwithlearningstepssuchthat, (cid:13) (cid:13)Î”ğœƒ ğ‘›ğ‘¡+1(cid:13) (cid:13)2 =(cid:13) (cid:13)Î”ğœƒ ğ‘›ğ‘¡ âˆ’ğ›¼ ğ‘ğ‘,ğ‘¡âˆ‡ğœƒğ‘›ğ‘‰ ğ‘›(cid:0)ğœƒğ‘¡(cid:1)(cid:13) (cid:13)2
âˆ‘ï¸ ğ‘¡âˆ =0ğ›¼ ğ‘ğ‘,ğ‘¡ =âˆ, âˆ‘ï¸ ğ‘¡âˆ =0ğ›¼ ğ‘ğ‘Ÿ,ğ‘¡ =âˆ, âˆ‘ï¸ ğ‘¡âˆ =0ğ›¼ ğ‘2 ğ‘Ÿ,ğ‘¡ <âˆ, âˆ‘ï¸ ğ‘¡âˆ =0ğ›¼ ğ‘2 ğ‘,ğ‘¡ <âˆ (2,
0)
ğ‘¡l â†’im âˆğ›¼ ğ›¼ğ‘ ğ‘ğ‘Ÿğ‘ ,, ğ‘¡ğ‘¡ =0, âˆ’= 2(cid:13) (cid:13) ğ›¼Î” ğ‘ğœƒ ğ‘ğ‘›ğ‘¡ ,ğ‘¡(cid:13) (cid:13) âˆ‡2 ğœƒ+ ğ‘›(cid:0) (cid:0)ğ›¼ ğ‘‰ğ‘ ğ‘›ğ‘, (cid:0)ğ‘¡ ğœƒ(cid:1) ğ‘¡2 (cid:1)(cid:13) (cid:13) (cid:1)âˆ‡ âŠ¤ğœƒ Î”ğ‘› ğœƒğ‘‰ ğ‘›ğ‘¡ğ‘›(cid:0)ğœƒğ‘¡(cid:1)(cid:13) (cid:13)2 (21)
algorithm(2)convergestotheuniquesubjectiveMarkovperfect â‰¤(cid:13) (cid:13)Î”ğœƒ ğ‘›ğ‘¡(cid:13) (cid:13)2 +(cid:0)ğ›¼ ğ‘ğ‘,ğ‘¡(cid:1)2ğœ‰ ğ‘›2 âˆ’2ğ›¼ ğ‘ğ‘,ğ‘¡âˆ‡ğœƒğ‘›ğ‘‰ ğ‘›(cid:0)ğœƒğ‘¡(cid:1)âŠ¤Î”ğœƒ ğ‘›ğ‘¡.
NashequilibriumoftheNAMG,asymptotically.
Proof. We prove that the actor update will converge to the Bysummingtheaboveequationoverdifferentğ‘›âˆˆ{1,...,ğ‘}and
uniqueNashpolicyoftheMarkovgame,whichexistsifAssump- definingÎ”ğœƒğ‘¡ =col(cid:16) Î”ğœƒğ‘¡...,Î”ğœƒğ‘¡ (cid:17) ,wehave
1 ğ‘
tions4and5aresatisfied,startingfromanyinitialcondition.We
rewritetheactorupdateforagentğ‘›below,
ğœƒ ğ‘¡ğ‘› +1:=ğœƒ ğ‘¡ğ‘› +ğ›¼ ğ‘ğ‘,ğ‘¡âˆ‡ğœƒğ‘‰ğ‘› (ğœƒ,ğ‘ 0).
(cid:13) (cid:13)Î”ğœƒğ‘¡+1(cid:13) (cid:13)2 â‰¤(cid:13) (cid:13)Î”ğœƒğ‘¡(cid:13) (cid:13)2 + ğ‘›âˆ‘ï¸ âˆˆNğœ‰ ğ‘›2 (cid:0)ğ›¼ ğ‘ğ‘,ğ‘¡(cid:1)2 âˆ’2ğ›¼ ğ‘ğ‘,ğ‘¡âˆ‡ğœƒğ‘‰ğ‘¡âŠ¤Î”ğœƒğ‘¡, (22)whereâˆ‡ğœƒğ‘‰ğ‘¡ =col(cid:0)âˆ‡ğœƒ1ğ‘‰1(cid:0)ğœƒğ‘¡(cid:1),...,âˆ‡ğœƒğ‘ğ‘‰
ğ‘
(cid:0)ğœƒğ‘¡(cid:1)(cid:1).Wealsoknow
thatâˆ‡ğœƒğ‘‰âˆ— =col(cid:0)âˆ‡ğœƒ1ğ‘‰1(ğœƒâˆ—),...,âˆ‡ğœƒğ‘ğ‘‰ ğ‘ (ğœƒâˆ—)(cid:1) =0.Therefore,ac- ğ‘… ğ‘ ğ‘’ğ‘™ğ‘“(ğ‘ ,ğ‘ğ‘– )âˆ¼ğ‘ğ‘œğ‘Ÿğ‘šğ‘ğ‘™(0.5,0.1),âˆ€ğ‘– âˆˆ1,...,ğ‘. (27)
cordingtoassumption5,
Also, the reward coefficients ğ‘… ğ‘ğ‘– ğ‘œğ‘š(ğ‘ ) is randomly generated
âˆ’âˆ‡ğœƒğ‘‰ğ‘¡âŠ¤Î”ğœƒğ‘¡ =âˆ’(cid:0)âˆ‡ğœƒğ½ğ‘¡ âˆ’âˆ‡ğœƒğ‘‰âˆ—(cid:1)âŠ¤Î”ğœƒğ‘¡ form
=âˆ’(cid:0)âˆ‡ğœƒğ‘‰ğ‘¡ (cid:0)ğœƒğ‘¡(cid:1)âˆ’âˆ‡ğœƒğ‘‰ğ‘¡ (cid:0)ğœƒâˆ—(cid:1)(cid:1)âŠ¤Î”ğœƒğ‘¡ (23)
ğ‘… ğ‘ğ‘– ğ‘œğ‘š(ğ‘ )âˆ¼5Â·ğ‘¢ğ‘›ğ‘–ğ‘“ğ‘œğ‘Ÿğ‘š[âˆ’0.5,0.5]. (28)
â‰¤âˆ’ğœ‡(cid:13) (cid:13)Î”ğœƒğ‘¡(cid:13) (cid:13)2 .
Theabovesetupimpliesahighriskfortheagentifitdecidesto
Finally,withtelescopicsummation, takeanactiongreaterthanğ‘ğ‘– = 0andbecomeinvolvedwithits
neighboringcommunity,e.g.,takeafinancialriskinaninteractive
âˆ
lim (cid:13) (cid:13)Î”ğœƒğ‘¡(cid:13) (cid:13)2 â‰¤(cid:13) (cid:13)Î”ğœƒ0(cid:13) (cid:13)2 + âˆ‘ï¸ ğœ‰ ğ‘›2âˆ‘ï¸(cid:0)ğ›¼ ğ‘ğ‘,ğ‘¡(cid:1)2 market.Thus,itcanbesaidthateachagentinthisnon-cooperative
ğ‘¡â†’âˆ environmentchoosesbyitsactionhowmuchriskitwantstotake
ğ‘›âˆˆN ğ‘¡=0
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) andtowhatdegreeitwantstointeractwiththecommunity(other
(cid:124) (cid:123)(cid:122) (cid:125)
ğ‘‡1
(24)
agentswhocould,forinstance,beeconomic,political,orsocial
âˆ competitors),andtiesitsreceivedrewardtotheiractions.Ifthe
âˆ’âˆ‘ï¸ ğœ‡ğ›¼ ğ‘ğ‘,ğ‘¡(cid:13) (cid:13)Î”ğœƒğ‘¡(cid:13) (cid:13)2 . agentchoosestheactionğ‘ğ‘– = 0,itwillsettleforalow,butrisk-
ğ‘¡= (cid:32)(cid:32)(cid:32)0 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) freeprofit.However,whenchoosinganotheraction,dependingon
(cid:124) (cid:123)(cid:122) (cid:125) theactionsoftheotheragents,itcanmakeasignificantprofitor
ğ‘‡2
lossthatisalsoaffectedbystochasticityoftheenvironment.Our
Sinceğœ‡ >0,ğ‘‡2 â‰¥0.Therefore,asğ‘‡1isbounded,ğ‘‡2isbounded
a ass ğ‘¡w â†’ell. âˆCo an ns de aq su aen rt el sy u, la ts ğœƒ(cid:205) coğ‘¡âˆ = n0 vğ›¼ erğ‘ gğ‘, eğ‘¡ s= toâˆ ğœƒ, âˆ—.âˆ¥Î”ğœƒğ‘¡âˆ¥2convergesto â–¡0 o pb arje ac mti ev te eris ğœ†t io ns tt hu edy cut mhe ua lag te ivn ets pâ€™ er ris sk p- ea cv tie vr esi to hn eole rv ye uls tib lia ts ye fd uo nn ctit oh ne
(Figure2).Thehigherthevalueofthisparameter,themoreloss-
Giventheasymptoticproofofconvergencefortheactorandthe aversetheagentis.Weexpectthatinthedesignednon-cooperative
criticandconsideringtheconditionsofthelearningstepsequences environment,amoreloss-averseagentwillchoosetheactionğ‘=0
in Theorem 2, we can apply Theorem 1.1 of Borkar [5], which withahigherprobabilityandhaveatendencytobecomesocially
shows the asymptotic convergence of the AC algorithm to the isolatedorconservative.Toevaluatethishypothesis,werunthe
uniqueMPNEoftheNAMG.NotethatifAssumptions4and5do proposedalgorithmforthisrisk-sensitiveNAMGandforfourdif-
nothold,giventheactorâ€™sgradientexpression,wecanonlyensure ferentloss-aversionscenarios.Inthefirstscenario,allagentsare
theconvergenceoftheACalgorithmtoalocallyoptimalpolicy risk-neutral(correspondingtoavanillaACalgorithmwithlinear
parameterforeachagent. functionapproximation).Inthesecondscenario,allagentshave
thesameleveloflossaversion(ğœ†=2.6).Inthethirdscenario,only
6 NUMERICALEXPERIMENT Agent1isrisk-sensitive(ğœ†=2.6),andotheragentsarerisk-neutral.
ToexaminetheempiricalconvergenceofDistributedNestedCPT Finally,inthelastscenario,allagentsarerisk-sensitive,butAgent
AC,weconstructedarisk-sensitiveNAMGwithaninterpretable onehasahigherlossaversioncoefficient(ğœ† = 3.2),whileothers
designformeasuringtheeffectoflossaversioninrisk-sensitive haveğœ†=2.6.Figure4showstheconvergenceofthevaluefunctions
agentsontheconvergedpolicies.NotethatduetotheCPToperator, correspondingtooneofthestatesinthesecondscenario.Figure5
Assumptions4and5arehardtoverifyinanyexperimentalsetup showstheprobabilityofchoosingaction0(aquantitativeindicator
andwedidnotexpectthealgorithmtoconvergetothesubjective ofsocialconservatism)intheconvergedpoliciesofagentsineach
MPNE(whichmaynotbeuniqueiftheaforementionedassumptions scenario.Asobserved,thelevelofsocialisolationandtheprobabil-
arenotsatisfied).IntheconstructedNAMG,therearefouragents, ityofchoosingtheconservativeactionğ‘=0isproportionaltothe
fivestates,andthreepossibleactions(N =4,S={0,1,2,3,4},A= risk-aversionleveloftheagentsinthecommunity.
{0,1,2}),andthecommunicationgraphisfully-connected.The
rewardfunctionisdefinedas 7 CONCLUSIONANDFUTUREWORKS
Inthiswork,weproposedadistributedrisk-sensitiveMARLalgo-
ğ‘…ğ‘– (ğ‘ ,ğ‘ğ‘–,ğœğ‘– (ğ‘âˆ’ğ‘– ))=ğ‘… ğ‘ ğ‘– ğ‘’ğ‘™ğ‘“(ğ‘ )+ğœğ‘– (ğ‘âˆ’ğ‘– )ğ‘… ğ‘ğ‘– ğ‘œğ‘š(ğ‘ )ğ‘ğ‘–, (25) rithminNAMGswiththeoreticalconvergenceguaranteesbasedon
cumulativeprospecttheory,acognitiveriskmeasurethatbroadens
wherethefirsttermistherewardsolelyaffectedbytheagentâ€™s
thescopeofthetraditionallyadoptedcoherentriskmeasure.We
action,andthesecondtermistherewardthatisaffectedbythe
empiricallyshowedthepositivecorrelationbetweenlossaversion
actionsoftheneighboringğ‘ğ‘œğ‘šğ‘šğ‘¢ğ‘›ğ‘–ğ‘¡ğ‘¦oftheagent.Theaggregative
andsocialisolationofagents.Weobservedthatscalingthepro-
termis
posedalgorithmtolargerenvironmentsandcontinuouscontrol
ğœğ‘– (ğ‘âˆ’ğ‘– )= ğ‘1 âˆ’1( âˆ‘ï¸ ğ‘ ğ‘—), (26) i es ven ro ,t ac po lm aup sa ibti lb el de iw rei ct th iot nhe oo fr fe ut ti uca relc wo on rv ke ir sg te hn ece apg pu rr oa pn rt ie ae tes. uH seow of-
ğ‘—âˆˆN\ğ‘– functionapproximationanddeepRLmethodstotacklethecurse
which indicates a communication graph with equal weights ofdimensionalityforlargestateandactionspacesinCPT-sensitive
amongtheneighbors.Therewardcoefficientğ‘…ğ‘– (ğ‘ )foragentğ‘–is RL,andingeneralrisk-averseRL[61],inanempiricalframework,
ğ‘ ğ‘’ğ‘™ğ‘“
randomlygeneratedfrom albeitwithouttheoreticalconvergencegurantees.[10] YinlamChow,MohammadGhavamzadeh,LucasJanson,andMarcoPavone.2017.
Risk-constrainedreinforcementlearningwithpercentileriskcriteria.TheJournal
6 Agent #1 ofMachineLearningResearch18,1(2017),6070â€“6120.
Agent #2 [11] FreddyDelbaen.2002. Coherentriskmeasuresongeneralprobabilityspaces.
5 A Ag ge en nt t # #3 4 Advancesinfinanceandstochastics:essaysinhonourofDieterSondermann(2002),
1â€“37.
[12] ZhenhuaDeng.2019.Distributedalgorithmdesignforresourceallocationprob-
4 lemsofsecond-ordermultiagentsystemsoverweight-balanceddigraphs.IEEE
TransactionsonSystems,Man,andCybernetics:Systems51,6(2019),3512â€“3521.
3 [13] ZhenhuaDeng.2021. Distributedalgorithmdesignforaggregativegamesof
Eulerâ€“Lagrangesystemsanditsapplicationtosmartgrids.IEEETransactionson
Cybernetics52,8(2021),8315â€“8325.
2
[14] DongshengDing,Chen-YuWei,KaiqingZhang,andMihailoJovanovic.2022.
Independentpolicygradientforlarge-scalemarkovpotentialgames:Sharper
1 rates,functionapproximation,andgame-agnosticconvergence.InInternational
ConferenceonMachineLearning.PMLR,5166â€“5220.
0 1000 2000 3000 4000 5000
Timestep [15] YingjieFei,ZhuoranYang,YudongChen,andZhaoranWang.2021.Exponential
bellmanequationandimprovedregretboundsforrisk-sensitivereinforcement
learning.AdvancesinNeuralInformationProcessingSystems34(2021),20436â€“
Figure4:Smoothedmeanvaluefunctionofagivenstateover 20446.
[16] RoyFox,StephenMMcaleer,WillOverman,andIoannisPanageas.2022.Inde-
eightindependentrunsinDistributedNestedCPT-ACfor pendentnaturalpolicygradientalwaysconvergesinMarkovpotentialgames.In
scenario2(allagentsarerisk-sensitivewithğœ†=2.6). InternationalConferenceonArtificialIntelligenceandStatistics.PMLR,4414â€“4425.
[17] MrinalKGhosh,SubrataGolui,ChandanPal,andSomnathPradhan.2022.
Nonzero-sumrisk-sensitivecontinuous-timestochasticgameswithergodiccosts.
AppliedMathematics&Optimization86,1(2022),6.
[18] MrinalKGhosh,SubrataGolui,ChandanPal,andSomnathPradhan.2023.
Discrete-timezero-sumgamesforMarkovchainswithrisk-sensitiveaverage
0.5 A Ag ge en nt t # #1 2 costcriterion.StochasticProcessesandtheirApplications158(2023),40â€“74.
Agent #3 [19] ChengJie,LAPrashanth,MichaelFu,SteveMarcus,andCsabaSzepesvÃ¡ri.2018.
Agent #4 Stochasticoptimizationinacumulativeprospecttheoryframework.IEEETrans.
0.4 Automat.Control63,9(2018),2867â€“2882.
[20] DANIELKahnemanandAmosTversky.1979.Prospecttheory:Ananalysisof
0.3 decisionunderrisk.Econometrica47,2(1979),363â€“391.
[21] PrashanthLaandMohammadGhavamzadeh.2013.Actor-criticalgorithmsfor
risk-sensitiveMDPs.Advancesinneuralinformationprocessingsystems26(2013).
0.2 [22] StefanosLeonardos,WillOverman,IoannisPanageas,andGeorgiosPiliouras.
2021. Globalconvergenceofmulti-agentpolicygradientinmarkovpotential
0.1 games.arXivpreprintarXiv:2106.01969(2021).
[23] KunLin.2013.Stochasticsystemswithcumulativeprospecttheory.Ph.D.Disserta-
tion.UniversityofMaryland,CollegePark.
0.0 Scenario 1 Scenario 2 Scenario 3 Scenario 4 [24] KunLin,ChengJie,andStevenIMarcus.2018.Probabilisticallydistortedrisk-
sensitiveinfinite-horizondynamicprogramming.Automatica97(2018),1â€“6.
[25] KunLinandStevenIMarcus.2013.Dynamicprogrammingwithnon-convex
risk-sensitivemeasures.In2013AmericanControlConference.IEEE,6778â€“6783.
Figure5:Meanconvergedpoliciesovereightindependent [26] MichaelLLittman.1994.Markovgamesasaframeworkformulti-agentrein-
runs for different loss aversion scenarios. Scenario 1: all forcementlearning.InMachinelearningproceedings1994.Elsevier,157â€“163.
[27] ChinmayMaheshwari,ManxiWu,DruvPai,andShankarSastry.2022. Inde-
agents are risk-neutral, scenario 2: all agents are risk-
pendentanddecentralizedlearninginmarkovpotentialgames.arXivpreprint
sensitive(ğœ†=2.6),scenario3:onlyAgent1isrisk-sensitive arXiv:2205.14590(2022).
(ğœ†=2.6),scenario4:Agent1hasahigherlossaversioncoeffi- [28] JaredMarkowitz,MarieChau,andI-JengWang.2021.DeepCPT-RL:Imparting
Human-LikeRiskSensitivitytoArtificialAgents..InSafeAI@AAAI.
cient(ğœ†=3.2)thanothers(ğœ†=2.6).
[29] DavidHMguni,YutongWu,YaliDu,YaodongYang,ZiyiWang,MinneLi,Ying
Wen,JoelJennings,andJunWang.2021.Learninginnonzero-sumstochastic
gameswithpotentials.InInternationalConferenceonMachineLearning.PMLR,
7688â€“7699.
REFERENCES
[30] MehrdadMoharrami,YashaswiniMurthy,ArghyadipRoy,andRayadurgam
[1] CarloAcerbi.2002. Spectralmeasuresofrisk:Acoherentrepresentationof Srikant.2022.APolicyGradientAlgorithmfortheRisk-SensitiveExponential
subjectiveriskaversion.JournalofBanking&Finance26,7(2002),1505â€“1518. CostMDP.arXivpreprintarXiv:2202.04157(2022).
[2] AhmetAlacaoglu,LucaViano,NiaoHe,andVolkanCevher.2022. Anatural [31] MdShirajumMunir,SarderFakhrulAbedin,NguyenHTran,ZhuHan,Eui-Nam
actor-criticframeworkforzero-sumMarkovgames.InInternationalConference Huh,andChoongSeonHong.2021. Risk-awareenergyschedulingforedge
onMachineLearning.PMLR,307â€“366. computingwithmicrogrid:Amulti-agentdeepreinforcementlearningapproach.
[3] PhilippeArtzner,FreddyDelbaen,Jean-MarcEber,andDavidHeath.1999.Co- IEEETransactionsonNetworkandServiceManagement18,3(2021),3476â€“3497.
herentmeasuresofrisk.Mathematicalfinance9,3(1999),203â€“228. [32] ErfaunNooraniandJohnSBaras.2022.Risk-attitudes,Trust,andEmergence
[4] TamerBaÅŸar.2021.Robustdesignsthroughrisksensitivity:Anoverview.Journal ofCoordinationinMulti-agentReinforcementLearningSystems:AStudyof
ofSystemsScienceandComplexity34(2021),1634â€“1665. IndependentRisk-sensitiveREINFORCE.In2022EuropeanControlConference
[5] VivekSBorkar.1997.Stochasticapproximationwithtwotimescales.Systems& (ECC).IEEE,2266â€“2271.
ControlLetters29,5(1997),291â€“294. [33] TakayukiOsogami.2012. Robustnessandrisk-sensitivityinMarkovdecision
[6] VivekSBorkar.2001.Asensitivityformulaforrisk-sensitivecostandtheactorâ€“ processes.Advancesinneuralinformationprocessingsystems25(2012).
criticalgorithm.Systems&ControlLetters44,5(2001),339â€“346. [34] ChandanPalandSomnathPradhan.2021. Zero-sumgamesforpurejump
[7] OzlemCavusandAndrzejRuszczynski.2014.Risk-aversecontrolofundiscounted processeswithrisk-sensitivediscountedcostcriteria.JournalofDynamicsand
transientMarkovmodels.SIAMJournalonControlandOptimization52,6(2014), Games9,1(2021),13â€“25.
3935â€“3966. [35] FrancescaParise,SergioGrammatico,BasilioGentile,andJohnLygeros.2020.
[8] CarloCenedese,GiuseppeBelgioioso,SergioGrammatico,andMingCao.2020. DistributedconvergencetoNashequilibriainnetworkandaverageaggregative
Time-varyingconstrainedproximaltypedynamicsinmulti-agentnetworkgames. games.Automatica117(2020),108959.
In2020EuropeanControlConference(ECC).IEEE,148â€“153. [36] JulienPerolat,BrunoScherrer,BilalPiot,andOlivierPietquin.2015.Approximate
[9] YinlamChowandMohammadGhavamzadeh.2014.AlgorithmsforCVaRopti- dynamicprogrammingfortwo-playerzero-sumMarkovgames.InInternational
mizationinMDPs.Advancesinneuralinformationprocessingsystems27(2014). ConferenceonMachineLearning.PMLR,1321â€“1329.
ytilibaborP
)0s(V[37] LAPrashanth,MichaelCFu,etal.2022.Risk-SensitiveReinforcementLearning [63] KaiqingZhang,ShamKakade,TamerBasar,andLinYang.2020.Model-based
viaPolicyGradientSearch.FoundationsandTrendsÂ®inMachineLearning15,5 multi-agentrlinzero-summarkovgameswithnear-optimalsamplecomplexity.
(2022),537â€“693. AdvancesinNeuralInformationProcessingSystems33(2020),1166â€“1178.
[38] LAPrashanthandMohammadGhavamzadeh.2016.Variance-constrainedactor- [64] KaiqingZhang,XiangyuanZhang,BinHu,andTamerBasar.2021.Derivative-free
criticalgorithmsfordiscountedandaveragerewardMDPs.MachineLearning policyoptimizationforlinearrisk-sensitiveandrobustcontroldesign:Implicit
105(2016),367â€“417. regularizationandsamplecomplexity.AdvancesinNeuralInformationProcessing
[39] LAPrashanth,ChengJie,MichaelFu,SteveMarcus,andCsabaSzepesvÃ¡ri.2016. Systems34(2021),2949â€“2964.
Cumulativeprospecttheorymeetsreinforcementlearning:Predictionandcontrol. [65] HaiZhong,YutakaShimizu,andJianyuChen.2022.Chance-ConstrainedIterative
InInternationalConferenceonMachineLearning.PMLR,1406â€“1415. Linear-QuadraticStochasticGames.IEEERoboticsandAutomationLetters8,1
[40] DrazenPrelec.1998.Theprobabilityweightingfunction.Econometrica(1998), (2022),440â€“447.
497â€“527. [66] KunZhu,EkramHossain,andDusitNiyato.2013. Pricing,spectrumsharing,
[41] ShuangQiu,XiaohanWei,JiepingYe,ZhaoranWang,andZhuoranYang.2021. andserviceselectionintwo-tiersmallcellnetworks:Ahierarchicaldynamic
Provablyefficientfictitiousplaypolicyoptimizationforzero-sumMarkovgames gameapproach.IEEETransactionsonMobileComputing13,8(2013),1843â€“1856.
withstructuredtransitions.InInternationalConferenceonMachineLearning. [67] ZiqingZhu,KaWingChan,SiqiBu,BinZhou,andShiweiXia.2022. Nash
PMLR,8715â€“8725. EquilibriumEstimationandAnalysisinJointPeer-to-PeerElectricityandCarbon
[42] WeiQiu,XinrunWang,RunshengYu,RundongWang,XuHe,BoAn,Svetlana EmissionAuctionMarketWithMicrogridProsumers.IEEETransactionsonPower
Obraztsova,andZinoviRabinovich.2021.RMIX:Learningrisk-sensitivepolicies Systems(2022).
forcooperativereinforcementlearningagents.AdvancesinNeuralInformation
ProcessingSystems34(2021),23049â€“23062.
[43] DSaiKotiReddy,AmritaSaha,SrikanthGTamilselvam,PriyankaAgrawal,and
PankajDayama.2019.Riskaversereinforcementlearningformixedmulti-agent
environments.InProceedingsofthe18thinternationalconferenceonautonomous
agentsandmultiagentsystems.2171â€“2173.
[44] JBenRosen.1965.Existenceanduniquenessofequilibriumpointsforconcave
n-persongames.Econometrica:JournaloftheEconometricSociety(1965),520â€“534.
[45] AndrzejRuszczyÅ„ski.2010. Risk-aversedynamicprogrammingforMarkov
decisionprocesses.Mathematicalprogramming125(2010),235â€“261.
[46] MohsenSaffar,HamedKebriaei,andDusitNiyato.2017.Pricingandrateopti-
mizationofcloudradioaccessnetworkusingrobusthierarchicaldynamicgame.
IEEETransactionsonWirelessCommunications16,11(2017),7404â€“7418.
[47] MuhammedSayin,KaiqingZhang,DavidLeslie,TamerBasar,andAsuman
Ozdaglar.2021.DecentralizedQ-learninginzero-sumMarkovgames.Advances
inNeuralInformationProcessingSystems34(2021),18320â€“18334.
[48] AlexanderShapiro,DarinkaDentcheva,andAndrzejRuszczynski.2021.Lectures
onstochasticprogramming:modelingandtheory.SIAM.
[49] LloydSShapley.1953.Stochasticgames.Proceedingsofthenationalacademyof
sciences39,10(1953),1095â€“1100.
[50] MohammadShokriandHamedKebriaei.2020. Leaderâ€“followernetworkag-
gregativegamewithstochasticagentsâ€™communicationandactiveness. IEEE
Trans.Automat.Control65,12(2020),5496â€“5502.
[51] MehdiNaderiSoorki,WalidSaad,MehdiBennis,andChoongSeonHong.2021.
Ultra-reliableindoormillimeterwavecommunicationsusingmultipleartificial
intelligence-poweredintelligentsurfaces.IEEETransactionsonCommunications
69,11(2021),7444â€“7457.
[52] RichardSSuttonandAndrewGBarto.2018.Reinforcementlearning:Anintro-
duction.MITpress.
[53] AvivTamar,YinlamChow,MohammadGhavamzadeh,andShieMannor.2015.
Policygradientforcoherentriskmeasures. Advancesinneuralinformation
processingsystems28(2015).
[54] AvivTamar,DotanDiCastro,andShieMannor.2012. Policygradientswith
variancerelatedriskcriteria.InProceedingsofthetwenty-ninthinternational
conferenceonmachinelearning.387â€“396.
[55] AvivTamar,DotanDiCastro,andShieMannor.2013. Temporaldifference
methodsforthevarianceoftherewardtogo.InInternationalConferenceon
MachineLearning.PMLR,495â€“503.
[56] AvivTamar,ShieMannor,andHuanXu.2014.ScalinguprobustMDPsusing
functionapproximation.InInternationalconferenceonmachinelearning.PMLR,
181â€“189.
[57] ShaolinTan,YaonanWang,andAthanasiosVVasilakos.2021. Distributed
populationdynamicsforsearchinggeneralizednashequilibriaofpopulation
gameswithgraphicalstrategyinteractions.IEEETransactionsonSystems,Man,
andCybernetics:Systems52,5(2021),3263â€“3272.
[58] RanTian,LitingSun,andMasayoshiTomizuka.2021.Boundedrisk-sensitive
markovgames:Forwardpolicydesignandinverserewardlearningwithiterative
reasoningandcumulativeprospecttheory.InProceedingsoftheAAAIConference
onArtificialIntelligence,Vol.35.6011â€“6020.
[59] JohnTsitsiklisandBenjaminVanRoy.1997.Ananalysisoftemporal-difference
learningwithfunctionapproximation.IEEETrans.Automat.Control42,5(1997),
674â€“690.
[60] AmosTverskyandDanielKahneman.1992.Advancesinprospecttheory:Cu-
mulativerepresentationofuncertainty.JournalofRiskanduncertainty5(1992),
297â€“323.
[61] NÃºriaArmengolUrpÃ­,SebastianCuri,andAndreasKrause.2021.Risk-averse
offlinereinforcementlearning.arXivpreprintarXiv:2102.05371(2021).
[62] JamesRWrightandKevinLeyton-Brown.2017. Predictinghumanbehavior
inunrepeated,simultaneous-movegames.GamesandEconomicBehavior106
(2017),16â€“37.