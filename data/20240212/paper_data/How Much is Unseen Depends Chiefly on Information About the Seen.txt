How Much is Unseen Depends Chiefly on Information About the Seen
SeongminLee1 MarcelBoÂ¨hme1
Abstract 1.1.Background
Itmightseemcounter-intuitiveatfirst: Wefind Consideramultinomialdistributionp=âŸ¨p 1,Â·Â·Â·p SâŸ©overa
that,inexpectation,theproportionofdatapoints supportsetX wheresupportsizeS =|X|andprobability
inanunknownpopulationâ€”thatbelongtoclasses values are unknown. Let Xn = âŸ¨X 1,Â·Â·Â·X nâŸ© be a set of
thatdonotappearinthetrainingdataâ€”isalmost independent and identically distributed random variables
entirelydeterminedbythenumberf ofclasses representingthesequenceofelementsobservedinnsam-
k
thatdoappearinthetrainingdatathesamenum- plesfromp. LetN xbethenumberoftimeselementxâˆˆX
ber of times. While in theory we show that the isobservedinthesampleXn. Fork :0â‰¤k â‰¤n,letÎ¦ k be
differenceoftheinducedestimatordecaysexpo-
thenumberofelementsappearingexactlyktimesinXn.
nentially in the size of the sample, in practice
n
(cid:88) (cid:88)
the high variance prevents us from using it di- N = 1(X =x) and Î¦ = 1(N =k)
x i k x
rectly for an estimator of the sample coverage.
i=1 xâˆˆX
However,ourprecisecharacterizationofthede-
Letf (n)betheexpectedvalueofÎ¦ (Good,1953),i.e.,
pendency between f â€™s induces a large search k k
k
spaceofdifferentrepresentationsoftheexpected (cid:18) n(cid:19)
(cid:88)
value,whichcanbedeterministicallyinstantiated f k(n)=
k
pk x(1âˆ’p x)nâˆ’k =E[Î¦ k]
asestimators. Hence,weturntooptimizationand xâˆˆX
developageneticalgorithmthat,givenonlythe
Estimatingthemultinomial.Suppose,wewanttoestimate
sample,searchesforanestimatorwithminimal
p. WecannotexpectallelementstoexistinXn. Whilethe
mean-squarederror(MSE).Inourexperiments,
empirical estimator pË†Emp = N /n is generally unbiased,
our genetic algorithm discovers estimators that x x
haveasubstantiallysmallerMSEthanthestate-of- pË†E xmp distributes the entire probability mass only over the
the-artGood-Turingestimator.Thisholdsforover observedelements.Thisleavesaâ€œmissingprobabilitymassâ€
96%ofrunswhenthereareatleastasmanysam- overtheunobservedelements. Inparticular,pË†E xmpgiventhat
plesasclasses. Ourestimatorsâ€™MSEisroughly N x >0overestimatesp x,i.e.,forobservedelements
80%oftheGood-Turingestimatorâ€™s. (cid:20) (cid:12) (cid:21)
E N nx (cid:12) (cid:12) (cid:12) N x >0 = 1âˆ’(1p âˆ’x p )n.
x
1.Introduction Wenoticethatthebiasincreasesasp x decreases. Biasis
maximizedfortherarestobservedelement.
Suppose, wearedrawingballswithreplacementfroman
Missingmass,realizability,andnaturalestimation. Good
urnwithcoloredballs. Whatistheproportionofballsin
andTuring(GT)(Good,1953)discoveredthattheexpected
that urn that have a color not observed in the sample; or
value of the probability M = (cid:80) p 1(N = k) that
equivalently,whatistheprobabilitythatthenextballhas k xâˆˆX x x
the(n+1)-thobservationX isanelementthathasbeen
apreviouslyunobservedcolor? Whatisthedistributionof n+1
observedexactlyktimesinXn(incl. k =0)isafunction
rarelyobservedcolorsinthaturn?Thesequestionsrepresent
oftheexpectednumberofcolorsf (n+1)thatwillbe
afundamentalprobleminmachinelearning: Howcanwe k+1
observedk+1timesinanenlargedsampleXnâˆªX .
extrapolatefrompropertiesofthetrainingdatatoproperties n+1
oftheunseen,underlyingdistributionofthedata? k+1
E[M ]= f (n+1). (1)
k n+1 k+1
1Max-PlanckInstituteforSecurityandPrivacy(MPI-SP),Ger-
WealsocallM astotalprobabilitymassovertheelements
many.Correspondenceto:SeongminLee<seongmin.lee@mpi- k
thathavebeenobservedexactlyktimes. Sinceoursample
sp.org>,MarcelBoÂ¨hme<marcel.boehme@mpi-sp.org>.
Xnisonlyofsizen,GTsuggestedtoestimatef (n+1)
k+1
usingÎ¦ . Concretely,MË†G = k+1Î¦ .
k+1 k n k+1
1
4202
beF
8
]GL.sc[
1v53850.2042:viXraHowMuchisUnseenDependsChieflyonInformationAbouttheSeen
Fork =0,M givestheâ€œmissingâ€(probability)massover CompetitivenessofGT.UsingthePoissonapproximation,
0
theelementsnotinthesample. Ingeneticsandbiostatistics, OrlitskyandSuresh(Orlitsky&Suresh,2015)showedthat
thecomplement1âˆ’M measuressamplecoverage,i.e.,the natural estimators from GT, i.e., pË†G = MË†G /Î¦ , per-
0 x Nx Nx
proportionofindividualsinthepopulationbelongingtoa formsclosetothebestnaturalestimator. Regret,themetric
speciesnotobservedinthesample(Chao&Jost,2012). In ofthecompetitivenessofanestimatoragainstthebestnat-
thecontextofsupervisedmachinelearning,assumingthe uralestimator,ismeasuredasKLdivergencebetweenthe
trainingdataisarandomsample,thesamplecoverageof estimate pË†and the actual distribution p, D (pË†||p). Orl-
KL
the training data gives the proportion of all data (seen or itskyandSureshalsoshowedthatfindingthebestnatural
unseen)withlabelsnotobservedinthetrainingdata. estimator for p is same as finding the best estimator for
M ={M }n .
UsinganestimateMË† ofM ,weestimatetheprobability k k=0
k k
p of an element x âˆˆ X that appears k times as MË† /Î¦ . Poissonapproximation. ThePoissonapproximationwith
x k k
The estimation of p in the presence of unseen elements parameterÎ» =p nhasoftenbeenusedtotackleamajor
x x
xÌ¸âˆˆXnisafundamentalprobleminmachinelearning(Or- challenge in the formal analysis of the missing mass and
litskyetal.,2003;Orlitsky&Suresh,2015;Painsky,2022; naturalestimators(Orlitsky&Suresh,2015;Orlitskyetal.,
Acharya et al., 2013; Hao & Li, 2020). For instance, in 2016;Acharyaetal.,2013;Efron&Thisted,1976;Valiant
naturallanguageprocessingtheestimationoftheprobabil- &Valiant,2016;Good,1953;Good&Toulmin,1956;Hao
ityofagivensequenceofwordsoccurringinasentenceis &Li,2020). Thechallengeisthedependenciesbetweenfre-
themainchallengeoflanguagemodels,particularlyinthe quenciesN fordifferentelementsxâˆˆX. InthisPoisson
x
presenceofsequencesthatappearinthetrainingdatararely Productmodel, acontinuous-timesamplingschemewith
ornotatall. Differentsmoothingandbackofftechniques S = |X| independent Poisson distributions is considered
havebeendevelopedtotacklethisdatasparsitychallenge. wherethefrequencyN ofanelementxisrepresentedas
x
aPoissonrandomvariablewithmeanp n. Inotherwords,
Anaturalestimator ofp assignsthesameprobabilityto x
x inthePoissonapproximation,thefrequenciesN aremod-
allelementsxappearingthesamenumberoftimesinthe x
elledasindependentrandomvariables. Consequently,GT
sample Xn (Orlitsky & Suresh, 2015). For k > 0, pË† =
x estimatorisanunbiasedestimatorforthePoissonProduct
M /Î¦ givesthehypothetical1bestnaturalestimatorofp
k k x model(Orlitskyetal.,2016),yetitisbiasedinthemulti-
foreveryelementxthathasbeenobservedktimes.
nomialdistribution(Juang&Lo,1994). Hence,wetackle
BiasofGood-Turing(GT).Intermsofbias,JuanandLo thedependenciesbetweenfrequenciesanalytically,without
(Juang&Lo,1994)observethattheGTestimatorMË†G = approximationviathePoissonProductmodel.
k
k+1Î¦ isanunbiasedestimateofM (Xnâˆ’1),i.e.,where
n k+1 k
then-thsamplewasdeletedfromXnandfind: 1.2.ContributionofthePaper
(cid:12) (cid:12) (cid:12)E(cid:104) MË† kGâˆ’M k(cid:105)(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12)E(cid:2) M k(Xnâˆ’1)âˆ’M k(Xn)(cid:3)(cid:12) (cid:12) I dn ist th rii bs up tia op ner e, sw time ar te ii on nfo wrc ite ht ahe prf eo cu isn ed ca hti ao rn as cto ef rim zau til oti nno om fti ha el
k+2 (cid:18) 1(cid:19) thedependenciesbetweenN = (cid:80)n 1(X = x)across
â‰¤ =O . x i=1 i
n+1 n differentxâˆˆX (ratherthanassumingindependenceandus-
ingthePoissonapproximation). Thetheoreticalanalysisis
ConvergenceofGT.McAllesterandSchapire(McAllester basedontheexpectedvalueofthefrequencyoffrequencies
& Schapire, 2000) analyzed the convergence. With high E[Î¦ k]=f k(n)betweendifferentkandn,whichis
probability, f (n) f (n+1) f (n+1)
k = k âˆ’ k+1 . (2)
|MË† kGâˆ’M k(n)| (cid:0)n k(cid:1) (cid:0)n+ k1(cid:1) (cid:0)n k++ 11(cid:1)
ï£± âˆš
O(1/ n) fork =0(missingmass) Exploringthisnewtheoreticaltool,webringtwocontribu-
ï£´ï£²
âˆš
= O(log(n)/ n) forsmallk(rareelements) tionstotheestimationofthetotalprobabilitymassM k for
Î´
ï£´ï£³O(k/âˆš
n) forlargek(abundantelements).
anyk :0â‰¤k â‰¤n. Firstly,weshowexactlytowhatextent
E[M ]canbeestimatedfromthesampleXnandhowmuch
k
ThisresultwasimprovedbyDrukhandMansour(Drukh remainstobeestimatedfromtheunderlyingdistributionp
&Mansour,2004)andmorerecentlybyPainsky(Painsky, andthenumberofelements|X|. Specifically,weshowthe
2022)whoshowedthatGTestimatorconvergesatarateof following.
âˆš
O(1/ n)forallkbasedonworst-casemeansquarederror Theorem1.1.
analysis. (cid:32) (cid:33)(cid:34)nâˆ’k (cid:44)(cid:32) (cid:33)(cid:35)
E[M ]= n (cid:88) (âˆ’1)iâˆ’1f (n) n +R
1Thebestnaturalestimatorisalsocalledoracle-aidedestimator k k k+i k+i n,k
i=1
foritsknowledgeaboutp (Orlitsky&Suresh,2015)butcannot
actuallybeusedforestimax tion. whereR =(cid:0)n(cid:1) (âˆ’1)nâˆ’kf (n+1)istheremainder.
n,k k n+1
2HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
ThisdecompositionshowsthattheGTestimatoristhefirst g k(n)âˆ’g k+1(n+1)=g k(n+1)
term of E[M ] using the plug-in estimator Î¦ for f (n). 0 -
k 1 1
Hence, it gives the exact bias of the GT estimator in the 1 -
g k(n)
multinomialsetting(whichwouldincorrectlybeidentified 2 -
asunbiasedusingthePoissonapproximation).
3 - g k(n+1) g k+1(n+1)
4 -
Our second contribution is the development of a class of 5 -
naturalestimators. Westartbydefininganearlyunbiased n6 -
estimator MË† kB = (cid:0)n k(cid:1)(cid:104) (cid:80)n i=âˆ’ 1k(âˆ’1)iâˆ’1Î¦ k+i(cid:46)(cid:0) kn +i(cid:1)(cid:105) that 7
8
-
-
g 1(n+1)=ð”¼[M 0(n)]
usestheplug-inestimatorÎ¦ forf (n)inTheorem1.1and 9 -
l l
drops R . While the bias of MË†B decays exponentially, 10 -
thevarian n, ck eofMË† kB istoohightobk epractical. 111
2
-
- â¶
â¸ g k+1(n+1)=ð”¼ [M nk C(n k)
]
Weobservethatwecancasttheestimationoftheexpected 13 - â·
k
totalmassasanoptimizationproblem. FromTheorem1.1 0 1 2 3 4 5 6 7 8 9 10 11 12 13
and Eqn. 2, we can see that there are many representa- Figure1. Lowertrianglematrixofg (n).
k
tionsofE[M ],allofwhichsuggestdifferentestimatorsfor
k
E[M k]. Weintroduceadeterministicmethodtoconstructa &Li,2020). Inthefollowing,wetacklethischallengeby
uniqueestimatorfromarepresentation,andshowhowtoes- formalizingthesedependenciesbetweenfrequencies. Thus,
timatethemeansquarederror(MSE)forsuchanestimator. weestablishalinkbetweentheexpectedvaluesofthecor-
Equippedwithalargesearchspaceofrepresentationsanda respondingtotalprobabilitymasses.
fitnessfunctiontoestimatetheMSEofacandidateestima-
tor,wedevelopageneticalgorithmthatfindsanestimator 2.1.DependencyAmongFrequencies
withaminimalMSE(andatmostNterms).
Recallthattheexpectedvaluef (n)ofthenumberofel-
k
Wecomparetheperformanceoftheminimal-biasestimator ements Î¦ that appear exactly k times in the sample Xn
MË† kB and the minimal-MSE estimators discovered by our isdefinedk asf k(n) = (cid:80)
xâˆˆX
(cid:0)n k(cid:1) pk x(1âˆ’p x)nâˆ’k. Forcon-
geneticalgorithtothethatofthewidelyusedGTestimator
vencience,letg (n)=f
(n)/(cid:0)n(cid:1)
. Wenoticethefollowing
onavarietyofmultinomialdistributionsusedforevaluation k k k
relationshipamongkandn:
inpreviouswork. Ourresultsshowthat1)theminimal-bias
estimatorhasasubstantiallysmallerbiasthantheGTesti- S
(cid:88)
matorbythousandsoforderofmagnitude,2)Ourgenetic g k(n+1)= pk x(1âˆ’p x)nâˆ’kÂ·(1âˆ’p x)
algorithmcanproduceestimatorswithMSEsmallerthan x=1
theGTestimatorover96%ofthetimewhenthereareat =g k(n)âˆ’g k+1(n+1) (3)
leastasmanysamplesasclasses;theirMSEisroughly80% nâˆ’k
(cid:88)
oftheGTestimator. Wealsopublishalldataandscriptsto = (âˆ’1)ig (n)+(âˆ’1)nâˆ’k+1g (n+1)
k+i n+1
reproduceourresults. i=0
We can now write the expected value E[M ] of the total
2.DependenciesBetweenFrequenciesN k
x
probability mass in terms of the frequencies with which
Firstly,weproposeanew,distribution-free2 methodology differentelementsxâˆˆX havebeenobservedinthesample
forreasoningaboutpropertiesofestimatorsofthemissing Xnofsizenasfollows
andtotalprobabilitymassesformultinomialdistributions. (cid:18) (cid:19)
(cid:88) n
ThemainchallengeforthestatisticalanalysisofM
k
has E[M k]=
k
pk x+1(1âˆ’p x)nâˆ’k
been reasoning in the presence of dependencies between xâˆˆX
frequenciesN fordifferentelementsxâˆˆX. Asdiscussed (cid:18) n(cid:19)
x
= g (n+1) (4)
in Section 1.1, a Poisson approximation with parameter k k+1
Î» x = p xnisoftenusedtorenderthesefrequenciesasin- (cid:18) (cid:19)(cid:34)nâˆ’k (cid:35)
dependent(Orlitsky&Suresh,2015;Orlitskyetal.,2016; = n (cid:88) (âˆ’1)iâˆ’1g (n) +R (5)
Acharya et al., 2013; Efron & Thisted, 1976; Valiant & k k+i n,k
i=1
Valiant, 2016; Good, 1953; Good & Toulmin, 1956; Hao
whereR =(cid:0)n(cid:1) (âˆ’1)nâˆ’kf (n+1)isaremainderterm.
2Adistribution-freeanalysisisfreeofassumptionsaboutthe Thisdemn, ok nstrak tesTheoremn+ 11
.1.
shapeoftheprobabilitydistributiongeneratingthesample.Inthis
case,wemakenoassumptionsaboutparametersporn. Figure1illustratestherelationshipbetweentheexpected
frequencyoffrequenciesf (n)=g
(n)/(cid:0)n(cid:1)
,thefrequency
k k k
3
- - - - - - - - - - - - - -HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
k,andthesamplesizen. They-andx-axisrepresentsthe 3.1.EstimatorwithExponentiallyDecayingBias
sample size n and the frequency k, respectively. As per
Let
Eqn.(3),forevery2x2lowertrianglematrix,thevalueof
(th ge (l now ))e mr il nef ut sc te hl el v(g
ak
lu( en o+ ft1 h) e) li os wv ea rlu rie gho tf cth ele lu (gpper (l nef +tc 1e )l )l
.
MË† kB
=âˆ’(cid:18) n k(cid:19)n (cid:88)âˆ’k (âˆ’ (cid:0)1) niÎ¦
(cid:1)k+i
k k+1 i=1 k+i
Wecanusethisvisualizationtoquicklyseehowtorewrite Bias. Forsomeconstantk :0â‰¤k â‰¤nandsomeconstant
g (n) as an alternating sum of values of the cells in the c>1,thebiasofMË†B isintheorderofO(nkcâˆ’n),i.e.,
k k
upper row, starting from the cell in the same column to
therightmostcell,andadding/subtractingthevalueofthe E(cid:104) MË†B âˆ’M (cid:105) =âˆ’R =(âˆ’1)nâˆ’kâˆ’1(cid:18) n(cid:19) (cid:88) pn+1
k k n,k k x
rightmost cell in the current row. For instance, the value
xâˆˆX
g in0( th1 e3) fii gn ut rh ee cb anot bto em eqr ue ad llc yel cl a( lr co uw latn ed= as13 th, eco vl au rm ion usk li= ne0 ar) (cid:12) (cid:12) (cid:12)E(cid:104) MË† kB âˆ’M k(cid:105)(cid:12) (cid:12) (cid:12)â‰¤(cid:18) n k(cid:19) (cid:88) câˆ’ xn â‰¤nk (cid:88) câˆ’ xn
combinationsofitssurroundingcells: (1)withg (12)and xâˆˆX xâˆˆX
0
g (13)(bluecolored),(2)withg (11),g (11),Â·Â·Â·,g (13)
1 0 1 4 wherec >1forallxâˆˆX areconstants.
x
(purple colored), or (3) with g (11), g (11), Â·Â·Â·, g (13)
0 1 8
(greencolored). Variance. ThevarianceofMË†B isgivenbythevariances
k
andcovariancesofthefrequenciesÎ¦ fori=1,Â·Â·Â· ,nâˆ’
k+i
Missing Mass. The missing probability mass M 0 gives k. Underthecertainconditions,thevarianceofMË†B also
the proportion of all possible observations for which the k
decaysexponentiallyinn.
elementsxâˆˆX havenotbeenobservedinXn. ByEqn.(4)
Theorem 3.1. The variance of MË†B decreases exponen-
and(5),theexpectedvalueofM 0is k
tiallywithnifp <0.5or (1âˆ’pmax)(1âˆ’pmin) <1,where
E[M 0]=g 1(n+1) p
max
=max xâˆˆXma px xandp
min
=minpm xâˆˆax
X
p x.
(cid:34) n (cid:35)
(cid:88)
= (âˆ’1)kâˆ’1g (n) +(âˆ’1)nf (n+1). TheproofispostponedtoAppendixB.
k n+1
k=1 ComparisontoGood-Turing(GT).ThebiasofMË†B not
k
ThevaluesinthesecondcolumnofFigure1(k = 1)rep- onlydecaysexponentiallyinnbutisalsosmallerthanthat
resentstheexpectedvaluesofmissingmass;E[M ]being ofGTestimatorMË†G byanexponentialfactor. Forasim-
0 k
the cumulative sum of (âˆ’1)kâˆ’1g k(n) is intuitively clear plervariantofGTestimator,MË† kGâ€² = nk âˆ’+1 kÎ¦ k+1(suggested
fromthefigure(theredcellintherown = 7). Itishere in(McAllester&Schapire,2000)),whichcorrespondsto
where we observe that E[M 0] = g 1(n+1) is almost en- thefirsttermintheexpectedtotalprobabilitymassE[M k]
tirelydeterminedbytheg âˆ—(n),theexpectedfrequenciesof inEqn.(5),weshowthatitsbiasislargerbyanexponential
frequencies in the sample Xn, and not by the number of factorthantheabsolutebiasofMË†B.Toseethis,weprovide
k
elements|X|ortheirunderlyingdistributionp. Infact,the boundsontheindividualsumsandthenonthebiasratio:
influenceofpintheremaindertermdecaysexponentially,
i.e., f n+1(n+1) = (cid:80)
xâˆˆX
pn x+1 â‰¤ (cid:80)
xâˆˆX
(cid:0) e1âˆ’px(cid:1)âˆ’nâˆ’1 Bias
Gâ€²
=E(cid:104) MË† kGâ€² âˆ’M k(cid:105) â‰¥(cid:18) n k(cid:19) pk m+ in2(1âˆ’p min)nâˆ’kâˆ’1
whichisdominatedbythediscoveryprobabilityofthemost
(6)
abundantelementmax(p).
(cid:12) (cid:104) (cid:105)(cid:12) (cid:18) n(cid:19)
TotalMass. Similarly,theexpectedvalueofthetotalprob- |Bias B|=(cid:12) (cid:12)E MË† kB âˆ’M k (cid:12) (cid:12)â‰¤ k Spn m+ ax1 (7)
abilitymassE[M ](theredcellintherown=10),which
k
isequalto(cid:0)n k(cid:1)
g k+1(n+1),isalmostentirelydeterminedby whereS =|X|,suchthat
theexpectedfrequenciesofthesampleXnwithremainder
R n,k =(cid:0)n k(cid:1)(cid:80) xâˆˆX pn x+1. (cid:12) (cid:12) (cid:12)Bias Gâ€²(cid:12) (cid:12) (cid:12)â‰¥ pk m+ in2 (cid:18) 1âˆ’p min(cid:19)nâˆ’kâˆ’1 .
(cid:12)Bias
B
(cid:12) Spk m+ ax2 p
max
3.ALargeClassofEstimators
Noticethat(1âˆ’p )/p >1foralldistributionsover
min max
FromtherepresentationofE[M ]intermsoffrequenciesin X,exceptwhereS =2andp={0.5,0.5}. Thesamecan
k
Eqn.(4)andtherelationshipacrossfrequenciesinEqn.(3), beshownfortheoriginalGTestimatorMË†G = k+1Î¦
k n k+1
wecanseethatthereisalargenumberofrepresentationsof forasufficientlylargesamplesize(seeAppendixA).
theexpectedtotalprobabilitymassE[M ]. Eachrepresen-
k Example(Missingmassfortheuniform).Suppose,weseek
tationmightsuggestdifferentestimators.
toestimatethemissingmassfromasequenceofelements
WestartbydefiningtheminimalbiasestimatorMË†B from Xn observedinnsamplesfromtheuniformdistribution;
k
therepresentationinEqn.(5)andexploreitsproperties. p = 1/S for all x âˆˆ X. MË†G overestimates M on the
x 0 0
4HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
averageby(Sâˆ’1)nâˆ’1/SnwhileournewestimatorMË†Bhas Algorithm1GeneticAlgorithm
0
abiasof(âˆ’1)n/Sn. Hence, fortheuniformdistribution, Input: Targetfrequencyk,SampleXn
our estimator exhibits a bias that is lower by a factor of Input: IterationlimitG,mutantsizem
1/(Sâˆ’1)nâˆ’1. 1: PopulationP
0
={r 0}
2: Fitnessfbest =f =fitness(r )
WhilethebiasofourestimatorMË† kB islowerthanthatof 3: LimitG
L
=G 0 0
MË† 0G byanexponentialfactor,thevarianceishigher. The 4: forgfrom1toG Ldo
varianceofMË†Bdependsonthevariancesofandcovariances 5: P =selectTopM(P gâˆ’1,m)
k 6: Pâ€² =lapply(P,mutate)
betweenÎ¦ ,Î¦ ,...,Î¦ ,i.e.,
k+1 k+2 n 7: P =Pâ€²âˆª{r }âˆªselectTopM(P ,3)
g 0 gâˆ’1
8: f =min(lapply(P ,fitness))
nâˆ’k g g
Var(cid:16) MË†B(cid:17) = (cid:88) c2Var(Î¦ ) 9: if(g=G L)âˆ§((f g =f 0)âˆ¨(fbest >0.95Â·f g))then
k i k+i 10: G =G +G
L L
i=1 (8) 11: fbest =f
(cid:88) g
+ (âˆ’1)i+jc c Cov(Î¦ ,Î¦ ), 12: endif
i j k+i k+j
13: endfor
iÌ¸=j 14: EstimatorMË†Evo =instantiate(selectTopM(P ,1))
k GL
where c
i
= (cid:0)n k(cid:1)(cid:14)(cid:0) kn +i(cid:1) . In contrast, the variance of MË† kG Output: Minimal-MSEEstimatorMË† kEvo
depends only on the variance of Î¦ . In the empirical
k+1
study,weinvestigatethedifferenceofthetwoestimatorsin
termsofthebiasandthevariance. accordingly. Forinstance,applyingEqn.(11)withÎ´ =0.5
andEqn.(13)tor ,weobtainthefollowingrepresentation
0
3.2.EstimationwithMinimalMSEasSearchProblem r 1ofE[M k]:
T thh ae tr ce aa nre bm ea cn oy nr se trp ur ces tee dnt bat yio rn es cuof rsE iv[ eM lyk r] e= wr(cid:0) itn k i(cid:1) ng gk+ te1 r( mn s+ a1 c)
-
ï£± ï£´ï£´ï£´ï£² ï£± ï£´ï£´ï£´ï£²(cid:0) (cid:0)n nk(cid:1) (cid:1)(cid:14) (cid:14)2
2
f fo or ri i= =k k+ +1 1a an nd dj
j
= =n n+1ï£¼ ï£´ï£´ï£´ï£½
(c co fr .d Ein qg n.to (3th &ed 4e ))p .e Tnd he en rc ey pra em seo nn tg atf ir oe nqu ue sen dci te os cw oe nsid tre un ct tifi oe ud
r
r 1 = ï£´ï£´ï£´ï£³Î± i,j = ï£´ï£´ï£´ï£³0âˆ’k(cid:0)n k(cid:1)(cid:14) 2 f oo thr ei r= wisk e.+2andj =n+1 ï£´ï£´ï£´ï£¾
minimal-biasestimatorMË†B wasoneofthem. However,we
k
noticethatthevarianceofMË† kB istoohightobepractical. Estimatorinstantiation. Toconstructauniqueestimator
Tofindarepresentationfromwhichanestimatorwithamin- MË†r ofM fromarepresentationrofE[M ],wepropose
k k k
imalmeansquarederror(MSE)canbederived,wecastthe a deterministic method. But first, we define our random
efficientestimationofM k asanoptimizationproblem. To variablesonsubsamplesofXn. Foranymâ‰¤n,letN x(m)
efficientlynavigatethelargesearchspaceofrepresentations bethenumberoftimeselementx âˆˆ X isobservedinthe
ofE[M k],wedevelopageneticalgorithm. subsampleXm =âŸ¨X 1,Â·Â·Â·X mâŸ©ofXn. LetÎ¦ k(m)bethe
Search space. Let E[M ] be represented by a suitable
numberofelementsappearingexactlyktimesinXm,i.e.,
k
choiceofcoefficientsÎ± suchthat
i,j m
(cid:88)
n+1n+1 N (m)= 1(X =x) NotethatN =N (n).
(cid:88)(cid:88) x i x x
E[M ]= Î± g (j). (9)
k i,j i i=1
i=1 j=i Î¦ (m)= (cid:88) 1(N (m)=k) NotethatÎ¦ =Î¦ (n).
k x k k
OnerepresentationofE[M ]=(cid:0)n(cid:1) g (n+1)is xâˆˆX
k k k+1
r
0
=(cid:40) Î±
i,j
=(cid:40)(cid:0) 0n k(cid:1) f oo thr ei r= wisk e.+1andj =n+1(cid:41) (10) Hence,givenarepresentationr,wecanconstructMË† kr as
M cou nsta trt uio ctn a. G nei wven rea pn rey sr ee np tare tis oe nnt ra â€²ti oo fn Er [o MfE ],[M
s.tk
.], Ew qne .c (a 9n
)
MË† kr
=ï£® ï£°(cid:88)n (cid:88)n Î±
(cid:0)ji, (cid:1)jÎ¦
i(j)ï£¹ ï£»+(cid:34) (cid:88)n Î±
(cid:0)ni, +n+ 1(cid:1)1Î¦
i(cid:35)
k
i=1 j=i i i=1 i
holdsbyrecursivelyconsideringthefollowingidentities:
Î±
i,j
Â·g i(j)=Î±
i,j
Â·((1âˆ’Î´)g i(j)+Î´g i(j)) (11) NoticethatÎ¦
i(j)(cid:14)(cid:0)j i(cid:1)
isjusttheplug-inestimatorforg i(j).
=Î± i,j Â·(g i(j+1)+g i+1(j+1)) (12) Fitnessfunction. Todefinethequantitytooptimize,any
=Î± Â·(g (jâˆ’1)âˆ’g (j)) (13) meta-heuristicsearchrequiresafitnessfunction. Ourfitness
i,j i i+1
functiontakesacandidaterepresentationrandreturnsan
=Î± Â·(g (jâˆ’1)+g (j)) (14)
i,j iâˆ’1 iâˆ’1 estimate of the MSE of the corresponding estimator MË†r.
k
foranychoiceofÎ´ :0â‰¤Î´ â‰¤1. Importantly,afterapplying We decompose the MSE as the sum of its variance and
these identities, we must work out the new coefficients squaredbias. Forconvenience,letg (n)=0.
n+1
5HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
4.Experiment
MSE(MË†
kr)=(cid:34)n (cid:88)+1
Î± i,n+1[g i(n+1)âˆ’g
i(n)](cid:35)2
(15)
W oue rmde is ni ig mn ae l-x bp iae sri em ste in mts att oo re MË†va Blu aa nte dt (h iie )p oe fr of uo rrm tha en mce in( ii m) ao lf
-
k
i=1 MSEestimatorMË†Evo thatisdiscoveredbyourgenetical-
n n (cid:32) (cid:33)2 k
+(cid:88)(cid:88) Î± i,j Var(Î¦ (j)) gorithmagainsttheperformanceofthewidely-usedGood-
(cid:0)j(cid:1) i TuringestimatorMË†G(Good,1953).
i=1 j=i i k
n n n n
+(cid:88)(cid:88)(cid:88) (cid:88) Î± i,j Î± l,mCov(Î¦ (j),Î¦(m)) Distibutions.Weusethesamesixmultinomialdistributions
(cid:0)j(cid:1) (cid:0)m(cid:1) i l
that are used in previous evaluations (Orlitsky & Suresh,
i=1 j=i l=1 m=l i l
lÌ¸=i mÌ¸=j 2015; Orlitsky et al., 2016; Hao & Li, 2020): a uniform
WeexpandonthecomputationoftheMSEinAppendixC.
distribution (uniform), a half-and-half distribution where
Sincetheunderlyingdistribution{p } isunknown,we halfoftheelementshavethreetimesoftheprobabilityofthe
x xâˆˆX
canonlyestimatetheMSE.Foranyelementxthathasbeen otherhalf(half&half),twoZipfdistributionswithparameters
observed exactly k > 0 time in the sample Xn, we use s = 1 and s = 0.5 (zipf-1, zipf-0.5, respectively), and
pË† =MË†G/Î¦ asnaturalestimatorofp ,whereMË†Gisthe distributionsgeneratedbyDirichlet-1priorandDirichlet-
x k k x k
GTestimator. Tohandleunobservedelements(k =0),we 0.5prior(diri-1,diri-0.5,respectively).
firstestimatethenumberofunseenelementsE[Î¦ ]=f (n)
0 0 OpenScienceandReplication. Forscrutinyandreplica-
usingChaoâ€™snonparamtericspeciesrichnessestimatorfË† =
0 bility,wepublishallourevaluationscriptsat:
nâˆ’ n1 2Î¦ Î¦2 1
2
(Chao,1984),andthenestimatetheprobabilityof https://anonymous.4open.science/r/Better-Turing-157F.
each such unseen element as pË† = MË†G/fË†, where MË†G
y 0 0 0
istheGTestimator. Finally,weplugtheseestimatesinto 4.1.EvaluatingourMinimal-BiasEstimator
Eqn.(15)toestimatetheMSE.Itisinterestingtonotethat â€¢ RQ1. HowdoesourestimatorforthemissingmassMË†B
itispreciselytheGTestimatorwhoseMSEourapproachis comparetotheGood-TuringestimatorMË†G interms0 of
supposedtoimproveupon. 0
biasasafunctionofsamplesizen?
Geneticalgorithm.Withtherequiredconceptsinplace,we â€¢ RQ2. How does our estimator for the total mass MË†B
k
arereadytointroduceourgeneticalgorithm(GA)(Mitchell, comparetotheGood-TuringestimatorMË†G intermsof
1998). Algorithm1sketchesthegeneralprocedure. Givena k
biasasafunctionoffrequencyk?
targetfrequencyk(incl.k =0),thesampleXn,aniteration
â€¢ RQ3. Howdotheestimatorscompareintermsofvari-
limitG,andthenumbermofcandidaterepresentationsto
anceandmean-squarederror?
be mutated in every iteration, the algorithm produces an
estimatorMË† kEvowithminimalMSE. We focus specifically on the bias of MË† kB, i.e., the aver-
Starting from the initial representation r (Eqn. (10); agedifferencebetweentheestimateandtheexpectedvalue
0 E[M ]. Weexpectthatthebiasofthemissingmassesti-
Line1),ourGAiterativelyimprovesapopulationofcandi- k
daterepresentationsP g,calledindividuals. Foreverygen-
mateMË† 0B asafunctionofnacrossdifferentdistributions
erationg(Line4),ourGAselectsthemfittestindividuals providesempiricalinsightforourclaimthathowmuchis
fromthepreviousgenerationP (Line5),mutatesthem unseenchieflydependsoninformationabouttheseen.
gâˆ’1
(Line6),andcreatesthecurrentgenerationP gbyaddingthe RQ.1. Figure 2a illustrates how fast our estimator MË†B
initialrepresentationr andtheTop-3individualsfromthe k
0 andthebaselineestimatorMË†G(GT)approachtheexpected
previousgeneration(Line7). TheinitialandpreviousTop-3 k
missingmassE[M ]asafunctionofsamplesizen. Asit
individuals are added to mitigate the risk of convergence 0
mightdifficultforthereadertodiscerndifferencesacross
toalocaloptimum. Tomutatearepresentationr,ourGA
distributionsforthebaselineestimator,werefertoFigure2b,
(i)choosesarandomtermr,(ii)appliesEqn.(11)whereÎ´is
wherewezoomintoarelevantregion.
chosenuniformlyatrandom,(iii)appliesarandomidentity
fromEqn.(12â€“14),and(iv)adjuststhecoefficientsforthe The magnitude of our estimatorâ€™s bias is significantly
resultingrepresentationrâ€²accordingly. Theiterationlimit smaller than the magnitude of GTâ€™s bias for all distribu-
G Lisincreasedifthecurrentindividualsdonotimproveon tions(bythousandsofordersofmagnitude).3 Figure2aalso
theinitialindividualr 0 orsubstantiallyimproveonthose nicelyillustratestheexponentialdecayofourestimatorin
discoveredrecently(Line9â€“12). terms of n and how our estimator is less biased than GT
byanexponentialfactor. InFigure2b,wecanobservethat
Distribution-free. Whileourapproachitselfisdistribution-
GTâ€™sbiasalsodecaysexponentially,althoughnotnearlyat
free,theoutputisdistribution-specific,i.e.,thediscovered
therateofourestimator.
estimator has a minimal MSE on the specific, unknown
distribution. Weexplorethispropertyinourexperiments. 3Recallthattheplotshowsthelogarithmoftheabsolutebias.
6HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
0e+00 âˆ’3e+00 0e+00 n MË† 0 Bias Var MSE
âˆ’4e+00 GT 3.6973e-003 2.3372e-03 2.3508e-03 âˆ’1e+04 âˆ’2e+03 100 ours 1.0000e-200 2.3515e-03 2.3515e-03
âˆ’5e+00
GT 6.6369e-005 1.1430e-05 1.1434e-05
âˆ’2e+04 âˆ’6e+00 âˆ’4e+03 500 ours <1.00e-200 1.1445e-05 1.1445e-05
âˆ’7e+00 GT 4.3607e-007 4.3439e-08 4.3439e-08
âˆ’3e+04 âˆ’6e+03 1000
2500 5000 7500 10000 2500 5000 7500 10000 500 1000 1500 2000 ours <1.00e-200 4.3441e-08 4.3441e-08
n n k
(a)M0(S=1000). (b)M0(S=1000,cropped). (c)Mk(S=1000,n=2000). (d)M0(uniform,S=100).
Figure2.Resultsforourminimal-biasestimatorMË†B.Forourandbaselineestimators,weshowthelogarithmoftheabsolutebias(a&b)
k
asafunctionofnfork=0and(c)asafunctionofkforn=2000.Wealsoshow(d)thebias,variance,andMSEofourandbaseline
estimatorforthreevaluesofn.MoreplotscanbefoundinAppendixE.
Table1.TheMSEofthebestevolvedestimatorMEvoandGTestimatorMË†GforthemissingmassM ,thesuccessrateAË† ,andtheratio
0 0 0 12
(Ratio,MSE(MË†Evo)/MSE(MG))forthreesamplesizesnandsixdistributionswithsupportsizeS =200.
0 0
n=S/2 n=S n=2S
Dist.
MSE(MË† 0G) MSE(M 0Evo) AË† 12 Ratio MSE(MË† 0G) MSE(M 0Evo) AË† 12 Ratio MSE(MË† 0G) MSE(M 0Evo) AË† 12 Ratio
uniform 3.32e-03 2.04e-03 0.95 61% 1.17e-03 8.90e-04 0.99 76% 2.01e-04 1.70e-04 0.93 84%
half&half 3.33e-03 1.97e-03 0.96 59% 1.09e-03 8.58e-04 0.99 78% 2.11e-04 1.72e-04 1.00 81%
zipf-1 2.32e-03 2.41e-03 0.74 103% 8.16e-04 7.24e-04 0.88 88% 2.39e-04 2.11e-04 0.96 88%
zipf-0.5 3.23e-03 2.29e-03 0.89 71% 1.09e-03 8.52e-04 0.97 78% 2.30e-04 1.93e-04 1.00 83%
diri-1 2.99e-03 2.36e-03 0.85 78% 8.88e-04 6.65e-04 1.00 74% 1.96e-04 1.65e-04 0.96 84%
diri-0.5 2.55e-03 1.81e-03 0.94 71% 6.88e-04 4.86e-04 0.98 70% 1.61e-04 1.31e-04 0.93 81%
Avg. 0.88 74% 0.96 77% 0.96 84%
Intermsofdistributions,acloserlookattheperformance matelythesameasthatofGT.ThereasonisthattheMSEis
differencesconfirmsoursuspicionthatthebiasofoures- dominatedbythevariance. Wemakethesameobservation
timator is strongly influenced by the probability p of forallotherdistributions(seeAppendixE).TheMSEsof
max
themostabundantelementwhilethebiasofGTisstrongly bothestimatorsarecomparable.
influencedbytheprobabilityp oftherarestelement. In
min
fact,byEqn.(7)theabsolutebiasofourestimatorismin- 4.2.EvaluatingourEstimatorDiscoveryAlgorithm
imized when p is minimized. By Eqn. (6), GTâ€™s bias
max
â€¢ RQ1 (Effectiveness). How does our estimator for the
isminimizedifp ismaximized. Sincebothistruefor
min missingmassMË†EvocomparetotheGood-Turingestima-
theuniform,bothestimatorsexhibitthelowestbiasforthe 0
torMË†GintermsofMSE?
uniformacrossallsixdistributions. GTperformssimilaron 0
alldistributionsapartfromtheuniform(wherebiasseems â€¢ RQ2(Efficiency). Howlongdoesittakeforourgenetic
minimal)andzipf-1(wherebiasismaximized). Forouresti- algorithmtogenerateanestimatorMEvogivenasample?
k
mator,ifwerankedthedistributionsbyvaluesofp with
max â€¢ RQ3 (Distribution-awareness). How well does an esti-
thesmallestvaluefirstâŸ¨uniform,half&half,zipf-0.5,zipf-1âŸ©,4
mator discovered from a sample from one distribution
we would arrive at the same ordering in terms of perfor-
performonanotherdistributionintermsofMSE?
manceofourestimatorasshowninFigure2a.
Tohandletherandomnessinourevaluation,werepeateach
RQ2. Figure2cillustratesforbothestimatorsofthetotal
experiment100times: 10runsoftheGAwith10different
massM howthebiasbehavesaskvariesbetween0and
k samplesXn.5 Moredetailsaboutourexperimentalsetup
n = 2000 when S = 1000. The trend is clear; the bias
canbefoundinAppendixD.
ofourestimatorisstrictlysmallerthanthebiasofGTfor
all k and all the distributions. The difference is the most RQ.1(Effectiveness). Table1showsaverageMSEofthe
significantforrareelements(smallk)andgetssmalleras estimatorMEvodiscoveredbyourgeneticalgorithmandthat
0
kincreases. Thebiasofourestimatorismaximizedwhen oftheGTestimatorMË†G forthemissingmassM across
0 0
k =1000=0.5n,thebiasforGTwhenk =0. threesamplesizes. WemeasureeffectsizeusingVargha-
DelaneyAË† (Vargha&Delaney,2000)(successrate),i.e.,
RQ3. Table2dshowsvarianceandMSEofbothestimators 12
the probability that the MSE of the estimator discovered
forthemissingmassM
0
fortheuniformandthreevalues
byourgeneticalgorithmhasasmallerMSEthantheGT
ofn. Aswecansee,theMSEofourestimatorisapproxi-
5Fordiri-1,diri-0.5,eachofthetensamplesXn issampled
4diri-1 and diri-0.5 are not considered in the order because
from10distributionssampledfromtheDirichletpriorwiththe
multipledistributionsaresampledfromtheDirichletprior.
sameparameterÎ±=0.5,1,respectively.
7
|saib|
01gol
|saib|
01gol
|saib|
01golHowMuchisUnseenDependsChieflyonInformationAbouttheSeen
1e 4 (target). Applyinganestimatefromthezipf-1onthezipf-1
7
Evolved from givestheoptimalMSE(right-mostredbox). However,ap-
6 uniform plyinganestimatorfromthezipf-1ontheuniform(leftred
5 half&half box)yieldsahugeincreaseinvariance. Intermsofeffect
zipf-0.5 size,wemeasureaVarghaDelaneyAË† >0.84betweenthe
4 zipf-1 12
â€œhomeâ€andâ€œawayâ€estimator. Whileeachoftheuniform
3 andhalf&halfalsoshowsthatthehomeestimatorperforms
2 bestonthehomedistribution(AË† 12 =0.68(medium),0.58
(small),respectively),thedifferencebetweentheestimators
1
fromuniform,half&half,andzipf-0.5islesssignificant. Per-
0 hapsunsurprisingly, anestimatorperformsoptimalwhen
uniform half&half zipf-0.5 zipf-1
Target distribution thesourceofthesamplesissimilartothetargetdistribution.
Summary. Tosummarize,ourGAiseffectiveinfinding
Figure3.The MSE of an estimator discovered using a sample
theestimatorwiththeminimalMSEforthemissingmass
fromonedistribution(individualboxes)appliedtoanothertarget
distribution(clustersofboxes). M 0 with the smaller MSE than GT estimator MË† 0G for all
distributionsandsamplesizes. Theeffectissubstantialand
estimator(largerisbetter). Moreover,wemeasuretheMSE significantandtheaveragedecreaseoftheMSEisroughly
of our estimator as a proprtion of the MSE of GT, called onefifthagainstGTestimatorMË†G.
0
ratio(smallerisbetter). ResultsforothersupportsizesS
canbefoundinAppendixE.
5.Discussion
Overall,theestimatordiscoveredbyourGAperformssig-
BeyondtheGeneralEstimator. Inthisstudy,wepropose
nificantlybetterthanGTestimatorintermsofMSE(avg.
AË† > 0.9; ratio < 85%). The performance difference a meta-level estimation methodology that can be applied
12
to a set of samples from a specific unknown distribution.
increaseswithsamplesizen. Whenthesamplesizeistwice
Theconventionalapproachistodevelopanestimator for
thesupportsize(n = 2S),in96%ofrunsourdiscovered
an arbitrary distribution. Yet, each distributions has its
estimator performs better. The average MSE of our esti-
owncharacteristics,and,becauseofthat,themannerofthe
mator is somewhere between 70% and 88% of the MSE
(frequenciesof)frequenciesoftheclassesinthesamplecan
ofGT.ThehighsuccessrateandthelowratiooftheMSE
bedifferfrom,forexample,theuniformdistributiontothe
showsthattheGAiseffectiveinfindingtheestimatorwith
Zipfdistribution. Incontrasttotheconventionalapproach,
theminimalMSEforthemissingmassM . AWilcoxon
0
weproposeadistribution-freemethodologytodiscoverthe
signed-ranktestshowsthatallperformancedifferencesare
statisticallysignificantatÎ±<10âˆ’9.
adistribution-specificestimatorwithlowMSE(givenonly
thesample). Notethat,whileweusethegeneticalgorithm
Intermsofdistributions,theperformanceofourestimator todiscovertheestimator,anyoptimizationmethodcanbe
issimilaracrossalldistributions,showingthegeneralityof usedtodiscovertheestimator,forinstance,aconstrained
ouralgorithm. Theonlyexceptionisthezipf-1,wherethe optimizationsolver.
successrateislowerthanforotherdistributionforn=S/2
ExtrapolatingtheFutureSampling. Estimatingthenum-
andS,andtheaverageratiois103%(yet,themedianratio
ber of unseen species is a well-known problem in many
is85%)forn = S/2. Thepotentialreasonforthisisdue
scientificfields,suchasecology,linguistics,andmachine
totheoverfittingtotheapproximateddistributionpË† . Since
x
learning. Givennsamples,theexpectednumberofhitherto
the zipf-1 is the most skewed distribution, there are more
unseenspeciesthatwouldbeuncoveredifttimesmoresam-
elementsunseeninthesamplethaninotherdistributions,
plesweretakenisE[U(t)] = f (n)âˆ’f (n+nt). Good
whichmakestheapproximateddistributionpË† lessaccurate. 0 0
x
&Toulmin(1956)proposedaseminalestimatorusingthe
Yet,theperformancealreadyimprovesandbecomesimilar
frequenciesoffrequenciesÎ¦ ,similartotheGood-Turing
tootherdistributionsforn=S andn=2S. k
estimator. Untilrecently,varioussubsequentstudieshave
RQ.2(Efficiency). Thetimeittakestodiscovertheestima- beenconductedtoimprovetheestimator(Efron&Thisted,
tor is reasonable. To compute an estimator fo Table 1, it 1976;Orlitskyetal.,2016;Hao&Li,2020),whilemostof
takesaboutseven(7)minutesonaverageandfive(5)min- themstillreliesonthePoissonapproximationtodesignthe
utesonmedian. Theaveragetimeforeachiterationis1.25s estimator. Webelievethatouranalysiscanbeextendedto
(median: 0.92). theGood-Toulminestimatorseekingmoreaccurateestima-
torsforU(t).
RQ.3(Distribution-awareness). Figure3showstheperfor-
manceofanestimatordiscoveredfromasamplefromone
distribution(source)whenappliedtoanotherdistribution
8
ESMHowMuchisUnseenDependsChieflyonInformationAbouttheSeen
References Mitchell,M. Anintroductiontogeneticalgorithms. MIT
press,1998.
Acharya,J.,Jafarpour,A.,Orlitsky,A.,andSuresh,A.T.
Optimalprobabilityestimationwithapplicationstopre- Orlitsky, A. and Suresh, A. T. Competitive distribution
diction and classification. In Shalev-Shwartz, S. and estimation: Why is good-turing good. In Cortes, C.,
Steinwart,I.(eds.),Proceedingsofthe26thAnnualCon- Lawrence, N., Lee, D., Sugiyama, M., and Garnett,
ferenceonLearningTheory,volume30ofProceedings R. (eds.), Advances in Neural Information Pro-
ofMachineLearningResearch,pp.764â€“796,Princeton, cessing Systems, volume 28. Curran Associates,
NJ,USA,12â€“14Jun2013.PMLR. Inc., 2015. URL https://proceedings.
neurips.cc/paper/2015/file/
Chao,A.Nonparametricestimationofthenumberofclasses
d759175de8ea5b1d9a2660e45554894f-Paper.
inapopulation. ScandinavianJournalofstatistics,pp.
pdf.
265â€“270,1984.
Orlitsky,A.,Santhanam,N.P.,andZhang,J. Alwaysgood
Chao,A.andJost,L.Coverage-basedrarefactionandextrap-
turing: Asymptotically optimal probability estimation.
olation: standardizing samples by completeness rather
Science,302(5644):427â€“431,2003. doi: 10.1126/science.
thansize. Ecology,9312:2533â€“47,2012.
1088284.
Drukh,E.andMansour,Y. Concentrationboundsforuni- Orlitsky, A., Suresh, A. T., and Wu, Y. Opti-
gramslanguagemodel. InShawe-Taylor,J.andSinger, mal prediction of the number of unseen species.
Y.(eds.),LearningTheory,pp.170â€“185,Berlin,Heidel- Proceedings of the National Academy of Sciences,
berg,2004.SpringerBerlinHeidelberg. ISBN978-3-540- 113(47):13283â€“13288, 2016. doi: 10.1073/pnas.
27819-1. 1607774113. URLhttps://www.pnas.org/doi/
abs/10.1073/pnas.1607774113.
Efron, B. and Thisted, R. Estimating the number of un-
sen species: How many words did shakespeare know? Painsky,A.Convergenceguaranteesforthegood-turingesti-
Biometrika,63(3):435â€“447,1976. ISSN00063444. URL mator.JournalofMachineLearningResearch,23(279):1â€“
http://www.jstor.org/stable/2335721. 37,2022. URLhttp://jmlr.org/papers/v23/
21-1528.html.
Good,I.J. Thepopulationfrequenciesofspeciesandthe
estimationofpopulationparameters.Biometrika,40(3-4): Valiant,G.andValiant,P. Instanceoptimallearningofdis-
237â€“264,1953. cretedistributions. InProceedingsoftheForty-Eighth
AnnualACMSymposiumonTheoryofComputing,STOC
Good, I. J. and Toulmin, G. H. The number of new
â€™16,pp.142â€“155,NewYork,NY,USA,2016.Associa-
species,andtheincreaseinpopulationcoverage,when
tionforComputingMachinery. ISBN9781450341325.
asampleisincreased. Biometrika,43(1/2):45â€“63,1956. doi: 10.1145/2897518.2897641. URLhttps://doi.
ISSN 00063444. URL http://www.jstor.org/ org/10.1145/2897518.2897641.
stable/2333577.
Vargha,A.andDelaney,H.D. Acritiqueandimprovement
Hao, Y. and Li, P. Optimal prediction of the number of the cl common language effect size statistics of mc-
of unseen species with multiplicity. In Larochelle, grawandwong. JournalofEducationalandBehavioral
H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, Statistics,25(2):101â€“132,2000.
H. (eds.), Advances in Neural Information Processing
Systems, volume 33, pp. 8553â€“8564. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/file/
618790ae971abb5610b16c826fb72d01-Paper.
pdf.
Juang, B.-H. and Lo, S. On the bias of the turing-good
estimateofprobabilities. IEEETransactionsonsignal
processing,2(42):496â€“498,1994.
McAllester,D.andSchapire,R.E. Ontheconvergencerate
ofgood-Turingestimators. InProceedingsofthe13th
AnnualConferenceonComputationalLearningTheory,
pp.1â€“6.MorganKaufmann,SanFrancisco,2000.
9HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
A.ComparingtheBiasoftheEstimators
InSection3.1,wehaveshownthatthebiasofasimplervariantofGT,MË†Gâ€² = k+1Î¦ ,islargerbyanexponentialfactor
k nâˆ’k k+1
thantheabsolutebiasofourminimalbiasestimatorMË†B. Inthissection,weshowthatthebiasoftheoriginalGTestimator
k
MË†G = k+1Î¦ isalsolargerbyanexponentialfactorthantheabsolutebiasofMË†B forasufficientlylargersamplesize.
k n k+1 k
Recallthat
Bias
=E(cid:104)
MË†Gâˆ’M
(cid:105)
=
k+1
f
(n)âˆ’(cid:18) n(cid:19)
g
(n+1)=(cid:88)(cid:18) n(cid:19)
pk+2(1âˆ’p )nâˆ’kâˆ’1, (16)
Gâ€² k k nâˆ’k k+1 k k+1 k x x
x
and
(cid:104) (cid:105) (cid:18) n(cid:19) k(k+1)
Bias =E MË†Gâˆ’M = g (n+1)âˆ’ f (n) (17)
G k k k k+2 n(nâˆ’k) k+1
(cid:18) (cid:19) (cid:18) (cid:19)
n nâˆ’1
= g (n+1)âˆ’ g (n) (18)
k k+2 kâˆ’1 k+1
(cid:18)(cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
(cid:88) n 1 nâˆ’1
= pk+2(1âˆ’p )nâˆ’kâˆ’1 âˆ’ Â· (19)
x x k p kâˆ’1
x
x
(cid:18) (cid:19) (cid:18) (cid:19)
(cid:88) n k
= pk+2(1âˆ’p )nâˆ’kâˆ’1 1âˆ’ (20)
k x x nÂ·p
x
x
(cid:18) (cid:19)
k
â‰¥ 1âˆ’ Bias , (21)
nÂ·p
Gâ€²
max
where1âˆ’ k >0whennissufficientlylarge. Aboveinequalityleadstothefollowing:
nÂ·pmin
Bias (cid:18) k (cid:19) Bias Spk+2 (cid:18) 1âˆ’p (cid:19)âˆ’n+k+1
G â‰¥ 1âˆ’ , while B â‰¤ max min , (22)
Bias
Gâ€²
nÂ·p
max
Bias
Gâ€²
pk m+ in2 p
max
whichprovesourclaim.
B.BoundingtheVarianceandtheMSEofMË†B
k
TheMSEofanestimatoreË†foranestimandeisdefinedasMSE(eË†) = E[(eË†âˆ’e)2] = Var(eË†)+Bias2(eË†). Aswehave
shownthebiasofMË†B inSection3.1,theremainingparttocomputetheMSEofMË†B istocomputeitsvariance.
k k
Thevarianceofthelinearcombinationofrandomvariablesisgivenby
(cid:32) (cid:33)
(cid:88) (cid:88) (cid:88)
Var c X = c2Var(X )+ c c Cov(X ,X ). (23)
i i i i i j i j
i i iÌ¸=j
Therefore,thevarianceandthecovarianceofÎ¦ (n)sarethemissingpiecestocomputethevarianceofMË†B.
i k
TheoremB.1. Giventhemultinomialdistributionp=(p ,...,p )withsupportsizeS,thevarianceofÎ¦ =Î¦ (n)from
1 S i i
nsamplesXnisgivenby
(cid:40) f (n)âˆ’f (n)2+(cid:80) n! pipi(1âˆ’p âˆ’p )nâˆ’2i if2iâ‰¤n,
Var(Î¦ (n))= i i xÌ¸=y i!2(nâˆ’2i)! x y x y (24)
i f (n)âˆ’f (n)2 otherwise.
i i
10HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
Proof.
Var(Î¦ )=E(cid:2) Î¦2(cid:3) âˆ’E[Î¦ ]2 (25)
i i i
ï£®(cid:32) (cid:33)2ï£¹
E(cid:2) Î¦2 i(cid:3) =E ï£° (cid:88) 1(N x =i) ï£» (26)
x
ï£® ï£¹
(cid:88) (cid:88)
=E ï£° 1(N x =i)+ 1(N x =iâˆ§N y =i)ï£» (27)
x xÌ¸=y
(cid:40) f (n)+(cid:80) n! pipi(1âˆ’p âˆ’p )nâˆ’2i if2iâ‰¤n,
= i xÌ¸=y i!2(nâˆ’2i)! x y x y (28)
f (n) otherwise.
i
(cid:40) f (n)+(cid:80) n! pipi(1âˆ’p âˆ’p )nâˆ’2iâˆ’f (n)2 if2iâ‰¤n,
âˆ´Var(Î¦ )= i xÌ¸=y i!2(nâˆ’2i)! x y x y i (29)
i f (n)âˆ’f (n)2 otherwise.
i i
NowwecomputetheupperboundofthevarianceofMË†B.
k
LemmaB.2.
(cid:40)
â‰¤Sf (n)âˆ’f (n)2 if2iâ‰¤n.
Var(Î¦ ) i i (30)
i =f (n)âˆ’f (n)2 otherwise.
i i
Proof. FromTheoremB.1,
ï£® ï£¹
E(cid:2) Î¦2 i(cid:3) =f i(n)+E ï£°(cid:88) 1(N
x
=iâˆ§N
y
=i)ï£» (31)
xÌ¸=y
(cid:34) (cid:35)
(cid:88)
â‰¤f (n)+(Sâˆ’1)E 1(N =i) (32)
i x
x
=Sf (n) (if2iâ‰¤n). (33)
i
(34)
Thelemmadirectlyfollowsfromtheaboveinequality.
LemmaB.3.
g (n)â‰¤SÂ·Î²âˆ’noi ,
i min max
whereS =|X|,p =max p ,Î² = 1 ,ando = pmax .
max xâˆˆX x min 1âˆ’pmin max 1âˆ’pmax
Proof. 1 and x areincreasingfunctionsforxâˆˆ(0,1). Therefore,
1âˆ’x 1âˆ’x
g (n)= (cid:88) pi(1âˆ’p )nâˆ’i =
(cid:88)(cid:18)
1
(cid:19)âˆ’n(cid:18)
p x
(cid:19)i
â‰¤|X|Â·Î²âˆ’noi .
i x x 1âˆ’p 1âˆ’p min max
x x
xâˆˆX xâˆˆX
TheoremB.4. ThevarianceoftheestimatorMË†B isboundedasfollows:
k
Var(MË†B)â‰¤c Â·n2k+1Â·câˆ’n,
k 1 2
wherec =SÂ·(cid:0)e(cid:1)2k ,c =min(cid:16) 1 , 1âˆ’pmax (cid:17) . Inotherwords,Var(MË†B)=O(n2k+1Â·Î²âˆ’n Â·max(1,on )).
1 k 2 1âˆ’pmin pmax(1âˆ’pmin) k min max
11HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
Proof. FromLemmaB.2inthesupplementary,wehaveVar(Î¦ )â‰¤Sf (n)âˆ’f (n)2. Thus,
i i i
(cid:32) (cid:33)
Î¦ Sf (n)âˆ’f (n)2 g (n) S2Â·Î²âˆ’nok+i
Var k+i â‰¤ k+i k+i â‰¤SÂ· k+i â‰¤ min max (byLemmaB.3)
(cid:0) n (cid:1) (cid:0) n (cid:1)2 (cid:0) n (cid:1) (cid:0) n (cid:1)
k+i k+i k+i k+1
(cid:16) (cid:17)k
(cid:18) n(cid:19)2 Var(cid:32)
Î¦
k+i(cid:33)
â‰¤SÎ²âˆ’n
(cid:0)n k(cid:1)2
ok+i â‰¤SÎ²âˆ’n
e2 kn 22
ok+i â‰¤SÎ²âˆ’n
(cid:18) e2n(k+i)(cid:19)k
ok+i
k (cid:0) n (cid:1) min(cid:0) n (cid:1) max min (cid:16) (cid:17)k max min k2 max
k+i k+1 n
k+i
(cid:18) e2n2(cid:19)k (cid:16)en(cid:17)2k
â‰¤SÎ²âˆ’n O =SÎ²âˆ’n O , whereO =max(ok ,on ),
min k2 M min k M M max max
Var(MË†B)=(cid:18) n(cid:19)2 Var(cid:32)n (cid:88)âˆ’k
(âˆ’1)iâˆ’1Î¦
k+1(cid:33)
k k (cid:0) n (cid:1)
i=1 k+1
(cid:18) n(cid:19)2 (cid:32)
Î¦
(cid:33)
â‰¤(nâˆ’k) Var k+1 (35)
k (cid:0) n (cid:1)
k+1
(cid:16)en(cid:17)2k
â‰¤S(nâˆ’k) Â·Î²âˆ’nO =O(n2k+1)Â·Î²âˆ’nO ,
k min M min M
where(35)followsfromCauchy-Schwarzinequality(Var((cid:80)M
X ) â‰¤ M
Â·(cid:80)M
Var(X )). Theprooffollowsfrom
j=1 i j=1 i
dividing the variance of the estimator into two cases: o < 1 and o > 1: If o < 1, O = ok , and
max max max M max
Var(MË†B)=O(n2k+1Î²âˆ’n). Ifo >1,O =on ,and,Var(MË†B)=O(n2k+1Î²âˆ’non ).
k min max M max k min max
Therefore,thevarianceexponentiallydecreaseswithnifp <0.5or 1âˆ’pmax <1.
max pmax(1âˆ’pmin)
CorollaryB.5. Thereexistsaconstantc>1suchthat
MSE(MË†B)â‰¤O(n2k+1câˆ’n).
k
Proof. FromEqu.(7)inthemanuscript,thebias|E(MË†B)âˆ’M |â‰¤SÂ·p Â·nkÂ·pn . Theprooffollowsfromthefact
k k max max
thatMSE=Var+Bias2andtheboundofthevarianceandthebias.
C.ComputingtheVarianceandtheMSEoftheEvolvedEstimators
SameasMË†B,theevolvedestimatorsfromthegeneticalgorithmarealsolinearcombinationsofÎ¦ (n)s(whilevarying
k k
bothk andnunlikeMË†B). GiventheevolvedestimatorMË†Evo = (cid:80) c Î¦ (n ),theexpectedvalueofMË†Evo isgivenby
k k i i ki i k
substitutingÎ¦ (n)withf (n):
k k
E(MË†Evo)=(cid:88)
c f (n ). (36)
k i ki i
i
Giventhemultinomialdistributionp,thecovariancebetweenÎ¦ (n)andÎ¦ (nâ€²),whichisneededtocomputethevariance
k kâ€²
ofMË†EvoasEqu.(23),canbecomputedasfollows:
k
TheoremC.1. Giventhemultinomialdistributionp = (p 1,...,p S)withsupportsizeS,letXntotal bethesetofn
total
samplesfromp. LetXn andXnâ€² bethefirstnandnâ€² samplesfromXntotal,respectively;WLOG,weassume1â‰¤nâ€² â‰¤
n â‰¤ n . Then,thecovarianceofÎ¦ (n) = Î¦ (Xn)andÎ¦ (nâ€²) = Î¦ (Xnâ€²)(1 â‰¤ k â‰¤ n,1 â‰¤ kâ€² â‰¤ nâ€²)isgivenby
total k k kâ€² kâ€²
following:
Cov(Î¦ (n),Î¦ (nâ€²))=E[Î¦ (n)Â·Î¦ (nâ€²)]âˆ’f (n)Â·f (nâ€²) (37)
k kâ€² k kâ€² k kâ€²
(cid:34)(cid:32) (cid:33) (cid:32) (cid:33)(cid:35)
(cid:88) (cid:88)
=E 1(N =k) Â· 1(Nâ€² =kâ€²) âˆ’f (n)Â·f (nâ€²) (38)
x xâ€² k kâ€²
x xâ€²
(cid:88)(cid:88)
= E[1(N =kâˆ§Nâ€² =kâ€²)]âˆ’f (n)Â·f (nâ€²), (39)
x xâ€² k kâ€²
x xâ€²
12HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
where Nâ€² is the number of occurrences of xâ€² in Xnâ€². Depending on the values of n,nâ€²,k,kâ€²,x, and xâ€², the
xâ€²
E[1(N =kâˆ§Nâ€² =kâ€²)]canbecomputedasfollows:
x xâ€²
âˆ€n,nâ€²s.t. âˆ€x,xâ€²s.t. âˆ€k,kâ€²s.t. E[1(N x=kâˆ§Nâ€²
xâ€²
=kâ€²)]
k=kâ€² (cid:0)n(cid:1) pk(1âˆ’p )nâˆ’k
x=xâ€² k x x
n=nâ€² kÌ¸=kâ€² 0(infeasible)
xÌ¸=xâ€²
k+kâ€²â‰¤n k!kâ€²!(nn âˆ’! kâˆ’kâ€²)!pk xpk xâ€² â€²(1âˆ’p xâˆ’p xâ€²)nâˆ’kâˆ’kâ€²
k+kâ€²>n 0(infeasible)
kâ€²â‰¤k (cid:0)nâ€²(cid:1) pkâ€²(1âˆ’p )nâ€²âˆ’kâ€²Â·(cid:0)nâˆ’nâ€²(cid:1) pkâˆ’kâ€²(1âˆ’p )(nâˆ’nâ€²)âˆ’(kâˆ’kâ€²)
x=xâ€² kâ€² x x kâˆ’kâ€² x x
nÌ¸=nâ€² kâ€²>k 0(infeasible)
xÌ¸=xâ€²
k+kâ€²â‰¤n (cid:80)m i=in m( ak x,n (0âˆ’ ,kk âˆ’â€²) (nâˆ’nâ€²))kâ€²!i!(nn â€²âˆ’â€²! kâ€²âˆ’i)!(kâˆ’i)!((( nn âˆ’âˆ’ nn â€²â€² )) âˆ’! (kâˆ’i))!pk xâ€² â€²pk xâ€² â€²(1âˆ’p xâˆ’p xâ€²)nâ€²âˆ’kâ€²âˆ’ipk x(1âˆ’p x)(nâˆ’nâ€²)âˆ’(kâˆ’i)
k+kâ€²>n 0(infeasible)
Proof. TheproofisstraightforwardfromthedefinitionofN andNâ€² .
x xâ€²
GiventheexpectedvalueandthevarianceofMË†Evo,theMSEofMË†Evonaturallyfollows.
k k
D.DetailsoftheHyperparametersoftheEvolutionaryAlgorithm
ForevaluatingAlgorithm1,weusethefollowinghyperparameters:
â€¢ SameastheOrlitskyâ€™sstudy(Orlitsky&Suresh,2015),whichassesstheperformanceoftheGood-Turingestimator,
weusethehybridestimatorpË†oftheempiricalestimateandtheGood-Turingestimatetoapproximatetheunderlying
distribution{p } forestimatingtheMSEoftheevolvedestimator. ThehybridestimatorpË†isdefinedasfollows: If
x xâˆˆX
N =k,
x
(cid:40)
cÂ· k ifk <Î¦ ,
pË† = N k+1
x
cÂ·
MË† kG
otherwise,
Î¦k
(cid:80)
wherecisanormalizationconstantsuchthat pË† =1.
xâˆˆX x
â€¢ The number of generations G = 100. To avoid the algorithm from converging to a local minimum, we limit the
maximumnumberofgenerationstobe2000.
â€¢ Themutantsizem=40.
â€¢ Whenselectingtheindividualsforthemutation,weusetournamentselectionwithtournamentsizet = 3,i.e.,we
randomlychoosethreeindividualswithreplacementandselectthebestone,andrepeatthisprocessmtimes.
â€¢ Whenchoosingthetopthreeindividualswhenconstructingthenextgeneration,weuseelitistselection,i.e.,choosing
thetopthreeindividualswiththesmallestfitnessvalues.
â€¢ Toavoidtheestimatorfrombeingtoocomplex,welimitthemaximumnumberoftermsintheestimatortobe20.
TheactualscriptimplementingAlgorithm1canbefoundatthepublicallyavailablerepository
https://anonymous.4open.science/r/Better-Turing-157F.
13HowMuchisUnseenDependsChieflyonInformationAbouttheSeen
E.AdditionalExperimentalResults
Table2.TheMSEoftheGood-TuringestimatorMË†G,minimalbiasestimatorMË†B,andthebestevolvedestimatorMË†Evo andforthe
0 0 0
missingmassM ,thesuccessrateAË† oftheevolvedestimator(X )againsttheGood-Turingestimator(X ),andtheratio(Ratio,
0 12 2 1
MSE(MEvo)/MSE(MË†G))fortwosupportsizesS =100and200,threesamplesizesnandsixdistributions.
0 0
S n/S Distribution MSE(MË†B) MSE(MË†G) MSE(MË†evo) AË† Ratio
0 0 0 12
uniform 6.834e-03 6.681e-03 6.267e-03 62% 93%
half&half 6.821e-03 6.694e-03 4.489e-03 85% 67%
zipf-0.5 6.676e-03 6.565e-03 3.311e-03 94% 50%
0.5
zipf-1 4.995e-03 4.943e-03 3.065e-03 98% 62%
diri-1 6.166e-03 6.086e-03 3.202e-03 96% 52%
diri-0.5 5.223e-03 5.167e-03 2.708e-03 100% 52%
uniform 2.365e-03 2.351e-03 1.905e-03 88% 81%
half&half 2.200e-03 2.190e-03 1.439e-03 97% 65%
100 zipf-0.5 2.207e-03 2.194e-03 1.982e-03 75% 90%
1.0
zipf-1 1.713e-03 1.704e-03 1.705e-03 75% 100%
diri-1 1.787e-03 1.778e-03 1.066e-03 100% 60%
diri-0.5 1.388e-03 1.381e-03 8.747e-04 97% 63%
uniform 4.047e-04 4.028e-04 3.428e-04 89% 85%
half&half 4.237e-04 4.221e-04 3.035e-04 97% 71%
zipf-0.5 4.580e-04 4.561e-04 3.321e-04 95% 72%
2.0
zipf-1 4.826e-04 4.810e-04 3.633e-04 98% 75%
diri-1 3.946e-04 3.932e-04 2.473e-04 100% 62%
diri-0.5 3.276e-04 3.264e-04 2.587e-04 87% 79%
uniform 3.361e-03 3.323e-03 2.044e-03 95% 61%
half&half 3.357e-03 3.326e-03 1.968e-03 96% 59%
zipf-0.5 3.254e-03 3.227e-03 2.293e-03 89% 71%
0.5
zipf-1 2.335e-03 2.324e-03 2.410e-03 74% 103%
diri-1 3.011e-03 2.992e-03 2.359e-03 85% 78%
diri-0.5 2.563e-03 2.550e-03 1.813e-03 94% 71%
uniform 1.172e-03 1.169e-03 8.900e-04 99% 76%
half&half 1.092e-03 1.090e-03 8.584e-04 99% 78%
200 zipf-0.5 1.091e-03 1.088e-03 8.525e-04 97% 78%
1.0
zipf-1 8.185e-04 8.165e-04 7.244e-04 88% 88%
diri-1 8.898e-04 8.876e-04 6.652e-04 100% 74%
diri-0.5 6.900e-04 6.882e-04 4.861e-04 98% 70%
uniform 2.017e-04 2.012e-04 1.702e-04 93% 84%
half&half 2.113e-04 2.109e-04 1.716e-04 100% 81%
zipf-0.5 2.307e-04 2.302e-04 1.929e-04 100% 83%
2.0
zipf-1 2.390e-04 2.387e-04 2.109e-04 96% 88%
diri-1 1.961e-04 1.958e-04 1.648e-04 96% 84%
diri-0.5 1.609e-04 1.607e-04 1.315e-04 93% 81%
14