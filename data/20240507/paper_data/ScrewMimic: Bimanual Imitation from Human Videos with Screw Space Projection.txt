S M : Bimanual Imitation from
CREW IMIC
Human Videos with Screw Space Projection
Arpit Bahety, Priyanka Mandikal, Ben Abbatematteo, Roberto Mart¬¥ƒ±n-Mart¬¥ƒ±n
The University of Texas at Austin
Abstract‚ÄîBimanual manipulation is a longstanding challenge
in robotics due to the large number of degrees of freedom
and the strict spatial and temporal synchronization required to
generate meaningful behavior. Humans learn bimanual manip-
ulation skills by watching other humans and by refining their
abilities through play. In this work, we aim to enable robots
to learn bimanual manipulation behaviors from human video
demonstrationsandfine-tunethemthroughinteraction.Inspired
by seminal work in psychology and biomechanics, we propose
modelingtheinteractionbetweentwohandsasaserialkinematic
linkage ‚Äî as a screw motion, in particular, that we use to
define a new action space for bimanual manipulation: screw
actions. We introduce SCREWMIMIC, a framework that lever-
ages this novel action representation to facilitate learning from
humandemonstrationandself-supervisedpolicyfine-tuning.Our
experiments demonstrate that SCREWMIMIC is able to learn
several complex bimanual behaviors from a single human video
demonstration, and that it outperforms baselines that interpret
demonstrations and fine-tune directly in the original space of
motion of both arms. For more information and video results,
Fig. 1. Bimanual manipulation tasks can be represented by a screw
https://robin-lab.cs.utexas.edu/ScrewMimic/
axis (red line) constraining and synchronizing the motion of both hands.
SCREWMIMIC maps a single human demonstration into a screw axis,
I. INTRODUCTION improvesitwithaniterativeinteractiveexplorationprocedure,andlearnsto
predictitfornewobjectinstancesandposes,enablingtheirmanipulation.
Manipulation in human environments often requires co-
ordinating the motion of two arms, e.g., opening a bottle, bimanual interaction may not be successful, necessitating an
cutting a block in two pieces, or stirring a pot. In dexterous exploratoryrefinementtoadapttotherobot‚Äôsembodimentand
bimanual manipulation, the agent has to generate behavior capabilities, which reintroduces the challenges of exploring
for both arms that are synchronized spatially and temporally, directly in the space of motion of both arms.
rendering it even more complex to generate than two in- The main insight in this work is that for many bimanual
dependent unimanual manipulations. Due to its complexity, manipulation tasks, the relative motion between hands can be
in nature, this kind of behavior is almost unique to higher- explained by a simple one-degree-of-freedom (1-DoF) screw
level primates [1, 2, 3, 4], and it requires several years to joint. This virtual joint constrains the motion in a way that
fully develop in humans [5, 6], being mastered only after matches an existing physical constraint in the environment
a significant amount of time of observing expert bimanual (e.g., when opening a laptop or a bottle with both hands) or
agents and practicing through trial-and-error. This work aims just facilitates the manipulation (e.g., when cutting a block or
to endow robots with novel capabilities to learn bimanual stirring a pot, see Fig. 1). The type of 1-DoF screw joint ‚Äî
manipulation tasks. prismatic, revolute, screw‚Äî captures different modalities of
Learning to generate dexterous bimanual manipulation in bimanualmanipulation,whilethescrewjointparametersfully
robots is challenging due to the large state and action spaces specify the motion. This insight works at several levels: in
resulting from the two arms, and the strict requirements of perception, it serves as a prior for interpreting noisy sensor
spatialandtemporalsynchronizationbetweenthemtoachieve signals and facilitates understanding a human-demonstrated
success[7,8,9].Asaresult,exploringrandomlyinthisspace bimanual manipulation. And, in exploration, it provides an
is prohibitively difficult, especially on real robot hardware, action space where both arm motions are coordinated by
limiting some of the successes to simulation [10, 11, 12, 13]. design, allowing efficient action fine-tuning to find successful
A promising approach to reduce the challenge of searching behaviors with the real robot‚Äôs embodiment.
for a successful bimanual manipulation policy is to observe We present a novel method, SCREWMIMIC, that leverages
a human performing a bimanual manipulation and imitate this insight for one-shot visual imitation learning of bimanual
it. However, due to the morphology differences between the manipulation from a human demonstration. Our method uses
human and the robot, the direct execution of the observed a single demonstration as input as an RGB-D video of a hu-
4202
yaM
6
]OR.sc[
1v66630.5042:viXramanperformingabimanualmanipulationtask.SCREWMIMIC gap which is exacerbated in contact-rich manipulation tasks
interprets the demonstration as a screw motion between both with complicated dynamics [38]. An alternative approach is
hands and uses the perceived bimanual grasp and virtual joint to employ movement primitives [39, 40, 41, 42, 43, 44, 45,
to train a prediction model on 3D point clouds. This model 46,47,48,49,50,51]whichreducethesearchspacebutlimit
predicts full bimanual manipulation behaviors composed of expressiveness and typically require substantial engineering
bimanual grasping strategies and two-arm relative motion in a effort. Recently, Grannen et al. [52] proposed a stabilizing-
possiblymovingreferenceframe,fornovelviewsoftheobject. acting bimanual manipulation framework where the stabiliz-
These predictions form the starting hypothesis for a self- ing hand is trained using human annotations and the acting
practicingiterativeprocess.Here,therobotengagesinbiman- hand is trained using kinesthetic demonstrations. In contrast,
ual interactions, learning to overcome morphological differ- SCREWMIMIC leverages a novel action representation that
ences by optimizing a reward signal generated autonomously, efficiently learns bimanual manipulation policies given only
resulting in successful bimanual manipulation strategies. The a single human video demonstration, and can correct failed
new strategy can then be used in a self-improving loop to actions through a self-supervised policy fine-tuning method.
retrainabetterpredictionmodelthatisalsoabletogeneralize
b) VisualImitationLearning: Recentworkhassoughtto
to new instances of the same object class thanks to a set of
imbue robots with the ability to learn from large collections
geometric augmentations.
of unstructured human videos like Ego4D [53] or YouTube
We demonstrate the performance of our solution in six
videos [54, 55]. Some works have proposed learning cost
challenging bimanual manipulation tasks involving different
functions from video and language data [56, 57, 58, 59],
types of screw motion between both hands, both in objects
whereas others propose pretraining objectives [60, 61]. More
with physical kinematic constraints and in tasks where the
direct approaches generally track human hands in videos (e.g.
constraints need to be virtually created by the agent. Our
with FrankMocap [62]), mapping the hand trajectories to the
experiments indicate that the projection into the screw-axis
robot‚Äôs action space [63, 64, 65, 66]. A common approach in
space is a robust representation for bimanual manipulation‚Äî
these works is to structure video understanding by modelling
leadingtosampleefficientexplorationandstrongperformance
manipulation using affordances (i.e. detecting contact points
in executing bimanual tasks.
[67])andsubsequentinteractiontrajectories.Sincetherobot‚Äôs
embodiment differs from a human demonstrator and tracking
II. RELATEDWORK
isgenerallynoisy,interactivefine-tuningisgenerallynecessary
SCREWMIMICisanovelsolutiontogenerateandrefineau-
to obtain a reliable behavior policy [68, 69, 70]. DEFT [66],
tonomousrobotbimanualmanipulationbehaviorbootstrapped
for example, trains an affordance prediction model on large-
with a single video of a human demonstration. In the follow-
scaledataandobtainstheinteractiontrajectorygivenahuman
ing,wecontrastSCREWMIMICtothemostrelevantpriorwork
demonstration at test time. This trajectory is then refined
in robot bimanual manipulation and visual imitation learning.
through interaction. Inspired by this line of work, we propose
a) Bimanual Manipulation: Early on, robotics re-
a novel formulation of synchronized bimanual manipulation
searchersacknowledgedtheneedforbimanualmanipulatorsto
learnedsolelybywatchinghuman-objectinteractionsinvideo.
solve tasks in unstructured environments [14, 15, 16]. Gener-
Incontrasttopriorworkinunimanualmanipulation,ourfocus
ating coordinated behavior for both arms became a significant
hereisontheactionrepresentation.Ouruniqueformulationof
challenge[8,17]thatresearchershaveattemptedtosolvewith
bimanual motion in terms of screw joints abstracts complex
planning[7,18,19],control[20],reinforcementlearning[21],
high-DoF manipulation into a unified framework‚Äîenabling
and imitation learning [22, 23]. To generate bimanual ma-
efficient imitation learning from video.
nipulation behavior, these solutions have to explore a large
action space with strict temporal and spatial synchronization.
A common strategy is to coordinate behavior using stable
III. PRELIMINARIES:SCREWTHEORY
static postures or keypoints [24, 25, 26], or with explicit
spatial or temporal constraints typically extracted via kines-
thetic teaching [27, 28, 29, 30]. Oftentimes, these approaches SCREWMIMIC models bimanual manipulation as a screw
necessitate specialized teleoperation hardware like custom motion between the two hands. Chasles‚Äô theorem states that
devices [31, 32, 33] or motion capture [34, 35] that limit any rigid body motion can be written as the composition of
their scalability and availability. In contrast, SCREWMIMIC a rotation of the body about a unique line in space and a
uses a single RGB-D video of a human demonstration, which translation along the same line. This line is referred to as the
ischeapertoacquire,scalableanddoesnotrequirecontrolling screw axis of that motion. A screw axis S can be represented
a robot. as (q,sÀÜ,h) where q ‚ààR3 is any point on the axis, sÀÜ‚ààR3 is a
Given the large action space and difficulty of exploration, unitvectorinthedirectionoftheaxis,andh‚ààR + isthepitch
reinforcement learning approaches to bimanual manipulation ofthescrew,definingtheratiooflinearmotionalongthescrew
are prohibitively costly to train on real robot hardware. axis to the rotational motion around the screw axis [72, 73].
As an alternative, researchers have explored sim-to-real ap- Assuming some angular displacement Œ∏ ‚ààR along a screw
proaches[12,36,37].Theseapproachessufferfromthereality axis S, the corresponding rigid body motion in exponentiala) Extracting Screw Action from a Human Demo b) Predicting a Screw Action from a Point Cloud
Contact Point Extraction
HT
Wrist Pose Axis
Extraction Extraction ( ,( )) ‚Ä¶ Training
Human Video Geometric Data ùùà Prediction
Demo (RGBD) Hand Poses Augmentations Model Predicted ùùà
c) Self-Supervised Screw-Action Policy Fine-Tuning
Self
Generated
Reward
Reward- Sampled
Weighted Strategy Exploration
Axis Sampling Rollouts
Robot ùùà Prediction Successful
New Trial Observation Model Predicted ùùà Initial Strategy CEM-Based Fine-Tuning Strategy Successful ùùà
Fig. 2. Overview of SCREWMIMIC. a) Given an RGB-D video of a human performing a bimanual task, we use off-the-shelf hand tracking (HT)
models [62, 67] to extract a trajectory of wrist poses œÑh and grasp contact points (g lh,g rh). SCREWMIMIC interprets œÑh as a screw motion between both
handstoestimatescrewaxisparametersSh (Sec.IV-A).b)Next,weapplygeometricaugmentationsonthe3DobjectpointcloudtotrainaPointNet[71]
modeltoestimatescrewactionsfornovelobjectviews(Sec.IV-B).c)Finally,thetrainedmodelgeneratesaninitialhypothesisthattherobotexecutesand
iterativelyrefinesusinganautonomouslygeneratedrewardsignal.Thesuccessfuldatapointisfurtherusedtoimprovethepredictionmodel(Sec.IV-C).
coordinates, Œæ =(œâ,v)‚ààR6 is given by for a given screw axis. In this work, we will consider three
(cid:20) (cid:21) (cid:20) (cid:21) screw types: pure translation (h = ‚àû, prismatic), pure
œâ sÀÜŒ∏
Œæ =SŒ∏ = = (1) rotation (h = 0, revolute), and rotation with a fixed
v ‚àísÀÜŒ∏√óq+hsÀÜŒ∏
orientation(revolute3D).Wedescribetheaxiscomputation
where œâ ‚àà R3 represents angular motion, and v ‚àà R3 and trajectory generation for each case in Sec. IV below.
represents linear motion. To transform this into a homoge-
neous transformation matrix, T ‚àà SE(3) (SE(3) the Spe-
IV. SCREWMIMIC:POLICYLEARNINGWITHSCREW
cial Euclidean Lie Group) we apply the matrix exponential:
ACTIONS
T =exp([S]Œ∏), where [S]Œ∏ ‚ààse(3) (se(3) is the Lie algebra) Seminal research in psychology and biomechanics [74]
is the matrix representation of the exponential coordinates: indicatesthatbimanualbehaviorinhumanscanbemodeledas
(cid:20) (cid:21) if‚Äúaserialkinematicchainwouldconnectbothhands‚Äù,where
[œâ] v
[S]Œ∏ = . (2) one hand (left) sets a spatial reference frame and the other
0 0
(right) moves relative to it. Inspired by this work, we propose
with [œâ] ‚àà so(3) is a skew-symmetric matrix representing a novel action space parametrization for robotic bimanual
orientation. manipulation that we call screw actions, that fully specifies
Conversely, given a rigid body transformation, T ‚ààSE(3), the behavior of both hands through a screw joint between the
we can compute the corresponding screw axis as follows. In hands. A screw action is defined as, œÉ = (g ,g ,S,œÑ ) in its
l r l
the case of a pure translation (h = ‚àû), the screw axis can mostgeneralform.g andg arethegrasping/placinglocations
l r
be recovered as sÀÜ pointing in the direction of linear motion, for left and right hands. S is a 1-DoF screw axis describing
and q is any point. In the case of pure rotation (h = 0), we the relative motion between left and right hands. Finally, œÑ
l
can recover the corresponding twist in matrix form [S] using is a possible sequence of left-hand pose changes during the
the matrix logarithm: [S]Œ∏ =log(T). Applying Eq. 1, we can interaction (e.g. moving a pot to the stove while stirring) that
obtain the screw axis parameters as can be empty if the left hand just fixates/stabilizes the object.
Given a screw action, the motion of both hands of the
œâ sÀÜ√óv
sÀÜ= , q = . (3) robot during the bimanual manipulation is fully specified.
||œâ|| ||œâ||
Our main hypothesis is that our new action space simplifies
For the general case with h‚àà/ {0,‚àû} and further details, we robot dexterous bimanual manipulation at two levels: first,
refer the reader to Lynch and Park [73]. it aids in learning from visual human demonstrations by
We will use the definitions above to infer a screw axis projecting noisy multi-hand motion into a simpler constrained
from a sequence of relative transformations between human space, and second, it facilitates fine-tuning the perceived
hands, and to generate relative motion between robot hands motions by providing a constrained space in which real-worldexploration is more efficient. We propose a novel solution,
SCREWMIMIC, that leverages this insight to learn bimanual
policies. SCREWMIMIC integratesthreemodules:aperceptual
module to interpret a single human demonstration as a screw
action, a prediction model that predicts screw actions based
on a point cloud of an object, and a self-supervised iterative
fine-tuning algorithm that explores in screw action space to
find optimal parameters for bimanual tasks. In the following,
we explain each of these modules in detail.
A. Extracting a Screw Action from a Human Demonstration
ThefirstmoduleofSCREWMIMIC(Fig.2a)parsesanRGB-
D video of a human demonstrating a bimanual task into a
suitable action representation for robot execution, in our case,
Fig. 3. Human demonstrations as screw actions. Three frames of a
a screw action œÉh = (g lh,g rh,Sh,œÑ lh) (h indicates human). human demonstration for three bimanual tasks (top row: opening a bottle,
SCREWMIMIC first extracts the grasping/placing location of m = revolute, middle row: stirring a pot, m = revolute3D, bottom
the human hands (gh and gh) using an off-the-shelf hand- row: opening a zipper, m = prismatic) and the perceived screw axis
l r explaining the motion (fourth column, orange indicates the axis line). Our
objectdetector[67],detectingthefirstintersectionofthehand screwactionrepresentationfacilitatestheinterpretationofnoisyhandtrajec-
and the object bounding boxes in the RGB image sequence, tory observations in a bimanual interaction as evidence of a simple 1-DoF
constraintbetweenbothhands
and projecting it into the 3D point cloud of the object, P,
using the information of the depth channel.
œÑh. To do this, SCREWMIMIC evaluates the likelihood of
SCREWMIMICthenextractsthe6-DoFtrajectories(position r
eachjointtype(m),bycomparingtheobserveddemonstration
and orientation) of the human hands using an off-the-shelf
trajectory œÑh with the trajectory computed based on m. This
hand-tracking solution (FrankMocap [62]) to detect the wrist r
comparison involves evaluating a score function that mea-
posesovertime,œÑh andœÑh.Adirectapproachwouldusethese
l r sures the distance between the two trajectories, considering
trajectoriestoimitateandfine-tunethebimanualmanipulation.
both positional and angular differences at each waypoint. A
However,theoriginaltrajectoriescontainnoisefromthevisual
lowerdistanceindicatesgreatersimilaritybetweentrajectories.
tracker, the motion of both hands is not constrained to be
SCREWMIMICpicksthejointtypewiththehighestlikelihood.
synchronized, and an embodiment gap exists between the
Examples of the extracted screw axes for each type are
humanhandandtherobotgripper,makingithardertoimitate
depicted in Figure 3.
andfine-tune(asshowninSec.V); SCREWMIMIC overcomes
these limitations by interpreting the trajectories as a screw
B. Predicting a Screw Action from a Point Cloud
action.
Inspired by models of human bimanual manipulation [74], Once the robot perceives the human demonstration of the
we assign an acting and reference role to the right and bimanualtaskandrepresentsitinthescrewactionspace,how
left hands, respectively, keeping œÑh as the trajectory of the can it generalize to novel object instances and configurations?
l
left hand. SCREWMIMIC finds then the screw axis Sh by To tackle this, the second module of SCREWMIMIC (Fig. 2b)
transforming œÑh to the left hand reference frame and ana- includes a PointNet [71] based model trained to predict the
r
lyzing the left-right relative motion to obtain the screw-joint screw action from an object‚Äôs point cloud. Concretely, given
type, m, and parameters, sÀÜ and q. For that, SCREWMIMIC an RGB-D observation, we use MDETR [75] to segment out
assumes three possible screw joint types, prismatic, revolute the object and extract its partial point cloud, P. The goal is
and revolute with fixed orientation, as explained in Sec. III. to learn a perception model M : P (cid:55)‚Üí œÉM = (gM,gM,SM)
l r
Assuming m = prismatic, SCREWMIMIC obtains the (M indicates Model). Here g lM,g rM refer to the grasp contact
screw parameters by fitting a 3D line to the trajectory of the points predicted by the perception model for the left and the
rightwrist.Whenm=revolute,thescrewaxisparameters right grippers respectively and SM refers to the predicted
can be obtained by transforming each pose of the right wrist screw axis. From here on, we omit the left-hand trajectory, œÑ ,
l
relative to the left wrist to exponential coordinates using the since our experiments focus on learning the relative motion
matrixlogarithm,applyingEq.3,andaveragingtheresultingsÀÜ between hands, but the left-hand motion is enabled by our
andq.Finally,ifm=revolute3D, SCREWMIMIC firstfits general formalism, as shown in additional trials in Appendix
a plane to the trajectory of the right wrist; the normal to the A and website.
plane provides, sÀÜ. The right wrist trajectory is then projected SCREWMIMIC benefits from the 3D nature of both the
onto the plane and SCREWMIMIC fits a circle to it; the center input observation and screw action representation that al-
of the circle provides q. We employ a Maximum a Posteriori lows for straightforward geometric augmentations of the data:
Estimation (MAP) method to determine the screw joint type translation, rotation, and scaling. These augmentations are
(m) corresponding to a human demonstration. In this case, applied to the point cloud, P, and corresponding robot action
we want to estimate m based on the hand pose observations, œÉM. As a result, we generate an extensive training datasetfrom just a single human demonstration. Using PointNet [71] to sample screw axis parameters around the initial screw
as the backbone, we construct two networks: a regression axis. E samples, Œæ , are drawn from this distribution. CEM
1,e
network trained with MSE loss to predict the axis and a then requires a reward to score each sample and guide a
segmentationnetworktrainedwithnegativeloglikelihoodloss reweighting of the sampling distribution (D) for the next
foridentifyingcontactpoints.Wetraintask-specificprediction epoch. In SCREWMIMIC, the CEM optimization process is
models. The training of each model is efficient, requiring on self-supervised through an autonomously generated reward
average 40 minutes for 2000 epochs on a RTX 4090 GPU. based on the length of the episode and the amount of force
This rapid training cycle enables quick model refinement and employed,measuredbyaforce-torque(FT)sensorintheright
incorporation of new data, as we will discuss in the next hand‚Äôs wrist. Concretely, after each epoch, all the trajectories
section. up to that epoch are ranked by their length: the longer an
episode runs without failure, the better it is. We implement
C. Self-Supervised Screw-Action Policy Fine-Tuning
three self-detected failures: 1) when the robot is not applying
Given that human pose tracking is inherently noisy, the enoughforce(normofthewrenchsignalisbelowathreshold),
prediction model trained on the human hand trajectory will indicating that it may be moving in free space instead of
necessarily have some error. Thus, if the robot directly ex- manipulating, 2) when the robot is applying too much force
ecutes the bimanual manipulation defined by the predicted (norm of the wrench signal is above a threshold), indicating
screw action, it will probably fail (as evidenced in our ex- that it is trying to manipulate an object in the wrong way,
periments). Nevertheless, the predicted screw action produces and 3) when the robot loses grasp (measured by the finger
a behavior close to a successful manipulation and thus can proprioception). After all the episodes are ranked by their
be used as initialization for a fine-tuning procedure through lengths, SCREWMIMIC takes the top T trajectories and ranks
interaction. The third module of SCREWMIMIC consists of thembythemeanwrenchesemployedovertheepisode;using
a self-supervised policy improvement algorithm that refines lower force for the manipulation is considered more efficient.
the noisy screw action (Fig. 2c). As our experiments indicate These episodes form the elite set. SCREWMIMIC updates
(Sec. V), the use of screw actions as policy parameterization the sampling distribution based on the elite set and uses the
is critical for more efficient bimanual exploration and allows new distribution in another epoch. The process repeats for N
SCREWMIMIC to achieve success in multiple tasks. In the epochsoruntilthebimanualmanipulationsucceeds.TheCEM
following, we first explain how a screw action is used by fine-tuning procedure is summarized in Algorithm 1.
SCREWMIMIC to generate a bimanual manipulation behavior, The successfully executed screw action œÉr = (gr,gr,Sr)
l r
and then, we describe the iterative process to fine-tune an is added to the training dataset (see Fig. 2) to enhance the
initial (failing) screw action into a successful one. action prediction model. This iterative process, if performed
Given a predicted screw action œÉM = (gM,gM,SM), the repeatedly, can facilitate continuous improvement of both
l r
two grippers first go to gM and gM at pre-defined orienta- the robot‚Äôs policy and the prediction model, creating a self-
l r
tions using a whole-body controller [76]. The end of this supervised feedback loop where each component bolsters the
initial motion is the beginning of the bimanual manipulation other as demonstrated in Sec. V.
described in Section III by the screw axis S. While the left
hand is possibly executing a trajectory œÑ l, the right hand Algorithm 1 Cross-Entropy Method Optimization
will move relative to it following the constraints indicated
Require: parameter distribution D, total epochs N, episodes
by S. SCREWMIMIC creates k ‚àà 1...K waypoints along
in each epoch E, elite trajectories threshold T, S =
init
the screw axis with steps of Œ∏ /K, where Œ∏ is a pre-
T T (sÀÜ ,q ) initial screw axis
init init
determined total amount of translation along the axis-line for
m=prismatictype,orthetotalamountofrotationaround
Œæ ‚Üê(sÀÜ ,q )
init init init
the axis-line for m = revolute and m = revolute3D D ‚ÜêN(0,œÉ2)
types. In the latter case, the orientation of the right hand is
for n=1...N do
kept constant during the motion. Assuming an initial 6D pose
for e=1...E do
for the right hand of Tright with respect to the left hand, the
right-hand poses will b0 e given by T i = exp([S]Œ∏ k) T 0right, S Ea xm ecp ul te eœµ Œæn n,e ,e‚àº =D Œæ init+œµ
n,e
where exp is the matrix exponential and Œ∏ k =kŒ∏ T/K. Collect reward R ; reset environment
n,e
Given the method explained above to generate bimanual end for
manipulation behavior based on a screw action, we now Œæ ,Œæ ...Œæ ‚Üê Order trajectories Œæ ,Œæ ... Œæ based
1 2 T 0,0 0,1 n,E
explain the iterative procedure to fine-tune an initially failing
on rewards
action. Inspired by prior exploratory approaches [66, 69, 77], ‚Ñ¶‚Üê{œµ ,œµ ... œµ }
SCREWMIMIC implements a sampling-based optimization Fit D
toŒæ1
‚Ñ¶
Œæ2 ŒæT
framework based on the cross-entropy method (CEM). The end for
process starts with obtaining the initial screw action from Œæ ‚ÜêŒæ +œµ
final init final
the prediction model that was trained on the human demon-
stration. Next, an initial sampling distribution, D, is usedV. EXPERIMENTALEVALUATION fine-tuning takes around 40 minutes, demonstrating a reason-
able real-world exploration time. Success is verified manually
We evaluate SCREWMIMIC on six real-world bimanual
after each episode, and a human resets the environment if
tasks: open bottle, close zipper, insert roll,
necessary.
close laptop, stir and cut. These tasks collectively
encompass three types of screw joint models: prismatic, Experiments and Results:
revolute, and revolute3D. They also involve screw
In our experiments, we aim to answer four questions:
actions in two types of objects: articulated objects with actual
Q1) Is a single human demonstration enough for
physical joints constraining their motion (as in bottles, rolls,
SCREWMIMIC to achieve success in bimanual manipulation
and laptops) and objects without constraints, where the screw
tasks?Toevaluatethisquestion,weperformthreetrialsperhu-
action creates a virtual joint that facilitates the correct biman-
mandemonstrationforeachofthetasksandobserveif,inthe
ual manipulation (as in stirring, cutting, and zipping a jacket).
trials, SCREWMIMIC successfullyachievesthebimanualtasks
Whilethemanipulationofarticulatedobjectshasbeenstudied
with its self-supervised fine-tuning in screw action space. We
moreextensivelyinthepast[78,79,80,81,82],thisisthefirst
alsoannotatetheamountofinteraction(episodes)necessaryon
time,tothebestofourknowledge,thataframeworkunifiesthe
average to succeed in the task. We use a single demonstration
bimanualmanipulationofrigidandarticulatedobjectsthrough
per task, but each trial starts with a different (novel) location
virtual joints. In the following, we explain each task in brief:
oftheobject(s).Therefore,SCREWMIMICneedstopredictthe
‚Ä¢ open bottle: A bottle with its cap closed is placed screw action in a new location and start the iterative process
upright on the table. The robot performs the opening action there. A trial for each task is depicted in each row of Fig. 4,
as defined by the screw action, followed by a lift arm columns 1 to 6. The results are summarized in Table I.
command. We consider success if the cap is separated from
TABLEI
the base of the bottle at the end.
GENERALIZATIONTONEWOBJECTPOSES
‚Ä¢ close zipper: A jacket is kept in a configuration as
shown in the first row of Fig. 4. We consider success if the #Successes AvgEpochsandEpisodes
robot zips 90% of the jacket at the end. OpenBottle 2/3 (3,18)
‚Ä¢ insert roll. A roll is placed beside the box aligned as CloseZipper 3/3 (2,11)
InsertRoll 3/3 (1,8)
shown in the fourth row of Fig. 4. We consider success if CloseLaptop 3/3 (2,12)
the robot inserts 90% of the roll inside the box at the end. Stir 2/3 (3,16)
‚Ä¢ close laptop: A laptop is placed on the table, opened Cut 3/3 (1,7)
to around 100‚ó¶. We consider success if the robot closes the
laptop (final opening <10‚ó¶). Overall, SCREWMIMIC achieves an aggregated success of
‚Ä¢ stir: A container with a ladle propped against its side is 90% in all trials. SCREWMIMIC failed only in one trial of
placed in front of the robot. The container has two different the open bottle and the stir tasks as the fine-tuning
coloredbeans,initiallyseparated.Weconsidersuccessifthe process finished without any successful screw action. Due to
two types of beans are significantly mixed after the stirring the small number of episodes (25 maximum), we observe a
as measured by a human evaluator. marked dependency on the first screw action samples for the
‚Ä¢ cut:Therobotisholdingascraperknifeinonegripperand fine-tuning procedure, which could be alleviated with a larger
tasked with cutting a block of clay (‚àº7 cm in height). We number of episodes per epoch. Despite that, we consider that
consider success if the block of clay is cut into two pieces our experiments indicate that, in most cases, SCREWMIMIC
at the end. succeeds in all studied bimanual manipulation tasks using
only a single video of a human demonstration, thanks to
In all our experiments, we use a PAL-Robotics Tiago++
the structure provided by the screw action for perception and
bimanualmanipulatorandcontrolitstwoarmsusingawhole-
self-supervised exploration. Additionally, we also conduct ex-
body controller that maps desired end-effector poses for both
perimentstoanalyzetherobustnessofSCREWMIMICtonoisy
arms to joint torques using an inverse-kinematics-based so-
demonstrations.Weobservethatdespitenoisydemonstrations,
lution with task-priority control to avoid self-collisions [76].
SCREWMIMICisabletoextractascrewaxissufficientlyaccu-
For perception, we use an Orbbec Astra S RGB-D camera
rate for fine-tuning. The details and results of the experiment
mounted on Tiago++‚Äôs head both to observe humans and to
are shown in Appendix. D
predict screw actions on objects, and an ATI mini45 force-
torque sensor mounted on the right hand‚Äôs wrist. Q2) Can a policy fine-tuning and model retraining loop
For each task, SCREWMIMIC begins with the screw action enable SCREWMIMIC to continually improve and generalize
predictedbythetrainedmodelafterobservingasinglehuman to new objects? We assess if SCREWMIMIC can use the
interaction using a perceived point cloud as input, and fine- corrected screw action obtained after fine-tuning to improve
tunesitusingitsself-supervisediterativeprocedure.Eachtrial the prediction model and generalize to unseen objects. In
of the procedure is limited to a maximum of 5 epochs, each this experiment, the screw action prediction model is first
containing5episodes,afterwhich,iftheproceduredidnotfind trained with the noisy screw action parsed from the human
a successful screw action, we consider the trial a failure. The demonstration (denoted M1 in Table II). The robot thenAxis
Predicted Axis Predicted Axis
Human Corresponding
Demonstration using M1 Robot Execution/Exploration To Success using M2
Fig. 4. Screw Action Fine-tuning and Prediction Model Re-training Result. The first column shows the human demonstration for each task. The
second column shows the axis predicted by M1, the model trained on the axis extracted from the human demonstration, with the object at a novel pose.
Columns3-5showsnapshotsfromanepisodeinthefine-tuningstage.Column6showstheaxiscorrespondingtothesuccessfultrajectoryobtainedduring
theaforementionedprocess.Column7showsthepredictedaxisforanovelobjectposefromthepredictionmodelre-trainedonthecorrectedaxis.Thisresult
showshowtherobotstartsfromanoisyscrewaxisandusingthescrewactionfine-tuning,correctstheaxis.Furthermore,italsoshowsthatthiscorrected
axiscanbeusedtore-trainthepredictionmodeltooutputamoreaccurateaxis.
executes and fine-tunes this screw action to obtain a corrected M3, SCREWMIMIC cancompletethetaskwiththenewobject
one and uses it to re-train the prediction model, obtaining almost zero-shot (Table II, fourth column). For the insert
the model M2. SCREWMIMIC then uses model M2 to predict roll task, the initial screw action prediction is good as the
and execute the bimanual tasks and performance is measured. structural difference is smaller between the old and the new
TableIIreportstheexplorationiterationsrequireduntilsuccess object.ThisindicatesthatSCREWMIMIChelpscreateaself-
is achieved as (epochs, episodes), where each epoch consists learning loop where the robot can continually expand its
of 5 episodes and the policy is updated after each epoch. manipulation capabilities to new objects. We also compare
Our experiments indicate that, after retraining, SCREWMIMIC training from scratch with a pre-trained PointNet model and
succeeds at the task with the same object (second column) observeanimprovementinthescrewaxispredictiononnovel
almost zero-shot, showing that the prediction model can be objects. The details and results are shown in Appendix. C
iteratively improved using the corrected action obtained from
TABLEII
the fine-tuning stage. GENERALIZATIONTONEWOBJECTS1
WethenassesshowSCREWMIMIChandlesnewobjects.We
Sameobject Newobject
place a novel object of the same category at a new pose and M1 M2 M2 M3
runthesameexperiment(seeFig.5).Fortheopen bottle
OpenBottle (3,16) (0,1) (2,14) (0,1)
task, initially, using M2, the screw action prediction is sub- InsertRoll (1,7) (0,2) (0,3) (0,1)
par(TableII,thirdcolumn).Thisisduetothelargegeometric
1 Resultsreportedas(Epochs,Episodes)untilasuccessisreached.
difference between the bottle that M2 was trained on and the
new bottle. However, after fine-tuning and obtaining model Q3) What benefits does the screw axis representation havePredicted Axis using M2 Robot Execution/Exploration Axis Corresponding To Predicted Axis using M3
Success
Fig. 5. Generalization to New Objects. The first column shows the axis predicted by M2, the model trained on the corrected screw action for the first
object.Columns2-4showsnapshotsfromanepisodeinthefine-tuningstage.Column5showstheaxiscorrespondingtothesuccessfultrajectoryobtained
duringtheaforementionedprocess.Column6showsthepredictedaxisfromthepredictionmodelre-trainedonthecorrectedaxis(M3).Thus,SCREWMIMIC
canobtainreasonablescrewactionpredictionsandfine-tunethemtogeneralizetonewobjects.
TABLEIII
screw (Fig. 6c).
ACTIONREPRESENTATIONCOMPARISON
Each baseline obtains an initial trajectory from a human
Task
Success? #Episodes DenseMetric demonstration, then performs the fine-tuning procedure in the
FM+N√ó6-DoF Bottle No 25/25 0% correspondingactionspace.Foreachmethod,werunonetrial
(DEFT*) Roll No 25/25 0% of the exploration process for each of the three tasks ‚Äîopen
Laptop No 25/25 10% bottle, insert roll and close laptop as shown in
Screw+N√ó6-DoF Bottle No 25/25 10% Table III. We indicate whether the robot can achieve success
Roll No 25/25 50%
in the allotted 5 epochs (5 episodes each), as well as the
Laptop No 25/25 50%
percentage of the task that the robot completes (Table III, last
Screw+Screw Bottle Yes 16/25 100%
column), measured by a human.
(SCREWMIMIC) Roll Yes 7/25 100%
Laptop Yes 11/25 100% Our results in Table III indicate that neither FM + N√ó6-
DoF nor Screw + N√ó6-DoF representations enable task
success, as exploring in the N√ó6-DoF space is much more
compared to a more direct, N√ó6-DoF representation? To
challenging. We hypothesize that, for FM + N√ó6-DoF, the
assess the importance of the screw axis representation we
failing behavior is not only caused by the large uncorre-
compare it with two baselines visualized in Fig. 6. First, the
lated exploration space but also by a more noisy initial
FM + N√ó6-DoF baseline (Table III, first row) extracts the
trajectory that keeps the inherent noise present in the hand-
initial trajectory from hand tracker using the wrist poses as
tracking module. In contrast, the use of screw actions enables
N waypoints directly. We call this space as N√ó6-DoF as
SCREWMIMIC to clean the perceived human demonstration
it has N waypoints with each waypoint described by a 6-
and also makes the fine-tuning process more efficient by
DoF pose. During fine-tuning, it explores in the N√ó6-DoF
exploring in the reduced screw axis space.
space by adding noise to the initial waypoints (Fig. 6a). With
this baseline, we ablate the screw representation both as the Q4) Are both autonomous reward signals correctly guid-
human demonstration parser and as the action space during ing the policy fine-tuning stage? The screw action policy
fine-tuning.NotethatthisbaselineemulatesDEFT‚Äôs[66]fine- fine-tuning stage requires a way to rank episodes in our
tuningstage.DuetotheunavailabilityofDEFT‚Äôscode/model, CEM procedure, guiding the robot to explore around good
weattempttoapproximateDEFT‚Äôsmethodologyascloselyas episodes while disregarding bad ones to converge to success.
possible. Key differences include: 1) the use of both hands in SCREWMIMIC uses two signals to rank any episode as de-
ourmethod,2)theextraction(ratherthanprediction)ofgrasp- scribedinSec.IV-C:thelengthofanepisode(basedonaloss
ing locations directly from demonstrations, and 3) a reliance ofgrasp/fixationorexceedingaForce-Torque(FT)threshold),
on SCREWMIMIC‚Äôs self-generated CEM reward, rather than and the mean wrench measured over the episode. To assess
human-assigned scores. The second baseline, Screw + N√ó6- the importance of these two signals for the fine-tuning stage,
DoF (TableIII,secondrow)extractstheinitialtrajectoryfrom we ablate each component individually. Since removing the
the output of the hand tracker using SCREWMIMIC‚Äôs parser FT sensor threshold can be dangerous for the robot and the
moduleasascrewaxis,butexploresbyaddingGaussiannoise objects, we always retain it. Fig. 7 provides examples of the
in SE(3) during fine-tuning (Fig. 6b). This baseline parses roles of these reward terms. The results in Table IV indicate
the human demonstration in the same way as SCREWMIMIC that for the open bottle and stir tasks, if either of the
but explores differently. We compare against our proposed two components is absent, the policy fine-tuning process fails
SCREWMIMIC (Table III, third row), indicated as screw + and the robot fails to complete the task within the allottedInitial Traj Explored True Screw Axis TABLEIV
ABLATIONOFREWARDCOMPONENTS
Task
Success? #Episodes DenseMetric
w/oGraspLost Bottle No 25/25 20%
Detection Roll Yes 7/25 100%
Stir No 25/25 None
w/oMeanEpisode Bottle No 25/25 30%
FT Roll No 25/25 10%
(a) FM + N √ó 6-DoF Noise (b) Screw + N √ó 6-DoF Noise (c) Screw + Screw Stir No 25/25 None
Fig. 6. Exploration Intuition. The different action representations from
Table III are illustrated in 2D for the insert tissue roll task. The SCREWMIMIC Bottle Yes 16/25 100%
true (prismatic) screw axis is visualized as a dashed line. The resulting Roll Yes 7/25 100%
initial and sampled exploration trajectories are visualized as black and red, Stir Yes 18/25 None
respectively.(a)ThebaselineFM+N√ó6-DoF:obtaininganinitialtrajectory
fromFrankMocapasN6-DoFwaypoints,thenperformingexplorationaround
that trajectory with noise in 6-DoF space. (b) The baseline Screw + N√ó6- future work. First, the screw action formulation, although
DoF: perceiving the initial trajectory as a screw axis but exploring with
versatile, does not fit all bimanual manipulation tasks, e.g.,
noise in 6-DoF space. (c) SCREWMIMIC (Screw + Screw), perceiving the
demonstrationasascrewactionandexploringinthespaceofscrewaxes. tasks where the hands are not constrained to move along
a single axis, such as cutting in a zig-zag motion. Future
work can extend SCREWMIMIC to include sequences of
screwaxes,requiringmorecomplexinferenceandexploration
algorithms. Second, we train a separate prediction model for
each object class, which limits the generalization capabilities
of SCREWMIMIC. This can be addressed by training a single
multi-task model on a diverse array of objects. Large-scale
human-activity datasets [53, 83] offer an exciting avenue to
scale up the range of tasks and objects that SCREWMIMIC
can learn from. For that, our method should also relax the
dependencyondepthsensors,whichcouldbeobtainedinstead
from RGB using recent algorithms [84, 85]. Third, episode
success is recorded manually in our experiments. This could
be automated in the future using vision-language foundation
models [86, 87]. Fourth, while SCREWMIMIC focuses on
improving the screw axis prediction, it could be beneficial
to also fine-tune the grasp contact points. Finally, due to 3D
sensor limitations, some reflective surfaces such as the laptop
cannot be correctly perceived from all angles and we need to
coverthem.Wedonotdeemthisaproblemof SCREWMIMIC
but rather of the (relatively outdated) depth sensor‚Äîusing
Fig.7. RewardIntuition.Eachrowshowsthescrewaxisandthesnapshots more modern 3D sensors would alleviate it. Despite these
ofthecorrespondingepisodetoshowcasetheuseoftherewardcomponents
limitations, SCREWMIMIC demonstrates that using a screw
‚ÄìGrasplost(firstrow),FTsensorreachingthethreshold(secondrow).Each
snapshothasthecorrespondingFTsensorreadinguntilthattimestep.Thepink axis space representation for bimanual actions facilitates ef-
lineshowstheFTthreshold.Thelastrowshowsanexampleofasuccesswith ficient exploration leading to strong improvements in task
thecorrespondingFTsensorreadings.
success. Additionally, the incorporation of a self-supervised
fine-tuningprocessallowstherobottoiterativelyrefineitsown
rollouts.Fortheinsert rolltask,therewasneveragrasp
actions.Ourworkisapromisingsteptowardsenablingrobots
lost in any episode, so SCREWMIMIC succeeds even without
to efficiently learn complex bimanual manipulation tasks by
the grasp loss detection. However, it fails without the mean
watching humans.
episode sensed wrench. This shows that both reward signals
are critical for SCREWMIMIC‚Äôs policy fine-tuning stage.
REFERENCES
VI. LESSONSANDCONCLUSION
[1] Sandra A Heldstab, Zaida K Kosonen, Sonja E Koski,
In this work, we present and validate SCREWMIMIC, a Judith M Burkart, Carel P van Schaik, and Karin Isler.
robust representational framework for bimanual manipulation Manipulation complexity in primates coevolved with
that significantly boosts performance by simplifying com- brainsizeandterrestriality.Scientificreports,6(1):24528,
plex tasks into screw actions derived from a single human 2016.
demonstration. While our results demonstrate the capabilities [2] O. V. Kazennikov, Brian I. Hyland, Michel R. Cor-
of SCREWMIMIC, it is not without limitations and scope for boz, Alexandre Babalian, Eric M. Rouiller, and MarioWiesendanger. Neuralactivityofsupplementaryandpri- nipulation and attachment via sim-to-real reinforcement
marymotorareasinmonkeysanditsrelationtobimanual learning. ArXiv, abs/2203.08277, 2022. URL https:
and unimanual movement sequences. Neuroscience, 89: //api.semanticscholar.org/CorpusID:247476081.
661‚Äì674, 1999. URL https://api.semanticscholar.org/ [14] Raymond C Goertz. Fundamentals of general-purpose
CorpusID:2804384. remote manipulators. Nucleonics, pages 36‚Äì42, 1952.
[3] Opher Donchin, A. D. Gribova, Orna Steinberg, Hagai [15] Robonaut: Nasa‚Äôs space humanoid. IEEE Intelligent
Bergman, and Eilon Vaadia. Primary motor cortex is Systems and Their Applications, 15(4):57‚Äì63, 2000.
involvedinbimanualcoordination.Nature,395:274‚Äì278, [16] Jonathan Bohren, Radu Bogdan Rusu, E. Gil Jones,
1998. URL https://api.semanticscholar.org/CorpusID: Eitan Marder-Eppstein, Caroline Pantofaru, Melonee
4370872. Wise, Lorenz Mo¬®senlechner, Wim Meeussen, and Stefan
[4] J. A. Scott Kelso. Phase transitions and critical behavior Holzer. Towards autonomous robotic butlers: Lessons
in human bimanual coordination. The American journal learned with the pr2. In 2011 IEEE International
of physiology, 246 6 Pt 2:R1000‚Äì4, 1984. URL https: Conference on Robotics and Automation, pages 5568‚Äì
//api.semanticscholar.org/CorpusID:45949058. 5575, 2011. doi: 10.1109/ICRA.2011.5980058.
[5] Karen E Adolph, Bennett I Bertenthal, Steven M Boker, [17] Aude Billard and Danica Kragic. Trends and challenges
Eugene C Goldfield, and Eleanor J Gibson. Learning in robot manipulation. Science, 364(6446):eaat8414,
in the development of infant locomotion. Monographs 2019.
of the society for research in child development, pages [18] NikolausVahrenkamp,DmitryBerenson,TamimAsfour,
i‚Äì162, 1997. James Kuffner, and Ru¬®diger Dillmann. Humanoid mo-
[6] Je¬¥roÀÜme Barral, Bettina DebuÀÜ, and Christina Rival. De- tion planning for dual-arm manipulation and re-grasping
velopmental changes in unimanual and bimanual aim- tasks. In 2009 IEEE/RSJ International Conference on
ing movements. Developmental neuropsychology, 29(3): IntelligentRobotsandSystems,pages2464‚Äì2470.IEEE,
415‚Äì429, 2006. 2009.
[7] Yoshihito Koga and J-C Latombe. Experiments in dual- [19] Benjamin Cohen, Sachin Chitta, and Maxim Likhachev.
arm manipulation planning. In Proceedings 1992 IEEE Single-and dual-arm motion planning with heuristic
International Conference on Robotics and Automation, search. The International Journal of Robotics Research,
pages 2238‚Äì2239. IEEE Computer Society, 1992. 33(2):305‚Äì320, 2014.
[8] Christian Smith, Yiannis Karayiannidis, Lazaros Nal- [20] Ping Hsu. Coordinated control of multiple manipulator
pantidis, Xavi Gratal, Peng Qi, Dimos V Dimarogonas, systems. IEEE Transactions on Robotics and Automa-
and Danica Kragic. Dual arm manipulation‚Äîa survey. tion, 9(4):400‚Äì410, 1993.
Robotics and Autonomous systems, 60(10):1340‚Äì1353, [21] Adria`Colome¬¥andCarmeTorras.Reinforcementlearning
2012. of bimanual robot skills. Springer, 2020.
[9] Adria` Colome¬¥ and Carme Torras. Dimensionality reduc- [22] R Zollner, Tamim Asfour, and Ru¬®diger Dillmann. Pro-
tion for dynamic movement primitives and application gramming by demonstration: dual-arm manipulation
to bimanual manipulation of clothes. IEEE Transac- tasks for humanoid robots. In 2004 IEEE/RSJ Inter-
tions on Robotics, 34:602‚Äì615, 2018. URL https://api. national Conference on Intelligent Robots and Systems
semanticscholar.org/CorpusID:20726581. (IROS)(IEEE Cat. No. 04CH37566), volume 1, pages
[10] Oliver Kroemer, Christian Daniel, Gerhard Neumann, 479‚Äì484. IEEE, 2004.
Herke van Hoof, and Jan Peters. Towards learning [23] Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Ku-
hierarchical skills for multi-phase manipulation tasks. niyoshi. Transformer-based deep imitation learning for
2015 IEEE International Conference on Robotics and dual-arm robot manipulation. In 2021 IEEE/RSJ Inter-
Automation (ICRA), pages 1503‚Äì1510, 2015. URL national Conference on Intelligent Robots and Systems
https://api.semanticscholar.org/CorpusID:12178097. (IROS), pages 8965‚Äì8972. IEEE, 2021.
[11] Kevin Sebastian Luck and Heni Ben Amor. Extracting [24] ElenaGribovskayaandAudeBillard.Combiningdynam-
bimanual synergies with reinforcement learning. 2017 ical systems control and programming by demonstration
IEEE/RSJInternationalConferenceonIntelligentRobots for teaching discrete bimanual coordination tasks to a
andSystems(IROS),pages4805‚Äì4812,2017. URLhttps: humanoid robot. In Proceedings of the 3rd ACM/IEEE
//api.semanticscholar.org/CorpusID:20155594. international conference on Human robot interaction,
[12] Yuanpei Chen, Tianhao Wu, Shengjie Wang, Xidong pages 33‚Äì40, 2008.
Feng, Jiechuan Jiang, Zongqing Lu, Stephen McAleer, [25] Tamim Asfour, Pedram Azad, Florian Gyarfas, and
HaoDong,Song-ChunZhu,andYaodongYang.Towards Ru¬®digerDillmann. Imitationlearningofdual-armmanip-
human-level bimanual dexterous manipulation with re- ulation tasks in humanoid robots. International journal
inforcement learning. Advances in Neural Information of humanoid robotics, 5(02):183‚Äì202, 2008.
Processing Systems, 35:5150‚Äì5163, 2022. [26] Joao Silve¬¥rio, Leonel Rozo, Sylvain Calinon, and Dar-
[13] Satoshi Kataoka, Seyed Kamyar Seyed Ghasemipour, win G Caldwell. Learning bimanual end-effector poses
Daniel Freeman, and Igor Mordatch. Bi-manual ma- fromdemonstrationsusingtask-parameterizeddynamicalsystems. In 2015 IEEE/RSJ international conference on 2020.
intelligent robots and systems (IROS), pages 464‚Äì470. [39] Rudolf Lioutikov, Oliver Kroemer, Guilherme Maeda,
IEEE, 2015. and Jan Peters. Learning manipulation by sequencing
[27] Sylvain Calinon, Zhibin Li, Tohid Alizadeh, Nikos G motor primitives with a two-armed robot. In Intelligent
Tsagarakis, and Darwin G Caldwell. Statistical dynami- Autonomous Systems 13: Proceedings of the 13th Inter-
cal systems for skills acquisition in humanoids. In 2012 nationalConferenceIAS-13,pages1601‚Äì1611.Springer,
12th IEEE-RAS International Conference on Humanoid 2016.
Robots (Humanoids 2012), pages 323‚Äì329. IEEE, 2012. [40] Yahav Avigal, Lars Berscheid, Tamim Asfour, Torsten
[28] AndrejGams,BojanNemec,AukeJanIjspeert,andAlesÀá Kro¬®ger, and Ken Goldberg. Speedfolding: Learning effi-
Ude. Couplingmovementprimitives:Interactionwiththe cient bimanual folding of garments. In 2022 IEEE/RSJ
environment and bimanual tasks. IEEE Transactions on International Conference on Intelligent Robots and Sys-
Robotics, 30(4):816‚Äì830, 2014. tems(IROS),pages1‚Äì8,2022. doi:10.1109/IROS47612.
[29] Lucia Pais Ureche and Aude Billard. Constraints extrac- 2022.9981402.
tion from asymmetrical bimanual tasks and their use in [41] Fan Xie, Alexander Chowdhury, M De Paolis Kaluza,
coordinatedbehavior. Roboticsandautonomoussystems, Linfeng Zhao, Lawson Wong, and Rose Yu. Deep
103:222‚Äì235, 2018. imitation learning for bimanual robotic manipulation.
[30] Nadia Figueroa and Aude Billard. Learning complex Advances in neural information processing systems, 33:
manipulation tasks from heterogeneous and unstructured 2327‚Äì2337, 2020.
demonstrations. In Proceedings of Workshop on Syner- [42] Thomas Weng, Sujay Man Bajracharya, Yufei Wang,
gies between Learning and Interaction, 2017. Khush Agrawal, and David Held. Fabricflownet: Bi-
[31] Ana-Lucia Pais Ureche and Aude Billard. Learning manual cloth manipulation with a flow-based policy. In
bimanual coordinated tasks from human demonstrations. Conference on Robot Learning, pages 192‚Äì202. PMLR,
In Proceedings of the Tenth Annual ACM/IEEE Interna- 2022.
tionalConferenceonHuman-RobotInteractionExtended [43] Huy Ha and Shuran Song. Flingbot: The unreasonable
Abstracts, pages 141‚Äì142, 2015. effectiveness of dynamic manipulation for cloth unfold-
[32] TonyZZhao,VikashKumar,SergeyLevine,andChelsea ing. In Conference on Robot Learning, pages 24‚Äì33.
Finn. Learning fine-grained bimanual manipulation with PMLR, 2022.
low-cost hardware. arXiv preprint arXiv:2304.13705, [44] Rohan Chitnis, Shubham Tulsiani, Saurabh Gupta, and
2023. Abhinav Gupta. Intrinsic motivation for encouraging
[33] Albert Tung, Josiah Wong, Ajay Mandlekar, Roberto synergistic behavior. arXiv preprint arXiv:2002.05189,
Mart¬¥ƒ±n-Mart¬¥ƒ±n, Yuke Zhu, Li Fei-Fei, and Silvio 2020.
Savarese. Learning multi-arm manipulation through [45] Jennifer Grannen, Priya Sundaresan, Brijen Thanan-
collaborative teleoperation. In 2021 IEEE International jeyan, Jeffrey Ichnowski, Ashwin Balakrishna, Minho
Conference on Robotics and Automation (ICRA), pages Hwang, Vainavi Viswanath, Michael Laskey, Joseph E
9212‚Äì9219. IEEE, 2021. Gonzalez, and Ken Goldberg. Untangling dense knots
[34] FranziskaKrebsandTamimAsfour.Abimanualmanipu- by learning task-relevant keypoints. arXiv preprint
lationtaxonomy. IEEERoboticsandAutomationLetters, arXiv:2011.04999, 2020.
7(4):11031‚Äì11038, 2022. [46] Jennifer Grannen, Yilin Wu, Suneel Belkhale, and Dorsa
[35] Christian RG Dreher and Tamim Asfour. Learning Sadigh. Learning bimanual scooping policies for food
temporal task models from human bimanual demonstra- acquisition. In 6th Annual Conference on Robot Learn-
tions. In 2022 IEEE/RSJ International Conference on ing, 2022.
IntelligentRobotsandSystems(IROS),pages7664‚Äì7671. [47] Aleksandar Batinica, Bojan Nemec, AlesÀá Ude, Mirko
IEEE, 2022. Rakovic¬¥, and Andrej Gams. Compliant movement prim-
[36] Satoshi Kataoka, Seyed Kamyar Seyed Ghasemipour, itives in a bimanual setting. In 2017 IEEE-RAS 17th
Daniel Freeman, and Igor Mordatch. Bi-manual ma- International Conference on Humanoid Robotics (Hu-
nipulation and attachment via sim-to-real reinforcement manoids), pages 365‚Äì371. IEEE, 2017.
learning. arXiv preprint arXiv:2203.08277, 2022. [48] Giovanni Franzese, Leandro de Souza Rosa, Tim Ver-
[37] Rohan Chitnis, Shubham Tulsiani, Saurabh Gupta, and burg,LukaPeternel,andJensKober.Interactiveimitation
Abhinav Gupta. Efficient bimanual manipulation using learning of bimanual movement primitives. IEEE/ASME
learned task schemas. In 2020 IEEE International Transactions on Mechatronics, 2023.
Conference on Robotics and Automation (ICRA), pages [49] Aditya Ganapathi, Priya Sundaresan, Brijen Thanan-
1149‚Äì1155. IEEE, 2020. jeyan, Ashwin Balakrishna, Daniel Seita, Jennifer
[38] Wenshuai Zhao, Jorge PenÀúa Queralta, and Tomi Wester- Grannen, Minho Hwang, Ryan Hoque, Joseph E Gon-
lund. Sim-to-realtransferindeepreinforcementlearning zalez, Nawid Jamali, et al. Learning dense visual corre-
forrobotics:asurvey. In2020IEEEsymposiumserieson spondencesinsimulationtosmoothandfoldrealfabrics.
computationalintelligence(SSCI),pages737‚Äì744.IEEE, In 2021 IEEE International Conference on Robotics andAutomation (ICRA), pages 11515‚Äì11522. IEEE, 2021. Conference on Robot Learning, pages 416‚Äì426. PMLR,
[50] Fabio Amadio, Adria` Colome¬¥, and Carme Torras. Ex- 2023.
ploiting symmetries in reinforcement learning of biman- [62] YuRong,TakaakiShiratori,andHanbyulJoo. Frankmo-
ualrobotictasks. IEEERoboticsandAutomationLetters, cap:Amonocular3dwhole-bodyposeestimationsystem
4(2):1838‚Äì1845, 2019. via regression and integration. In IEEE International
[51] Arpit Bahety, Shreeya Jain, Huy Ha, Nathalie Hager, Conference on Computer Vision Workshops, 2021.
Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, and [63] Priyanka Mandikal and Kristen Grauman. Dexvip:
ShuranSong. Bagallyouneed:Learningageneralizable Learning dexterous grasping with human hand pose
bagging strategy for heterogeneous objects, 2023. priors from video. In Conference on Robot Learning,
[52] Jennifer Grannen, Yilin Wu, Brandon Vu, and Dorsa pages 651‚Äì661. PMLR, 2022.
Sadigh. Stabilize to act: Learning to coordinate for bi- [64] Kenneth Shaw, Shikhar Bahl, and Deepak Pathak.
manualmanipulation. InConferenceonRobotLearning, Videodex: Learning dexterity from internet videos. In
pages 563‚Äì576. PMLR, 2023. Conference on Robot Learning, pages 654‚Äì665. PMLR,
[53] Kristen Grauman, Andrew Westbury, Eugene Byrne, 2023.
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jack- [65] Kenneth Shaw, Shikhar Bahl, Aravind Sivakumar,
son Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Aditya Kannan, and Deepak Pathak. Learning dex-
Ego4d: Around the world in 3,000 hours of egocentric terity from human hand motion in internet videos.
video. In Proceedings of the IEEE/CVF Conference on The International Journal of Robotics Research, page
ComputerVisionandPatternRecognition,pages18995‚Äì 02783649241227559, 2024.
19012, 2022. [66] Aditya Kannan, Kenneth Shaw, Shikhar Bahl, Pragna
[54] Aravind Sivakumar, Kenneth Shaw, and Deepak Pathak. Mannam, and Deepak Pathak. Deft: Dexterous fine-
Robotic telekinesis: Learning a robotic hand imitator by tuning for real-world hand policies. CoRL, 2023.
watching humans on youtube. In Robotics: Science and [67] Dandan Shan, Jiaqi Geng, Michelle Shu, and David F.
Systems, 2022. Fouhey. Understanding human hands in contact at
[55] Haoyu Xiong, Quanzhou Li, Yun-Chun Chen, Homanga internet scale. CoRR, abs/2006.06669, 2020. URL
Bharadhwaj, Samarth Sinha, and Animesh Garg. Learn- https://arxiv.org/abs/2006.06669.
ing by watching: Physical imitation of manipulation [68] Homanga Bharadhwaj, Abhinav Gupta, Shubham Tul-
skills from human videos. In 2021 IEEE/RSJ Inter- siani, and Vikash Kumar. Zero-shot robot manipulation
national Conference on Intelligent Robots and Systems from passive human videos, 2023.
(IROS), pages 7827‚Äì7834. IEEE, 2021. [69] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak.
[56] Annie S Chen, Suraj Nair, and Chelsea Finn. Learn- Human-to-robot imitation in the wild. 2022.
ing generalizable robotic reward functions from‚Äù in-the- [70] Russell Mendonca, Shikhar Bahl, and Deepak Pathak.
wild‚Äù human videos. arXiv preprint arXiv:2103.16817, Structured world models from human videos. 2023.
2021. [71] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J
[57] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, Guibas. Pointnet: Deep learning on point sets for
and Jeannette Bohg. Concept2robot: Learning manipu- 3d classification and segmentation. arXiv preprint
lation concepts from instructions and human demonstra- arXiv:1612.00593, 2016.
tions. The International Journal of Robotics Research, [72] Bruno Siciliano, Oussama Khatib, and Torsten Kro¬®ger.
40(12-14):1419‚Äì1434, 2021. Springer handbook of robotics, volume 200. Springer,
[58] YechengJasonMa,WilliamLiang,VaidehiSom,Vikash 2008.
Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayara- [73] Kevin M Lynch and Frank C Park. Modern robotics.
man. Liv: Language-image representations and rewards Cambridge University Press, 2017.
for robotic control. arXiv preprint arXiv:2306.00958, [74] Yves Guiard. Asymmetric division of labor in human
2023. skilledbimanualaction:Thekinematicchainasamodel.
[59] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayara- Journal of motor behavior, 19(4):486‚Äì517, 1987.
man, Osbert Bastani, Vikash Kumar, and Amy Zhang. [75] AishwaryaKamath,MannatSingh,YannLeCun,Gabriel
Vip: Towards universal visual reward and represen- Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr ‚Äì
tation via value-implicit pre-training. arXiv preprint modulated detection for end-to-end multi-modal under-
arXiv:2210.00030, 2022. standing, 2021.
[60] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea [76] Nicolas Mansard, Olivier Stasse, Paul Evrard, and Ab-
Finn, and Abhinav Gupta. R3m: A universal visual derrahmane Kheddar. A versatile generalized inverted
representation for robot manipulation. arXiv preprint kinematicsimplementationforcollaborativeworkinghu-
arXiv:2203.12601, 2022. manoidrobots:Thestackoftasks. In2009International
[61] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter conferenceonadvancedrobotics,pages1‚Äì6.IEEE,2009.
Abbeel, Jitendra Malik, and Trevor Darrell. Real-world [77] Freek Stulp and Olivier Sigaud. Path integral policy
robot learning with masked visual pre-training. In improvement with covariance matrix adaptation. arXivpreprint arXiv:1206.4621, 2012. 10.1109/CVPR.2015.7298801.
[78] Ju¬®rgen Sturm, Advait Jain, Cyrill Stachniss, Charles C
Kemp, and Wolfram Burgard. Operating articulated
objects based on experience. In 2010 IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems,
pages 2739‚Äì2744. IEEE, 2010.
[79] Yiannis Karayiannidis, Christian Smith, Francisco
EliVinaBarrientos,PetterO¬®gren,andDanicaKragic.An
adaptivecontrolapproachforopeningdoorsanddrawers
under uncertainties. IEEE Transactions on Robotics, 32
(1):161‚Äì175, 2016.
[80] Roberto Mart¬¥ƒ±n-Mart¬¥ƒ±n and Oliver Brock. Cross-modal
interpretation of multi-modal sensor streams in interac-
tive perception based on coupled recursion. In 2017
IEEE/RSJInternationalConferenceonIntelligentRobots
and Systems (IROS), pages 3289‚Äì3295. IEEE, 2017.
[81] Ben Abbatematteo, Stefanie Tellex, and George
Konidaris. Learning to generalize kinematic models to
novel objects. In Proceedings of the 3rd Conference on
Robot Learning, 2019.
[82] Yian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan
Fan, Leonidas J Guibas, and Hao Dong. Adaafford:
Learning to adapt manipulation affordance for 3d artic-
ulated objects via few-shot interactions. In European
ConferenceonComputerVision,pages90‚Äì107.Springer,
2022.
[83] DimaDamen,HazelDoughty,GiovanniMariaFarinella,
Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Da-
vide Moltisanti, Jonathan Munro, Toby Perrett, Will
Price, and Michael Wray. Scaling egocentric vision:
The epic-kitchens dataset. In European Conference on
Computer Vision (ECCV), 2018.
[84] Reiner Birkl, Diana Wofk, and Matthias Mu¬®ller. Midas
v3. 1‚Äìa model zoo for robust monocular relative depth
estimation. arXiv preprint arXiv:2307.14460, 2023.
[85] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu,
Jiashi Feng, and Hengshuang Zhao. Depth anything:
Unleashing the power of large-scale unlabeled data.
arXiv preprint arXiv:2401.10891, 2024.
[86] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al.
Learning transferable visual models from natural lan-
guage supervision. In International conference on ma-
chine learning. PMLR, 2021.
[87] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toineMiech,IainBarr,YanaHasson,KarelLenc,Arthur
Mensch, Katherine Millican, Malcolm Reynolds, et al.
Flamingo:avisuallanguagemodelforfew-shotlearning.
Advances in Neural Information Processing Systems,
2022.
[88] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu,
Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d
shapenets: A deep representation for volumetric shapes.
In 2015 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 1912‚Äì1920, 2015. doi:APPENDIX ‚Ä¢ Meananglebetweenthepredictedandgroundtruthscrew
axis (in degrees)
A. Screw Action with Left-Hand Trajectory
As shown in Table VI, with a pre-trained PointNet, we
Fig. 8 depicts three steps of a robot execution with a
indeedobserveanimprovementinthescrewaxispredictionon
non-empty left-hand trajectory. Since ScrewMimic defines the
novelobjects,althoughtheperformanceonthetrainingobject
screwaxisofmanipulationastherelativemotionbetweenboth
remains the same. While this would not affect the exploration
hands, the absolute motion of one of them does not affect the
in case of the training object, it would lead to more efficient
motiongeneratedfromthesamescrewaction.Weconsiderthe
optimization in the CEM phase for novel objects and better
actuation part of the bimanual manipulation to be the effect
generalization capabilities of the policy
of this relative motion between hands rather than the absolute
motion of them.
Fig. 8. Screw Action execution with left-hand trajectory Three steps (a) (b)
of a robot execution of a stir task with non-zero left-hand motion. Since
SCREWMIMIC focuses on the generation of relative motion between hands, Fig.9. (a)Trainingbottle.(b)TestingBottles
thesamescrewactioncanbeusedevenwhenthereexistsanyabsolutemotion
ofoneofthem(lefthand).
TABLEVI
POINTNETTRAINEDFROMSCRATCH(M1)VSPRETRAINED+FINETUNED
(M2)
B. Hyperparameters
M1 M2
TABLEV
BlueBottle(Trainingbottle) 0.01m,2.1‚ó¶ 0.01m,2.0‚ó¶
SCREWACTIONPREDICTIONMODELHYPERPARAMETERS
GreenBottle(withorangecap) 0.03m,3.6‚ó¶ 0.01m,2.3‚ó¶
BlackBottle 0.05m,5.3‚ó¶ 0.02m,3.8‚ó¶
Hyperparameters Value
OrangeBottle(withblackcap) 0.09m,6‚ó¶ 0.03m,4.2‚ó¶
trainepochs 2000
batchsize 16
optimizer Adam D. Robustness to Noisy Demonstrations
learningrate 1e-3
pointcloudencoder PointNet[71] ToanalyzetherobustnessofSCREWMIMICtonoisyhuman
numberofpoints 2048
demonstrations we conduct the following two experiments:
layer-activations ReLU
ScrewAxisRegression a) Artificiallyaddingincreasingamountsofnoise(con-
Architecture Conv1d(64,128,1024) trolledstudy): Inthefirstexperiment,weinvestigatehowwell
+FC(1024,512,256,6) ScrewMimic can adapt when increasing amounts of artificial
Loss MSE
noise are introduced to a trajectory. We focus on a specific
GraspContactSegmentation
Architecture Conv1d(64,128,128,512,2048, task ‚Äî open bottle. We manually annotate the ground
256,256,128,3) truth screw axis and compute the corresponding noise-free
Loss negativeloglikelihood
ground truth hand trajectory (shown in green in Fig. 10).
This trajectory corresponds to the trajectory of an acting
hand relative to a reference hand. We introduce five different
C. Using Pretrained PointNet Model
levels of noise to the ground truth hand trajectory, affecting
We conduct experiments to analyze if using a pre-trained both position and orientation, and observe the changes in the
PointNet model helps to better generalize to new objects as screw axis computed by ScrewMimic with increasing noise
compared to training from scratch. We pretrain a PointNet levels. These trajectories and their respective screw axes are
model on the ModelNet-40 dataset [88] for the classification illustrated in Fig. 10, where colors transitioning from light
task. We then use the pretrained feature encoder and fine- to dark depict the sequence of actions from start to finish.
tune it on the screw axis prediction task. Finally, we compare We only visualize the positions (and not the orientations)
this model (M2) to our original model that was trained from for clarity. We created 20 noisy trajectories for each noise
scratch (M1) on the screw action prediction task. The test set level, resulting in 100 test trajectories. We use two metrics
consists of 4 different bottles (1 bottle seen during training to evaluate the performance for each noise level: a) mean
and 3 unseen bottles) as shown in Fig. 9 The test set consists distance error between predicted and ground truth screw axes
of 10 poses for each of the four bottles. We use two metrics (in meters), and b) mean angle error between the predicted
to evaluate their performance: and ground truth screw axis (in degrees).
‚Ä¢ Mean distance between predicted and ground truth screw Tab. VII and Fig. 10 show the results of our experiment.
axis (in meters) We observe that the accuracy of the screw axis detected byLevel 1 Level 2 Level 3 Level 4 Level 5
Green Trajectory: GT Hand trajectory ; Red Trajectory: Noisy hand trajectory obtained by adding noise to the GT hand trajectory
Green Axis: GT Screw Axis ; Red Axis: Screw Axis extracted from ScrewMimic
Fig.10. ScrewMimic‚Äôsaxisextractionwithincreasinglynoisydemonstrations.Thefivelevelsrepresentincreasingnoiseappliedtothegroundtruthhand
trajectory(positionandorientation).ComparingeachscrewaxiswiththegroundtruthscrewaxisinthisfigureandthenumbersinTab.VII,andperforming
thefine-tuningexperimentwiththeaxisinferredfromthehighestnoiselevel,weobservethatalthoughScrewMimicdoessufferfromincreasingnoiseinthe
handtrajectories,itisabletoextractanaxissufficientlyaccurateforfine-tuning.
TABLEVII
SCREWMIMIC‚ÄôSAXISEXTRACTIONWITHINCREASINGLYNOISYDEMONSTRATIONS
Level 1 Level 2 Level 3 Level 4 Level 5
pos=N(0,1.0cm) pos=N(0,1.5cm) pos=N(0,2.0cm) pos=N(0,2.5cm) pos=N(0,3.0cm)
orn=N(0,2.5‚ó¶) orn=N(0,5.0‚ó¶) orn=N(0,7.5‚ó¶) orn=N(0,10.0‚ó¶) orn=N(0,12.5‚ó¶)
Distance between
GT axis and
Extracted axis (cm) 0.5cm¬±10‚àí5 0.8cm¬±10‚àí4 1.1cm¬±10‚àí5 1.7cm¬±10‚àí4 2.1cm¬±10‚àí2
Angle between
GT axis and
Extracted axis (degrees) 4.0‚ó¶¬±2.0‚ó¶ 6.5‚ó¶¬±3.0‚ó¶ 9.4‚ó¶¬±5.0‚ó¶ 11.1‚ó¶¬±8.5‚ó¶ 13.1‚ó¶¬±6.0‚ó¶
ScrewMimic declines as we increase the noise in the hand by ScrewMimic. Note that the trajectory corresponds to the
trajectories. To test if ScrewMimic can perform a successful trajectory of the acting hand (right hand in this case) relative
fine-tuning even with the highest noise level, we conduct the tothereferencehand(lefthand).Fig.11(b)ontherightshows
followingexperiment:weusetheaxisinferredbyScrewMimic a comparison of the trajectory and computed screw axis with
fromthetrajectoryinlevel5tobootstraptheScrewMimicfine- the ground truth trajectory and screw axis for each of the five
tuningstep.Weobservethateveninthisadversarialcondition, human demonstrations.
success is achieved after 4 epochs and 21 episodes. This is Tab.VIIIshowsthequantitativeresultsofthedistanceerror
comparable to the performance of ScrewMimic on the bottle and angle error between the axis computed by Screwmimic
opening task in our original experiments as shown in Tab. I. and the ground truth axis. These axis errors are comparable
Thus, ScrewMimic is able to ‚Äúclean up‚Äù the noise and extract to the axis errors for the open bottle task in our original
an axis good enough to bootstrap the fine-tuning step. This experiments (refer to Sec. V Q1) which are 1.42cm mean
shows that even though the quality of the screw axis inferred distance error and 11.2‚ó¶ mean angle error. As can be seen
by ScrewMimic declines with increasing noise, the axis still in the first row of Tab. I in the main paper, ScrewMimic‚Äôs
proves adequate for initiating the fine-tuning process. fine-tuning process is able to correct the initial noisy axis and
b) Naturally occurring noise (perceptual noise): In the succeed at the task for the most part. Since the errors in the
secondexperiment,weevaluatetherobustnessofScrewMimic axes as shown in Tab. VIII are comparable to the error in
to naturally occurring noise when perceiving human demon- our original open bottle experiments, we can infer that
strations. We collect five different human demonstrations for ScrewMimic would be able to fine-tune these noisy axes.
the bottle opening task for the same pose of the bottle. Vari- This shows that despite the diversity in the trajectories due to
ability in the trajectories arises from differences in individual variationsindemonstrationsanddetectionnoise,ScrewMimic
demonstrations and noise from the hand-pose detector. We consistently infers a screw axis with an accuracy that proves
compare the screw axis computed by ScrewMimic for these adequate for initiating the fine-tuning process.
five demonstrations to a manually annotated ground truth
axis. Fig. 11 shows the qualitative results for this experiment:
Fig. 11 (a) on the left helps visually compare the five human
trajectories and the corresponding screw axes as computed(a) Comparing Screw Axis extracted from (b) Comparing each screw axis extracted by
ScrewMimic for 5 human demonstrations ScrewMimic with the ground-truth screw axis
Fig.11. Screwaxisextractedforfivehumandemonstrations.(a)FivehumantrajectoriesandtheircorrespondingscrewaxesextractedbyScrewMimic.
(b) Individual trajectories and extracted screw axis along with the ground truth trajectory and screw axis. Despite the diversity in the trajectories due to
variationsindemonstrationsanddetectionnoises,ScrewMimicisabletoextractascrewaxissufficientlyaccurateforfine-tuning.
TABLEVIII
ANALYZINGSCREWAXISEXTRACTEDFROMFIVEHUMANDEMONSTRATIONS
Demo 1 Demo 2 Demo 3 Demo 4 Demo 5
Distance between GT and
Extracted axes (cm) 0.91 cm 1.26 cm 1.45 cm 1.33 cm 1.10 cm
Angle between GT axis
and Extracted axis (degrees) 6.0‚ó¶ 6.5‚ó¶ 6.4‚ó¶ 12.3‚ó¶ 8.7‚ó¶