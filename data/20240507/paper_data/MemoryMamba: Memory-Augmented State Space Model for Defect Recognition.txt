MemoryMamba: Memory-Augmented State Space
Model for Defect Recognition
QianningWang1,HeHu2,YuchengZhou3
1NanjingAuditUniversity,2Xi‚ÄôanUniversityofScienceandTechnology
3SKL-IOTSC,CIS,UniversityofMacau
yucheng.zhou@connect.um.edu.mo
Abstract
As automation advances in manufacturing, the demand for precise and sophis-
ticated defect detection technologies grows. Existing vision models for defect
recognitionmethodsareinsufficientforhandlingthecomplexitiesandvariationsof
defectsincontemporarymanufacturingsettings. Thesemodelsespeciallystruggle
inscenariosinvolvinglimitedorimbalanceddefectdata.Inthiswork,weintroduce
MemoryMamba,anovelmemory-augmentedstatespacemodel(SSM),designed
toovercomethelimitationsofexistingdefectrecognitionmodels. MemoryMamba
integratesthestatespacemodelwiththememoryaugmentationmechanism,en-
ablingthesystemtomaintainandretrieveessentialdefect-specificinformationin
training. Itsarchitectureisdesignedtocapturedependenciesandintricatedefect
characteristics,whicharecrucialforeffectivedefectdetection. Intheexperiments,
MemoryMambawasevaluatedacrossfourindustrialdatasetswithdiversedefect
types and complexities. The model consistently outperformed other methods,
demonstratingitscapabilitytoadapttovariousdefectrecognitionscenarios.
1 Introduction
Theadventofdeeplearningtechnologieshassignificantlyadvancedvariousindustries[7,32,29,6,5],
particularlymanufacturing,bytransformingdefectrecognitionprocessesessentialforqualitycontrol
[41,46]. Inmanufacturing,defectrecognitionplaysapivotalroleinenhancingproductionefficiency,
reducingcosts,andensuringproductreliability. Thedemandforsophisticatedandprecisedefect
recognition systems has intensified as industries progress towards more automated and precise
manufacturing techniques. These systems are essential not only for reducing the incidence of
defectiveproductsreachingconsumersbutalsoforenhancingtheoverallsustainabilityofproduction
linesthroughwastereductionandimprovedsafetyprotocols.
TheadventofConvolutionalNeuralNetworks(CNNs)hassignificantlytransformeddefectrecog-
nition,enhancingbothaccuracyandautomation[52,64]. Thesemodelsspanfromadaptationsof
established architectures like VGG [1] and ResNet [40] to more intricate configurations such as
T-CNN[38]andGCNN[54],havesignificantlyelevatedtheprecisionandspeedofdefectrecognition
systems[26]. Theincorporationoftechniquessuchastransferlearningandclassifierfusionfurther
enrichestheiradaptabilityandrobustnessacrossdiversemanufacturingenvironments. Neverthe-
less,thesemodelsfrequentlyencounterchallengesinscenarioscharacterizedbylimitedorhighly
imbalanceddefectsamples,i.e.,conditionsthatareprevalentinspecializedindustrialsettings. The
dependency of these models on extensively annotated datasets to achieve high performance is a
significantlimitation,particularlyinenvironmentswheresuchdataisscarce.
StateSpaceModels(SSMs)[21]haverecentlyprovidednewavenuestoaddressthesechallenges
effectively. The Mamba model [19] and its variants in Computer Vision, such as VMamba [35]
Preprint.Underreview.
4202
yaM
6
]VC.sc[
1v37630.5042:viXraandVIM[69],havedemonstratedconsiderablepotentialinimprovingvisionrecognition. VMamba
reducescomputationalcomplexitybyleveragingtheCross-ScanModule(CSM),whichperforms1D
selectivescanningina2Dimagespace,therebyenablingglobalreceptivefieldswithouttheheavy
computationalcostassociatedwithVisionTransformers(ViTs). Ontheotherhand,VIMemploys
bidirectionalstatespacemodelsalongwithpositionembeddingstohandleimagesequences,which
helpsincapturingcomprehensivevisualdatanecessaryforidentifyingsubtleandcomplexdefects.
Moreover,theefficiencyandscalabilityoftheMambamodel,duetoitshardware-awaredesign,make
itidealfordeploymentinindustrialsettingswherereal-timeprocessingisessential.
To address the above problems, we propose MemoryMamba, a novel memory-augmented state
spacemodelspecificallydesignedfordefectrecognitiontasks. ThearchitectureofMemoryMamba
combinesstatespacetechniqueswithmemoryaugmentationtoeffectivelycapturedependenciesand
intricatedefectcharacteristics. Themodelincorporatescoarse-andfine-grainedmemorynetworksto
betterretainandaccesscriticaldefectinformationfrompreviouslytrainedsamples. Additionally,we
introduceafusionmodulethatintegratesfeaturesextractedfromthesememorynetworks,enhancing
themodel‚Äôscapability. Optimizationstrategiesbasedoncontrastivelearningandmutualinformation
maximizationarealsoproposedforcoarse-andfine-grainedmemorynetworks,respectively.
Intheexperiments,weevaluatetheeffectivenessofMemoryMambabyconductingcomprehensive
experimentsacrossfourindustrialdefectrecognitiondatasets,encompassingarangeofdefecttypes
andcomplexities. Inaddition,MemoryMambaconsistentlyoutperformstheexistingmodels.
Themaincontributionsofourpaperareasfollows:
‚Ä¢ To the best of our knowledge, MemoryMamba is the first state space model for defect
recognitioninindustrialapplications.
‚Ä¢ TheMemoryMambamodelincorporatesanovelmemory-augmentedmechanismthatallows
fortheretentionandefficientretrievalofcriticaldefect-relatedinformationfromhistorical
data.
‚Ä¢ Wedesigntheoptimizationmethodsforcoarse-andfine-grainedmemorynetworks,and
propose a fusion module to integrate the visual feature and memory vector. Moreover,
weproposeoptimizationstrategies,basedoncontrastivelearningandmutualinformation
maximization,forcoarse-grainedandfine-grainedmemorynetworks,respectively.
‚Ä¢ Our experiments conducted a comprehensive comparative analysis with existing defect
detectionmodels,demonstratingMemoryMamba‚Äôssuperiorperformance.
2 RelatedWork
2.1 DefectRecognition
Withtheadvancementofdeeplearningapplicationsacrossvariousindustries[8,33,68,67],defect
detectiontechnologieshavebecomecrucialinenhancingproductqualityandoperationalefficiency
withinthemanufacturingindustry,particularlywiththeemergenceofIndustry4.0.Thesetechnologies
integratemachinelearningandcomputervision,transformingtraditionaldefectdetectionmethods
toachieveunprecedentedaccuracyandefficiency[2,3,11]. Significantadvancementsincludethe
TensorConvolutionalNeuralNetwork(T-CNN)developedbyMartin-Ramiroetal.[38],whichoffers
reduced parameter counts and improved training speeds without sacrificing accuracy. Similarly,
Shi et al. [43] introduced the Center-based Transfer Feature Learning with Classifier Adaptation
(CTFLCA),effectivelyadaptingtodiverseimagedistributionsindifferentmanufacturingsettingsand
achievinghighdefectdetectionaccuracies. Zaghdoudietal.[61]deviseaclassifierfusionapproach
for steel defect classification, combining SVM and RF with Bayesian rule, enhancing accuracy
andspeed. Moreover,thereisanattempttoenhancetheVGG19intoaMultipathVGG19,which
improveddefect detectionacross various datasetsApostolopoulos andTzani [1]. In contrast, Fu
etal.[14]effectivelyusedapretrainedVGG16modelwithacustomCNNclassifierfordetecting
surfacedefectsonsteelstrips,particularlyindata-limitedscenarios. IntheareaofX-rayimaging
forqualitycontrol, Rafieietal.[40]utilizedaResNet-basedmodeloptimizedthroughstructural
pruningtoenhancedefectdetectioninmineralwoolproduction. Garc√≠aP√©rezetal.[18]developed
AutomatedDefectRecognition(ADR)systemsusingCNNsthatboostthereliabilityandspeedof
industrialX-rayanalysiswhileminimizingsubjectivediscrepancies. Gamdhaetal.[15]develop
2Sim-ADR,usingsyntheticdataandraytracingtotrainCNNsforX-rayanomalydetectionwith87%
accuracy. Inaddition,Gaoetal.[16]proposedamethodbasedonmultilevelinformationfusion,ideal
forsmallsamplesizes,usingaGaussianpyramidwiththreeVGG16networkstoenhancerecognition
accuracy. asubsequentwork[17]involvedasemi-supervisedlearningapproachwithCNNsenhanced
byPseudo-Label,leveragingbothlabeledandunlabeleddatatoimprovedefectdetection.
DifferentfromCNN-basedmethods,Wangetal.[54]introducedtheGraphguidedConvolutional
NeuralNetwork(GCNN),whichincorporatesagraphguidancemechanismintoCNNstoenhance
featureextractionandmanagevariationswithindefectclasses. asubsequentwork[50]introduced
adeformableconvolutionalnetwork(DC-Net)designedformixed-typedefectdetectioninwafer
maps,achievinghighaccuracywithitsnovelstructuraldesign. YuandLu[59]presentamanifold
learningsystemwithJLNDAfordefectdetectioninwafermaps,outperformingtraditionalmethods
usingWM-811Kdata. SpecializedCNNadaptationsforspecificindustrialapplicationsincludeTao
etal.[47]forspring-wiresocketsandMentourietal.[39]foronlinesurfacedefectmonitoringinthe
steel-makingindustry. Yangetal.[58]developedamodelfordetectingwindturbinebladedamage,
demonstratingsuperiorperformanceinchallengingenvironments. Otherworksindefectdetection
includetheuseofadvancedneuralnetworkmodelsbyKonovalenkoetal.[28]andChengandYu
[9]forrolledmetalandsteelsurfacedefects. Suetal.[45]developedaComplementaryAttention
Network(CAN)withinafasterR-CNNframeworktorefinedefectdetectioninsolarcellimages.
Zhangetal.[63]enhanceDETRwithResNet,ECA-Net,dynamicanchors,anddeformableattention
forsuperiorcastingdefectdetection.
2.2 StateSpaceModels
Different from Transformers [49, 65, 66, 30], State Space Models (SSMs) continue to evolve,
increasinglyshapingthefrontierofsequencemodelingbytacklingthechallengesofcomputational
efficiency and long-range dependency management across various data types [53, 20, 62]. For
timeseriesforecasting,Xuetal.[57]introducedMambaformer,ahybridmodelcombiningMamba
andTransformer,efficientlymanagingbothlongandshort-rangedependenciesandoutperforming
traditionalmodels.Liangetal.[31]proposeBi-Mamba4TS,amodelenhancinglong-termforecasting
withefficientcomputationandadaptivetokenization,outperformingcurrentmethodsinaccuracy.
In computer vision, Chen and Ge [4] presented MambaUIE, optimizing state space models for
Underwater Image Enhancement by efficiently combining global and local features, drastically
reducingcomputationaldemandswhilemaintaininghighaccuracy.
Forimagerestoration, DengandGu[12]introducedtheChannel-AwareU-ShapedMamba(CU-
Mamba)model,whichincorporatesadualStateSpaceModelintotheU-Netarchitecturetoefficiently
encodeglobalcontextandpreservechannelcorrelations. Inhyperspectralimagedenoising,Liuetal.
[34]presentedHSIDMamba,aSelectiveStateSpaceModelthatintegratesadvancedspatial-spectral
mechanisms,significantlyimprovingefficiencyandperformance,outperformingexistingtransformer-
basedmethodsby30%. Moreover,Wangetal.[51]introducedInsectMamba,integratingStateSpace
Models, CNNs, Multi-Head Self-Attention, and MLPs in Mix-SSM blocks to effectively extract
detailedfeaturesforpreciseinsectpestclassification,demonstratingsuperiorperformanceonvarious
datasets. Inthe3Dpointcloudanalysis,Hanetal.[22]developedMamba3D,whichleveragesLocal
NormPoolingandabidirectionalSSM,significantlysurpassingTransformermodelsinbothaccuracy
andscalability.
For medical imaging, several models demonstrate the application of SSMs: Wu et al. [55] pro-
posedH-vmunet, enhancingfeatureextractionwithHigh-order2D-selective-scan(H-SS2D)and
Local-SS2D modules. Yue and Li [60] introduced MedMamba, leveraging Conv-SSM modules
toefficientlycapturelong-rangedependencies. RuanandXiang[42]developedVM-UNet,using
VisualStateSpaceblocksforenhancedcontextualinformationcapture. Furthermore,Wuetal.[56]
presentedUltraLightVM-UNet,ahighlyefficientmodelusinganovelPVMLayerforparallelfeature
processing.
InenhancingthecapabilitiesofSSMs,Heetal.[24]introducedDenseSSM,integratingshallow-layer
hiddenstatesintodeeperlayerstoimproveperformancewhilemaintainingefficiency.Smithetal.[44]
introducedConvS5,aconvolutionalstatespacemodelthatexcelsinlongspatiotemporalsequence
modeling,trainingfasterandgeneratingsamplesmoreefficientlythancompetitors. Fathullahetal.
[13]developedMH-SSM,amulti-headstatespacemodelthatsurpassesthetransformertransducer
onLibriSpeechandachievesstate-of-the-artresultswhenintegratedintotheStateformer.
33 Preliminaries
3.1 StateSpaceModels
State space models (SSMs) constitute a robust framework for the analysis of time series data,
encapsulating the dynamics of systems through a series of mathematical representations. These
models articulate the time series as a function of latent states and observations, with the state
equationsdelineatingtheevolutionoftheselatentstates,andtheobservationequationsdescribingthe
measurementsderivedfromthesestates.
Theevolutionofthestatevectorx attimetisgovernedbythestatetransitionequation:
t
x =F x +G w , (1)
t t t‚àí1 t t
whereF denotesthestatetransitionmatrixthatdefinesthedynamicsofthestatevector,G represents
t t
thecontrol-inputmatrixmodulatingtheinfluenceoftheprocessnoise,andw isassumedtofollowa
t
GaussiandistributionwithzeromeanandcovariancematrixQ .
t
Theobservationmodelrelatestheobserveddatay tothestatevectorthrough:
t
y =H x +v , (2)
t t t t
whereH istheobservationmatrixfacilitatingthemappingfromthestatespacetotheobserveddata,
t
andv istheobservationnoise,typicallymodeledasGaussianwithzeromeanandcovariancematrix
t
R .
t
The efficacy of state space models in capturing the dynamics of various systems hinges on the
precise characterization of the matrices F , G , H , and the noise processes Q and R . These
t t t t t
matricesmaybestaticormayvarywithtime,reflectingthechangingdynamicsofthesystemunder
study. Theestimationofthelatentstatesx fromtheobservationsy generallyemploysrecursive
t t
algorithms such as the Kalman filter for linear models and particle filters for nonlinear variants.
Thesemethodologiesrelyonassumptionsregardingtheinitialstatedistributionandthestatistical
propertiesofthenoisecomponents. Thisfoundationaldescriptionunderscorestheadaptabilityof
statespacemodelsinaddressingamyriadofapplicationsacrossfields,wheretheyarepivotalin
modelingdynamicsystemssubjectedtostochasticdisturbancesandobservationalnoise.
4 MemoryMamba
Inthissection,wefirstelaborateontheoverallarchitectureofMemoryMamba. Subsequently,we
introduceourMem-SSMBlock,whichincludesourproposedCoarse-andFine-GrainedMemory
EncodingandtheFusionModule.
4.1 OverallArchitecture
GivenanimageIwithsizeofH√óW√ó3,theMemoryMambamodelstartswiththePatchEmbedding
procedure,asshowninFigure1. TheinputimageI istransformedintoembeddedpatchfeaturesF
0
withdimensionsof H √ó W √óC,i.e.,
4 4
F =PatchEmbed(I), (3)
0
whereF 0‚ààRH 4√óW 4 √óC denotestheembeddedpatchfeatures. AfterthePatchEmbedding,weemploy
theMem-SSMBlockstoiterativelyrefinethefeaturerepresentations,i.e.,
F =Mem-SSM-Block(F ), (4)
i i‚àí1
where F and F are the feature sets at the input and output of the i-th Mem-SSM Block, re-
i i‚àí1
spectively. OurmodelconsistsofN Mem-SSMBlocks. Eachblockoperationfurthercompresses
spatial dimensions and augments the channel capacity, effectively trading spatial granularity for
featuredepth. ThefinaloutputF isahigh-dimensionalrepresentationthatencapsulatestheimage‚Äôs
N
semanticcontent.
ThefinaloutputF isthenfedintoaMultilayerperceptron(MLP)forclassification,i.e.,
N
h=MLP(F ), (5)
N
4ùêª ùëä ùêª ùëä ùêª ùëä ùêª ùëä
√ó √óùê∂ √ó √ó2ùê∂ √ó √ó8ùê∂ √ó √ó2ùê∂
4 4 8 8 16 16 32 32
H√óW√ó3
Mem-SSM Mem-SSM Mem-SSM Mem-SSM
Block Block Block Block
√ó2 √ó2 √ó2 √ó2
Figure1: Overviewofourmethod.
Mem-SSM Coarse-& Fine-Grained FusionModule
LayerNorm Memory Encoding Feature
Coarse-Grained Image Fine -Grained
Linearlayer Convolution MemoryFeature Feature MemoryFeature
Coarse-&Fine- Coarse-Grained Fine-Grained
GrainedMemory MemoryNetwork MemoryNetwork
Encoding
Linearlayer Pooling Pooling Pooling
2D Selective
Scan module
LayerNorm √ó MM eM mem oe rm o yro y Cry C elC e lle lll √ó MM emM e omMe rm oM ye rM mo Cye rm eoe y C lrm o leyCr lo y leCr ly C lelC e lle lll Similarity Similarity
√ó
Fusion Mapping Mapping
Linearlayer Module
+ + √ó √ó
Figure2: DetailsofMem-SSMBlock.
wherehdenotesthehiddenvectoroftheMLP. Then,weutilizethesoftmaxfunctiontocompute
thepredictedprobabilitydistributionovertheclasses,i.e.,
p=softmax(h), (6)
wherep‚ààRK denotesthepredictedprobabilitydistributionoverK classes. Finally,ourmethod
canbetrainedend-to-endbyminimizingthecross-entropylossbetweenthepredictedprobability
distributionpandtheground-truthlabely,i.e.,
L =‚àíy¬∑log(p), (7)
cls
whereL denotesthecross-entropyloss.
cls
4.2 Mem-SSMBlock
Duetothelimitedavailabilityofdefectsamples,wedesigntheMem-SSMBlock,whichintegrates
memorynetworkstolearnmemoryinformationfromtrainingsamples. AsshowninFigure2,the
Mem-SSMBlockconsistsofmemoryencodingandselectivescanning,therebyextractingarobust
representationoftheinputthatisparticularlysensitivetothenuancesrequiredforaccuratedefect
detection.
Theblockoperatesontheinput features F byinitiallypassingthemthroughalinear layerto
i‚àí1
produceanintermediatefeaturesetZ ,whichcanbeformallyexpressedas:
i
Z =Linear(F ;Œ∏ ), (8)
i i‚àí1 Linear
whereŒ∏ representsthelearnableparametersofthelineartransformation.
Linear
Followingtheinitiallineartransformation,theMem-SSMBlockintroducestheCoarse-andFine-
GrainedMemoryEncodingmodule,whichisresponsibleforcapturingandencodinghierarchical
memorystates. Thememoryencodingcanbeformulatedas:
Mcoarse,Mfine =MemoryEncode(Z ;Œò ), (9)
i i i MemoryEncode
whereMcoarseandMfinedenotethecoarseandfinememorystates,respectively,andŒò
i i MemoryEncode
denotesthesetofparametersgoverningthememoryencodingprocess.
5
gniddebmEhctaP
gnigreMhctaP gnigreMhctaP gnigreMhctaPTointegratetheseencodedmemorystateswiththeintermediatefeatures,afusionmoduleisintro-
duced:
F¬Ø =FusionModule(Mcoarse,Mfine,Z ;Œò ), (10)
i i i i Fusion
whereF¬Ø denotesthefusedfeaturesetandŒò representstheparametersofthefusionmodule.
i Fusion
ThefusedfeaturesF¬Ø arethenprocessedthroughthe2DSelectiveScanmodule,whichselectively
i
emphasizesimportantfeatureresponseswhilesuppressinglessrelevantones:
S =SelectiveScan(F¬Ø;Œ∏ ), (11)
i i SelectiveScan
whereS istheselectivelyscannedfeatureset,andŒ∏ aretheparametersofthismodule.
i SelectiveScan
TheoutputoftheSelectiveScanmoduleisthennormalizedusingaLayerNormalizationstep:
N =LayerNorm(S ;Œ≥,Œ≤), (12)
i i
whereŒ≥ andŒ≤ aretheparametersforscalingandshiftingduringthenormalizationprocess.
Thenormalizedfeaturesarecombinedwiththeoriginalinputfeaturesthrougharesidualconnection,
followedbyasecondlineartransformation:
F =Linear(F +N ;Œ∏‚Ä≤ ), (13)
i i‚àí1 i Linear
whereŒ∏‚Ä≤ representsthelearnableparametersofthesecondlinearlayer,whichcompletesthe
Linear
processingwithintheMem-SSMBlock. Theresidualconnectionhelpspreservetheoriginalfeature
informationwhileallowingthenetworktolearnmodificationstothefeaturesetadaptively.
By repeatedly applying the Mem-SSM Block, MemoryMamba progressively refines the visual
representation.
4.2.1 Coarse-&Fine-GrainedMemoryEncoding
TheCoarse-&Fine-GrainedMemoryEncodingisthecruxofourMem-SSMBlock,asitunderpins
themodel‚Äôsabilitytodiscernandencodevaryinglevelsoffeaturedetails. Thisencodingmechanism
ispivotalfornuancedtaskssuchasdefectdetection,wherefinedistinctionscandeterminethecorrect
classification.
Thememoryencodingprocessbeginswiththeapplicationofaconvolutionoperationtotheinterme-
diatefeaturesetZ ,whichservestoextractspatialhierarchieswithinthedata:
i
FÀú =Conv(Z ;Œ∏ ), (14)
i i Conv
where FÀú is the convolution-processed feature set and Œ∏ denotes the convolutional layer‚Äôs
i Conv
parameters.
UponobtainingFÀú,weproceedtomapthesefeaturesintomemoryvectorsthatembodythememory
i
stateatbothcoarseandfinelevels. Thismappingisachievedthroughaseriesoftransformationsthat
aredesignedtopreservethespatialcorrelationswithinthefeaturemapswhilereducingdimensionality
tothedesiredmemorysize:
h =MLP (FÀú),h ‚ààRc,
c c i c
h =MLP (FÀú),h ‚ààRf, (15)
f f i f
whereh andh representsthememoryqueryvectors,andcandf aresizesofcoarse-&fine-grained
c f
memorynetwork.
UponobtainingFÀú,weproceedtomapthesefeaturesintomemoryvectorsthatembodythememory
i
stateatbothcoarseandfinelevels. Thismappingisachievedthroughaseriesoftransformationsthat
aredesignedtopreservethespatialcorrelationswithinthefeaturemapswhilereducingdimensionality
tothedesiredmemorysize:
h =MLP (FÀú),h ‚ààRc,
c c i c
h =MLP (FÀú),h ‚ààRf, (16)
f f i f
6whereh andh representsthememoryqueryvectors,andcandf aresizesofcoarse-&fine-grained
c f
memorynetwork.
Toassignrelevancetothesememoryqueryvectors,asoftmaxlayerisappliedtogenerateasetof
attentionweights,therebyenablingthemodeltofocusonthemostpertinentmemoryvectorsduring
theretrievalprocess:
Œ± =softmax(h )
c c
Œ± =softmax(h ) (17)
f f
whereŒ± andŒ± representtheattentionweightsformemorynetworks.
c f
Thefinalstepinmemoryencodinginvolvestheaggregationofmemoryknowledgebyweightingthe
rawmemoryvectorswiththeattentionweights,whichisformalizedasfollows:
M¬Ø =(cid:88) Œ± ¬∑M ,
c j c,j
j
M¬Ø =(cid:88) Œ± ¬∑M , (18)
f t f,t
t
whereM¬Ø andM¬Ø symbolizetheaggregatedmemoryvectorsatbothcoarseandfinelevels,andj
c f
andtindextheindividualmemoryvectors. ThememoryvectorsM¬Ø andM¬Ø aresubsequentlymade
c f
availabletotheFusionModule,wheretheyarefusedwithZ .
i
4.2.2 FusionModule
TheFusionModuleisdesignedtomergetheinformationfromboththeCoarse-andFine-Grained
MemoryEncodingwiththeintermediatefeaturesetZ . Thefusionprocessbeginswiththealignment
i
ofthememoryvectorswiththeintermediatefeatures,i.e.,
v =Pooling(M¬Ø ),
c c
v =Pooling(M¬Ø ), (19)
f f
z =Pooling(Z ), (20)
i i
Then,wecalculatethesimilaritybetweenmemoryvectorsandthefeature:
Œ≤ =Similarity(v ,z )
c c i
Œ≤ =Similarity(v ,z ) (21)
f f i
where Œ≤ and Œ≤ are the similarity scores between the coarse and fine memory vectors and the
c f
intermediatefeaturesetrespectively.
Thesimilarityscoresareusedtomodulatethecontributionofthememoryfeatures. Thisisachieved
throughaweightingmechanism,whichamplifiesfeaturesthatarerelevantandsuppressestheless
significantones:
w =Œ≤ ¬∑v ,
c c c
w =Œ≤ ¬∑v , (22)
f f f
wherew andw representtheweightedcoarseandfinememoryvectors. Then,weextendw and
c f c
w tomatchthedimensionalityofZ . TheseexpandedvectorsarethenaddedtoZ toformthe
f i i
enhancedfeaturesetF¬Ø.
i
4.3 MemoryNetworkOptimization
ToenhancetheperformanceoftheMemoryMambaarchitecture,specializedoptimizationstrategies
areemployedfortheCoarse-GrainedandFine-GrainedMemoryNetworks. Thesestrategiesare
designedtorefinethememoryencodingprocessesbyleveragingbothclassificationlossandunique
memory-basedlosses.
74.3.1 Coarse-GrainedMemoryNetworkOptimization
TheCoarse-GrainedMemoryNetworkisoptimizedusingacontrastivelearningapproach,which
leveragesthequerymemoryvectorsh fromdifferentclasses. Thismethodencouragesthenetwork
c
todistinguishbetweenthecoarsefeaturesofvariousclassesmoreeffectively,i.e.,
K K
(cid:88)(cid:88)
L = max(0,Œ¥‚àícos(h ,h )), (23)
Contrastive c,k c,j
k=1j=1
jÃ∏=k
whereh andh arethequerymemoryvectorsofthek-thandj-thclass,respectively,cosdenotes
c,k c,j
thecosinesimilarity,andŒ¥isamarginthatdefinestheminimumacceptabledistancebetweenclasses.
4.3.2 Fine-GrainedMemoryNetworkOptimization
FortheFine-GrainedMemoryNetwork,optimizationisfocusedonmaximizingthemutualinfor-
mationbetweentheintermediatefeaturesZ andtheircorrespondingmemoryrepresentationsM¬Ø ,
i f
whichhavebeenprocessedthroughanMLP.Themutualinformationismaximizedtoensurethatthe
memorynetworkcapturesdetailedandrelevantfeaturesthatarecrucialforfine-grainedtasks,i.e.,
(cid:34) (cid:35)
esim(Zi,M¬Ø f)
L NCE =‚àíE (Zi,M¬Ø f) log (cid:80)
MÀÜ f‚ààN
esim(Zi,MÀÜ f) , (24)
wheresim(¬∑,¬∑)denotesasimilaritymetric(e.g., dotproduct), andN representsasetofnegative
samplesdrawnfromthememorythatarenotcorrespondingtoZ .
i
4.4 OverallTrainingObjective
Theoveralltrainingobjectivecombinestheclassificationlosswiththememory-specificlossesto
traintheMemoryMambamodeleffectively:
L =L +Œª L +Œª L , (25)
total cls c contrastive f NCE
whereŒª andŒª areweightingfactorsthatbalancethecontributionofthecontrastiveandnoise-
c f
contrastiveestimationlosses,respectively.
5 Experiments
5.1 Dataset
The Aluminum1, GC10 [37], MT [25], and Table1: DetailsofDefectRecognitionDatasets.
NEU[10]datasetsarecrucialforevaluatingthe
Dataset Category Train Test
performanceofthedefectrecognitionmodels.
Eachdatasetincludesadistinctnumberofcat- Aluminum 4 277 123
egoriesandasplitbetweentrainingandtesting GC10 10 1,834 458
data,asshowninTable1. Toassessmodelper- MT 6 1,878 810
formance, we calculate the following metrics: NEU 6 228 3,372
Accuracy(ACC),Precision(Prec),Recall(Rec),
and F1Score. Thecomprehensive evaluationusing these metricsallowsus to thoroughlyassess
theperformanceofourdefectrecognitionmodels,ensuringthattheyarerobustandeffectiveacross
differentscenariosanddatasetcharacteristics.
5.2 ExperimentalSetting
Inthisstudy,weutilizedtheAdamoptimizer[27]tofacilitatethelearningprocessofourmodel.
Thelearningrateissetto2√ó10‚àí5,andthetrainingprocessis10epochs. Theweightdecaywas
implementedatarateof0.01toregularizeandpreventtheco-adaptationofneurons. Additionally,we
incorporatedalinearlearningratedecayoverthecourseoftraining,andawarm-upphase,accounting
1https://aistudio.baidu.com/datasetdetail/133083
8Table2:PerformanceComparisononAluminum Table 3: Performance Comparison on GC10
Dataset. Dataset.
Method ACC Prec Rec F1 Method ACC Prec Rec F1
ResNet18 0.36 0.36 0.38 0.37 ResNet18 0.55 0.55 0.53 0.48
ResNet50 0.42 0.42 0.39 0.40 ResNet50 0.71 0.71 0.67 0.67
ResNet101 0.51 0.51 0.41 0.45 ResNet101 0.74 0.74 0.68 0.66
ResNet152 0.59 0.59 0.52 0.55 ResNet152 0.77 0.77 0.73 0.71
DeiT-S 0.36 0.36 0.28 0.32 DeiT-S 0.83 0.83 0.79 0.79
DeiT-B 0.52 0.52 0.45 0.48 DeiT-B 0.84 0.84 0.82 0.82
Swin-T 0.24 0.24 0.35 0.28 Swin-T 0.71 0.71 0.75 0.70
Swin-S 0.49 0.49 0.51 0.50 Swin-S 0.79 0.79 0.70 0.70
Swin-B 0.62 0.62 0.55 0.58 Swin-B 0.86 0.86 0.78 0.79
Vmamba-T 0.35 0.36 0.38 0.37 Vmamba-T 0.79 0.79 0.80 0.78
Vmamba-S 0.55 0.55 0.59 0.57 Vmamba-S 0.83 0.83 0.78 0.79
Vmamba-B 0.65 0.65 0.52 0.58 Vmamba-B 0.83 0.83 0.84 0.82
MemoryMamba 0.75 0.75 0.54 0.63 MemoryMamba 0.90 0.90 0.89 0.89
Table 4: Performance Comparison on MT Table 5: Model performance comparison on
Dataset NEUDataset.
Method ACC Prec Rec F1 Method ACC Prec Rec F1
ResNet18 0.69 0.69 0.56 0.55 ResNet18 0.86 0.86 0.78 0.72
ResNet50 0.83 0.83 0.71 0.72 ResNet50 0.89 0.86 0.89 0.87
ResNet101 0.86 0.86 0.77 0.79 ResNet101 0.94 0.94 0.93 0.93
ResNet152 0.88 0.88 0.73 0.78 ResNet152 0.94 0.94 0.94 0.94
DeiT-S 0.80 0.80 0.75 0.75 DeiT-S 0.89 0.89 0.86 0.86
DeiT-B 0.84 0.84 0.77 0.78 DeiT-B 0.93 0.93 0.91 0.91
Swin-T 0.82 0.82 0.62 0.67 Swin-T 0.88 0.87 0.88 0.88
Swin-S 0.83 0.83 0.71 0.74 Swin-S 0.91 0.91 0.85 0.84
Swin-B 0.88 0.88 0.76 0.80 Swin-B 0.92 0.92 0.92 0.92
Vmamba-T 0.87 0.87 0.89 0.86 Vmamba-T 0.91 0.92 0.89 0.90
Vmamba-S 0.91 0.91 0.91 0.90 Vmamba-S 0.92 0.91 0.91 0.91
Vmamba-B 0.92 0.92 0.87 0.89 Vmamba-B 0.94 0.94 0.94 0.94
MemoryMamba 0.96 0.96 0.97 0.96 MemoryMamba 0.99 0.99 0.99 0.99
for5%ofthetotaltrainingduration,wasalsointegrated.Duringthisphase,thelearningrategradually
increasedfromzerotothesetinitialrate. Thebatchsizeissetat64tooptimizethemodel. Training
isconductedusinganNVIDIA80GBA100GPU.ThecomparisonmethodsincludeResNet[23],
DeiT[48],Swin-Transformer(Swin[36]),andVmamba[35].
5.3 Results
Ourexperimentalresultsacrossfourdatasets,i.e.,Aluminum,GC10,MT,andNEU,demonstratethe
superiorperformanceoftheMemoryMambamodel. ThecomparisonresultsareshowninTable2,
Table3,Table4,andTable5.IncomparisontotraditionalarchitectureslikeResNetandtransformer-
basedmodelssuchasDeiTandSwinTransformers,MemoryMambaconsistentlyachievedthehighest
scoresinAccuracy,Precision,Recall,andF1Score. Notably,itoutperformedinchallengingdefect
detection scenarios, achieving as high as 99% in all evaluated metrics on the NEU dataset. The
integrationofCoarse-andFine-GrainedMemoryEncodingsignificantlyenhancesthemodel‚Äôsability
tocapturedetailedcontextualinformation,thusimprovingitseffectivenessincomplexvisualpattern
recognitiontasksacrossdiverseconditions.
5.4 AblationStudy
Intheablationstudy,weevaluatethecontributionoftheCoarse-GrainedMemoryNetwork(CMN),
Fine-GrainedMemoryNetwork(FMN),andtheFusionModulewithintheMemoryMambaarchitec-
ture. TheremovalofeachcomponentconsistentlyledtoadecreaseinbothaccuracyandF1scores,
underscoringtheirindividualandcollectiveimportance. Themostsignificantperformancedrops
occurredwhenmultiplecomponentswereexcludedsimultaneously,highlightingtheirsynergistic
9Table6: AblationStudyofMemoryMamba.
Aluminum GC10 MT NEU
Method
ACC F1 ACC F1 ACC F1 ACC F1
MemoryMamba 0.75 0.63 0.90 0.89 0.96 0.96 0.99 0.99
‚ô¢w/oCMN 0.71 0.61 0.87 0.86 0.95 0.94 0.97 0.97
‚ô¢w/oFMN 0.72 0.62 0.88 0.87 0.95 0.94 0.98 0.98
‚ô¢w/oFusion 0.72 0.61 0.88 0.88 0.95 0.94 0.97 0.98
‚ô¢w/oCMN,Fusion 0.70 0.60 0.86 0.85 0.94 0.92 0.96 0.96
‚ô¢w/oFMN,Fusion 0.70 0.60 0.87 0.85 0.94 0.93 0.96 0.96
‚ô¢w/oCMN,FMN,Fusion 0.65 0.58 0.83 0.82 0.92 0.89 0.94 0.94
0.80 0.94
Cosine Cosine
0.75 0.92
L1 Distance L1 Distance
0.70 L2 Distance 0.90 L2 Distance
0.88
0.65
0.86
0.60
0.84
ACC F1 ACC F1
Figure3: ComparisonofsimilarityevaluationmethodsforFusionModuleonAluminum(left)and
GC10(right)datasets.
0.90
0.74
0.72 0.89
0.70
0.68 0.88
0.66
0.64 ACC 0.87 ACC
F1 F1
0.62
0.86
2 4 6 8 2 4 6 8
Memory Size Memory Size
Figure4: InvestigationofdifferentmemorysizeforCoarse-grainedMemorynetworkonAluminum
(left)andGC10(right)datasets.
effect. ThesefindingsemphasizethecriticalrolesofCMNandFMNincapturinghierarchicalfeature
detailsandtheFusionModuleineffectivelyintegratingthesefeatures,whicharevitalforthemodel‚Äôs
performanceandrobustnessinindustrialdefectdetectiontasks.
5.5 ImpactonFusionModule
TheFusionModuleiscriticaltoMemoryMamba,directlyinfluencingitsclassificationaccuracyand
F1scores,asdemonstratedinFigure3. Thismodule‚ÄôsroleinintegratingCoarse-andFine-Grained
Memory Encodings with the feature set is validated through performance metrics on Aluminum
andGC10datasetsusingdifferentsimilarityevaluationmethods. OurfindingshighlightthatCosine
similarityachieveshigherperformanceonAluminumandGC10. Thechoiceofsimilaritymetric
thusplaysacrucialroleintuningtheFusionModuleforoptimaldefectdetectionperformance.
5.6 ImpactonMemoryNetworks
Theperformanceofmemorynetworksiscrucialfortherobustnessofdefectrecognitionsystems.
Our investigation, illustrated in Figures 4 and 5, highlights the influence of memory size on the
accuracy(ACC)andF1ScoreofCoarse-grainedandFine-grainedMemorynetworks,respectively.
FortheAluminumdataset,theCoarse-grainedMemorynetworkexhibitsapeakperformanceata
memorysizeof4,withdiminishingreturnsasthesizeincreases. OnthemorecomplexGC10dataset,
10
serocS
ecnamrofreP
serocS
ecnamrofreP0.90
0.74 ACC
0.72
F1
0.70 ACC
0.68 0.89
F1
0.66
0.64
0.62
0.88
2 4 6 8 2 4 6 8
Memory Size Memory Size
Figure5: InvestigationofdifferentmemorysizeforFine-grainedMemorynetworkonAluminum
(left)andGC10(right)datasets.
0.80 0.94
Cosine Cosine
0.75 0.92
L1 Distance L1 Distance
0.70 L2 Distance 0.90 L2 Distance
0.88
0.65
0.86
0.60
0.84
ACC F1 ACC F1
Figure6: ImpactofdifferentsimilaritycalculationmethodsforCoarse-grainedMemorynetworkon
Aluminum(left)andGC10(right)datasets.
0.80 0.94
Cosine Cosine
0.75 0.92
L1 Distance L1 Distance
0.70 L2 Distance 0.90 L2 Distance
0.88
0.65
0.86
0.60
0.84
ACC F1 ACC F1
Figure7: ImpactofdifferentsimilaritycalculationmethodsforFine-grainedMemorynetworkon
Aluminum(left)andGC10(right)datasets.
bothmemorynetworktypesshowasignificantperformancevariationwithmemorysizechanges.
Theseobservationssuggestthattheoptimalmemorysizeiscontextuallydependentonthedataset
granularityandthenetwork‚Äôsmemorytype.
5.7 SimilarityCalculationonMemoryNetworks
Formemorynetworks,thechoiceofsimilaritycalculationmethodcansignificantlyinfluencethe
performance of the model. To investigate this, we employed three different similarity measures:
cosinesimilarity,L1distance,andL2distance,oncoarse-andfine-grainedmemorynetworks.Figures
6and7illustratetheimpactofthesemethodsontheAluminumandGC10datasets. Ourobservations
revealthatcosinesimilarityconsistentlyoutperformstheL1andL2distancesintheaccuracy(ACC)
andF1score.
6 Conclusion
Inthiswork,weintegratestatespacemodelswithmemoryaugmentationtoaddressthelimitations
of other methods in defect recognition systems. We demonstrate that MemoryMamba‚Äôs unique
architecture, whichcombinescoarse-grainedandfine-grainedmemorynetworkswithanovelfu-
sionmodule,effectivelycapturesandutilizeshistoricaldefect-relateddata. Thiscapabilityallows
for enhanced detection of complex and subtle defects that previous models may overlook. The
11
ecnamrofreP
serocS
serocS
ecnamrofreP
serocS
serocSapplicationofcontrastivelearningandmutualinformationmaximizationstrategiesinoptimizing
thesememorynetworksfurtherenrichestherobustnessandaccuracyofthedefectdetectionprocess.
TheexperimentalresultsfromfourdistinctindustrialdatasetshaveunderscoredMemoryMamba‚Äôs
superiorityoverexistingtechnologiessuchasCNNsandVisionTransformers.
References
[1] Ioannis D Apostolopoulos and Mpesiana Tzani. Industrial object, machine part and defect
recognitiontowardsfullyautomatedindustrialmonitoringemployingdeeplearning.thecaseof
multilevelvgg19. arXivpreprintarXiv:2011.11305,2020.
[2] PraharMBhatt,RishiKMalhan,PradeepRajendran,BrualCShah,ShantanuThakar,YeoJung
Yoon,andSatyandraKGupta. Image-basedsurfacedefectdetectionusingdeeplearning: A
review. JournalofComputingandInformationScienceinEngineering,21(4):040801,2021.
[3] YunkangCao,XiaohaoXu,JiangningZhang,YuqiCheng,XiaonanHuang,GuansongPang,
andWeimingShen. Asurveyonvisualanomalydetection: Challenge,approach,andprospect.
arXivpreprintarXiv:2401.16402,2024.
[4] ZhihaoChenandYiyuanGe. Mambauie&sr: Unravelingtheocean‚Äôssecretswithonly2.8flops,
2024.
[5] Zhimin Chen, Longlong Jing, Yang Liang, YingLi Tian, and Bing Li. Multimodal semi-
supervisedlearningfor3dobjects. arXivpreprintarXiv:2110.11601,2021.
[6] ZhiminChen,LonglongJing,LiangYang,YingweiLi,andBingLi. Class-levelconfidence
based3dsemi-supervisedlearning. InProceedingsoftheIEEE/CVFWinterConferenceon
ApplicationsofComputerVision,pages633‚Äì642,2023.
[7] ZhiminChen,YingweiLi,LonglongJing,LiangYang,andBingLi. Pointcloudself-supervised
learningvia3dtomulti-viewmaskedautoencoder. arXivpreprintarXiv:2311.10887,2023.
[8] Zhimin Chen, Longlong Jing, Yingwei Li, and Bing Li. Bridging the domain gap: Self-
supervised3dsceneunderstandingwithfoundationmodels. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[9] XunChengandJianboYu. Retinanetwithdifferencechannelattentionandadaptivelyspatial
featurefusionforsteelsurfacedefectdetection. IEEETransactionsonInstrumentationand
Measurement,70:1‚Äì11,2020.
[10] Ryan Cohn and Elizabeth Holm. Unsupervised machine learning via transfer learning and
k-meansclusteringtoclassifymaterialsimagedata. IntegratingMaterialsandManufacturing
Innovation,10:231‚Äì244,2021. doi: 10.1007/s40192-021-00205-8.
[11] Tam√°sCzimmermann,GastoneCiuti,MarioMilazzo,MarcelloChiurazzi,StefanoRoccella,
Calogero Maria Oddo, and Paolo Dario. Visual-based defect detection and classification
approachesforindustrialapplications‚Äîasurvey. Sensors,20(5):1459,2020.
[12] RuiDengandTianpeiGu. Cu-mamba: Selectivestatespacemodelswithchannellearningfor
imagerestoration,2024.
[13] YassirFathullah,ChunyangWu,YuanShangguan,JuntengJia,WenhanXiong,JayMahadeokar,
ChunxiLiu,YangyangShi,OzlemKalinli,MikeSeltzer,andMarkJ.F.Gales. Multi-headstate
spacemodelforspeechrecognition,2023.
[14] JingwenFu,XiaoyanZhu,andYingbinLi. Recognitionofsurfacedefectsonsteelsheetusing
transferlearning. arXivpreprintarXiv:1909.03258,2019.
[15] Dhruv Gamdha, Sreedhar Unnikrishnakurup, KJ Jyothir Rose, M Surekha, Padma Pu-
rushothaman,BikashGhose,andKrishnanBalasubramaniam. Automateddefectrecognition
on x-ray radiographs of solid propellant using deep learning based on convolutional neural
networks. JournalofNondestructiveEvaluation,40(1):18,2021.
12[16] YipingGao,LiangGao,XinyuLi,andXiVincentWang. Amultilevelinformationfusion-based
deeplearningmethodforvision-baseddefectrecognition.IEEETransactionsonInstrumentation
andMeasurement,69(7):3980‚Äì3991,2019.
[17] YipingGao,LiangGao,XinyuLi,andXuguoYan. Asemi-supervisedconvolutionalneural
network-basedmethodforsteelsurfacedefectrecognition. RoboticsandComputer-Integrated
Manufacturing,61:101825,2020.
[18] AlbertoGarc√≠aP√©rez,Mar√≠aJos√©G√≥mezSilva,andADeLaEscaleraHueso. Automateddefect
recognitionofcastingsdefectsusingneuralnetworks. Journalofnondestructiveevaluation,41
(1):11,2022.
[19] AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselectivestatespaces.
arXivpreprintarXiv:2312.00752,2023.
[20] AlbertGu,KaranGoel,andChristopherR√©.Efficientlymodelinglongsequenceswithstructured
statespaces. arXivpreprintarXiv:2111.00396,2021.
[21] JamesDHamilton. State-spacemodels. Handbookofeconometrics,4:3039‚Äì3080,1994.
[22] XuHan,YuanTang,ZhaoxuanWang,andXianzhiLi. Mamba3d: Enhancinglocalfeaturesfor
3dpointcloudanalysisviastatespacemodel,2024.
[23] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition. InCVPR2016, LasVegas, NV,USA,June27-30, 2016, pages770‚Äì778. IEEE
ComputerSociety,2016.
[24] WeiHe,KaiHan,YehuiTang,ChengchengWang,YujieYang,TianyuGuo,andYunheWang.
Densemamba: Statespacemodelswithdensehiddenconnectionforefficientlargelanguage
models,2024.
[25] YibinHuang,CongyingQiu,andKuiYuan. Surfacedefectsaliencyofmagnetictile. TheVisual
Computer,36(1):85‚Äì96,2020.
[26] ShashiBhushanJhaandRaduFBabiceanu. Deepcnn-basedvisualdefectdetection: Surveyof
currentliterature. ComputersinIndustry,148:103911,2023.
[27] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd
InternationalConferenceonLearningRepresentations,ICLR2015,SanDiego,CA,USA,May
7-9,2015,ConferenceTrackProceedings,2015.
[28] IhorKonovalenko,PavloMaruschak,JanetteBrezinov√°,J√°nVinÀá√°≈°,andJakubBrezina. Steel
surfacedefectclassificationusingdeepresidualneuralnetwork. Metals,10(6):846,2020.
[29] Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Anna Hovakimyan, and Naira Hov-
akimyan. Languagemodelsarefreeboostersforbiomedicalimagingtasks. arXivpreprint
arXiv:2403.17343,2024.
[30] ZhixinLai,XueshengZhang,andSuiyaoChen. Adaptiveensemblesoffine-tunedtransformers
forllm-generatedtextdetection,2024.
[31] AoboLiang,XingguoJiang,YanSun,andChangLu. Bi-mamba4ts: Bidirectionalmambafor
timeseriesforecasting,2024.
[32] TianruiLiu,ChangxinXu,YuxinQiao,ChufengJiang,andWeishengChen. Newsrecommen-
dationwithattentionmechanism. arXivpreprintarXiv:2402.07422,2024.
[33] TianruiLiu,ChangxinXu,YuxinQiao,ChufengJiang,andJiqiangYu. Particlefilterslamfor
vehiclelocalization. arXivpreprintarXiv:2402.07429,2024.
[34] Yang Liu, Jiahua Xiao, Yu Guo, Peilin Jiang, Haiwei Yang, and Fei Wang. Hsidmamba:
Exploringbidirectionalstate-spacemodelsforhyperspectraldenoising,2024.
[35] YueLiu,YunjieTian,YuzhongZhao,HongtianYu,LingxiXie,YaoweiWang,QixiangYe,and
YunfanLiu. Vmamba: Visualstatespacemodel. arXivpreprintarXiv:2401.10166,2024.
13[36] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBaining
Guo. Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InICCV2021,
Montreal,QC,Canada,October10-17,2021,pages9992‚Äì10002.IEEE,2021.
[37] XiaomingLv,FajieDuan,Jia-jiaJiang,XiaoFu,andLinGan. Deepmetallicsurfacedefect
detection: Thenewbenchmarkanddetectionnetwork. Sensors,20(6):1562,2020.
[38] Pablo Martin-Ramiro, Unai Sainz de la Maza, Roman Orus, and Samuel Mugel. Boosting
defectdetectioninmanufacturingusingtensorconvolutionalneuralnetworks,2023.
[39] ZoheirMentouri,HakimDoghmane,AbdelkrimMoussaoui,andHocineBourouba. Improved
cross pattern approach for steel surface defect recognition. The International Journal of
AdvancedManufacturingTechnology,110(11):3091‚Äì3100,2020.
[40] Mehdi Rafiei, Dat Thanh Tran, and Alexandros Iosifidis. Recognition of defective mineral
woolusingprunedresnetmodels. In2023IEEE21stInternationalConferenceonIndustrial
Informatics(INDIN).IEEE,July2023.
[41] ZhongheRen,FengzhouFang,NingYan,andYouWu. Stateoftheartindefectdetectionbased
onmachinevision. InternationalJournalofPrecisionEngineeringandManufacturing-Green
Technology,9(2):661‚Äì691,2022.
[42] JiachengRuanandSunchengXiang. Vm-unet: Visionmambaunetformedicalimagesegmen-
tation,2024.
[43] Yan Shi, Lei Li, Jun Yang, Yixuan Wang, and Songhua Hao. Center-based transfer feature
learning with classifier adaptation for surface defect recognition. Mechanical Systems and
SignalProcessing,188:110001,2023.
[44] JimmyT.H.Smith,ShaliniDeMello,JanKautz,ScottW.Linderman,andWonminByeon.
Convolutionalstatespacemodelsforlong-rangespatiotemporalmodeling,2023.
[45] BinyiSu,HaiyongChen,PengChen,GuibinBian,KunLiu,andWeipengLiu. Deeplearning-
basedsolar-cellmanufacturingdefectdetectionwithcomplementaryattentionnetwork. IEEE
TransactionsonIndustrialinformatics,17(6):4084‚Äì4095,2020.
[46] JingSu,ChufengJiang,XinJin,YuxinQiao,TingsongXiao,HongdaMa,RongWei,ZhiJing,
JiajunXu,andJunhongLin. Largelanguagemodelsforforecastingandanomalydetection: A
systematicliteraturereview. arXivpreprintarXiv:2402.10350,2024.
[47] XianTao,ZihaoWang,ZhengtaoZhang,DapengZhang,DeXu,XinyiGong,andLeiZhang.
Wiredefectrecognitionofspring-wiresocketusingmultitaskconvolutionalneuralnetworks.
IEEETransactionsonComponents,PackagingandManufacturingTechnology,8(4):689‚Äì698,
2018.
[48] HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,and
Herv√©J√©gou. Trainingdata-efficientimagetransformers&distillationthroughattention. In
ICML2021,18-24July2021,VirtualEvent,volume139ofProceedingsofMachineLearning
Research,pages10347‚Äì10357.PMLR,2021.
[49] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
≈ÅukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformation
processingsystems,30,2017.
[50] JunliangWang,ChuqiaoXu,ZhengliangYang,JieZhang,andXiaoouLi. Deformableconvolu-
tionalnetworksforefficientmixed-typewaferdefectpatternrecognition. IEEETransactionson
SemiconductorManufacturing,33(4):587‚Äì596,2020.
[51] QianningWang,ChenglinWang,ZhixinLai,andYuchengZhou. Insectmamba: Insectpest
classificationwithstatespacemodel. arXivpreprintarXiv:2404.03611,2024.
[52] TianWang,YangChen,MeinaQiao,andHichemSnoussi. Afastandrobustconvolutional
neural network-based defect detection model in product quality control. The International
JournalofAdvancedManufacturingTechnology,94:3465‚Äì3471,2018.
14[53] Xiao Wang, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, Yao Rong, Weizhe Kong,
JuHuang,ShihaoLi,HaoxiangYang,etal. Statespacemodelfornew-generationnetwork
alternativetotransformers: Asurvey. arXivpreprintarXiv:2404.09516,2024.
[54] YuchengWang, LiangGao, YipingGao, andXinyuLi. Agraphguidedconvolutionalneu-
ral network for surface defect recognition. IEEE Transactions on Automation Science and
Engineering,19(3):1392‚Äì1404,2022.
[55] RenkaiWu,YinghaoLiu,PengchenLiang,andQingChang. H-vmunet: High-ordervision
mambaunetformedicalimagesegmentation,2024.
[56] RenkaiWu,YinghaoLiu,PengchenLiang,andQingChang. Ultralightvm-unet: Parallelvision
mambasignificantlyreducesparametersforskinlesionsegmentation,2024.
[57] XiongxiaoXu,YueqingLiang,BaixiangHuang,ZhilingLan,andKaiShu. Integratingmamba
andtransformerforlong-shortrangetimeseriesforecasting,2024.
[58] XiyunYang, YanfengZhang, WeiLv, andDongWang. Imagerecognitionofwindturbine
bladedamagebasedonadeeplearningmodelwithtransferlearningandanensemblelearning
classifier. RenewableEnergy,163:386‚Äì397,2021.
[59] JianboYuandXiaoleiLu. Wafermapdefectdetectionandrecognitionusingjointlocaland
nonlocallineardiscriminantanalysis. IEEETransactionsonSemiconductorManufacturing,29
(1):33‚Äì43,2015.
[60] YubiaoYueandZhenzhangLi. Medmamba: Visionmambaformedicalimageclassification,
2024.
[61] RachidZaghdoudi,AbdelmalekBouguettaya,andAdelBoudiaf.Steelsurfacedefectrecognition
usingclassifiercombination.TheInternationalJournalofAdvancedManufacturingTechnology,
pages1‚Äì17,2024.
[62] HanweiZhang,YingZhu,DanWang,LijunZhang,TianxiangChen,andZiYe. Asurveyon
visualmamba,2024.
[63] LongZhang,Sai-feiYan,JunHong,QianXie,FeiZhou,andSong-linRan. Animproveddefect
recognitionframeworkforcastingbasedondetralgorithm. JournalofIronandSteelResearch
International,30(5):949‚Äì959,2023.
[64] MeiZhang,JinglanWu,HuifengLin,PengYuan,andYananSong. Theapplicationofone-class
classifierbasedoncnninimagedefectdetection. Procediacomputerscience,114:341‚Äì348,
2017.
[65] Yucheng Zhou and Guodong Long. Multimodal event transformer for image-guided story
ending generation. In Proceedings of the 17th Conference of the European Chapter of the
AssociationforComputationalLinguistics,pages3434‚Äì3444,2023.
[66] Yucheng Zhou and Guodong Long. Style-aware contrastive learning for multi-style image
captioning. InFindingsoftheAssociationforComputationalLinguistics: EACL2023,pages
2257‚Äì2267,2023.
[67] YuchengZhou,TaoShen,XiuboGeng,ChongyangTao,CanXu,GuodongLong,BinxingJiao,
andDaxinJiang. Towardsrobustrankerfortextretrieval. InFindingsoftheAssociationfor
ComputationalLinguistics: ACL2023,pages5387‚Äì5401,2023.
[68] YuchengZhou,TaoShen,XiuboGeng,ChongyangTao,JianbingShen,GuodongLong,Can
Xu,andDaxinJiang. Fine-graineddistillationforlongdocumentretrieval. InProceedingsof
theAAAIConferenceonArtificialIntelligence,volume38,pages19732‚Äì19740,2024.
[69] LianghuiZhu,BenchengLiao,QianZhang,XinlongWang,WenyuLiu,andXinggangWang.
Visionmamba: Efficientvisualrepresentationlearningwithbidirectionalstatespacemodel.
arXivpreprintarXiv:2401.09417,2024.
15