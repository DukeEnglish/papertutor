Diffusion2: Dynamic 3D Content Generation via Score
Composition of Orthogonal Diffusion Models
ZeyuYangâˆ— ZijiePanâˆ— ChunGu LiZhangâ€ 
FudanUniversity
https://github.com/fudan-zvg/diffusion-square
Abstract
Recentadvancementsin3Dgenerationarepredominantlypropelledbyimprove-
mentsin3D-awareimagediffusionmodelswhicharepretrainedonInternet-scale
imagedataandfine-tunedonmassive3Ddata,offeringthecapabilityofproducing
highlyconsistentmulti-viewimages. However,duetothescarcityofsynchronized
multi-viewvideodata,itisimpracticaltoadaptthisparadigmto4Dgenerationdi-
rectly. Despitethat,theavailablevideoand3Ddataareadequatefortrainingvideo
andmulti-viewdiffusionmodelsthatcanprovidesatisfactorydynamicandgeo-
metricpriorsrespectively. Inthispaper,wepresentDiffusion2,anovelframework
fordynamic3Dcontentcreationthatleveragestheknowledgeaboutgeometric
consistencyandtemporalsmoothnessfromthesemodelstodirectlysampledense
multi-viewandmulti-frameimageswhichcanbeemployedtooptimizecontinuous
4Drepresentation. Specifically,wedesignasimpleyeteffectivedenoisingstrategy
viascorecompositionofvideoandmulti-viewdiffusionmodelsbasedontheprob-
abilitystructureoftheimagestobegenerated. Owingtothehighparallelismofthe
imagegenerationandtheefficiencyofthemodern4Dreconstructionpipeline,our
frameworkcangenerate4Dcontentwithinfewminutes. Furthermore,ourmethod
circumventstherelianceon4Ddata,therebyhavingthepotentialtobenefitfrom
thescalabilityofthefoundationvideoandmulti-viewdiffusionmodels. Extensive
experimentsdemonstratetheefficacyofourproposedframeworkanditscapability
toflexiblyadapttovarioustypesofprompts.
1 Introduction
Spurredbytheadvancesfromgenerativeimagemodels[13,34,35,17,51],automatic3Dcontent
creation[27,42,37,14]haswitnessedremarkableprogressintermsofefficiency,fidelity,diversity,
andcontrollability.Coupledwiththebreakthroughsin4Drepresentation[47,45],theseadvancements
furtherfostersubstantialdevelopmentindynamiccontent(4D)generation[33,1,15,53,29,10],
which holds significant value across a wide range of applications in animation, film, game, and
MetaVerse.
Recently, 3D content generation has achieved considerable breakthroughs in efficiency. Some
works [20, 21, 32, 41, 38] inject stereo knowledge into the image generation model, enabling
these 3D-aware image generators to produce consistent multi-view images, thereby effectively
stabilizingandacceleratingtheoptimization. Otherefforts[14,7,40,54,36]attempttodirectly
generate3Drepresentations,suchastriplane[6]orGaussiansplatting[18]. However,theefficiency
improvementfromtheseworksislargelydata-driven[46,28,50,8]. Consequently,itisinfeasibleto
âˆ—Equallycontributed
â€ Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with School of Data Science, Fudan
University.
Preprint.
4202
rpA
2
]VC.sc[
1v84120.4042:viXraRef. view
Novel view
Ref. view
Novel view
Ref. view
Novel view
Ref. view
Novel view
Input Time
Figure 1: Diffusion2 is designed to generate dynamic content by creating a dense multi-frame
multi-viewimagematrixinahighlyparalleldenoisingdiffusionprocesswiththecombinationofthe
foundationvideodiffusionmodelandmulti-viewdiffusionmodel. Thegeneratedimagecanbeused
toconstructafull4Drepresentationbybeingfedintoanoff-the-shelf4Dreconstructionpipeline.
adapttheseapproachesto4Dgenerationduetothescarcityofsynchronizedmulti-viewvideodata.
Therefore,mostexistingoptimization-based4Dgenerationworksstillsufferfromslowandunstable
optimization.
However,despitethepaucityof4Ddata,therearevastavailablemonocularvideodataandstatic
multi-viewimagedata. Existingworkshavedemonstratedthatitisfeasibletotraindiffusion-based
generativemodelstolearnthedistributionofthesetwoclassesofdataseparately[40,21,38,3,2].
Consideringthatvideodiffusionmodelstoresthepriorofmotionandtemporalsmoothness, and
multi-viewdiffusionmodelhassoundknowledgeofgeometricalconsistency,combiningthetwo
generativemodelstogenerate4Dassetsbecomesahighlypromisingandappealingapproach.
Therefore, inthispaperweproposeanovel4Dgenerationframework, whichcombinesboththe
videodiffusionmodelandmulti-viewdiffusionmodeltodirectlysamplemulti-framemulti-view
imagearrayimitatingthephotographingprocessof4Dcontent. Todemonstratehowwerealizethis
combination,weassumethatsuchanimagematrixhasanicestructure: theelementsnotinthesame
rowandcolumnareconditionallyindependentofeachother. Basedontheproperty,wedesigna
simpleyeteffectivedenoisingstrategyinwhichtheestimatedscoreisjusttheconvexcombinationof
thescorespredictedbytwofoundationdiffusionmodels. Ourformulationiseasytobeadaptedto
variouspromptsincludingsingleimage,single-viewvideo,andstatic3Dcontent. Unliketheexisting
optimization-basedcounterparts,ourimagegenerationishighlyparallel. Combinedwithefficient
modern4Dreconstructionmethods,wecangeneratehigh-fidelityanddiverse4Dassetswithinjust
severalminutes. Besides,ourapproachcanalsopotentiallybenefitfromthefurtherdevelopmentof
foundationdiffusionmodels[4].
Ourcontributionscanbesummarizedintothreefold: (i)Wepresentanovel4Dcontentgeneration
frameworkthatachieveszero-shotgenerationofrealisticmulti-viewmulti-frameimagearrays,which
can be integrated with an off-the-shelf modern 4D reconstruct pipeline to efficiently create 4D
content. (ii)Weidentifytheconditionalindependencethatexistedinthedistributionoftheelements
composingtheimagearrays.Andbasedonit,wedesignasimpleyeteffectivejointdenoisingstrategy
thatcombinesvideodiffusionmodelandmulti-viewdiffusionmodeltodirectlysamplemulti-view
multi-frameimagearraysfromtheirnaturaldistribution. (iii)Systematicexperimentsdemonstrate
2thatourproposedmethodcanachievesatisfactoryresultsunderdifferenttypesofprompts,including
singleimage,single-viewvideo,andstatic3Dcontent.
2 Reletaedwork
3Dgeneration 3Dgenerationaimsatcreatingstatic3Dcontentfromdifferentpromptsliketextor
image. EarlyeffortsemployedGAN-basedapproaches[9,31]. Recently,significantbreakthroughs
havebeenachievedalongsidetheemergenceofdiffusionmodels[13]inthisdomain. DreamFu-
sion[27]introducedscoredistillationsampling(SDS)tounleashthecreativityindiffusionmodels.
Although such approach has exhibited promising results, the original form of SDS encounters
challengessuchasmodecollapse,multi-faceJanusissues,andtheslowoptimization. Aseriesof
subsequentworks[42,32,41,25,37,48]trytoaddresstheseproblemsbymodifyingthismechanism.
On the other hand, some studies [24, 16, 14, 36, 43] have explored the direct generation of 3D
representations using diffusion models. Another line of research [20, 21, 22, 7, 38] focuses on
generatingdensemulti-viewimageswithsufficient3Dconsistencybytrainingorfine-tuning2D
diffusionmodelson3Ddatasetstomakethemmoresuitablefor3Dgenerationtasks. Thegenerated
imagescanbeusedforreconstructiontoobtaintexturedmeshes,pointcloudsorimplicitradiance
fields. Wealsoadoptthisapproachofdirectlygeneratingconsistentimagesforreconstruction. But
unlikethese3Dcounterparts,thereisnolarge-scalemulti-viewsynchronizedvideodata. Therefore,
weopttocombinegeometricalconsistencypriorsandvideodynamicpriorstogenerateimages.
Videogeneration Videogenerationandpredictionisanactivefieldthathasgainedincreasing
popularity. Recentdiffusionmodel-basedvideogenerationmethodshaveexhibitedunprecedented
levelsofrealism,diversity,andcontrollability. Notably,recentbreakthroughshavedemonstratedthe
scalabilityofvideogenerationmodelscombinedwithTransformersandtheirpotentialasphysical
worldsimulators. VideoLDM[3]wasamongthefirstworkstoapplythelatentdiffusionframe-
work[30]tovideogeneration. ThesubsequentworkSVD[2]followeditsarchitectureandmade
effectiveimprovementstothetrainingrecipe. W.A.L.T[11]employedatransformerwithwindowat-
tentiontailoredforspatiotemporalgenerativemodelingtogeneratehigh-resolutionvideos. VDT[23]
introducedthevideodiffusiontransformertoflexiblycapturelong-distancespatiotemporalcontextin
videosandaspatial-temporalmaskmechanismtouniformlyhandledifferentvideogenerationtasks.
TherecentlyintroducedSORA[4]demonstratedaremarkablecapabilitytogeneratearbitrarilysized
longvideoswithintuitivelyphysicalfidelity. Modelstrainedonlarge-scalevideodatacangenerate
videoswithconsistentgeometryandrealisticdynamics. Furthermore,videodiffusionmodelscan
alsobeconsideredaseffective3Dgenerators[7,12,40]togeneratemulti-viewconsistentimages.
Therefore,webuildourmethodonthisflourishingdomain.
4Dgeneration Animatingcategory-agnosticstuffisachallengingproblemandhasbeenreceivinga
lotofattentionfrombothacademiaandindustry. Comparedto3Dgeneration,4Dgenerationrequires
notonlypredictingconsistentgeometrybutalsogeneratingrealisticanddiversedynamics. Recent
workson4Dgenerationcouldbeclassifiedintotwomainstreamsbasedonthetypeofinputprompt.
Thefirstclassofmethodspredicts4Drepresentationsgivenasingleimageandtextdescriptionas
input. Forinstance,MAV3D[33]directlydeploysSDSinto4Dgenerationandproposesathree-stage
trainingschemetostablygeneratehigh-resolutionvideos. DreamGaussian4D[29]adoptasimilar
three-stage training scheme but switches the underlying 4D representation from Hexplane [5] to
thedeformable3DGaussianandreplacesthethirdstagewiththerefinementofhigh-resolutionUV
texturemaps,therebyachievingsignificantlyefficiencyimprovement. 4DGen[49]firstgeneratesa
setofcandidate3Dassetsaccordingtotheimageortextpromptandthengroundsthe4Dgeneration
onuser-specifiedstatic3Dassetsandmonocularvideosequences. 4D-fy[1]alsoadoptstheideaof
combiningdiffusionpriors. Butdifferentfromourmethod,itcombinesthemduringSDS,resultingin
anextremelyslowgenerationprocessthattypicallytakesseveraldays. Anotherlineofworkpredicts
dynamicobjectsfromasingle-viewvideos. Althoughthemotionislargelydictatedunderthissetting,
generatingspatiotemporalconsistentstructurestillentailsconsiderableuncertaintyduetothelimited
view. Therefore, Consistent4D[15]isfirstproposestouseagenerationapproachtoaddressthis
task. Subsequently,Efficient4D[26]mimicsaphotogrammetry-based3Dobjectcapturepipelineby
directlygeneratingmulti-framemulti-viewcapturesofdynamic3DobjectsthroughSyncDreamer-T
andreconstructing4Drepresentationswiththem. NotethatalthoughEfficient4Dsharesthesame
SDS-freephilosophyasours,itpossessesastrongarchitecturebiasofthefoundationdiffusionmodel
3Input: reference image For do Input: ref-view video and Input: multi-view multi-frame
Output: ref-view multi-view images image array
video and multi- Multi- ğ‘–ğ‘– âˆˆ 0,â‹¯,ğ‘ğ‘âˆ’1 Output: multi-view multi- Output: continuous 4D
view images View frame image array representation
Diffusion
Model â„0,1 â„0,2 â„0,3 â‹¯ â„0,ğ‘‰ğ‘‰
Video
Denoiser
â„1,0 bl &end â„Ì‚1ğ‘–ğ‘–,1 â„Ì‚1ğ‘–ğ‘–,2 â„Ì‚1ğ‘–ğ‘–,3 â‹¯ â„Ì‚1ğ‘–ğ‘–,ğ‘‰ğ‘‰
DV iffi ud se io o n â„2,0 M Viu el wti- step â„Ì‚2ğ‘–ğ‘–,1 Nâ„Ì‚ i2ğ‘–ğ‘–o m,2is ay gâ„ Ì‚ el2ğ‘–ğ‘–a ,3 ate rn ratâ‹¯ yof â„Ì‚2ğ‘–ğ‘–,ğ‘‰ğ‘‰
Model â„3,0 Denoiser â„Ì‚3ğ‘–ğ‘–,1 â„Ì‚3ğ‘–ğ‘–,2 â„Ì‚3ğ‘–ğ‘–,3 â‹¯ â„Ì‚1ğ‘–ğ‘–,ğ‘‰ğ‘‰
â‹¯ â‹¯ â‹¯ â‹¯ â‹¯ â‹¯
(i)Independent condition generation â„ğ¹ğ¹,0 (ii)Parallel denoising for synchronized mulâ„Ì‚ tğ¹ğ¹ğ‘–ğ‘– i,1 -viâ„eÌ‚ğ¹ğ¹ğ‘–ğ‘– w,2 vâ„iÌ‚ğ¹ğ¹ dğ‘–ğ‘–,3 eosâ‹¯ â„Ì‚ğ¹ğ¹ğ‘–ğ‘–,ğ‘‰ğ‘‰ (iii)4D reconstruction
Figure2: TheoverallpipelineofDiffusion2. (i)Givenareferenceimage,Diffusion2 firstindepen-
dentlygeneratestheanimationunderthereferenceview(denotedI )andthemulti-viewimages
0,1:F
atthereferencetime(denotedI )astheconditionforthesubsequentgenerationofthefullmatrix,
0,1:F
denotedI. Dependingonthetypeofgivenprompt,theconditionimagesI orI canbe
1:V,0 0,1:F
specified by users. (ii) Then, Diffusion2 directly samples a dense multi-frame multi-view image
arraybyblendingtheestimatedscoresfrompretrainedvideoandmulti-viewdiffusionmodelsin
thereverse-timeSDE.(iii)Thegeneratedimagearraysareemployedassupervisiontooptimizea
continuous4Dcontentrepresentation.
andisincapableofsynthesizingnoveldynamics. Comparedtopreviousworks,ourframeworkcan
efficientlygeneratediversedynamic4Dcontent,avoidingtheslow,unstable,andintricatemulti-stage
optimization, andhasthepotentialtocontinuouslybenefitfromthescalabilityoftheunderlying
diffusionmodel.
3 Method
Inthissection,wepresentanovelframeworkdesignedforefficientandscalablegenerationof4D
content,whichconsistsoftwostagesasdepictedinFigure2. InSection3.1(stage-1),wewilldiscuss
howtogenerateadensemulti-viewmulti-frameimagearrayforreconstructionthroughahighly
parallelizabledenoisingprocessbyintegratingthepretrainedvideodiffusionmodelandmulti-view
diffusionmodel,andwhyitisfeasible. InSection3.2(stage-2),wewillbrieflyillustratehowto
robustlyreconstruct4Dcontentfromtheimagematrixproducedinthefirststage.
3.1 Imagematrixgeneration
Inthisstage,ourgoalistogeneratedensemulti-framemulti-viewimagesforreconstruction,which
canbedenotedasamatrixofimage
ï£®I Â·Â·Â· I Â·Â·Â· I ï£¹
1,1 1,j 1,F
ï£¯ . . . ... . . . ... . . . ï£º
ï£¯ ï£º
I =(cid:8) I âˆˆRHÃ—WÃ—3(cid:9)V,F =ï£¯I Â·Â·Â· I Â·Â·Â· I ï£º, (1)
i,j i=1,j=1 ï£¯ i,1 i,j i,Fï£º
ï£¯ ï£° . . . ... . . . ... . . . ï£º ï£»
I Â·Â·Â· I Â·Â·Â· I
V,1 V,j V,F
whereV isthenumberofviews,F isthenumberofvideoframes,and(H,W)isthesizeofimages.
WeaimtoconstructagenerativemodelthatallowsustodirectlysampleI âˆ¼p(I).
Now, let us first distract our attention to reviewing existing diffusion-based generators for video
andmulti-viewimages,whichcanbeutilizedforsamplingrealisticimagesthroughthefollowing
probabilisticflowODE:
dx=âˆ’ÏƒË™(t)Ïƒ(t)âˆ‡ logp(x;Ïƒ(t))dt. (2)
x
Here,x=(cid:8)
I
âˆˆRHÃ—WÃ—3(cid:9)N
isaseriesofimageswithN framesorN views,âˆ‡ logp(x;Ïƒ)is
i i=1 x
thescorefunction,whichcanbeparameterizedasâˆ‡ logp(x;Ïƒ)â‰ˆ(D (x;Ïƒ))/Ïƒ2[17,2],where
x Î¸
D (x;Ïƒ)isaneuralnetworktrainedviadenoisingscorematching.
Î¸
4WewanttoextendtheaboveformulationtothesamplingofI. Thequestionis,howdoweestimate
thescorefunctionofthejointdistributionofV Ã—F images?
For simplicity, let I â‰œ {I |1 â‰¤ iâ€² â‰¤ V,iâ€² Ì¸= i}, I â‰œ {I |1 â‰¤ jâ€² â‰¤ F,jâ€² Ì¸= j} and
âˆ’i,j iâ€²,j i,âˆ’j i,jâ€²
I â‰œ {I |1 â‰¤ iâ€² â‰¤ V,1 â‰¤ jâ€² â‰¤ F,iâ€² Ì¸= i,jâ€² Ì¸= j}. We first make an assumption on the
âˆ’i,âˆ’j iâ€²,jâ€²
structureofp(I).
Assumption3.1. GivenanyimageI ,theunderlyinggeometryI andthedynamicsI are
i,j âˆ’i,j i,âˆ’j
conditionallyindependent,i.e.,
p(I ,I |I )=p(I |I )p(I |I ). (3)
âˆ’i,j i,âˆ’j i,j âˆ’i,j i,j i,âˆ’j i,j
Theassumption3.1impliesthatgiventhefrontviewofa3Dobject, itsmotionasseenfromthe
frontdoesnotcorrelatewithitsappearancefromtheback,whichalignswithourintuition. Anatural
corollaryisthatthemollifieddistributionderivedbyaddingGaussiannoiseintothedatadistribution
stillmaintainsconditionalindependence:
Corollary3.1. DenoteIË†asthenoisyversionofI,i.e.,
IË† ={IË† âˆˆRHÃ—WÃ—3}V,F withIË† =Î±I +Îµ , (4)
i,j i=1,j=1 i,j i,j i,j
whereÎ±âˆˆRisaconstantandÎµ âˆˆRHÃ—WÃ—3areindependentGaussiannoises. Thenwehave
i,j
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
p IË† ,IË† |IË† =p IË† |IË† p IË† |IË† . (5)
âˆ’i,j i,âˆ’j i,j âˆ’i,j i,j i,âˆ’j i,j
Thisnicepropertyallowsustosamplethedesiredimagematrixbyprogressivelydenoisingfrompure
Gaussiannoisethroughthecombinationoftwoestimatedscoresofitsmarginaldistribution,which
canbeobtainedfromthepretrainedvideoandmulti-viewdiffusionmodelsrespectively. Therefore,
wecanderiveourmaintheorem.
Theorem3.1. Forx=IË† ,wehave
i,j
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
âˆ‡ logp IË† =âˆ‡ logp IË† +âˆ‡ logp IË† âˆ’âˆ‡ logp IË† . (6)
x x {1:V},j x i,{1:F} x i,j
(cid:16) (cid:17)
Proof. Wefirstdecomposep IË† by
(cid:16) (cid:17) (cid:16) (cid:17)
p IË† =p IË† ,IË† ,IË† ,IË† (7)
i,j âˆ’i,j i,âˆ’j âˆ’i,âˆ’j
(cid:16) (cid:17) (cid:16) (cid:17)
=p IË† ,IË† |IË† ,IË† p IË† ,IË† . (8)
i,j âˆ’i,âˆ’j âˆ’i,j i,âˆ’j âˆ’i,j i,âˆ’j
NotethatforanyIË† âˆˆIË† ,I andI areindependentconditionedonI bycorollary3.1,
iâ€²,jâ€² âˆ’i,âˆ’j iâ€²,jâ€² i,j iâ€²,j
hence
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
p IË† ,IË† |IË† ,IË† =p IË† |IË† ,IË† p IË† |IË† ,IË† . (9)
i,j âˆ’i,âˆ’j âˆ’i,j i,âˆ’j âˆ’i,âˆ’j âˆ’i,j i,âˆ’j i,j âˆ’i,j i,âˆ’j
(cid:16) (cid:17)
Sincep IË† |IË† ,IË† doesnotcontainI ,itsderivativewithrespecttoI iszero. Then
âˆ’i,âˆ’j âˆ’i,j i,âˆ’j i,j i,j
(cid:16) (cid:17)
combinedwithequation(8)andequation(9),takingthederivativeoflogp IË† withrespecttox,we
achieve
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
âˆ‡ logp IË† =âˆ‡ logp IË† |IË† ,IË† p IË† ,IË† (10)
x x i,j âˆ’i,j i,âˆ’j âˆ’i,j i,âˆ’j
(cid:16) (cid:17)
=âˆ‡ logp IË† ,IË† ,IË† . (11)
x i,j âˆ’i,j i,âˆ’j
(cid:16) (cid:17)
Finally,byfurtherdecomposingp IË† ,IË† ,IË† anddirectlyapplyingcorollary3.1,weobtain
i,j âˆ’i,j i,âˆ’j
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
âˆ‡ logp IË† =âˆ‡ logp IË† ,IË† |IË† p IË† (12)
x x âˆ’i,j i,âˆ’j i,j i,j
5(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
=âˆ‡ logp IË† |IË† p IË† |IË† p IË† (13)
x âˆ’i,j i,j i,âˆ’j i,j i,j
(cid:16) (cid:17) (cid:16) (cid:17)
p IË† p IË†
{1:V},j i,{1:F}
=âˆ‡ log (14)
x (cid:16) (cid:17)
p IË†
i,j
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
=âˆ‡ logp IË† +âˆ‡ logp IË† âˆ’âˆ‡ logp IË† . (15)
x {1:V},j x i,{1:F} x i,j
(cid:16) (cid:17) (cid:16) (cid:17)
Hereâˆ‡ logp IË† andâˆ‡ logp IË† arejustthescorefunctionsofthevideodiffusion
x i,{1:F} x {1:V},j
modelandthemulti-viewdiffusionmodelrespectively. Andweusetheconvexcombinationofthem
(cid:16) (cid:17)
toestimateâˆ‡ logp IË† as:
x i,j
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
âˆ‡ logp IË† =(1âˆ’s)âˆ‡ logp IË† +sâˆ‡ logp IË† . (16)
x i,j x i,{1:F} x {1:V},j
In practice, we employ a logistic schedule to adjust the change of s with denoising step. Given
the currentdenoising step i and thenumber of totalsteps N, we set s = 1âˆ’ 1 . This
1+ek(i/Nâˆ’s0)
functionhasasigmoidalcurve, whichisrelativelyflatattheextremesawayfrommiddles and
0
changessharplynearit,withthederivativecontrolledbyk. Thisscheduledecouplesthegeneration
ofconsistentgeometryandtemporallysmoothappearancetosomeextent.
Samplinginlatentspace Forconvenience,theabovetheoremassumesthatthesampleobjectis
theimageintheoriginalRGBspace. However,modernhigh-resolutiondiffusionmodelstypically
generateimagesencodedintoalatentspacebyVQVAE[39]. Thelegitimacyoftheaforementioned
derivationrequiresthatthemulti-viewgeneratorandthevideogeneratorsharethesamelatentspace.
Although this requirement is not met in the most current instantiation of them, we believe that
this condition will be increasingly satisfied by more multi-view generation models in the future.
BecauseaspointedoutbySVD[2,7,4,19],videogenerationmodelstrainedonlarge-scalevideo
datasetshavelearnedastrongstereoknowledge,thuscanprovideabetterpretrainingforfine-tuning
multi-viewdiffusionmodelsthanthosetrainedsolelyonimagedata.Andthelatentencoderisusually
frozenduringfine-tuningonthemulti-viewimages.
Generationwithvariousinputconditions Notethattheformulationdescribedaboveisbasedon
unconditionalgeneration. However,wearemoreinterestedincontrollablegenerationinpractice.
Thenwewillextendtheaforementionedprocesstoconditionalgeneration. Considertheaugmented
matrixI definedby
aug
(cid:20) (cid:21)
I I
I = 0,0 0,{1:V} , (17)
aug I I
{1:F},0
whereI aretheinputimage,andI ,I isthefirstrowandcolumnofI weneedto
0,0 0,{1:V} {1:F},0 aug
firstcreateastheconditionforthesubsequentgenerationoffullmatrix. Thenwewilldemonstrate
how we obtain them from various conditions. For convenience, we denote V as the multi-view
diffusionmodelandFasthevideodiffusionmodel.
â€¢ Singleimage. GivenI asinput, weuseV togenerateI whichdictatesthegeometry
0,0 0,{1:V}
of the generated4D assets and use F to generate I which endows the static imagewith
{1:F},0
dynamics.
â€¢ Single-viewvideo. GivenI asinput,weusethelastframeI astheconditionofVto
{0:F},0 0,0
generateI .
0,{1:V}
â€¢ Static3Dmodel. Similarly,GivenI asinput,weusethefrontviewI astheconditionof
0,{0:V} 0,0
FtogenerateI .
{1:F},0
Assumption3.1ensuresthesafetyofindependentlygeneratingthegeometryI andthemotion
0,{1:V}
I . Additionally,thereisnocomputationalordatadependencybetweenthesetwogeneration
{1:F},0
processes,allowingtheirtotaltimecosttobereducedtoasinglereversediffusionprocess. Thenwe
willdenoisetherestpartofI frompureGaussiannoise. Ineachstep,werunthescoreestimators
aug
6Table 1: User study on image-to-4D generation. The proportions of different methods that best
matchuserpreferencesunderthreecriteriaarereported.
Temporal Overallmodel Generation
Method Details Geometry
smoothness quality time
Animate124 11.3% 31.0% 16.0% 18.0% 9h
DreamGaussian4D 27.7% 24.3% 48.0% 25.7% 12m
Ours 61.0% 44.7% 36.0% 56.3% 10m
foreachrowandcolumnconditionedontheI andI ,andcombinetheirresultsasin
0,{1:V} {1:F},0
Theorem3.1toupdatethenoisylatent. Sincethescoreestimationforeachrowandcolumncanalso
beparallelized,thetimecostcanbedecreasedtorunningasinglediffusionstep. Therefore,with
sufficientGPUmemory,thetotaltimespentontheprocessillustratedinFigure2(ii)remainsthe
sameasthatforgeneratingasinglevideo.
3.2 Robustreconstruction
4Drepresentation Givengeneratedsynchronizedmulti-viewvideosconditionedonanytypeof
prompts,therearenumerousmethodsthatcanbeemployedtoreconstruct4Dassets. Amongthe
numerouscandidates,weadoptthe4DGaussianSplattingduetoitssuperiorfittingcapabilitiesand
efficientoptimizationgiventhedensemulti-viewsupervision.
Optimization Althoughtheimagesgeneratedinthefirststagealreadyhaveintuitivelysatisfactory
spatiotemporalconsistency,theperformancelimitationofthefoundationalmulti-viewgeneration
componentsstillmakesitdifficulttoachieveprecisepixel-levelmatchingacrossdifferentviewsand
frames. Therefore, wefollow[7]tooptimizethecombinationofperceptionlossL [52]and
lpips
D-SSIML [44]whileignoringL1loss. Inaddition,weweighteachtermwiththeconfidence
ssim
score,thenthetotalobjectiveisdefinedasL =Î» C L +Î» C L ,where
total lpips lpips lpips ssim ssim ssim
C isjustthessimbetweengroundtruthandrenderedimagesandC isdefinedas1âˆ’L .
ssim lpips lpips
4 Experiments
4.1 Implementationdetails
In the first stage, we use Stable Video Diffusion [2] as our foundation video diffusion model,
predicting 25 frames each time according to the image prompt. SV3Dp [40] is chosen as the
foundationmulti-viewdiffusionmodel. Forsimplicity,weonlygenerateorbitalvideosthathave
21uniformlyspacedazimuthsandfixedelevationwithmanualfilterofsideviewastheseviews
typicallycontainthinstructuresthatposechallengesforthevideogenerationmodelandsubsequent
reconstructionprocesses. Bydefault,wesetthenumberofsamplingstepsto50forbothgenerative
models. Inthereconstructionstage,weoptimized4DGaussianSplattingfor5kiterationswithout
bellsandwhistles. Theimagesizeissetto(576Ã—576)inbothstages.
4.2 4Dgenerationfromsingleimage
Figure3(a),weshowtheresultsgeneratedbytheproposedmethodandprovidethecomparisonwith
otheralternatives. Itcanbeobservedthatourconciseandelegantpipelineiscapableofgenerating
4D assets of comparable quality to those produced by state-of-the-art SDS-based methods with
sophisticatedmulti-stageoptimization. Furthermore,theparallelevaluationofalarge2Dgenerative
modelcanprovideapotentialefficiencyadvantageforourmethod. Wealsoconductedauserstudy
(Seeappendixfordetails),theresultsofwhicharereportedinTable1. Itsuggeststhatourmethod
garnered the highest human preference in the multi-view consistency, detail, and overall model
quality.
7Animate124
Animate124
DreamGaussian
4D
DreamGaussian
4D
Ours
Stage-2
Ours
Stage-2
Ours
Stage-1
Ours
Stage-1 (a) Image-to-4D
(a)Image-to-4D
(a) Image-to-4D
Consistent4D
Consistent4D
Efficient4D
Efficient4D
DreamGaussian
4D
DreamGaussian
4D
4DGen
4DGen
Ours
Stage-2
Ours
Stage-2
Ours
Stage-1
Ours
Stage-1 (b) Video-to-4D
(b) Video-to-4D
(b)Video-to-4D
Figure3: Qualitativecomparisonson(a)image-to-4Dgenerationand(b)video-to-4Dgeneration.
Table2: Quantitativecomparisonsonvideo-to-4Dgeneration.
Method Type CLIPSimilarityâ†‘ Generationtimeâ†“
Consistent4D Optimization-based 0.87 2h
4DGen Optimization-based 0.89 2h10m
Efficient4D Photogrammetry-based 0.92 14m
Our Photogrammetry-based 0.94 10m
4.3 4Dgenerationfromsingleviewvideo
Generating4Ddynamicobjectsfromfixed-viewvideoisapracticaltaskfirstintroducedin[15].
Comparedtogenerationfromasingleimage,thistaskadditionallyconstrainstheobjectâ€™smotion.
OurproposedframeworkcanbeeasilyadaptedtodealwiththistaskasdetailedinSection3.1. We
performbothquantitativeandqualitativecomparisonswithothercounterpartsunderthissetting. The
qualitativeresultisshowninFigure3(b). Itcanbeseenthatourmethodslightlyreducestheover-
saturatedappearance. Forqualitativeevaluation,wereporttheCLIP-similaritybetweengenerated
views and ground truth images to indicate overall semantic consistency and the recognizable of
generatedimages. Thequantitativemetricsalsosupportoursuperiority.
4.4 4Dgenerationfromstatic3Dcontent
Naturally,Diffusion2 canalsoanimatestatic3DmodelsintodynamicobjectsasdescribedinSec-
tion3.1,whichhassubstantialpracticalimplications. FromFigure4wecanobservethatourmethod
iscapableofendowing3Dmodelswithdiverseandrealisticdynamicswhilemaintainingsatisfactory
temporalandgeometricalconsistency.
8View Time View Time
Reference Synthesized view Reference Synthesized view
Figure4: Synthesizedimagesfromstatic3Dmodels.
4.5 Ablationstudies
(a)
Refence SVD only SV3D only
ğ‘ ğ‘ =0, ğ‘ ğ‘ =0.25,ğ‘˜ğ‘˜=20 ğ‘ ğ‘ =0.5,ğ‘˜ğ‘˜=20 ğ‘ ğ‘ =0.75,ğ‘˜ğ‘˜=20 ğ‘ ğ‘ =1,
Refence 10 20
ğ‘ ğ‘ =0.5,ğ‘˜ğ‘˜=5 ğ‘ ğ‘ =0.5,ğ‘˜ğ‘˜= ğ‘ ğ‘ =0.5,ğ‘˜ğ‘˜=15 ğ‘ ğ‘ =0.5,ğ‘˜ğ‘˜= ğ‘ ğ‘ =0.5,ğ‘˜ğ‘˜=30
(b)
Constant Linear Logistic
Figure5: Ablationstudies. (a)Theparametercontrollingthethelogisticschedule. (b)Different
typeofscaleschedule. Bestviewedwithzoom-in.
Sincepreviousmethodshaveexploreddynamiccontentreconstructionwell,wemainlyfocusonthe
ablationofkeydesignchoicesinthestageofimagematrixgeneration. InFigure5(a),weadjusted
twoparametersthatcontrolthecurveofthelogisticschedule. Theresultsrevealthat: whenonly
using video prior, the generated images fail to ensure consistency in perspective, geometry, and
detail with other views. For example, in the second row and second column of Figure 5 (a), the
dressstillevenlysagsdownbothsidesofthebody, unlikeinthereferenceviewwherethedress
flutterstotheleftsideofthebody. Thisindicatesthatwithouttheguidanceofageometryprior,the
dynamicsofeachviewwillbetotallyindependentofeachother. Anotherextremeiswhenwesets
9to1,thatis,usingonlymulti-viewprior,whereitcannolongerguaranteetheconsistencyofdetails
betweendifferentframesofinvisibleviews. Thecomparisonofthelastcolumnofthefirsttworows
ofFigure5(a)showsaremarkablechangeintheshapeofthetailofthehair. Sowefinallyadopta
compromisedoptiontosetsto0.5,i.e.,startingtodrasticallyreducetheweightofthemulti-view
scoreathalfwayofthedenoisingprocess. Inmostcases,thischoicecanachievesmoothtemporal
transitionsaswellasgeometricalconsistency. Inaddition,thedecreasingspeedofsmayalsoaffect
thequalityofgeneratedimages,whichiscontrolledbyanotherparameterk. Therefore,wetestthe
imagegenerationunderdifferentkgivens=0.5. Itcanbeseenthatwhenkisrelativelysmall,the
generatedimagesmayexhibitghostingeffects. Ontheotherhand,whenkistoolarge,itmayweaken
thetemporalsmoothingeffectbroughtbythevideodiffusionmodelintheearlystagesofdenoising.
Consequently,weadopts=0.5,k =20asthedefaultsetting. Finally,wealsoexaminedtheimpact
ofdifferentnoiseschedules,asshowninFigure5(b). Itcanbeobservedthatallotherchoiceswould
leadtoghostingartifacts.
5 Conclusion
In this work, we present a novel 4D content generation framework, dubbed Diffusion2, which
efficientlygeneratesdense,consistentmulti-viewmulti-frameimagearrayswithhighparallelismand
thenfeedstheminto4Dreconstructionpipelinetocreatefull4Dpresentation. Ourkeyassumption
is that elements I and I (i Ì¸= k,j Ì¸= l) in the multi-view multi-frame image array I are
i,j k,l
conditionallyindependentgivenI orI . Thisalignswithourintuition: pastorfuturemotionand
i,l k,j
theappearanceinotherviewsaredecoupledtoalargeextent. Basedonthisassumption,weprove
thatwecandirectlysamplesynchronizedmulti-viewvideosI inadenoisingprocessbycombining
pretrainedvideodiffusionmodelsandmulti-viewdiffusionmodels. Experimentalresultsshowthat
the proposed framework can flexibly adapt to various types of prompts. We hope that our work
caninspirefutureresearchonunleashingandcombiningthegeometricalanddynamicpriorsfrom
foundation3Dandvideodiffusionmodels.
References
[1] SherwinBahmani,IvanSkorokhodov,VictorRong,GordonWetzstein,LeonidasGuibas,Peter
Wonka,SergeyTulyakov,JeongJoonPark,AndreaTagliasacchi,andDavidBLindell. 4d-fy:
Text-to-4dgenerationusinghybridscoredistillationsampling. InCVPR,2024. 1,3
[2] AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Do-
minikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion:
Scalinglatentvideodiffusionmodelstolargedatasets. arXivpreprint,2023. 2,3,4,6,7
[3] AndreasBlattmann, RobinRombach, HuanLing, TimDockhorn, SeungWookKim, Sanja
Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent
diffusionmodels. InCVPR,2023. 2,3
[4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David
Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya
Ramesh. Videogenerationmodelsasworldsimulators. https://openai.com/research/
video-generation-models-as-world-simulators,2024. 2,3,6
[5] AngCaoandJustinJohnson. Hexplane: Afastrepresentationfordynamicscenes. InCVPR,
2023. 3
[6] AnpeiChen,ZexiangXu,AndreasGeiger,JingyiYu,andHaoSu. Tensorf: Tensorialradiance
fields. InECCV,2022. 1
[7] ZilongChen,YikaiWang,FengWang,ZhengyiWang,andHuapingLiu. V3d: Videodiffusion
modelsareeffective3dgenerators. arXivpreprint,2024. 1,3,6,7
[8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt,
LudwigSchmidt,KianaEhsani,AniruddhaKembhavi,andAliFarhadi. Objaverse: Auniverse
ofannotated3dobjects. InCVPR,2023. 1
[9] JunGao,TianchangShen,ZianWang,WenzhengChen,KangxueYin,DaiqingLi,OrLitany,
ZanGojcic,andSanjaFidler. Get3d: Agenerativemodelofhighquality3dtexturedshapes
learnedfromimages. InNeurIPS,2022. 3
10[10] QuankaiGao,QiangengXu,ZheCao,BenMildenhall,WenchaoMa,LeChen,DanhangTang,
andUlrichNeumann. Gaussianflow:Splattinggaussiandynamicsfor4dcontentcreation. arXiv
preprint,2024. 1
[11] AgrimGupta,LijunYu,KihyukSohn,XiuyeGu,MeeraHahn,LiFei-Fei,IrfanEssa,LuJiang,
andJosÃ©Lezama. Photorealisticvideogenerationwithdiffusionmodels. arXivpreprint,2023.
3
[12] JunlinHan,FilipposKokkinos,andPhilipTorr. Vfusion3d: Learningscalable3dgenerative
modelsfromvideodiffusionmodels. arXivpreprint,2024. 3
[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In
NeurIPS,2020. 1,3
[14] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan
Sunkavalli,TrungBui,andHaoTan. Lrm: Largereconstructionmodelforsingleimageto3d.
InICLR,2024. 1,3
[15] YanqinJiang, LiZhang, JinGao, WeiminHu, andYaoYao. Consistent4d: Consistent360
{\deg}dynamicobjectgenerationfrommonocularvideo. InICLR,2024. 1,3,8
[16] HeewooJunandAlexNichol. Shap-e: Generatingconditional3dimplicitfunctions. arXiv
preprint,2023. 3
[17] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-basedgenerativemodels. InNeurIPS,2022. 1,4
[18] BernhardKerbl,GeorgiosKopanas,ThomasLeimkÃ¼hler,andGeorgeDrettakis. 3dgaussian
splattingforreal-timeradiancefieldrendering. InACMTOG,2023. 1
[19] XuanyiLi,DaquanZhou,ChenxuZhang,ShaodongWei,QibinHou,andMing-MingCheng.
Sorageneratesvideoswithstunninggeometricalconsistency. arXivpreprint,2024. 6
[20] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl
Vondrick. Zero-1-to-3: Zero-shotoneimageto3dobject. InICCV,2023. 1,3
[21] YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,LingjieLiu,TakuKomura,andWenping
Wang. Syncdreamer: Generatingmultiview-consistentimagesfromasingle-viewimage. In
ICLR,2024. 1,2,3
[22] XiaoxiaoLong,Yuan-ChenGuo,ChengLin,YuanLiu,ZhiyangDou,LingjieLiu,YuexinMa,
Song-HaiZhang,MarcHabermann,ChristianTheobalt,etal. Wonder3d: Singleimageto3d
usingcross-domaindiffusion. arXivpreprint,2023. 3
[23] HaoyuLu,GuoxingYang,NanyiFei,YuqiHuo,ZhiwuLu,PingLuo,andMingyuDing. Vdt:
General-purposevideodiffusiontransformersviamaskmodeling. InICLR,2024. 3
[24] AlexNichol,HeewooJun,PrafullaDhariwal,PamelaMishkin,andMarkChen. Point-e: A
systemforgenerating3dpointcloudsfromcomplexprompts. arXivpreprint,2022. 3
[25] ZijiePan,JiachenLu,XiatianZhu,andLiZhang. Enhancinghigh-resolution3dgeneration
throughpixel-wisegradientclipping. InICLR,2024. 3
[26] ZijiePan,ZeyuYang,XiatianZhu,andLiZhang. Fastdynamic3dobjectgenerationfroma
single-viewvideo. arXivpreprint,2024. 3
[27] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall. Dreamfusion: Text-to-3dusing
2ddiffusion. InICLR,2023. 1,3
[28] JeremyReizenstein,RomanShapovalov,PhilippHenzler,LucaSbordone,PatrickLabatut,and
DavidNovotny. Commonobjectsin3d: Large-scalelearningandevaluationofreal-life3d
categoryreconstruction. InICCV,2021. 1
[29] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu.
Dreamgaussian4d: Generative4dgaussiansplatting. arXivpreprint,2023. 1,3
[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.
High-resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,2022. 3
[31] KatjaSchwarz,YiyiLiao,MichaelNiemeyer,andAndreasGeiger. Graf: Generativeradiance
fieldsfor3d-awareimagesynthesis. InNeurIPS,2020. 3
[32] YichunShi,PengWang,JianglongYe,MaiLong,KejieLi,andXiaoYang. Mvdream: Multi-
viewdiffusionfor3dgeneration. arXivpreprint,2023. 1,3
11[33] UrielSinger,ShellySheynin,AdamPolyak,OronAshual,IuriiMakarov,FilipposKokkinos,
NamanGoyal,AndreaVedaldi,DeviParikh,JustinJohnson,etal. Text-to-4ddynamicscene
generation. arXivpreprint,2023. 1,3
[34] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. In
ICLR,2021. 1
[35] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,and
BenPoole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. InICLR,
2021. 1
[36] JiaxiangTang,ZhaoxiChen,XiaokangChen,TengfeiWang,GangZeng,andZiweiLiu. Lgm:
Largemulti-viewgaussianmodelforhigh-resolution3dcontentcreation. arXivpreprint,2024.
1,3
[37] JiaxiangTang,JiaweiRen,HangZhou,ZiweiLiu,andGangZeng. Dreamgaussian: Generative
gaussiansplattingforefficient3dcontentcreation. InICLR,2024. 1,3
[38] ShitaoTang,JiachengChen,DilinWang,ChengzhouTang,FuyangZhang,YuchenFan,Vikas
Chandra,YasutakaFurukawa,andRakeshRanjan. Mvdiffusion++: Adensehigh-resolution
multi-viewdiffusionmodelforsingleorsparse-view3dobjectreconstruction. arXivpreprint,
2024. 1,2,3
[39] AaronVanDenOord,OriolVinyals,etal. Neuraldiscreterepresentationlearning. InNeurIPS,
2017. 6
[40] VikramVoleti, Chun-HanYao, MarkBoss, AdamLetts, DavidPankratz, DmitryTochilkin,
ChristianLaforte,RobinRombach,andVarunJampani. Sv3d: Novelmulti-viewsynthesisand
3dgenerationfromasingleimageusinglatentvideodiffusion. arXivpreprint,2024. 1,2,3,7
[41] PengWangandYichunShi. Imagedream:Image-promptmulti-viewdiffusionfor3dgeneration.
arXivpreprint,2023. 1,3
[42] ZhengyiWang,ChengLu,YikaiWang,FanBao,ChongxuanLi,HangSu,andJunZhu. Pro-
lificdreamer: High-fidelityanddiversetext-to-3dgenerationwithvariationalscoredistillation.
InNeurIPS,2023. 1,3
[43] ZhengyiWang,YikaiWang,YifeiChen,ChendongXiang,ShuoChen,DajiangYu,Chongxuan
Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convolutional
reconstructionmodel. arXivpreprint,2024. 3
[44] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSimoncelli. Imagequalityassessment:
fromerrorvisibilitytostructuralsimilarity. InIEEETIP,2004. 7
[45] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu,
QiTian,andXinggangWang. 4dgaussiansplattingforreal-timedynamicscenerendering. In
CVPR,2024. 1
[46] TongWu,JiaruiZhang,XiaoFu,YuxinWang,LiangPanJiaweiRen,WayneWu,LeiYang,
JiaqiWang,ChenQian,DahuaLin,andZiweiLiu. Omniobject3d: Large-vocabulary3dobject
datasetforrealisticperception,reconstructionandgeneration. InCVPR,2023. 1
[47] ZiyiYang,XinyuGao,WenZhou,ShaohuiJiao,YuqingZhang,andXiaogangJin. Deformable
3dgaussiansforhigh-fidelitymonoculardynamicscenereconstruction. InCVPR,2024. 1
[48] TaoranYi,JieminFang,GuanjunWu,LingxiXie,XiaopengZhang,WenyuLiu,QiTian,and
XinggangWang. Gaussiandreamer: Fastgenerationfromtextto3dgaussiansplattingwith
pointcloudpriors. InCVPR,2024. 3
[49] YuyangYin,DejiaXu,ZhangyangWang,YaoZhao,andYunchaoWei. 4dgen: Grounded4d
contentgenerationwithspatial-temporalconsistency. arXivpreprint,2023. 3
[50] XianggangYu,MutianXu,YidanZhang,HaolinLiu,ChongjieYe,YushuangWu,ZizhengYan,
ChenmingZhu,ZhangyangXiong,TianyouLiang,etal. Mvimgnet: Alarge-scaledatasetof
multi-viewimages. InCVPR,2023. 1
[51] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-image
diffusionmodels. InICCV,2023. 1
[52] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreason-
ableeffectivenessofdeepfeaturesasaperceptualmetric. InCVPR,2018. 7
12[53] YuyangZhao,ZhiwenYan,EnzeXie,LanqingHong,ZhenguoLi,andGimHeeLee. Ani-
mate124: Animatingoneimageto4ddynamicscene. arXivpreprint,2023. 1
[54] QiZuo,XiaodongGu,LingtengQiu,YuanDong,ZhengyiZhao,WeihaoYuan,RuiPeng,Siyu
Zhu,ZilongDong,LiefengBo,etal. Videomv: Consistentmulti-viewgenerationbasedon
largevideogenerativemodel. arXivpreprint,2024. 1
13