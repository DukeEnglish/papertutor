Topic-basedWatermarksforLLM-GeneratedText
ALEXANDERNEMECEK,YUZHOUJIANG,andERMANAYDAY,CaseWesternReserveUniversity,USA
Recentadvancementsoflargelanguagemodels(LLMs)haveresultedinindistinguishabletextoutputscomparabletohuman-generated
text.WatermarkingalgorithmsarepotentialtoolsthatofferawaytodifferentiatebetweenLLM-andhuman-generatedtextby
embeddingdetectablesignatureswithinLLM-generatedoutput.However,currentwatermarkingschemeslackrobustnessagainst
knownattacksagainstwatermarkingalgorithms.Inaddition,theyareimpracticalconsideringanLLMgeneratestensofthousands
oftextoutputsperdayandthewatermarkingalgorithmneedstomemorizeeachoutputitgeneratesforthedetectiontowork.In
thiswork,focusingonthelimitationsofcurrentwatermarkingschemes,weproposetheconceptofaâ€œtopic-basedwatermarking
algorithmâ€forLLMs.TheproposedalgorithmdetermineshowtogeneratetokensforthewatermarkedLLMoutputbasedonextracted
topicsofaninputpromptortheoutputofanon-watermarkedLLM.Inspiredfrompreviouswork,weproposeusingapairoflists
(thataregeneratedbasedonthespecifiedextractedtopic(s))thatspecifycertaintokenstobeincludedorexcludedwhilegenerating
thewatermarkedoutputoftheLLM.Usingtheproposedwatermarkingalgorithm,weshowthepracticalityofawatermarkdetection
algorithm.Furthermore,wediscussawiderangeofattacksthatcanemergeagainstwatermarkingalgorithmsforLLMsandthebenefit
oftheproposedwatermarkingschemeforthefeasibilityofmodelingapotentialattackerconsideringitsbenefitvs.loss.
AdditionalKeyWordsandPhrases:LargeLanguageModels(LLMs),Watermarking,Fingerprinting
1 INTRODUCTION
Therapidexpansionofartificialintelligenceandnaturallanguageprocessing(NLP)inrecentyearshasallowedforthe
developmentoflargelanguagemodels(LLMs)tobecomeaprominentaspectinacademiaandindustry[6].Modelssuch
asOpenAIâ€™sChatGPT[16],enablescapabilitiesforgeneratinghuman-liketextbyunderstandinguserinputandlearning
fromextensiveamountsofdata[18,26].Asmoreusersaredrawntothesesystems,theamountofLLM-generatedtext
overtheinternetincreases,causingpotentialmisinformation,copyright,andplagiarismissues[10,19].Theseconcerns
resultintheneedfortoolsthatcandifferentiateLLM-andhuman-generatedmaterial.
WatermarkingalgorithmshavepotentialtoprovidewaysinwhichonecanidentifyLLM-generatedtextbyembedding
anidentifiablepatterninthetextsequence[23].CurrentwatermarkingschemesforLLMsinvolveadjustingtheoutput
token(i.e.,unitsoftextusedinNLPanalysis,suchaswords,phrases,orcharactersaselementsforprocessinginNLP
tasks)distributionstrategicallybyseparatingtokensintoâ€œgreenâ€ğº andâ€œredâ€ğ‘…listsandenhancingthelikelihood
ofğº listedtokensoverğ‘…listedtokenstobeusedintheLLMoutput[11,27].ThedetectionofawatermarkedLLM-
orhuman-generatedtextsequenceisdeterminedbythenumberofğ‘…listedtokensinthetextsequence.Ahuman-
generatedoutputisexpectedtocontainsignificantlymoretokensintheğ‘…listthanintheğº list,theoppositeistruefor
LLM-generatedoutputcontainingmoreğº listedtokenscomparedtothenumberofğ‘…listedtokens.
Onemajorlimitationofcurrentwatermarkingalgorithmsisthelackofrobustnessagainstdifferentattackmodels
concerningtextinsertion,manipulation,substitution,anddeletion.Insuchattacks,thegoalofapotentialattackeris
totamperwiththeLLMoutputtoremoveordistortthewatermark,sothewatermarkdetectionmechanismcannot
identifyLLM-generatedoutputwithhighconfidence.Inaddition,efficiencyandpracticalitylimitationsarisewith
thegrowthofLLMsandtheirusage.Thecreationofğº andğ‘…listsforawatermarkbecomesimpracticalbecausethe
detectionmechanismâ€™scomputationalloadrequiresittoexhaustivelyexamineallseedsusedtorandomlypartitionthe
vocabularyoftheLLMintoağº listthegeneratedoutputissampledfrom.Withcurrentmodelschemes,themodel
Authorsâ€™address:AlexanderNemecek,ajn98@case.edu;YuzhouJiang,yxj466@case.edu;ErmanAyday,exa208@case.edu,CaseWesternReserve
University,10900EuclidAve.,Cleveland,Ohio,USA,44106-1715.
1
4202
rpA
2
]RC.sc[
1v83120.4042:viXra2 Nemeceketal.
ownermemorizeseachqueryreceivedinordertodifferentiatebetweenanLLM-andhuman-generatedtextsequence
byusingtheğº andğ‘…pairoflists.
Toresolvetheaforementionedlimitationsofcurrentwatermarkingalgorithms,weproposetheconceptofanew
modeltoenhancetherobustnessandefficiencybyutilizingğº andğ‘…listsinatopic-basedmanner.Usinganinputtext
sequenceortheoutputfromanon-watermarkedLLMoutput,topics(e.g.,sports,health,politics,technology,etc.)are
extractedtogenerateapairoflistsforeachtopic.Thedesignatedlistsforeachtopicareemployedtodetectwhethera
targettextsequencewasgeneratedbyanLLMorbyahuman.ThepracticalityoftheproposedschemeallowstheLLM
ownertodisregardtheinfeasibletaskofiteratingoverallpossiblelistsorseedsforthelistpairsofanindividualtext
inputsequenceandtogenerateonlytopic-basedseedsforpairsofliststowatermarkthetextoutputsequence.On
theotherhand,potentialattacksagainstwatermarkingalgorithmsmainlyaimattemperingwiththewatermarkin
suchawaythatthedetectionalgorithmfailstodetectthewatermark.Usingatopic-basedwatermarkingalgorithmfor
LLMsalsomakesiteasiertomodelthebenefitvs.lossofsuchattacks.Forinstance,ifanattacker,aimingtodistort
thewatermark,modifiestheLLMoutputinsuchawaythatthetopicoftheoutputchanges,andhencethedetection
algorithmfailswithhighprobability,alsosignificantlyreducestheutilityoftheLLMoutput.Therefore,topic-based
watermarkingcanbeusedtoquantifythe(i)successoftheattackerincheatingthedetectionalgorithmand(ii)decrease
intheutilityoftheLLMoutputduetotheattack.
Thefocusofthisarticleistodiscusscurrentwatermarkingalgorithms,highlighttheirlimitations,andprovidethe
conceptofanewwatermarkingschemethatmitigatessuchlimitations.Specifically,weexaminethefunctionalityof
contemporarywatermarkingandextendablerobustnessschemes,includinganalgorithmicandlimitationstandpoint
foreachscheme.Wethenproposeanoverviewofourtopic-basedwatermarkingschemeforLLM-generatedtext,how
ourmodelcanaddresscurrentlimitations,andlimitationsoftheproposedmodelitself.Finally,wepresentfuture
researchandstudiestoempiricallyevaluatetheproposedtopic-basedscheme.
2 BACKGROUND
IntroducedbyVaswanietal.[25],LLMsleverageatransformarchitecturewhichutilizeattentionmechanisms,allowing
ittoweighrelevantfeatures(wordsortokens)inasentence,regardlessoftheirpositionaldistanceinrelationfrom
oneanother[4].TraininganLLMinvolvesfeedingitextensiveamountsoftextdataandadjustinginternalparameters
basedonpredictionerrorsthemodelmakes[21].TheobjectivefortrainingtheLLMisfocusedaroundnext-token
predictionswheretheaimistopredictthenexttokeninasequencegiventhetokenswhichprecedeit.
ThegenerationoftokenstooutputcoherenttextbyanLLMisasequentialprocess,whereeachnewtokenis
producedbasedonthecontextprovidedbypriortokens.Thisprocessreliesonaprobabilisticmodelwhereforany
givensequenceoftokens,theLLMcalculatestheprobabilitydistributionoveritsvocabularyforthenexttokeninthe
sequence.DifferentstrategiestheLLMemploystoselectthenexttokensuchas,selectingthetokenthathasthehighest
probability(greedyornaivemethods)[3].Theprobabilitiesassociatedwitheachnexttokenarederivedfromthe
LLMâ€™slogits.Logitsaretheraw,unnormalizedoutputsoftheLLMâ€™sfinallayer(directoutputsfromtheLLMâ€™sneural
networkcomputationsbeforeanynormalizationhasbeenapplied)whichyieldstheprobabilitiesforeachtokeninthe
vocabulary.Theseprobabilitiesreflectthemodelâ€™slearnedassociationsanditsconfidenceindifferentcontinuationsof
theoutputtextsequence.Topic-basedWatermarksforLLM-GeneratedText 3
3 RELATEDWORK
Inthissection,wediscuss,fingerprintingmethodologies(thatdonottargetLLMs),tothebestofourknowledge,the
firstwatermarkingschemeforLLMsandextensionsofrobustwatermarkingschemes.
3.1 Fingerprinting
Fingerprintingisatechniqueusedtoidentifycontentbycustomizing(fingerprinting)everyinstanceofadistributed
copyofthecontent.Bydoingthis,ifthecontentisdistributedwithoutauthorization,theoriginalsourceofthedataleak
canbeidentified[1].Themethodisanalogoustohowhumanfingerprintsareusedtoidentifyindividuals.Research
concerningtherobustnessoffingerprintingmethodologiesexistsinavarietyoffieldstoallowclaimtocopyrightand
toidentifysourcesofdatabreaches.PriorresearchbyJietal.[7]focusonrobustfingerprintingmethodsofgenomic
databases.Theyguaranteeliabilitywhensharinggenomicdata,specificallymitigatingcorrelationattacks(anattack
aimingtodetectanddistorttheaddedfingerprint)againstmaliciousdatabaserecipientsbychangingorfingerprinting
databaseentries.Multiplefingerprintingschemeshaveshownrobustnessagainstdifferentthreatmodels[7â€“9],however
anotablelimitationofmanyfingerprintapproachesistheirinapplicabilitywithinthecontentofanLLM.
3.2 WatermarkingAlgorithmsforLLMs[11]
Tothebestofourknowledge,thefirstwatermarkingschemeutilizesaprobabilisticpairofâ€œredâ€ğ‘…(tokensthatcannot
beused)andâ€œgreenâ€ğº (tokensthatcanbeused)listsforLLM-generatedtextoutput.TheworkbyKirchenbaueret
al.[11],offersthegenerationoftwotypesofğ‘…lists,asoftğ‘…listğ‘…
ğ‘ 
andahardğ‘…listğ‘… â„,alongwiththeirrespectiveğº
lists.Bothtypesofğ‘…andğº listsareusedinthedetectionofhuman-versusLLM-generatedtextoutput,comparingthe
numberoftokensfromtheğ‘…andğº liststothetargettextsequence(e.g.,outputofanLLMorhumanindividual).
Thegenerationoftheğ‘… â„ andğ‘… ğ‘  listsutilizetheinputpromptandapriorsequenceofknowntokenscontaining
thefirstğ‘¡âˆ’1tokensalreadyproducedbytheLLMtoeithergenerateprobabilities(probabilityofeachwordoverthe
vocabularyoftheLLM)orlogits(discreteprobabilitiesoverthevocabularyoftheLLM)dependingonthetypeofğ‘…list.
Byutilizingthelastknowntokenovertheinputsequence,ahashiscomputedtoseedarandomnumbergenerator
whichisthenusedtorandomlypartitionthevocabularytheintoğº andğ‘…lists.
Thedifferencebetweentheğ‘… ğ‘ andğ‘… â„listsisduetotheğ‘… ğ‘ listutilizingtheadditionofaconstantâ€œhardnessâ€parameter
toeachoftheğºlistlogits.Thehardnessparameterallowsthesamplingdistributionforthenexttokeninthegenerated
sequencetostronglyfavortheğº listforhighentropytokens.Highentropytokensareasequenceoftokensthatare
goodchoicesforthenexttokeninthegeneratedsequence,whilelowentropytokensconsistofoneorfewgoodchoices
oftokenstochoosefromasthenexttokeninthegeneratedoutput.Thedetectionofthewatermarkforbothğ‘…listsare
similaranditisdonebycheckingeachtokeninatargettextsequenceandexamineforviolationsintheğ‘…list.
Therearelimitationsconcerningtheğ‘…listgenerationofthewatermarkedLLM-generatedoutput.Tobeabletodetect
thewatermark,theğ‘…listsneedtobecorrectlyreproducedspecifictothegeneratedoutputtext.Limitationsariseifthe
listscannotbeproperlygeneratedwhichmayleadtoincorrectconclusionsontheanalyzedtargettext(i.e.,whether
itwasLLM-orhuman-generated).Overheadonmaintainingthewatermarkingschemealsoaddscomplexitytothe
LLM.Itbecomesinfeasibletoexhaustivelyiterateoverallmemorizedlistsortheirspecifichashfunctionsandrandom
numbergeneratorsduetothenumberofusersproprietyLLMshaveexperiencedinrecentyears.Ourproposedmodel
ishypothesizedtodetectwatermarkedLLM-generatedtextsequencesefficiencyandfeasibly,reducingthecomplexity
ofthesizeofseedsforeachpairoflists,leadingtoaccurateclassificationsbetweenLLM-andhuman-generatedoutput.4 Nemeceketal.
3.3 RobustWatermarking
Limitations considering robustness of watermarking schemes has led to an influx of research for improving the
constraintsoftheseinitialalgorithms[2,12â€“14,22,24,27].Thefirstwatermarkingalgorithmextendsitsrobustness,
describingarobustprivatewatermarkingalgorithm[11].Therobustnessextensionofthewatermarkinvolvesthesame
generativeprocessofpreviouslymentionedwatermarkingschemeswiththeadditionofasinglesecretkeyormultiple
secretkeys.Thesecretkey(s)enhancesthesecurityandrobustnessonthewatermarkbygeneratinguniquetokensfor
theğ‘…listutilizingapseudorandomfunction.Theadditionsofthekey(s)furtheraddressthelimitationofreproducibility
concernsfortheğ‘…andğº lists.
Continuationofworkdedicatedtothelackofattackrobustnessandsecurityrobustness,Liuetal.[14]examine
aproposedwatermarkingalgorithmtoenhanceattackrobustnessondifferentsemanticallyinvariantperturbations
(e.g.,textmodifications,textalterations),embeddingwatermarkingsbasedonsemanticsoftheentirepriortextinstead
ofafixednumberofpriortokens.Analysisoftheirproposedsemanticinvariantwatermarkingschememaintained
robustnessagainstattacksschemessuchastextparaphrasing(rewritingthewatermarkedtextwhilepreservingits
originalmeaning)andsynonymsubstitution(certainwatermarkedwordsarereplacedwiththeirrespectivesynonyms).
Christetal.[2]buildonsecretkeyutilizationproposinganundetectablewatermarkingschemeutilizingcryptographic
one-way functions making it computationally infeasible to distinguish a watermarked output compared to other
non-watermarkedtext.Thewatermarkingschemeembedsasecretintheoutputtexttowhichitdoesnotaffectthetext
qualitygeneratedbytheLLM.Thedetectionalgorithmoftheundetectablewatermarkingschemeidentifiestheoutput
ofthewatermarkedLLMwithoutanyinformationotherthantheoutputtextsequenceandthesecretkey.
WhileresearchinrobustnessofwatermarkingschemesforLLMsisquicklyadvancing,toourknowledge,thereis
littleworkconcerningactualcommercializedscenariosinwhichausablewatermarkingdetectionschemeisfeasible.
Lietal.[12],proposesawatermarkschemeusableinthecommercialsettings,introducingdifferentbackdoordata
paradigmswhichintegratethewatermarktotheLLMduringthetuningprocess.Themotivationoftheauthorsisto
guaranteethecopyrightofacustomizedLLMfromabusinessperspectivecomparabletopriorfingerprintingschemes
concerningthecopyrightofgenomicdatabases[7].Priorresearchhasalsoassumedblack-boxdetectionapproaches
wheretheinternalworkingsofthemodelareundisclosedtotheuser.Limitationsforblack-boxdetectionschemes
revolvearounddatabiaseswherethedatacollectionofLLMscanintroducebiasesimpactingperformanceandthe
abilitytogeneralizeacrossdifferentcontextofthedetectionmechanism,confidencecalibrationaffectingtheoverall
evaluationmetricsoftheLLM,indicatingthereliabilityandtrustworthinessoftheLLM,andthelackofadaptabilityto
advancethelearningoftheLLM[24].Ourproposedwatermarkingmethodologydifferentiatesfromexistingliterature
withanapproachtostoretheğºandğ‘…liststailoredtoaspecifiedtopicofthetextinputsequencereducingcomputation
whilestillprovidingrobustnessfromanattackmodelperspective.
4 PROPOSEDMETHOD
Thissectionprovidesanoverviewoftheproposedwatermarkingframeworkandotheraspectsofthemodeltobe
considered.First,inSection4.1,wediscussdifferentrelevantattackmodels.InSection4.2,weintroducethehighlights
ofourproposedwatermarkingframeworkwhilecomparingittotheworkofKirchenbaueretal.[11].Then,inSection
4.3,wediscussourwatermarkdetectionschemetodifferentiatebetweenhuman-andLLM-generatedoutput.Finally,in
Section4.4,wetouchonthelimitationsofourproposedwatermarkingmethodology.Topic-basedWatermarksforLLM-GeneratedText 5
4.1 ThreatModel
EmbeddedwatermarksinanLLMâ€™sgeneratedoutputaresubjecttovarioustypesofattackssummarizedinthefollowing.
Notethatallconsideredthreatmodelsarethosewherethemotivationofanattackeristodistortorremovethewatermark
fromthetargettextsequencetoavoiddetection.However,wehypothesizemanipulationtothetextoutputsequence
willdegradetheoverallqualityoftextandthetopicaccuracy.Toassesstheimpactofmanipulationsonthetextquality,
weutilizetheevaluationmetric,perplexityseeninpriorresearch[11]tomeasurehowwellaprobabilitymodelpredicts
asampledtoken.Theperplexitymetricisrelevantforquantifyingthedegradationintextqualityduetounexpected
tokensasaresultofeffortstoevadewatermarkdetection.
Whileweexaminearangeofreal-lifeapplicationsforwatermarking,wespecificallyhighlightthesubmissionof
courseworkwithinacademicenvironmentsasanexample.Thisscenarioservestoillustratethebroaderrelevanceand
necessityofwatermarkingtechniques,withoutconfiningthediscussionsolelytoacademia.Inthiscontext,thequality
ofmanipulatedtextisdeterminedbyassessingitscoherence,measuredbytheevaluationmetricofperplexity.Italso
examineswhetherthetextsequenceaccuratelyaddressesandanswersspecificquestionsrelatedtothecoursework.
Thisapproachhighlightstheimportanceofbothintegrityandaccuracyinanacademiccontext.Weassumethat,
inordertoremainundetectabletothedetectionmechanism,amalicioususerwillmanuallychangetokenswhile
preservingtextqualityinrelationtothespecifiedtopic(s).However,inthecaseofcourseworksubmissions,preserving
thequalityextendsbeyondtopicrelevancetoincludeprecisionandaccuracyoftheresponsesoftheposedquestionsin
thecourseworksubmission.Modelingofthisadditionalqualityisyettoberealized,butaddressestheimportanceof
generalandcontext-specificconcernsinwatermarkdetection.
Potentialattacksconcerntextinsertion,substitution,anddeletioninthegeneratedoutputtextsequenceatthe
character,word,sentence,andmulti-modellevel[5,11].Insertionistheadditionorinsertionoftokensintothetext
sequence.Withthewatermarkedtextencompassingsignificantlymoreğºlistedtokens,theadditionofnewtokensmay
violatetheğ‘…list,increasingthecountofğ‘…listedtokensrespectivetotheğº listedtokens.Weassumetheattackerwill
needtoinsertasignificantamountofğ‘…listedtokenstodecreasedetectionpowerofthewatermarktoclassifythetext
ashuman-generated.Inthecontextofcourseworksubmission,givena1-pageLLMoutputforanassignment,astudent
canaddanotheroneormoresubsequentpagesoftext(human-generated)todecreasedetectionpower.However,this
involvesatrade-offwhereifthestudentisabletowriteoneormorepagesworthofcontentmanuallytoremovethe
watermark,therewouldbenoneedtouseLLM-generatedtextatall.Thescenarioofinsertionattacksisrealisticbut
thescenarioinwhichanattackerwouldneedtomanuallygeneratethesameormorecontentthantheLLM-generated
outputisunrealistic,henceweexpectareasonablelevelofinsertiontokensduringourevaluation.Substitutionof
tokens,involveswappingtokensinthegeneratedoutputtext.Forexample,anattackermaysubstitutetokensinthe
textsequencewiththeirrespectivesynonym.Themotivationoftheattackerinaninstanceofsubstitutionistoremove
ğº listedtokensandaddğ‘…listedtokenstodegradeorremovethewatermark.Deletioninvolvesdeletingtokensfrom
ageneratedoutputsequence.Ifasignificantamountofğº listedtokensaredeleted,theamountofğ‘…tokenswould
increasehowever,ifonlytokensaredeletedfromatextsequence,thereexistsapotentialtrade-offbetweenthetext
qualityanddetectionaccuracy.Wewouldalsoexpectareasonablelevelofdeletiontoexistintheevaluationphaseof
ourresearch.Tokeninsertion,manipulation,anddeletionareusedintandemtodistortorremovethewatermarkofan
LLM-generatedoutput.Intherestofthissection,wediscussconsideredthreatmodelsanattackercanperformagainst
theLLMinourevaluation.6 Nemeceketal.
4.1.1 BaselineAttack. Baselineattacksconsistoftheinsertion,substitution,anddeletionoftextforagivenoutput
sequence.Theattackerselectsasingleoracombinationoftechniques(insertion,substitution,anddeletion)withthe
objectivetodiminishdetectionaccuracy.Indoingso,thisapproachmayleadtoacompromiseinthetextqualityofthe
manipulatedsequence.
4.1.2 ParaphrasingAttack. Aparaphrasingattackisacategoryofabaselinesubstitutionattack.Aspreviouslystated,
executionofthisattackmaybemanualbyanindividualorbyrephrasingtheoutputviaanLLM.Theproposed
watermarkingschemeisrobustagainstvaryingscenariosofanattackerparaphrasingtheoutputfromthewatermarked
LLM.Weconsiderseparateparaphrasingattackscenarios.
â€¢ ParaphrasingwiththeOriginalLLM:TheattackergeneratesawatermarkedtextsequencebyfeedingtheLLM
aninputprompt.Weassumetheattackerdoesnotmanuallyalteranytokensintheoutput.Theattacker
subsequentlysuppliesthewatermarkedoutputintotheidenticalLLMwhichoriginallyproducedit,along
withaparaphrasecommand(forexample,â€œrephrasethistextpromptâ€).Theproposedwatermarkingschemeis
robustagainstparaphrasingthroughtheoriginalLLMduetothenatureoftopicsthatformthebasisofthe
watermarkingprocess.Forinstance,iftheattackerâ€™sinitialpromptcontainedthegeneratedtopicofâ€œsportsâ€,
anyparaphrasedoutputwouldlikelymaintainâ€œsportsâ€asthetopic,ensuringthewatermarkfortheoriginal
andparaphrasedoutputsareconstant.
â€¢ ManualParaphrasing:TheattackergeneratesanoutputtextsequencebythewatermarkedLLMfromagiven
input prompt. The paraphrasing is conducted manually by the attacker without the aid of external tools.
Thismanualparaphrasingapproachisconsideredforshortersequencesoftext,aslongersequenceswould
necessitatemoreextensiveinterventionfromtheattacker.Thetopic-basedwatermarkingschemeproposedis
particularlyeffectiveagainstmanualparaphrasing,especiallyforlengthiertextsequences,whichwouldrequire
significantmodificationtomisleadthedetectionmechanismintoincorrectlyclassifyingtheLLM-generated
textforhuman-generatedtext.Therobustnessoftheproposedwatermarkingschemeisfurtherenhancedby
thepotentialdeteriorationintextqualityandthepossiblechangeintheoveralltopicsoftheparaphrasedtext
sequence.
4.1.3 TokenizationAttack. Atokenizationattackisclassifiedasaformofinsertion,whereatokenismodifiedinto
multiplesubsequenttokens.Forexample,giventhetokenâ€œsportsâ€withinağº list.Anattackercanaddcharacters
suchasâ€˜_â€™orâ€˜*â€™,creatingadditionaltokenscategorizedasğ‘…listedtokensbythedetectionscheme.Theoriginalğº
listedtoken,â€œsportsâ€,ismanipulatedintoâ€œs_p_o_r_t_sâ€,creatingmultiplenewğ‘…listedtokensinthetextsequence.
Therobustnessoftheproposedtopic-basedwatermarkingschemeconcernsthedegradationintextqualityduetothe
intentionaladditionofnoise(unwantedcharacters)withintokensintheğº list.
4.1.4 DiscreteAlterationAttack. Anattackerâ€™smotivationbehindthisattackistoinducealterationsintokenssub-
sequentlyintroducingmisspellingandgrammarerrors.Thesemanipulationscanbeexecutedthroughinsertionor
deletionofsingleormultiplecharacters,orevenentirestingsofcharacterswithinthespecifiedoutputsequence,
withtheaimofdiminishingtheeffectivenessofthewatermarkindetection.Weconsideranattackerwhichseeksto
underminetheintegrityofawatermarkedtextsequencewithoutcompletelyalteringthetext.Byinsertingordeleting
characterswithincertaintokens,theattackercanmodifythetexttointroduceerrors,whileminor,canimpactthe
detectabilityofthewatermark.Forexample,alteringthewordâ€œwatermarkedâ€toâ€œwat3rmark3dâ€throughcharacter
substitutionintroducingmisspellingbycomplicatingtheprocessofwatermarkrecognition.Theproposedtopic-basedTopic-basedWatermarksforLLM-GeneratedText 7
watermarkaddressescurrentwatermarkingalgorithmlimitationsthroughtheimplementationofthegeneralizedtopic
scheme.Eveninthepresenceofminorspellingorgrammaticalerrors,theoverarchingtopicoftheoutputremains
unaffected.Thisensuresthatthewatermarkâ€™sintegrityismaintained,showcasingtherobustnessoftheproposed
approachagainsttextualalterations.
4.1.5 CollusionAttack. AcollusionattackbeginswithanoriginaltextsequencethathasbeenprocessedbyanLLM,
whichincorporatesawatermarkingschemebasedontopicrelevance.WeassumethatallLLMshaveatopic-based
watermarkingschemeimplementedforoutputtextgeneration.Theattackerthenfeedsthiswatermarkedoutputintoa
differentLLM,aimingtoalterthecontentofthetextthroughinsertion,manipulation,ordeletion.Thisprocessmight
modifythetopicsextractedfromthetext,dependingontheextractionmethodsandthedefinedlistpairsofgeneral
andspecifictopics(ğºandğ‘…).ShouldthesecondLLMemployitsdetectionmechanismforclassificationpurposes,it
wouldlabeltheoutputasâ€œLLM-generatedâ€,effectivelycounteringcollusionattacksbyrecognizingthemanipulation.
HowevertheintroductionofmultipleLLMsintothisprocessdisruptsthecorrelationbetweentheğº andğ‘…listpairs,
renderingthedetectionofthetextsequencechallengingandpotentiallyleadingtoincorrectclassifications.Whilethe
useofmultipleLLMsintextmanipulationattackspresentsacomplexchallengeforthedetectionmechanismdueto
thepotentialalterationofextractedtopicsandthebreakdownofgeneralandspecifictopiccorrelations,employing
detectionmechanismscanhelpidentifyLLM-generatedcontent.Nonetheless,theeffectivenessofthesemechanisms
mightbecompromisedasthenumberofinterveningLLMsincreases,highlightingtheneedforadvancedstrategiesin
watermarkinganddetectiontomitigatesuchthreats.
4.2 ProposedWatermarkingScheme
AsdiscussedinSection2,LLMsgeneratethenexttokenintheoutputtextsequencefromthepreviouslygenerated
token,specificallytheworkby[11],thetokensaretakenfromthespecifiedğºlist.Ourproposedwatermarkingscheme
is inspired by this process, however our methodology differs with the utilization of extracted topics from a non-
watermarkedLLM-generatedoutputandthegenerationofğº andğ‘…listpairsforeachtopicinfluencingawatermarked
LLM.
Weproposetheconceptoftopic-basedğº andğ‘…listpairscombinedwithatopic-basedwatermarkingmechanism
(TBWM). The proposed TBWM framework consists of two different LLM components, a watermarked and non-
watermarkedLLM.TheTBWMextractsthetopicfromtheinputtextsequenceoranon-watermarkedLLM-generated
outputbasedontheinputtextsequence.Figure1describestheproposedtopic-basedwatermarkingscheme.
4.2.1 Non-WatermarkedLLMTopicExtraction. Aproposedmethodfortopicextractionemploysanunwatermarked
LLM.ThisprocessinitiateswithauserpromptingtheLLMwithaninputtextsequence.Subsequently,thisinput
sequenceisintroducedtotheunwatermarkedLLMwhichproducesanon-watermarkedoutputtextsequence.This
outputservesasthebasesforextractingtopicsrelevanttotheoriginalinputprovidedbytheuser.Furthermore,the
non-watermarkedoutputisthenfedintoawatermarkedLLMincombinationwithacommandtorephrasethetext,
suchasâ€œrephrasethistext.â€Boththenon-watermarkedoutputandtheadjustmentsintokenweight,derivedfromthe
extractedtopics,areemployedwithinthewatermarkedLLMtogenerateawatermarkontheoutputtextsequence.The
utilizationofthenon-watermarkedovertheinputtextsequenceistoextractaccuratetopicswhen(i)asmallinput
promptoccurs,consistingofacouplewordstoasentenceinlengthand(ii)whenmisspellingsandothergrammarissues
arepresent.Theoutputofthenon-watermarkedLLMallowsforasufficientamountofqualitytexttobegeneratedin
ordertoextractanaccuratetopic,identifyingthetopicofinterestofthecorrespondinguser.Furthermore,forasingle8 Nemeceketal.
Fig.1. Topic-basedwatermarkingmechanism(TBWM).Theinputpromptispassedtonon-watermarkedLLMtoextractthetopic(s)
ofinterest.Utilizingthetopic(s),tokenweightsareadjustedforthewatermarkedLLMwiththeoriginalnon-watermarkedoutput.
inputtextsequence,thewatermarkingschemecanextractmultipletopics.Forinstance,inaninputaskingformedical
injuriesindifferenttypesofsports,thenon-watermarkedoutputcouldsummarizethegeneralizedtopicsfromthis
specificoutputasâ€œsportsâ€andâ€œmedicalinjuries.â€However,astonotdegrademodelperformanceortopicextraction
quality,givenalargenon-watermarkedoutput(e.g.,paragraph(s)),thetopicextractionmayiterateoverallsentencesin
thetextsequencetoanalyzeforrelevanttopicsandskipthosethatarenotstronglyrelatedtospecifiedtopics.The
extractedtopicorcombinationoftopicsareusedforseedandğº andğ‘…listpairgenerationbasedonthenumberof
extractedtopics.Theğº andğ‘…listpairsareutilizedtotunethetokenweightsforthewatermarkedLLMalongwith
thepriornon-watermarkedLLM-generatedoutputthatwasusedfortopicextraction.ThewatermarkedLLMutilizes
thenon-watermarkedoutputtextsequenceincombinationwitharephrasecommandtocorrectlysamplefromthe
generatedğº listpertopic.SimilartothewatermarkingalgorithmproposedbyKirchenbaueretal.[11],wesample
tokensfromtheğº listtogenerateanoutputsequencefromtheoriginalinputprompt.
Theutilizationoftheglobaltopic-basedğº andğ‘…listsallowforthewatermarkingofaninputtextsequencetobe
computationallysufficientbygeneratingcertainlistsbasedonthetopicsallottedbythemodel.Thisgeneralizationofğº
andğ‘…listssignificantlydecreasestheoverallnumberoflistpairsneededforthedetectionofthewatermark.
4.2.2 InputPromptTopicExtraction. Thereexisttwodifferentmethodsfortheextractionoftopics:eitherdirectlyfrom
theuserâ€™sinputtextsequenceorfromanoutputproducedbyanon-watermarkedLLM,whichitselfisbasedonthe
inputtextsequence.Theextractionoftopicsfromtheinputtextisfeasiblewhenthetextisfreefromspellingand
grammaticalerrorsandcontainsenoughtexttoaccuratelyextracttopics.Simultaneously,theinputtextisprocessed
throughanon-watermarkedLLM,resultinginanunwatermarkedoutputbasedonthesaidinput.Thenon-watermarkedTopic-basedWatermarksforLLM-GeneratedText 9
outputservesastheinputforthewatermarkedLLMalongwiththeoutputandtheadjustmentsintokenweight,
derivedfromthetopicsextractedfromtheinputtextsequence,areutilizedwithinthewatermarkedLLMtoproducea
watermarkedoutputtextsequence.Theapproachwheretopicsareemployedtogeneratetheğºandğ‘…listpairsremains
constantacrossboththenon-watermarkedtopicextractionandtheinputprompttopicextractionmethods.
4.3 WatermarkDetectionMechanism
Existingwatermarkingalgorithmsthatrelyontheğº andğ‘…listpairmethodologyarebasedontheoccurrenceofğº and
ğ‘…listedwordswithinthetargettext(LLMoutputtextsequence)describedinSection3.2.Thisdetectionmethodfocuses
onthefrequencyofğ‘…listviolationswithinthetargettext.Textproducedbyhumanindividualstypicallyexhibitahigher
usageofğ‘…listedtokens,resultinginfewerinstancesofğº listedtokensinthetargettext.WatermarkedLLM-generated
textincludessignificantlymoreğº listedtokensoverğ‘…listedtokens.Kirchenbaueretal.[11]demonstratedthelow
likelihoodofanindividualbeingabletogenerateatextsequence,aligningwithğº listedtokenssimilartothatofa
watermarkedLLM-generatedoutput.
Theproposedtopic-baseddetectionframeworkseeninFigure2,mirrorstheapproachofleveragingğº andğ‘…token
listpairs.However,itintroducesanovelmechanismclassifyingtextaseitherhuman-orLLM-generatedbyextracting
andanalyzingthetopicspresentintheinputtextsequence.Theoverallprocessinvolvesthecreationofspecificğº
andğ‘…listpairsforeachidentifiedtopicallowingthedetectionmechanismtoiteratethroughalllistpairspertopicto
performclassification.Specifically,uponreceivinganinputtextsequence,thewatermarkingframeworkidentifiesone
ormultipletopicspresentwithintheinputtext,utilizingtopicmodelingtechniques(e.g.,LatentDirichletallocation,
non-negativefactorization[15]).Foreachofthetopicsidentified,theframeworkaccessesthepredefinedpairofğº and
ğ‘…lists,quantifyingtheoccurrencesoftokenfromthepairoflistswithinthetext.Giventhedistributionsofğº and
ğ‘…tokensacrossmultipledifferenttopicswithintheinputtextsequence,weemployhypothesistestingtodetermine
thetargettextâ€™sclassification.Thespecificstatisticaltestschosenwilldependonthedistributioncharacteristicsof
thetokenoccurrence,aimedatevaluatingtheconfidencelevel,whetherthetargettextishuman-orLLM-generated.
Kirchenbaueretal.[11]utilizedaâ€œoneportionz-testâ€involvingtheğºlistoftokenswiththeexpectedvalueoftokensğ‘‡
asğ‘‡/2andthevarianceofğ‘‡/4.Theoverallz-testisdenotedas:
âˆš
ğ‘§=2(ğºâˆ’ğ‘‡/2)/ ğ‘‡ (1)
whichwewillevaluateinourproposeddetectionscheme.Thetopic-basedapproachallowsforageneralizedanalysis
ofthetargettext,whichwehypothesizewillaccommodatethemultipletopictextsequencesandprovideaworkflow
incorporatingstatisticalteststoensuretheaccuracyofthewatermarkingscheme.
4.4 Limitations
Theproposedwatermarkingschemeisdesignedwithcomputationalfeasibilityforenablingthegenerationofğº andğ‘…
listpairs.ThepracticallyofthemodelimplementationwilldependonthenumberofcategoriesallottedfortheTBWM.
Theanticipatedtrade-offtobemaintainedconcernscomputationalfeasibilityandthetextqualityoftheLLM.For
instance,shouldtheinputpromptbecategorizedunderâ€œsportsâ€withoutacorrespondingcomprehensivelistonthe
specifictopic,theLLMâ€™soutputqualitymaybecompromised.Asgranularityofthegloballistpairsincreases,sotoo
doestextqualityattheexpenseofincreasedcomputationaldemandanddiminishedmodelfeasibility.10 Nemeceketal.
Fig.2. Detectionoftopic-basedwatermarkingscheme.Tokenlistpairs,generatedfromtheextractedtopicsoftheinputtext,are
comparedtotheoutputtextsequenceofthewatermarkedLLMtodeterminetheclassificationofthetargettextashuman-or
LLM-generated.
Althoughnotthefocusofthisarticle,itisimportanttoacknowledgethepotentialforspoofingattacks,asignificant
threatmodelthatcompromisestheprivacyandsecurityofwatermarkedLLMs.TheseattacksdeceiveeithertheLLMor
itsusersintobelievingthatagiventextoutputsequenceoriginatesfromadifferentsource(differentLLM).Previous
researchhasshowedthesusceptibilityofLLMstosuchattacks,withstudiesbySadasivanetal.[20],investigatingthe
capabilityofattackerstogeneratederogatoryoutputsfromLLMsbydeducingtextsignatureswithoutaccesstothe
detectionmechanisms.Conversely,Pangetal.[17]concentratedonthewatermarkingdimensions,fabricatingoutputs
thaterroneouslyappearwatermarked,bothwiththeunderlyingmotiveoftarnishingtheLLMâ€™soritsdevelopersâ€™
reputation.Furthermore,therobustnessoftheproposedtopic-basedwatermarkingschemeagainstcertainattacks
representsnotablelimitations[11,17].Specifically,theschemeisvulnerabletoattacksaimedatremovingthewatermark
throughalterationsintheoutputsequence,suchastheaddition,modification,ordeletionoftokens.Despitethese
constraints,thetopic-basedapproach,leveraginggeneralizedğº andğ‘… listspairs,couldpotentiallyofferenhanced
robustnessthroughadjustmentofweightswithintheglobaltopiclists.Theapproachcontrastswithgeneratinga
uniquepairoflistsforeachoutput,potentiallyprovidingalayerofdefenseagainstbothdirectalterationsandthe
threatofspoofingattacksbymakingthemodellesspredictableandhardertomanipulate.Theresilienceofthisscheme
tomanualmodificationsintheoutput,aswellasitseffectivenessindefenseagainstspoofingattacksthatexploitthe
modelâ€™ssensitivitytotopicpredictions,willbesubjectsforfutureexploration,aimingtodevelopawatermarking
solutionthatbalancesthedemandsofsecurity,practicality,andtextquality.
Furthermore,thisworkwillunderstandthetrade-offthatemergesfromrefininggranularityofgeneralizedcom-
prehensivelistpairswiththetopic-basedwatermarkingscheme.Therefinementofthistrade-off,whilebeneficialfor
enhancingthespecificityofthetextoutputsequence,unintentionallyintroducesincreasedsensitivitytotheaccuracy
oftopicprediction.SensitivitycouldpotentiallylowerthethresholdforadversariestoexploittheLLMbymanipulating
themodeltobypassthewatermarkingschemeorproducefalseoutputsequences.Thelimitationfallsinachievingthe
optimalbalancetomaintaintheintegrityofthewatermarkingframeworkbutalsoadheretothepossiblemanipulationTopic-basedWatermarksforLLM-GeneratedText 11
whichcouldcompromisethemodelâ€™sfeasibility.Theevaluationforthistrade-offwillinvolvequantitativeassessments
ofcomputationalloadandtextqualityandenhancedrobustnessagainstmentionedattacks(paraphrasing,tokenization,
discretealterations,etc.).Refiningthegloballistpairsandthelimitationsoftheproposedmodel,furthermotivatefora
schemeconsistingofidealgranularityandtheabilitytoadapttoevolvingthreatlandscapes,preservingthequalityof
outputtextsequences.
5 CONCLUSION
Inthiswork,weproposeacomputationallyfeasibletopic-basedwatermarkingschemethatgeneratespairsofâ€œgreenâ€and
â€œredâ€listsbasedonaninputtextprompttopicofanLLM.Weprovidebackgroundknowledgeofcurrentwatermarking
algorithmsanddiscusstheirlimitationsbeingthemotivationoftheproposedframeworkwiththelackofrobustness
againstknownattacksandtheimpracticallyofthememorizationoftextoutputs.Forfuturework,willinvolvethe
implementationandevaluationoftheproposedwatermarkingschemewiththefocusofimprovingconstraintsinterms
ofrobustnessandcomputation.Furthermore,thereispotentialfortheexpansionofourproposedwatermarkingscheme
beyondatopic-basedapproach,suchasgeneralizingotheraspectsofhowlistsaregeneratedtodetecttextsequences.
REFERENCES
[1] JonathanBailey.2007.Watermarkingvs.Fingerprinting:AWarinTerminology.(2007).https://www.plagiarismtoday.com/2007/10/09/watermarking-
vs-fingerprinting-a-war-in-terminology/
[2] MirandaChrist,SamGunn,andOrZamir.2023.UndetectableWatermarksforLanguageModels.arXivpreprintarXiv:2306.09194(2023). https:
//doi.org/10.48550/arXiv.2306.09194
[3] SaeedDehqan.[n.d.].ExploringTokenGenerationStrategies. https://www.packtpub.com/article-hub/exploring-token-generation-strategies
[4] AndreaGalassi,MarcoLippi,andPaoloTorroni.2019. Attention,please!ACriticalReviewofNeuralAttentionModelsinNaturalLanguage
Processing.CoRRabs/1902.02181(2019).arXiv:1902.02181 http://arxiv.org/abs/1902.02181
[5] ShreyaGoyal,SumanthDoddapaneni,MiteshM.Khapra,andBalaramanRavindran.2023.ASurveyofAdversarialDefencesandRobustnessin
NLP.arXivpreprintarXiv:2203.06414(2023). https://doi.org/10.48550/arXiv.2203.06414
[6] HuggingFaceInc.2024.HuggingFaceModels. https://huggingface.co/models
[7] TianxiJi,ErmanAyday,EmreYilmaz,andPanLi.2022.Robustfingerprintingofgenomicdatabases.Bioinformatics38,Supplement1(062022),
i143â€“i152. https://doi.org/10.1093/bioinformatics/btac243
[8] TianxiJi,ErmanAyday,EmreYilmaz,andPanLi.2023.TowardsRobustFingerprintingofRelationalDatabasesbyMitigatingCorrelationAttacks.
IEEETransactionsonDependableandSecureComputing20,4(2023),2939â€“2953. https://doi.org/10.1109/TDSC.2022.3191117
[9] YuzhouJiang,EmreYilmaz,andErmanAyday.2022. RobustFingerprintofLocationTrajectoriesUnderDifferentialPrivacy. arXivpreprint
arXiv:2204.04792(2022). https://doi.org/10.48550/arXiv.2204.04792
[10] DineshKallaandSivarajuKuraku.2023. Advantages,DisadvantagesandRisksassociatedwithChatGPTandAIonCybersecurity. (102023).
https://doi.org/10.6084/m9.jetir.JETIR2310612
[11] JohnKirchenbauer,JonasGeiping,YuxinWen,JonathanKatz,IanMiers,andTomGoldstein.2023.AWatermarkforLargeLanguageModels.arXiv
preprintarXiv:2301.10226(2023). https://doi.org/10.48550/arXiv.2301.10226
[12] ShenLi,LiuyiYao,JinyangGao,LanZhang,andLiYaliang.2024.Double-IWatermark:ProtectingModelCopyrightforLLMFine-tuning.arXiv
preprintarXiv:2402.14883(2024). https://doi.org/10.48550/arXiv.2402.14883
[13] AiweiLiu,LeyiPan,XumingHu,Shuâ€™angLi,LijieWen,IrwinKing,andPhilipS.Yu.2023.AnUnforgeablePubliclyVerifiableWatermarkforLarge
LanguageModels.arXivpreprintarXiv:2307.16230(2023). https://doi.org/10.48550/arXiv.2307.16230
[14] AiweiLiu,LeyiPan,XumingHu,ShiaoMeng,andLijieWen.2024.ASemanticInvariantRobustWatermarkforLargeLanguageModels.arXiv
preprintarXiv:2310.06356(2024). https://doi.org/10.48550/arXiv.2310.06356
[15] MadhurimaNath.2023.Topicmodelingalgorithms.Medium(2023). https://medium.com/@m.nath/topic-modeling-algorithms-b7f97cec6005#:~:
text=The%20most%20established%20go%2Dto,model%20which%20uses%20matrix%20factorization.
[16] OpenAI.2024.Chatgpt:Optimizinglanguagemodelsfordialogue. https://openai.com/blog/chatgpt/
[17] QiPang,ShengyuanHu,WentingZheng,andVirginiaSmith.2024.AttackingLLMWatermarksbyExploitingTheirStrengths.arXivpreprint
arXiv:2402.16187(2024). https://doi.org/10.48550/arXiv.2402.16187
[18] ParthaP.Ray.2023.ChatGPT:Acomprehensivereviewonbackground,applications,keychallenges,bias,ethics,limitationsandfuturescope.
InternetofThingsandCyber-PhysicalSystems3(2023),121â€“154. https://doi.org/10.1016/j.iotcps.2023.04.00312 Nemeceketal.
[19] MatthiasC.Rillig,MarleneÃ…gerstrand,MohanBi,KennethA.Gould,andUliSauerland.2023.RisksandBenefitsofLargeLanguageModelsforthe
Environment.EnvironmentalScience&Technology57,9(2023),3464â€“3466. https://doi.org/10.1021/acs.est.3c01106
[20] VinuS.Sadasivan,AounonKumar,SriramBalasubramanian,WenxiaoWang,andSoheilFelzi.2023.CanAI-GeneratedTextbeReliablyDetected?
arXivpreprintarXiv:2303.11156(2023). https://doi.org/10.48550/arXiv.2303.11156
[21] VitaliiShevchuk.2023.GPT-4ParametersExplained:EverythingYouNeedtoKnow. https://levelup.gitconnected.com/gpt-4-parameters-explained-
everything-you-need-to-know-e210c20576ca
[22] VictoriaSmith,AliS.Shamsabadi,CarolynAshurst,andAdrianWeller.2023.IdentifyingandMitigatingPrivacyRisksStemmingfromLanguage
Models:ASurvey.arXivpreprintarXiv:2310.01424(2023). https://doi.org/10.48550/arXiv.2310.01424
[23] SiddarthSrinivasan.2024. DetectingAIfingerprints:Aguidetowatermarkingandbeyond. https://www.brookings.edu/articles/detecting-ai-
fingerprints-a-guide-to-watermarking-and-beyond/
[24] RuixiangTang,Yu-NengChuang,andXiaHu.2023. TheScienceofDetectingLLM-GeneratedTexts. arXivpreprintarXiv:2303.07205(2023).
https://doi.org/10.48550/arXiv.2303.07205
[25] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,LukaszKaiser,andIlliaPolosukhin.2017.AttentionIs
AllYouNeed.arXivpreprintarXiv:1706.03762(2017). https://doi.org/10.48550/arXiv.1706.03762
[26] WayneX.Zhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,YingqianMin,BeichenZhang,JunjieZhang,ZicanDong,YifanDu,
ChenYang,YushuoChen,ZhipengChen,JinhaoJiang,RuiyangRen,YifanLi,XinyuTang,ZikangLiu,PeiyuLiu,Jian-YunNie,andJi-RongWen.
2023.ASurveyofLargeLanguageModels.arXivpreprintarXiv:2303.18223v13(2023). https://doi.org/10.48550/arXiv.2303.18223
[27] XuandongZhao,PrabhanjanAnanth,LeiLi,andYu-XiangWang.2023.ProvableRobustWatermarkingforAI-GeneratedText.arXivpreprint
arXiv:2306.17439(2023). https://doi.org/10.48550/arXiv.2306.17439