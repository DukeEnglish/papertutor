Towards Coarse-grained Visual Language Navigation Task
Planning Enhanced by Event Knowledge Graph
KaichenZhaoâˆ— YaoxianSongâˆ— HaiquanZhao
22210240394@m.fudan.edu.cn songyaoxian@zju.edu.cn 22210240393@m.fudan.edu.cn
ShanghaiKeyLaboratoryofData ZhejiangUniversity ShanghaiKeyLaboratoryofData
Science,SchoolofComputerScience, Hangzhou,China Science,SchoolofComputerScience,
FudanUniversity FudanUniversity
Shanghai,China Shanghai,China
HaoyuLiu TiefengLi ZhixuLiâ€ 
hyuliu20@gmail.com litiefeng@zju.edu.cn zhixuli@fudan.edu.cn
ResearchCenterforIntelligent ZhejiangUniversity ShanghaiKeyLaboratoryofData
Robotics,ZhejiangLab Hangzhou,China Science,SchoolofComputerScience,
Hangzhou,China FudanUniversity
Shanghai,China
ABSTRACT CCSCONCEPTS
Visuallanguagenavigation(VLN)isoneoftheimportantresearch â€¢Computingmethodologiesâ†’Informationextraction.
inembodiedAI.Itaimstoenableanagenttounderstandthesur-
roundingenvironmentandcompletenavigationtasks.VLNinstruc- KEYWORDS
tionscouldbecategorizedintocoarse-grainedandfine-grained EventKnowledgeGraph,KnowledgeRetrieval,VisualLanguage
commands.Fine-grainedcommanddescribesawholetaskwith Navigation,Taskplanning,DynamicBacktracking
subtasksstep-by-step.Incontrast,coarse-grainedcommandgives
ACMReferenceFormat:
anabstracttaskdescription,whichmoresuiteshumanhabits.Most
KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixu
existingworkfocusesontheformerkindofinstructioninVLN
Li.2024.TowardsCoarse-grainedVisualLanguageNavigationTask
tasks,ignoringthelatterabstractinstructionsbelongingtodaily
PlanningEnhancedbyEventKnowledgeGraph.InProceedingsofthe
lifescenarios.Toovercometheabovechallengeinabstractinstruc- 33rdACMInternationalConferenceonInformationandKnowledgeManage-
tion,weattempttoconsidercoarse-grainedinstructioninVLN ment(CIKMâ€™24),October21â€“25,2024,Boise,ID,USA.ACM,NewYork,NY,
byeventknowledgeenhancement.Specifically,wefirstpropose USA,11pages.https://doi.org/10.1145/3627673.3679711
aprompt-basedframeworktoextractaneventknowledgegraph
(namedVLN-EventKG)forVLNintegrallyovermultiplemain-
stream benchmark datasets. Through small and large language
modelcollaboration,werealizeknowledge-enhancednavigation
planning(namedEventNav)forVLNtaskswithcoarse-grained
instructioninput.Additionally,wedesignanoveldynamichistory
backtracking module to correct potential error action planning
inrealtime.Experimentalresultsinvariouspublicbenchmarks
showourknowledge-enhancedmethodhassuperiorityincoarse-
grained-instructionVLNusingourproposedVLN-EventKGwith
over5%improvementinsuccessrate.Ourprojectisavailableat
https://sites.google.com/view/vln-eventkg
âˆ—EqualContribution.
â€ Correspondingauthor.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or Figure1:ForConventionalVLNtasks,fine-grainedtaskde-
republish,topostonserversortoredistributetolists,requirespriorspecificpermission scriptionsareprovidedtotheagent.However,inthereal
and/orafee.Requestpermissionsfrompermissions@acm.org.
world,humansoftenonlyprovidecoarse-grainedtaskde-
CIKMâ€™24,October21â€“25,2024,Boise,ID,USA
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. scriptions
ACMISBN979-8-4007-0436-9/24/10
https://doi.org/10.1145/3627673.3679711
4202
guA
5
]RI.sc[
1v53520.8042:viXraCIKMâ€™24,October21â€“25,2024,Boise,ID,USA KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixuLi
1 INTRODUCTION aVLN-specificeventknowledgegraph,usedtoenhanceLLMâ€™s
Visuallanguagenavigation(VLN)task[1,50]aimstoenableagents planningcapabilitiesinVLNtasks.Inaddition,wealsoproposea
tounderstandthesurroundingenvironmentandaccomplishtasks dynamicbacktrackingmechanismtoalleviatetheaccumulationof
basedonhumaninstructions.Itisoneofthecrucialresearchareas errorsinthemodelduringtaskexecution.
forenablingAItointeractwiththerealphysicalworld. ToverifytheeffectivenessofourproposedmethodVLN-EventKG,
ExistingVLNmethods[14,31,33,38,40]facethechallengesin weevaluateourproposedevent-knowledge-enhancedmodelEvent-
coarse-grainedinstructionsunderstanding,inwhichoraclefine- NavonpublicR2R[2],REVERIE[37],andALFRED[40]datasets.
grainedsubtaskdescriptionsaresupplementedtoassistnaviga- Ablationstudiesaboutthescaleofeventknowledgeanddynamic
tionplanning.However,forreal-worldscenarios,instructionsin backtrackingmechanismareconductedtofurtherverifythesupe-
human-robotinteractionandnavigationareusuallyabstractand rioritiesofourproposedmethod.
coarse-grained.Forinstance,aseriesoffine-graineddetailedin- Insummary,thecontributionsofthispaperareasfollows:
structionsmakenavigationplanningeasyforanagent("First,go
upstairs,thenpassthesofa,enterthekitchen,andfinallystopin (1) Wefirstintroduceeventknowledgeintothesequentialdecision-
frontoftherefrigerator"),however,inrealworld,humansoften makingprocess(i.e.VLN),considerthecorrelationofevents
givecoarse-grainedinstructions:("gototherefrigerator"),asshown intheVLNscenario,andguidethemodeltomakecorrect
inFigure.1.Themainstreamapproaches[15,31,33]useatrans- taskplanning;
formerarchitecturemodeltosolvetheentireVLNtaskend-to-end. (2) WeproposeaneweventknowledgegraphforVLNtasks,
Thesemodelsarepre-trainedonlargeVLNdatasetsusingvarious whichextractsricheventknowledgeoverpublicVLNbench-
dataaugmentationstrategies.Extrafine-grainedoraclesubtaskde- marks,usedtohelpLLMperformmorereasonabletaskplan-
scriptionsareusedduringintrainingprocess,whichareproven ning;
toimproveperformancesignificantlycomparedtomodelstrained (3) Weproposeanoveldynamicbacktrackingmechanism,al-
withoutusingthat[40]. lowingthemodeltoassesswhetherthecurrenttaskcan
Recently,withtherapiddevelopmentoflargelanguagemodels, besuccessfullyexecutedinrealtimeandbacktrackwhen
severalworks[18,30]haveexploredtouseLLM(LargeLanguage necessary;
Models)fortaskplanning,aimingtoexploitcoarse-grainedinstruc- (4) Experimental results on R2R [2], REVERIE [37], and AL-
tionstopredictfine-grainedinstructions.However,commonsense FRED [40] show that under task settings that only pro-
knowledgeinsidethegeneralLLMisnotaccuratelyapplicableto videcoarse-grainedtaskdescriptions,ourEventNavoutper-
thespecificVLNdomain,whichalsocouldbringunnecessarynoise formsthebestexistingmodelsbymorethan5%averagely
knowledgeleadingtobadperformance[7,19]ontaskplanningin insuccessrate.
VLNdatasets.Furthermore,mostofthemfocusonthewholeper-
formanceachievementmissingcoarse-grainedtaskunderstanding
andtaskdecompositionmodelingatthesymboliclevel.
2 RELATEDWORK
Tomakeupforthedeficiencyofabstractandcoarse-grainedun-
derstandinginVLN,weattempttoinvestigateVLNplanningusing VisualLanguageNavigationInrecentyears,thevisuallanguage
onlycoarse-grainedtaskinstructions.ComparedtoexistingVLNre- navigationtaskhasattractedwidespreadattentionfromresearchers.
search,twokeypointsareconsidered:1)Coarse-grainedinstruc- Thistask[1,50]aimstobuildanintelligentagentthatcanactin
tionunderstandingNooraclefine-grainedinstructionsareused athree-dimensionalenvironmentandfollowhumaninstructions.
inthemodelingstageofVLNplanner,whichrequiresthemodel Buildingasystemthatunderstandsandexecuteshumaninstruc-
tounderstandandreasontasksfromabstractandcoarse-grained tionshasbeenthesubjectofmuchpreviouswork.Theseinstruction
instructions.2)LLM-KGjointVLNmodelLLMandstructured typesinclude,butarenotlimitedto,structuredcommandsorlogic
externalknowledgegraphareutilizedcomplementarily,whichis programs[34,35],naturallanguage[4,46],images[24],oramix-
desiredtoobtainthereasoningabilityofLLMandVLN-domain tureofmodalities[25].Theseeffortsfocusonmappingthecontext
knowledgefromtheexternalknowledgegrapheliminatingout-of- ofinstructionsandstructuralwordsontofinalactions.However,
domainnoisyinformationfromgeneralLLM. intherealworld,agentsneedtobeabletoprocessrawsensory
Forcoarse-grainedinstructionunderstanding,weproposean input.Therefore,thevisuallanguagenavigationtaskintroduces
eventknowledgegraphforVLNtasksnamedVLN-EventKG.Most richunstructuredvisualcontexttoinformtheagentâ€™sexploration,
existingVLNbenchmarksareusedasdatasourcestoextractevent- perceptionandexecution[1,5,20].
levelknowledgeofsequentialdecisionsforVLNplanning.Specifi- ConventionalMethodforVLNAlgorithmsinvisuallanguage
cally,weperformknowledgeextractiononmultipleVLNdatasetsto navigationarebasedonreinforcementlearning[21]orimitation
obtainfine-grainedtasksequencesforeachcoarse-grainedtask,in learning[8].Inaddition,auxiliarytasks,suchaspre-trainedon
whichconsequentrelationsareconceptualized.OurVLN-EventKG subtasks[54],speaker-drivenrouteselection[8],crossmodalmatch-
isusedasanexternalknowledgegraphtoguidefine-grainedsub- ing[17,49],text-basedpre-trained[6,41],progressestimation[26,
tasksgenerationfromtheinputofcoarse-grainedinstructions.For 27],areproposedtoimprovetheperformanceandgeneralization
LLM-KGjointVLNmodel,wedesignalarge-small-modelcollab- ofneuralagentsinseenandunseenenvironments.Fordata-centric
orativeframeworkusingeventknowledge,namedEventNav,in learning,researchersexplorehowtoutilizedatamoreeffectively
whichweextracttheeventknowledgeofVLNscenariostobuild andsynthesizemorediversedata.Speaker-follower[8]introducesa
speakermodeltoenhanceinstructiontrajectorypairs.Envdrop[45]TowardsCoarse-grainedVisualLanguageNavigationTaskPlanningEnhancedbyEventKnowledgeGraph CIKMâ€™24,October21â€“25,2024,Boise,ID,USA
Figure2:Theconstructionpipelineoftheeventknowledgegraph.
breaksthelimitofvisibleenvironmentvariabilitythroughenviron- 3 PROBLEMSTATEMENT
mentdropout.CCC[48]aimstolearninstruction-following(fol- Thevisuallanguagenavigationtaskisdesignedtoallowintelligent
lower)andinstructiongeneration(speaker)simultaneously.Most agentstointeractintheenvironmentaccordingtohumaninstruc-
ofthesemethodsuserecurrentneuralnetworks,andthecoreidea tionstocompletecorrespondingtasks.Foreachtask,itcontainsa
istoencodepreviousvisualobservationsandbehaviorsintoahid- triple(ğ‘¥ 1:ğ¿,ğ‘£ 1:ğ‘¡,ğ‘ 1:ğ‘¡),consistingofinstructions,visualsequences,
denstate.However,recurrentneuralnetworkshavelong-range andactionsequences.Amongthem,theinstructionğ‘¥ 1:ğ¿ isanat-
dependencyproblemsandtheirabilitytomodellongsequences urallanguagewithlengthğ¿.Thevisualsequenceğ‘£
1:ğ‘¡
containsğ‘¡
ispoor.Inrecentyears,withtheintroductionoftransformerar- images,representingtherealtrajectoryduringtheexecutionofthe
chitecture[47],itcanprovideglobal-levelattentioninteractionfor task.Theactionsequencesğ‘ 1:ğ‘¡ isasequenceofğ‘¡ actiontypelabels
longsequencetasks.VLN-Bert[29]trainsatransformermodelto ğ‘ ğ‘¡ âˆˆ {ğ´}whereğ´isactionspace.Thetaskgoalistofitafunc-
modelcompatibilitybetweeninstructionsandasetofgenerated tionğ‘“,whoseinputisinstructioninformation,visualinformation,
trajectories.RecurrentVLBERT[14]usesexplicitrecursivestate historicalactions,andtheoutputisthepredictednextaction.
andpre-trainedVLBERTtoprocessobservationsatarbitrarytime
steps,butthissolutionisdifficulttosolvelongsequenceVLNtasks. ğ‘
ğ‘¡
=ğ‘“(ğ‘¥ 1:ğ¿,ğ‘£ 1:ğ‘¡,ğ‘ 1:ğ‘¡âˆ’1).
(1)
Transformershavesuccessfullyimplementedawiderangeofclassi-
Theaboveprocessiscarriedoutiteratively.Theagentcontinuously
ficationandgenerationtasks,fromlanguagetoimagesandvideos.
predictsnewactionstoobtainnewvisualenvironments.Itstops
In[32],theauthorsshowthatlong-distancetaskplanningusing
untiltheagentthinksthetaskiscompletedorreachesthemaximum
reinforcementlearningtransformersischallengingandpropose
steplimit.
asolution.Thetransformerarchitecturecanalsoeffectivelyfuse
Sinceğ‘¥usuallycontainsfine-grainedtaskinstructions,andthere
informationfromdifferentmodalities,whereaunifiedtransformer
isalargegapwiththerealworld,weaimtostudytheVLNalgorithm
modelisresponsibleforsolvingproblemsthatrequiremulti-modal
thatonlycontainscoarse-grainedtaskinstructions.Inthistask
information,suchasvisualquestionanswering[23],videosubtitles
setting,duetothelackoffine-grainedinstructions,weexplore
andtemporalprediction[44]orretrieval[9].E.T.[33]proposedan
utilizingexternaleventknowledgetoenhancenavigationplanning.
algorithmtomodeltheentireVLNtaskusingaunifiedtransformer
Specifically,weconstructanovelVLN-specificeventknowledge
architecture.
graphandproposeanevent-enhancedVLNplannertoimproveVLN
LLM-basedVLNInrecentyears,thebirthofLLMhasgreatly
performanceundertheconditionofcoarse-grainedinstructions.
promotedthedevelopmentoftheentirefieldofnaturallanguage
processing.BecauseLLMhasrichcommonsenseknowledgeand
4 EVENTKNOWLEDGEGRAPHFORVLN
instruction-followingcapabilities,someworkisalsoactivelyex-
4.1 EventsinVLNScenery
ploringtheapplicationofLLMtovisuallanguagenavigation.On
task.Shahetal.[39]adoptedGPT-3[3]toidentifyâ€œlandmarksâ€ Aneventrepresentsaspecificactivityorsituationthatoccursata
orsub-objectives,whileHuangetal[16]focusedtheireffortson specificeventandlocation.Structurallyrepresentingeventsina
applying LLM to code generation code. Zhou et al [53] used a knowledgegraph,canhelpthesystemunderstandthecorrelation
LLMtoextractcommon-senseknowledgeabouttherelationship betweeneventsandhelpmodelretrieval,reasoning,andanalysis.
betweentargetobjectsandobjectsinobservationsandperform IntheVLNscenario,tocompleteacoarse-grainedinstruction,the
zero-examplereasoningobjectnavigation(zson)[28].Thesetasks modelneedstoseriallyexecuteaseriesoffine-grainedinstructions.
requireLLMâ€™sinternalknowledgefortaskplanning.Thediffer- Wedefinethesefine-grainedinstructionsaseventknowledgein
encefromtheaboveworkisthatwefocusoneventknowledge VLNtasks.Foramissionplanningmodel,onlybyfullyunderstand-
specifictoVLNscenariosandbuilditintoaknowledgegraphina ingtheinternaleventknowledgeoftheVLNscenariocanbetter
structuredform,therebyassistingLLMinmakingmoreaccurate decisionsbemade.AlthoughthegeneralLLMhasrichexperimental
missionplanning. knowledge,theVLNtaskhasalargeamountofpriorknowledge
thatfitsthescenery,whichisspecificallyreflectedinthecorrela-
tionbetweeneachsubtask.Thisknowledgeisnotyetavailablein
thegeneralLLM.Forexample,fortheevent"pickupanapple",in
therealworld,subsequenteventsmayinclude"washtheapple",CIKMâ€™24,October21â€“25,2024,Boise,ID,USA KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixuLi
Figure3:TheframeworkofourproposedEventNavwithlarge-small-modelcollaborationusingeventknowledge.
"eattheapple","cuttheapple","peel"andsoon,butinaspecific processoftheeventknowledgegraphandthepromptdesignduring
VLNscenery,thesubsequenteventsmayonlyinclude"cleaning knowledgeextractionrespectively.Intheend,weget8.4w,2.9w,
theapple"and"cuttingtheapple".Thismismatchineventdistribu- and3.9wsequentialsubtasksintheALFRED[40],R2R[2],and
tionmaycauseLLMtogeneratesubtasksthatdonotfittheVLN REVERIE[37]datasetsrespectively.Bymergingtheknowledgein
sceneryduringprediction.Therefore,eventknowledgeespecially thethreedatasets,weobtainedatotalof150k+nodesand120k+
withsequentialrelationshipsthatfitstheVLNscenerycanbeof relationshipseventknowledgegraph.Weusetheembeddingmodel
greathelptoLLMintaskplanning.Itcanconstrainthesubtasks toembedallnodesintoavectordatabaseforsubsequentretrieval.
predictedbyLLMwithinasmallandreasonablerange,thereby
promptingtheLLMtopredictmoreaccuratesubtasks. 5 EVENT-KNOWLEDGE-ENHANCEDVLN
5.1 FrameworkOverview
4.2 DataCollectionandConstruction Unlikemostalgorithmsthatprovidefine-grainedtaskinstructions,
Inourwork,wefirstintegrateeventknowledgewithincurrent thisworkfocusesonVLNtasksthatonlyprovidecoarse-grained
mainstreamVLNbenchmarkstobuildanovelVLN-specificevent instructions.However,simplyeliminatingallfine-grainedsubtasks
knowledge.Specifically,wetrytoextractallcoarse-grainedtasks will greatly affect the performance of the entire model. So, we
andcorrespondingsubtasksequencesinthedataset.Thusbuilding usethepowerfultaskplanningcapabilitiesoftheLLMtopredict
aneventknowledgegraph.Theeventknowledgegraphdescribes subtasks.Besides,weextractedknowledgefromtheVLNdataset
theexecutionsequenceofsubtaskscorrespondingtoeachcoarse- andbuiltaneventknowledgegraphcalledVLN-EventKG.Wede-
grainedtask.Thesesubtasksserveasexternalknowledgetoassist signacollaborativemodelarchitectureoflargeandsmallmodels.
thetaskplanningoftheLLM.WestudiedthreetypicalVLNdatasets: Thearchitectureconsistsoftwo-levelloops,theouteris"Subtask
ALFRED[40],R2R[2],andREVERIE[37].InALFRED[40],the PlanningLoop"andtheinneris"ActionPlanningLoop".
corresponding subtask sequence of each coarse-grained task is ForSubtaskPlanningLoop,LLMactsontheouterloop,which
given in the form of key-value pairs. This is already the result obtainsthreeinputs,includingthecoarse-grainedtask(Input1)
wewant,sothereisnoneedforadditionalextractionwork.For theimage-densecaptionafterBLIP2conversion(Input2),andthe
theR2R[2]dataset,thecoarse-grainedtasksandsubtasksarea similarsubtasksequenceretrievedfromtheeventknowledgegraph
unifiedtextparagraph.FortheREVERIE[37]dataset,thecoarse- VLN-EventKGbasedonthelastsubtask.Thesetwoinputswith
grainedtaskandsubtaskareseparated,butthesubtaskisatext retrievedknowledgeareconvertedintopromptsforLLMtopredict
paragraph.Therefore,fortheR2R[2]andREVERIE[37]datasets, thenextsubtaskğ‘¥ 1ğ‘ğ‘¢ :ğ¿ğ‘Ÿ 2ğ‘Ÿğ‘’ğ‘›ğ‘¡âˆ’ğ‘ ğ‘¢ğ‘ğ‘¡ğ‘ğ‘ ğ‘˜ .
weuseLLMtoextractcoarse-grainedtasksandsubtasksinthem ForActionPlanningLoop,accordingtoEq.(1),thesmallmodel
tobuildaneventknowledgegraph.Figure2showtheconstruction actsontheinnerlooptopredicttheaction(Output1)ğ´ ğ‘¡atthetimeTowardsCoarse-grainedVisualLanguageNavigationTaskPlanningEnhancedbyEventKnowledgeGraph CIKMâ€™24,October21â€“25,2024,Boise,ID,USA
Figure4:Subtaskgenerationbasedoneventknowledge(VLN-EventKG)retrievalandLLMusingpromotinglearning.
ğ‘¡ followingtheinputtextualinstructionsğ‘¥ 1:ğ¿,visualobservation above-mentionedprocessasthedynamicbacktrackingmechanism
ğ‘£ 1:ğ‘¡ andhistoricalactionsğ‘ 1:ğ‘¡âˆ’1.Specifically,textualinstructions inSec.5.3.TheoverallmodelarchitectureisshowninFigure.3.
ğ‘¥ 1:ğ¿canberewritteninto:
ğ‘¥ 1:ğ¿ = [ğ‘¥ 1ğ‘ :ğ‘œ ğ¿ğ‘ 1ğ‘Ÿğ‘ ğ‘’âˆ’ğ‘”ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘’ğ‘‘ ,ğ‘¥ 1ğ‘ğ‘¢ :ğ¿ğ‘Ÿ 2ğ‘Ÿğ‘’ğ‘›ğ‘¡âˆ’ğ‘ ğ‘¢ğ‘ğ‘¡ğ‘ğ‘ ğ‘˜ ], (2) 5.2 Knowledge-enhancedPlanner
whereğ‘¥ğ‘ğ‘œğ‘ğ‘Ÿğ‘ ğ‘’ğ‘‘âˆ’ğ‘”ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘’ğ‘‘ andğ‘¥ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡âˆ’ğ‘ ğ‘¢ğ‘ğ‘¡ğ‘ğ‘ ğ‘˜
representtextualin-
Theeventknowledgegraphservesasplug-inknowledgeforthe
1:ğ¿1 1:ğ¿2 LLM.InthetaskplanningprocessoftheLLM,itistoconstrain
structions.ğ¿1andğ¿2indicatethetextlengthofcoarse-grained
themodeltopredictsubtasksthataremoresuitablefortheVLN
instructionandthecurrentsubtaskinstructiongeneratedbythe scenery.Assumingthatthecurrentsubtaskisğ‘‡ ğ‘–,thepredictionof
SubtaskPlanningLoop.Visualobservationğ‘£ 1:ğ‘¡includeshistorical thenextsubtaskğ‘‡ ğ‘–+1canbeobtained:
visualinformationandcurrentvisualinformationattimeğ‘¡.
ğ‘£ 1:ğ‘¡ = [ğ‘£ 1:ğ‘¡âˆ’1,ğ‘£ ğ‘¡], (3) ğ‘‡ ğ‘–+1=ğ¿ğ¿ğ‘€(ğ·,ğ‘‰,ğ»,ğ¿),
(4)
Besides,ineachstep,thesmallmodelalsooutputstwoadditional ğ¿=ğ‘…ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘ğ‘™(VLN-EventKG,ğ‘‡ ğ‘–),
signals(Output2)ğ‘† ={0,1}andğ‘… âˆˆ [0,1].ğ‘†representswhether
thecurrentsubtaskiscompleted,andğ‘…representstheprobability whereğ· representsthedefinitionofthecoarse-grainedtask,ğ‘‰
ofthecurrentsubtaskcompletion.ğ‘†hashigherpriorityifthereis representstheimage-densecaptionafterBLIP2[22]conversion,
aconflictbetweenğ‘†andğ‘…. andğ» represents the subtask that has been executed. Figure. 4
Whenthesmallmodelpredictsthatthesubtaskhasbeencom- showsthepromptoftheLLMusedinsubtaskprediction.Since
pleted,ğ‘†is1,itpromotestheLLMtoplanthenextsubtask.When thesubtaskspredictedbytheLLMmaynotbecorrect,thesmall
thesmallmodelpredictsalowprobabilityğ‘…ofthecurrentsubtask modelwillalsoasktheLLMtore-planthesubtaskbytriggering
completion,theLLMwillre-planthecurrentsubtask.Wedesignthe thedynamicbacktrackingmechanism,shownintheblueframe.CIKMâ€™24,October21â€“25,2024,Boise,ID,USA KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixuLi
Dataset ğ· ğ‘ğ‘£ğ‘”
R2R[2] 2.35
REVERIE[37] 3.57
ALFRED[40] 8.26
Table1:Theaveragenumberofactionsrequiredforeach
subtaskineachdatasetbenchmarks
Inaddition,ğ‘…ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘ğ‘™ representsthevectorretrieval.Weuse
the embedding model to retrieve the current subtaskğ‘‡ ğ‘– as the
queryintheEventKnowledgeGraphVLN-EventKG.Weretrieve
ğ‘¡ğ‘œğ‘ğ‘˜subtasks{ğ‘  1,ğ‘  2,...,ğ‘  ğ‘˜}similartothecurrentsubtaskğ‘‡ ğ‘–,that
is{ğ‘‡ ğ‘–ğ‘  1,ğ‘‡ ğ‘–ğ‘  2,...,ğ‘‡ ğ‘–ğ‘ ğ‘˜},andobtaintheirsubsequentsubtasks Figure5:Thetrainingdetailsofthedynamicbacktracking
{ğ‘‡ğ‘  1 ,ğ‘‡ğ‘  2 ,...,ğ‘‡ğ‘ ğ‘˜ }.Ultimately,ğ¿isasetofsimilarsubtasks mechanism.
ğ‘–â†’ğ‘–+1 ğ‘–â†’ğ‘–+1 ğ‘–â†’ğ‘–+1
andtheirsubsequentsubtasks:
ğ¿={(ğ‘‡ ğ‘–ğ‘  1,ğ‘‡ ğ‘–ğ‘  â†’1 ğ‘–+1),(ğ‘‡ ğ‘–ğ‘  2,ğ‘‡ ğ‘–ğ‘  â†’2 ğ‘–+1),...,(ğ‘‡ ğ‘–ğ‘ ğ‘˜,ğ‘‡ ğ‘–ğ‘  â†’ğ‘˜ ğ‘–+1)}. (5)
5.3 DynamicHistoryBacktrackingMechanism basedonthecurrentcompletedsubtaskcontext.Weuseatransistor
Existingvisuallanguagenavigationmethodspredictthenextaction switchflagtovisualizethisprocess,shownontheleftofFigure.3.
entirelybasedonlanguageinstructionsandvisualinformation.It
Priority2:ğ‘…showsacontinuousdownwardtrendorthevalue
isterminatedwhenthemodelpredictsthetaskhasbeencompleted
ofğ‘… islessthan0.25.Itmeansthecurrentsubtaskcouldnotbe
orexecutionreachesthemaximumstep.However,anyerrorduring completed,inwhichtheagentneedstogobacktothestartposi-
taskexecutionmaybecontinuouslyamplifiedandeventuallylead tionofthecurrentsubtask,andtheLLMispromptedtore-plan
to task failure. Under the framework of large and small model thesubtaskbasedonthelasteffectivesubtaskcontext.Aspecific
collaboration,theLLMisusedforsubtaskplanningandpredicts promptingcaseisshowninFigure.4.
thenextsubtask,whilethesmallmodelpredictsthenextaction
basedoncoarse-grainedtasks,visualinformation,andsubtasks 6 EXPERIMENTANDANALYSIS
predictedbytheLLM.SincethesubtaskspredictedbyLLMmaynot
6.1 DatasetsandSimulationEnvironments
becompletelycorrect,incorrectsubtasksmayresultinincorrect
actiongenerationwhicheventuallyleadstofailureofthenavigation AllVLNdatasetsneedtobebuiltbasedonacertainsimulationenvi-
task.However,thesubtaskspredictedbyLLMarenotcompletely ronment.TheR2R[2]andREVERIE[37]datasetsarebuiltonMat-
fixed.IfLLMcanadjustthepredictionresultsofthesubtasksbased terPort3Dsimulation.Inthesedatasets,theagentcannavigatethe
on certain signals and guide the small model to perform extra houseaccordingtothenavigationgraph.Thenavigationgraphcon-
actions,thefailureofthetaskcouldbeavoided.Specifically,during sistsofviewpointsandedges.Eachviewpointcontainsapanoramic
theexecutionofthegeneratedaction,inadditiontopredictingthe view,andthereisabidirectionaledgebetweenanytwonavigable
nextaction,thesmallmodelalsopredictstwoadditionalvalues viewpoints,thatis,theagentcanmovebidirectionallybetween
(signals),namelyğ‘†andğ‘….ğ‘†representswhetherthecurrentsubtask twoadjacentviewpoints.Initially,theagentisplacedatarandom
iscompleted(completed,set1,andviceversa),andğ‘…represents viewpointandinputsanaturallanguageinstruction.Ateachstep,
theprobabilityofthecurrentsubtaskcompletion. ğ‘¡,theagentobtainsapanoramicgraphğ‘‚ ğ‘¡ = {ğ‘œ ğ‘¡,ğ‘–},1 â‰¤ ğ‘– â‰¤ 36,
Thepredictionwithprobabilityğ‘… isthetriggeringcondition whichincludesseverallocalviews.Eachlocalviewğ‘œ ğ‘¡ represents
forthebacktrackingmechanism.Duringthemodeltrainingpro- anavigableviewpoint,andtheagentneedstochooseoneofthe
cess,weobtainnegativesamplesbyrandomsamplingfromaction viewpointstogo.
space.Specifically,wesetthatthesubtaskrequiresğ‘Š actionsto TheALFRED[40]datasetisbuiltonAI2THORsimulation.Inthis
completeandthestartpointvalueofğ‘… 1 = 0.5.Forthepositive dataset,theagentisplacedatarandomposition,andineachstep,a
action trajectory, after theğ‘– âˆ’ğ‘¡â„ action of the subtask is com- definedactionisselectedforexecution.Theseactionsincludetwo
pleted,ğ‘… ğ‘– = ğ‘… 1+ 2ğ‘Šğ‘– ,ğ‘– â‰¤ ğ‘Š.Fornegativeactiontrajectory,we types:movementandnavigation.Thereare6typesofmovement
useweaknegativesamplestoapproximaterealnegativesamples, actions(i.e.forward,backward,leftturn,rightturn,headup,head
byrandomsamplingwithinactionspace,thenthecorresponding down),and7typesofinteractiveactions(i.e.pickingup,putting
ğ‘… ğ‘– =ğ‘… 0âˆ’ 2ğ‘Šğ‘– ,ğ‘– â‰¤ğ‘Š.ThewholeprocessisshowninFigure.5. down,opening,closing,washing,cooking,andchopping).The7
Duringmodelinference,twopriorityconditionsaresettopro- typesofinteractiveactionsinvolveinteractingwithtargetobjects
moteanewsubtask. intheenvironment.Thetargetobjectoftheactionisrepresentedby
Priority1:ğ‘†predictedtobe1.Thisconditionhashigherpriority amask.Therefore,forinteractiveactions,theagentneedstooutput
comparedtoanotherone,whichmeansthecurrentsubtaskhas notonlytheactioncategorybutalsothemaskoftheinteractive
beencompletedandtheLLMcandirectlypredictthenextsubtask object.TowardsCoarse-grainedVisualLanguageNavigationTaskPlanningEnhancedbyEventKnowledgeGraph CIKMâ€™24,October21â€“25,2024,Boise,ID,USA
ValidationSeen ValidationUnseen TestUnseen
Methods
SRâ†‘ NEâ†“ SPLâ†‘ TL SRâ†‘ NEâ†“ SPLâ†‘ TL SRâ†‘ NEâ†“ SPLâ†‘ TL
Random 16 9.45 - 9.58 16 9.23 - 9.77 13 9.79 12 9.89
Human - - - - - - - - 86 1.61 76 11.85
Seq2Seq-SF[1] 39 6.01 40 11.33 22 7.81 19 11.67 20 7.85 18 8.13
Speaker-Follower[8] 54 4.29 49 12.58 29 7.92 27 13.02 28 7.84 24 15.19
EnvDrop[45] 55 3.82 56 9.69 39 5.92 36 10.24 41 6.02 43 11.24
RecBERT[15] 67 3.49 63 11.45 58 4.54 54 12.94 58 4.61 53 12.46
HOP[38] 68 3.23 65 11.37 57 4.52 50 12.79 56 4.39 56 12.93
AirBERT[11] 69 3.19 64 11.56 55 4.29 51 12.56 55 4.33 52 12.57
EventNav(ours) 72 2.77 63 12.58 60 4.23 51 14.56 59 4.25 49 14.55
Table2:ComparativeresultsbetweenourmethodandothermainstreammethodsontheR2R[2]datasetthatonlyprovides
coarse-grainedinstructions.
ValidationSeen ValidationUnseen TestUnseen
Methods
SRâ†‘ OSRâ†‘ SPLâ†‘ TL SRâ†‘ OSRâ†‘ SPLâ†‘ TL SRâ†‘ OSRâ†‘ SPLâ†‘ TL
Random 3 8.92 2 11.99 2 11.93 1 10.76 2 8.88 1 10.34
Human - - - - - - - - 81 86.83 54 21.18
RCM[49] 23 29.44 22 10.70 9 14.23 7 11.98 8 11.68 7 10.60
FAST-MATTN[37] 50 55.17 26 16.35 14 28.20 6 29.70 14 23.36 9 30.69
AirBert[11] 47 48.98 42 15.16 28 34.51 22 18.71 30 34.20 23 17.91
RecBERT[15] 51 53.90 48 13.44 31 35.02 25 16.78 30 32.91 24 15.86
HOP[38] 53 54.88 37 13.80 32 36.24 26 16.46 30 33.06 24 16.38
ORIST[36] 45 49.12 42 10.73 17 25.02 15 10.90 22 29.20 19 11.38
CKR[10] 57 61.91 53 12.16 19 31.44 11 26.26 22 30.40 14 22.46
EventNav(ours) 61 63.29 45 15.25 34 39.28 23 16.25 34 39.36 23 16.10
Table3:ComparativeresultsbetweenourmethodandothermainstreammethodsontheREVERIE[37]datasetthatonly
providescoarse-grainedinstructions.
TestSeen TestUnseen
Methods
SRâ†‘ GCâ†‘ PLWSRâ†‘ PLWGCâ†‘ SRâ†‘ GCâ†‘ PLWSRâ†‘ PLWGCâ†‘
Human - - - - 91 94.5 - -
SEQ2SEQ[40] 3 8.00 7.29 12.56 1 7.30 2.42 9.09
E.T.[33] 22 29.31 12.39 17.64 4 8.60 5.77 10.24
MOCA[42] 22 28.37 12.36 26.78 5 14.30 9.24 19.19
FILM[31] 26 36.15 10.39 19.17 - - - -
EmBert[43] 25 37.69 11.29 25.60 7 12.49 9.79 18.22
EventNav(ours) 31 42.20 11.97 24.62 10 18.75 9.26 19.48
Table4:ComparativeresultsbetweenourmethodandothermainstreammethodsontheALFRED[40]datasetthatonly
providescoarse-grainedinstructions.
6.2 ImplementationDetails In"ActionPlanningLoop",wefollowtheworkof[33]andadopt
Theframeworkofourproposedmethodincludes"SubtaskPlanning aunifiedtransformer-basedmodel.Forallvisualimages,wefirst
Loop"and"ActionPlanningLoop".LLM(i.e.ChatGPT[3])isused feedthemintoResNet50[13]toobtainthevisualrepresentation
in"SubtaskPlanningLoop".Wefollowtheworkof[52]anduse vector,andthenthevectorisusedasatokenforthetransformer-
theBLIP2[22]toprovideLLMwithanimage-densecaptionofthe basedmodel.Inaddition,fortheALFRED[40]datasetbenchmark,
currentvisualenvironment.FortheeventknowledgegraphVLN- itstasksnotonlyrequiretheagenttomovebutalsorequireitto
EventKG,weusethebge-large-en[51]semanticmodeltoperform interactwiththeenvironment.Therefore,themodelalsoneedsto
vectorsimilarityretrievalandretrievesimilarsubtaskstoprompt generateamasktorepresentwhichparttointeractwith.Weuse
theLLMfortaskplanning. ResNet-50MaskR-CNN[12]togenerateatargetmask.Overall,CIKMâ€™24,October21â€“25,2024,Boise,ID,USA KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixuLi
onlythetransformer-basedmodelandMaskR-CNN[12]arein-
volvedinthetrainingprocess,andtherestoftheparametersof
themodelsarefrozen.Wetrainourproposedmodelfor20epochs
usingPytorch1.13onfourV100GPUplatforms,withbatchsize64,
Adamoptimizer,andlearningrate1ğ‘’âˆ’3.
Duringtheinferenceprocess,theLLMisusedtosubtaskplan-
ning,andthesmallmodelisusedtogenerateactionstobeexecuted.
Forknowledgeenhancement,weretrieveğ‘¡ğ‘œğ‘ğ‘˜similarsubtasksin
VLN-EventKGeachtime.Here,wechooseğ‘¡ğ‘œğ‘ğ‘˜ =5.Inthedynamic
backtrackingmechanism,whentheprobabilityğ‘…predictedbythe
smallmodelmeetsoneofthefollowingtwoconditions,theLLM
will be required to re-plan the subtask: 1)ğ‘… < 0.25; 2)ğ‘… shows
adownwardtrendğ‘Š consecutivetimes.Forğ‘Š inFigure1and
Sec.5.3,Theaverageexecutionlengthğ· ğ‘ğ‘£ğ‘”ofeachsubtaskover Figure6:Taskplanningcasestudyunderknowledgeenhance-
differentdatasetsisshowninTable1.Wesetğ‘Š withweighted ment
ğ· ğ‘ğ‘£ğ‘”fordifferentdatasets(i.e.R2R[2]:2Ã—ğ· ğ‘ğ‘£ğ‘”,REVERIE[37]and
ALFRED[40]:ğ· ğ‘ğ‘£ğ‘”).
6.3 EvaluationMetrics tablesondifferentdatasetbenchmarks,wecanfindthatourVLN-
EventKGprovidesVLNplannerswithusefuleventknowledge,and
ForR2R[2]:
coupledwiththedynamicbacktrackingmechanism,ourproposed
â€¢ TrajectoryLength(TL):theaveragepathlengthinmeters;
EventNavmodelforVLNtaskachievecompetitiveperformance
â€¢ NavigationError(NE):theaveragedistancebetweenthe
withcoarse-grainedinstructionsinput.Figure6alsoshowsacase
agentâ€™sfinalpositionandthetargetinmeters;
studyonthecoarse-grainedVLNtask,withtheenhancementof
â€¢ SuccessRate(SR):theratioofstoppingwithin3metersto
theeventknowledgegraphofVLN-EventKG,LLMcanobtainthe
thetarget;
eventknowledgeoftheVLNscenario,therebyrealizingamore
â€¢ SuccessPathLength(SPL):thesuccessrateweightedbythe
reasonabletaskplanning.
normalizedinverseofthePathLength.
ForREVERIE[37]alsoemploysOracleSuccessRate(OSR):theratio 6.5 AblationStudy
ofhavingaviewpointalongthetrajectorywherethetargetobject
VLN-EventKGFortheutilizationofeventknowledge,weconsider
isvisible.
theimpactoftheeventknowledgegraphVLN-EventKGonthe
ForALFRED[40]:
performanceofthemodel.Weuseasmallmodelthatonlyprovides
â€¢ SuccessRate(SR):thesuccessrateofthetotaltask;
coarse-grainedinstructionsasabaseline(shortasbase)toconsider
â€¢ ConditionalSuccess(GC):theratioofsubtaskscompleted
thefollowingsituations:
overthewholetask;
â€¢ PathlengthweightedSR(PLWSR):theratioof(lengthofthe (1) base+planD:Withoutusingtheeventknowledgegraph,let
expertpath)and(lengthtakenbytheagent); LLMplansubtasksDirectly;
â€¢ PathlengthweightedGC(PLWGC):theratioof(lengthof (2) base+planS:KnowledgeisextractedSeparatelyforeach
thegroundtruthPath)and(lengthtakenbytheagent). datasetbenchmark,andtheeventknowledgegraphcorre-
spondingtoeachdatasetbenchmarkisusedduringinfer-
6.4 ComparisonsonDifferentVLNbenchmarks ence;
(3) base+planF:FusingtheeventknowledgefromVLNdataset
Basedonthesettingofcoarse-grainedtaskdescriptions,Weevalu-
benchmarkdomainsandusingtheentireeventknowledge
atetheperformanceofEventNavandmainstreamVLNmodelson
graphduringinference.
threedatasetbenchmarks:R2R[2],REVERIE[37]andALFRED[40].
Table2presentsthecomparativeresultsbetweenourmethodand AsshownresultsofSuccessRate(SR)inTable5,wefindthatin
other VLN models in R2R dataset [2] and our proposed model R2R[2],REVERIE[37],andALFRED[40]benchmark,themodels
achieves strong performance inmost metrics. The scores of SR fusedeventknowledgegraph(base+planF)performbetterthan
showanaverageimprovementof 2%overtheexistingresults.In thesingleeventknowledgegraph(base+planS).Inparticular,for
Table3,themostsignificantimprovementisobservedintheun- R2R[2]andREVERIEâ€™s[37],themodelfusedeventknowledge
seentestdatasetofREVERIE[37],witha4%increaseinSRmetric. graphhassignificantlyimprovedthemodeleffect,withtheaverage
ComparedtotheexistingmethodsonSRandOSRmetrics,our SRimprovementover2%.Weattributeittothesimilardistributions
modelachievesanaverageimprovementof3.3%and3.2%improve- oftaskdescriptionsinabovementionedtwodatasets.Therefore,the
ments,respectively.TheresultofALFRED[40]isshowninTable4. modelsfusedeventknowledgegraphcanobtainbroaderanddiverse
Weevaluateperformanceintwosettingswithfourmetricsand externalknowledgepromotingthenextsubtaskfromtheLLM.In
achieveSOTAinfivemetrics.Comparedwiththeexistingmodels, contrast,subtasksintheALFRED[40]datasetbenchmarkmainly
theSRofourmodelisimprovedby5%and3%onTestSeenand involveinteractingwiththeenvironment,whichexistsrelative
Testunseensettings.Fromtheresultsshownintheabovethree differencesfromthefirsttwobenchmarkdomains.ThisreducesTowardsCoarse-grainedVisualLanguageNavigationTaskPlanningEnhancedbyEventKnowledgeGraph CIKMâ€™24,October21â€“25,2024,Boise,ID,USA
theeffectivenessofeventknowledgetoimproveperformancein Method/Dataset R2R[2] REVERIE[37] ALFRED[40]
base 55 22 14
theALFRED[40]benchmark.
base+planD 49 20 13
WhennotusingtheeventknowledgegraphandlettingLLM
base+planS 61 45 26
plansubtasksdirectly(base+planD),wefoundthatduetothelack base+planF 63 48 26
ofeventknowledgefromtheknowledgegraph,thenextsubtask base+planF+backtrace(Ours) 72 61 31
predictedbythemodelisusuallyverydifferentfromtheoriginal Table5:Theimpactofeventknowledgegraphonmodeltask
subtaskdistributionofthedatasetbenchmark.Thisapproachcould planningeffect.
damage the performance of the action prediction for the small
model.Therefore,ourVLN-EventKGplaysaconstraintroleinthe
subtask planning process based on coarse-grained instructions, ğ‘¥ R2R[2] REVERIE[37] ALFRED[40]
whichguaranteestheLLMplansubtasksclosetothedistribution ğ‘Š 0.1 0.25 0.5 0.1 0.25 0.5 0.1 0.25 0.5
oftheoriginaldatasetbenchmark,therebypromotingtheabstract 0.5Ã—ğ· ğ‘ğ‘£ğ‘” 63 66 60 52 58 55 27 29 22
taskunderstandingandactiongenerationofthesmallmodel. ğ· ğ‘ğ‘£ğ‘” 69 69 67 59 61 51 30 31 29
DynamicBacktrackingMechanismDynamicbacktracking 2Ã—ğ· ğ‘ğ‘£ğ‘” 71 72 70 58 60 51 26 30 25
mechanismaimstodeterminewhetherthecurrentsubtaskcanbe 4Ã—ğ· ğ‘ğ‘£ğ‘” 70 70 69 58 55 50 24 27 25
executedsuccessfullybasedontheprobabilitylevelğ‘… predicted
Table6:Inthedynamicbacktrackingmechanism,theimpact
by the small model, and guide the LLM to re-plan the subtask
ofğ‘¥ andğ‘Š ontheexperimentalresults.
attheappropriatetime.Specifically,whenoneofthefollowing
twoconditionsoccurs,thebacktrackingmechanismistriggered,
denotedinSec.6.2:
(1) ğ‘… <ğ‘¥; thefirsteventknowledgegraphVLN-EventKGforVLNtasksby
(2) ğ‘…showsadownwardtrendğ‘Š consecutivetimes. extractingandconceptualizingtheactivitysequencesacrossvari-
whereğ‘¥ andğ‘Š aretwohyperparametersofthemodelinference ouspublicVLNbenchmarks.Anevent-knowledge-enhancedVLN
planningmodelisdesignedunderthelarge-small-modelcollabo-
process.Thetriggeredtimingofbacktrackinghasagreatimpact
rativeframeworkEventNavtorealizecoarse-grainedinstruction
ontheoverallperformanceofthemodel.Backtrackingtooearly
understandinganddecomposition.Adynamicbacktrackingmech-
couldcausethemodeltoexitearlyonthecorrecttrajectorywhile
anismisconsideredtofurtherimprovethesuccessrateofVLNby
backtrackingtoolatecouldfailbecausethemaximumstepsize
in-timecorrectionofintermediatedecisions.Experimentalresults
specifiedbythetaskisreached.
indicatetheimportanceofeventknowledgeinsequentialdecisions,
Fordifferentdatasetbenchmarks,wechoosedifferentvaluesof
ğ‘¥ andğ‘Š toconductablationstudies.Weselectthevaluesofğ‘¥ as (i.e.VLNplanning).OurproposedVLN-EventKGcombinedwith
(0.1,0.25,0.5) respectively.Wechoosethevaluesofğ‘Š as (0.5Ã— EventNaveffectivelyimproveabstracthumaninstructionsunder-
ğ· ğ‘ğ‘£ğ‘”,ğ· ğ‘ğ‘£ğ‘”,2Ã—ğ· ğ‘ğ‘£ğ‘”,4Ã—ğ· ğ‘ğ‘£ğ‘”),whereğ· ğ‘ğ‘£ğ‘” denotestheaverage standingandhierarchicaltaskplanningbyover5%averagesuccess
rateinvariouspublicbenchmarks.
lengthofitssubtasksforaspecificdatasetinTable1.
Table6showstheSuccessRate(SR)correspondingtodifferentğ‘¥
8 ACKNOWLEDGMENTS
andğ‘Š underdifferentdatasetbenchmarks.Thevalueof0.25forğ‘¥
gavethebestmodelresults.Inaddition,thebestresultsareobtained Wethanktheanonymousreviewersfortheirvaluablecomments.
whenğ‘Š is2Ã—ğ· ğ‘ğ‘£ğ‘” forR2R[2]andğ· ğ‘ğ‘£ğ‘” forREVERIE[37]and ThisworkissupportedbyPostdoctoralFellowshipProgramofCPSF
ALFRED[40].Twiceğ· ğ‘ğ‘£ğ‘”usedinR2R[2],wethinkitisbecause underGrantNumberGZC20232292,NationalNaturalScienceFoun-
thelengthsofsubtaskstepsinthisbenchmarkareshorterthan dationofChina(No.62072323,U21A20488,No.62102276),Shanghai
theothertwo,andselectingğ‘Š =ğ· ğ‘ğ‘£ğ‘” couldcausethemodelto ScienceandTechnologyInnovationActionPlan(No.22511104700),
earlybacktracktoterminatethecorrectactionplanningloop.In ChinaPostdoctoralScienceFoundation(GrantNo.2023M732563),
contrast,ğ‘Š shouldbeappropriatelyselectedforalargervaluefor andZhejiangLabOpenResearchProject(No.K2022NB0AB04).
REVERIE [37] and ALFRED [40] benchmarks due to the longer
averagesubtasksteps.Whenğ‘…decreasesğ· ğ‘ğ‘£ğ‘”timescontinuously REFERENCES
canroughlydeterminethatasubtaskwilleventuallyfail,soğ‘Š [1] PeterAnderson,QiWu,DamienTeney,JakeBruce,MarkJohnson,NikoSÃ¼nder-
shouldbeğ· ğ‘ğ‘£ğ‘”.Inaddition,Table5alsoshowstheimpactofthe h na au vf ig,I aa tn ioR ne :i Id n, tS et re pp rh ete in ngG vo iu sl ud a, la lyn -d gA ron ut no dn eV da nn aD vie gn atH ioe nng inel s. trV ui csi to ion n-a sn ind- rla en alg eu na vg ie
-
dynamicbacktrackingmechanismontheoverallsuccessrateof ronments.InProceedingsoftheIEEEconferenceoncomputervisionandpattern
themodel.Ourmethod(ğ‘ğ‘ğ‘ ğ‘’+ğ‘ğ‘™ğ‘ğ‘›ğ¹ +ğ‘ğ‘ğ‘ğ‘˜ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘’)integratesthe recognition,pages3674â€“3683,2018.
[2] PeterAnderson,QiWu,DamienTeney,JakeBruce,MarkJohnson,NikoSÃ¼nder-
dynamicbacktracemechanismwiththewholeeventknowledge hauf,IanReid,StephenGould,andAntonVanDenHengel.Vision-and-language
graphshowingsignificantimprovementinthesuccessrateofover navigation:Interpretingvisually-groundednavigationinstructionsinrealenvi-
ronments.InProceedingsoftheIEEEconferenceoncomputervisionandpattern
allVLNbenchmarks.
recognition,pages3674â€“3683,2018.
[3] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,
7 CONCLUSION PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda
Askell,etal.Languagemodelsarefew-shotlearners.Advancesinneuralinfor-
Inthispaper,weinvestigatecoarse-grainedVLNplanningguided mationprocessingsystems,33:1877â€“1901,2020.
[4] DavidChenandRaymondMooney. Learningtointerpretnaturallanguage
byeventknowledge.Eventknowledgeespeciallyconsequentrela-
navigationinstructionsfromobservations.InProceedingsoftheAAAIConference
tionsisconsideredforsequentialdecisions.Specifically,wepropose onArtificialIntelligence,volume25,pages859â€“865,2011.CIKMâ€™24,October21â€“25,2024,Boise,ID,USA KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixuLi
[5] HowardChen,AlaneSuhr,DipendraMisra,NoahSnavely,andYoavArtzi.Touch- estimation.arXivpreprintarXiv:1901.03035,2019.
down:Naturallanguagenavigationandspatialreasoninginvisualstreetenvi- [27] Chih-YaoMa,ZuxuanWu,GhassanAlRegib,CaimingXiong,andZsoltKira.
ronments. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand Theregretfulagent:Heuristic-aidednavigationthroughprogressestimation.In
PatternRecognition,pages12538â€“12547,2019. ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
[6] Marc-AlexandreCÃ´tÃ©,AkosKÃ¡dÃ¡r,XingdiYuan,BenKybartas,TavianBarnes, pages6732â€“6740,2019.
EmeryFine,JamesMoore,MatthewHausknecht,LaylaElAsri,MahmoudAdada, [28] ArjunMajumdar,GunjanAggarwal,BhavikaDevnani,JudyHoffman,andDhruv
etal. Textworld:Alearningenvironmentfortext-basedgames. InComputer Batra.Zson:Zero-shotobject-goalnavigationusingmultimodalgoalembeddings.
Games:7thWorkshop,CGW2018,HeldinConjunctionwiththe27thInternational AdvancesinNeuralInformationProcessingSystems,35:32340â€“32352,2022.
ConferenceonArtificialIntelligence,IJCAI2018,Stockholm,Sweden,July13,2018, [29] ArjunMajumdar,AyushShrivastava,StefanLee,PeterAnderson,DeviParikh,
RevisedSelectedPapers7,pages41â€“75.Springer,2019. andDhruvBatra.Improvingvision-and-languagenavigationwithimage-text
[7] VishnuSashankDorbala,JamesFMullenJr,andDineshManocha.Cananem- pairsfromtheweb.InComputerVisionâ€“ECCV2020:16thEuropeanConference,
bodiedagentfindyourâ€œcat-shapedmugâ€?llm-basedzero-shotobjectnavigation. Glasgow,UK,August23â€“28,2020,Proceedings,PartVI16,pages259â€“274.Springer,
IEEERoboticsandAutomationLetters,2023. 2020.
[8] DanielFried,RonghangHu,VolkanCirik,AnnaRohrbach,JacobAndreas,Louis- [30] VincentMicheliandFranÃ§oisFleuret. Languagemodelsarefew-shotbutlers.
PhilippeMorency,TaylorBerg-Kirkpatrick,KateSaenko,DanKlein,andTrevor arXivpreprintarXiv:2104.07972,2021.
Darrell.Speaker-followermodelsforvision-and-languagenavigation.Advances [31] SoYeonMin,DevendraSinghChaplot,PradeepRavikumar,YonatanBisk,and
inneuralinformationprocessingsystems,31,2018. RuslanSalakhutdinov.Film:Followinginstructionsinlanguagewithmodular
[9] ValentinGabeur,ChenSun,KarteekAlahari,andCordeliaSchmid.Multi-modal methods.arXivpreprintarXiv:2110.07342,2021.
transformerforvideoretrieval.InComputerVisionâ€“ECCV2020:16thEuropean [32] EmilioParisotto,FrancisSong,JackRae,RazvanPascanu,CaglarGulcehre,Sid-
Conference,Glasgow,UK,August23â€“28,2020,Proceedings,PartIV16,pages214â€“ dhantJayakumar,MaxJaderberg,RaphaelLopezKaufman,AidanClark,Seb
229.Springer,2020. Noury,etal.Stabilizingtransformersforreinforcementlearning.InInternational
[10] ChenGao,SiLiu,JinyuChen,LutingWang,QiWu,BoLi,andQiTian.Room- conferenceonmachinelearning,pages7487â€“7498.PMLR,2020.
objectentitypromptingandreasoningforembodiedreferringexpression.IEEE [33] AlexanderPashevich,CordeliaSchmid,andChenSun.Episodictransformerfor
TransactionsonPatternAnalysisandMachineIntelligence,2023. vision-and-languagenavigation.in2021ieee.InCVFInternationalConferenceon
[11] Pierre-LouisGuhur,MakarandTapaswi,ShizheChen,IvanLaptev,andCordelia ComputerVision(ICCV),pages15922â€“15932,2021.
Schmid.Airbert:In-domainpretrainingforvision-and-languagenavigation.In [34] AmirPnueliandZoharManna.Thetemporallogicofreactiveandconcurrent
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages systems.Springer,16:12,1992.
1634â€“1643,2021. [35] XavierPuig,KevinRa,MarkoBoben,JiamanLi,TingwuWang,SanjaFidler,and
[12] KaimingHe,GeorgiaGkioxari,PiotrDollÃ¡r,andRossGirshick. Maskr-cnn. AntonioTorralba.Virtualhome:Simulatinghouseholdactivitiesviaprograms.
InProceedingsoftheIEEEinternationalconferenceoncomputervision,pages InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
2961â€“2969,2017. pages8494â€“8502,2018.
[13] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearning [36] YuankaiQi,ZizhengPan,YicongHong,Ming-HsuanYang,AntonVanDenHen-
forimagerecognition.InProceedingsoftheIEEEconferenceoncomputervision gel,andQiWu.Theroadtoknow-where:Anobject-and-roominformedsequen-
andpatternrecognition,pages770â€“778,2016. tialbertforindoorvision-languagenavigation.InProceedingsoftheIEEE/CVF
[14] YHong,QWu,YQi,CRodriguez-Opazo,andSGould. Arecurrentvision- InternationalConferenceonComputerVision,pages1655â€“1664,2021.
and-languagebertfornavigation.arxiv2021.arXivpreprintarXiv:2011.13922,1, [37] YuankaiQi,QiWu,PeterAnderson,XinWang,WilliamYangWang,Chunhua
2021. Shen,andAntonvandenHengel.Reverie:Remoteembodiedvisualreferring
[15] YicongHong,QiWu,YuankaiQi,CristianRodriguez-Opazo,andStephenGould. expressioninrealindoorenvironments.InProceedingsoftheIEEE/CVFConference
Vlnbert:Arecurrentvision-and-languagebertfornavigation. InProceedings onComputerVisionandPatternRecognition,pages9982â€“9991,2020.
oftheIEEE/CVFconferenceonComputerVisionandPatternRecognition,pages [38] YanyuanQiao,YuankaiQi,YicongHong,ZhengYu,PengWang,andQiWu.
1643â€“1653,2021. Hop:History-and-orderawarepre-trainingforvision-and-languagenavigation,
[16] ChenguangHuang,OierMees,AndyZeng,andWolframBurgard.Visuallan- 2022.
guagemapsforrobotnavigation.In2023IEEEInternationalConferenceonRobotics [39] DhruvShah,BÅ‚aÅ¼ejOsiÅ„ski,SergeyLevine,etal.Lm-nav:Roboticnavigation
andAutomation(ICRA),pages10608â€“10615.IEEE,2023. withlargepre-trainedmodelsoflanguage,vision,andaction.InConferenceon
[17] HaoshuoHuang,VihanJain,HarshMehta,AlexanderKu,GabrielMagalhaes, robotlearning,pages492â€“504.PMLR,2023.
JasonBaldridge,andEugeneIe.Transferablerepresentationlearninginvision- [40] MohitShridhar,JesseThomason,DanielGordon,YonatanBisk,WinsonHan,
and-languagenavigation.InProceedingsoftheIEEE/CVFinternationalconference RoozbehMottaghi,LukeZettlemoyer,andDieterFox. Alfred:Abenchmark
oncomputervision,pages7404â€“7413,2019. forinterpretinggroundedinstructionsforeverydaytasks.InProceedingsofthe
[18] WenlongHuang,PieterAbbeel,DeepakPathak,andIgorMordatch.Language IEEE/CVFconferenceoncomputervisionandpatternrecognition,pages10740â€“
modelsaszero-shotplanners:Extractingactionableknowledgeforembodied 10749,2020.
agents.InInternationalConferenceonMachineLearning,pages9118â€“9147.PMLR, [41] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre CÃ´tÃ©, Yonatan Bisk, Adam
2022. Trischler,andMatthewHausknecht. Alfworld:Aligningtextandembodied
[19] WenlongHuang,PieterAbbeel,DeepakPathak,andIgorMordatch.Language environmentsforinteractivelearning.arXivpreprintarXiv:2010.03768,2020.
modelsaszero-shotplanners:Extractingactionableknowledgeforembodied [42] KunalPratapSingh,SuvaanshBhambri,ByeonghwiKim,RoozbehMottaghi,and
agents.InInternationalConferenceonMachineLearning,pages9118â€“9147.PMLR, JonghyunChoi. Factorizingperceptionandpolicyforinteractiveinstruction
2022. following.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
[20] JacobKrantz,ErikWijmans,ArjunMajumdar,DhruvBatra,andStefanLee. Vision,pages1888â€“1897,2021.
Beyondthenav-graph:Vision-and-languagenavigationincontinuousenviron- [43] AlessandroSuglia,QiaoziGao,JesseThomason,GovindThattai,andGaurav
ments.InComputerVisionâ€“ECCV2020:16thEuropeanConference,Glasgow,UK, Sukhatme.Embodiedbert:Atransformermodelforembodied,language-guided
August23â€“28,2020,Proceedings,PartXXVIII16,pages104â€“120.Springer,2020. visualtaskcompletion.arXivpreprintarXiv:2108.04927,2021.
[21] JunchengLi,XinWang,SiliangTang,HaizhouShi,FeiWu,YuetingZhuang, [44] ChenSun,AustinMyers,CarlVondrick,KevinMurphy,andCordeliaSchmid.
andWilliamYangWang.Unsupervisedreinforcementlearningoftransferable Videobert:Ajointmodelforvideoandlanguagerepresentationlearning. In
meta-skillsforembodiednavigation.InProceedingsoftheIEEE/CVFConference ProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages
onComputerVisionandPatternRecognition,pages12123â€“12132,2020. 7464â€“7473,2019.
[22] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2:Bootstrapping [45] HaoTan,LichengYu,andMohitBansal. Learningtonavigateunseenen-
language-imagepre-trainingwithfrozenimageencodersandlargelanguage vironments: Back translation with environmental dropout. arXiv preprint
models. InInternationalconferenceonmachinelearning,pages19730â€“19742. arXiv:1904.04195,2019.
PMLR,2023. [46] StefanieTellex,ThomasKollar,StevenDickerson,MatthewWalter,AshisBaner-
[23] JiasenLu,DhruvBatra,DeviParikh,andStefanLee.Vilbert:Pretrainingtask- jee,SethTeller,andNicholasRoy.Understandingnaturallanguagecommands
agnosticvisiolinguisticrepresentationsforvision-and-languagetasks.Advances forroboticnavigationandmobilemanipulation. InProceedingsoftheAAAI
inneuralinformationprocessingsystems,32,2019. ConferenceonArtificialIntelligence,volume25,pages1507â€“1514,2011.
[24] CoreyLynch,MohiKhansari,TedXiao,VikashKumar,JonathanTompson,Sergey [47] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
Levine,andPierreSermanet.Learninglatentplansfromplay.InConferenceon AidanNGomez,ÅukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.
robotlearning,pages1113â€“1132.PMLR,2020. Advancesinneuralinformationprocessingsystems,30,2017.
[25] CoreyLynchandPierreSermanet.Groundinglanguageinplay.arXivpreprint [48] HanqingWang,WeiLiang,JianbingShen,LucVanGool,andWenguanWang.
arXiv:2005.07648,40(396):105,2020. Counterfactualcycle-consistentlearningforinstructionfollowingandgeneration
[26] Chih-YaoMa,JiasenLu,ZuxuanWu,GhassanAlRegib,ZsoltKira,RichardSocher, invision-languagenavigation. InProceedingsoftheIEEE/CVFconferenceon
andCaimingXiong. Self-monitoringnavigationagentviaauxiliaryprogress computervisionandpatternrecognition,pages15471â€“15481,2022.TowardsCoarse-grainedVisualLanguageNavigationTaskPlanningEnhancedbyEventKnowledgeGraph CIKMâ€™24,October21â€“25,2024,Boise,ID,USA
[49] XinWang,QiuyuanHuang,AsliCelikyilmaz,JianfengGao,DinghanShen,Yuan- 2023.
FangWang,WilliamYangWang,andLeiZhang.Reinforcedcross-modalmatch- [52] GengzeZhou,YicongHong,andQiWu.Navgpt:Explicitreasoninginvision-
ingandself-supervisedimitationlearningforvision-languagenavigation. In and-languagenavigationwithlargelanguagemodels.InProceedingsoftheAAAI
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition, ConferenceonArtificialIntelligence,volume38,pages7641â€“7649,2024.
pages6629â€“6638,2019. [53] KaiwenZhou,KaizhiZheng,ConnorPryor,YilinShen,HongxiaJin,LiseGetoor,
[50] XinWang,QiuyuanHuang,AsliCelikyilmaz,JianfengGao,DinghanShen,Yuan- andXinEricWang. Esc:Explorationwithsoftcommonsenseconstraintsfor
FangWang,WilliamYangWang,andLeiZhang.Vision-languagenavigation zero-shotobjectnavigation. InInternationalConferenceonMachineLearning,
policylearningandadaptation.IEEEtransactionsonpatternanalysisandmachine pages42829â€“42842.PMLR,2023.
intelligence,43(12):4205â€“4216,2020. [54] WangZhu,HexiangHu,JiachengChen,ZhiweiDeng,VihanJain,EugeneIe,and
[51] ShitaoXiao,ZhengLiu,PeitianZhang,andNiklasMuennighof.C-pack:Packaged FeiSha.Babywalk:Goingfartherinvision-and-languagenavigationbytaking
resourcestoadvancegeneralchineseembedding.arXivpreprintarXiv:2309.07597, babysteps.arXivpreprintarXiv:2005.04625,2020.