Video Annotator: A framework for efficiently building video
classifiers using vision-language models and active learning
AmirZiai AneeshVartakavi
aziai@netflix.com avartakavi@netflix.com
NetflixInc NetflixInc
LosGatos,CA,USA LosGatos,CA,USA
ABSTRACT ACMReferenceFormat:
High-qualityandconsistentannotationsarefundamentaltothesuc- AmirZiaiandAneeshVartakavi.2024.VideoAnnotator:Aframework
forefficientlybuildingvideoclassifiersusingvision-languagemodelsand
cessfuldevelopmentofrobustmachinelearningmodels.Traditional
activelearning.InProceedingsofKDD(KDDâ€™24).ACM,NewYork,NY,USA,
dataannotationmethodsareresource-intensiveandinefficient,of-
9pages.https://doi.org/XXXXXXX.XXXXXXX
tenleadingtoarelianceonthird-partyannotatorswhoarenot
the domain experts. Hard samples, which are usually the most 1 INTRODUCTION
informativeformodeltraining,tendtobedifficulttolabelaccu-
Conventionaltechniquesfortrainingmachinelearningclassifiers
ratelyandconsistentlywithoutbusinesscontext.Thesecanarise
areresourceintensive.Theyinvolveacyclewheredomainexperts
unpredictablyduringtheannotationprocess,requiringavariable
annotateadataset,whichisthentransferredtodatascientiststo
numberofiterationsandroundsoffeedback,leadingtounforeseen
trainmodels,reviewoutcomes,andmakechanges.Thislabeling
expensesandtimecommitmentstoguaranteequality.
processtendstobetime-consumingandinefficient,oftenhalting
Wepositthatmoredirectinvolvementofdomainexperts,using
afterafewannotationcycles.Consequently,lesseffortisinvested
ahuman-in-the-loopsystem,canresolvemanyofthesepractical
inannotatinglargerdatasetscomparedtoiteratingonmorecom-
challenges.WeproposeanovelframeworkwecallVideoAnnotator
plexmodelsandalgorithmicmethodstoimproveperformanceand
(VA)forannotating,managing,anditeratingonvideoclassification
fixedgecases,asaresultofwhichMLsystemsgrowrapidlyin
datasets. Our approach offers a new paradigm for an end-user-
complexity.Furthermore,constraintsontimeandresourcesoften
centered model development process, enhancing the efficiency,
resultinleveragingthird-partyannotatorsratherthandomainex-
usability,andeffectivenessofvideoclassifiers.Uniquely,VAallows
perts.Theseannotatorsperformthelabelingtaskwithoutadeep
foracontinuousannotationprocess,seamlesslyintegratingdata
understandingofthemodelâ€™sintendeddeploymentorusage,often
collectionandmodeltraining.
makingconsistentlabelingofborderlineorhardexamples,espe-
Weleveragethezero-shotcapabilitiesofvision-languagefounda-
ciallyinmoresubjectivetasks,achallenge.Thisoftennecessitates
tionmodelscombinedwithactivelearningtechniques,anddemon-
multiplereviewroundswithdomainexperts,leadingtounexpected
stratethatVAenablestheefficientcreationofhigh-qualitymodels.
costsanddelays.Thislengthycyclecanalsoresultinmodeldrift,as
VAachievesamedian8.3pointimprovementinAveragePrecision
ittakeslongertofixedgecasesanddeploynewmodels,potentially
relativetothemostcompetitivebaselineacrossawide-ranging
hurtingusefulnessandstakeholdertrustinthesetechnologies.
assortmentoftasks.Wereleaseadatasetwith153klabelsacross56
We suggest that more direct involvement of domain experts,
videounderstandingtasksannotatedbythreeprofessionalvideo
usingahuman-in-the-loopsystem,canresolvemanyoftheseprac-
editorsusingVA,andalsoreleasecodetoreplicateourexperiments
ticalchallengesdescribedabove.Weintroduceanovelframework,
atgithub.com/netflix/videoannotator.
VideoAnnotator(VA),forannotating,managing,anditeratingon
videoclassificationdatasets.Ourinteractiveframeworkemploys
CCSCONCEPTS
activelearningtechniques,toguideuserstofocustheireffortson
â€¢Computingmethodologiesâ†’Neuralnetworks;Activelearn-
progressivelyharderexamples,enhancingthemodelâ€™ssampleeffi-
ingsettings;â€¢Human-centeredcomputingâ†’Interactivesys-
ciencyandkeepingcostslow.Equippedwithsufficientknowledge
temsandtools.
andcontext,theycanrapidlymakeinformeddecisionsonhard
samplesduringtheannotationprocess.Weleveragethezero-shot
KEYWORDS
capabilitiesoflargevision-languagemodelstoaidtheannotation
visionlanguagemodels,activelearning,dataannotationtools,video processandtoserveasbackbonesforlightweightbinaryclassi-
understanding fiers that we train for each label. This design simplifies model
deployment,enablingustoscaletoalargenumberoflabelswith-
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
outsignificantlyincreasingcomplexity.VAseamlesslyintegrates
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation modelbuildingintothedataannotationprocess,facilitatinguser
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe validationofthemodelbeforedeployment,thereforehelpingwith
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
buildingtrustandfosteringasenseofownership.VAsupportsa
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. continuousannotationprocess,allowinguserstorapidlydeploy
KDDâ€™24,August25-29,2024,Barcelona,Spain models,monitortheirqualityinproduction,andswiftlyfixany
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
edgecasesbyannotatingafewmoreexamplesanddeployinganew
ACMISBN978-x-xxxx-xxxx-x/YY/MM
https://doi.org/XXXXXXX.XXXXXXX modelversion.Thisself-servicearchitectureempowersusersto
4202
beF
9
]VC.sc[
1v06560.2042:viXraKDDâ€™24,August25-29,2024,Barcelona,Spain AmirZiaiandAneeshVartakavi
imagedatabases.SimilartoSeeSaw,weuseVLMstoenablesearch.
IncontrasttoSeeSaw,ourapproachfocusesonvideoclassification
forthepurposeofgeneralizationtounseenvideosvs.interactive
searchsessions,andusesactivelearninginsteadofqueryalignment.
2.2 VideoAnnotationTooling
Figure1:Functionalviewofabinaryvideoclassifier.Afew-
Numerouscommercialandopen-sourcetoolsforvideoanalytics
secondclipfrom"OperationVarsityBlues:TheCollegeAd-
andannotationexist[2,39,41,47].Manyoftheseapproachesfocus
missions Scandal" [42] is passed to a binary classifier for
onidentifyingandtrackingspecificobjectswithinavideo[4,29,43],
detectingthe"establishingshots"[35]label.Theclassifier
whileothersfocusoninteractivesearchsessions[17,36,39].In
outputsaveryhighscore,indicatingthatthevideoclipis
contrast,ourapproachfocusesonvideoclassificationforhighly
verylikelyanestablishingshot.
specifictasks.Ourgoalistoconstructmodelsthatcangeneralize
tounseendataandcanbeusedbyeitherend-usersorotheralgo-
makeimprovementswithoutactiveinvolvementofdatascientists rithms,ratherthanfocusingoninteractivesearchsessionsovera
orthird-partyannotators,allowingforfastiteration. fixeddataset.Nevertheless,weoutlinesomeapproachesthatshare
WedesignVAtoassistingranularvideounderstanding[18,26] similaritieswithourwork.
whichrequirestheidentificationofvisuals,concepts,andevents Zelda[39]isavideoanalyticstoolthatemploysVLMstoassist
withinvideosegments.Videounderstandingisfundamentalfor usersinretrievingrelevantanddiverseresultsthroughprompt
numerousapplicationssuchassearchanddiscovery[7,20,24,25], engineeringtechniques.SimilartoZelda,weuseVLMstofacilitate
personalization[5,12,21],andthecreationofpromotionalassets semanticsearchusingnaturallanguage.IncontrasttoZelda,we
[14,21,23].Someformsofvideounderstandingrequiresknowl- leverageactivelearningtoannotatealabeleddatasetthatcaptures
edgeofabroadercontextornarrative,forexample,identifyingthe nuancesthatarehardtoexpresssolelywithpromptengineering.
"incitingincidents"orstoryturningpoints[37].Otherformsare VOCALExplore[17]offersasystemforearlydataexploration
relatively"context-free",forexampledeterminingwhetheraclipis onvideodatasetsusingactivelearning.Bothapproachesharness
setduringthedayornightisindependentoftheotherpartsofthe VLMs,butVOCALExploreusesthemonlyasfeatureextractors,
video.Wefocuson"context-free"videoclassificationinourwork, whileweleverageVLMsasbothfeatureextractorsandtoenable
whichisthetaskofassigningalabeltoanarbitrary-lengthvideo text-videosearch.WhilebothVAandVOCALExploreareinter-
clipwithoutbroadercontext,asillustratedinFig1. activesystemsaimingtominimizelatency,theyadoptdifferent
Insummary,ourmaincontributionsare: strategiestoachievethisgoal.VAemploysapre-processingstep
â€¢ WepresentVideoAnnotator(VA),anovelframeworkto overalargecorpusofvideos,enablingbothtext-to-videosearchand
useslightweightmodelsoveraVLMbackbonetoreducelatency.In
buildvideoclassifiersefficientlyusingpre-trainedvisual-
contrast,VOCALExploreusesaTaskSchedulerandactivelearning
languagemodels,activelearning,anddirectinvolvementof
strategiestoavoidthepre-processingstep.VAislessrestrictive
domainexpertsinthetrainingprocess.
â€¢ WedeployedVAinternallyatNetflixandfoundapplicability andoffersmoreuserchoiceandfreedom,allowinguserstofreely
searchforspecificexamplesthattheyareinterestedinlabeling,
inavarietyofbusinessproblems.Inthiswork,weconduct
whileVOCALExplorerequiresuserstospecifyalabelingbudget
experimentsthatdemonstratethatoursystemenjoysim-
andsuggestsabatchofsamplestoreview.
provedsampleefficiency,visualdiversity,andmodelquality,
Tothebestofourknowledge,VArepresentsthefirst-of-its-kind
witha8.3medianpointimprovementinAveragePrecision
systemforgranularvideoclassification,leveragingVLMstofacili-
(AP)[50]relativetothemostcompetitivebaseline.
â€¢ Wereleaseadatasetwith153klabelsacross56videounder- tatesemanticsearchthroughnaturallanguageandactivelearning
toenhancesampleefficiency.Itoffersauniqueapproachtoan-
standingtasksannotatedbythreeprofessionalvideoeditors
notating,managing,anditeratingonvideoclassificationdatasets,
usingVA,andalsoreleasecodetoreplicateourexperiments.
emphasizingtheimportanceofdirectinvolvementofdomainex-
2 RELATEDWORK pertsinahuman-in-the-loopsystem.
2.1 VisionLanguageModels(VLMs)
3 METHODOLOGY
Self-supervisedmethodsusingconvolutionalortransformer-based
architectureshavedominatedmuchoftheprogressinvideoun- Wedescribeourproposedframework,calledVideoAnnotator(VA),
derstandinginrecentyears[1,9,28,30,32,33,38,46].Specifically, whichisdesignedforbuildingbinaryvideoclassifiersforanexten-
VLMssuchasCLIP[38],demonstrateverygoodzero-shotnatural siblesetoflabels(seeFig2),enablinggranularvideounderstanding.
languageunderstandingperformanceacrossawidevarietyofvi- Thisformulationallowsforimprovingonemodelindependentof
sualtasks[3,15,31,33,45].However,zero-shotmethodsoftenlack theothers.Fig3depictsanoverviewofVA.Oursystemusesvideo
therequisitenuanceandreliabilityinmanyreal-worldapplications (ğ‘’ ğ‘£)andtextencoders(ğ‘’ ğ‘¡)fromaVision-LanguageModel(VLM)to
[44],especiallyforthelongtailofvisualconcepts[36][39]. extractembeddings.Wegatheradiversesetofvideoclips,theem-
SeeSaw[36]tacklesthisproblembyaligningCLIP[38]query beddingsforwhichareextractedusingğ‘’ ğ‘£,andstoredforefficient
vectorsusinguserfeedbackviaaninteractivesystemforsearchover retrieval(e.g.FAISS[27])alongsideothermetadataabouteachclip.VideoAnnotator:Aframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning KDDâ€™24,August25-29,2024,Barcelona,Spain
Thetop-scoringpositiveandnegativefeedsdisplayexamples
withthehighestandlowestscoresrespectively.Ourusersreported
thatthisprovidedavaluableindicationastowhethertheclassifier
haspickedupthecorrectconceptsintheearlystagesoftraining,
since clips that have already been annotated tend to appear in
thesefeeds.Ouruserswerealsoabletospotcaseswhereamodel
waslearningincorrectconceptduetoabiasinthetrainingdata
thattheywereabletosubsequentlyfix.Forexample,ifallpositive
instancesoccurduringthedayandnegativesatnight,themodel
maymistakenlylearntodifferentiatebetweendayandnight,rather
thanaccuratelyidentifyingtheintendedlabel.
Thethirdfeedcontainsexamplesthatthemodelisnotconfident
about.Weuseasimpleuncertaintysamplingmethod,theabsolute
deviationofthemodelscorefrom0.5,whichrequirestheuseof
calibratedmodels[16].Thisfeedhelpswithdiscoveringinteresting
edgecasesandinspirestheneedforlabelingadditionalconcepts.
Forinstance,theannotatornoticesthatthemodelisconfusedabout
animatedcontent,andcanfindandlabeladditionalexamplesusing
visualsearch.
Thefourthfeedconsistsofrandomlyselectedclipswhichhelps
Figure2:Threevideoclipsandthecorrespondingbinaryclas- toannotatediverseexamples.Ourearlyexperimentssuggested
sifierscoresforthreevideounderstandinglabels.Notethat thatonlyfocusingonsearchanduncertaintysamplingmayleadto
theselabelsarenotmutuallyexclusive.Videoclipsarefrom biaseddatasetsthatgeneralizepoorlytounseenclips.
OperationVarsityBlues:TheCollegeAdmissionsScandal
[42],6Underground[10],andLeaveTheWorldBehind[19], 3.1 Metrics
respectively.
VAguidestheannotatorthroughthelabelingprocessbydisplaying
modelqualityanddatadiversityscores.
Userscanbeginbuildingclassifiersafterthispre-processingstepis 3.1.1 Modelqualityscore. Wewanttheclassifiertoreliablysep-
complete. arate positives from negatives. To measure this, we use ğ¾-fold
Usersbeginbyfindinganinitialsetofexamplestobootstrapthe cross-validationandreportthe25thpercentileacrosstheğ¾ Bal-
annotationprocess.Weleveragetext-to-videosearchtoenablethis, ancedAccuracy(BA)[11]scores.WepickedBAbecauseitâ€™snot
usingthetextencoder(ğ‘’ ğ‘¡)ofaVLM.Forexample,anannotator sensitivetoclassimbalanceandisanintuitivemetrictounderstand.
workingonthe"establishingshots"[35]labelmaystarttheprocess Wereportthe25thpercentileinsteadofmean/medianinorderto
bysearchingfor"wideshotsofbuildings".Infilm-making,anestab- bemoreconservativeinourreporting.Weuseğ¾ =5inthiswork.
lishingshotisawideshot(i.e.videoclipbetweentwoconsecutive
3.1.2 Datadiversityscore. Amodelsolelyoptimizedforquality
cuts)ofabuildingoralandscapethatisintendedforestablishing
couldpotentiallyexhibitbiasandfailtogeneralize.Adiversetrain-
thetimeandlocationofthescene.Wealsoallowuserstosearch
ingsetencompassingabroadspectrumofvisualconceptsisneces-
usingotheravailablefields(e.g.clipsfromaspecificmovie).Fig
saryfortheconstructionofrobustmodels.Wethereforeformulated
4illustratesthisexample.Annotatorsareencouragedtolabelas
aheuristicwecallthedatadiversityscoretoguideourusers.
manydiverseexamplesaspossible,andVArequiresatleast10pos-
Thegoalistoaccumulateannotationsfromavarietyofvisual
itiveand10negativeannotationsbeforetheannotatorcanproceed
elements. We identify these elements usingğ‘˜-means clustering
tothenextstage.
overtheembeddingsofalltheclipswithinthecorpus.Wemap
ThenextstageinvolvesaclassicActiveLearning(AL)loop[40].
thepositiveandnegativeannotationsindependentlyontothese
VAbuildsabinaryclassifieroverthevideoembeddings,which
clusters,andcomputetheresultingcounts,cappingittoamaximum
issubsequentlyusedtoscoreallclipsinthecorpus.Wechoose
valueğ‘ .Wethensumallthecounts,andnormalize(i.e.dividingby
lightweightclassifiers(e.g.logisticregression)tominimizelatency,
2Ã—ğ‘˜Ã—ğ‘ ).AnexampleisdepictedinFigure6.Weuseğ‘˜ =ğ‘  =10for
sothatusersonlyhavetowaitforafewsecondsfortheresults.
thisstudy.Thescoresrangefrom0to1.Labelingexamplesfrom
Scoredclipsarepresentedinfourfeeds(i.e.orderedlistofclips)
therandomlyselectedfeedhelpsimprovethisscore,butnotethat
forfurtherannotationandrefinement;top-scoringpositive,top-
itmaybedifficulttoachieveaperfectscore.Intheworstcase,the
scoringnegative,borderline(i.e.lowconfidence),andrandom,as
bestachievablescoreis0.5,whichhappenswhenalabelperfectly
illustrated in Fig 5. The annotator can label additional clips in
alignswithoneoftheclusters.
anyofthefourfeedsandbuildanewclassifier,repeatingtheAL
processforasmanyiterationsasdesired.Wealsoincludeaview
4 DATA
ofallthelabeledexamplessofar,allowinguserstoreviewalltheir
annotationsinasingleplace.VAkeepsahistoryofallannotated WeusedVAtocollect153Kannotationsacross56labelsthatcover
clipsandthecorrespondingclassifiers. avarietyoftasksinvideounderstanding,categorizedinto8groupsKDDâ€™24,August25-29,2024,Barcelona,Spain AmirZiaiandAneeshVartakavi
Figure3:ThemainannotationprocessofVideoAnnotator(VA)involvesthreesteps,illustratedbythegreencomponents.In
step1,theannotatorretrievesvideoclipsfromtextquerieswhichareencodedusingğ‘’ ğ‘¡.InStep2,theclipsarelabeledandused
tobuildaclassifierforfurtherrefinement.Finally,step3isanopportunitytoreviewallannotatedclips.Notethatthisprocess
israrelylinear,andtheannotatorcaneasilynavigatebetweensteps.
Figure4:Step1:Text-to-videosearchtobootstraptheanno-
tationprocess.
Figure5:Step2-ActiveLearning(AL)loop.Theannotator
clicksonbuild,whichinitiatesclassifiertrainingandscoring
asshowninTable1.WeusedtheCondensedMoviesDataset[8], ofallclipsinğ¶.Scoredclipsaredisplayedinfourfeeds.
whichconsistsofcloseto30kmoviescenesbelongingto3.5kmovies
acrossawiderangeofgenres,decades,andcountries.First,weused
PySceneDetect[13]tosegmenteachsceneintoindividualshots.
Then,weextractedshot-levelClip4CLIP(C4C)[33]embeddings embeddingextractionandde-duplicationapproachin[14],which
andusedthosetoremovenear-duplicateshots.Wefollowedthe yieldedafinalsetof500kshotsthatweuseasourvideocorpus.TheVideoAnnotator:Aframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning KDDâ€™24,August25-29,2024,Barcelona,Spain
Group # Labels
Emotions 5 anger,happy,laughter,sad,scared
Events/actions 4 carchase,fight,interview,run
Focus 3 animal,character,object
Genres 6 action, drama, fantasy, horror, ro-
mance,sci-fi
Motion 6 handheld, jump-scare, pan, slow-
motion,timelapse,zoom
Figure6:Datascorecalculationforasimulateddatasetwith
Sensitivities 7 alcohol,drugs,gore,intimacy,nudity,
77annotations(31positiveand46negative)andğ‘˜ =4.Each
smoking,violence
element is capped to a maximum valueğ‘  = 10, summed,
anddividedby2Ã—ğ‘˜Ã—ğ‘  = 80.Inthiscase,thedatascoreis Shottypes 22 aerial, closeup, cowboy-shot, dutch-
(10+5+3+10+0+5+9+10)/(2Ã—4Ã—10)=0.65. angle, establishing-shots, extreme-
close-up,extreme-wide-shot,eye-level,
group-shot, high-angle, insert-shot,
averageshotdurationforthisfinalsetis3.4seconds.The10thand low-angle,medium,over-the-shoulder-
90thpercentileshotdurationsare0.8and6.5secondsrespectively. shot, overhead-shot, point-of-view-
We utilized the skills of three professional video editors, all shot, shutter-shot, single-shot,
ofwhompossessadeepunderstandingoffilmandvideoediting static-shot,tilt-shot,two-shot,wide
techniques.Weconductedaninitialhour-longtrainingsessionon
Time/location 3 day,golden-hour,interior
theusageofVA.Wesharedatextdescriptionandafewrelevant
keywordsforeachlabel,alongwithsamplepositiveandnegative Table1:Annotatedlabelscategorizedinto8groups.Thesec-
clips.Wecollectedthreeannotationsforeachlabel.SinceVAal- ondcolumnisthenumberoflabelsforeachgroup.
lowstheusersthefreedomtochoosewhatclipstoannotate,it
isunlikelythatalltheuserswouldlabelthesameexamples.We
thereforerandomlyselecteda"primaryannotator"foreachlabel
whowouldcompletethetaskfirst.Theremaining"secondaryan- themajorityvoteamongthethreeannotatorsasthegroundtruth
notators"wouldindependentlylabelthesetofexampleslabeled binarylabelforğ´ğ‘ƒ computation.
bytheprimaryannotator.Weinstructedtheprimaryannotators
tolabelatleast100positiveandnegativeannotationseach,and 5.1 Experiment1:SampleEfficiency
stopwhentheyachievedmodelanddatascoresofatleast80,or
WecomparethesampleefficiencyofVAtothefollowingmethods
theyhadannotated1Ktotalexamples.Wefoundhighagreement
byevaluatingperformanceatdifferenttrainingsetsizes(denoted
amongannotatorsformostlabels.Agreement[48],definedasthe
byğ‘›):
percentageofannotationswhereall3annotatorspickedexactly
thesamelabel,hasmin,max,andaverageof62%,99%,and84% â€¢ Method1:Baseline(B):Weuseexpectedğ´ğ‘ƒasthebaseline,
respectively. whichcorrespondstoğ´ğ‘ƒ forrandomranking[49,50].
TobenchmarktheperformanceofVAagainstsomebaselines, â€¢ Method2:ZeroShot(ZS):UsingtheC4C[33]model,we
wealsocollecteddatausingtwootherstrategies:randomsampling usethecosinesimilaritybetweentheembeddingofeachclip
andzero-shotretrieval.Tomaintainconsistency,wehadthesame andthetextrepresentationofthelabel,whichweusefor
threeeditorslabeltheseexamplesindependently.Theyconducted computingAP.
thistaskwithoutusingtheVAinterfaceandskippedthesamples â€¢ Method3:ZeroShotClassifier(ZSC):Wetrainabinary
alreadylabeledusingVA.Fortherandomstrategy,wesampled
classifierusingthetop-ğ‘›annotatedzero-shotclips(seeSec-
1Krandomshotsandhadtheselabeledindependentlyforeach tion4fordetails),sortedindescendingorderbycosinesimi-
label(i.e.thesetissharedacrosslabels).Forzero-shotretrieval,we larity.
employedC4C[33]toextractthetop1Kclipsusingtext-to-video â€¢ Method4:Randomly-selectedClipClassifier(RCC):We
retrieval,usingthelabeltextasthequery.
trainabinaryclassifierusingthefirstğ‘›annotatedrandom
clips(seeSection4fordetails).
5 EXPERIMENTS â€¢ Method5:Combinedclassifier(CC):Weuseabinary
classifiertrainedwith50%ofthedataselectedfromzero-
Thissectiondetailsourexperimentstostudythesampleefficiency
shotandtheother50%fromrandomclips.
ofVAcomparedtobaselines.Foreverylabel,20%ofthelabeleddata,
obtainedthroughuniformsampling,isallocatedfortesting,while Ininstanceswherethenumberofpositivesamplesisextremely
theremainderservesasthetrainingset.Eachexperimentisrepeated limited,randomsamplingorzero-shotretrievalmaynotyieldsuffi-
fivetimes,eachtimewithindependentlysampledbootstrapsofthe cientpositiveornegativeclipsformodeltraining[17].Thisproblem
trainingsetandevaluatedagainstthesametestset.Theevaluation tendstobemorepronouncedforsmallervaluesofğ‘›.Wedetailthis
metricemployedisthemeanAveragePrecision(ğ´ğ‘ƒ)[50].Weused issueinthenextsection.KDDâ€™24,August25-29,2024,Barcelona,Spain AmirZiaiandAneeshVartakavi
Figure7:Percentageoflabelswithatleastğ¾ positiveand Figure8:Modelquality(i.e.AP)asafunctionofğ‘›,forthe
negativelabelsfortrainingaclassifier,asafunctionofğ‘›.
"establishingshots"label.Weobservethatallmethodsout-
Withsmallervaluesofğ‘›,randomsamplingisunlikelyto
performthebaseline,andthatallmethodsbenefitfromad-
retrieveenoughpositiveclipsforlabelswithlowpositive ditionalannotateddata,albeittovaryingdegrees.
rate.
5.1.1 Coverage. Asthesamplesizedecreases,thelikelihoodof
findingadequatepositivesamplestotrainwithalsodrops.Sincewe
useğ¾-foldcross-validationwithğ¾ =5,werequireaminimumof5
positiveand5negativesamples.Fig7illustratesthepercentageof
labelsforwhichaclassifiercanbetrainedasafunctionofsample
size.Forexample,with25samples(i.e.ğ‘›=25),only34%oflabels
yieldenoughpositiveswhenclipsarerandomlyselected.Similarly,
only50%yieldsufficientpositiveandnegativeexampleswhenusing
zero-shotretrieval.
Sincemodelscannotbetrainedforthesecases,wecaneither
imputebyzero,whichwillbringdowntheaggregateperformance
fortheimpactedmethods,orexcludethem.Wechosethelatter,
andexcludelabelswithoutaminimumofğ¾ positiveornegative
annotationsforeachvalueofğ‘›fromfurtheranalysis.NotethatVA,
bydesign,ensurestheselectionofaminimumquantityofpositives
andnegativesbytheannotator.
Figure9:PercentageoflabelswhereVAbeatsothermethods
5.1.2 Modelquality. Inthissection,weexaminemodelquality, as a function ofğ‘›. We see that VA outperforms all other
quantifiedbyAveragePrecision(AP),asafunctionofsamplesize methodsinthemajorityofcasesacrossallvaluesofğ‘›.
ğ‘›.Weillustratethevaluesforthe"establishingshot"labelsinFig8.
WeaggregateperformanceoveralllabelsinFig9.Wefindthatall
methodsoutperformthebaseline,andVAsurpassesothermethods
TheresultsfromthisexperimentsuggestthatVA,inaddition
for a majority of labels across all sample sizes. As discussed in
toenhancedsampleefficiency,offerscontinuousimprovementsin
Section5.1.1,theperformanceofZS,ZSC,andRCCforsmaller
modelqualityasthenumberofsamplesincreases.
valuesofğ‘›areinflatedduetotheremovaloflabelswithinsufficient
positiveandnegativesamples. 5.1.3 Datadiversityscore. AsreferencedinSection3.1.2,VAmoti-
Wealsofindthattheimprovementinmodelqualityvariesacross vatesannotatorstolabelvisuallydiverseclips.Fig10representsthe
labelgroups.Table2detailsthemedianAPgainforVAincom- aggregateddatadiversityscoreasafunctionofğ‘›.WefindthatVA
parisontoCCacrossvaryingğ‘› values,withCCselecteddueto producesdatasetswithhighervisualdiversityrelativetotheother
itssuperiorperformanceamongothermethods.Thegainforeach methods,whichpossiblycontributestotheimprovedgeneralization
labeliscalculatedasfollows:(ğ´ğ‘ƒ VAâˆ’ğ´ğ‘ƒ CC)Ã—100. performancewenotedearlier.VideoAnnotator:Aframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning KDDâ€™24,August25-29,2024,Barcelona,Spain
Group n=25 n=50 n=100 n=1000 user,givingthemthefreedomtochoosewhatsamplestolabelnext.
However,wehypothesizedthatthisfreedomcouldbepreserved
Emotions -6.0 -5.1 -4.2 3.7
whilealsoassistingusersbyintermittentlyrecommendingabatch
Events/actions 6.9 5.5 5.7 3.8 ofexamplesforannotation.Totestthishypothesis,wedesignedan
experimentusingamulti-armedbanditformulation,whichrecom-
Focus 1.2 -1.2 -0.4 1.8
mendedoneofthreeactionstotheuser:theVAprocess,annotating
Genres 6.6 5.3 3.9 1.8 randomsamples,orannotatingzero-shotsamples.
Motion 31.5 1.2 5.6 3.0
Eachsourceisdividedintobatchesoffixedsizeğ‘‘.Ateachstep
ğ‘¡,analgorithmdeterminesonesourceforannotation.Weemulate
Sensitivities 0.5 -0.3 0.8 3.4 thepresentationofabatchofğ‘‘ clipstoanannotatorandrecord
Shottypes 1.2 0.6 0.5 2.3 theabsolutegaininmodelqualitycomparedtothepreviousstep.
Ateachtimestepğ‘¡,thefollowingoccur:
Time/location 4.6 -8.5 1.3 1.4
(1) Selection:Thealgorithmpicksasourceor"action",denoted
Overall 1.2 0.4 1.5 2.9
asğ‘–.Thechoiceofsourcedependsonthebanditalgorithm
Table2:ModelqualitygainforusingVArelativetoCombined
andthescoreassociatedwitheachsource,whichwedenote
Classifier (CC). For each group, we take the median gain asğ‘  ğ‘–.
acrossalllabelsbelongingtothegroupandaspecificvalue
(2) Action:Weaddthenextbatchfromthechosensourceto
ofğ‘›.Thelastrowrepresentsthemedianacrossalllabels.
thedatafromthepreviousstep,whichwedenoteasğ· ğ‘¡âˆ’1.
Thisresultsinanewdataset,ğ· ğ‘¡,onwhichwetrainamodel
(usingthesamesetupasinthepreviousexperiment).We
thenrecordthemediantestAveragePrecision(AP)asğ‘£ ğ‘¡.
(3) Update:Weupdatetheper-sourcescoreforthechosensource,
settingğ‘  ğ‘– equaltothedifferencebetweenthecurrentğ‘£ ğ‘¡ and
themostrecentvalueğ‘£ ğ‘¡âˆ’1.Wealsoranexperimentsusing
simple and exponentially-weighted averages over the se-
quenceofscoresforeachsource,butfoundthattheywere
inferiortousingthemostrecentvalue.
Wedeterminetheinitialscoreforeachsource,ğ‘  ğ‘–,byevaluating
thefirstbatchforeachsourceindependently.Forthefirsttimestep
ğ‘¡ = 1,wesetğ· 0 totheconcatenationofthefirstbatchfromall
threesources.AnillustrationofthisprocesscanbefoundinFig11.
5.2.1 Algorithms. Weimplementthefollowingalgorithms:
(1) Roundrobin:Servingasabaseline,thisalgorithmsimply
cyclesthroughthethreeactionswithoutreferringtoğ‘  ğ‘–.
(2) ğœ–-greedy:Withaprobabilityofğœ–,thisalgorithmrandomly
selectsanaction,andwithaprobabilityof1âˆ’ğœ–,itgreedily
selectsthebestaction(i.e.,highestğ‘  ğ‘–).Weexploredafew
Figure 10: Mean and standard deviation of the data score valuesofğœ– âˆˆ [0.1,0.5]andfoundthatğœ– =0.25worksbest.
for different methods as a function ofğ‘›. Upper bound is (3) UpperConfidenceBound(UCB)[6]:Thisalgorithmchooses
themaximumpossiblescoreateachvalueğ‘›.Datascorefor thesourcewiththehighestvalueofğ‘  ğ‘– +ğ‘âˆšï¸ lnğ‘/ğ‘› ğ‘–,where
allmethodsexceptforVAarecomputedatfixedvaluesğ‘›âˆˆ ğ‘ isthetotalnumberofstepsexecutedsofar,andğ‘› ğ‘– isthe
{25,50,100,500,1000}. numberoftimesarmğ‘–hasbeenselectedbeforethecurrent
step.Wetestedvariousğ‘ âˆˆ [10âˆ’3,1]andfoundğ‘ =10âˆ’2to
performthebest.
5.2 Experiment2:annotationcandidatesource (4) Greedyoracle:Intendedasagreedyupperbound,thisalgo-
selection rithmevaluatesallthreeactionsateachstepandselectsthe
oneyieldingthehighestğ‘  ğ‘–.Notethatthisisanoraclemethod
VAoffersannotatorstheflexibilitytochoosethesourceofcandi- andaprioriknowledgeofğ‘  ğ‘– isnotpossibleinpractice.
datesforannotation,providingoptionsbetweensearchandany
ofthefourfeeds(asdiscussedinSection3).Asdemonstratedin 5.2.2 Experimentalsetup. ThedesignofVAenablesuserstohalt
Experiment1,thisstrategytypicallyyieldssuperiormodelquality thelabelingprocessoncecertainminimumconditionsaresatisfied
anddatadiversityscores. (seeSection3).Weusedthefinalsetofannotationsandthecorre-
Nevertheless,anecdotalfeedbacksuggestedthatourusers,while spondingmodelforeachlabel(i.e.thelargestvalueofğ‘›),which
appreciatingtheVAworkflowanduserexperience,weresome- variesacrosslabels.
timesuncertainabouttheoptimalactiontotakefollowingamodel Weexperimentedwithvariousvaluesofbatchsizeğ‘‘ andset
retrainingstep.VAdoesnâ€™timposeanyspecificdirectiononthe ittoğ‘‘ =25forthisexperiment,whichresultedinamaximumofKDDâ€™24,August25-29,2024,Barcelona,Spain AmirZiaiandAneeshVartakavi
6 LIMITATIONSANDFUTUREWORK
WeintroducedVideoAnnotator(VA)asaframeworkforefficiently
constructingbinaryvideoclassifiers.Thereisavastspaceofdesign
decisionsthatcanbeexploredinfuturework.
Atahighlevel,theframeworkâ€™sapplicabilityextendsbeyond
videotoothermediaassetssuchasaudio,images,text,and3D
objects.Itcanleverageadvancementsinjointembeddingsacross
multiplemodalities,suchasImageBind[22].Whileweprimarily
exploredcontext-freelabelsinthiswork,wecouldalsoextendthis
tolong-formvideobyincorporatingcontextualencoders.
Wecantrytoimprovemodelperformanceinmanyways,for
exampleasuperiorvideoencodercouldimprovebothsampleeffi-
ciencyandthefinalmodelâ€™squality.Leveragingmultimodalrepre-
sentations(e.g.,audioandsubtitles)mightenhanceperformancefor
certaintasks,andadvancedactivelearningstrategiescouldfurther
augmentsampleefficiency.
Intermsofuserexperience,VAcouldbeimprovedbyrefining
metrics(modelanddatascores),proactivelyidentifyingpotential
issues(e.g.wherethemodelâ€™spredictionandtheuserslabeldonot
Figure11:Wefirstdividethedataforeachsourceintobatches agree),andsuggestingadditionalsearchterms[34].Moreadvanced
ofsizeğ‘‘,illustratedhereasVAğ‘–,ZSğ‘– (zeroshot),andğ‘… ğ‘– (ran- searches,suchastextualcombinedwithmetadata(e.g.,genre,year),
dom).Atğ‘¡ =0,weindependentlyevaluatethefirstbatchfor orvideo-to-videosearches(i.e.,identifyingvideossimilartoaref-
eachsourceandcomputeğ‘  ğ‘–.Atğ‘¡ =1,thealgorithmchooses erencevideo)couldalsoprovidebenefits.
VA,promptingustoconcatenatethefirstthreebatcheswith
anewVAbatchofsizeğ‘‘ (i.e.,VA2).Atğ‘¡ = 2,thealgorithm
selectsğ‘…,whichleadstoappendingğ‘… 2.
7 CONCLUSION
WepresentedVideoAnnotator(VA),anovel,interactiveframework
40stepsandachievedthehighestperformance.Uponreplicating thataddressesmanychallengesassociatedwithconventionaltech-
theseexperimentswithalargerbatchsizeofğ‘‘ =50,weobserved niquesfortrainingmachinelearningclassifiers.VAleveragesthe
verysimilaroutcomes.However,bothlarger(ğ‘‘ â‰¥100)andsmaller zero-shotcapabilitiesoflargevision-languagemodelsandactive
(ğ‘‘ â‰¤10)batchsizesledtoadeclineinperformance. learningtechniquestoenhancesampleefficiencyandreducecosts.
Itoffersauniqueapproachtoannotating,managing,anditerating
5.2.3 Results. Foreachlabel,wecalculatetheğ´ğ‘ƒ gain(similarto
onvideoclassificationdatasets,emphasizingthedirectinvolvement
Section5.1.2)comparedtoVAwithoutanalgorithmforcandidate
ofdomainexpertsinahuman-in-the-loopsystem.Byenablingthese
sourceselectionasthebaseline.Table3summarizesthedistribution
userstorapidlymakeinformeddecisionsonhardsamplesduring
foreachalgorithm.Wefindthatthemedianğ´ğ‘ƒ gainispositive
theannotationprocess,VAincreasesthesystemâ€™soverallefficiency.
forallalgorithms.UCBachievesthehighestperformanceof3.4
Moreover,itallowsforacontinuousannotationprocess,allowing
points(excludingthegreedyoracle),resultinginacumulativeme-
userstoswiftlydeploymodels,monitortheirqualityinproduction,
dianimprovementof8.3pointsoverthemostcompetitivebaseline
andrapidlyfixanyedgecases.Thisself-servicearchitectureem-
methodCC(seeSection5.1).Interestingly,evenabasicroundrobin
powersdomainexpertstomakeimprovementswithouttheactive
approachcanimproveperformanceoverthebaseline.
involvementofdatascientistsorthird-partyannotators,andfosters
asenseofownership,therebybuildingtrustinthesystem.
Method ğ‘ 10 ğ‘ 25 ğ‘ 50 ğ‘ 75 ğ‘ 90 WeconductedexperimentstostudytheperformanceofVA,and
foundthatityieldsamedian8.3pointimprovementinAverage
Roundrobin -5.9 -0.3 2.5 7.4 15.1
Precisionrelativetothemostcompetitivebaselineacrossawide-
ğœ–-greedy -6.5 -0.6 2.9 7.6 16.7 ranging assortment of video understanding tasks. We release a
datasetwith153klabelsacross56videounderstandingtasksanno-
UCB -7.5 -0.2 3.4 9.2 15.9
tatedbythreeprofessionalvideoeditorsusingVA,andalsorelease
Greedyoracle -5.4 0.9 3.9 9.6 16.2
codetoreplicateourexperiments.
Table3:Distributionofmodelqualitygainwhenusingsource
selectionrelativetoVAwithoutanalgorithmforcandidate
sourceselection.Columnğ‘ ğ‘›representstheğ‘›-thpercentile,
8 ACKNOWLEDGEMENTS
whereğ‘ 50isthemedian.
Weâ€™dliketothankKelliGriggs,EugeneLok,YvonneJukes,Anna
Pulido,AlexAlonso,andMadelineRileyforvaluablediscussions
andfeedback.VideoAnnotator:Aframeworkforefficientlybuildingvideoclassifiersusingvision-languagemodelsandactivelearning KDDâ€™24,August25-29,2024,Barcelona,Spain
REFERENCES
[28] SalmanKhan,MuzammalNaseer,MunawarHayat,SyedWaqasZamir,Fa-
[1] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,Yana hadShahbazKhan,andMubarakShah.2022.Transformersinvision:Asurvey.
Hasson,KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. ACMcomputingsurveys(CSUR)54,10s(2022),1â€“41.
2022. Flamingo:avisuallanguagemodelforfew-shotlearning. Advancesin [29] JoosungKim,Ryu-HyeokGwon,Jin-TakPark,HakilKim,andYoo-SungKim.
NeuralInformationProcessingSystems35(2022),23716â€“23736. 2013. Asemi-automaticvideoannotationtooltogenerategroundtruthfor
[2] AreenAlsaidandJohnDLee.2022. TheDataScope:Amixed-initiativearchi- intelligentvideosurveillancesystems.InProceedingsofInternationalConference
tecturefordatalabeling.InProceedingsoftheHumanFactorsandErgonomics onAdvancesinMobileComputing&Multimedia.509â€“513.
SocietyAnnualMeeting,Vol.66.SAGEPublicationsSageCA:LosAngeles,CA, [30] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi.2022.Blip:Bootstrapping
1559â€“1563. language-imagepre-trainingforunifiedvision-languageunderstandingand
[3] AnuragArnab,MostafaDehghani,GeorgHeigold,ChenSun,MarioLuÄiÄ‡,and generation.InInternationalConferenceonMachineLearning.PMLR,12888â€“12900.
CordeliaSchmid.2021.Vivit:Avideovisiontransformer.InProceedingsofthe [31] KunchangLi,YaliWang,YizhuoLi,YiWang,YinanHe,LiminWang,andYu
IEEE/CVFinternationalconferenceoncomputervision.6836â€“6846. Qiao.2023.UnmaskedTeacher:TowardsTraining-EfficientVideoFoundation
[4] KithmiAshangani,KUWickramasinghe,DWNDeSilva,VMGamwara,Anupiya Models. arXiv:2303.16058[cs.CV]
Nugaliyadde,andYashasMallawarachchi.2016.Semanticvideosearchbyauto- [32] ZeLiu,JiaNing,YueCao,YixuanWei,ZhengZhang,StephenLin,andHan
maticvideoannotationusingTensorFlow.In2016Manufacturing&Industrial Hu.2022.Videoswintransformer.InProceedingsoftheIEEE/CVFconferenceon
EngineeringSymposium(MIES).IEEE,1â€“4. computervisionandpatternrecognition.3202â€“3211.
[5] JustinBasilico-TonyJebaraAshokChandrashekar,FernandoAmat.2017.Artwork [33] HuaishaoLuo,LeiJi,MingZhong,YangChen,WenLei,NanDuan,andTianrui
PersonalizationatNetflix.NetflixTechBlog(2017). Li.2022.Clip4clip:Anempiricalstudyofclipforendtoendvideoclipretrieval
[6] PeterAuer.2002.Usingconfidenceboundsforexploitation-explorationtrade-offs. andcaptioning.Neurocomputing508(2022),293â€“304.
JournalofMachineLearningResearch3,Nov(2002),397â€“422. [34] MayugManiparambil,ChrisVorster,DerekMolloy,NoelMurphy,KevinMcGuin-
[7] HosseinTaghaviAvneeshSaluja,AndyYao.2023.DetectingSceneChangesin ness,andNoelEOâ€™Connor.2023.Enhancingclipwithgpt-4:Harnessingvisual
AudiovisualContent.Medium(2023). descriptionsasprompts.InProceedingsoftheIEEE/CVFInternationalConference
[8] MaxBain,ArshaNagrani,AndrewBrown,andAndrewZisserman.2020.Con- onComputerVision.262â€“271.
densedmovies:Storybasedretrievalwithcontextualembeddings.InProceedings [35] AMittalandL-FCheong.2007. Detectingestablishmentshotsusingcamera
oftheAsianConferenceonComputerVision. motion.InternationalJournalofComputersandApplications29,3(2007),232â€“238.
[9] HangboBao,LiDong,SonghaoPiao,andFuruWei.2021.Beit:Bertpre-training [36] OscarMoll,ManuelFavela,SamuelMadden,VijayGadepally,andMichaelCa-
ofimagetransformers.arXivpreprintarXiv:2106.08254(2021). farella.2023.SeeSaw:interactivead-hocsearchoverimagedatabases.Proceedings
[10] MichaelBay.2019.6Underground.Netflix(2019). oftheACMonManagementofData1,4(2023),1â€“26.
[11] KayHenningBrodersen,ChengSoonOng,KlaasEnnoStephan,andJoachimM [37] PinelopiPapalampidi,FrankKeller,andMirellaLapata.2019.Movieplotanalysis
Buhmann.2010.Thebalancedaccuracyanditsposteriordistribution.In2010 viaturningpointidentification.arXivpreprintarXiv:1908.10328(2019).
20thinternationalconferenceonpatternrecognition.IEEE,3121â€“3124. [38] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,
[12] LeticiaKwokBruceWobbe.2023.TheNextStepinPersonalization:Dynamic SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,
Sizzles.NetflixTechBlog(2023). etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
[13] BrandonCastellano.[n.d.].Breakthrough/pyscenedetect:Pythonandopencv- InInternationalconferenceonmachinelearning.PMLR,8748â€“8763.
basedscenecut/transitiondetectionprogramandlibrary. https://github.com/ [39] FranciscoRomero,CalebWinston,JohannHauswald,MateiZaharia,andChristos
Breakthrough/PySceneDetect Kozyrakis.2023.Zelda:VideoAnalyticsusingVision-LanguageModels.arXiv
[14] BorisChen,AmirZiai,RebeccaSTucker,andYuchenXie.2023.MatchCutting: preprintarXiv:2305.03785(2023).
FindingCutswithSmoothVisualTransitions.InProceedingsoftheIEEE/CVF [40] BurrSettles.2009.Activelearningliteraturesurvey.(2009).
WinterConferenceonApplicationsofComputerVision.2115â€“2125. [41] SneheshShrestha,WilliamSentosatio,HuiashuPeng,CorneliaFermuller,and
[15] SihanChen,XingjianHe,LongtengGuo,XinxinZhu,WeiningWang,JinhuiTang, YiannisAloimonos.2023.FEVA:FastEventVideoAnnotationTool.arXivpreprint
andJingLiu.2023.Valor:Vision-audio-languageomni-perceptionpretraining arXiv:2301.00482(2023).
modelanddataset.arXivpreprintarXiv:2304.08345(2023). [42] ChrisSmith.2021. OperationVarsityBlues:TheCollegeAdmissionsScandal.
[16] IraCohenandMoisesGoldszmidt.2004.Propertiesandbenefitsofcalibrated Netflix(2021).
classifiers.InEuropeanconferenceonprinciplesofdataminingandknowledge [43] LubomirStanchev,HansonEgbert,andBenjaminRuttenberg.2020.Automating
discovery.Springer,125â€“136. deep-seavideoannotationusingmachinelearning.In2020IEEE14thInternational
[17] MaureenDaum,EnhaoZhang,DongHe,StephenMussmann,BrandonHaynes, ConferenceonSemanticComputing(ICSC).IEEE,17â€“24.
RanjayKrishna,andMagdalenaBalazinska.2023.VOCALExplore:Pay-as-You- [44] JiaqiWang,ZhengliangLiu,LinZhao,ZihaoWu,ChongMa,SigangYu,Haixing
GoVideoDataExplorationandModelBuilding.arXivpreprintarXiv:2303.04068 Dai,QiushiYang,YihengLiu,SongyaoZhang,etal.2023.Reviewoflargevision
(2023). modelsandvisualpromptengineering.Meta-Radiology(2023),100047.
[18] AliDiba,MohsenFayyaz,VivekSharma,ManoharPaluri,JÃ¼rgenGall,Rainer [45] HongweiXue,YuchongSun,BeiLiu,JianlongFu,RuihuaSong,HouqiangLi,
Stiefelhagen,andLucVanGool.2020.Largescaleholisticvideounderstanding. andJieboLuo.2022.Clip-vip:Adaptingpre-trainedimage-textmodeltovideo-
InComputerVisionâ€“ECCV2020:16thEuropeanConference,Glasgow,UK,August languagerepresentationalignment.arXivpreprintarXiv:2209.06430(2022).
23â€“28,2020,Proceedings,PartV16.Springer,593â€“610. [46] PengchuanZhang,XiujunLi,XiaoweiHu,JianweiYang,LeiZhang,LijuanWang,
[19] SamEsmail.2023.LeaveTheWorldBehind.Netflix(2023). YejinChoi,andJianfengGao.2021.Vinvl:Revisitingvisualrepresentationsin
[20] BorisChenetal.2023.BuildingIn-VideoSearch.NetflixTechBlog(2023). vision-languagemodels.InProceedingsoftheIEEE/CVFconferenceoncomputer
[21] ViIyengaret.al.2022.NewSeries:CreatingMediawithMachineLearning.Netflix visionandpatternrecognition.5579â€“5588.
TechBlog(2022). https://netflixtechblog.com/new-series-creating-media-with- [47] ShikunZhang,OmidJafari,andParthNagarkar.2021. Asurveyonmachine
machine-learning-5067ac110bcd learningtechniquesforautolabelingofvideo,audio,andtextdata.arXivpreprint
[22] RohitGirdhar,AlaaeldinEl-Nouby,ZhuangLiu,MannatSingh,KalyanVasudev arXiv:2109.03784(2021).
Alwala,ArmandJoulin,andIshanMisra.2023.ImageBind:OneEmbeddingSpace [48] AmirZiai.2017.Inter-rateragreementKappas.TowardsDataScience(2017).
ToBindThemAll.InCVPR. [49] AmirZiai.2023.Rankingmetricsfromfirstprinciples.Medium(2023).
[23] JulijaBagdonaite-CristinaSegalinViIyengarGraceTang,AneeshVartakavi. [50] AmirZiai.2023.Rankingmetricsfromfirstprinciples:Averageprecision.Medium
2023.DiscoveringCreativeInsightsinPromotionalArtwork.NetflixTechBlog (2023).
(2023).
[24] JonathanSolÃ³rzano-HamiltonKelliGriggsViIyengarGuruTahasildar,AmirZiai.
2023. BuildingaMediaUnderstandingPlatformforMLInnovations. Netflix
TechBlog(2023).
[25] TiffanyLowHamidShahid,LauraJohnson.2023.AVADiscoveryView:Surfacing
AuthenticMoments.NetflixTechBlog(2023).
[26] De-An Huang, Vignesh Ramanathan, Dhruv Mahajan, Lorenzo Torresani,
ManoharPaluri,LiFei-Fei,andJuanCarlosNiebles.2018.Whatmakesavideo
avideo:Analyzingtemporalinformationinvideounderstandingmodelsand
datasets.InProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition.7366â€“7375.
[27] JeffJohnson,MatthijsDouze,andHervÃ©JÃ©gou.2019. Billion-scalesimilarity
searchwithgpus.IEEETransactionsonBigData7,3(2019),535â€“547.