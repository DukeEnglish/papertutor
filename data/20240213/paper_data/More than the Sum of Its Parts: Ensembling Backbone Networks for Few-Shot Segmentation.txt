More than the Sum of Its Parts: Ensembling
Backbone Networks for Few-Shot Segmentation
Nico Catalano‚àó, Alessandro Maranelli‚Ä†, Agnese Chiatti‚àó, Matteo Matteucci‚àó
Department of Electronics, Information and Bioengineering
Politecnico di Milano
‚àó{name.surname}@polimi.it
‚Ä† {name.surname}@mail.polimi.it
Abstract‚ÄîSemantic segmentation is a key prerequisite to Deep Learning (DL) models have been introduced, including
robust image understanding for applications in Artificial In- U-Net [10], Mask R-CNN [11], and PSPNet [12], that exhibit
telligence and Robotics. Few Shot Segmentation, in particular,
an impressive performance on popular benchmark datasets in
concerns the extension and optimization of traditional segmen-
ComputerVision[2],[13]‚Äì[15].However,thesemethodsshare
tation methods in challenging conditions where limited training
examples are available. A predominant approach in Few Shot the significant drawback of relying on large-scale training
Segmentation is to rely on a single backbone for visual feature datasets that are expensive to curate. This characteristic of
extraction. Choosing which backbone to leverage is a deciding traditional DL methods for semantic segmentation drastically
factor contributing to the overall performance. In this work, we
limitstheirapplicabilityinscenariosofdatascarcity,aswellas
interrogate on whether fusing features from different backbones
theirabilitytogeneralizebeyondthetrainingdatadistribution.
can improve the ability of Few Shot Segmentation models to
capturerichervisualfeatures.Totacklethisquestion,wepropose Toaddressthislimitationandfacilitatethewidespreadadop-
and compare two ensembling techniques‚ÄîIndependent Voting tion of segmentation models in domain-specific applications,
andFeatureFusion.AmongtheavailableFewShotSegmentation thefieldofFewShotSegmentation(FSS)hasemerged.Inthe
methods, we implement the proposed ensembling techniques
FSS framework, the objective is to design a model that can
on PANet. The module dedicated to predicting segmentation
learn from limited training examples to accurately segment
masksfromthebackboneembeddingsinPANetavoidstrainable
parameters, creating a controlled ‚Äòin vitro‚Äô setting for isolating novel classes as soon as these classes are first observed.
the impact of different ensembling strategies. Leveraging the Typically, this involves providing the model with as few as
complementary strengths of different backbones, our approach one to five labeled examples.
outperformstheoriginalsingle-backbonePANetacrossstandard
A proven strategy for implementing FSS involves fine-
benchmarks even in challenging one-shot learning scenarios.
tuning backbone architectures that have been pre-trained on
Specifically, it achieved a performance improvement of +7.37%
on PASCAL-5i and of +10.68% on COCO-20i in the top- large-scale, general-purpose datasets, leveraging the benefits
performingscenariowherethreebackbonesarecombined.These of transfer learning. This strategy capitalizes on the diverse
results, together with the qualitative inspection of the predicted and informative features implicitly learned by the backbone
subject masks, suggest that relying on multiple backbones in
duringpre-trainingonageneraldataset.Acommonpracticeis
PANetleadstoamorecomprehensivefeaturerepresentation,thus
thentocapitaliseonthericherfeatureslearnedonlarger-scale
expediting the successful application of Few Shot Segmentation
methods in challenging, data-scarce environments. setstoadaptthemodeltoatargetdomain-specificrepresented
Index Terms‚ÄîComputer Vision, Semantic Segmentation, Few by a few training examples.
Shot Segmentation, Ensembling In this context, choosing a specific backbone can sig-
nificantly influence the final performance. Widely adopted
I. INTRODUCTION
backbones, such as VGG [16], ResNet [17] and MobileNet
Efficient and robust image understanding is a crucial miss- [18] are each characterised by their distinct design and, con-
ing capability in Artificial Intelligence (AI) and Robotics sequently, provide embeddings that represent different feature
that supports key tasks ranging from autonomous driving [1], sets. To underscore the impact of backbone selection on the
[2] to precision agriculture [3]‚Äì[5] and clinical analysis [6], overall performance of the model, a common practice in the
to name just a few. One key prerequisite to robust image FSS community is to compare the performance of different
understanding is the semantic segmentation problem, which backbones. However, the impact of backbone selection on the
involvespredictingcategorylabelsatthepixellevelinagiven finalperformancehasnotyetbeenfullystudied.Inthispaper,
image [7]‚Äì[9]. Following recent AI advancements, various we build on the intuition that embeddings ensembled from
multiple backbones can capture a more comprehensive and
This study was conducted within the Agritech National Research Center
descriptive set of image features than those extracted from a
andreceivedfundingfromtheEuropeanUnionNext-GenerationEU(PIANO
NAZIONALEDIRIPRESAERESILIENZA(PNRR)‚ÄìMISSIONE4COM- single backbone. The underlying expectation is that adopting
PONENTE2,INVESTIMENTO1.4‚ÄìD.D.103217/06/2022,CN00000022). ensemblingstrategieswillimprovetheperformanceofamodel
This manuscript reflects only the authors‚Äô views and opinions, neither the
on FSS.
EuropeanUnionnortheEuropeanCommissioncanbeconsideredresponsible
forthem. To test this hypothesis, we focused on the PANet [19] FSS
4202
beF
9
]VC.sc[
1v18560.2042:viXramodel. In PANet, segmentation masks are directly predicted fundamentals of ensembling techniques, laying the ground for
fromthebackboneembeddingswithoutintroducinganytrain- amoredetailedexplorationoftheirroleinthecontextofFSS.
able parameters. As such, PANet is an ideal candidate for
A. Few Shot Segmentation
conductingan‚Äòinvitro‚Äôexperiment.Moreover,becausePANet
is not dependent on mask prediction parameters, it provides Numerous studies in the literature [21]‚Äì[28] frame the Few
a fully modular solution for the integration and evaluation Shot Segmentation (FSS) problem as one of predicting the
of multiple backbones. That is, results obtained with this region mask MÀÜ q of a subject class l in a query image I q,
setup can be more easily abstracted and extended to different given a support set S composed of k image-mask pairs. In
architectures and tasks. Another factor we control for in our this context, for a semantic class l, the support set
experimentsistheimpactofthedatasetchosenforpre-training S(l)={(Ii,Mi)}k
l i=1
the different backbones. Namely, we will rely on backbones
that have all been pre-trained on ImageNet [20]. isthecollectionofkimage-maskpairsthatdescribesthenovel
These methodological choices allow us to systematically class l. On the other hand, the query image I q is the image
evaluate the effects of leveraging embeddings extracted from on which the model will predict the segmentation mask MÀÜ q
different backbones and ensembled with different policies of the class l. Then, the learning objective of the FSS model
while removing the effects of both the mask prediction stage is the function f Œ∏
andthedatasetusedforpre-trainingeachbackbone.Therefore, MÀÜ =f (I ,S(l)),
q Œ∏ q
resultsobtainedinthisexperimentalsetupcanonlybeascribed
to the introduction of ensembling strategies. whichpredictsthebinarymaskMÀÜ q forthesemanticclassl in
Additionally, we focus on the more challenging scenario thequeryimageI q describedbythek elementsinthesupport
whereonlyoneexampleisprovidedtotheFSSmodel,putting setS(l).Inthiswork,wespecificallyfocusonthecasewhere
even more emphasis on the ability of the model to generalize only one support example is available (k = 1), also known
andadapteffectivelyinalow-datasetting.Combiningmultiple as one-shot semantic segmentation. This scenario represents a
backbonesin thesame pipelinemakes particularsense inFSS particularinstanceoftheFSSproblem.Thus,thekeyconcepts
and one-shot learning scenarios. On the one hand, adding and definitions introduced also hold in this case.
multiple backbones also increases the number of training A prevalent approach in the FSS field is to adopt a meta-
parameters. However, because FSS methods are trained to learning framework known as episodic training, which was
generalize to unseen classes, once trained, the model will originally proposed by Vinyals et al. [29] in the context of
adapt to an unseen class without requiring additional training, one-shot semantic segmentation scenarios. Episodic training,
differently from traditional fully-supervised methods. as the name suggests, concerns feeding the learning model
In sum, in this paper we make the following contributions: withasequenceof‚Äúepisodes‚Äùineachofwhichthemodelhas
to learn a new class. Specifically, in each training episode, a
‚Ä¢ wepresentwhatis,tothebestofourknowledge,thefirst
label class l is first sampled from the set of training classes
study of ensembling features learned through different
L . Then, the model is presented with: i) a support set S
backbones for Few Shot Segmentation. train
of images and mask pairs where only the l class is labelled,
‚Ä¢ we devise a series of controlled experiments to disentan-
and ii) a query image I with its corresponding ground truth
gle the performance effects exclusively related to ensem- q
maskM .Ineachepisode,thetrainingobjectiveisminimizing
bling from other contributing effects, namely the impact q
the loss between the predicted mask MÀÜ and the ground truth
of the mask prediction module training and the data q
mask M . Similarly, the model performance can be assessed
chosen for pre-training the backbones. This experimental q
through a series of meta-testing episodes, in which subject
design ultimately facilitates the abstraction of general
classesareselectedfromL ,whichcontainsonlyexamples
findings (i.e., applicable to different tasks, models, and test
unseen at training time.
domains) from the individual experiments presented in
To train and compare FSS models via episodic training,
this paper.
researchers commonly resort to the PASCAL-5i and COCO-
‚Ä¢ we demonstrate that ensembling multiple backbones can
20i datasets. These datasets are derived from the well-known
drastically improve the Few Shot Segmentation perfor-
PASCAL VOC 2012 [13] and MS COCO [14] collections of
mance on popular benchmarks datasets, improving up to
natural images. While the former set includes 20 classes, the
+7.37% on PASCAL-5i and up +10.68% on COCO-20i
latter one covers a wider set of 80 subject classes.
in terms of average mIoU.
PASCAL-5i and COCO-20i are organized into four folds
so that the label set L of all subject classes in each original
II. BACKGROUND
datasetispartitionedintofoursubsets.Foreachsplit,theunion
Before exploring the application of ensembling techniques of three label subsets will form the L set used to sample
train
for the FSS problem, in this section we thoroughly define the training episodes, while the remaining subset will serve as
FSS task, as well as the main background concepts related to L and is thus devoted to sampling testing episodes. In
test
this task. Concurrently, we illustrate how FSS is approached this way are constructed four folds, each one with different
in the PANet architecture. Subsequently, we delve into the L .ThetwodatasetsPASCAL-5i andCOCO-20i arewidely
testadopted in the literature, and their division into four folds Finally, the predicted segmentation mask is obtained by se-
has become a standard practice. Given their prevalence, it is lecting the class index of highest probability for each spatial
customary to report performance metrics for each of the four location:
foldsindividuallyandthenprovideanaverageacrossthefolds MÀÜ(x,y) =argmaxMÀú(x,y). (4)
q q;j
j
to summarize the results. In line with this common practice,
we follow the same approach, as detailed in Section V and C. Bayesian Voting
exemplified in Tables I and II. Among ensembling methods, Bayesian Voting [30] is a
techniquethatleveragesaprobabilisticframeworktocombine
B. PANet
the predictions of multiple base classifiers. It builds on the
PANet[19]addressestheFSStaskthroughametriclearning
premise of modelling the classification problem through a
approach, where class-specific prototypes are derived from
Bayesian perspective.
the embeddings of images in the support set. Namely, in
GivenadatasetX withN samplesandcorrespondinglabels
eachepisode,referenceembeddingsforthesupportandquery
y, let h (x),h (x),...,h (x) represent the predictions of B
1 2 B
images are extracted from a shared backbone network. Subse-
base classifiers on an input instance x. Each base classifier
quently,maskedaveragepooling[23]isappliedtothesupport
provides a probability distribution over the possible classes
set embeddings and corresponding masks, yielding a compact
forx.TheBayesianVotingprocesscombinestheseprobability
prototypicalrepresentationofthenovelclass.Thequeryimage
distributions to derive the final class probabilities for x. The
is projected into the same feature space as the generated
final predicted class label is often determined by selecting the
embeddings. Lastly, image segmentation is performed by
class with the highest probability:
matchingembeddingswiththelearnedprototypesateachpixel
location. An overview of the inference process of PANet is B
(cid:88)
depicted in Fig.1. P(y|x)= c bP b(y|x), (5)
In mathematical terms, given a support set S l = b=1
{(I l,k,M l,k)} as input, PANet first computes a feature map whereP b(y|x)istheprobabilitydistributiongivenbytheb-
F l,k for the image I l,k. Here, l = 1,...,L indexes the thbaseclassifierforclassy oninputx,c b isthemultiplicative
label class from the set C of L considered label classes coefficient for the b-th base classifier, and P(y|x) is the final
and k = 1,...,K indexes the support image. The set C can combined probability distribution.
correspondtoeitherL orL dependingonwhetherthe
train test
model is in training or testing mode. From F , the prototype III. RELATEDWORK
l,k
of label class l is computed via masked average pooling [23] Ensembling methods, involve strategically combining mul-
as follows: tiple individual models to enhance predictive performance.
While the importance of selecting methods with complemen-
p =
1 (cid:88)K (cid:80) x,yF l( ,x k,y)1[M l( ,x k,y) =l]
, (1)
tary strengths has been emphasized by Dietterich et al. [30],
l K (cid:80) 1[M(x,y) =l] who highlighted that an effective ensemble relies on accurate
k=1 x,y l,k
individual predictors making errors in different regions of the
where(x,y)arethespatiallocationsand1()isanindicator
input space, the application of ensembling to FSS remains
function that always outputs 1 if the argument is true and
relatively unexplored.
0 otherwise. Similarly, the prototype representation of the
In the realm of semantic segmentation, ensembling can be
background is computed by:
implemented as the combination of multi-scale feature sets
generated by feature pyramid network methods [31] and fed
p bg =
L‚àó1
K
(cid:88)(cid:88)K (cid:80)
x
(cid:80),yF l( ,x
k
1,y [M)1[ (M x,yl( ),x k, ‚àà/y) C‚àà/
]
C]
. (2)
i bn yto Bin od ue sp see ln hd ae mnt ed tec ao ld .e [r 3s, 2]c .re Aat din dg itia on nae ln lys ,em Kb hl ie r, oa ds kae rxp el tor ae ld
.
l‚ààCk=1 x,y l,k [33] proposed an ensembling chain where each model is
Thesemanticsegmentationtaskcanbeseenasclassification conditioned on both the input image and the prediction of the
at each spatial location. Thus, PANet computes the cosine previous model, allowing each model in the chain to correct
distance between the query feature vector and each computed the error of the previous.
prototype at each spatial location. Then, it applies a softmax In the context of Few-Shot Classification, Dvornik et al.
operationtothedistancestoproduceaprobabilitydistribution [34]appliedensemblingbycombiningdifferentConvolutional
MÀÜ overthetargetclasses,includingthebackgroundclass.Let Networks trained to produce a single output prediction.
q
cos be the cosine distance function, P ={p |l ‚ààC}‚à™{p } While ensembling methods have demonstrated promise in
l bg
the set of all prototypes, and F the query feature volume. related fields such as few-shot classification and semantic
q
Then,foreachsubjectclassp ‚ààP theprobabilitymapatthe segmentation,theirapplicationtoFSShasbeenratherlimited.
j
spatial location (x,y) is defined as: To the best of our knowledge, only Yang et al. [35] have
explored ensembling methods in the context of FSS. In their
exp(‚àícos(F(x,y),p ))
MÀú(x,y) = q j . (3) work, they addressed the inadequacy of single prototypes per
q;j (cid:80) exp(‚àícos(F(x,y),p )) semantic class in FSS by learning multiple prototypes per
pj‚ààP q jNon-parametric Metric Learning
Support Set Support Masked Prototypes
Backbone with Features
Average
shared weights
Pooling
Predicted
maskùëÄ
ùëû
Query Image cos
Query Features
Fig. 1: Adapted from [19], this diagram illustrates the inference process of PANet. First, features are extracted from both the
Query Image and the examples in the Support Set through a shared backbone. Then, Masked Average Pooling is applied to
featuresextractedfromthesupportimages,generatingprototypesforeachlabeledsubjectclass.Ultimately,thecosinedistance
is computed between the embeddings at each spatial location within the query feature volume and each prototype, yielding
the predicted mask MÀÜ .
q
class, presenting a form of ensembling as the final predictions combination of features learned through different methods to
require integrating multiple probability maps for the same produce the final predictions.
subject class. However, while Yang et al. ensembled the In our implementation, we extract the probability map
multiple prototypes of a class, our exploration focuses on from each backbone before the application of the softmax
ensembling different embeddings from the same image. function.Theseindividualprobabilitymapsarethencombined
to generate a comprehensive probability map, and the pre-
IV. METHODS diction of the ensemble is subsequently derived by applying
the softmax function to this combined probability map. A
This section describes the experimental methodology we
schematicrepresentationofthisprocessispresentedinFig.2.
followed to investigate the utility of introducing ensembling
Weoptedtoensureanequalcontributionfromeachbackbone
techniques in the context of FSS pipelines. Motivated by
in the ensemble assigning fixed and equal weights, setting
the limited exploration of ensembling methods in FSS, we
them as the inverses of the number of available backbones.
specifically focus on the PANet architecture with embeddings The ensemble probability map MÀú for each subject class is
q
produced by different backbones: VGG16, ResNet50, and
computed accordingly:
MobileNet-V3-Large. We chose PANet among the many FSS
methods precisely because of the lack of trainable parameters
in the model component responsible for processing embed- MÀú(x,y) =(cid:88) 1 exp(‚àíd(F q( ,x b,y),p j,b)) , (6)
dings and predicting masks. This configuration allows us q;j |B|(cid:80) exp(‚àíd(F(x,y),p ))
to isolate the impact of different ensembling strategies by b‚ààB pj‚ààPb q,b j
directly examining the evaluation metrics while preventing
whereB representsthesetofallbackbones,P ={p |c‚àà
b c,b
the model from learning any implicit properties of the new
C }‚à™{p } is the set of all prototypes for the backbone b,
i,b bg,b
latent space. In particular, we focus our experimentation on
F denotes the query feature map for backbone b, and |B| is
q,b
thechallengingone-shotscenario,whichrequirestogeneralize
the number of involved backbones.
from a single example per class. We consider two distinct
ensemblingmethods:IndependentVoting,andFeatureVolume B. Feature Volume Fusion
Fusion, which we illustrate in more detail in the remainder of
FeatureVolumefusionisappliedforconcatenatingmultiple
this section.
backboneembeddings.Afterpassinginputimagesthroughthe
backbones, the feature volumes produced by each backbone
A. Independent Voting
are concatenated to be later parsed by the Non-parametric
In this approach, diverse backbones generate independent metric learning module of PANet. This method is depicted in
probability maps, and these maps are then aggregated to form Fig. 3. The concatenation operation is functional to producing
a unified prediction, akin to the principles of Bayesian Voting richer feature volumes, comprehensively capturing all the
[30]. This strategy aims to maintain the autonomy of each availablefeaturesextractedontheinputdatabythebackbones
model during both training and inference, while enabling the and happens as follows:classes, i.e., on different data folds. Thus, to accompany
F(x,y) =F(x,y)||F(x,y)||...||F(x,y), (7) this metric with global figures that summarise results across
q q,b1 q,b2 q,bn
different classes and folds, we also consider the mean of
where F(x,y) is the feature volume produced by the back- the mIoU across the four folds, reported in the last row of
q,bi
boneb i atthespatiallocation(x,y),andthesymbol||denotes Tables I and II. This metric offers a comprehensive overview
the concatenation of feature maps over the channel axis. by averaging the model performance across all folds of the
dataset. Consequently, the mean of mIoU across the four
C. Evaluation Metrics
folds enables the fair comparison of the different models and
To comprehensively assess the segmentation quality of
configurations tested in this study.
PANet under different ensembling configurations, we em-
ployedtwokeymetrics:theIntersectionoverUnion(IoU)and V. EXPERIMENTRESULTS
themeanIntersectionoverUnion(mIoU).TheIoUistheratio Our experimental evaluation, detailed in Tables I and II,
of the intersection area between predicted and ground truth provides valuable insights into the segmentation performance
masks to their union area. It provides a quantifiable measure ofPANetonthePASCAL-5i andCOCO-20i datasets.Initially,
of the overlap between the predicted and ground truth masks, we contrasted baseline methods utilizing individual back-
with values ranging from 0 to 1, 1 indicating a perfect match. bones‚ÄîVGG16, ResNet50, and MobileNet-V3-Large. The
Viewing segmentation as the problem of classifying indi- mIoU scores averaged across folds were consistent across all
vidual image pixels, we define TP , TN , FP , and FN as backbones, with VGG16 and ResNet50 emerging as the best-
c c c c
thecountsoftruepositives,truenegatives,falsepositives,and performing baselines for PASCAL-5i and COCO-20i.
false negatives, predictions for the class c at the pixel level. Subsequently, Independent Voting and Feature Volume Fu-
The IoU, specific to class c, is then computed as: sion were applied for combining two and three backbones
and evaluated against the best-performing baselines for each
TP
IoU = c . (8) dataset.
c TP +FP +FN
c c c IndependentVotingdemonstratedsignificantimprovements,
To gauge the overall segmentation performance across the achieving a 5% mean mIoU increase for PASCAL-5i with
multipleclassesinadatafold,wealsokeeptrackofthemIoU, respecttothetop-performingbaselineforthisdatasetVGG16,
representing the average IoU value across all object classes in and over 4% improvement for COCO-20i compared to the
a fixed fold. Mathematically, mIoU is expressed as: best-performing baseline for this dataset ResNet50. The most
notable results were obtained when combining all three back-
C
1 (cid:88) bones, with Independent Voting achieving a 7.37% improve-
mIoU = IoU , (9)
C c ment for PASCAL-5i and 9.91% for COCO-20i compared to
c=1
the respective best-performing baselines.
where c assumes all the index values of a subject class. Feature Volume Fusion, applied to pairs of backbones,
Considering that testing a model on a single fold entails exhibited improvements of up to 2.95% for PASCAL-5i over
testing it on a subset of the labeled classes in the dataset, the the best-performing baseline for this dataset (VGG16) and
mIoU, if considered in isolation, may not adequately reflect 6.24% for COCO-20i over the best-performing baseline for
the overall model performance. Indeed, different backbones
and ensembling methods may perform differently on different
PANet
Backbone #1
Support Set
ùêπùëû,1
Support Set PANet
ùëÄùëû,1
QueryImage
N Mo en tr- icp a Lr ea am rnet ir ni gc
QueryImage
ùêπùëû Non- Mpa er ta rim cetric
Learning
Backbone #1 ‚àë ùêπùëû,2
Predicted maskùëÄùëû
Support Set PANet
Backbone #2
Non-parametric Predicted maskùëÄùëû
Metric Learning Fig. 3: Feature Volume Fusion: This diagram illustrates the
QueryImage
ùëÄùëû,2 FeatureVolumeFusionprocess,wheretwoormorebackbones
Backbone #2
are applied for extracting features from the Query Image
Fig. 2: Independent Voting: the Query Image and Support and examples in the Support Set. These features are then
Set examples are passed in parallel through multiple PANet concatenated along the channel axis, forming a consolidated
branches, each employing a distinct backbone. The individual ensembled feature map. The ensembled feature map is sub-
probabilitymapsgeneratedbyeachbrancharethencombined sequently given as an input to the non-parametric Metric
using Bayesian voting to produce the final prediction, MÀÜ . Learning stage of PANet.
qTABLE I: Results on PASCAL-5i.
Baseline IndipendentVoting FeatureVolumeFusion
VGG16 ResNet50 MobileNet VGG16 ResNet50 VGG16 VGG16 VGG16 ResNet50 VGG16 VGG16
+ +Mo- +Mo- + + +Mo- +Mo- +
ResNet50 bileNet bileNet ResNet50 ResNet50 bileNet bileNet ResNet50
+Mo- +Mo-
bileNet bileNet
Fold0 0.4075 0.4069 0.4557 0.4447 0.4625 0.4587 0.4605 0.4381 0.4293 0.4320 0.4405
Fold1 0.5751 0.5667 0.5578 0.6012 0.5875 0.5928 0.6043 0.5857 0.5835 0.5824 0.5870
Fold2 0.5053 0.5005 0.4752 0.5332 0.5398 0.5266 0.5466 0.5175 0.5202 0.5092 0.5219
Fold3 0.4108 0.3984 0.4056 0.4215 0.4159 0.4236 0.4274 0.4136 0.4063 0.4167 0.4169
Mean 0.4747 0.4681 0.4736 0.5002 0.5014 0.5004 0.5097 0.4887 0.4848 0.4851 0.4916
(+5.36%) (+5.63%) (+5.41%) (+7.37%) (+2.95%) (+2.13%) (+2.18%) (+3.56%)
TABLE II: Results on COCO-20i.
Baseline IndipendentVoting FeatureVolumeFusion
VGG16 ResNet50 MobileNet VGG16 ResNet50 VGG16 VGG16 VGG16 ResNet50 VGG16 VGG16
+ +Mo- +Mo- + + +Mo- +Mo- +
ResNet50 bileNet bileNet ResNet50 ResNet50 bileNet bileNet ResNet50
+Mo- +Mo-
bileNet bileNet
Fold0 0.2849 0.2900 0.2684 0.3149 0.3065 0.3144 0.3254 0.3214 0.2908 0.3140 0.3315
Fold1 0.2072 0.2212 0.1871 0.2342 0.2291 0.2261 0.2390 0.2343 0.2214 0.2213 0.2394
Fold2 0.1889 0.2267 0.2077 0.2229 0.2306 0.2260 0.2405 0.2226 0.2345 0.2322 0.2383
Fold3 0.1521 0.1535 0.1459 0.1662 0.1650 0.1712 0.1750 0.1688 0.1670 0.1667 0.1775
Mean 0.2083 0.2229 0.2023 0.2346 0.2328 0.2344 0.2450 0.2368 0.2284 0.2336 0.2467
(+5.25%) (+4.46%) (+5.19%) (+9.91%) (+6.24%) (+2.50%) (+4.80%) (+10.68%)
thisdataset(ResNet50).Integrationacrossallthreebackbones of only 0.0017 points on the mean of mIoU across folds.
further increased the performance, with a 3.56% mean mIoU This difference corresponds to a 0.76% variation over the
increaseforPASCAL-5i,and10.68%forCOCO-20i overtheir improvement rate compared to the best-performing backbone.
respective best-performing baselines. Furthermore, Fig. 4 presents a few qualitative examples,
Overall, ensembling different backbones consistently re- where the ground truth regions are shown alongside the pre-
sulted in improved metrics. Pipelines relying on the combina- dictions from individual baselines and ensembling strategies
tion of three backbones invariably outperformed methods that that combine all three backbones. A visual analysis of the
ensemble only two feature vectors. This performance trend segmentation masks exposes notable segmentation errors in
could be linked to each backbone capturing different feature the baseline predictions, which correspond to the lower per-
sets. Once combined, these complementary feature sets can formancefiguresinTablesIandII.Crucially,bothensembling
lead to a more informative description of a given image. strategies significantly reduce the number of false positive
predictions and improve the overall coverage of the subject.
Moreover, it is worth noting how performance differences
The enhanced quality of masks produced by both ensembling
between different ensembling strategies are influenced by the
strategies is consistent with the superior numerical results
specificdatasetconsideredfortheevaluation.Indeed,Indepen-
presented in Tables I and II.
dent Voting demonstrate a clear superiority on PASCAL-5i,
while results on COCO-20i indicated a less notable difference VI. IMPLEMENTATIONDETAILS
between ensembling strategies. Specifically, when using two The original implementation of PANet uses VGG16 as
backbonesonCOCO-20i,eitherIndependentVotingorFeature
backbone, with weights pre-trained on ImageNet [20]. In
Volume Fusion provided a higher performance depending on addition to this default configuration, in our experiments, we
thespecificbackbonecombinationbeingconsidered.However, exploredtheintegrationofalternativebackbonesinthePANet
theseperformancedifferenceswerenotremarkable,especially model: ResNet50, MobileNet-V3-Large, and their ensembled
when considering the mean IoU across different data folds. ablations. In all configurations, we initialised the model with
A dataset-dependent trend in the effectiveness of each weights learned from pre-training on ImageNet.
strategy can be similarly observed when combining all three Throughoutboththetrainingandtestingphases,weadhered
backbones. For PASCAL-5i experiments, Independent Voting to the methodology proposed by Wang et al. [19]. Input
wasthemosteffectiveensemblingstrategy,surpassingFeature images were resized to 417 √ó 417 and augmentated via
Volume Fusion by 0.0181 points on the mean of mIoU randomhorizontalflipping.End-to-endtrainingwasperformed
across folds. This difference in scores corresponds to a 3.81% via stochastic gradient descent, with momentum set to 0.9
delta in the improvement rate relative to the best-performing over 30,000 iterations. The learning rate was set 1e-3 and
backbone. Conversely, for COCO-20i, Feature Volume Fusion incrementally decreased by 0.1 every 10,000 iterations, while
outperformed Independent Voting, albeit with a slight margin also applying a weight decay of 0.0005.(a) Ground Truth (b) MobileNet (c) VGG16 (d) ResNet50 (e) Independent (f) Feature
Voting Volume Fusion
0.22 IoU 0.64 IoU 0.51 IoU 0.58 IoU 0.80 IoU
0.01 IoU 0.37 IoU 0.01 IoU 0.19 IoU 0.71 IoU
0.49 IoU 0.42 IoU 0.23 IoU 0.59 IoU 0.70 IoU
0.00 IoU 0.09 IoU 0.06 IoU 0.55 IoU 0.32 IoU
Fig. 4: Qualitative Results: column (a) shows Query images with ground truth labels. The predictions of the baseline models
aredisplayedincolumn(b)forMobileNet-V3-Large,column(c)VGG16,andcolumn(d)forResNet50.Theseincludenotable
false-positive and false-negative predictions, revealing the challenges in accurately capturing certain object parts. In contrast,
predictions from ensemble techniques configured with all three backbones demonstrate significant improvements. As shown in
columns(e)IndependentVotingand(f)FeatureVolumeFusion,subjectcoverageisenhanced,compensatingforthelimitations
observed for the individual baselines. Under each prediction we also report the IoU score achieved.
We relied on the PyTorch framework for implementing our absence of learnable parameters in the embedding processing
experiments1, building upon the PANet codebase shared by and mask prediction stage. As such they are only inherent to
Wang et al. [19]. All experiments were run on an NVIDIA the choice of multiple backbones.
TITAN X and GTX 1080 Ti GPU with 12GB of memory. Overall, this evidence builds a compelling case for appling
ensemblingtosupportFSStasks,adoptingaholisticapproach
VII. CONCLUSIONS that leverages different backbones. Findings from this paper
In conclusion, experimental results have consistently high- can be exploited to simplify the process of backbone selec-
lighted the superior performance of Independent Voting and tion, as combining multiple backbones was found to be the
FeatureVolumeFusionensemblingtechniquesoverindividual preferable choice in all tested scenarios.
baselines. These results hint toward latent complementarities The modular design proposed in this paper opens up op-
betweenembeddingsextractedfromdifferentbackbones.Cru- portunities to extend the study of ensembling strategies on
cially,thesesynergisticeffectswerefoundinascenariowhere differentarchitecturesandtasks.Futureopportunitiestoextend
the pre-training set was kept fixed across trials and in the this work lie in exploring embeddings derived from trans-
formers and feature patches acquired through self-attention
1Codeisredactedforanonymityanditwillbereleaseduponacceptance. mechanisms [36], [37].Another promising research avenue is the investigation of [21] A.Shaban,S.Bansal,Z.Liu,I.Essa,andB.Boots,‚ÄúOne-shotlearning
state-of-the-art models that operate without attention in a forsemanticsegmentation,‚ÄùinProceedingsoftheBritishMachineVision
Conference(BMVC),BMVAPress,2017.
more resource-efficient setup, a particularly desirable feature
[22] K.Rakelly,E.Shelhamer,T.Darrell,A.A.Efros,andS.Levine,‚ÄúCon-
in FSS settings - see, e.g., [38]. These future directions could ditionalnetworksforfew-shotsemanticsegmentation,‚ÄùinInternational
further accelerate the progress on Domain Adaptation tasks ConferenceonLearningRepresentations,2018.
[23] X.Zhang,Y.Wei,Y.Yang,andT.S.Huang,‚ÄúSg-one:Similarityguid-
that require robust FSS capabilities.
ance network for one-shot semantic segmentation,‚Äù IEEE transactions
oncybernetics,2020.
REFERENCES [24] C.Zhang,G.Lin,F.Liu,R.Yao,andC.Shen,‚ÄúCanet:Class-agnostic
segmentationnetworkswithiterativerefinementandattentivefew-shot
learning,‚Äù in Proceedings of the IEEE/CVF Conference on Computer
[1] M.Bellusci,P.Cudrano,S.Mentasti,R.E.F.Cortelazzo,andM.Mat-
VisionandPatternRecognition,2019.
teucci,‚ÄúSemanticinterpretationofrawsurveyvehiclesensorydatafor
[25] Z.Tian,H.Zhao,M.Shu,Z.Yang,R.Li,andJ.Jia,‚ÄúPriorguidedfeature
lane-levelhdmapgeneration,‚ÄùRoboticsandAutonomousSystems,2024.
enrichment network for few-shot segmentation,‚Äù IEEE transactions on
[2] A. Geiger, P. Lenz, and R. Urtasun, ‚ÄúAre we ready for autonomous
patternanalysisandmachineintelligence,2020.
driving?thekittivisionbenchmarksuite,‚Äùin2012IEEEconferenceon
[26] Y.Liu,X.Zhang,S.Zhang,andX.He,‚ÄúPart-awareprototypenetwork
computervisionandpatternrecognition,IEEE,2012.
forfew-shotsemanticsegmentation,‚ÄùinEuropeanConferenceonCom-
[3] A. Chiatti, R. Bertoglio, N. Catalano, M. Gatti, and M. Matteucci,
puterVision,2020.
‚ÄúSurgicalfine-tuningforgrapebunchsegmentationundervisualdomain
[27] Z. Wu, X. Shi, G. Lin, and J. Cai, ‚ÄúLearning meta-class memory
shifts,‚Äùin2023EuropeanConferenceonMobileRobots(ECMR),2023.
forfew-shotsemanticsegmentation,‚ÄùinProceedingsoftheIEEE/CVF
[4] D.I.Patr¬¥ƒ±cioandR.Rieder,‚ÄúComputervisionandartificialintelligence InternationalConferenceonComputerVision,2021.
inprecisionagricultureforgraincrops:Asystematicreview,‚ÄùComputers
[28] E. Iqbal, S. Safarov, and S. Bang, ‚ÄúMsanet: Multi-similarity and at-
andelectronicsinagriculture,2018.
tention guidance for boosting few-shot segmentation,‚Äù arXiv preprint
[5] E. Mavridou, E. Vrochidou, G. A. Papakostas, T. Pachidis, and V. G. arXiv:2206.09667,2022.
Kaburlasos, ‚ÄúMachine vision systems in precision agriculture for crop [29] O.Vinyals,C.Blundell,T.Lillicrap,k.kavukcuoglu,andD.Wierstra,
farming,‚ÄùJournalofImaging,2019. ‚ÄúMatching networks for one shot learning,‚Äù in Advances in Neural
[6] A.Esteva,K.Chou,S.Yeung,N.Naik,A.Madani,A.Mottaghi,Y.Liu, InformationProcessingSystems,CurranAssociates,Inc.,2016.
E. Topol, J. Dean, and R. Socher, ‚ÄúDeep learning-enabled medical [30] T.G.Dietterich,‚ÄúEnsemblemethodsinmachinelearning,‚ÄùinInterna-
computervision,‚ÄùNPJdigitalmedicine,2021. tionalworkshoponmultipleclassifiersystems,Springer.
[7] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V. Villena-Martinez, [31] T.-Y.Lin,P.Dolla¬¥r,R.Girshick,K.He,B.Hariharan,andS.Belongie,
P.Martinez-Gonzalez,andJ.Garcia-Rodriguez,‚ÄúAsurveyondeeplearn- ‚ÄúFeaturepyramidnetworksforobjectdetection,‚ÄùinProceedingsofthe
ingtechniquesforimageandvideosemanticsegmentation,‚ÄùAppliedSoft IEEEconferenceoncomputervisionandpatternrecognition,2017.
Computing,2018. [32] W.Bousselham,G.Thibault,L.Pagano,A.Machireddy,J.Gray,Y.H.
[8] Y. Guo, Y. Liu, T. Georgiou, and M. S. Lew, ‚ÄúA review of semantic Chang, and X. Song, ‚ÄúEfficient self-ensemble for semantic segmenta-
segmentation using deep neural networks,‚Äù International journal of tion,‚Äùin33rdBritishMachineVisionConference,2022.
multimediainformationretrieval,2018. [33] R. Khirodkar, B. Smith, S. Chandra, A. Agrawal, and A. Criminisi,
[9] B.Li,Y.Shi,Z.Qi,andZ.Chen,‚ÄúAsurveyonsemanticsegmentation,‚Äù ‚ÄúSequential ensembling for semantic segmentation,‚Äù arXiv preprint
in 2018 IEEE International Conference on Data Mining Workshops arXiv:2210.05387,2022.
(ICDMW),2018. [34] N. Dvornik, C. Schmid, and J. Mairal, ‚ÄúDiversity with cooperation:
[10] O.Ronneberger,P.Fischer,andT.Brox,‚ÄúU-net:Convolutionalnetworks Ensemble methods for few-shot classification,‚Äù in Proceedings of the
forbiomedicalimagesegmentation,‚ÄùinMedicalImageComputingand IEEE/CVFInternationalConferenceonComputerVision(ICCV),2019.
Computer-AssistedIntervention(MICCAI),2015. [35] B.Yang,C.Liu,B.Li,J.Jiao,andQ.Ye,‚ÄúPrototypemixturemodels
[11] K. He, G. Gkioxari, P. Dolla¬¥r, and R. Girshick, ‚ÄúMask r-cnn,‚Äù in forfew-shotsemanticsegmentation,‚ÄùinComputerVision‚ÄìECCV2020,
Proceedings of the IEEE international conference on computer vision, Springer,2020.
2017. [36] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
[12] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, ‚ÄúPyramid scene parsing T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gelly,etal.,
network,‚Äù in Proceedings of the IEEE conference on computer vision ‚ÄúAn image is worth 16x16 words: Transformers for image recognition
andpatternrecognition,2017. atscale,‚ÄùarXivpreprintarXiv:2010.11929,2020.
[13] M. Everingham, L. Gool, C. K. Williams, J. Winn, and A. Zisserman, [37] K.He,X.Chen,S.Xie,Y.Li,P.Dolla¬¥r,andR.Girshick,‚ÄúMaskedau-
‚ÄúThepascalvisualobjectclasses(voc)challenge,‚ÄùInternationalJournal toencodersarescalablevisionlearners,‚ÄùinProceedingsoftheIEEE/CVF
ofComputerVision,2010. conferenceoncomputervisionandpatternrecognition,2022.
[14] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, [38] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, ‚ÄúVision
P.Dollar,andL.Zitnick,‚ÄúMicrosoftcoco:Commonobjectsincontext,‚Äù mamba:Efficientvisualrepresentationlearningwithbidirectionalstate
inECCV,EuropeanConferenceonComputerVision,2014. spacemodel,‚ÄùarXivpreprintarXiv:2401.09417,2024.
[15] M.Cordts,M.Omran,S.Ramos,T.Rehfeld,M.Enzweiler,R.Benen-
son, U. Franke, S. Roth, and B. Schiele, ‚ÄúThe cityscapes dataset for
semanticurbansceneunderstanding,‚ÄùinProc.oftheIEEEConference
onComputerVisionandPatternRecognition(CVPR),2016.
[16] K.SimonyanandA.Zisserman,‚ÄúVerydeepconvolutionalnetworksfor
large-scaleimagerecognition,‚ÄùarXivpreprintarXiv:1409.1556,2014.
[17] K.He,X.Zhang,S.Ren,andJ.Sun,‚ÄúDeepresiduallearningforimage
recognition,‚ÄùinProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition,2016.
[18] A.Howard,M.Sandler,G.Chu,L.-C.Chen,B.Chen,M.Tan,W.Wang,
Y. Zhu, R. Pang, V. Vasudevan, et al., ‚ÄúSearching for mobilenetv3,‚Äù
inProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision,2019.
[19] K.Wang,J.H.Liew,Y.Zou,D.Zhou,andJ.Feng,‚ÄúPanet:Few-shot
image semantic segmentation with prototype alignment,‚Äù in The IEEE
InternationalConferenceonComputerVision(ICCV),2019.
[20] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei,‚ÄúImagenet:
Alarge-scalehierarchicalimagedatabase,‚Äùin2009IEEEconferenceon
computervisionandpatternrecognition,Ieee,2009.