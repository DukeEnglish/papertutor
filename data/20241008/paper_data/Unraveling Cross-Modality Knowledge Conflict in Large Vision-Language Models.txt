Preprint
UNRAVELING CROSS-MODALITY KNOWLEDGE
CONFLICTS IN LARGE VISION-LANGUAGE MODELS
TinghuiZhuâ™  QinLiuâ™£ FeiWangâ™¡ ZhengzhongTuâ™¢ MuhaoChenâ™£
â™ FudanUniversity â™£UniversityofCalifornia,Davis
â™¡UniversityofSouthernCalifornia â™¢TexasA&MUniversity
thzhu22@m.fudan.edu.cn {qinli, muhchen}@ucdavis.edu
fwang598@usc.edu tzz@tamu.edu
ABSTRACT
Large Vision-Language Models (LVLMs) have demonstrated impressive capa-
bilities for capturing and reasoning over multimodal inputs. However, these
models are prone to parametric knowledge conflicts, which arise from incon-
sistencies of represented knowledge between their vision and language compo-
nents. In this paper, we formally define the problem of cross-modality para-
metric knowledge conflict and present a systematic approach to detect, inter-
pret, and mitigate them. We introduce a pipeline that identifies conflicts be-
tweenvisualandtextualanswers,showingapersistentlyhighconflictrateacross
modalities in recent LVLMs regardless of the model size. We further investi-
gate how these conflicts interfere with the inference process and propose a con-
trastive metric to discern the conflicting samples from the others. Building on
theseinsights,wedevelopanoveldynamiccontrastivedecodingmethodthatre-
moves undesirable logits inferred from the less confident modality components
based on answer confidence. For models that do not provide logits, we also
introduce two prompt-based strategies to mitigate the conflicts. Our methods
achievepromisingimprovementsinaccuracyonboththeViQuAEandInfoSeek
datasets. Specifically, using LLaVA-34B, our proposed dynamic contrastive de-
coding improves an average accuracy of 2.24%. Code and data are available at
https://github.com/luka-group/vlm-knowledge-conflict.
1 INTRODUCTION
Large Vision-Language Models (LVLMs; OpenAI 2023; Anil et al. 2023; Liu et al. 2024) have
demonstrated potent capabilities for perceiving and understanding information across different
modalities. Thesemodelstypicallyconsistofavisualencoderandalargelanguagemodel(LLM),
alignedbyaprojectionlayer(Lietal.,2022a;Alayracetal.,2022;Liuetal.,2024). Thisalignment
and collaboration mechanism between language and vision components allows users to input text
andimagessimultaneously,breedingsomeofthewildestapplications,includingretrievinginforma-
tionbasedonacombinationoftextualandvisualqueries(Karthiketal.,2023)andaccomplishing
complexreal-worldtaskswithmultimodalagents(Zhang&Zhang,2023;Zhengetal.,2024).
However,thedisentangledtrainingprocessesanddistinctlearningresourcesleveragedbythevision
andlanguagecomponentsofanLVLM,respectively,inherentlybringalonginconsistenciesintheir
learned representations, captured knowledge, as well as their influence during inference (Bartsch
et al., 2023; Rabinovich et al., 2023). Given that the visual encoder and the LLM are separately
trained on different datasets with distinct training objectives, their parametric knowledge across
languageandvisionmodalitiesissusceptibletoconflicts(Wangetal.,2024b), potentiallyleading
to hallucinations (Ji et al., 2023) and inconsistencies in prediction (Chang & Bergen, 2024). As
illustrated in Fig. 1, we present a conflict case from an LVLM. When asked a question about the
same entity presented in two different modalities, the LVLM provides two contradictory answers.
EventhoughthevisualencoderisabletorecognizetheSydney Opera House,themodelstill
failstointegratethisinformationcoherentlyacrossmodalities. Thisphenomenonrevealsacrucial
challenge: the disparity between the knowledge captured by the vision and language components
of LVLMs. However, there has been limited research on parametric knowledge conflicts within
1
4202
tcO
4
]VC.sc[
1v95630.0142:viXraPreprint
Language Input: Not activated during inference
Vision
This is Sydney Visual Projection LLM 1973
opera house Encoder Layer
Question: Conflict
Vision Language Model
When was this Happens
opera house Visual Projection LLM 1976
formally opened? Encoder Layer
Figure1: Aconflictcaseofdifferentinputmodalitieswiththesameinformation. Theconflictstill
happensevenwhenthevisualcomponentsrecognizetheSydneyOperaHouse.
Textual probability when conflict
t 1h 9e 7s 3emodels,espe ð‘ð‘c ð‘¡ð‘¡ially |1 lo9c g7o3 ð‘ð‘n ð‘£ð‘£ce âˆ’rn loi gng ð‘ð‘ð‘¡ð‘¡c |ross H- im gho d dia ffl eit ry enc co en flicts. Thus,inthispaper,wesystematically
i 1n 9v 7e 1stigate the phenome1n97o1n of cross-mo od
f
a pali rt ay mp ea trr ica metric knowledge conflict as defined in Â§3.
Weaimtoaddressseveralprincipledresearchquestions,asfurtherdetailedbelow:
1975 1975 knowledge
R1Q9716: Howtodetectcro1s9s7-m6odalityparametricknowledgeconflicts?
Visual probability when consistent
In Â§4, we introduce a pipeline for detecting such conflicts using a multiple-choice question an-
s1w9e7r3ingformatfoð‘ð‘ð‘£ð‘£cused|l1 oo9 gn7 ð‘ð‘3nð‘£ð‘£aâˆ’mleodgð‘ð‘eLð‘¡ð‘¡n|otwit ideisff.erSepneccei fically, wepresenteachnamedentityindifferent
m19o7d1alitiesandposethe1s9a7m1equestioof npaarbaomuettirtic. Theresultinganswersderivedfromtheknowledge
o1f97e5ach modality are the1n97c5omparekdnotwoleddegteermine if a conflict exists. Our findings reveal a per-
s1is9t7e6ntlyhighconflictra1te97a6crossvariousmodelscalesandarchitectures,inMdiictaigtinagtitohnatincreasing
modelsizealonedoesnotresolvetheseconflicts.
RQ2: How can cross-modalityparametricknowledgeconflicts be interpreted, especially how they
intervenetheinferenceprocess?
ð‘ð‘ð‘¡ð‘¡Ã—logð‘ð‘ð‘¡ð‘¡ âˆ’ð‘ð‘ð‘£ð‘£Ã—logð‘ð‘ð‘£ð‘£, ð‘ð‘ð‘¡ð‘¡>ð‘ð‘ð‘£ð‘£
GiventheseverityofknowledgeconflictsinLVLMs,thisinltorgigð‘ð‘uð‘ð‘ð‘ð‘in=gï¿½questionarises. Onemightini-
ð‘ð‘ð‘£ð‘£Ã—logð‘ð‘ð‘£ð‘£âˆ’ð‘ð‘ð‘¡ð‘¡Ã—logð‘ð‘ð‘¡ð‘¡, ð‘ð‘ð‘¡ð‘¡â‰¤ð‘ð‘ð‘£ð‘£
tiallyassumethatsuchcross-modalconflictswouldreducethepredictionconfidenceintheoriginal
answerduetoconflictingparametricknowledge.However,ouranalysesdemonstratethatconfidence
cannotreliablydistinguishbetweencorrectandincorrectanswers,necessitatingamorenuancedin-
terpretation of these conflicts. To address this issue, we propose a contrastive metric in Â§5 that
moreeffectivelyidentifiesconflictingsamples. Thismetricsuggeststhatcross-modalityknowledge
conflictsactuallywidentheinformationgapembeddedinthetokens.
RQ3:Whatstrategiescanbeintroducedtomitigatecross-modalityknowledgeconflictsatinference?
Havinggainedanunderstandingofhowtheseconflictsaffecttheinference,weseektoaddressthis
question. Inspired by the strong discriminatory power of the contrastive metric, we propose a dy-
namiccontrastivedecodingmethodinÂ§6.Thismethodselectivelyremovesundesiredlogitsinferred
fromthelessreliablemodalitybasedonanswerconfidence. Additionally,weproposetwoprompt-
basedstrategiestomitigatecross-modalityknowledgeconflictsincaseswherethemodeldoesnot
providelogits. Ourdynamiccontrastivedecodingmethodprovidesmoreconsistentimprovements.
In summary, the main contributions of this paper are threefold: 1) To the best of our knowledge,
thisisthefirst-of-its-kindworktodefineandstudycross-modalityparametricknowledgeconflicts
inLVLMs. 2)Weproposeapracticalpipelinefordetectingsuchconflicts,alongwithametricthat
distinguishesconflictingsamplesfromnon-conflictingones. 3)Weintroduceadynamiccontrastive
decoding method to mitigate these conflicts, as well as two prompt-based strategies for resolving
conflictsinclosed-sourcemodels.
2 RELATED WORK
Knowledge Conflict. Knowledge conflict is a critical problem in context-specific tasks, such as
machinereadingcomprehension(Longpreetal.,2021;Zhouetal.,2023;Wangetal.,2023a)and
informationextraction(Wangetal.,2022;Fangetal.,2024;Xuetal.,2022;Wangetal.,2023b;c)
IntherealmofLLMs,recentstudiescanbecategorizedintocontext-memoryconflict,inter-context
conflict, andintra-memoryconflict(Xuetal.,2024b). Thecontext-memoryconflictandtheinter-
context conflict are concerned mainly in the process of Retrieval Augmented Generation (RAG).
TheyfindthatLLMstendtooverlyrelyontheirownparametricmemorywhenfacingcontradictory
evidence(Xieetal.,2023;Wuetal.,2024). Theintra-memoryconflict,ontheotherhand,isrooted
inthepre-trainingcorpuswhichcontainsinaccurateandmisleadinginformation(Benderetal.,2021;
2Preprint
Lin et al., 2021; Kandpal et al., 2023). The inconsistency of knowledge causes LLMs to generate
outputsthatarecontradictorytoeachotherwhengivendifferentpromptswiththesameinformation
(Elazar et al., 2022; Grosse et al., 2023), undermining their reliability. In this context, prior work
hasnotsystematicallystudiedthisproblemforLVLMs,whichisexactlythefocusofthiswork.
Robustness Issues of LVLMs. Although LVLMs have demonstrated significant potential in un-
derstanding and reasoning over multimodal inputs, they also face several robustness challenges,
including language bias (Niu et al., 2021; Zhang et al., 2024; Wang et al., 2024b), hallucinations
(Huangetal.,2024;Zhuetal.,2024),andthevisualperceptiongap(Ghoshetal.,2024). Language
biasreferstothetendencyofLVLMstorelyonlanguagepatternslearnedduringLLMpretraining
(Niu et al., 2021; Zhang et al., 2024; Wang et al., 2024b). Hallucinations, which originate from
LLMs,pertaintothediscrepanciesbetweengeneratedcontentsandfactsfromeitherreal-worldor
userinputs. (Huangetal.,2023;2024). Thevisualperceptiongapreferstothephenomenonthatthe
LVLMsdemonstrateproficientknowledgeandvisualrecognitionabilitiesbutfailtolinktheirvisual
recognitiontothisknowledge(Leeetal.,2023;Ghoshetal.,2024). Theseissuesoftenoverlookthe
potentialconflictsbetweenthevisualandtextualcomponentsoftheLVLM,whichmaycontribute
totheaforementionedchallenges.
Inference-time Intervention. Inference-time intervention encompasses a range of techniques de-
signedtoinfluencetheinferenceorgenerationprocessofLLMs(DameraVenkata&Bhattacharyya,
2022;Lietal.,2024d). Thesetechniqueseitherdirectlymanipulatethelogitsofthegeneratedto-
kens or adjust the parameters of the LLM during inference. One of the most notable strategies in
thiscontextiscontrastivedecoding(Lietal.,2022b;Lengetal.,2024;Zhangetal.,2024),which
mitigatesundesireddistributionsbyremovingthemfromtheoriginaldistribution.Anotherapproach
involvesmodifyingspecificlayersoftheLLMs. Forinstance, ITI(Lietal.,2024d)adjustsmodel
activationduringinferencebyfollowingasetofdirectionsacrossseveralattentionheads,enhancing
thetruthfulnessofLLMs. Thesemethodsprovideameansfortraining-freeadjustmentstoLVLMs,
significantlyreducingthecostcomparedtoreadjustingmodelparameters.
3 PRELIMINARIES
Before diving into parametric knowledge conflicts in LVLMs, we will first outline key definitions
relevanttoouranalysisandprovideanoverviewofthegeneralexperimentalsetup.
3.1 DEFINITIONS
To ground our analysis, we need to define 1) a typical LVLM architecture, and 2) cross-modality
parametricknowledgeconflicts.
LVLMArchitecture. WefocusonthegeneralarchitecturethatisadoptedbyavarietyofLVLMs,
including LLaVA (Liu et al., 2024), Blip (Li et al., 2023), and Qwen-VL (Bai et al., 2023). Typi-
cally,thesemodelsconsistofavisualencoderV,aprojectorF,andalanguagemodelLM. Given
amultimodalinputx = {x ,x },wherex isthevisualinputandx isthetextualinput,LVLM
m v t v t
firstprocessesx withV,resultinginp = V(x ). Then,throughtheprojectorF,p isprojected
v v v v
intothetextualembeddingspace: e = F(p ). Finally,x isembeddedintotheembeddingspace
v v t
by the embedding layer of the LM, resulting in e = embed(x ). The language model then gen-
t t
erates the output by the probability p (y|e ,e ). So, a contemporary LVLM can be defined as
LM v t
p (y|F(V(x )),embed(x )).
LM v t
Cross-Modality Parametric Knowledge Conflict. Since training a large model from scratch is
prohibitivelycostly,LVLMstypicallyalignavisionencoderontoanexistinglanguagemodel. For
example, LLaVA (Liu et al., 2024) aligns the pre-trained CLIP visual encoder ViT-L/14 (Radford
etal.,2021)withVicuna(Chiangetal.,2023),whichhavebeenseparatelytrainedondifferentdata
distributions,leadingtopotentialinconsistentparametricknowledge(Grosseetal.,2023).
To elicit parametric knowledge, we propose to use answers from different modalities as the indi-
cators of the specific parametric knowledge from each modality. Specifically, given a multimodal
inputx = {x ,q},whereq isthequestionregardingtheentityintheimagex ,theoutputy is
m v v m
generatedbyp (F(V(x )),embed(q )),whichwedefineasthevisualanswer. Onthecontrary,
LM v m
givenatextualinputx ={x ,q},wherex isthetextualdescriptionofanamedentityandqisthe
t e e
3Preprint
questiontothenamedentity,theoutputy isgeneratedbyp (embed(q )),whichwedefineasthe
t LM t
textualanswer. Ify Ì¸=y ,thenaparametricknowledgeconflictisidentified.
m t
3.2 EXPERIMENTALSETUP
3.2.1 DATASETSCONSTRUCTION
OriginalDatasets.Followingpriorstudies(Xieetal.,2023;Wuetal.,2024),weadoptthemultiple-
choice question answering (QA) as the form of evaluating cross-modality parametric knowledge
conflicts. Wechoosetwotasksofknowledge-basedvisualquestionansweringaboutnamedentities:
â€¢ ViQuAE(Lerneretal.,2022)isasemi-automaticallyconstructeddatasetcomprising3.7Kques-
tionsaboutnamedentitiesgroundedinavisualcontext,builtuponTriviaQA(Joshietal.,2017).
The named entity in the original question is replaced with an image depicting it, requiring the
modeltoanswerthequestionbasedonthevisualcontextprovided.
â€¢ InfoSeek(Chenetal.,2023)isadatasetcontaining1.3Mquestionsaboutover11Kvisualenti-
ties,designedtoevaluatetheperformanceofLVLMsinprocessingvisualcontentwhileacquir-
ing relevant knowledge. The dataset is automatically constructed from templates of over 300
relationsinWikidata,ensuringadiversesetofquestions.
Multiple Choices Construction. Given that the Table 1: Statistics of the constructed
original datasets are free-form question answering, multiple-choiceQAdataset.
we synthesize distractor choices for each question.
ViQuAE InfoSeek
These distractor choices must be relevant to the
questions to some extent but factually incorrect, to Original MCQA Original MCQA
effectivelyevaluatethemodelâ€™sabilitytodiscernthe #samples 3,697 3,010 73,620 3,000
correctanswers. Tothisend,weemployLLaMA-3-
8B(AI@Meta,2024)tosynthesizerelevantbutincorrectdistractorchoices.Thepromptusedinthis
processislistedinAppendixAppx.Â§B.2. Wealsodown-sampletheInfoSeekdatasettomatchthe
samplesizeoftheViQuAEdataset. ThestatisticsofthedatasetsarepresentedinTab.1.
3.2.2 EVALUATIONMETRICS
SinceweadoptMCQAastheevaluationform,wecandirectlycalculatetheaccuracy:
N
1 (cid:88)
Acc= 1(y =yË†), (1)
N i i
i=1
whereN isthenumberofsamplesandyË† isthegoldanswer. Moreover, toinvestigateparametric
i
knowledgeconflicts,wedefinetheinconsistencybetweenthegeneratedanswersasfliprate(FR):
N
1 (cid:88)
FR= 1(y Ì¸=y ), (2)
N vi ti
i=1
wherey isthevisualanswerandy isthetextualanswer.Thismetricindicateshowmanysamples
vi ti
encounterconflictinganswersbetweentextualandvisualmodalities.
3.2.3 MODELS
Following prior works regarding LVLMs (Zhang et al., 2024; Zhu et al., 2024), we choose the
LLaVAseries(Lietal.,2024a)forevaluation,astheyprovidestrongperformanceandafullrange
ofmodelscales. Moreover,toevaluatehowthearchitectureofLVLMsaffectsthephenomenonof
knowledgeconflicts,weadoptInstructBlip(Daietal.,2023)andQwen-VL(Baietal.,2023).
4 DETECTING PARAMETRIC KNOWLEDGE CONFLICTS
Inthissection,wediscussthepipelinefordetectingparametricknowledgeconflictsinLVLMsand
evaluatetheseverityoftheseconflicts.
4Preprint
Table 2: Results of detecting cross-modality parametric knowledge conflict. We report accuracy
(Acc),recognizedaccuracy(R.Acc),fliprate(FR)andconflictrate(CR).
ViQuAE InfoSeek
Model
Accâ†‘ R.Accâ†‘ FRâ†“ CRâ†“ Accâ†‘ R.Accâ†‘ FRâ†“ CRâ†“
Textual 75.65 78.43 52.74 54.55
LLaVA-7b 41.68 21.36 70.13 42.85
Visual 53.26 58.11 22.11 27.27
Textual 75.65 69.63 56.31 55.41
LLaVA-13b 36.47 28.10 58.44 38.53
Visual 58.57 61.26 31.33 35.50
Textual 82.46 82.32 66.02 64.07
LLaVA-34b 24.90 20.53 43.72 28.57
Visual 69.14 77.95 44.35 48.92
Textual 81.73 80.42 50.53 53.68
InstructBlip-7b 55.35 20.56 59.74 40.16
Visual 43.09 45.63 35.17 38.10
Textual 79.30 78.56 63.24 62.77
Qwen2-VL-7b 28.65 22.46 22.51 20.35
Visual 67.97 72.37 61.69 60.61
4.1 METHOD
Inputs. As defined in Â§3.1, the visual answer is generated by asking a question about the entity
presentedintheimage,whilethetextualanswerisinducedbyreplacingtheimagewiththetextual
description of the named entity. To ensure that equal information is provided across modalities,
we design distinct inputs for each, as illustrated in Fig. 1. Specifically, given a multimodal input
x = {x ,q} âˆˆ D, whereD isthedataset, x istheimagecontainingthenamedentity, andq is
m v v
thequestiontothenamedentityinx ,thevisualanswerisgeneratedby:
v
y âˆ¼p (x ,q)=p (F(V(x )),embed(q)). (3)
v VLM v LM v
Togeneratethetextualanswer,weaddanindicatorpromptpbeforetheoriginalquestion,informing
thelanguagemodelaboutthenamedentityinthequestion. piswrittenasThis is an image
of $named entity. Thus, the input of the textual answer becomes x = p+q. The textual
t
answeristhengeneratedby:
y âˆ¼p (x )=p (embed(x )). (4)
t VLM t LM t
Irrelevant Factor Mitigation in Conflict Detection. The results generated from the aforemen-
tioned inputs can be regarded as the elicited parametric knowledge from LVLMs. However, these
answersareinfluencedbyvariousotherfactors. Forexample, thevisualperceiverV mightfailto
recognize the entity in x , resulting in a random guess. These potential issues impede our ability
v
to accurately detect cross-modality parametric knowledge conflicts. To mitigate irrelevant factors,
wefirstinstructtheLVLMtoidentifytheentitydepictedinx . Ifthemodelcorrectlypredictsthe
v
namedentity,weassumethattheknowledgerelatedtothenamedentityisstoredintheparametric
memoryofV andF,implyingthatanysuchconflictisnotduetoalackofknowledgeinV andF.
4.2 METRIC
Despite efforts to mitigate irrelevant factors in the process of de-
All Samples
tecting cross-modality parametric knowledge conflict, certain fac-
tors remain difficult to disentangle. For instance, the visual per-
Conflict Samples
ceiver V might recognize the entity in x , but be unable to link it
v
to the parametric knowledge within the LVLMs through the pro-
jector F (Ghosh et al., 2024). Alternatively, the LVLM may be Knowledge Performance
Conflict Gap
limited in its reasoning ability to relate the recognized named en-
tity to the question. We classify these potential limitations as the Conflict Rate
performancegap. Therelationshipbetweenconflictcasesandper-
formance gap cases is illustrated in Fig. 2. The performance gap
leads to failures in generating the correct answer, resulting in an
Figure2:Relationshipofcon-
overallperformancedecline, whichcanbe quantifiedbytheaccu-
flictingsamples.
racydifferenceâˆ†Acc = Acc âˆ’Acc . Thenumberofflip
textual visual
samplesattributabletotheperformancegapcanbecalculatedasN = N Ã—âˆ†Acc,whilethetotal
p
5Preprint
number of flip samples is N = N Ã—FR, where N represents the total number of samples. To
f
assesstheseverityoftheconflicts,wecalculateitslowerboundasN â‰¥ N âˆ’N . Accordingly,
kc f p
thelowerboundoftheparametricknowledgeconflictratecanbeexpressedas:
N N âˆ’N
CR= kc â‰¥ f p =FRâˆ’âˆ†Acc. (5)
N N
4.3 ANALYSIS
WeconductexperimentswithLVLMsfollowingtheaforementionedprocedure,andtheresultsare
presentedinTab.2. Wereporttheaccuracy(Acc)onthecompleteevaluationsetandtherecognized
accuracy (R. Acc) on the subset of the evaluation set recognized by the LVLM. Additionally, we
calculatethefliprate(FR)andtheconflictrate(CR)basedontherecognizedevaluationset.
Performance. Forbothdatasets,theLLaVA-34bmodeldemonstratesthehighestaccuracyforboth
textual and visual inputs. However, a significant performance gap exists between the textual and
visual answers. The most pronounced performance gap in the LLaVA family is observed in the
LLaVA-7bmodel,wheretheaccuracydifferenceexceeds20%. Thisperformancegapisattributed
tothecross-modalityparametricknowledgeconflictandtheaforementionedreasons. Furthermore,
thereisanotableimprovementintherecognizedaccuracy(R.Acc)acrossallmodelscomparedto
theoverallaccuracy(Acc). Thisindicatesthatthemodelsperformbetteronrecognizedentitiesand
thattherecognitionprocesseffectivelymitigatespotentialfactorsinfluencingthefinalperformance.
Conflict Rate. The flip rate (FR) decreases with increasing model size on both datasets, ranging
from55.35%to24.90%ontheViQuAEdataset. Concurrently,theâˆ†Accalsodeclineswithlarger
model sizes, decreasing from 20.32% to 4.37% on the ViQuAE dataset. This trend likely results
from the improved ability of larger models to link visual perception to parametric knowledge and
theirenhancedreasoningability,ratherthanareducedlikelihoodofparametricknowledgeconflicts
inlargermodels. WhencalculatingthelowerboundoftheparametricknowledgeconflictrateCR,
a consistent pattern emerges across the datasets: LLaVA-7b/13b/34b exhibits values of 21.36%,
28.10%, and 20.53%, respectively. This pattern suggests that regardless of the modelâ€™s scale and
architecture,thelikelihoodofparametricknowledgeconflictsremainsrelativelyconstant.
KeyTakeaway
Thereisacleartrendthatasthemodelsizeincreases,boththeFRandtheâˆ†Accbetweentextual
and visual answers decrease. However, the lower bound of the knowledge conflict rate (CR)
remains consistently high. This suggests that although scaling up models can enhance their
overallperformanceandconsistency,itdoesnotresolvecross-modalityknowledgeconflicts.
5 INTERPRETING PARAMETRIC KNOWLEDGE CONFLICTS
The constantly large conflict rate across datasets highlights the phenomenon caused by cross-
modalityknowledgeconflicts. Inthissection, wewilltakeacloserlook, throughthesample-wise
perspective,athowparametricknowledgeinvisualcomponents,i.e.,thevisualencoderV andthe
projectorF,causescross-modalityparametricknowledgeconflictbyinterveningtheinferencepro-
cess of the LLM. In particular, we explore how these conflicts influence answer confidence and
proposeametricthatcanserveasanindicatorofthepresenceofsuchconflicts.
5.1 ISPROBABILITYARELIABLEINDICATOROFANSWERCORRECTNESS?
Method. Since the answer probability reflects the modelâ€™s confidence in a given response, it is
naturaltoconsiderhowparametricknowledgeconflictsmightaffectthisprobability. Forinstance,
such conflicts may either reduce confidence in the original answer or introduce a more confident
alternative answer. Given that embed(x ) and F(V(x )) might encapsulate different knowledge,
e v
thisdiscrepancycanaffecttheprobabilitydistributionoverpossibleanswers,resultinginashiftin
confidenceinthefinaloutput. Toinvestigatehowcross-modalityparametricknowledgeconflictin-
fluencesanswerconfidence,wedesignexperimentstodeterminewhethertheanswerconfidencecan
serveasanindicatorofconflictandwhetheritcansuggestthecorrectnessoftheanswer.
6Preprint
Toelicittheanswerprobability,wecalculatethetextualanswerprobabilityp andthevisualanswer
t
probabilityp usingEq.3andEq.4.SinceweadoptMCQAasthetaskformat,weextractthelogits
v
oftheanswertoken,i.e. â€œA,â€â€œB,â€â€œC,â€andâ€œDâ€andapplythesoftmaxfunctiontothem. Thus,the
extracted confidence can be presented as c = softmax(log(p[A]),log(p[B]),log(p[C]),log(p[D])),
wherep[A]indicatestheprobabilityoftokenâ€œA,â€andsoon. Then,weusethefollowingstrategies
tounderstandhowvisualcomponentsinfluencetheinference:
1. Maxconfidence: max(c [y ],c [y ]),wherethemostconfidentanswerisconsideredcorrect.
t t v v
2. Maxconfidenceshift: max(c [y ]âˆ’c [y ],c [y ]âˆ’c [y ]),wherey isthetextualanswerand
t t t v v v v t t
y is the visual answer, indicating that the modality with the most significant influence on the
v
answerisdeemedthedominantmodalityforthequestion.
3. Minvariance: min(Ïƒ(c [y ]),Ïƒ(c [y ])),wheretheanswerwiththeleastvarianceunderdistur-
t t v v
bance is considered the final answer. We introduce disturbance through two methods: writing
diversepromptsandapplyingtheMonteCarlodropout(Gal&Ghahramani,2016).
Results. The results of three strategies are listed in Table 3: Testing different answer correct-
Tab. 3, and the complete experimental setup is de- nessindicatorsbasedonanswerconfidence.
scribed in Appx. Â§B.1. From these results, it is evi-
dentthatnoneofthestrategiesbasedontokenproba- ViQuAE
Method
bility reliably selects the correct answer when con- Acc R.Acc
TextualAnswer 75.65 78.43
flicts arise between the textual and visual answers.
VisualAnswer 53.26 58.11
This suggests that: 1) Confidence is not necessar-
MaxConfidence 54.22 60.14
ily reduced by conflicts. The presence of a cross-
MaxConfidenceShift 54.29 60.14
modality parametric knowledge conflict does not in-
MinVariancePrompt 55.51 61.41
herentlylowertheconfidenceleveloftheanswer. In- MinVarianceDropout 46.51 50.72
stead, theconflictoftenintroducesanalternativean-
swer with higher confidence, overshadowing the original, potentially correct answer. This obser-
vation indicates that high confidence alone is not a reliable indicator of answer correctness in the
presence of such conflicts. 2) Confidence shifts are not indicative of reliability. The results show
that a greater shift in confidence between the textual and visual answers does not necessarily cor-
relate with the reliability of the final answer. 3) Cross-modality parametric knowledge conflict is
notanuncertaintyissue. Thetablealsorevealsthatmethodsbasedonvariancedonotcontributeto
theperformance. Althoughthesemethodsattempttoselectthemorestableanswerbyselectingthe
answerwithminimumvarianceintokenprobability, theresultsshowreductionsinaccuracy. This
impliesthatminimizingvariancedoesnoteffectivelyaddresstheunderlyingknowledgeconflicts.
5.2 CONTRASTIVEMETRICASINDICATOROFCONFLICTS
Method. Toeffectivelyunderstandhowconflictingknowledgeaffectstheinference,weutilizethe
concept of Contrastive Decoding (Li et al., 2022b). Its objective, which subtracts an undesired
distributionfromtheoriginaldistribution,servesasametricforevaluatingthedegreeofdivergence
between the two distributions. Given that we are using MCQA as the task format, our focus is
specificallyonthedistributionoftheanswertoken,particularlythefirsttoken.
Specifically,givenamultimodalinputx = {x ,q},wherex istheimageandq isthequestion,
m v v
andatextualinputx = {x ,q},wherex isthetextualdescriptionofthenamedentityinx ,the
t e e v
predictedfirsttokendistributionofanswersforeachmodalitycanberepresentedasEquations(3)
and(4). Thecontrastiveobjectivecanthenbewrittenas:
p (y |x ,q) p (y |F(V(x )),embed(q))
log(p )=log(p )âˆ’log(p )=log( VLM v v )=log( LM v v ). (6)
cd v t p (y |x ,q) p (y |embed(x ),embed(q))
VLM t e LM t e
Ideally,ifF(V(x )andembed(x )providethesameinformationforq,Eq.6shouldbeequalto0.
v e
However, due to the parametric knowledge conflict between the visual components and the LLM,
V(F(x )) may not embed the same knowledge as embed(x ), leading to log(p ) Ì¸â‰ˆ 0. Thus,
v e cd
|log(p )|canbeinterpretedasthedegreeofdifferencebetweenV(F(x ))andembed(x ). Addi-
cd v e
tionally, thecontrastivedecodingobjectivealsoallowsustoelicitvisualmemoriesbyeliminating
theinfluenceoftextualknowledge. TheanalysesoftheelicitedmemoriesarelistedinAppx.Â§A.
Result. InFig.3,wepresentthedistributionofthecontrastivemetric,specificallyseparatingsam-
ples with consistent answers across modalities from those with conflicting answers. The figure
7Preprint
ViQuAE
llava-v1.6-vicuna-7b-hf llava-v1.6-vicuna-13b-hf llava-v1.6-34b-hf
All All All
Consistent Consistent Consistent
Conflict Conflict Conflict
Consistent: 0.69 Consistent: 0.13 Consistent: 0.12
All: 1.81 All: 0.55 All: 0.37
Conflict: 2.34 Conflict: 1.71 Conflict: 1.38
0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 3.0 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 3.0 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 3.0
InfoSeek
llava-v1.6-vicuna-7b-hf llava-v1.6-vicuna-13b-hf llava-v1.6-34b-hf
All All All
Consistent Consistent Consistent
Conflict Conflict Conflict
Consistent: 0.59 Consistent: 0.28 Consistent: 0.26
All: 0.84 All: 0.59 All: 0.53
Conflict: 1.07 Conflict: 1.05 Conflict: 1.05
0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 3.0 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 3.0 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 3.0
Figure 3: Distribution of the contrastive metric on all samples, samples with modality-consistent
answers,andsampleswithmodality-conflictanswers. Thedashedlinesindicatethemedians.
revealsasignificantdisparitybetweentheconsistentandconflictingsamples. Mostconsistentsam-
ples fall within the range of 0-0.6, while conflicting samples exhibit greater variability, with an
average median of 1.46. This similar trend suggests that the extent of conflicts, as measured by
the contrastive metric, is relatively consistent across different models, despite variations in model
scales and architectures, implying that the cross-modality parametric knowledge conflicts are not
solelydependentonthemodelâ€™sarchitectureorsizebutareintrinsicchallengesthatpersistacross
current training datasets. The figure also suggests that the contrastive metric is effective in distin-
guishingbetweenconsistentandconflictinganswers.Fromtheperspectiveofthecontrastivemetric,
itquantifiesthedivergencebetweentheknowledgeencodedinthevisualcomponentsandtheLLM.
Thus, the misaligned knowledge leads to the information gap embedded in the tokens of different
modalities,whichisultimatelypresentedbytheconflictinganswer.
KeyTakeaway
Confidence alone is not a reliable indicator of answer correctness when confronted with con-
flictsamples.Theproposedcontrastivemetriceffectivelydistinguishesconflictingsamplesfrom
consistentones,suggestingthatcross-modalityknowledgeconflictstendtoexacerbatetheinfor-
mationgapbetweentokensacrossdifferentmodalities,regardlessofthemodelsize.
6 MITIGATING PARAMETRIC KNOWLEDGE CONFLICTS AT INFERENCE TIME
Having established an understanding of cross-modality parametric knowledge conflicts, we now
shift our focus to strategies for mitigating these conflicts. Since the contrastive metric has proven
effectiveindistinguishingconflictingsamplesfromconsistentones,wefirstproposeastrategythat
leveragestheprinciplesofcontrastivedecoding. Moreover,wealsodesignanalternativeapproach
basedonpromptingformodelsthatdonotprovideaccesstologitsduringinference.
6.1 DYNAMICCONTRASTIVEDECODING
Method.Inanidealapplicationofcontrastivedecoding,wewouldhaveanaprioriknowledgeofthe
logits,whichenablesustodefinetheundesiredlogits.Thatistosay,toresolvecross-modalitypara-
metric knowledge conflicts, the logits from the incorrect, conflicting modality should be excluded
from those of the correct modality. However, in real-world scenarios, without external validation,
itisimpossibletodefinitivelydeterminethecorrectnessofananswer. Therefore,weproposeusing
themodelâ€™sanswerconfidenceasatrendforcorrectness,alsotreatingitasascalingfactorforthe
originallogits. Wethenapplythesescaledlogitstothecontrastivedecodingalgorithm,formulating
thedynamiccontrastivedecoding(DCD).Thisapproachadjuststhecontrastivedecodingobjec-
8Preprint
Table4:Resultsofthedynamiccontrastivedecodingcomparedtothebaselines. Boldindicatesbest
resultsandunderlineindicatessecondbests.
ViQuAE InfoSeek
Model Method
Acc R.Acc Acc R.Acc
TextualAnswer 75.65 78.43 52.74 54.55
LLaVA-7b VisualAnswer 53.26 58.11 22.11 27.27
DCD 76.49(+0.84) 79.51(+1.08) 54.90(+2.16) 58.87(+4.32)
TextualAnswer 75.65 69.63 56.31 55.41
LLaVA-13b VisualAnswer 58.57 61.26 31.33 35.50
DCD 76.58(+0.93) 74.14(+4.51) 58.03(+1.72) 56.52(+1.11)
TextualAnswer 80.99 82.32 66.02 64.07
LLaVA-34b VisualAnswer 69.14 77.95 44.35 48.92
DCD 83.35(+2.36) 85.33(+3.01) 68.14(+2.12) 67.72(+3.65)
TextualAnswer 81.73 80.42 50.53 53.68
InstructBlip-7b VisualAnswer 43.09 45.63 35.17 38.10
DCD 82.47(+0.74) 80.59(+0.17) 50.53(+0.00) 54.38(+0.70)
TextualAnswer 79.30 78.56 63.24 62.77
Qwen2-VL-7b VisualAnswer 67.97 72.37 61.69 60.61
DCD 80.76(+1.46) 80.59(+2.03) 64.30(+1.06) 63.34(+0.57)
tive byincorporating confidence asa dynamic factor tomore accurately measurethe difference in
informationembeddedbythetextualandvisualcomponents.
Specifically,giventhetextualanswery withitsprobabilitiesp (y |x ,q)andthevisualanswery
t t t e v
withitsprobabilitiesp (y |x ,q),wefirstcalculatetheconfidenceforeachanswerasfollows:
v v v
c =max(softmax(log(p [A]),log(p [B]),log(p [C]),log(p [D]))), (7)
t t t t t
c =max(softmax(log(p [A]),log(p [B]),log(p [C]),log(p [D]))), (8)
v v v v v
wherep[A]indicatestheprobabilityfortokenâ€œA,â€andsimilarlyforothertokens. Next,thescaled
logits are computed as s = c Ã—log(p ) and s = c Ã—log(p ). To assess which modality is
t t t v v v
more likely to provide the correct answer, we view the confidence as the likelihood, selecting the
modalitywiththehigherconfidence. However,asdiscussedinÂ§5.1,confidencealoneisinsufficient
to determine correctness. Therefore, we subtract the scaled logits of the less confident modality
fromthoseofthemoreconfidentone. Thisleadstotheapplicationofcontrastivedecodingonthe
scaledlogits,conditionedbytheanswerconfidence:
(cid:26)
c Ã—log(p(y |x ,q))âˆ’c Ã—log(p(y |x ,q)), ifc >c
log(p (y|x))= t t e v v v t v (9)
cd c Ã—log(p(y |x ,q))âˆ’c Ã—log(p(y |x ,q)), otherwise.
v v v t t e
Results. Tab.4presentstheaccuracyandtherecognizedaccuracyfordifferentmethodsacrossthe
ViQuAEandInfoSeekdatasets. Acrossbothdatasetsandallmodelsizes,DCDconsistentlyoutper-
forms both the textual and visual answers. For instance, in the LLaVA-7b model, DCD improves
the accuracy from 75.65% to 76.49% on the ViQuAE dataset. Similarly, on the InfoSeek dataset,
accuracyincreasesfrom52.74%to54.90%. Theseimprovementsareevenmorepronouncedinthe
larger models. For example, in the LLaVA-34b model, DCD increases accuracy by 2.36% on the
ViQuAEdatasetandby2.12%onInfoSeek,indicatingitspotentialinmodelswithlargerscales.
DCD demonstrates particularly significant gains in recognized accuracy (R. Acc). For instance,
on the InfoSeek dataset, the recognized accuracy for the LLaVA-34b model increases by 3.65%
when using DCD compared to the textual answer. This trend is consistent across all model sizes,
indicating that DCD is particularly effective in improving the performance on recognized entities.
Theimprovementinrecognizedaccuracyislikelyduetothefactthatthevisualanswerswithinthe
recognizedsetareexpectedtocontainmorerelevantinformationthanthoseintheunrecognizedset,
asthevisualcomponentshavesomepriorknowledgeoftheseentities. Consequently,theDCDcan
moreeffectivelyleveragethisinformationtodiscernwhichoptioniscorrect.
9Preprint
Table5: Resultsoftheprompt-basedstrategiescomparedtothebaselines. Sincetheinputsofthis
experimentarethesameastheoneofthevisualanswerexceptfortheprompt,wecomparethemto
theresultsofthevisualanswer. Boldindicatesbestresultsandunderlineindicatessecondbests.
ViQuAE InfoSeek
Method
Acc R.Acc Acc R.Acc
LLaVA-7b
VisualAnswer 53.26 58.11 22.11 27.27
ReminderPrompt 53.99(-1.66) 57.25(-2.53) 21.25(-0.86) 27.99(+0.72)
AnswerConflictPrompt 54.58(-1.07) 58.51(-1.27) 20.23(-1.88) 27.39(+0.12)
LLaVA-13b
VisualAnswer 58.57 61.26 31.33 35.50
ReminderPrompt 58.57(+0.00) 61.26(+0.00) 35.53(+4.20) 38.10(+2.60)
AnswerConflictPrompt 57.59(-0.98) 59.67(-1.59) 34.27(+2.94) 39.06(+3.56)
LLaVA-34b
VisualAnswer 69.14 77.95 44.35 48.92
ReminderPrompt 72.99(+3.85) 79.28(+1.33) 45.15(+0.80) 49.62(+0.70)
AnswerConflictPrompt 73.62(+4.48) 79.66(+1.71) 52.43(+8.08) 53.68(+4.76)
6.2 PROMPTINGSTRATEGY
Method. Sincenotallmodelsprovidethelogitsofthegeneratedcontents,weproposetwoprompt-
based improvement strategy for those models. To address cross-modality parametric knowledge
conflict,wedesigntwotypesofpromptsandthedetailsofthesepromptsareprovidedinAppx.Â§B.2.
1. Reminderprompt. Onceaknowledgeconflictisdetected,themodelispromptedtoregenerate
theanswer,butthistimewithareminderthathighlightsthepresenceofconflictingknowledge.
2. Answerprompt.Sincebothtextualandvisualanswersarealreadygeneratedduringthedetection
process,thispromptasksthemodeltodeterminewhichoneiscorrect.
Results. Tab.5presentstheresultsofprompt-basedimprovementsusingtwostrategiesacrosstwo
datasets and different model sizes. The effectiveness of these strategies varies depending on the
modelsize. Forsmallermodels,bothpromptsnegativelyimpactperformanceacrossbothdatasets,
withaccuracydroppingbyatleast1.07%ontheViQuAEdatasetand0.86%ontheInfoSeekdataset.
This suggests that smaller models may struggle to handle prompts reminding them of potential
knowledgeconflicts,astheyseemunabletodiscernwhichansweriscorrect. Furthermore,present-
ingsmallermodelswithconflictinganswersseemstointroduceadditionalconfusion,asevidenced
by the more substantial accuracy declines. In contrast, larger models are more effective at pro-
cessingthe informationprovided inthe prompts, demonstrating anaccuracy gainof 4.48%on the
ViQuAE dataset and 8.08% on the InfoSeek dataset. These results indicate that larger models are
betterequippedtointerpretandrespondtotheinformationintheprompt, likelyduetotheirmore
advancedreasoningandunderstandingcapabilities,whichenablethemtodeterminewhichmodality
is more reliable in resolving the conflict. Overall, these findings indicate that the effectiveness of
prompt-basedconflictresolutionstrategiesimproveswithmodelscale,particularlywhentheprompt
providesthemodelwithbothconflictinganswers,aidinginconflictresolution.
KeyTakeaway
Dynamiccontrastivedecoding(DCD)bringsuniversalimprovementsagainstthebaselines. The
performanceofprompting-basedstrategiesvariesdependingonthemodelsize. Largermodels
arebetteratunderstandingandprocessingtheinformationinthedesignedprompts.
7 CONCLUSIONS
Inthispaper,weintroducetheconceptofcross-modalityparametricknowledgeconflictsinLVLMs,
asignificantissuearisingfromthemisalignmentbetweenvisualandtextualmodalities. Wepropose
asystematicapproachtodetecttheseconflicts,revealingapersistentlyhighconflictrateacrossall
model sizes. Our findings indicate that simply scaling up models does not resolve these conflicts,
highlightingtheneedfortargetedinterventionstrategies. Toaddressthesechallenges, wepropose
10Preprint
thecontrastivemetric,whicheffectivelyidentifiesconflictingsamplesbymeasuringtheinformation
gapbetweenmodalities. Buildingonthis, weintroducethedynamiccontrastivedecoding(DCD),
whichselectivelyremovesunreliablelogitstoimproveansweraccuracy. Formodelswithoutaccess
to logits, we propose two prompt-based strategies. These approaches collectively improve model
performance.OnLLaVA-34B,thedynamiccontrastivedecodingachievesanaccuracyimprovement
of2.36%ontheViQuAEdatasetand2.12%ontheInfoSeekdataset. Ourcontributionsadvancethe
understanding of cross-modality parametric knowledge conflicts in LVLMs and provide practical
solutionstomitigatetheseconflicts,leadingtomorerobustandaccuratemultimodalinference.
Futureworkmayextendourmethodtobroadermultimodaldataincludingmulti-imagesandvideos
(Wangetal.,2024a;Lietal.,2024b),orapplyourmethodtoenhancemultimodalin-contextlearn-
ing (Xu et al., 2024a) and retrieval-augmented generation (Chen et al., 2022). Additionally, our
methodmaycontributetoensuringtrustworthinessofLVLMsinrisk-sensitivedomains,suchasthe
biomedicalfield(Lietal.,2024c;Chavesetal.,2024).
ETHICS STATEMENT
OurstudyhighlightsacriticalconcerninrecentLVLMs: theparametricmemoriesofthevisionand
languagecomponentsarepronetoconflicts.Thisissueunderscoresthepotentiallimitationsofthese
models, as they may produce inconsistent or unreliable outputs if these conflicts are not properly
addressed. Asresearchers,ourgoalistomitigatetheseriskswhilemaximizingthebenefits.
REPRODUCIBILITY STATEMENT
Ourexperimentsareconductedusingfiveopen-sourceLVLMstoensurereproducibility. Tofacili-
tatereplicationofourresults,wehaveprovidedthepromptsusedinourexperimentsinAppx.Â§B.2.
Additionally,thedatasetsutilizedinourstudyareincludedinthesupplementarymaterialsforfur-
therreference.
REFERENCES
AI@Meta.Llama3modelcard,2024.URLhttps://github.com/meta-llama/llama3/
blob/main/MODEL_CARD.md.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguage
model for few-shot learning. Advances in neural information processing systems, 35:23716â€“
23736,2022.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.
arXivpreprintarXiv:2305.10403,2023.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou,andJingrenZhou. Qwen-vl: Aversatilevision-languagemodelforunderstanding,local-
ization,textreading,andbeyond,2023.
Henning Bartsch, Ole Jorgensen, Domenic Rosati, Jason Hoelscher-Obermaier, and Jacob Pfau.
Self-consistency of large language models under ambiguity. arXiv preprint arXiv:2310.13439,
2023.
Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the
dangersofstochasticparrots: Canlanguagemodelsbetoobig? InProceedingsofthe2021ACM
conferenceonfairness,accountability,andtransparency,pp.610â€“623,2021.
Tyler A Chang and Benjamin K Bergen. Language model behavior: A comprehensive survey.
ComputationalLinguistics,50(1):293â€“350,2024.
11Preprint
Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu, Naoto Usuyama,
ShengZhang,FeiWang,YujiaXie,MahmoudKhademi,ZiyiYang,etal. Trainingsmallmulti-
modalmodelstobridgebiomedicalcompetencygap: Acasestudyinradiologyimaging. arXiv
preprintarXiv:2403.08002,2024.
WenhuChen,HexiangHu,XiChen,PatVerga,andWilliamCohen. Murag: Multimodalretrieval-
augmented generator for open question answering over images and text. In Proceedings of the
2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.5558â€“5570,2022.
Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei
Chang.Canpre-trainedvisionandlanguagemodelsanswervisualinformation-seekingquestions?
arXivpreprintarXiv:2302.11713,2023.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
BoyangLi,PascaleFung,andStevenHoi.Instructblip:Towardsgeneral-purposevision-language
modelswithinstructiontuning. arXivpreprintarXiv:2308.01525,2023.
Niranjan Damera Venkata and Chiranjib Bhattacharyya. When to intervene: Learning optimal in-
tervention policies for critical events. Advances in Neural Information Processing Systems, 35:
30114â€“30126,2022.
Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Marius Mos-
bach,YonatanBelinkov,HinrichSchuÂ¨tze,andYoavGoldberg. Measuringcausaleffectsofdata
statisticsonlanguagemodelâ€™sfactualâ€™predictions. arXivpreprintarXiv:2207.14251,2022.
TianqingFang,ZhaoweiWang,WenxuanZhou,HongmingZhang,YangqiuSong,andMuhaoChen.
Gettingsickafterseeingadoctor?diagnosingandmitigatingknowledgeconflictsineventtempo-
ralreasoning. InKevinDuh,HelenaGomez,andStevenBethard(eds.),FindingsoftheAssocia-
tion for Computational Linguistics: NAACL 2024, pp. 3846â€“3868, Mexico City, Mexico, June
2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.244.
URLhttps://aclanthology.org/2024.findings-naacl.244.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050â€“1059.
PMLR,2016.
Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Utkarsh Tyagi, Oriol Nieto, Zeyu Jin,
andDineshManocha. Vdgd: Mitigatinglvlmhallucinationsincognitivepromptsbybridgingthe
visualperceptiongap,2024.
Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit
Steiner,DustinLi,EsinDurmus,EthanPerez,etal.Studyinglargelanguagemodelgeneralization
withinfluencefunctions. arXivpreprintarXiv:2308.03296,2023.
LeiHuang, WeijiangYu, WeitaoMa, WeihongZhong, ZhangyinFeng, HaotianWang, Qianglong
Chen,WeihuaPeng,XiaochengFeng,BingQin,etal.Asurveyonhallucinationinlargelanguage
models:Principles,taxonomy,challenges,andopenquestions. arXivpreprintarXiv:2311.05232,
2023.
WenHuang,HongbinLiu,MinxinGuo,andNeilZhenqiangGong. Visualhallucinationsofmulti-
modallargelanguagemodels. arXivpreprintarXiv:2402.14683,2024.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,
AndreaMadotto,andPascaleFung. Surveyofhallucinationinnaturallanguagegeneration. ACM
ComputingSurveys,55(12):1â€“38,2023.
MandarJoshi,EunsolChoi,DanielSWeld,andLukeZettlemoyer. Triviaqa: Alargescaledistantly
supervisedchallengedatasetforreadingcomprehension.arXivpreprintarXiv:1705.03551,2017.
12Preprint
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language
modelsstruggletolearnlong-tailknowledge. InInternationalConferenceonMachineLearning,
pp.15696â€“15707.PMLR,2023.
ShyamgopalKarthik,KarstenRoth,MassimilianoMancini,andZeynepAkata. Vision-by-language
fortraining-freecompositionalimageretrieval. arXivpreprintarXiv:2310.09291,2023.
Jiyoung Lee, Seungho Kim, Seunghyun Won, Joonseok Lee, Marzyeh Ghassemi, James Thorne,
Jaeseok Choi, O-Kil Kwon, and Edward Choi. Visalign: Dataset for measuring the degree of
alignmentbetweenaiandhumansinvisualperception. arXivpreprintarXiv:2308.01525,2023.
SicongLeng,HangZhang,GuanzhengChen,XinLi,ShijianLu,ChunyanMiao,andLidongBing.
Mitigating object hallucinations in large vision-language models through visual contrastive de-
coding. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion,pp.13872â€“13882,2024.
Paul Lerner, Olivier Ferret, Camille Guinaudeau, HerveÂ´ Le Borgne, Romaric BesancÂ¸on, JoseÂ´ G
Moreno, and JesuÂ´s LovoÂ´n Melgarejo. Viquae, a dataset for knowledge-based visual question
answeringaboutnamedentities.InProceedingsofthe45thInternationalACMSIGIRConference
onResearchandDevelopmentinInformationRetrieval,pp.3108â€“3120,2022.
Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang,
Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal ca-
pabilities in the wild, May 2024a. URL https://llava-vl.github.io/blog/
2024-05-10-llava-next-stronger-llms/.
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei
Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint
arXiv:2408.03326,2024b.
ChunyuanLi,CliffWong,ShengZhang,NaotoUsuyama,HaotianLiu,JianweiYang,TristanNau-
mann,HoifungPoon,andJianfengGao. Llava-med: Trainingalargelanguage-and-visionassis-
tantforbiomedicineinoneday. AdvancesinNeuralInformationProcessingSystems,36,2024c.
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: Bootstrappinglanguage-imagepre-
trainingforunifiedvision-languageunderstandingandgeneration.InInternationalconferenceon
machinelearning,pp.12888â€“12900.PMLR,2022a.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InInternationalconference
onmachinelearning,pp.19730â€“19742.PMLR,2023.
KennethLi,OamPatel,FernandaVieÂ´gas,HanspeterPfister,andMartinWattenberg. Inference-time
intervention: Elicitingtruthfulanswersfromalanguagemodel. AdvancesinNeuralInformation
ProcessingSystems,36,2024d.
XiangLisaLi,AriHoltzman,DanielFried,PercyLiang,JasonEisner,TatsunoriHashimoto,Luke
Zettlemoyer,andMikeLewis.Contrastivedecoding:Open-endedtextgenerationasoptimization.
arXivpreprintarXiv:2210.15097,2022b.
StephanieLin,JacobHilton,andOwainEvans. Truthfulqa: Measuringhowmodelsmimichuman
falsehoods. arXivpreprintarXiv:2109.07958,2021.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. Advances
inneuralinformationprocessingsystems,36,2024.
Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer
Singh. Entity-based knowledge conflicts in question answering. In Marie-Francine Moens,
Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language Processing, pp. 7052â€“7063, Online and
Punta Cana, Dominican Republic, November 2021. Association for Computational Linguis-
tics.doi:10.18653/v1/2021.emnlp-main.565.URLhttps://aclanthology.org/2021.
emnlp-main.565.
13Preprint
YuleiNiu,KaihuaTang,HanwangZhang,ZhiwuLu,Xian-ShengHua,andJi-RongWen. Counter-
factualvqa:Acause-effectlookatlanguagebias. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pp.12700â€“12710,2021.
OpenAI. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023.
Ella Rabinovich, Samuel Ackerman, Orna Raz, Eitan Farchi, and Ateret Anaby-Tavor. Predicting
question-answeringperformanceoflargelanguagemodelsthroughsemanticconsistency. arXiv
preprintarXiv:2311.01152,2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learningtransferablevisualmodelsfromnaturallanguagesupervision,2021.
FeiWang,WenjieMo,YiweiWang,WenxuanZhou,andMuhaoChen. Acausalviewofentitybias
in(large)languagemodels.InHoudaBouamor,JuanPino,andKalikaBali(eds.),Findingsofthe
AssociationforComputationalLinguistics: EMNLP2023,pp.15173â€“15184,Singapore,Decem-
ber 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.
1013. URLhttps://aclanthology.org/2023.findings-emnlp.1013.
Fei Wang, Xingyu Fu, James Y Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma,
NanXu,WenxuanZhou,KaiZhang,etal. Muirbench: Acomprehensivebenchmarkforrobust
multi-imageunderstanding. arXivpreprintarXiv:2406.09411,2024a.
Fei Wang, Wenxuan Zhou, James Y Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao
Chen. mdpo: Conditional preference optimization for multimodal large language models. In
EMNLP,2024b.
Haoyu Wang, Hongming Zhang, Yuqian Deng, Jacob Gardner, Dan Roth, and Muhao Chen. Ex-
tracting or guessing? improving faithfulness of event temporal relation extraction. In Andreas
Vlachos and Isabelle Augenstein (eds.), Proceedings of the 17th Conference of the European
ChapteroftheAssociationforComputationalLinguistics,pp.541â€“553,Dubrovnik,Croatia,May
2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.39. URL
https://aclanthology.org/2023.eacl-main.39.
Yiwei Wang, Muhao Chen, Wenxuan Zhou, Yujun Cai, Yuxuan Liang, Dayiheng Liu, Baosong
Yang,JunchengLiu,andBryanHooi. Shouldwerelyonentitymentionsforrelationextraction?
debiasing relation extraction with counterfactual analysis. In Marine Carpuat, Marie-Catherine
de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pp. 3071â€“3081, Seattle, United States, July 2022. Association for Computational
Linguistics. doi: 10.18653/v1/2022.naacl-main.224. URLhttps://aclanthology.org/
2022.naacl-main.224.
Yiwei Wang, Bryan Hooi, Fei Wang, Yujun Cai, Yuxuan Liang, Wenxuan Zhou, Jing Tang, Man-
juan Duan, and Muhao Chen. How fragile is relation extraction under entity replacements? In
Proceedingsofthe27thConferenceonComputationalNaturalLanguageLearning(CoNLL),pp.
414â€“423,2023c.
Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao. How easily do
irrelevantinputsskewtheresponsesoflargelanguagemodels? arXivpreprintarXiv:2404.03302,
2024.
Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn
sloth: Revealing the behavior of large language models in knowledge conflicts. arXiv preprint
arXiv:2305.13300,2023.
Nan Xu, Fei Wang, Bangzheng Li, Mingtao Dong, and Muhao Chen. Does your model classify
entities reasonably? diagnosing and mitigating spurious correlations in entity typing. In Yoav
Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing, pp. 8642â€“8658, Abu Dhabi, United Arab
Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.
emnlp-main.592. URLhttps://aclanthology.org/2022.emnlp-main.592.
14Preprint
Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, and Muhao Chen. From introspection to best
practices:Principledanalysisofdemonstrationsinmultimodalin-contextlearning.arXivpreprint
arXiv:2407.00902,2024a.
Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. Knowledge
conflictsforllms: Asurvey. arXivpreprintarXiv:2403.08319,2024b.
Yi-FanZhang,WeichenYu,QingsongWen,XueWang,ZhangZhang,LiangWang,RongJin,and
TieniuTan. Debiasinglargevisuallanguagemodels. arXivpreprintarXiv:2403.05262,2024.
ZhuoshengZhangandAstonZhang. Youonlylookatscreens: Multimodalchain-of-actionagents.
arXivpreprintarXiv:2309.11436,2023.
BoyuanZheng, BoyuGou, JihyungKil, HuanSun, andYuSu. Gpt-4v(ision)isageneralistweb
agent,ifgrounded. arXivpreprintarXiv:2401.01614,2024.
Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for
largelanguagemodels. InHoudaBouamor,JuanPino,andKalikaBali(eds.),FindingsoftheAs-
sociationforComputationalLinguistics: EMNLP2023,pp.14544â€“14556,Singapore,December
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.968.
URLhttps://aclanthology.org/2023.findings-emnlp.968.
Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, and Jun Liu. Ibd: Alleviating
hallucinations in large vision-language models via image-biased decoding. arXiv preprint
arXiv:2402.18476,2024.
15Preprint
Question TextualMemory VisualMemory
In what city did HongKong. SanFrancisco,California,USA.
BruceLeegrowup?
GeorgeHarrisonwas George Harrison was deported George Harrison of the Beatles
deported from which from Liverpool, England be- wasdeportedfromHamburgbe-
city where the early causeofhisyouth. causeheliedabouthisagewhen
Beatles had long tryingtoenterGermanylegally.
term engagements,
becauseofhisyouth?
What species of fly Calliphora vomitoria is a Calliphora vomitoria is com-
has the Latin name speciesofflycommonlyknown monlyknownasvomitingflyor
calliphoravomitoria? as the â€vomiting flyâ€ or â€fruit green bottle fly. It belongs to
fly.â€ It is a small, yellowish- family Calliphoridae which in-
brown insect with a short life cludes blowflies and horseflies.
cycle and a strong ability to These insects play an essential
reproduce quickly. The species roleindecompositionprocesses
iswidelydistributedthroughout by feeding on decaying organic
the world and is often found matter...
nearfruitorvegetablesources.
What is the name of The name of Bob Marleyâ€™s Live Forever Volume Two by
Bob Marleyâ€™s great- greatesthitsalbumisâ€œBobMar- Various Artists featuring Bob
esthitsalbum? ley and the Wailers: Greatest Marley & The Wailers includes
Hits.â€ performancescapturedlivedur-
ing concerts throughout his ca-
reer culminating with perfor-
mances shortly before his pass-
inginMayof1981.
Table6: Examplesofelicitedtextualandvisualmemoriesusingthecontrastivedecodingobjective.
A INTERPRETING CROSS-MODALITY KNOWLEDGE CONFLICTS
ThecontrastivedecodingobjectivedescribedinÂ§5offersavaluabletoolforexaminingthememory
embedded within the visual components of LVLMs. Specifically, the contrastive decoding metric
canbereformulatedinanautoregressiveform:
n n
p
(y|x)=(cid:89)
p (y |x,y
)=(cid:89) p LM(y v|F(V(x v)),embed(q),y <i)
, (10)
cd cd i <i p (y |embed(x ),embed(q),y )
LM t e <i
i=1 i=1
where x is the inputs from both modalities and y indicates the tokens generated before step i.
<i
This autoregressive form of contrastive decoding metric allows us to elicit visual memory from
the visual components by removing the influence of textual knowledge. We accomplish this by
transforming the question into a free-form query without predefined options and then examining
theelicitedmemoryofthevisualcomponents. Theexamplesoftheelicitedmemoriesarelistedin
Tab.6.
Fromthesememories,severalobservationscanbemade:
1. LLMisbetteratmemorizingdateandlocation. Thisalignsintuitivelywiththenatureofthe
LLMâ€™strainingprocess,wheresuchfactualknowledgefrequentlyappearsinthetextcorpora. It
correspondswellwiththeexpectationthatlanguagemodelsacquirestructuredknowledgefrom
reading-baseddata.
2. Visual components are better at memorizing the correlation between an entity and its
names and the relationship among entities. For example, when asked the common name
for Calliphora Vomitoria, the LLM fails to answer correctly, while the visual answer
is correct. This is likely due to the training objective of aligning visual components with the
LLM, during which visual components learn entity-specific knowledge by mapping images to
thelanguagespace.
16Preprint
Giventhequestionanditsgoldanswer, pleasegenerateamultiplechoiceversionof
thisquestion. Notethatthewrongchoicesshouldberelevanttothequestionandthe
goldanswershouldbeexactlycopiedfromwhatisgiven. Youcanrandomlyputthe
gold answer wherever you want. Please output as a json format: {â€œAâ€: Answer A,
â€œBâ€:AnswerB,â€œCâ€:AnswerC,â€œDâ€:AnswerD}. Nofurtherexplanationornote.
Table 7: Prompt for generating false options to construct the multiple-choice question answering
datasets.
Youareanexpertatquestionanswering.Giventhequestion,pleaseoutputtheanswer.
Noexplanationandfurtherquestion. Beawarethatyourvisualmemorymightdiffer
fromyourtextualmemory,causingaconflictinyourknowledge.
Table8: Reminderprompttomitigatecross-modalityparametricknowledgeconflicts.
B EXPERIMENTAL DETAILS
B.1 EXPERIMENTALSETUP
ConfidenceAnalysis. WewilldescribetheexperimentalsetupoftheMinvariancestrategyinÂ§5.1.
For both settings, we sample 10 times with disturbance. For the prompt disturbance, we ask the
LLaMA-3-8b(AI@Meta,2024)torephrasetheoriginalprompttoobtain10differentpromptsand
generatetheanswerwitheachofthem. Forthedropoutdisturbance,wesetthedropoutrateto0.1
andsample10times. Thenweextracttheconfidenceofthegoldanswerandcalculatethevariance.
B.2 PROMPTS
The details of the prompts used in our experiments are listed here. The prompt to generate false
optionsisinTab.7. ThereminderprompttomitigateknowledgeconflictsisinTab.8. Theanswer
conflictprompttomitigateknowledgeconflictsisinTab.9.
17Preprint
Youareanexpertatquestionanswering.Giventhequestion,pleaseoutputtheanswer.
Noexplanationandfurtherquestion. Beawarethatyourvisualmemorymightdiffer
fromyourtextmemory, causingaconflictinyourknowledge. Yourtextmemoryis:
{textualanswer}andyourvisualmemoryis: {visualanswer}.
Table9: Answerconflictprompttomitigatecross-modalityparametricknowledgeconflicts.
18