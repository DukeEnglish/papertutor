ON THE HARDNESS OF LEARNING ONE HIDDEN LAYER NEURAL NETWORKS
SHUCHENLIÂ§,ILIASZADIKÂ§,MANOLISZAMPETAKISÂ§
Abstract. Inthiswork,weconsidertheproblemoflearningonehiddenlayerReLUneuralnet-
workswithinputsfromâ„ğ‘‘. Weshowthatthislearningproblemishardunderstandardcrypto-
graphicassumptionsevenwhen:(1)thesizeoftheneuralnetworkispolynomialinğ‘‘,(2)itsinput
distributionisastandardGaussian,and(3)thenoiseisGaussianandpolynomiallysmallinğ‘‘.Our
hardnessresultisbasedonthehardnessoftheContinuousLearningwithErrors(CLWE)problem,
and in particular, is based on the largely believed worst-case hardness of approximately solving
theshortestvectorproblemuptoamultiplicativepolynomialfactor.
1. Introduction
In this paper, we examine the fundamental computational limitations of learning neural net-
works in a distribution-specific setting. Our focus is on the following canonical regression sce-
nario: let ğ‘“ be an unknown target function that can be represented as a simple neural network,
let ğ’Ÿ be a ğ‘‘-dimensional distribution from which samples ğ‘¥ ğ‘– are drawn, i.e., ğ‘¥ ğ‘– â€ ğ’Ÿ, and
let ğœ‚ ğ‘– â€ ğ‘p0,ğœ2 q be small Gaussian observation noise. The statistician receives ğ‘š indepen-
dent and identically distributed samples of the form pğ‘¥ ğ‘–, ğ‘“pğ‘¥ ğ‘–q `ğœ‚ ğ‘–q for ğ‘– â€œ 1,...,ğ‘š with the
Ë†
goal of constructing an estimator ğ‘“ that is computable in polynomial time and achieves a small
mean squared error (MSE) on a new sample drawn from ğ’Ÿ. Specifically, we aim to minimize
â€ Ä±
ğ”¼
ğ‘¥â€ğ’Ÿ
|ğ‘“Ë† pğ‘¥qÂ´ ğ‘“pğ‘¥q|2 . WeconsiderthefollowingtwoobjectivesintermsoftheMSE:
(1) Achieving Vanishing MSE: Obtaining an MSE that approaches zero as the dimension ğ‘‘
increases.
(2) Weak Learning: Attaining an MSE slightly better than that of the trivial mean estimator.
Formally,thismeansensuringforlargeenough ğ‘‘:
â€ Ä±
1
ğ”¼ ğ‘¥â€ğ’Ÿ |ğ‘“Ë† pğ‘¥qÂ´ ğ‘“pğ‘¥q|2 Ä Var ğ‘¥â€ğ’Ÿpğ‘“qÂ´ polypğ‘‘q.
It is well known due to [KS09] that without further assumptions on the distribution ğ’Ÿ, e.g.,
when ğ’Ÿ can be supported over the Boolean hypercube, learning even one-hidden layer neu-
ral networks is impossible (or â€œhardâ€1) for polynomial-time estimators under standard crypto-
graphicassumptions. Giventhesuccessofneuralnetworksinpractice,alonglineofrecentwork
has attempted to study instead the canonical continuous input distribution case where ğ’Ÿ is the
isotropic Gaussian, i.e., ğ’Ÿ â€œ ğ‘ p0,ğ¼ ğ‘‘q which is also the setting that we follow in this work. Yet,
despitealonglineofresearch,thefollowingimportantquestionremainsopen.
Isthereapoly-timealgorithmforlearning1hiddenlayerneuralnetworkswhen ğ’Ÿ â€œ ğ‘ p0,ğ¼ ğ‘‘q?
Â§YaleUniversity.
Emails:shuchen.li@yale.edu,ilias.zadik@yale.edu,manolis.zampetakis@yale.edu.
1Followingastandardconvention,werefertoacomputationaltaskasâ€œhardâ€ifitisimpossibileforpolynomial-
timemethods.
1
4202
tcO
4
]GL.sc[
1v77430.0142:viXra2 S.LI,I.ZADIK,M.ZAMPETAKIS
It is known that a single neuron, i.e., 0-hidden layer neural network, can be learned in poly-
nomial time [Zar+24], while neural networks with more that 2 hidden layers are hard to learn
[Che+22]. Nevertheless, the case of 1-hidden layer neural networks is not well understood. In
this paper we close this gap in the literature by answering the question above. We show that it
is hard to learn 1-hidden layer neural networks under Gaussian input assuming the hardness of
some standard cryptographic assumptions. Our result settles an important gap in the computa-
tional complexity of learning neural networks with simple input distributions ğ’Ÿ as we explain
inSection1.2below.
1.1. Prior work. We now provide more details on the literature prior to this work. We first
remind the reader that, formally, polynomial-sized 1-hidden layer neural networks can be ex-
pressed using some width parameter ğ‘˜ â€œ polypğ‘‘q, some weights ğ‘¤ ğ‘– P â„ğ‘‘ and some ğ‘ ğ‘–,ğ‘ ğ‘– P â„
asfollows
Ã¿ğ‘˜
ğ‘“pğ‘¥q â€œ ğ‘ ğ‘–pxğ‘¥ ğ‘–,ğ‘¤ ğ‘–y`ğ‘ ğ‘–q `.
ğ‘–â€œ1
Nowforthisclassofsinglehiddenlayerneuralnetworks,apowerfulalgorithmictoolboxhasbeen
created under the Gaussian input assumption including the works of [JSA15, Bru+17, GLM17,
Zho+17,AZLL18,Zha+19,BJW19,Dia+20,ATV21,SZB21]. Interestinglymostoftheseproposed
algorithmic constructions assume the so-called â€œrealizableâ€ (or noiseless) case where ğœ â€œ 0. Yet,
withtheimportantexceptionofthebrittlelattice-basedmethodsusedin[SZB21],thetechniques
usedarecustomarilyexpectedtobeatleastlymildlyrobusttonoise,andinparticulargeneralize
to the most realistic case where ğœ is positive but polynomially small. Another yet significantly
moreconcerningrestrictionoftheabovepositiveresultsisthattheyallrequiresomeassumptions
on the weights. For example, a common such required assumption is that the weights ğ‘¤ ğ‘–,ğ‘– â€œ
1,2,...,ğ‘š are linearly independent (see e.g., [ATV21] and references therein). It is natural to
wonderwhetherrequiringanysuchassumptionisnecessaryforanypolynomial-timeestimator
to learn the class of one hidden layer neural networks, or simply an artifact of the employed
techniques.
Inthatdirection,researchershavemanagedtoestablishcertainunconditionalandconditional
lower bounds for this problem. Specifically, in terms of conditional lower bounds, [Goe+20] and
[Dia+20] proved that under a worst-case choice of weights the class of the so-called correlation
Statistical Query (cSQ) methods (containing e.g., gradient descent with respect to the squared
loss)failstolearntheclassofonehiddenlayerneuralnetworkswithsuper-constantwidtheven
in the noiseless regime. With respect to unconditional lower bounds, [SZB21] has proven that
under cryptographic assumptions (specifically the continuous Learning with Errors (CLWE) as-
sumption),ifthenoisepersampleğœ‚
ğ‘–
isallowedtobepolynomiallysmallbutadversariallychosen
(andnotGaussian)thennopolynomial-time ğ‘“Ë† cansucceed2. Althoughbotharequiteinteresting
results, they unfortunately come with their drawbacks. The cSQ model is known in many set-
tings to be underperforming compared to multiple other natural polynomial-time methods (see
e.g.,[And+19,CKM22,Dam+24]),whicharguablylimitsthegeneralityofsuchanunconditional
2Formally,thelowerboundin[SZB21]isaboutthecosineactivationfunction(andnottheReLUweassumein
thiswork). Yet,standardapproximationresults[Bac17]cantransfertheresulttoonehiddenlayerneuralnetworks
atthecostofextrapolynomiallysmalladditiveapproximationerror(seealso[SZB21,AppendixE])ONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 3
lower bound. Moreover, while [SZB21] is now a lower bound against all polynomial-time algo-
rithms one could argue that the computational hardness arises exactly because of the addition
of adversarial noise and may not be inherent to learning one hidden layer neural networks. In
particular, as we mentioned in our main question above, it remains an intriguing open problem
intheaboveliteraturewhethersomepolynomial-timeestimatorcaninfactlearnthewholeclass
ofpolynomial-sizeonehiddenlayerneuralnetworksundersmallGaussiannoise.
We note that a cleaner hardness picture has been established when the neural networks have
at least two hidden layers. [DV21] has proven, via an elegant lifting technique, that crypto-
graphic assumptions (specifically the existence of a local pseudorandom generator with polyno-
mial stretch) imply that learning three hidden layers neural networks is computationally hard
even in the noiseless case where ğœ â€œ 0. Moreover, [Che+22] built on the lifting technique of
[DV21]andprovedthatunderdifferentcryptographicassumptions(specificallythelearningwith
rounding assumption) that learning two hidden layers neural networks is also computationally
hardagaininthenoiselesscase. Ontopofthat,[Che+22]alsoprovedageneralStatisticalQuery
(SQ)lowerboundinthiscaseoftwohiddenlayerneuralnetworks. Albeitthesepowerfulrecent
results,itremainselusivewhetherasimilartechniquecanprovethehardnessforthemorebasic
case of one hidden layer neural networks, something also highlighted as one of the main open
questionsin[Che+22].
1.2. Contribution. In this work, we establish that under the CLWE assumption from cryptog-
raphy [Bru+21] learning the class of one hidden layer neural network with polynomial small
Gaussian noise is indeed computationally hard. Importantly, solving CLWE in polynomial-time
implies a polynomial-time quantum algorithm that approximates within polynomial factors the
worst-case Gap shortest vector problem (GapSVP), a widely believed hard task in cryptography
and algorithmic theory of lattices [MR09]. Interestingly, our lower bound holds even under the
requirement of weakly learning the neural network. We present our findings in the following
informaltheorem.
Theorem 1.1 (Informal; see Theorem 5.4). Let â„± ğ‘˜ the class of widthağ‘˜ one hidden layer neural
networks and arbitrary noise variance ğœ â€œ 1{polypğ‘‘q. For any ğ‘˜ â€œ ğœ”p ğ‘‘logğ‘‘q, if there exists a
polynomial-timealgorithmthatcanweaklylearn â„± ğ‘˜ underGaussiannoiseofvariance ğœ thenthere
existsapolynomial-timequantumalgorithmthatapproximatesGapSVPwithinapolypğ‘‘qfactor.
The above result settles the computational question of learning one hidden layer neural net-
works under polynomially small Gaussian noise. It is perhaps natural to wonder if we can also
obtainalowerboundagainstevensmallerlevelsofnoise.
First,wehighlightthataswealsomentionedintheIntroduction,thisisalreadyasignificantly
smallamountofnoise;mostnaturalalgorithmicschemesinlearningtheoryaremildlyrobustto
noise, and therefore they can tolerate polynomially-small levels of Gaussian noise (if not a con-
stant level). That being said, we also mention that one can prove a more general version of our
resultbycombiningthereductionsbetweenCLWEandclassicalLWE[GVV22];ifapolynomial-
a
time estimator can weakly learn â„± ğ‘˜ for some ğœ”p ğ‘‘logğ‘‘ Â¨logpğ‘‘{ğœqq â€œ ğ‘˜ â€œ polypğ‘‘q under
Gaussiannoiseofarbitrary variance ğœ â€œ ğœ ğ‘‘ suchthatlogp1{ğœq â€œ polypğ‘‘q,thenthereexistsalso
a polynomial-time quantum algorithm that approximates GapSVP within a factor polyp1{ğœ,ğ‘‘q.
Inparticular,giventhatthecurrentstate-of-the-artalgorithmforGapSVPremainssince1982the4 S.LI,I.ZADIK,M.ZAMPETAKIS
celebratedLenstra-Lenstra-LovaÂ´sz(LLL)latticebasisreductionalgorithm[LLL82]whichhasap-
proximationfactorexppÎ˜pğ‘‘qq,weprovethatanylearningalgorithmforonehiddenlayerneural
?
networkssucceedingforanyexppÂ´ğ‘œp ğ‘‘qq Ä ğœwouldimmediatelyimplyamajorbreakthrough
in the algorithmic theory of lattices (see Section 6 for a lengthier discussion on this and more
detailsonthisconnection).
The only case that is left open by our results is that some very brittle algorithm can learn
in polynomial-time the class of one hidden layer neural networks (only) for exponentially small
valuesofğœ. Infact,thatisproventobethecaseusingthebrittleLLL-basedmethodsforthecaseof
cosineneuronin[SZB21],andformultipleotherâ€œnoiselessâ€settingsintherecentlearningtheory
literature [And+17, ZG18, GKZ21, Zad+22, DK22]. Yet, while we believe this is an interesting
and potentially highly non-trivial theoretical question, the value of any such brittle algorithmic
method in learning or statistics is unfortunately unclear since a non-negligible amount of noise
alwaysexistsinthesecases.
1.3. Organization. WebegininSection2withthetheformulationofPAC-learningneuralnet-
works and the necessary preliminaries on lattice-based cryptography that we utilize to present
our hardness result. Then in Section 3 we state formally our main result and we provide a proof
sketch. In Sections 4 and 5 we provide the proof of our result in two steps. First, we show the
hardnessoflearninganysingleperiodicneuralnetwork,andthenweshowhowthisimpliesthe
hardnessoflearning1-hiddenlayerneuralnetworks.
2. Preliminaries
2.1. Notations. Throughout the paper we use the standard asymptotic notation, ğ‘œ,ğœ”,ğ‘‚,Î˜,Î©
for comparing the growth of two positive sequences pğ‘ ğ‘‘qğ‘‘Pâ„• and pğ‘ ğ‘‘qğ‘‘Pâ„•: we say ğ‘ ğ‘‘ â€œ Î˜pğ‘ ğ‘‘q
if there is an absolute constant ğ‘ Ä… 0 such that 1{ğ‘ Ä ğ‘ ğ‘‘{ğ‘ ğ‘‘ Ä ğ‘; ğ‘ ğ‘‘ â€œ Î©pğ‘ ğ‘‘q or ğ‘ ğ‘‘ â€œ ğ‘‚pğ‘ ğ‘‘q if
there exists an absolute constant ğ‘ Ä… 0 such that ğ‘ ğ‘‘{ğ‘ ğ‘‘ Ä› ğ‘; and ğ‘ ğ‘‘ â€œ ğœ”pğ‘ ğ‘‘q or ğ‘ ğ‘‘ â€œ ğ‘œpğ‘ ğ‘‘q if
lim ğ‘‘ ğ‘ ğ‘‘{ğ‘ ğ‘‘ â€œ 0. We say ğ‘¥ â€œ polypğ‘‘q if for some ğ‘Ÿ Ä… 0 it holds ğ‘¥ â€œ ğ‘‚pğ‘‘ğ‘Ÿ q. Let ğ‘pğœ‡,ğœ2 q denote
the Gaussian distribution with mean ğœ‡ and variance ğœ2, and ğ‘pğœ‡,Î£q denote the multivariate
Gaussiandistributionwithmeanğœ‡andcovarianceÎ£.
2.2. PAC-learningwithGaussianinputdistribution. Ourfocusonthisworkistheproblem
oflearningasequenceofreal-valuedfunctionclassestâ„± ğ‘‘uğ‘‘Pâ„•,eachoverthestandardGaussian
inputdistributiononâ„ğ‘‘. Theinputisamultisetofi.i.d.labeledexamplespğ‘¥,ğ‘¦q P â„ğ‘‘ Ë†â„,where
ğ‘¥ â€ ğ‘p0,ğ¼ ğ‘‘q, ğ‘¦ â€œ ğ‘“pğ‘¥q`ğœ‰, ğ‘“ P â„± ğ‘‘, and ğœ‰ â€ ğ‘p0,ğœ2 q for some ğœ2 Ä… 0. We denote by ğ· â€œ ğ· ğ‘“
theresultingdatadistribution. Thegoalofthelearneristooutputanhypothesis â„ : â„ğ‘‘ Ã‘ â„that
isclosetothetargetfunction ğ‘“ inthesquaredlosssenseovertheGaussianinputdistribution.
Throughout the paper we define â„“ : â„ Ë† â„ Ã‘ â„ Ä›0 the squared loss function defined by
â„“pğ‘¦,ğ‘§q â€œ pğ‘¦Â´ğ‘§q2. Foragivenhypothesis â„ andadatadistribution ğ· onpairspğ‘¥,ğ‘§q P â„ğ‘‘ Ë†â„,
wedefineitspopulationloss ğ¿ ğ·pâ„qoveradatadistribution ğ· by
ğ¿ ğ·pâ„q â€œ ğ”¼ pğ‘¥,ğ‘¦qâ€ğ·râ„“pâ„pğ‘¥q,ğ‘¦qs . (2.1)
Wenowdefinetheimportantnotionofweaklearning.
Definition 2.1 (Weak learning). Let ğœ€ â€œ ğœ€pğ‘‘q Ä… 0 be a sequence of numbers, ğ›¿ P p0,1q a fixed
constant, and let tâ„± ğ‘‘uğ‘‘Pâ„• be a sequence of function classes defined on input space â„ğ‘‘. We say
that a (randomized) learning algorithm ğ’œ ğœ€-weakly learns tâ„± ğ‘‘uğ‘‘Pâ„• over the standard GaussianONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 5
distribution if for every ğ‘“ P â„± ğ‘‘ the algorithm outputs a hypothesis â„ ğ‘‘ such that for large values
of ğ‘‘ withprobabilityatleast1Â´ ğ›¿
ğ¿ ğ· pâ„ ğ‘‘q Ä ğ¿ ğ· pğ”¼rğ‘“pğ‘¥qsqÂ´ ğœ€ .
ğ‘“ ğ‘“
Notethatğ”¼
ğ‘¥â€ğ‘p0,ğ¼
ğ‘‘qrğ‘“pğ‘¥qsisthebestpredictoragnostictotheinputdatainthissetting. Hence,
we refer to ğ¿ ğ·pğ”¼rğ‘“pğ‘¥qsq â€œ Var ğ‘â€ğ‘p0,ğ¼ ğ‘‘qpğ‘“pğ‘qq, as the trivial loss, and ğœ€ as the edge of the
learningalgorithm.
For simplicity, we refer to an hypothesis as weakly learning a function class if it can achieve
edge ğœ€ which is depending inverse polynomially in ğ‘‘. Moreover, we simply set from now on
ğ›¿ â€œ 1{3whenwerefertoweaklearning.
2.3. Worst-Case Lattice Problems. Some background on lattice problems is required for our
work. Westartwiththedefinitionofalattice.
Definition2.2. Givenlinearlyindependent ğ‘ 1,...,ğ‘ ğ‘‘ P â„ğ‘‘,let
# +
Ã¿ğ‘‘
Î› â€œ Î›pğ‘ 1,...,ğ‘ ğ‘‘q â€œ ğœ† ğ‘–ğ‘ ğ‘– : ğœ† ğ‘– P â„¤,ğ‘– â€œ 1,...,ğ‘‘ , (2.2)
ğ‘–â€œ1
whichwerefertoasthelatticegeneratedby ğ‘ ,...,ğ‘ .
1 ğ‘‘
Acoreworst-casedecisionalgorithmicproblemonlatticesisGapSVP.InGapSVP,wearegiven
an instance of the form pÎ›,ğ‘¡q, where Î› is a ğ‘‘-dimensional lattice and ğ‘¡ P â„, the goal is to
distinguish between the case where ğœ† 1pÎ›q, the â„“ 2-norm of the shortest non-zero vector in Î›,
satisfies ğœ† 1pÎ›q Äƒ ğ‘¡ from the case where ğœ† 1pÎ›q Ä› ğ›¼pğ‘‘qÂ¨ğ‘¡ for some â€œgapâ€ ğ›¼pğ‘‘q Ä› 1. We refer to
anysuchsuccessfulalgorithmassolvingGapSVPwithinan ğ›¼pğ‘‘qfactor.
GapSVPisknowntobeNP-hardforâ€œalmostâ€polynomialapproximationfactors,thatis,2plogğ‘‘q1Â´ğœ€
foranyconstantğœ€ Ä… 0,assumingproblemsinNPcannotbesolvedinquasi-polynomialtime[Kho05,
HR07]. Moreover, importantly for this work, GapSVP is strongly believed to be computationally
hard (even with quantum computation), for any polynomial approximation factor ğ›¼pğ‘‘q [MR09],
asdescribedinthefollowingconjecture.
Conjecture 2.3 ([MR09, Conjecture 1.2]). There is no polynomial-time quantum algorithm that
solvesGapSVPtowithinpolynomialfactors.
Wecommentontheversionofthisconjectureforsuper-polynomialfactorsinSection6.
2.4. ContinuousLearningwithErrors(CLWE)[Bru+21]. Ofcrucialimportancetousisthe
CLWEdecision(ordetection)problem. WedefinetheCLWEdistributionCLWE ondimension
ğ›½,ğ›¾
ğ‘‘ with frequency ğ›¾ â€œ ğ›¾pğ‘‘q Ä› 0, and noise rate ğ›½ â€œ ğ›½pğ‘‘q Ä› 0 to be the distribution of i.i.d.
samples of the form pğ‘¥,ğ‘§q P â„ğ‘‘ Ë† rÂ´1{2,1{2q where ğ‘¥ â€ ğ‘p0,ğ¼ ğ‘‘q,ğœ‰ â€ ğ‘p0,ğ›½q, ğ‘¤ uniformly
chosenfromthesphereğ’®ğ‘‘Â´1 and
ğ‘§ â€œ ğ›¾xğ‘¥,ğ‘¤y`ğœ‰ mod 1 . (2.3)
Notethatforthemod1operation,wetaketherepresentativesinrÂ´1{2,1{2q. TheCLWEproblem
consists of detecting between i.i.d. samples from the CLWE distribution or the null distribution
ğ‘p0,ğ¼ ğ‘‘qË†ğ‘ˆprÂ´1{2,1{2qqwhichwedenoteby ğ´ 0.6 S.LI,I.ZADIK,M.ZAMPETAKIS
Given ğ›¾ â€œ ğ›¾pğ‘‘q and ğ›½ â€œ ğ›½pğ‘‘q, we consider a sequence of decision problems tCLWE ğ›½,ğ›¾uğ‘‘Pâ„•,
indexed by the input dimension ğ‘‘, in which the learner receives ğ‘š samples from an unknown
distribution ğ· such that either ğ· â€œ ğ´ ğ›½,ğ›¾ or ğ· â€œ ğ´ 0. We consider the classical hypothesis
testing setting that we aim to construct a polynomial-time binary-valued testing algorithm ğ’œ
which uses as input the samples and distinguishes the two distributions. Specifically, ğ’œ takes
valuesintğ´ ğ›½,ğ›¾,ğ´ 0uandseekstooutputâ€œğ´ ğ›½,ğ›¾â€when ğ· â€œ ğ´ ğ›½,ğ›¾ andâ€œğ´ 0â€when ğ· â€œ ğ´ 0. Under
thissetup,wedefinetheadvantage of ğ’œ tobethefollowingdifference,
Ë‡ Ë‡
Ë‡ Ë‡
Ë‡Pğ‘¥â€pğ´ ğ›½,ğ›¾qbğ‘šrğ’œpğ‘¥q â€œ ğ´ 0sÂ´P ğ‘¥â€ğ´bğ‘šrğ’œpğ‘¥q â€œ ğ´ 0sË‡ .
0
Note that the advantage simply equals to one minus the sum of the type I and type II errors in
statistical terminology. We call the advantage non-negligible if it decays at most polynomially
fasti.e.,itisÎ©pğ‘‘Â´ğ¶ qforsome ğ¶ Ä… 0.
[Bru+21]providedworst-caseevidencebasedonthehardnessofGapSVP(Conjecture2.3)that
solving the CLWE decision problem with non-negligible advantage is computationally hard for
?
any ğ›½ â€œ 1{polypğ‘‘q as long as ğ›¾ Ä› 2 ğ‘‘. This is an immediate corollary of the result below
combinedwithConjecture2.3.
?
Theorem 2.4 ([Bru+21, Corollary 3.2]). Let ğ›½ â€œ ğ›½pğ‘‘q â€œ 1{polypğ‘‘q and 2 ğ‘‘ Ä ğ›¾ â€œ ğ›¾pğ‘‘q â€œ
polypğ‘‘q. Then,ifthereexistsapolynomial-timealgorithmforCLWE ğ›½,ğ›¾ withnon-negligibleadvan-
tage,thenthereisapolynomial-timequantumalgorithmforsolvesGapSVPwithinpolypğ‘‘qfactors.
Forsimplicity,wesaythatsomealgorithmâ€œsolvesCLWEâ€torefertothefactthatthealgorithm
hasnon-negligibleadvantageforthedecisionversionofCLWE.
3. MainResult
Webeginwithdefiningtheclass â„±NN ofonehiddenlayerneuralnetworks
ğ‘˜
$ ,
& Ã¿ğ‘˜ .
â„± ğ‘˜NN â€œ %ğ‘“ ğ‘Š,ğ‘pğ‘¥q â€œ ğ‘ ğ‘—pxğ‘¤ ğ‘—,ğ‘¥y`ğ‘ ğ‘—q
`
| ğ‘ P â„ğ‘˜,ğ‘Š P â„ğ‘‘Ë†ğ‘˜,ğ‘ P â„ğ‘˜ -.
ğ‘—â€œ1
Ourmainresultisthefollowing.
a
Theorem 3.1. Let ğ‘‘ P â„•, and arbitrary ğœ â€œ polypğ‘‘qÂ´1 , ğœ€ â€œ polypğ‘‘qÂ´1 , and ğ‘˜ â€œ ğœ”p ğ‘‘logğ‘‘q.
Thenapolynomial-timeestimatorthatğœ€-weaklylearnsthefunctionclassâ„±NNoverGaussianinputs
ğ‘˜
ğ‘¥ i. â€i.d. ğ‘p0,ğ¼ ğ‘‘qunderGaussiannoiseğœ‰ i. â€i.d. ğ‘p0,ğœ2 qimpliesapolynomial-timequantumalgorithm
thatapproximatesGapSVPtowithinpolynomialfactors.
NoticethatdirectlyfromourTheorem3.1andthewidelybelievedConjecture2.3wecancon-
clude that no polynomial-time estimator can weakly learn the class of one hidden layer neural
networksunderarbitrarypolynomiallysmallGaussiannoise.
3.1. ProofSketchandComparisonwith[SZB21]. Our(simple)proofisanappropriatecom-
bination of two key steps. We first establish in Section 4 that solving the CLWE problem re-
ducestolearningLipschitzperiodicneuronsunderpolynomiallysmallGaussiannoise(seeTheo-
rem4.2). Thisisadirectimprovementuponthekeyresult[SZB21,Theorem3.3]thatestablishes
that CLWE reduces to learning Lipschitz periodic neurons under polynomially small adversarialONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 7
noise. Our approach is to perhaps interestingly show that one can â€œGaussianizeâ€ the adversarial
noiseinthelabelsgeneratedviathereductionfollowedby[SZB21]bysimplyinjectingadditional
Gaussiannoiseofappropriatevariancetothem(seeLemma4.1).
Recall that, using standard approximation results [SZB21, Appendix E], learning 1-Lipschitz
periodic neurons under polynomially small adversarial noise is equivalent with learning one
hidden layer neural networks of appropriate polynomial width under (slightly larger) polyno-
mially small adversarial noise. Unfortunately, we cannot straightforwardly generalize this logic
toGaussianerrorsusingourfirststep,becausetheinducedapproximationerrorcaninprinciple
be too large for our â€œGaussianizationâ€ lemma to work. Regardless, instead of using approxima-
tion results, in Section 5, we follow a more direct route and prove that for any arbitrary large
bounded interval rÂ´ğ‘…,ğ‘…s one can explicitly construct an appropriate Lipschitz periodic neuron
and a polynomial-width neural network that exactly agree on rÂ´ğ‘…,ğ‘…s, i.e., have zero â€œapproxi-
mationâ€error(seeLemma5.1). ThislemmacombinedwithourfirststepTheorem4.2allowusto
reduceCLWEtolearningonehiddenlayerneuralnetworksunderGaussiannoise. Combiningthe
abovewiththereductionfromGapSVPtoCLWE(Theorem2.4)letusthenconcludeTheorem3.1.
4. CLWEreductiontoLipschitzPeriodicNeuronsunderGaussiannoise
We first recall the notion of Lipschitz periodic neurons from [SZB21]. Let ğ›¾ â€œ ğ›¾pğ‘‘q Ä… 1 be
a sequence of numbers indexed by the input dimension ğ‘‘ P NN, and let ğœ™ : â„ Ã‘ rÂ´1,1s be a
ğœ™
1-Lipschitzand1-periodicfunction. Wedenoteby â„± thefunctionclass
ğ›¾
â„± ğ›¾ğœ™ â€œ tğ‘“ : â„ğ‘‘ Ã‘ rÂ´1,1s | ğ‘“pğ‘¥q â€œ ğœ™pğ›¾xğ‘¤,ğ‘¥yq,ğ‘¤ P ğ‘†ğ‘‘Â´1 u (4.1)
Notethateachmemberofthefunctionclassâ„± ğ›¾ğœ™ isfullycharacterizedbyaunitvectorğ‘¤ P ğ‘†ğ‘‘Â´1.
WerefersuchfunctionclassesasLipschitzperiodicneurons.
[SZB21] has established that solving CLWE reduces to learning in polynomial-time the class
ğœ™
â„± under polynomially small adversarial noise. Their reduction is very simple; given a CLWE
ğ›¾
pğ‘¥,ğ›¾xğ‘¤,ğ‘¥y`ğœ‰ mod 1qonecancreateasamplepğ‘¥,ğœ™pğ›¾xğ‘¤,ğ‘¥y`ğœ‰qbyapplying ğœ™ since ğœ™ is1-
periodic. Butsinceğœ™is1-Lipschitzandğœ‰isâ€œsmallâ€,noticeğœ™pğ›¾xğ‘¤,ğ‘¥y`ğœ‰q â€œ ğœ™pğ›¾xğ‘¤,ğ‘¥yq`ğœ‰1for
someğœ‰1 alsoâ€œsmallâ€as|ğœ‰1| Ä |ğœ‰|.Hence,theauthorsof[SZB21]constructfromaCLWEsample,
ğœ™
a sample from the Lipschitz periodic neuron class â„± , but under the somewhat cumbersome
ğ›¾
noise variable ğœ‰1 which we can only control its magnitude â€“ for this reason ğœ‰1 is referred to as
smalladversarialnoisein[SZB21].
Our first step is to improve upon [SZB21] and construct instead a sample from the Lipschitz
ğœ™
periodic neuron class â„± , but under simply Gaussian noise ğœ‰1. Our idea to do so is to simply
ğ›¾
inject additional small Gaussian noise to ğœ™pğ›¾xğ‘¤,ğ‘¥y`ğœ‰q. We prove that as long as the variance
of the added noise is of slightly larger magnitude than the magnitude of the (already polynomi-
allysmall)noise ğœ‰,intotalvariationdistancethesampleapproximatelyequalsindistributionto
ğœ™pğ›¾xğ‘¤,ğ‘¥yq`ğœ‰1 wherenow ğœ‰1 isGaussian. Thisresultisdescribedinthefollowinglemma.
Lemma 4.1. Let ğœ™ : â„ Ã‘ â„ be an 1-Lipschitz function. For fixed ğ›¾ Ä… 0 and ğ‘¤ P ğ‘†ğ‘‘Â´1 , and
ğ‘¥ â€ ğ‘p0,ğ¼ ğ‘‘q,ğœ‰ 0 â€ ğ‘p0,ğ›½q,ğœ‰ â€ ğ‘p0,ğœ2 q,thetotalvariationdi ?stancebetweenthedistributionsof
ğ›½
pğ‘¥,ğœ™pğ›¾xğ‘¤,ğ‘¥y`ğœ‰ 0q`ğœ‰qandpğ‘¥,ğœ™pğ›¾xğ‘¤,ğ‘¥yq`ğœ‰qisatmost ? .
2ğœ‹ğœ8 S.LI,I.ZADIK,M.ZAMPETAKIS
Proof. Since the first entries of the two pairs are the same, it suffices to upper bound the total
variancedistancebetweenthedistributionsofğ‘§ 1 â€œ ğœ™pğ›¾xğ‘¤,ğ‘¥y`ğœ‰ 0q`ğœ‰andğ‘§ 2 â€œ ğœ™pğ›¾xğ‘¤,ğ‘¥yq`ğœ‰
conditioning on ğ‘¥. Note that conditioning on ğœ‰
0
and ğ‘¥, the distribution of ğ‘§
1
is ğ‘pğœ™pğ›¾xğ‘¤,ğ‘¥y`
ğœ‰ 0q,ğœ2 qandthedistributionof ğ‘§
2
is ğ‘pğœ™pğ›¾xğ‘¤,ğ‘¥yq,ğœ2 q. Thus,
Â«c ff
KLpğ‘§ 1|pğœ‰ 0,ğ‘¥q}ğ‘§ 2|pğœ‰ 0,ğ‘¥qq
TVpğ‘§ 1|ğ‘¥,ğ‘§ 2|ğ‘¥q Ä ğ”¼ ğœ‰ 0â€ğ‘p0,ğ›½qrTVpğ‘§ 1|pğœ‰ 0,ğ‘¥q,ğ‘§ 2|pğœ‰ 0,ğ‘¥qqs Ä ğ”¼ ğœ‰ 0â€ğ‘p0,ğ›½q 2
â€ È· â€ È· a
|ğœ™pğ›¾xğ‘¤,ğ‘¥y`ğœ‰ 0qÂ´ ğœ™pğ›¾xğ‘¤,ğ‘¥yq| |ğœ‰ 0| ğ›½
â€œ ğ”¼ ğœ‰ 0â€ğ‘p0,ğ›½q 2ğœ Ä ğ”¼ ğœ‰ 0â€ğ‘p0,ğ›½q 2ğœ â€œ ? 2ğœ‹ğœ,
where the first inequality is from the triangle inequality, the second inequality is from Pinskerâ€™s
inequality,theequalityinthethirdlineisfromtheKLdivergencebetweentwosingledimensional
Guassians,andthelastinequalityisfromthe1-Lipschitzcontinuityof ğœ™. â–¡
Lemma4.1allowustoestablishthefollowingkeyCLWEhardnessresultforLipschitzperiodic
neurons.
a
Theorem4.2. Letğ‘‘ P â„•,ğ›¾ â€œ ğœ”p logğ‘‘q,ğœ P p0,1q,ğœ€ â€œ polypğ‘‘qÂ´1 ,ğ‘š 1 â€œ polypğ‘‘q. Moreover,let
ğœ™ : â„ Ã‘ rÂ´1,1sbean1-Lipschitz,1-periodicfunction. Then,apolynomial-timelearningalgorithm
using ğ‘š 1 samples that ğœ€-weakly learns the function class â„± ğ›¾ğœ™ over Gaussian inputs ğ‘¥ i. â€i.d. ğ‘p0,ğ¼ ğ‘‘q
and under Gaussi!an label no)ise ğœ‰ i. â€i.d. ğ‘p0,ğœ2 q implies a polynomial-time algorithm for CLWEğ›½,ğ›¾
forany ğ›½ Ä min ğœ2 , ğœ€2 .
104ğ‘š2 103
1
WedefertheproofofthetheoremtoSection4.1. NoticethatadirectcorollaryofTheorem4.2is
ğœ™
thataweaklearningalgorithmfortheclassâ„± ,impliesaquantumalgorithmforapproximating
ğ›¾
GapSVPwithingpolynomialfactors.
?
Corollary4.3. Letğ‘‘ P â„•,ğ›¾ â€œ polypğ‘‘qwithğ›¾ Ä› 2 ğ‘‘,ğœ â€œ polypğ‘‘qÂ´1 ,ğœ€ â€œ polypğ‘‘qÂ´1 . Moreover,
let ğœ™ : â„ Ã‘ rÂ´1,1s be an 1-Lipschitz 1-periodic function. Then, a polynomial-time algorithm that
ğœ€-weakly learns the function class â„± ğ›¾ğœ™ â€œ tğ‘“ ğ›¾,ğ‘¤pğ‘¥q â€œ ğœ™pğ›¾xğ‘¤,ğ‘¥yq | ğ‘¤ P ğ‘†ğ‘‘Â´1 u over Gaussian
inputs ğ‘¥ i. â€i.d. ğ‘p0,ğ¼ ğ‘‘q under Gaussian noise ğœ‰ i. â€i.d. ğ‘p0,ğœ2 q implies a polynomial-time quantum
algorithmthatapproximatesGapSVPtowithinpolynomialfactors.
Proof. Sincetheweaklearning !algorithm )runsinpolynomialtime,thenumberofsamplesituses
is ğ‘š 1 â€œ polypğ‘‘q. Let ğ›½ â€œ min 10ğœ 4ğ‘š2 2, 1ğœ€ 02 3 â€œ polypğ‘‘qÂ´1. By Theorem 4.2, there is a polynomial-
1 ?
time algorithm for CLWE ğ›½,ğ›¾. Moreover, since ğ›½ â€œ polypğ‘‘qÂ´1 and 2 ğ‘‘ Ä ğ›¾ â€œ polypğ‘‘q, from
Theorem 2.4, there is a polynomial-time quantum algorithm that solves GapSVP within polypğ‘‘q
factors. â–¡
4.1. ProofofTheorem4.2.
Proof. We begin with introducing some definitions and notation about CLWE and about weak
ğœ™
learnersfortheclass â„± thatwillbeusefultousduringtheproof.
ğ›¾
CLWE: We denote by ğ‘ƒ 0 the CLWE distribution, i.e., samples pğ‘¥ ğ‘–,ğ‘§ ğ‘– â€œ ğ›¾xğ‘¤,ğ‘¥ ğ‘–y`ğœ‰ 0,ğ‘– mod 1q
where ğ‘¤ â€ ğ‘ˆpğ‘†ğ‘‘Â´1 q, ğ‘¥ ğ‘– â€ ğ‘p0,ğ¼ ğ‘‘q, and ğœ‰ 0,ğ‘– â€ ğ‘p0,ğ›½q. Let ğ‘„ 0 denote the null distribution,
i.e., samples pğ‘¥ ğ‘–,ğ‘¦ ğ‘–q where ğ‘¥ ğ‘– â€ ğ‘p0,ğ¼ ğ‘‘q, and ğ‘¦ ğ‘– â€ ğ‘ˆr0,1s. For some appropriately largeONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 9
ğ‘š 2 â€œ polypğ‘‘q that will be determined in the proof, we study whether a polynomial-time
algorithm for CLWE can distinguish between ğ‘š i.i.d. samples from ğ‘ƒ and ğ‘š i.i.d. samples
ğ›½,ğ›¾ 0
from ğ‘„ 0,for ğ‘š â€œ ğ‘š 1 `ğ‘š 2 â€œ polypğ‘‘q,withnon-negligibleadvantage.
Foralabeledsamplepğ‘¥,ğ‘¦q P â„ğ‘‘ Ë†â„,wedefine
ğ¹ ğœ‰pğ‘¥,ğ‘¦q â€œ pğ‘¥,ğœ™pğ‘¦q`ğœ‰q. (4.2)
Let ğ‘ƒ 1 and ğ‘„ 1 denotethedistributionsof ğ‘ƒ 0 and ğ‘„ 0 afterapplying ğ¹ ğœ‰ for ğœ‰ â€ ğ‘p0,ğœ2 q.
ğœ™
WeakLearning â„± ğ›¾ : Let ğ‘ƒ ğœ™ denote the distribution of the samples pğ‘¥,ğœ™pğ›¾xğ‘¤,ğ‘¥yq ` ğœ‰q. For
ğœ€ â€œ polypğ‘‘qÂ´1 a weak learner for ğ‘ƒ ğœ™ is a polynomial time learning algorithm ğ’œ that take as
input ğ‘š 1 samples from ğ‘ƒ ğœ™ and with probability 2{3 outputs a hypothesis â„1 : â„ Ã‘ â„ such
Ëœ
that ğ¿ ğ‘ƒ pâ„1q Ä ğ¿ ğ‘ƒ pğ”¼rğ‘“ ğ›¾,ğ‘¤pğ‘¥qsqÂ´ğœ€. Sinceweareusingthesquaredloss, â„pğ‘¥q â€œ sgnpâ„1pğ‘¥qqÂ¨
ğœ™ ğœ™
minp|â„1pğ‘¥q|,1q is always no worse than â„1pğ‘¥q, since ğœ™pğ‘¥q P rÂ´1,1s and the noise on the label
isunbiased. Thus,wecanassumewithoutlossofgeneralitythat â„1pğ‘¥q P rÂ´1,1s.
ğœ™
Our goal in this proof is to assume access to a weak learner for the function class â„± with
ğ›¾
ğ‘š samples, and design an algorithm for solving the CLWE problem (as defined above) with
1
ğ‘š â€œ ğ‘š 1 ` ğ‘š 2 for an appropriate number of additional samples ğ‘š 2 â€œ polypğ‘‘q. More precisely,
we want to design an efficient algorithm â„¬ to distinguish between a set of samples from ğ‘ƒ and
0
a set of samples from ğ‘„ 0 with ğ‘š â€œ ğ‘š 1 ` ğ‘š 2,ğ‘š 2 â€œ polypğ‘‘q samples, using a weak learning
ğœ™
algorithm ğ’œ for â„± with ğ‘š samples.
ğ›¾ 1
Definition of Algorithm â„¬. For a sufficiently large ğ‘š 2 â€œ polypğ‘‘q for the purposes of the proof
the follows, we are given ğ‘š â€œ ğ‘š 1 `ğ‘š 2 â€œ polypğ‘‘q i.i.d. samples tpğ‘¥ ğ‘–,ğ‘§ ğ‘–quğ‘š ğ‘–â€œ1 from an unknown
distribution ğ·,whichiseither ğ‘ƒ or ğ‘„ ,algorithm â„¬ followsthefollowingsteps.
0 0
(1) Sample ğœ‰ ğ‘– i. â€i.d. ğ‘p0,ğœ2 q, ğ‘– â€œ 1,2,...,ğ‘š.
(2) Foreachğ‘– â€œ 1,2,...,ğ‘š,applyğ¹ ğœ‰ ,definedin(4.2),topğ‘¥ ğ‘–,ğ‘§ ğ‘–qtogetasamplepğ‘¥ ğ‘–,ğ‘  ğ‘–qfrom
ğ‘–
ğ· ,whichiseither ğ‘ƒ or ğ‘„ .
1 1 1
(3) Run ğ’œ onthefirst ğ‘š 1 ofthesamplesfrom ğ· 1,andlet â„ : â„ Ã‘ rÂ´1,1sbethehypothesis
that ğ’œ outputs.
(4) Generate ğ‘š samplesfrom ğ‘„ .
2 1
(5) Compute the empirical loss ğ¿Ë†
ğ·
pâ„q of â„ on the remaining ğ‘š
2
samples from ğ· 1, and the
1
empiricalloss ğ¿
ğ‘„
pâ„qonthe ğ‘š
2
samplesgeneratedfrom ğ‘„ 1.
1
(6) Testwhether ğ¿Ë† ğ· pâ„q Ä ğ¿Ë† ğ‘„ pâ„qÂ´ ğœ€{5ornot.
1 1
(7) Intheend,conclude ğ· â€œ ğ‘ƒ 0 if â„ passesthetestinstep(6)and ğ· â€œ ğ‘„ 0 otherwise.
Proof of Correctness of â„¬. Next we prove the correctness of this algorithm â„¬ assuming the cor-
rectness of ğ’œ and using Lemma 4.1. We first show that if ğ· â€œ ğ‘ƒ 0 then â„ will pass the test
ğ¿Ë† ğ· pâ„q Ä ğ¿Ë† ğ‘„ pâ„qÂ´ ğœ€{5andthenweshowthatif ğ· â€œ ğ‘„ 0 then â„ willfailthistest.
1 1
CaseI:D â€œ P 0. Recallthat â„ and â„1denotetheoutputofğ’œ givenğ‘š 1samplesfromğ· 1 â€œ ğ‘ƒ 1and
ğ‘ƒ respectively,employingthenotationweintroducedabove. Bythedataprocessinginequality,
ğœ™ ` Ë˜
ğ‘‡ğ‘‰pâ„,â„1q Ä TV ğ‘ƒ 1bğ‘š 1,ğ‘ƒ ğœ™bğ‘š 1 Ä ğ‘š 1 Â¨TVpğ‘ƒ 1,ğ‘ƒ ğœ™q, where TVpâ„,â„1q refers to the total variation
betweenthedistributionof â„,andthedistributionof â„1. Hence,byLemma4.1,TVpâ„,â„1qisupper10 S.LI,I.ZADIK,M.ZAMPETAKIS
?
ğ‘š ğ›½
boundedby 1 . Then,
?
2ğœ‹ğœ
Ë‡ Ë‡
Ë‡ Ë‡
Ë‡ Ë‡
Ë‡â„™ â„Ãğ’œpğ‘ƒbğ‘š1 qrğ¿ ğ‘ƒ ğœ™pâ„q Ä ğ¿ ğ‘ƒ ğœ™pğ”¼rğ‘“ ğ›¾,ğ‘¤pğ‘¥qsqÂ´ ğœ€sÂ´â„™ â„1Ãğ’œpğ‘ƒbğ‘š1 qrğ¿ ğ‘ƒ ğœ™pâ„q Ä ğ¿ ğ‘ƒ ğœ™pğ”¼rğ‘“ ğ›¾,ğ‘¤pğ‘¥qsqÂ´ ğœ€sË‡
1 ğœ™
a
ğ‘š 2ğ›½
1
Ä 2TVpâ„,â„1q Ä ? Ä 0.01,
ğœ‹ğœ
sincewehavechosen ğ›½ Ä ğœ2 . Thus,wehavewithprobabilityatleast2{3Â´0.01that
104ğ‘š2
1
ğ¿ ğ‘ƒ pâ„q Ä ğ¿ ğ‘ƒ pğ”¼rğ‘“ ğ›¾,ğ‘¤pğ‘¥qsqÂ´ ğœ€. (4.3)
ğœ™ ğœ™
Note that ğ¿ ğ‘ƒ ğœ™pâ„q â€œ ğ”¼ pğ‘¥,ğ‘§qâ€ğ‘ƒ ğœ™pâ„pğ‘¥q Â´ ğ‘§q2 â€œ ğ”¼ ğ‘ƒ ğœ™pâ„pğ‘¥q Â´ ğ‘“ ğ›¾,ğ‘¤pğ‘¥qq2 ` ğ”¼ğœ‰2, and similarly,
ğ¿ ğ‘ƒ pâ„q â€œ ğ”¼ ğ‘ƒ pâ„pğ‘¥qÂ´ ğœ™pğ›¾xğ‘¤,ğ‘¥y`ğœ‰ 0qq2 `ğ”¼ğœ‰2. Hence,
1 1
Ë‡ Ë‡ Ë‡ Ë‡
Ë‡ Ë‡ Ë‡ Ë‡
Ë‡ğ¿ ğ‘ƒ pâ„qÂ´ğ¿ ğ‘ƒ pâ„qË‡ â€œ Ë‡ğ”¼ ğ‘ƒ pâ„pğ‘¥qÂ´ ğ‘“ ğ›¾,ğ‘¤pğ‘¥qq2 Â´ğ”¼ ğ‘ƒ pâ„pğ‘¥qÂ´ ğœ™pğ›¾xğ‘¤,ğ‘¥y`ğœ‰ 0qq2Ë‡
ğœ™ 1 ğœ™ 1
Ë‡ ` Ë˜` Ë˜Ë‡
Ë‡ Ë‡
â€œ ğ”¼ ğ‘¥,ğœ‰ ğœ™pğ›¾xğ‘¤,ğ‘¥yqÂ´ ğœ™pğ›¾xğ‘¤,ğ‘¥y`ğœ‰ 0q ğœ™pğ›¾xğ‘¤,ğ‘¥yq` ğœ™pğ›¾xğ‘¤,ğ‘¥y`ğœ‰ 0qÂ´2â„pğ‘¥q
0 b
Ä 4ğ”¼ ğ‘¥,ğœ‰ |ğœ™pğ›¾xğ‘¤,ğ‘¥yqÂ´ ğœ™pğ›¾xğ‘¤,ğ‘¥y`ğœ‰ 0q| Ä 4 2ğ›½{ğœ‹ Ä ğœ€{5, (4.4)
0
where the last inequality is because we have chosen ğ›½ Ä 1ğœ€ 02 3. Let ğ‘ â€œ ğ”¼ ğ‘¦â€ğ‘ˆr0,1srğœ™pğ‘¦qs then,
cancelingğ”¼ğœ‰2 similarly,
Ë‡ Ë‡ Ë‡ Ë‡
Ë‡ Ë‡ Ë‡ Ë‡
Ë‡ğ¿ ğ‘ƒ pğ‘qÂ´ğ¿ ğ‘„ pğ‘qË‡ â€œ Ë‡ğ”¼ ğ‘ƒ pğ‘ Â´ ğœ™pğ›¾xğ‘¤,ğ‘¥yqq2 Â´ğ”¼ ğ‘„ pğ‘ Â´ ğœ™pğ‘¦qq2Ë‡
ğœ™ 1 ğœ™ 1
Ë‡ Ë‡
Ë‡ Ë‡
â€œ Ë‡ğ”¼ ğ‘¦â€ğ‘ƒ ğ‘¦pğ‘ Â´ ğœ™pğ‘¦qq2 Â´ğ”¼ ğ‘¦â€ğ‘ˆr0,1spğ‘ Â´ ğœ™pğ‘¦qq2Ë‡
â€º â€º
Ä
2â€º
pğ‘ Â´
ğœ™pğ‘¦qq2â€º
Â¨TVpğ‘ƒ ğ‘¦,ğ‘ˆr0,1sq
8
Ä 16exppÂ´2ğœ‹2ğ›¾2 q Ä ğ‘œppolypğ‘‘qÂ´1 q Ä ğœ€{5, (4.5)
where ğ‘ƒ ğ‘¦ denotesthedistributionofpğ›¾xğ‘¤,ğ‘¥y mod 1qfor ğ‘¥ â€ ğ‘p0,ğ¼ ğ‘‘q,thesecondinequalityis
from[SZB21,ClaimI.6],andthelastinequalityisbecause ğœ€ â€œ polypğ‘‘qÂ´1. Combining(4.3),(4.4),
and(4.5)wegetthatwithprobabilityatleast2{3Â´0.01,
ğ¿ ğ‘ƒ pâ„q Ä ğ¿ ğ‘ƒ pâ„q` ğœ€{5 Ä ğ¿ ğ‘ƒ pğ”¼rğ‘“ ğ›¾,ğ‘¤pğ‘¥qsqÂ´4ğœ€{5
1 ğœ™ ğœ™
Ä ğ¿ ğ‘ƒ pğ‘qÂ´4ğœ€{5 Ä ğ¿ ğ‘„ pğ‘qÂ´3ğœ€{5 Ä ğ¿ ğ‘„ pâ„qÂ´3ğœ€{5, (4.6)
ğœ™ 1 1
where the third inequality is from the optimality of ğ”¼rğ‘“ ğ›¾,ğ‘¤pğ‘¥qs among constant predictors for
ğ‘ƒ ,andthelastinequalityisfromtheoptimalityof ğ‘ amongallpredictorsfor ğ‘„ .
ğœ™ 1
Usingtheremainingğ‘š 2 samplespğ‘¥ ğ‘–,ğ‘§ ğ‘–qfromğ‘ƒ 0,andthenewlygen Å™eratedğ‘š 2`samplespğ‘¥1 ğ‘–,ğ‘¦ ğ‘–1 Ë˜q
fromğ‘„ 0,ğ‘– â€œ ğ‘š 1` Å™1,...,ğ‘š,c `omputetheem Ë˜piricallossesğ¿Ë† ğ‘ƒ 1pâ„q â€œ ğ‘š1
2
ğ‘š ğ‘–â€œğ‘š 1`1â„“ â„pğ‘¥ ğ‘–q,ğ¹ ğœ‰ ğ‘–pğ‘§ ğ‘–q
andğ¿Ë† ğ‘„ 1pâ„q â€œ ğ‘š1
2
ğ‘š ğ‘–â€œğ‘š 1`1â„“ â„pğ‘¥1 ğ‘–q,ğ¹ ğœ‰ ğ‘–pğ‘¦ ğ‘–1q . Weknowthat|â„pğ‘¥qÂ´ğœ™pğ‘¦q| Ä 2,andğœ‰isGaussian
withvariance ğœ2 Äƒ 1. Hence â„pğ‘¥qÂ´ğ¹ ğœ‰pğ‘¦qissub-Gaussianwithsomeabsoluteconstantparam-
eter. Thereforeâ„“pâ„pğ‘¥q,ğ¹ ğœ‰pğ‘¦qqissub-exponentialwithsomeabsoluteconstantparameters. Then
byBernsteinâ€™sinequality,|ğ¿Ë† ğ· pâ„qÂ´ğ¿ ğ‘ƒ pâ„q| Ä ğœ€{5and|ğ¿Ë† ğ‘„ pâ„qÂ´ğ¿ ğ‘„ pâ„q| Ä ğœ€{5,bothwithprob-
1 1 1 1
ability at least 1Â´exppÂ´mintÎ©pğ‘š 2ğœ€2 q,Î©pğ‘š 2ğœ€quq which is 1Â´ ğ‘œp1q, as long as ğ‘š 2 â€œ ğœ”pğœ€Â´2 q,ONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 11
which can be polypğ‘‘q. Using these concentration bounds together with (4.6) we get that with
probabilityatleast2{3Â´0.01Â´ ğ‘œp1q,itholdsthat
ğ¿Ë† ğ· pâ„q Ä ğ¿ ğ‘ƒ pâ„q` ğœ€{5 Ä ğ¿ ğ‘„ pâ„qÂ´2ğœ€{5 Ä ğ¿Ë† ğ‘„ pâ„qÂ´ ğœ€{5
1 1 1 1
hence â„ willpassthetestofthestep6ofalgorithm â„¬ and â„¬ willreturnthecorrectanswer.
Case II: D â€œ Q 0. In this case ğ· 1 â€œ ğ‘„ 1. Using ğ‘š 2 â€œ ğœ”pğœ€Â´2 q large enough and applying Bern-
steinâ€™sinequalityagainwegetthat|ğ¿Ë† ğ· pâ„qÂ´ğ¿ ğ‘„ pâ„q| Ä ğœ€{20and|ğ¿Ë† ğ‘„ pâ„qÂ´ğ¿ ğ‘„ pâ„q| Ä ğœ€{20,both
1 1 1 1
withprobabilityatleast1Â´exppÂ´Î©pğ‘š 2ğœ€2 qq â€œ 1Â´ğ‘œp1q. Whichmeansthat|ğ¿Ë† ğ· pâ„qÂ´ğ¿Ë† ğ‘„ pâ„q| Ä
1 1
ğœ€{10 with probability at least 1Â´ ğ‘œp1q. Hence, ğ¿Ë† ğ· pâ„q Ä… ğ¿Ë† ğ‘„ pâ„qÂ´ ğœ€{5 and the test in step 6 of
1 1
thealgorithm â„¬ fails.
In both cases, the test correctly concludes ğ· â€œ ğ‘ƒ 0 or ğ· â€œ ğ‘„ 0, by using the empirical loss
ğ¿Ë†
ğ·
pâ„qandcomparingittothevalue ğ¿Ë†
ğ‘„
pâ„qÂ´ ğœ€{5. â–¡
1 1
5. TheCryptographicHardnessofLearningOneHiddenLayerNeuralNetworks
In this section we complete the proof of Theorem 3.1. To do this we construct a family of one
hiddenlayerneuralnetworkswithpolynomialsizethatis1-Lipschitzand1-periodicoverafinite
rangerÂ´ğ‘…,ğ‘…s. Becauseourinput ğ‘¥ isGaussian,andhencehasâ€œlightâ€tails,weshowthatthisis
enoughtoapplyourTheorem4.2fromtheprevioussectionandconcludeourhardnessresult.
Webeginbyremindingthereadertheclass â„±NN ofonehiddenlayerneuralnetworksdefined
ğ‘˜
in(3). Letusconsiderthefollowingfunction,
#
Ë‡ Z ^Ë‡
Ë‡ Ë‡ 3 1 Ë‡ Ë‡ 1 ğ‘¥ Â´ ğ‘˜, ğ‘¥ P rğ‘˜ Â´1{4,ğ‘˜ `1{4s,
ğœ™pğ‘¥q â€œ Ë‡ğ‘¥ Â´ Â´ ğ‘¥ Â´ Ë‡Â´ â€œ ğ‘˜ P â„¤,
4 4 4 1{2Â´pğ‘¥ Â´ ğ‘˜q, ğ‘¥ P rğ‘˜ `1{4,ğ‘˜ `3{4s,
which is 1-periodic, 1-Lipschitz, and |ğœ™pğ‘¥q| Ä 1{4 for all ğ‘§ P â„ (see Figure 1). The following
lemma shows that it interestingly coincides with an one hidden layer neural network on some
interval.
Lemma5.1. For ğ‘… P â„•,let
Ë† Ë™ Ë† Ë™
Ã¿2ğ‘…
1 3
nnpğ‘¥q â€œ pğ‘¥ `ğ‘…q Â´pğ‘¥ Â´ğ‘…q `2 ğ‘¥ `ğ‘… ` Â´ ğ‘˜ Â´ ğ‘¥ `ğ‘… ` Â´ ğ‘˜ .
` ` 4 4
ğ‘˜â€œ1 ` `
Then,nnpğ‘¥q â€œ ğœ™pğ‘¥qÂ¨1tğ‘¥ P rÂ´ğ‘…,ğ‘…su.
Proof. For ğ‘¥ Ä Â´ğ‘…, all the ReLU functions evaluate to 0, and nnpğ‘¥q Å™â€œ 0. `For ğ‘¥ Ä› ğ‘…, all Ë˜the
R `eLU functions Ë˜evaluate to id, and nnpğ‘¥q â€œ pğ‘¥ ` ğ‘…q Â´ pğ‘¥ Â´ ğ‘…q ` 2 2 ğ‘˜ğ‘…
â€œ1
ğ‘¥ `ğ‘… ` 41 Â´ ğ‘˜ Â´
ğ‘¥ `ğ‘… ` 3 Â´ ğ‘˜ â€œ 2ğ‘… Â´2ğ‘… â€œ 0. The interesting case is of course when ğ‘¥ P rÂ´ğ‘…,ğ‘…s. Observe
4
thatfor ğ‘˜ â€œ 1,2,...,2ğ‘…,
$
Ë† Ë™ Ë† Ë™ â€™ &0, ğ‘¥ Äƒ Â´ğ‘… Â´3{4` ğ‘˜,
1 3
ğ‘¥ `ğ‘… ` Â´ ğ‘˜ Â´ ğ‘¥ `ğ‘… ` Â´ ğ‘˜ â€œ Â´1{2, ğ‘¥ Ä… Â´ğ‘… Â´1{4` ğ‘˜,
4 4 â€™
%
` ` Â´ğ‘¥ Â´ğ‘… Â´3{4` ğ‘˜, otherwise.
Ifğ‘¥ P rğ‘˜Â´1{4,ğ‘˜`1{4sforsome ğ‘˜ P â„¤,thennnpğ‘¥q â€œ pğ‘¥`ğ‘…q`2pÂ´1{2qpğ‘…`ğ‘˜q â€œ ğ‘¥Â´ğ‘˜ â€œ ğœ™pğ‘¥q.
If ğ‘¥ P rğ‘˜ `1{4,ğ‘˜ `3{4s for some ğ‘˜ P â„¤, then nnpğ‘¥q â€œ pğ‘¥ ` ğ‘…q`2pÂ´ğ‘¥ Â´ ğ‘… Â´3{4` ğ‘… ` ğ‘˜ `
1q`2pÂ´1{2qpğ‘… ` ğ‘˜q â€œ 1{2Â´pğ‘¥ Â´ ğ‘˜q â€œ ğœ™pğ‘¥q. â–¡12 S.LI,I.ZADIK,M.ZAMPETAKIS
Ï•(x) nn(x)
0.25
Out[ï]=
-6 -5 -4 -3 -2 -1 1 2 3 4 5 6
-0.25
Figure1. ğœ™pğ‘¥qandnnpğ‘¥qfor ğ‘… â€œ 3
We next consider an ainstantiation of nn from Lemma 5.1 that coincides with ğœ™ on rÂ´ğ‘…,ğ‘…s
for arbitrary ğ‘… â€œ ğœ”pğ›¾ logğ‘‘q. Observe that nn which takes a single input, one hidden layer
neural network withp4ğ‘… `2q ReLU neurons. We can then define the multivariate version of nn
as NNpğ‘¥q â€œ nnpğ›¾xğ‘¤,ğ‘¥yq, which is also an one hidden layer neural network. This way we can
definethefollowingsubclassofonehiddenlayerneuralnetworks â„±NN givenby,
ğ‘˜
! )
â„±NN â€œ ğ‘“pğ‘¥q â€œ nnpğ›¾xğ‘¤,ğ‘¥yq | ğ‘¤ P â„ğ‘‘,ğ›¾ P â„ , (5.1)
ğ‘…
which contains one hidden layer neural networks with width ğ‘‚pğ‘…q. Recall from the previous
sectionthat,forğ‘¥ â€ ğ‘p0,ğ¼ ğ‘‘qandğœ‰ â€ ğ‘p0,ğœ2 q,ğ‘ƒ ğœ™ denotesthedistributionofpğ‘¥,ğœ™pğ›¾xğ‘¤,ğ‘¥yq`
ğœ‰q, which is the input for learning the periodic function ğœ™. Let ğ‘ƒ denote the distribution of
NN
pğ‘¥,NNpğ‘¥q ` ğœ‰q, which is the input for learning the one hidden layer neural network NN. We
next show that samples generated from ğ‘ƒ are essentially the same as samples generated from
ğœ™
ğ‘ƒ .
NN
LeÂ´mma 5.2. ForÂ¯ğ‘… P â„•, the total variance distance abetween ğ‘ƒ ğœ™ and ğ‘ƒ NN is upper bounded by
ğ‘‚
ğ›¾exppÂ´ğ‘…2{2ğ›¾2q
. When ğ›¾ â€œ polypğ‘‘q, and ğ‘… â€œ ğ›¾ ğœ”plogğ‘‘q`2logp1{ğœq, the total variation
ğœğ‘…
distanceis
ğ‘‚ppolypğ‘‘qexppÂ´ğœ”plogğ‘‘qqq â€œ ğ‘‘Â´ğœ”p1q.
Proof. Notethatconditioningonğ‘¥,ğ‘§ 1 â€œ ğœ™pğ›¾xğ‘¤,ğ‘¥yq`ğœ‰andğ‘§ 2 â€œ nnpğ›¾xğ‘¤,ğ‘¥yq`ğœ‰areGaussians
withmean ğœ™pğ›¾xğ‘¤,ğ‘¥yqandnnpğ›¾xğ‘¤,ğ‘¥yq,andvariance ğœ2. Thus,
â€ È·
|ğœ™pğ›¾xğ‘¤,ğ‘¥yqÂ´nnpğ›¾xğ‘¤,ğ‘¥yq|
TVpğ‘ƒ ğœ™,ğ‘ƒ NNq Ä ğ”¼ ğ‘¥â€ğ‘p0,ğ¼ ğ‘‘qrTVpğ‘§ 1|ğ‘¥,ğ‘§ 2|ğ‘¥qs Ä ğ”¼ ğ‘¥â€ğ‘p0,ğ¼ ğ‘‘q 2ğœ
1 1
â€œ 2ğœğ”¼ ğ‘¥â€ğ‘p0,1qr|ğœ™pğ›¾ğ‘¥qÂ´nnpğ›¾ğ‘¥q|s Ä 8ğœPğ‘¥â€ğ‘p0,1qr|ğ‘¥| Ä› ğ‘…{ğ›¾s
Ë† Ë™
1 exppÂ´ğ‘…2 {2ğ›¾2 q ğ›¾exppÂ´ğ‘…2 {2ğ›¾2 q
Ä ? Ä ğ‘‚ .
4ğœ 2ğœ‹ğ‘…{ğ›¾ ğœğ‘…
â–¡
FromLemmas5.1and5.2wehavethatthereexistsasubclass â„±NN ofonehiddenlayerneural
ğ‘…
ğœ™
networks that produces the same polynomially-many samples as the class â„± for this carefully
ğ›¾ONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 13
picked1-Lipschitzand1-periodicfunction ğœ™. Wenextshowthatinfactalearningalgorithmfor
ğœ™
â„±NN impliesalearningalgorithmfor â„± .
ğ‘… ğ›¾
Lemma 5.3. Let ğ‘‘ P â„•, ğ›¾ â€œ polypğ‘‘q, ğœ â€œağœpğ‘‘q P p0,1q, ğœ€ â€œ polypğ‘‘qÂ´1 . Moreover, let
ğœ™pğ‘¥q â€œ |ğ‘¥ Â´3{4Â´tğ‘¥ Â´1{4u|Â´1{4,andğ‘… â€œ ğ›¾ ğœ”plog â£ğ‘‘q`2logp1{ğœq. Thenapolynomial-tim(e
algorithmthatğœ€-weaklylearnsthefunctionclassâ„± ğ‘…NN â€œ ğ‘“ ğ‘¤pğ‘¥q â€œ nnpğ›¾xğ‘¤,ğ‘¥yq | ğ‘¤ P â„ğ‘‘,ğ›¾ P â„
over Gaussian inputs ğ‘¥ i. â€i.d. ğ‘p0,ğ¼ ğ‘‘q under Gaussian noise ğœ‰ i. â€i.d. ğ‘p0,ğœ2 q implies a polynomial-
timealgorithmthat ğœ€ 2-weaklylearnsthefunctionclassâ„± ğ›¾ğœ™ â€œ tğ‘“ ğ›¾,ğ‘¤pğ‘¥q â€œ ğœ™pğ›¾xğ‘¤,ğ‘¥yq | ğ‘¤ P ğ‘†ğ‘‘Â´1 u
overthesameinputandnoisedistribution.
Proof. Let nn be athe function in Lemma 5.1 that coincides with ğœ™ on rÂ´ğ‘…,ğ‘…s for arbitrary ğ‘…
satisfyingğ‘… â€œ ğ›¾ ğœ”plogğ‘‘q`2logp1{ğœq. Then,letNNpğ‘¥q â€œ nnpğ›¾xğ‘¤,ğ‘¥yq,andthusNN P â„±NN.
ğ‘…
For ğœ€ â€œ polypğ‘‘qÂ´1, and ğ’œ be a polynomial-time learning algorithm that takes as input ğ‘š â€œ
polypğ‘‘q samples from ğ‘ƒ and with probability 2{3 outputs a hypothesis â„1 : â„ Ã‘ â„ such that
NN
Ëœ
ğ¿ ğ‘ƒ pâ„1q Ä ğ¿ ğ‘ƒ pğ”¼rNNpğ‘¥qsq Â´ ğœ€. Since we are using the squared loss, â„pğ‘¥q â€œ sgnpâ„1pğ‘¥qq Â¨
NN NN
maxp|â„1pğ‘¥q|,1{4q is always no worse than â„1pğ‘¥q, as NNpğ‘¥q P rÂ´1{4,1{4s and the noise on the
labelisunbiased. Thus,wecanassumewithoutlossofgeneralitythat â„1pğ‘¥q P rÂ´1{4,1{4s.
ğœ™
To learn the function class â„± ğ›¾ given ğ‘š â€œ polypğ‘‘q samples from ğ‘ƒ ğœ™, run ğ’œ directly on these
samples,whichgives â„,andoutput â„. Similarly,bythedataprocessinginequality,
TVpâ„,â„1q Ä TVpğ‘ƒ ğœ™bğ‘š,ğ‘ƒ Nb Nğ‘š q Ä ğ‘š Â¨TVpğ‘ƒ ğœ™,ğ‘ƒ NNq.
By Lemma 5.2, this is upper bounded by ğ‘š Â¨ ğ‘‘Â´ğœ”p1q â€œ ğ‘‘Â´ğœ”p1q Äƒ 0.01. Thus, with probability
at least 2{3 Â´ 0.01, we have ğ¿ ğ‘ƒ pâ„q Ä ğ¿ ğ‘ƒ pğ”¼rNNpğ‘¥qsq Â´ ğœ€. Since ğ”¼rNNpğ‘¥qs is the optimal
NN NN
constantpredictorforğ‘ƒ NN,wehaveğ¿ ğ‘ƒ NNpâ„q Ä ğ¿ ğ‘ƒ NNpğ”¼rNNpğ‘¥qsqÂ´ğœ€ Ä ğ¿ ğ‘ƒ NNpğ”¼rğœ™pğ›¾xğ‘¤,ğ‘¥yqsqÂ´ğœ€.
SimilarlytotheproofofTheorem4.2,compute
Ë‡ Ë‡
|ğ¿ ğ‘ƒ NNpâ„qÂ´ğ¿ ğ‘ƒ ğœ™pâ„q| â€œ Ë‡ ğ”¼ ğ‘¥â€ğ‘p0,ğ¼ ğ‘‘qpâ„pğ‘¥qÂ´NNpğ‘¥qq2 Â´ğ”¼ ğ‘¥â€ğ‘p0,ğ¼ ğ‘‘qpâ„pğ‘¥qÂ´ ğœ™pğ›¾xğ‘¤,ğ‘¥yqq2Ë‡
â€œ |ğ”¼ ğ‘¥rpNNpğ‘¥qÂ´ ğœ™pğ›¾xğ‘¤,ğ‘¥yqqpNNpğ‘¥q` ğœ™pğ›¾xğ‘¤,ğ‘¥yqÂ´2â„pğ‘¥qqs|
Ä ğ”¼ ğ‘¥|NNpğ‘¥qÂ´ ğœ™pğ›¾xğ‘¤,ğ‘¥yq|.
We know from the proof of Lemma 5.2 that ğ”¼ ğ‘¥|NNpğ‘¥q Â´ ğœ™pğ›¾xğ‘¤,ğ‘¥yq| Ä ğ‘‘Â´ğœ”p1q Ä ğœ€{4. Thus,
|ğ¿ ğ‘ƒ pâ„qÂ´ğ¿ ğ‘ƒ pâ„q| Ä ğœ€{4. Let ğ‘ â€œ ğ”¼rğœ™pğ›¾xğ‘¤,ğ‘¥yqs P rÂ´1{4,1{4s. Thenbythesameargument,
NN ğœ™
Ë‡ Ë‡
|ğ¿ ğ‘ƒ NNpğ‘qÂ´ğ¿ ğ‘ƒ ğœ™pğ‘q| â€œ Ë‡ ğ”¼ ğ‘¥â€ğ‘p0,ğ¼ ğ‘‘qpğ‘ Â´NNpğ‘¥qq2 Â´ğ”¼ ğ‘¥â€ğ‘p0,ğ¼ ğ‘‘qpğ‘ Â´ ğœ™pğ‘¥qq2Ë‡
â€œ |ğ”¼ ğ‘¥rpNNpğ‘¥qÂ´ ğœ™pğ›¾xğ‘¤,ğ‘¥yqqpNNpğ‘¥q` ğœ™pğ›¾xğ‘¤,ğ‘¥yqÂ´2ğ‘qs|
Ä ğ”¼ ğ‘¥|NNpğ‘¥qÂ´ ğœ™pğ›¾xğ‘¤,ğ‘¥yq| Ä ğœ€{4.
Therefore,withprobabilityatleast2{3Â´0.01,wehaveğ¿ ğ‘ƒ ğœ™pâ„q Ä ğ¿ ğ‘ƒ NNpâ„q`ğœ€ 4 Ä ğ¿ ğ‘ƒ NNrğ”¼rğœ™pğ›¾xğ‘¤,ğ‘¥yqssÂ´
3 4ğœ€ Ä ğ¿ ğ‘ƒ ğœ™rğ”¼rğœ™pğ›¾xğ‘¤,ğ‘¥yqssÂ´ ğœ€ 2. â–¡
ğœ™
Thefinalstepistocombinethiswiththehardnessoflearning â„± fromTheorem4.2withthe
ğ›¾
ğœ™
equivalence of learning â„± and â„±NN to get the following result, which directly implies Theo-
ğ›¾ ğ‘…
rem3.1.14 S.LI,I.ZADIK,M.ZAMPETAKIS
a
Theorem 5.4. Let ğ‘‘ P â„•, ğœ â€œ polypğ‘‘qÂ´1 , ğœ€ â€œ polypğ‘‘qÂ´1 , and ğ‘… â€œ ğœ”p ğ‘‘logğ‘‘q. Then a
polynomial-timealgorithmthatğœ€-weaklylearnsthefunctionclassâ„±NN,definedin(5.1)overGauss-
ğ‘…
ianinputsğ‘¥ i. â€i.d. ğ‘p0,ğ¼ ğ‘‘qunderGaussiannoiseğœ‰ i. â€i.d. ğ‘p0,ğœ2 qimpliesapolynomial-timequantum
algorithmthatapproximatesGapSVPtowithinpolynomialfactors.
?
Proof. Let ğ›¾ â€œ 2 ğ‘‘ and ğœ™pğ‘¥q â€œ |ğ‘¥ Â´ 3{4 Â´ tğ‘¥ Â´1{4u| Â´ 1{4, which is 1-Lipschitz, 1-periodic,
and ğœ™pğ‘¥q P rÂ´1{4,1{4s for all ğ‘¥ P â„. Then by Lemma 5.3, there is a polynomial-time algorithm
that ğœ€ 2-weakly learns the function class â„± ğ›¾ğœ™ â€œ tğ‘“ ğ›¾,ğ‘¤pğ‘¥q â€œ ğœ™pğ›¾xğ‘¤,ğ‘¥yq | ğ‘¤ P ğ‘†ğ‘‘Â´1 u over the
same input and noise distribution. Then by Corollary 4.3, there is a polynomial-time quantum
algorithmthatapproximatesSVPtowithinpolynomialfactors. â–¡
6. Super-PolynomiallySmallNoise
In this section we show that our lower bound holds even if we make the noise negligible, i.e.,
smallerthananyinversepolynomialin ğ‘‘. Eveninthisverylownoiseregime,anyalgorithmfor
learning1-hiddenlayerneuralnetworkswithGaussianinputimpliesabreakthroughincryptog-
raphyandalgorithmictheoryoflattices. Wemakethisclaimprecisebelow.
Ifweremovetherestrictionof ğœ â€œ polyp1{ğ‘‘qinTheorem5.4,thenusingthesameoutlineofthe
proofwecanshowthefollowinglemmathatreduceslearning1-hiddenlayerneuralnetworksto
CLWEevenwhen ğ›½ isnegligible.
a
Lemma 6.1a. Let ğ‘‘ P â„•, ğ›¾ â€œ polypğ‘‘q with ğ›¾ â€œ ğœ”p logğ‘‘q, ğœ â€œ ğœpğ‘‘q P p0,1q, ğœ€ â€œ polypğ‘‘qÂ´1 ,
and ğ‘… â€œ ğ›¾ ğœ”plogğ‘‘q`2logp1{ğœq. Then a polynomial-time algorithm that ğœ€-weakly learns the
function class â„± ğ‘…NN, defined in (5.1) over Gaussian inputs ğ‘¥ i. â€i.d. ğ‘p0,ğ¼ ğ‘‘q under Gaussian noise
ğœ‰ i. â€i.d. ğ‘p0,ğœ2 qimpliesapolynomial-timealgorithmforCLWE ğ›½,ğ›¾ forany ğ›½ Ä ğœ{polypğ‘‘q.
Proof. Let ğœ™pğ‘¥q â€œ |ğ‘¥ Â´ 3{4 Â´ tğ‘¥ Â´1{4u| Â´ 1{4, which is 1-Lipschitz, 1-periodic, and ğœ™pğ‘¥q P
rÂ´1{4,1{4s for all ğ‘¥ P â„. Then by Lemma 5.3, there is a polynomial-time algorithm that ğœ€-
2
weaklylearnsthefunctionclass â„± ğ›¾ğœ™ â€œ tğ‘“ ğ›¾,ğ‘¤pğ‘¥q â€œ ğœ™pğ›¾xğ‘¤,ğ‘¥yq | ğ‘¤ P ğ‘†ğ‘‘Â´1 uoverthesameinput
and noise distribution. Then by Theorem 4.2, there is a polynomial-time algorithm for CLWE
ğ›½,ğ›¾
forany ğ›½ Ä ğœ{polypğ‘‘q. â–¡
Therefore an algorithm for learning 1-hidden layer neural network implies that we can solve
CLWE
ğ›½,ğ›¾
as long as ğ›½ is smaller than ğœ{polypğ‘‘q. The next step is to connect an algorithm for
CLWE to an algorithm for worst-case lattice problems even when ğ›½ is negligible. For the
ğ›½,ğ›¾
case ğ›½ Ä› polypğ‘‘qÂ´1 we used the CLWE hardness fromxs Theorem 2.4x, which requires ğ›¾{ğ›½ â€œ
polypğ‘‘q. Nevertheless, we can bypass this condition by using the following recent theorem that
reduces classical LWE to CLWE from [GVV22], together with the stronger quantum reduction
fromworst-caselatticeproblemtoLWEdueto[Reg05].
Theorem6.2([GVV22,Corollary5]). Letğ‘‘,ğ‘›,ğ‘ P â„•, ğ›¾,ğ›½,ğœ1 Ä… 0. Thenforsomeconstantğ‘ Ä… 0,
apolynomial-timealgorithmforCLWEğ›½,ğ›¾ indimension ğ‘‘ impliesapolynomial-timealgorithmfor
LWE ğ‘,ğ· indimension ğ‘›,for
â„¤,ğœ1 Ëœ Â¸
?
a ğœ1 ğ‘‘
ğ›¾ â€œ ğœ”p ğ‘‘logğ‘‘q, ğ›½ Ä› ğ‘ ,
ğ‘ONTHEHARDNESSOFLEARNINGONEHIDDENLAYERNEURALNETWORKS 15
a
aslongaslogpğ‘q{2ğ‘› â€œ ğ‘œppolypğ‘›qÂ´1 q, ğ‘‘ Ä› 2ğ‘›logğ‘ ` ğœ”plogğ‘›q,and ğœ1 Ä› ğœ”p logğ‘›q.
Theorem 6.3 ([Reg05, Theorem 3.1, Lemma 3.20]). Let ğ‘›,ğ‘ P â„•, ğ›¼ P p0,1q such that ğ›¼ğ‘ Ä…
?
2 ğ‘›. Then apolynomial-time algorithmforLWE ğ‘,ğ· in dimension ğ‘› impliesa polynomial-time
â„¤,ğ›¼ğ‘
quantumalgorithmfor ğ‘‚pğ‘›{ğ›¼q-GapSVPindimension ğ‘›.
If we combine these two results we get the following reduction from GapSVP to CLWE even
forsuper-polynomiallysmall ğ›½.
Corollary6.4. Let ğ‘‘,ğ‘› P â„•, ğ›¾ Ä… 0,ğ›½ P p0,1q. Thenapolynomial-timealgorithmforCLWEğ›½,ğ›¾ in
?
dimension ğ‘‘ aimplies a polynomial-time quantum algorithm for ğ‘‚pğ‘› ğ‘‘{ğ›½q-GapSVP in dimension
ğ‘›,if ğ›¾ â€œ ğœ”p ğ‘‘logğ‘‘q,logp1{ğ›½q Ä polypğ‘›q,and3ğ‘›logpğ‘‘{ğ›½q Ä ğ‘‘ Ä polypğ‘›q.
?
Proof. Fortheconstant ğ‘ Ä… 0fromTheorem6.2,let ğ›¼ â€œ ğ‘Â´1ğ›½{ ğ‘‘, ğ‘ â€œ 2ğ‘‘{ğ›½, ğœ1 â€œ ğ›¼ğ‘.
?
Indeed, ğ›¾ directlysatisfiesthesameassumption, ğ›½ satisfies ğ›½ Ä› ğ‘ğœ1 ğ‘‘{ğ‘, and ğ‘ satisfies
logpğ‘q â€œ logp2ğ‘‘{ğ›½q â€œ ğ‘‚plogğ‘› `logp1{ğ›½qq Ä polypğ‘›q â€œ ğ‘œp2ğ‘› ppolypğ‘›qqÂ´1 q
and ğ‘‘ satisfies
ğ‘‘ Ä› 3ğ‘›logpğ‘‘{ğ›½q Ä› 2ğ‘›logpğ‘q` ğœ”plogğ‘›q.
? a ? a ?
Finally,alsoclearly ğœ1 â€œ ğ‘Â´1 ğ‘‘ â€œ ğœ”p logğ‘›qand ğ›¼ğ‘ â€œ ğ‘Â´1 ğ‘‘ Ä› ğ‘Â´1 3ğ‘›logpğ‘‘{ğ›½q Ä… 2 ğ‘›.
Then by Theorem 6.2, there is a polynomial-time algorithm for LWE in dimension ğ‘›.
ğ‘,ğ·
? â„¤,ğ›¼ğ‘ ?
Further, since ğ›¼ğ‘ Ä… 2 ğ‘›, by Theorem 6.3, there is a polynomial-time algorithm for ğ‘‚pğ‘› ğ‘‘{ğ›½q-
GapSVPalgorithm. â–¡
Finally,wecanuseCorollary6.4insteadofTheorem2.4intheproofofTheorem5.4togetthe
followingresult.
Theorema6.5. Let ğ‘› P â„•, ğœ Ä› ğ‘’Â´polypğ‘›q, Î©pğ‘›logpğ‘›{ğœqq Ä ğ‘‘ Ä polypğ‘›q, ğœ€ â€œ polypğ‘‘qÂ´1 , and
ğ‘… â€œ ğœ”p ğ‘‘logğ‘‘ Â¨logpğ‘‘{ğœqq. Thenapolynomial-timealgorithmthat ğœ€-weaklylearnsthefunction
classâ„± ğ‘…NN,definedin(5.1)overGaussianinputsğ‘¥ i. â€i.d. ğ‘p0,ğ¼ ğ‘‘qunderGaussiannoiseğœ‰ i. â€i.d. ğ‘p0,ğœ2 q
impliesapolynomial-timequantumalgorithmforppolypğ‘›q{ğœq-GapSVPindimension ğ‘›.
a
Proof. Let ğ›½ â€œ ğœ{polypğ‘‘q, ğ›¾ â€œ ğœ”p ğ‘‘logğ‘‘q, then by Lemma 6.1, there is a polynomial-time
algorithm for CLWE . By Corollary 6.4, there is a polynomial-time quantum algorithm for
ğ›½,ğ›¾
?
GapSVPwithfactor ğ‘‚pğ‘› ğ‘‘{ğ›½q â€œ polypğ‘›q{ğœ indimension ğ‘›. â–¡
Bychoosing ğœ â€œ 2Â´ğ‘‘ğœ‚ forconstantğœ‚ P p0,1{2q,wecangetthefollowingcorollary.
a
Corollary6.6. Forconstantğœ‚ P p0,1{2q,let ğœ â€œ 2Â´ğ‘‘ğœ‚ , ğœ€ â€œ polypğ‘‘qÂ´1 ,andğ‘… â€œ ğœ”p ğ‘‘1`ğœ‚logğ‘‘q.
Thenfor ğ‘› P â„•with ğ‘› â€œ Î˜pğ‘‘1Â´ğœ‚ q,apolynomial-timealgorithmthat ğœ€-weaklylearnsthefunction
classâ„± ğ‘…NN,definedin(5.1)overGaussianinputsğ‘¥ i. â€i.d. ğ‘p0,ğ¼ ğ‘‘qunderGaussiannoiseğœ‰ i. â€i.d. ğ‘p0,ğœ2 q
ğœ‚
impliesapolynomial-timequantumalgorithmfor2ğ‘‚pğ‘›1Â´ğœ‚q-GapSVPindimension ğ‘›.
Proof. Sinceğœ‚ isaconstant,indeedwehave ğœ Ä› ğ‘’Â´polypğ‘›q,
ğ‘‘ â€œ Î˜pğ‘›1Â´1 ğœ‚q Ä› Î©pğ‘› Â¨ğ‘›1Â´ğœ‚ ğœ‚q Ä› Î©pğ‘›ğ‘‘ğœ‚ q â€œ Î©pğ‘›logpğ‘›{ğœqq,
a
and ğ‘… â€œ ğœ”p ğ‘‘logğ‘‘ Â¨logpğ‘‘{ğœqq. Thus, from Theorem 6.5, there is a polynomial-time quantum
ğœ‚
algorithmforGapSVPwithfactorpolypğ‘›q{ğœ â€œ 2ğ‘‚pğ‘›1Â´ğœ‚q indimension ğ‘›. â–¡16 S.LI,I.ZADIK,M.ZAMPETAKIS
According to Corollary 6.6, Theorem 6.5 shows an interesting hardness result even when ğœ is
super-polynomiallysmall. Tomaketheconnectionmoreprecise,observethatforany ğ›¿ P p0,1q,
if we set ğœ‚ :â€œ ğ›¿ P p0,1{2q, and ğœ â€œ 2Â´ğ‘‘ğœ‚, then, due to Corollary 6.6, the existence of a
1`ğ›¿
polynomial-time algorithm that weakly learns one hidden layer neural networks with polyno-
mial width under Gaussian noise with only 2Â´ğ‘‘ğœ‚ standard deviation, implies a polynomial-time
quantum algorithm for 2ğ‘‚pğ‘›ğ›¿q-GapSVP in dimension ğ‘› â€” a problem which is considered hard
in cryptography and algorithmic theory of lattices. We remind the reader, that the main reason
behindthisconjecturedhardnessisthatthestate-of-the-art(since1982)powerfulLLLalgorithm
forGapSVPisonlyabletoachievean2Î˜pğ‘›q-approximation,andanyimprovementonitwouldbe
consideredamajorbreakthrough. Wealsohighlightthatanysuchalgorithmwouldbreakinfact
severalbreakthroughcryptographicconstructionssuchastherecentcelebratedresultof[JLS21].
7. Conclusions
Inthispaper,weprovedthehardnessoflearningonehiddenlayerneuralnetworkswithwidth
a
ğœ”p ğ‘‘logğ‘‘qunderGaussianinputandanyinverse-polynomiallysmallGaussiannoise,assuming
the hardness of GapSVP with polynomial factors. En route, we proved the hardness of learning
Lipschitz periodic functions under Gaussian input and any inverse-polynomially small Gauss-
ian noise. This improves a similar result from [SZB21], which proved the hardness for inverse-
polynomiallysmalladversarial noise.
Moreover,ifweassumethehardnessof2ğ‘‚pğ‘‘ğ›¿q-GapSVPfor ğ›¿ P p0,1q,wealsogetthehardness
oflearningonehiddenlayerneuralnetworkswithpolynomialwidthunderGaussiannoisewith
2Â´ğ‘‘ğœ‚ variance,whereğœ‚ â€œ ğ›¿ P p0,1{2q.
1`ğ›¿
References
[And+17] AlexandrAndoni,DanielHsu,KevinShi,andXiaoruiSun.Correspondenceretrieval.
ConferenceonLearningTheory.PMLR.2017,pp.105â€“126(cit.onp.4).
[And+19] AlexandrAndoni,RishabhDudeja,DanielHsu,andKiranVodrahalli.Attribute-efficient
learningofmonomialsoverhighly-correlatedvariables.AlgorithmicLearningTheory.
PMLR.2019,pp.127â€“161(cit.onp.2).
[ATV21] PranjalAwasthi,AlexTang,andAravindanVijayaraghavan.Efficientalgorithmsfor
learning depth-2 neural networks with general relu activations. Advances in Neural
InformationProcessingSystems 34(2021),pp.13485â€“13496(cit.onp.2).
[AZLL18] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in
overparameterized neural networks, going beyond two layers. 2018. arXiv: 1811.
04918(cit.onp.2).
[Bac17] Francis Bach. Breaking the curse of dimensionality with convex neural networks. J.
Mach.Learn.Res.18:1(2017),629â€“681(cit.onp.2).
[BJW19] Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified
neural networks in polynomial time. Conference on Learning Theory. PMLR. 2019,
pp.195â€“268(cit.onp.2).
[Bru+17] AlonBrutzkus,AmirGloberson,EranMalach,andShaiShalev-Shwartz.SGDlearns
over-parameterized networks that provably generalize on linearly separable data
(2017)(cit.onp.2).REFERENCES 17
[Bru+21] Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous LWE. Proceedings
of the 53rd Annual ACM SIGACT Symposium on Theory of Computing. 2021 (cit. on
pp.3,5,6).
[Che+22] SitanChen,AravindGollakota,AdamRKlivans,andRaghuMeka.HardnessofNoise-
FreeLearningforTwo-Hidden-LayerNeuralNetworks.arXivpreprintarXiv:2202.05258
(2022)(cit.onpp.2,3).
[CKM22] SitanChen,AdamRKlivans,andRaghuMeka.Learningdeeprelunetworksisfixed-
parameter tractable. 2021 IEEE 62nd Annual Symposium on Foundations of Computer
Science(FOCS).IEEE.2022,pp.696â€“707(cit.onp.2).
[Dam+24] Alex Damian, Loucas Pillaud-Vivien, Jason Lee, and Joan Bruna. Computational-
Statistical Gaps in Gaussian Single-Index Models. The Thirty Seventh Annual Con-
ferenceonLearningTheory.PMLR.2024,pp.1262â€“1262(cit.onp.2).
[Dia+20] Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, and Nikos Zarifis. Algorithms
andsqlowerboundsforpaclearningone-hidden-layerrelunetworks.Conferenceon
LearningTheory.PMLR.2020,pp.1514â€“1539(cit.onp.2).
[DK22] IliasDiakonikolasandDanielKane.Non-gaussiancomponentanalysisvialatticeba-
sisreduction.ConferenceonLearningTheory.PMLR.2022,pp.4535â€“4547(cit.onp.4).
[DV21] Amit Daniely and Gal Vardi. From local pseudorandom generators to hardness of
learning.ConferenceonLearningTheory.PMLR.2021,pp.1358â€“1394(cit.onp.3).
[GKZ21] DavidGamarnik,ErenCKÄ±zÄ±ldagË˜,andIliasZadik.Inferenceinhigh-dimensionallin-
ear regression via lattice basis reduction and integer relation detection. IEEE Trans-
actionsonInformationTheory 67:12(2021),pp.8109â€“8139(cit.onp.4).
[GLM17] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks
withlandscapedesign.2017.arXiv:1711.00501(cit.onp.2).
[Goe+20] Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, and Adam Klivans.
Superpolynomial lower bounds for learning one-layer neural networks using gra-
dient descent. International Conference on Machine Learning. PMLR. 2020, pp. 3587â€“
3596(cit.onp.2).
[GVV22] AparnaGupte,NeekonVafa,andVinodVaikuntanathan.Continuouslweisashardas
lwe&applicationstolearninggaussianmixtures.2022IEEE63rdAnnualSymposium
on Foundations of Computer Science (FOCS). IEEE. 2022, pp. 1162â€“1173 (cit. on pp. 3,
14).
[HR07] IshayHavivandOdedRegev.Tensor-BasedHardnessoftheShortestVectorProblem
to within Almost Polynomial Factors. Proceedings of the Thirty-Ninth Annual ACM
SymposiumonTheoryofComputing.2007,469â€“477(cit.onp.5).
[JLS21] AayushJain,HuijiaLin,andAmitSahai.Indistinguishabilityobfuscationfromwell-
founded assumptions. Proceedings of the 53rd Annual ACM SIGACT Symposium on
TheoryofComputing.2021,pp.60â€“73(cit.onp.16).
[JSA15] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-
convexity:Guaranteedtrainingofneuralnetworksusingtensormethods.arXivpreprint
arXiv:1506.08473 (2015)(cit.onp.2).
[Kho05] Subhash Khot. Hardness of Approximating the Shortest Vector Problem in Lattices.
J.ACM 52:5(2005),789â€“808(cit.onp.5).18 REFERENCES
[KS09] AdamRKlivansandAlexanderASherstov.Cryptographichardnessforlearningin-
tersectionsofhalfspaces.JournalofComputerandSystemSciences75:1(2009),pp.2â€“
12(cit.onp.1).
[LLL82] Arjen Klaas Lenstra, Hendrik Willem Lenstra, and LaÂ´szloÂ´ LovaÂ´sz. Factoring polyno-
mials with rational coefficients. Mathematische Annalen 261: 4 (1982), pp. 515â€“534
(cit.onp.4).
[MR09] DanieleMicciancioandOdedRegev.Lattice-basedCryptography.Post-QuantumCryp-
tography. Berlin, Heidelberg: Springer Berlin Heidelberg, 2009, pp. 147â€“191 (cit. on
pp.3,5).
[Reg05] Oded Regev. On lattices, learning with errors, random linear codes, and cryptogra-
phy.STOC.2005,pp.84â€“93(cit.onpp.14,15).
[SZB21] MinJaeSong,IliasZadik,andJoanBruna.OntheCryptographicHardnessofLearn-
ing Single Periodic Neurons. Advances in Neural Information Processing Systems 34:
Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, De-
cember 6-14, 2021, virtual. Ed. by Marcâ€™Aurelio Ranzato, Alina Beygelzimer, Yann N.
Dauphin, Percy Liang, and Jennifer Wortman Vaughan. 2021, pp. 29602â€“29615 (cit.
onpp.2â€“4,6,7,10,16).
[Zad+22] IliasZadik,MinJaeSong,AlexanderSWein,andJoanBruna.Lattice-basedmethods
surpass sum-of-squares in clustering. Conference on Learning Theory. PMLR. 2022,
pp.1247â€“1248(cit.onp.4).
[Zar+24] Nikos Zarifis, Puqian Wang, Ilias Diakonikolas, and Jelena Diakonikolas. Robustly
LearningSingle-IndexModelsviaAlignmentSharpness.Forty-firstInternationalCon-
ferenceonMachineLearning.2024(cit.onp.2).
[ZG18] Ilias Zadik and David Gamarnik. High dimensional linear regression using lattice
basis reduction. Advances in Neural Information Processing Systems 31 (2018) (cit. on
p.4).
[Zha+19] Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-
layerrelunetworksviagradientdescent.The22ndinternationalconferenceonartificial
intelligenceandstatistics.PMLR.2019,pp.1524â€“1534(cit.onp.2).
[Zho+17] Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recov-
eryguaranteesforone-hidden-layerneuralnetworks.Internationalconferenceonma-
chinelearning.PMLR.2017,pp.4140â€“4149(cit.onp.2).