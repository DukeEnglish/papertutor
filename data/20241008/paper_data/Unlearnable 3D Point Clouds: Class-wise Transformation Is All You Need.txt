Unlearnable 3D Point Clouds: Class-wise
Transformation Is All You Need
âˆ—
XianlongWang1,2,4,5â€ ,MinghuiLiâ€¡,WeiLiu1,2,4,5â€ ,HangtaoZhang4,5â€ ,
ShengshanHu1,2,4,5â€ ,YechaoZhang1,2,4,5â€ ,ZiqiZhou1,2,3Â§,HaiJin1,2,3Â§
1NationalEngineeringResearchCenterforBigDataTechnologyandSystem
2ServicesComputingTechnologyandSystemLab 3ClusterandGridComputingLab
4HubeiEngineeringResearchCenteronBigDataSecurity
5HubeiKeyLaboratoryofDistributedSystemSecurity
â€ SchoolofCyberScienceandEngineering,HuazhongUniversityofScienceandTechnology
â€¡SchoolofSoftwareEngineering,HuazhongUniversityofScienceandTechnology
Â§SchoolofComputerScienceandTechnology,HuazhongUniversityofScienceandTechnology
{wxl99,minghuili,weiliu73,hangt_zhang,hushengshan,ycz,zhouziqi,hjin}@hust.edu.cn
Abstract
Traditionalunlearnablestrategieshavebeenproposedtopreventunauthorizedusers
fromtrainingonthe2Dimagedata. Withmore3Dpointclouddatacontaining
sensitivityinformation,unauthorizedusageofthisnewtypedatahasalsobecome
aseriousconcern. Toaddressthis,weproposethefirstintegralunlearnableframe-
workfor3Dpointcloudsincludingtwoprocesses: (i)weproposeanunlearnable
dataprotectionscheme,involvingaclass-wisesettingestablishedbyacategory-
adaptiveallocationstrategyandmulti-transformationsassignedtosamples;(ii)we
proposeadatarestorationschemethatutilizesclass-wiseinversematrixtransforma-
tion,thusenablingauthorized-onlytrainingforunlearnabledata. Thisrestoration
processisapracticalissueoverlookedinmostexistingunlearnableliterature,i.e.,
evenauthorizedusersstruggletogainknowledgefrom3Dunlearnabledata. Both
theoretical and empirical results (including 6 datasets, 16 models, and 2 tasks)
demonstratetheeffectivenessofourproposedunlearnableframework. Ourcodeis
availableathttps://github.com/CGCL-codes/UnlearnablePC
1 Introduction
Recently,3Dpointclouddeeplearninghasbeenmakingremarkablestridesinvariousdomains,e.g.,
self-driving[5]andvirtualreality[1,44]. Specifically,numerous3Dsensorsscanthesurrounding
environmentandsynthesizemassive3Dpointclouddatacontainingsensitiveinformationsuchas
pedestrianandvehicles[30]tothecloudserverfordeeplearninganalysis[10,21]. However,theraw
pointclouddatacanbeexploitedforpointcloudunauthorizeddeeplearningifadatabreachoccurs,
posingasignificantprivacythreat. Fortunately, theprivacyprotectionapproachesforpreventing
unauthorizedtraininghavebeenextensivelystudiedinthe2Dimagedomain[8,17,26,27,37,40].
Theyapplyelaborateperturbationsonimagessuchthattrainednetworksoverthemexhibitextremely
low generalization, thus failing to learn knowledge from the protected data, known as "making
dataunlearnable". Nonetheless,thestarkdisparitybetween2Dimagesand3Dpointcloudsposes
significantchallengesfordrawinglessonsfromexisting2Dsolutions.
Specifically, migrating 2D unlearnable schemes to 3D suffers from following challenges: (i) In-
compatibilitywith3Ddata. Numerousmodel-agnostic2Dimageunlearnableschemesoperate
âˆ—MinghuiLiisthecorrespondingauthor.
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
tcO
4
]VC.sc[
1v44630.0142:viXraOriginal sample Rotation Scaling Shear Reflection Translation
Transformations None Rotation Scaling Shear Twisting Tapering Reflection Translation
âˆ— âˆ— âˆ— âˆ—
Illustrations of 3D
point cloud samples
Changing the Changing the Stretching or Twisting the Narrowing or Creating a Changing the
Descriptions Clean sample
angles size compressing shape shortening mirror object position
Is it reversible?
Quantity of possible
Infinite Infinite Infinite Infinite Infinite 3 Infinite
transformations
Dimension of the
multiplicative
transformation matrix
ğŸ‘ğŸ‘Ã—ğŸ‘ğŸ‘ ğŸ‘ğŸ‘Ã—ğŸ‘ğŸ‘ ğŸ‘ğŸ‘Ã—ğŸ‘ğŸ‘ ğŸ‘ğŸ‘Ã—ğŸ‘ğŸ‘ ğŸ‘ğŸ‘Ã—ğŸ‘ğŸ‘ ğŸ‘ğŸ‘Ã—ğŸ‘ğŸ‘ ğŸ’ğŸ’Ã—ğŸ’ğŸ’
â„ â„ â„ â„ â„ â„ â„
Figure1:Anoverviewofexistingseventypesof3Dtransformations.â€œ*"denotesrigidtransformationsthatdo
notaltertheshapeofthepointcloudsamples,whiletheremainingtransformationsarenon-rigidtransformations.
inthepixelspace,suchasconvolutionaloperations[37,40],makingthemfailtobedirectlytrans-
ferredtothe3Dpointspace. (ii)Poorvisualquality. Migratingmodel-dependent2Dunlearnable
methods[4,8,17,26]to3Dpointcloudsrequiresperturbingsubstantialpoints,leadingtoirregular
three-dimensionalshiftswhichmaysignificantlydegradevisualquality. Hence,thesechallengesspur
ustostartdirectlyfromthecharacteristicsofpointcloudsforproposing3Dunlearnablesolutions.
Recentworksobservethat3Dtransformationscanaltertest-timeresultsofmodels[6,15,42]. To
explorethis,weconductanin-depthinvestigationintothepropertiesofseven3Dtransformationsas
showninFig.1andrevealthemechanismsbywhichtransformationsemployedinacertainpattern
serveasunlearnableschemes(Sec.3.2). Inlightofthis,weproposethefirstunlearnableapproachin
3Dpointcloudsviamulticlass-wisetransformation(UMT),transformingsamplestovariousforms
for privacy protection. Concretely, we newly propose a category-adaptive allocation strategy by
leveraginguniformdistributionsamplingandcategoryconstraintstoestablishaclass-wisesetting,
therebymultiplyingmulti-transformationstosamplesbasedoncategories. Totheoreticallyanalyze
UMT,wedefineabinaryclassificationsetupsimilartothatusedin[18,31,37]. Meanwhile,we
employaGaussianMixtureModel(GMM)[36]tomodelthecleantrainingsetandusetheBayesian
optimaldecisionboundarytomodelthepointcloudclassifier. Theoretically,weprovethatthere
existsaUMTtrainingsetfollowsaGMMdistributionandtheclassificationaccuracyofUMTdataset
islowerthanthatofthecleandatasetinaBayesianclassifier.
Moreover, anincompatibleissueinexistingunlearnableworks[8,17,26,27,37,41]wasidenti-
fied[52],i.e.,theseapproachespreventunauthorizedlearningtoprotecteddata,buttheyalsoimpede
authorized users from effectively learning from unlearnable data. To address this, we propose a
datarestorationschemethatappliesclass-wiseinversetransformations,determinedbyalightweight
messagereceivedfromtheprotector. OurproposedunlearnableframeworkincludingUMTapproach
anddatarestorationschemeisdepictedinFig.3.
Extensiveexperimentson6benchmarkdatasets(includingsyntheticandreal-worlddatasets)using
16 point cloud models across CNN, MLP, Graph-based Network, and Transformer on two tasks
(classificationandsemanticsegmentation),verifiedtheeffectivenessofourproposedunlearnable
scheme. Wesummarizeourmaincontributionsasfollows:
â€¢ The First Integral 3D Unlearnable Framework. To the best of our knowledge, we
proposethefirstintegralunlearnable3Dpointcloudframework,utilizingclass-wisemulti-
transformation as its unlearnable mechanism (effectively safeguarding point cloud data
against unauthorized exploitation) and proposing a novel data restoration approach that
leverages class-wise reversible 3D transformation matrices (addressing an incompatible
issueinmostexistingunlearnableworks,whereevenauthorizeduserscannoteffectively
learnknowledgefromunlearnabledata).
â€¢ TheoreticalAnalysis. Wetheoreticallyindicatetheexistenceofanunlearnablesituation
thattheclassificationaccuracyoftheUMTdatasetislowerthanthatofthecleandataset
underthedecisionboundaryoftheBayesclassifierinGaussianMixtureModel.
â€¢ ExperimentalEvaluation. Extensiveexperimentson3syntheticdatasetsand3real-world
datasetsusing16widelyusedpointcloudmodelarchitecturesonclassificationandsemantic
segmentationtasksverifythesuperiorityofourproposedschemes.
22 Preliminaries
Notation. Consideringtherawpointclouddata(X,Y)âˆˆX Ã—Y sampledfromacleandistribution
Dfortrainingapointcloudnetwork,theuserâ€™sgoalistoobtainamodelF :X â†’Y byminimizing
thelossfunction(e.g.,cross-entropyloss)L(F(X),Y). LetTâˆˆT bea3Dtransformationmatrix
thatdoesnotseriouslydamagethevisualqualityofpointclouds. NotethatX âˆˆR3Ã—p,T âˆˆR3Ã—3,
and p represents the number of points. In theoretical analysis, following [18, 31], we simplify a
trainingdatasetD toaGaussianMixtureModel(GMM)[36]N(yÂµ,I),wherey âˆˆ{Â±1}denotes
k
theclasslabels,ÂµâˆˆRddenotesthemeanvalue,andI âˆˆRdÃ—ddenotestheidentitymatrix. Thusthe
BayesoptimaldecisionboundaryforclassifyingD isdefinedbyP(x)â‰¡ÂµTx=0. Theaccuracy
k
ofthedecisionboundaryP onD isequaltoÏ•(||Âµ|| ),whereÏ•denotestheCumulativeDistribution
k 2
Function(CDF)ofthestandardnormaldistribution.
DataprotectorG . G aimstoprotecttheknowledgefromthecleantrainingset(withsizeofn)
p p
D = {X ,Y }n âˆ¼ Dbycompromisingtheunauthorizedmodelswhotrainontheunlearnable
c i i i=1
point cloud data {T (X ),Y }n , resulting in extremely poor generalization on the clean test
i i i i=1
distributionD âŠ†D. Thisobjectivecanbeformalizedas:
t
(cid:88)
max E L(F(X;Î¸ ),Y), s.t. Î¸ =argmin L(F(T (X );Î¸),Y ). (1)
u u i i i
(X,Y)âˆ¼Dt Î¸
(Xi,Yi)âˆˆDc
whereG assumesthattrainingsamplesarealltransformedintounlearnableoneswhilemaintain-
p
ing normal visual effects, in line with previous unlearnable works [17, 26, 37, 40]. By the way,
solvingEq.(1)directlyisinfeasibleforneuralnetworksbecauseitnecessitatesunrollingtheentire
trainingprocedurewithintheinnerobjectiveandperformingbackpropagationthroughittoexecutea
singlestepofgradientdescentontheouterobjective[7].
AuthorizeduserG . G aimstoapplyanothertransformationTâ€² âˆˆT ontheunlearnablesample,
a a
makingtheprotecteddatalearnable. Thisisformallydefinedas:
(cid:88)
min E L(F(X;Î¸ ),Y), s.t. Î¸ =argmin L(F(Tâ€²(T (X ));Î¸),Y ). (2)
r r i i i i
(X,Y)âˆ¼Dt Î¸
(Xi,Yi)âˆˆDc
where G assumes that, without access to any clean training samples, Tâ€² can be constructed by
a
utilizingalightweightmessageM receivedfromdataprotectors.
3 OurProposedUnlearnableSchemes
3.1 KeyIntuition
Several recent works [6, 11, 42] reveal employing 3D transformations can mislead the modelâ€™s
classificationresults. Suchaphenomenonimpliesthattheremightbesomedefectsinpointcloud
classifiers whenprocessing transformedsamples, leading usto infer that3D transformationsare
probablecandidatesfordataprotectionagainstunauthorizedtraining. Ifthetransformedpointcloud
dataareusedtotrainunauthorizedDNNs,onlysimplelinearfeaturesinherentin3Dtransformations
(atwhichtransformationsmayactasshortcuts[12])arecapturedbytheDNNs,successfullyprotecting
pointclouddataprivacy.
Ap trp al nyi sn fog r c mla as ts i- ow ni sse DNN Shortcut
â€œbedâ€
â€œchairâ€
Original Transfor- Establishing a new mapping â€œGtroouinled-tâ€
samples mations truth label
(a) (b)
Figure2: (a)TrainingonthetransformedModelNet10dataset(employingsample-wise, dataset-wise, and
class-wisepatterns)usingPointNetclassifier.(b)Thehigh-leveloverviewoftheclass-wisesetting.
3
â‹…â‹…â‹…
â‹…â‹…â‹…
â‹…â‹…â‹…
â‹…â‹…â‹…3.2 ExploringtheMechanism
Wesummarizeexisting3DtransformationsinFig.1andformallydefinetheminAppendixA.Toseek
clarityontheapplicationandselectionoftransformations,weexplorethreeaspects: (i)execution
mechanism,(ii)exclusionmechanism,and(iii)workingmechanismasfollows.
(i)WhichexecutionmechanismsuccessfullysatisfyEq.(1)? Theextensivelyemployedexecution
patternsin2Dunlearnableapproachesaresample-wise[8,17]andclass-wise[17,37]settings. We
furthercomplementthedataset-wisesetting(usinguniversaltransformation)andimplementtheabove
executionmechanismsfortrainingaPointNetclassifier[33]onthetransformedModelNet10[48],
obtainingtestaccuracyresultsinFig.2(a). Wediscoverthatmodelachievesconsiderablylowtest
accuracy under the class-wise setting, satisfying Eq. (1). Sample-wise and dataset-wise settings
donotobviouslycompromisemodelperformance, whichcannotserveaspromisingunlearnable
routes.Moreover,wenotethatsample-wisetransformationisoftenconsideredasadataaugmentation
schemetoimprovegeneralization,whichcontradictsouraimofusingclass-wisetransformationto
lowermodelgeneralization.
(ii)Whichtransformationsneedtobeexcluded? Notalltransformationsaresuitablecandidates.
We exclude three transformations, tapering, reflection, and translation. (1) The tapering matrix
maycausepointcloudsamplestobecomeaplanarprojectionwhenÎ·z definedinEq.(18)equals
to-1,renderingthetaperedsamplesmeaningless;(2)Thereflectionmatrixhasonlythreedistinct
transformationmatrices,renderingitincapableofassigningclass-wisetransformationswhenthe
numberofcategoriesexceedsthree;(3)Thetranslationtransformationisastraightforwardandsimple
additivetransformationthatistooeasilydefeatedbypointclouddatapre-processingapproaches.
(iii)Whydoesclass-wisetransformationwork? Weconductexperimentsusingclass-wisetrans-
formations(seeTab.6),indicatingthatthemodeltrainingontheclass-wisetransformedtraining
setachievesarelativelyhighaccuracyontheclass-wisetransformedtestset(usingthesametrans-
formation process as the training set). Besides, if we permute the class-wise transformation for
thetestset, weobtainasignificantlowaccuracyonthetestset. Therefore, weconcludethatthe
reasonwhyclass-wisetransformationworksisthatthemodellearnsthemappingbetweenclass-wise
transformationsandcorrespondingcategorylabelsasshowninFig.2(b),whichresultsinthemodel
beingunabletopredictthecorrespondinglabelsonacleantestsetlackingtransformations. This
analyticalprocessyieldsconclusionsthatareinagreementwithpriorresearch[37,42].
3.3 OurDesignforUMT
3.3.1 Category-AdaptiveAllocationStrategy
Weassigntransformationparametersbasedoncategoriestorealizeclass-wisesetting. Forrotation
transformationRâˆˆR3Ã—3,werefertoÎ±andÎ² asslightanglesimposedonthexandyaxes,Î³ asthe
primaryangleforzaxis. WegeneraterandomanglesforA timesinthreedirections:
N
(cid:108)âˆš (cid:109)
Î±,Î² âˆ¼U(0,r ),Î³ âˆ¼U(0,r ),A = 3N (3)
s p N
whereU denotesuniformdistribution,N denotesthenumberofcategories,r isasmallrangethat
s
controlsÎ±andÎ²,whiler isalargerangethatcontrolsÎ³. A iscomputedinsuchawaytoensure
p N
thatthenumberofcombinationsofthreeanglesisgreaterthanorequaltoN. Concretely,inthe
rotationoperation,eachofthethreedirectionshasA distinctangles,whichmeansthatthefinal
N
rotationmatrixhasA3 possiblecombinations. Tosatisfytheclass-wisesetup,A3 mustbeatleast
N (cid:108)âˆš (cid:109) N
N,requiringA tobenolessthan 3N . Finally,werandomlyselectN combinationsofangles
N
fortheallocation. ThescalingtransformationS âˆˆ R3Ã—3 resizesthepositionofeachpointinthe
3D point cloud sample by a certain scaling factor Î», which is sampled N times from a uniform
distributionU:
Î»âˆ¼U(b ,b ) (4)
l u
where b and b represent the lower bound and upper bound of the scaling factor, respectively.
l u
ForshearH âˆˆ R3Ã—3 definedinEq.(13), twistingW âˆˆ R3Ã—3 definedinEq.(16), theprocessof
generatingparameterswithinranges(Ï‰ ,Ï‰ )and(h ,h )isconsistenttoscaling. Therangeofthese
l u l u
parametersensuresthevisualeffectofthesample.
4Our unlearnable scheme: UMT Airplane
(i-thclass)
Category-Adaptive Allocation Strategy Data Restoration Scheme
Number of Uniform distribution U Parameter lists Secure Reverse transformation Training
categories N
ğ‘˜ğ‘˜,ğ‘Ÿğ‘Ÿğ‘ ğ‘ ,ğ‘Ÿğ‘Ÿğ‘ğ‘,ğ‘ğ‘ğ‘™ğ‘™,ğ‘ğ‘ğ‘¢ğ‘¢â€¦, ,ğ‘¤ğ‘¤ğ‘™ğ‘™,ğ‘¤ğ‘¤ğ‘¢ğ‘¢ â„’â„›,â„’ğ‘†ğ‘†,â€¦,â„’â„‹
(Lt igra hn twsp eo igr ht t) Aut uh so er rized Training Unau ut sh eo rrized
Employing Class-wise
Data protector Transformations Z mulM tipa lt icri ax t ion Z Legally accessed Z Illegally accessed
Assigning class-wise ğ›¾ğ›¾ğ‘–ğ‘– ğœ˜ğœ˜ğ‘–ğ‘–
parameters O X ï¿½ O ï¿½ ï¿½ ğœ†ğœ†ğ‘–ğ‘– X O X
â€¦
â€¦
Input raw 3D point
cloud data (w/o
ğ›½ğ›½ğ‘–ğ‘– Y ï¿½ ğ›¼ğ›¼ğ‘–ğ‘– Yï¿½ ï¿½ ï¿½ Yï¿½ ï¿½
any protection) Class-wise rotation Class-wisescaling Class-wiseshear Unlearnable
Raw point cloud data transformation transformation transformation point cloud data
ğ“¡ğ“¡ ğ‘ºğ‘º ğ‘¯ğ‘¯
Figure3:Anoverviewofourproposedintegralunlearnablepipeline
Property1. SincerotationmatricesR ,R ,andR aroundthreedirectionsareallorthogonal
Î± Î² Î³
matrices,Risalsoanorthogonalmatrix,whichcanbedefinedas:
âˆ€Mâˆˆ{R ,R ,R ,R}, s.t. MMT =I (5)
Î± Î² Î³
wherewecandeterminetheorthogonalitybymatrixmultiplicationthroughthedefinitionsofEq.(11).
SinceR=R R R andRRT =R R R RTRTRT =I,soRisalsoanorthogonalmatrix.
Î± Î² Î³ Î± Î² Î³ Î³ Î² Î±
Property2. Allfourtransformationmatricesweemploy,R,S,W,andH,andthemultiplicative
combinationsofanythesematricesareallinvertiblematrices,whichcanbeformallydefinedas:
âˆ€J âˆˆ{f(R)f(S)f(W)f(H)|f(x)âˆˆ{x,I}}, âˆƒK s.t. JK=KJ =I (6)
wheretheinversematricesofR,S,W,andHaregiveninAppendixA.Thispropertyallowsthe
authorizeduserstonormallytrainontheprotecteddataduetothatmultiplyingamatrixbyitsinverse
resultsintheidentitymatrix,leadingustoproposeadatarestorationschemeinSec.3.4.
3.3.2 EmployingClass-wiseTransformations
AssumingthepointcloudtrainingsetD isdefinedas{(X ,Y ),(X ,Y ),...,(X ,Y )}N ,
c c1 i c2 i cni i i=1
wheren ,n ,...,n representthenumberofsamplesinthe1st,2nd,...,N-thcategory,respectively.
1 2 N
Weformallydefinethespectrumoftransformationsasktoindicatethenumberoftransformations
involved. ThustheultimateunlearnabletransformationmatrixT isdefinedas:
k
T
=(cid:81)k
V , âˆ€iÌ¸=j, s.t. V ,V âˆˆ{R,S,W,H},V Ì¸=V (7)
k i=1 i i j i j
Onceweemploytheproposedcategory-adaptiveallocationstrategytoT ,theunlearnablepoint
k
clouddatasetD isconstructedas:
u
D ={(T (X ),Y ),(T (X ),Y ),...,(T (X ),Y )}N (8)
u ki c1 i ki c2 i ki cni i i=1
OurproposedUMTschemeisdescribedinAlgorithm1. Weenumeratepossibletransformations
inEq.(7)toobtaintheunlearnabilityinTab.7andselectonetypeofclass-wisetransformationfor
eachkformorecomprehensiveexperimentsinTab.1. TofacilitatethetheoreticalstudyofUMT2,we
optforRS asthetransformationmatrixT,whichachievesthebestunlearnableeffectassuggested
inTabs.1and7. Thus, intheGMMscenario, theclass-wisetransformationmatrixisdefinedas
T =R S =Î» R âˆˆRdÃ—d,whereÎ» âˆˆRisthescalingfactor.
y y y y y y
Lemma3. TheunlearnabledatasetD generatedusingUMTonD canalsoberepresentedusinga
u c
GMM,i.e.,D âˆ¼N(yT Âµ,Î»2I).
u y y
Proof: See Appendix D.1. Lemma 3 demonstrates that the unlearnable dataset D can also be
u
representedasaGMM,whichisderivedfromProperty1.
Lemma4. TheBayesoptimaldecisionboundaryforclassifyingD isgivenbyP (x)â‰¡AxâŠ¤x+
u u
BâŠ¤x+C =0,whereA=Î»âˆ’2âˆ’Î»âˆ’2,B =2(Î»âˆ’2T +Î»âˆ’2T )Âµ,andC =ln |Î»2 âˆ’1I| .
âˆ’1 1 âˆ’1 âˆ’1 1 1 |Î»2I|
1
Proof: SeeAppendixD.2. Lemma4revealsthattheBayesiandecisionboundaryforclassifyingD
u
isaquadraticsurfacebasedontheGMMexpressionofD .
u
2Inthesubsequenttheoreticaldescriptions,UMTreferstoUMTwithk=2usingtransformationmatrixRS.
5Lemma5. Letz âˆ¼N(0,I),Z =zâŠ¤z+bâŠ¤z+c,whereb= B,c= C,andâˆ¥Â·âˆ¥ denote2-normof
A A 2
vectors. Foranytâ‰¥0andÎ³ âˆˆR,weemployChernoffboundtohave:
(cid:110) (cid:111)
exp t2 ||b||2âˆ’t(Î³+d)
P{Z â‰¥E[Z]+Î³}â‰¤ 2(1âˆ’2t) 2
|(1âˆ’2t)I|21
Proof: SeeAppendixD.3. Lemma5enablesustoestablishanupperboundontheaccuracyofthe
unlearnabledecisionboundaryP appliedtothecleandatasetD ,denotedasÏ„ (P ),aspresented
u c Dc u
inTheorem6below.
Theorem6. Foranyconstantt andt satisfying0â‰¤t < 1 and0â‰¤t < 1,theaccuracyofthe
1 2 1 2 2 2
unlearnabledecisionboundaryP onD canbeupper-boundedas:
u c
Ï„ (P )â‰¤
exp(cid:110) 2(1âˆ’t2 1 2t1)||b+2Âµ||2 2+t 1(ÂµâŠ¤Âµ+bâŠ¤Âµ+c)(cid:111)
Dc u 2|(1âˆ’2t 1)I|21
+
exp(cid:110) 2(1âˆ’t2 2 2t2)||bâˆ’2Âµ||2 2âˆ’t 2(ÂµâŠ¤Âµâˆ’bâŠ¤Âµ+c+2d)(cid:111)
2|(1âˆ’2t 2)I|1 2
:=p +p
1 2
Furthermore,ifÂµâŠ¤Âµ+bâŠ¤Âµ+c+d < 0andâˆ’ÂµâŠ¤Âµ+bâŠ¤Âµâˆ’câˆ’d < 0,wehaveÏ„ (P ) < 1.
Dc u
Moreover,foranyÂµÌ¸=0,âˆƒmatrixT suchthatÏ„ (P )<Ï„ (P),whereP istheBayesoptimal
i Dc u Dc
decisionboundaryforclassifyingD .
c
Proof: SeeAppendixD.4. TheunlearnableeffecttakesplacewhenÏ„ (P )<Ï„ (P). Toachieve
Dc u Dc
this, we elaborately choose T , which is formalized as
ÂµâŠ¤Î»âˆ’ âˆ’2 1TâŠ¤ âˆ’1+Î»âˆ’ 12TâŠ¤
1 Âµ â‰ª 0. Therefore,
y Î»âˆ’2âˆ’Î»âˆ’2
âˆ’1 1
Theorem6theoreticallyexplainswhyUMTiseffectiveingeneratingunlearnablepointclouddata.
3.4 DataRestorationScheme
Toensurethatauthorizeduserscanachievebettergeneralizationaftertrainingonunlearnabledata,
i.e.,satisfyingEq.(2),weexploittheinversepropertiesof3Dtransformations,presentedinProperty
2,tocalculatetheinversematrixofT as:
k
T âˆ’1 =(cid:81)1 V âˆ’1, âˆ€iÌ¸=j, s.t. V âˆ’1,V âˆ’1 âˆˆ{Râˆ’1,Sâˆ’1,Wâˆ’1,Hâˆ’1},V âˆ’1 Ì¸=V âˆ’1 (9)
k i=k i i j i j
In particular, we note that Râˆ’1 = RT, Sâˆ’1 = 1I. Afterwards, the authorized user receives a
Î»
lightweightmessageM containingclass-wiseparametersfromthedataprotectorthroughasecure
channel, therebyassigningM totheinversetransformationmatrixinEq.(9)formultiplyingthe
unlearnablesamples. OurproposedintegralunlearnableprocessisillustratedinFig.3.
4 Experiments
4.1 ExperimentalDetails
DatasetsandModels. Threesynthetic3Dpointclouddatasets,ModelNet40[48],ModelNet10[48],
ShapeNetPart[3]andthreereal-worlddatasetsincludingautonomousdrivingdatasetKITTI[30]
and indoor datasets ScanObjectNN [39], S3DIS [2] are used. We choose 16 widely used 3D
point cloud models PointNet [33], PointNet++ [34], DGCNN [43], PointCNN [23], PCT [14],
PointConv [46], CurveNet [49], SimpleView [13], 3DGCN [24], LGR-Net [57], RIConv [55],
RIConv++ [56], PointMLP [29], PointNN [54], PointTransformerV3 [47], and SegNN [60] for
evaluationofclassificationandsemanticsegmentationtasks.
Experimental setup. The training process involves Adam optimizer [20], CosineAnnealingLR
scheduler[28],initiallearningrateof0.001,weightdecayof0.0001. Weempiricallysetr ,r ,b ,b ,
s p l u
Ï‰ ,Ï‰ ,h ,andh to15â—¦,120â—¦,0.6,0.8,0â—¦,20â—¦,0,and0.4respectively. Themainresultsofdifferent
l u l u
konunlearnabilityisshowninTab.1.Specifically,k =1usesR,k =2usesRS,k =3usesRSW,
andk =4usesRSWH. Moreresultsfordifferentcombinationsofclass-wisetransformationsare
providedinTab.7. Thetablevaluescoveredby gray denotethebestunlearnableeffect.
6Datasets Schemes PointNet PointNet++ DGCNN PointCNN PCT PointConv CurveNet SimpleView RIConv++ 3DGCN PointNN PointMLP AVG
Clean 89.85Â±0.54 92.11Â±1.57 91.81Â±0.84 87.99Â±1.58 90.18Â±2.23 91.26Â±1.12 91.88Â±1.51 89.52Â±0.41 87.90Â±1.64 88.51Â±5.39 83.22Â±0.73 91.07Â±0.19 89.61Â±0.42
UMT(k=1) 40.64Â±10.69 28.73Â±2.00 27.41Â±6.71 32.82Â±2.70 31.58Â±4.27 33.64Â±6.63 39.78Â±6.80 45.31Â±11.85 86.72Â±1.46 28.01Â±8.72 34.47Â±0.29 32.85Â±2.35 38.50Â±2.01
ModelNet10 UMT(k=2) 21.18Â±0.93 26.36Â±1.71 18.84Â±6.14 21.97Â±4.04 19.72Â±4.52 20.84Â±5.96 25.04Â±2.15 22.75Â±2.43 16.53Â±3.63 32.79Â±3.23 31.94Â±2.11 25.41Â±3.96 23.61Â±1.06
UMT(k=3) 22.84Â±1.71 23.81Â±8.43 29.16Â±4.30 27.03Â±9.97 29.43Â±9.11 18.49Â±7.39 25.51Â±9.57 32.12Â±12.58 19.94Â±1.13 36.19Â±13.38 30.54Â±1.24 32.21Â±5.28 27.27Â±6.20
UMT(k=4) 18.83Â±2.40 20.56Â±14.61 15.92Â±1.51 20.52Â±6.06 20.29Â±2.14 21.66Â±2.24 23.46Â±12.58 24.12Â±6.20 26.41Â±2.47 34.28Â±17.20 25.59Â±0.25 26.65Â±11.36 23.19Â±5.98
Clean 86.18Â±0.07 90.55Â±0.73 89.51Â±0.86 81.89Â±5.35 87.11Â±1.39 88.90Â±0.89 87.82Â±0.23 85.25Â±0.31 84.59Â±1.07 86.81Â±1.69 74.81Â±0.16 87.31Â±0.91 85.89Â±0.40
UMT(k=1) 28.62Â±1.80 20.69Â±0.87 28.60Â±1.65 21.92Â±1.86 29.06Â±0.38 24.99Â±2.63 33.29Â±3.92 26.89Â±1.48 75.86Â±8.43 26.30Â±1.92 28.57Â±0.39 29.28Â±1.72 31.17Â±0.20
ModelNet40 UMT(k=2) 8.30Â±0.87 18.71Â±1.65 11.60Â±1.97 10.85Â±2.61 9.21Â±2.31 12.99Â±0.99 12.71Â±3.70 12.05Â±3.88 10.48Â±2.01 26.17Â±0.35 25.05Â±0.06 10.90Â±4.74 14.09Â±0.78
UMT(k=3) 7.48Â±1.66 17.62Â±3.10 10.41Â±5.48 9.43Â±3.09 9.91Â±4.19 11.28Â±4.93 11.23Â±3.07 11.21Â±4.09 10.92Â±0.48 25.31Â±1.29 24.53Â±0.10 8.94Â±3.43 13.19Â±2.41
UMT(k=4) 7.83Â±2.25 15.72Â±4.12 10.86Â±2.57 8.60Â±2.28 9.68Â±1.58 10.52Â±2.09 10.91Â±3.00 14.32Â±0.92 18.09Â±3.83 17.16Â±5.47 24.31Â±0.28 12.38Â±1.51 13.36Â±0.43
Clean 98.21Â±0.03 98.50Â±0.12 98.06Â±0.65 97.54Â±0.22 96.38Â±0.84 98.23Â±0.23 98.42Â±0.08 98.26Â±0.21 96.70Â±0.64 96.18Â±1.90 94.66Â±0.11 98.39Â±0.12 97.46Â±0.22
UMT(k=1) 63.85Â±12.71 50.30Â±21.71 64.71Â±13.44 51.95Â±14.13 54.39Â±3.36 29.34Â±0.14 71.38Â±9.34 56.22Â±6.48 96.93Â±0.09 37.90Â±9.67 58.44Â±0.61 47.32Â±8.26 56.89Â±3.00
ShapeNetPart UMT(k=2) 18.41Â±6.45 45.61Â±4.64 25.99Â±2.73 28.97Â±3.73 37.72Â±16.2625.60Â±8.59 26.49Â±8.58 38.38Â±5.96 5.05Â±3.63 32.21Â±13.01 49.82Â±1.56 34.19Â±4.43 30.70Â±1.43
UMT(k=3) 32.50Â±10.03 39.64Â±12.52 37.29Â±11.20 46.77Â±19.59 43.52Â±22.2431.28Â±4.67 37.27Â±10.30 41.51Â±2.66 6.01Â±6.11 47.84Â±7.05 50.85Â±0.07 47.80Â±21.37 38.52Â±6.17
UMT(k=4) 23.72Â±15.04 23.30Â±4.43 36.18Â±10.00 34.52Â±16.15 45.07Â±18.9929.48Â±11.09 29.79Â±4.79 40.04Â±9.97 32.22Â±1.38 33.06Â±28.16 51.91Â±0.03 40.93Â±21.81 35.02Â±11.23
Clean 98.04Â±2.23 99.49Â±0.50 99.33Â±0.34 99.23Â±0.25 98.93Â±0.54 98.04Â±1.68 99.10Â±1.05 99.38Â±0.33 99.64Â±0.09 99.67Â±0.24 99.49Â±0.50 95.72Â±5.46 98.84Â±0.87
UMT(k=1) 36.23Â±30.18 62.90Â±20.51 38.93Â±33.08 57.80Â±13.99 58.26Â±21.6639.99Â±6.82 33.33Â±10.75 56.71Â±30.96 98.53Â±1.19 68.34Â±10.46 70.34Â±0.98 47.20Â±20.79 55.71Â±5.54
KITTI UMT(k=2) 31.24Â±7.11 51.07Â±20.29 27.90Â±10.33 49.69Â±11.52 51.47Â±17.3625.95Â±11.38 20.55Â±4.93 51.77Â±8.53 99.80Â±0.09 70.42Â±27.97 47.31Â±10.5839.90Â±10.00 47.26Â±5.95
UMT(k=3) 19.13Â±6.93 53.73Â±12.44 31.26Â±21.68 56.08Â±17.90 54.45Â±11.7742.54Â±21.46 27.38Â±19.20 32.62Â±11.76 98.48Â±1.61 69.51Â±8.27 70.14Â±1.30 58.44Â±6.61 51.15Â±4.79
UMT(k=4) 26.84Â±16.26 61.79Â±6.65 29.89Â±15.72 54.27Â±11.99 64.21Â±10.7657.29Â±12.05 37.70Â±11.60 54.63Â±13.07 99.34Â±0.23 72.33Â±3.47 70.49Â±0.76 56.25Â±12.33 57.09Â±5.97
Clean 65.20Â±1.49 77.38Â±2.60 72.66Â±2.56 66.96Â±6.42 50.75Â±15.1675.42Â±2.06 70.92Â±1.06 51.93Â±2.89 66.34Â±1.21 73.84Â±2.75 58.17Â±0.30 73.32Â±1.39 66.91Â±0.72
UMT(k=1) 56.55Â±0.11 63.01Â±2.38 57.93Â±3.11 51.97Â±11.99 47.05Â±7.12 56.71Â±3.61 65.49Â±3.15 38.96Â±2.08 62.33Â±10.68 52.89Â±1.13 54.56Â±0.17 62.96Â±4.19 55.87Â±0.95
ScanObjectNNUMT(k=2) 14.55Â±1.81 49.41Â±3.44 20.96Â±3.99 13.61Â±4.97 15.10Â±2.05 20.78Â±4.74 21.35Â±2.59 20.73Â±9.64 10.76Â±1.76 52.37Â±5.62 48.25Â±0.20 16.32Â±4.78 25.35Â±0.49
UMT(k=3) 10.92Â±3.87 41.96Â±3.84 14.62Â±5.56 10.62Â±2.35 22.02Â±7.83 19.96Â±3.13 21.43Â±8.67 11.79Â±3.96 11.69Â±1.41 56.32Â±0.83 48.94Â±0.70 23.05Â±16.32 24.44Â±2.46
UMT(k=4) 5.77Â±2.10 34.65Â±8.31 12.16Â±4.84 9.87Â±0.95 17.86Â±11.6312.79Â±7.46 15.28Â±4.86 20.09Â±6.26 31.83Â±2.20 46.06Â±14.07 45.27Â±0.17 8.76Â±3.18 21.70Â±3.61
Table1:Mainresults:Theaveragetestaccuracy(%)resultswithstandarddeviationsfromthreeruns(random
seedsaresetto23,1023,and2023)ofclassificationmodelstrainedontheUMTdatasets.
Pre-processschemes PointNet PointNet++ DGCNN PointCNN CurveNet SimpleView RIConv++ 3DGCN PointNN PointMLP AVG
Cleanbaseline 86.10 91.13 89.02 75.73 87.76 85.49 85.82 84.89 75.00 87.66 84.86
SOR 9.93 19.17 9.36 10.45 11.57 14.81 6.66 25.08 25.04 15.22 14.73
SRS 9.24 22.20 10.25 7.78 12.66 14.04 11.32 24.72 26.01 12.18 15.04
Randomrotation 10.94 26.74 11.59 10.21 12.13 6.86 12.09 55.64 29.74 20.37 19.63
Randomscaling 22.33 22.45 30.15 20.22 25.61 23.25 73.62 27.48 26.50 24.39 29.60
Randomjitter 9.64 21.72 10.05 7.74 9.98 16.68 13.27 23.74 26.90 9.09 14.88
Randomrotation&scaling 63.49 34.44 44.65 37.56 72.89 51.62 78.04 64.00 36.26 78.94 56.19
Table2:Robustnessresults:Thetestaccuracy(%)resultsonUMT-ModelNet40againstpre-processschemes.
Evaluation Metrics. For classification, we report the test accuracy (in %) derived from the
classificationaccuracyoncleantestset,whichalignswiththemetricsusedinthe2Dunlearnable
schemes[17,26,37]. Forsemanticsegmentation,weuseevalaccuracyandmeanIntersectionover
Union(mIoU)asevaluationmetrics(in%),whereevalaccuracyistheratioofthenumberofpoints
classifiedcorrectlytothetotalnumberofpointsinthepointcloud,mIoUcalculatestheIoUforeach
classbetweenthegroundtruthandthepredictedsegmentation[59],andthentakestheaverageof
theseratios. Thelowerthesemetricsare,thebettertheeffectoftheunlearnablescheme.
4.2 EvaluationofProposedUnlearnableSchemes
Effectiveness. AsshowninTab.1,UMTresults
inasignificantdecreaseintestaccuracycompared 100 ModelNet10 100 ShapeNetPart
to clean baseline, indicating the unlearnable ef-
80 80
fectiveness of UMT. Moreover, all values of k
achievegoodunlearnableresults. Theaverageper- 60 C R Uel Me sa Tton dr d aa ta t ait o sa n es te dt ataset 60 C R Uel Me sa Tton dr d aa ta t ait o sa n es te dt ataset
40 40
formance of RS is the best, which is employed
20 20
asthedefaultintheremainingexperiments. We
PointNet PointNet++ DGCNN PointCNN PointNet PointNet++ DGCNN PointCNN
notethatrigidtransformationsareeasilydefeated Figure4:Thetestaccuracy(%)resultsobtainedafter
byinvariancenetworks[9,24,56]. Therefore,in trainingontheclean,UMT,andrestorationdatasets.
practicalsettings,wewillincludenon-rigidtrans-
formations,usingcombinedtransformationstoenhancetherobustnessoftheUMTscheme.
Robustness. (i)Dataaugmentations. Similarto[16,22],weemploydataaugmentationslikerandom
scaling,randomjitter,randomrotation,StatisticOutlierRemoval(SOR)[58],andSimpleRandom
Sampling(SRS)[51]againstUMT. Tab.2suggestsUMTisrobusttorandomdataaugmentations.
SORdetectsandremovesoutliersornoisypointsbutisineffectiveincounteringUMTbecauseUMT
doesnotintroduceirregularperturbationsoraddoutliers. SRSrandomlyselectsasmallsubsetfrom
theentiresetofpointswithequalprobability. UMTisrobusttoSRSasitaltersthecoordinatesof
allpoints. Evenifanarbitrarysubsetischosen,allpointswithinthesubsethavealreadyundergone
the unlearnable transformations. (ii) Adaptive attack. We assume the unauthorized user gains
knowledgeabouttheG â€™suseofRS. Thusweproposerandomrotation&scalingasanadaptive
p
scheme. InTab.2,theadaptiveschemeexhibitsahigheraccuracythanotherschemes,confirmingits
effectiveness. Nonetheless,itremains28.67%lowerthancleanbaseline,revealingtherobustnessof
UMTagainstadaptiveattack. MoreresultsofadaptiveattacksareprovidedinAppendixC.3.
7
)%(
ycarucca
tseT
)%(
ycarucca
tseTEvaluationmetrics Evalaccuracy(%) mIoU(%)
Segmentationmodels PointNet++[34] PointTransformerv3[47] SegNN[60] AVG PointNet++[34] PointTransformerv3[47] SegNN[60] AVG
Cleanbaseline 74.76 74.72 79.00 76.16 40.06 40.57 50.27 43.63
UMT(k=2) 15.89 25.61 66.41 35.97 7.45 18.32 46.30 24.02
Table3:Semanticsegmentation:EvaluationofUMTonsemanticsegmentationtaskusingS3DISdataset.
Datasets Modulesâ†“Modelsâˆ’â†’ PointNet PointNet++ DGCNN PointCNN CurveNet SimpleView RIConv LGR-Net 3DGCN PointNN PointMLP AVG
ModuleR 27.19 20.02 28.61 21.64 36.87 28.32 88.25 40.24 24.59 28.77 27.72 33.84
ModelNet40 ModuleS 9.36 48.70 15.07 7.41 13.92 20.33 32.25 12.56 88.72 65.32 18.79 30.22
UMT(k=2) 9.20 18.52 13.86 9.08 13.90 7.58 24.39 9.85 26.58 25.12 7.31 15.04
ModuleR 29.85 30.51 35.13 31.17 47.25 58.70 91.38 43.72 36.50 34.58 30.47 42.66
ModelNet10 ModuleS 34.80 57.05 42.62 33.81 30.36 43.75 42.24 32.60 89.40 73.79 41.29 47.43
UMT(k=2) 22.25 28.08 14.10 21.48 24.34 21.37 38.18 19.82 31.28 33.37 29.35 25.78
Table4:Ablationstudy: Thetestaccuracy(%)resultsderivedfromunlearnabledatawithdifferentmodules.
VisualEffect. WevisualizeUMTsamplesinFigs.7to10,indicatingthattheunlearnablepointcloud
samplesstillretaintheirnormalfeaturestructurewithvisualrationality.
EvaluationofSemanticSegmentation. WeevaluateUMTusingcommonmetricsforpointcloud
semanticsegmentationtasksinTab.3. Ascanbeseen,theperformanceofsemanticsegmentationof
dataprotectedbyUMTsignificantlydecreases. TheunderlyingreasonisthattheDNNslearnthe
featuresofclass-wisetransformationsandestablishanewmapping,whichleadstotheinabilityof
testsampleswithouttransformationstobecorrectlysegmentedbythesegmentationmodel.
EvaluationofDataRestoration. WemultiplyUMTsamplesbythetransformationmatrixinEq.(9).
Thedatabecomeslearnableaftertherestorationprocess,withtestaccuracyreachingalevelcompara-
bletothecleanbaselineasshowninFig.4. Thisstronglyvalidatestheeffectivenessoftheproposed
datarestorationscheme.
(b)
Figure5:Hyper-parametersensitivityanalysis:Theimpactofhyperparametersr ,r ,b,andb onthetest
s p l u
accuracyresults(%)ontheUMT(usingRS)ModelNet10dataset.
4.3 AblationStudyandHyper-ParameterSensitivityAnalysis
AblationonRotationModule. AsshowninTab.4,theaverageaccuracyincreasesby15.18%and
21.65%,respectively,whenonlyusingS. Thissuggeststheimportanceofclass-wiserotationmodule.
Thehightestaccuracydemonstratedby3DGCN[24]canbeattributedtoitsscalinginvariance,which
endowsitwithrobustnessagainstscalingtransformation.
AblationonScalingModule. AsalsoshowninTab.4,theaverageaccuracyincreasesby18.80%
and16.88%whenonlyusingtherotationmodule, respectively. Thehightestaccuracyachieved
onRIConv[55]andLGR-Net[57]isduetothefactthatbothnetworksarerotation-invariant,thus
providingresistanceagainstrotationtransformations. Theseablationresultsfurthermoreemphasize
theimportanceofincorporatingmorenon-rigidtransformations.
Hyper-Parameter Analysis. We analyze four hyperparameters r , r , b , and b in Fig. 5. The
s p l u
influenceofr andr ontheaccuracyremainsrelativelysmall, exhibitingtheirbestunlearnable
s p
effect when set to 15â—¦ and 120â—¦, respectively. We attribute this to the crucial role played by the
class-wisesetting,whileitisnothighlysensitivetothesizeofspecificvalues. Theunlearnableeffect
isthebestwhenb andb aresetto0.6and0.8,respectively. Similarly,thevariationsinb andb do
l u l u
notsignificantlyaltertheeffectduetotheclass-wisesetting.
4.4 InsightfulAnalysisIntoUMT
WeformalizeL(D)=E [L (F(X;Î¸),Y)],whereL isthecross-entropyloss,X,Yisthe
(X,Y)âˆ¼D c c
pointclouddatasampledfromdatasetD. WedefineD ,D ,andD asthetrainingset,unlearnable
tr u c
8With clean training data With clean training data
ğœ½ğœ½ğ’•ğ’•âˆ’ğŸğŸ ğœ½ğœ½ğ’•ğ’• ğœ½ğœ½ğ’•ğ’•+ğŸğŸ ğœ½ğœ½ğ’•ğ’•âˆ’ğŸğŸ ğœ½ğœ½ğ’•ğ’• ğœ½ğœ½ğ’•ğ’•+ğŸğŸ
model w ğœ½ğœ½eğŸğŸight trW ainit ih
n
gU M daT
t a
Low
ğ“›ğ“›ğ’•ğ’•ğ’•ğ’•ğ’•ğ’•ğœ½ğœ½ ğ’•ğ’•ğ’•ğ’•ğ’„ğ’„âˆ— model
ğœ½ğœ½
wğŸğŸeight trW ainit ih
n
gU M daT
t a
âˆ—L ğœ½ğœ½o ğ’„ğ’„âˆ—w ğ“›ğ“›ğ’•ğ’•ğ’•ğ’•ğ’•ğ’•ğ’•ğ’•ğ’•ğ’•
ğœ½ğœ½ğ’–ğ’–
âˆ—
(a)
ğœ½ğœ½ğ’–ğ’–
(b) Low ğ“›ğ“›ğ’–ğ’–
Figure6: UMTinweightspace. ThebluearrowrepresentsthecleantrainingtrajectoryoftheweightsÎ¸ at
i
stepi,whiletheredarrowsdenotetheUMTtrainingtrajectory.Thevaluesforplottingthisfigureareprovided
inAppendixC.4.(a)Testingoncleantestset(blueandredellipses);(b)TestingonUMTtestset(greenellipse).
testset(i.e.,testsettransformedbyUMT),andcleantestset,respectively. Thuswehavethetraining
lossL =L(D ),unlearnabletestlossL =L(D ),andcleantestlossL =L(D ).
train tr u u c c
The models trained on both clean and UMT training sets exhibit low L as shown in Fig. 6
train
(yellowellipses),indicatingthemodelsconvergewellduringtraining. Furthermore,whentestedon
D asshowninFig.6(a),thecleanmodel(trainedwithcleantrainingset)achievesalowL (blue
c c
ellipse),whiletheUMTmodel(trainedwithUMTtrainingset)exhibitsahighL (redellipse),also
c
supportingtheunlearnableeffectivenessofUMT. Fig.6(b)revealsthatthecleanmodelandUMT
modelbothexhibitlowL (greenellipse),suggestingthattheycanclassifyUMTsamples. Butthe
u
mechanismsunderlyingthetwocasesoflowL differ. Thecleanoneisduetothatthesemanticsof
u
samplescanberemainedbyUMTandthusnormallyclassified. TheUMToneisthattheUMTmodel
learnsthemappingbetweentransformationsandlabels, therebycorrectlypredictingthesamples
containingthesametransformations. Weconcludethatthecleanmodeleffectivelyclassifiesboth
clean and UMT samples, while the UMT model successfully classifies UMT samples (the UMT
processisthesameforbothtrainingandtestsamples)butfailstoclassifycleansamples.
5 RelatedWork
5.1 2DUnlearnableSchemes
Thedevelopmentof2Dunlearnableschemes[17,27,35,37,40,53]hasbeenbooming. Specifically,
model-dependentmethodsareinitiallyproposedinabundance[8,17,27]. Afterwards,numerous
model-agnosticmethodsthatsignificantlyimprovethegenerationefficiencyhavesurfaced[37,40,45].
However, duetothestructuraldisparitiesbetween3Dpointclouddataand2Dimages, applying
unlearnablemethodsdirectlyfrom2Dto3Drevealssignificantchallenges.
5.2 Protecting3DPointCloudDataPrivacy
Some works proposed an encryption scheme based on chaotic mapping [19] or optical chaotic
encryption[25],anda3Dobjectreconstructiontechniquewasintroduced[32],bothachievingprivacy
protectionforindividual3Dpointclouddata. Nevertheless,noprivacy-preservingsolutionhasbeen
proposedspecificallyforthescenarioofunauthorizedDNNlearningonabundantraw3Dpointcloud
data. Itisworthmentioningthatbothparallelworks[42,61]studiedavailabilitypoisoningattacks
against3Dpointcloudnetworks,whichlargelyreducemodelaccuracy,andbothhavethepotential
tobeappliedasunlearnableschemes. However,thefeaturecollisionerror-minimizationpoisoning
schemeproposedbyZhuetal.[61]overlookstheproblemofeffectivetrainingforauthorizedusers,
which limits its practical use in real-world applications. The rotation-based poisoning approach
proposedbyWangetal.[42]iseasilydefeatedbyrotation-invariantnetworks[55,56,57],asrevealed
inTabs.1and4.
6 Conclusion,Limitation,andBroaderImpacts
Inthisresearch,weproposethefirstintegralunlearnableframeworkin3Dpointclouds,whichutilizes
class-wisemultitransformations,preventingunauthorizeddeeplearningwhileallowingauthorized
training. Extensiveexperimentsonsyntheticandreal-worlddatasetsandtheoreticalevidenceverify
thesuperiorityofourframework. Thetransformationsincluderotation,scaling,twisting,andshear,
9whichareallcommon3Dtransformationoperations. Ifunauthorizedusersdesignanetworkthat
is invariant to all these transformations, they could potentially defeat our proposed UMT. So far,
onlynetworksinvarianttorigidtransformationslikerotationandscalinghavebeenproposed,while
networksinvarianttonon-rigidtransformationsliketwistingandshear,havenotyetbeenintroduced.
Therefore,ourresearchalsocontributestothedesignofmoretransformation-invariantnetworks.
Our research calls for the design of more robust point cloud networks, which helps improve the
robustnessandsecurityof3Dpointcloudprocessingsystems. Ontheotherhand,ifourproposed
UMTschemeismaliciouslyexploited, itmayhavenegativeimpactsonsociety, suchascausing
a sharp decline in the performance of models trained on it, affecting the security and reliability
oftechnologiesbasedon3Dpointcloudnetworks. Moretransformation-invariant3Dpointcloud
networksneedtobeproposedinthefuturetoavoidpotentialnegativeimpacts.
References
[1] EvangelosAlexiou,NanyangYang,andTouradjEbrahimi. Pointxr: Atoolboxforvisualiza-
tion and subjective evaluation of point clouds in virtual reality. In Proceedings of the 12th
InternationalConferenceonQualityofMultimediaExperience(QoMEXâ€™20),pages1â€“6,2020.
1
[2] IroArmeni, OzanSener, AmirRZamir, HelenJiang, IoannisBrilakis, MartinFischer, and
SilvioSavarese. 3dsemanticparsingoflarge-scaleindoorspaces. InProceedingsofthe2016
IEEEConferenceonComputerVisionandPatternRecognition(CVPRâ€™16),pages1534â€“1543,
2016. 6,18
[3] AngelX.Chang,ThomasFunkhouser,LeonidasGuibas,PatHanrahan,QixingHuang,Zimo
Li,SilvioSavarese,ManolisSavva,ShuranSong,HaoSu,etal. Shapenet: Aninformation-rich
3dmodelrepository. arXivpreprintarXiv:1512.03012,2015. 6,18,20
[4] SizheChen,GengYuan,XinwenCheng,YifanGong,MinghaiQin,YanzhiWang,andXiaolin
Huang.Self-ensembleprotection:Trainingcheckpointsaregooddataprotectors.InProceedings
ofthe11thInternationalConferenceonLearningRepresentations(ICLRâ€™23),2023. 2
[5] XiaozhiChen,HuiminMa,JiWan,BoLi,andTianXia.Multi-view3dobjectdetectionnetwork
forautonomousdriving. InProceedingsofthe2017IEEEConferenceonComputerVisionand
PatternRecognition(CVPRâ€™17),pages1907â€“1915,2017. 1
[6] Wenda Chu, Linyi Li, and Bo Li. Tpc: Transformation-specific smoothing for point cloud
models. InProceedingsofthe39thInternationalConferenceonMachineLearning(ICMLâ€™22),
pages4035â€“4056,2022. 2,3,15,16
[7] Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojciech Czaja, and Tom
Goldstein. Adversarial examples make strong poisons. In Proceedings of the 35th Neural
InformationProcessingSystems(NeurIPSâ€™21),pages30339â€“30351,2021. 3
[8] Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, and Dacheng Tao. Robust unlearnable
examples: Protectingdataagainstadversariallearning. InProceedingsofthe10thInternational
ConferenceonLearningRepresentations(ICLRâ€™22),2022. 1,2,4,9
[9] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE (3)-transformers: 3D
roto-translationequivariantattentionnetworks. InProceedingsofthe34thNeuralInformation
ProcessingSystems(NeurIPSâ€™20),volume33,pages1970â€“1981,2020. 7
[10] CongGao,GengWang,WeisongShi,ZhongminWang,andYanpingChen.Autonomousdriving
security: Stateoftheartandchallenges. IEEEInternetofThingsJournal,9(10):7572â€“7595,
2021. 1
[11] KuofengGao,JiawangBai,BaoyuanWu,MengxiYa,andShu-TaoXia. Imperceptibleand
robustbackdoorattackin3dpointcloud. IEEETransactionsonInformationForensicsand
Security(TIFSâ€™23),19:1267â€“1282,2023. 3
[12] RobertGeirhos,JÃ¶rn-HenrikJacobsen,ClaudioMichaelis,RichardZemel,WielandBrendel,
MatthiasBethge,andFelixAWichmann. Shortcutlearningindeepneuralnetworks. Nature
MachineIntelligence,pages665â€“673,2020. 3
[13] AnkitGoyal,HeiLaw,BoweiLiu,AlejandroNewell,andJiaDeng.Revisitingpointcloudshape
classificationwithasimpleandeffectivebaseline. InProceedingsofthe38thInternational
ConferenceonMachineLearning(ICMLâ€™21),pages3809â€“3820.PMLR,2021. 6
[14] Meng-HaoGuo,Jun-XiongCai,Zheng-NingLiu,Tai-JiangMu,RalphRMartin,andShi-Min
Hu. Pct: Pointcloudtransformer. ComputationalVisualMedia,7:187â€“199,2021. 6,17
[15] ShengshanHu,WeiLiu,MinghuiLi,YechaoZhang,XiaogengLiu,XianlongWang,LeoYu
Zhang,andJunhuiHou.Pointcrt:Detectingbackdoorin3dpointcloudviacorruptionrobustness.
10InProceedingsofthe31stACMInternationalConferenceonMultimedia(MMâ€™23),pages666â€“
675,2023. 2,17
[16] ShengshanHu,JunweiZhang,WeiLiu,JunhuiHou,MinghuiLi,LeoYuZhang,HaiJin,and
LichaoSun. Pointca: Evaluatingtherobustnessof3dpointcloudcompletionmodelsagainst
adversarialexamples. InProceedingsofthe37thAAAIConferenceonArtificialIntelligence
(AAAIâ€™23),pages872â€“880,2023. 7
[17] HanxunHuang,XingjunMa,SarahMonazamErfani,JamesBailey,andYisenWang. Unlearn-
ableexamples: Makingpersonaldataunexploitable. InProceedingsofthe9thInternational
ConferenceonLearningRepresentations(ICLRâ€™21),2021. 1,2,3,4,7,9
[18] AdelJavanmardandMahdiSoltanolkotabi. Precisestatisticalanalysisofclassificationaccura-
ciesforadversarialtraining. TheAnnalsofStatistics,50(4):2127â€“2156,2022. 2,3
[19] XinJin,ZhaoxingWu,ChenggenSong,ChunweiZhang,andXiaodongLi. 3dpointcloud
encryptionthroughchaoticmapping. InProceedingsofthe17thPacificRimConferenceon
Multimedia(PCMâ€™16),pages119â€“129,2016. 9
[20] DiederikPKingmaandJimmyBa. Adam:Amethodforstochasticoptimization. arXivpreprint
arXiv:1412.6980,2014. 6,17
[21] PeiliangLi,SiqiLiu,andShaojieShen. Multi-sensor3dobjectboxrefinementforautonomous
driving. arXivpreprintarXiv:1909.04942,2019. 1
[22] XinkeLi,ZhiruiChen,YueZhao,ZekunTong,YabangZhao,AndrewLim,andJoeyTianyi
Zhou. Pointba: Towards backdoor attacks in 3d point cloud. In Proceedings of the 18th
IEEE/CVFInternationalConferenceonComputerVision(ICCVâ€™21),pages16492â€“16501,2021.
7
[23] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn:
Convolutiononx-transformedpoints.InProceedingsofthe32ndNeuralInformationProcessing
Systems(NeurIPSâ€™18),pages828â€“838,2018. 6
[24] Zhi-HaoLin,Sheng-YuHuang,andYu-ChiangFrankWang.Convolutioninthecloud:Learning
deformablekernelsin3dgraphconvolutionnetworksforpointcloudanalysis. InProceedings
ofthe2020IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPRâ€™20),
pages1800â€“1809,2020. 6,7,8
[25] BochengLiu,YongxiangLiu,YiyuanXie,XiaoJiang,YichenYe,TingtingSong,Junxiong
Chai, MengLiu, ManyingFeng, andHaodongYuan. Privacyprotectionfor3dpointcloud
classificationbasedonanopticalchaoticencryptionscheme. OpticsExpress,31(5):8820â€“8843,
2023. 9
[26] ShuangLiu,YihanWang,andXiao-ShanGao. Game-theoreticunlearnableexamplegenerator.
InProceedingsoftheAAAIConferenceonArtificialIntelligence(AAAIâ€™24),2024. 1,2,3,7
[27] YixinLiu,KaidiXu,XunChen,andLichaoSun. Stableunlearnableexample: Enhancingthe
robustnessofunlearnableexamplesviastableerror-minimizingnoise. InProceedingsofthe
AAAIConferenceonArtificialIntelligence(AAAIâ€™24),pages3783â€“3791,2024. 1,2,9
[28] IlyaLoshchilovandFrankHutter. Sgdr: Stochasticgradientdescentwithwarmrestarts. arXiv
preprintarXiv:1608.03983,2016. 6,17
[29] XuMa,CanQin,HaoxuanYou,HaoxiRan,andYunFu. Rethinkingnetworkdesignandlocal
geometryinpointcloud: Asimpleresidualmlpframework. arXivpreprintarXiv:2202.07123,
2022. 6
[30] MoritzMenzeandAndreasGeiger. Objectsceneflowforautonomousvehicles. InProceedings
ofthe2015IEEEConferenceonComputerVisionandPatternRecognition(CVPRâ€™15),pages
3061â€“3070,2015. 1,6
[31] YifeiMin,LinChen,andAminKarbasi. Thecuriouscaseofadversariallyrobustmodels: More
datacanhelp,doubledescend,orhurtgeneralization. InUncertaintyinArtificialIntelligence,
pages129â€“139.PMLR,2021. 2,3
[32] ArpitNama,AmayaDharmasiri,KanchanaThilakarathna,AlbertZomaya,andJaybieAgullo
de Guzman. User configurable 3d object regeneration for spatial privacy. arXiv preprint
arXiv:2108.08273,2021. 9
[33] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep
learningonpointsetsfor3dclassificationandsegmentation. InProceedingsofthe2017IEEE
ConferenceonComputerVisionandPatternRecognition(CVPRâ€™17),pages652â€“660,2017. 4,
6
[34] CharlesRuizhongtaiQi,LiYi,HaoSu,andLeonidasJ.Guibas. Pointnet++: Deephierarchical
featurelearningonpointsetsinametricspace. InProceedingsofthe31stNeuralInformation
ProcessingSystems(NeurIPSâ€™17),pages5099â€“5108,2017. 6,8
[35] Jie Ren, Han Xu, Yuxuan Wan, Xingjun Ma, Lichao Sun, and Jiliang Tang. Transferable
unlearnable examples. In Proceedings of the 11th International Conference on Learning
11Representations(ICLRâ€™23),2023. 9
[36] DouglasAReynoldsetal.Gaussianmixturemodels.Encyclopediaofbiometrics,741(659-663),
2009. 2,3
[37] VinuSankarSadasivan,MahdiSoltanolkotabi,andSoheilFeizi. CUDA:Convolution-based
unlearnabledatasets. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPRâ€™23),pages3862â€“3871,2023. 1,2,3,4,7,9
[38] StubbornAtom. Distributionsofquadraticformofanormalrandomvariable. CrossValidated,
2020. (version: 2020-07-23). 23
[39] Mikaela AngelinaUy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit
Yeung. Revisiting point cloud classification: A new benchmark dataset and classification
modelonreal-worlddata. InProceedingsofthe17thIEEE/CVFInternationalConferenceon
ComputerVision(ICCVâ€™19),2019. 6,18,20
[40] XianlongWang,ShengshanHu,MinghuiLi,ZhifeiYu,ZiqiZhou,andLeoYuZhang. Cor-
ruptingconvolution-basedunlearnabledatasetswithpixel-basedimagetransformations. arXiv
preprintarXiv:2311.18403,2023. 1,2,3,9
[41] XianlongWang,ShengshanHu,YechaoZhang,ZiqiZhou,LeoYuZhang,PengXu,WeiWan,
and Hai Jin. ECLIPSE: Expunging clean-label indiscriminate poisons via sparse diffusion
purification.InProceedingsofthe29thEuropeanSymposiumonResearchinComputerSecurity
(ESORICSâ€™24),pages146â€“166,2024. 2
[42] XianlongWang,MinghuiLi,PengXu,WeiLiu,LeoYuZhang,ShengshanHu,andYanjun
Zhang. PointAPA:Towardsavailabilitypoisoningattacksin3Dpointclouds. InProceedings
of the 29th European Symposium on Research in Computer Security (ESORICSâ€™24), pages
125â€“145,2024. 2,3,4,9
[43] YueWang,YongbinSun,ZiweiLiu,SanjayE.Sarma,MichaelM.Bronstein,andJustinM.
Solomon. Dynamicgraphcnnforlearningonpointclouds. ACMTransactionsOnGraphics
(TOGâ€™19),pages1â€“12,2019. 6
[44] FlorianWirth,JannikQuehl,JeffreyOta,andChristophStiller. Pointatme: efficient3dpoint
cloudlabelinginvirtualreality.InProceedingsofthe2019IEEEIntelligentVehiclesSymposium
(IVâ€™19),pages1693â€“1698,2019. 1
[45] ShutongWu,SizheChen,CihangXie,andXiaolinHuang. One-pixelshortcut: onthelearning
preferenceofdeepneuralnetworks. InProceedingsofthe11thInternationalConferenceon
LearningRepresentations(ICLRâ€™23),2023. 9
[46] WenxuanWu,ZhongangQi,andLiFuxin. Pointconv:Deepconvolutionalnetworkson3dpoint
clouds. InProceedingsofthe2019IEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPRâ€™19),pages9621â€“9630,2019. 6
[47] XiaoyangWu,LiJiang,Peng-ShuaiWang,ZhijianLiu,XihuiLiu,YuQiao,WanliOuyang,
TongHe,andHengshuangZhao. Pointtransformerv3: Simpler,faster,stronger. arXivpreprint
arXiv:2312.10035,2023. 6,8
[48] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and
JianxiongXiao. 3dshapenets: Adeeprepresentationforvolumetricshapes. InProceedings
ofthe2015IEEEConferenceonComputerVisionandPatternRecognition(CVPRâ€™15),pages
1912â€“1920,2015. 4,6,18,20,21
[49] TiangeXiang,ChaoyiZhang,YangSong,JianhuiYu,andWeidongCai. Walkinthecloud:
Learningcurvesforpointcloudsshapeanalysis. InProceedingsofthe18thIEEE/CVFInterna-
tionalConferenceonComputerVision(ICCVâ€™21),pages915â€“924,2021. 6
[50] ZhenXiang,DavidJ.Miller,SihengChen,XiLi,andGeorgeKesidis.Abackdoorattackagainst
3dpointcloudclassifiers. InProceedingsofthe18thIEEE/CVFInternationalConferenceon
ComputerVision(ICCVâ€™21),pages7597â€“7607,2021. 17
[51] JianchengYang,QiangZhang,RongyaoFang,BingbingNi,JinxianLiu,andQiTian. Adver-
sarialattackanddefenseonpointsets. arXivpreprintarXiv:1902.10899,2019. 7
[52] JingwenYeandXinchaoWang. Ungeneralizableexamples. InProceedingsofthe2024IEEE
ConferenceonComputerVisionandPatternRecognition(CVPRâ€™24),2024. 2
[53] JiamingZhang,XingjunMa,QiYi,JitaoSang,YugangJiang,YaoweiWang,andChangsheng
Xu. Unlearnableclusters: Towardslabel-agnosticunlearnableexamples. InProceedingsofthe
2023IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPRâ€™23),2023. 9
[54] RenruiZhang,LiuhuiWang,YaliWang,PengGao,HongshengLi,andJianboShi. Parameter
isnotallyouneed: Startingfromnon-parametricnetworksfor3dpointcloudanalysis. arXiv
preprintarXiv:2303.08134,2023. 6
[55] Zhiyuan Zhang, Binh-Son Hua, David W Rosen, and Sai-Kit Yeung. Rotation invariant
convolutions for 3d point clouds deep learning. In Proceedings of the 2019 International
Conferenceon3DVision(3DVâ€™19),pages204â€“213.IEEE,2019. 6,8,9
12[56] Zhiyuan Zhang, Binh-Son Hua, and Sai-Kit Yeung. Riconv++: Effective rotation invariant
convolutions for 3d point clouds deep learning. International Journal of Computer Vision
(IJCVâ€™22),130(5):1228â€“1243,2022. 6,7,9
[57] ChenZhao, JiaqiYang, XinXiong, AngfanZhu, ZhiguoCao, andXinLi. Rotationinvari-
antpointcloudclassification: Wherelocalgeometrymeetsglobaltopology. arXivpreprint
arXiv:1911.00195,2019. 6,8,9
[58] HangZhou,KejiangChen,WeimingZhang,HanFang,WenboZhou,andNenghaiYu. Dup-net:
Denoiserandupsamplernetworkfor3Dadversarialpointcloudsdefense. InProceedingsofthe
17thIEEE/CVFInternationalConferenceonComputerVision(ICCVâ€™19),pages1961â€“1970,
2019. 7
[59] ZiqiZhou,YufeiSong,MinghuiLi,ShengshanHu,XianlongWang,LeoYuZhang,Dezhong
Yao,andHaiJin.Darksam:Foolingsegmentanythingmodeltosegmentnothing.arXivpreprint
arXiv:2409.17874,2024. 7
[60] XiangyangZhu,RenruiZhang,BoweiHe,ZiyuGuo,JiamingLiu,HanXiao,ChaoyouFu,Hao
Dong,andPengGao. Notimetotrain: Empoweringnon-parametricnetworksforfew-shot3d
scenesegmentation. InProceedingsofthe2024IEEEConferenceonComputerVisionand
PatternRecognition(CVPRâ€™24),2024. 6,8
[61] YifanZhu,YiboMiao,YinpengDong,andXiao-ShanGao. Towardavailabilityattacksin3D
pointclouds. arXivpreprintarXiv:2407.11011,2024. 9
13Appendix: Unlearnable3DPointClouds: Class-wiseTransformationIsAllYou
Need
A Definitionsof3DTransformations
Existing3DtransformationsaresummarizedinFig.1andformallydefinedinthissection. The3D
transformationsaremathematicaloperationsappliedtothree-dimensionalobjectstochangetheir
position,orientation,andscaleinspace,whichareoftenrepresentedusingtransformationmatrices.
WeformallydefinethetransformedpointcloudsamplewithtransformationmatrixTâˆˆR3Ã—3as:
X =TÂ·X (10)
t
whereXâˆˆR3Ã—pisthecleanpointcloudsample,X âˆˆR3Ã—pisthetransformedpointcloudsample.
t
A.1 RotationTransformation
Therotationtransformationthatalterstheorientationandangleof3Dpointcloudsiscontrolledby
threeanglesÎ±,Î²,andÎ³. Therotationmatricesinthreedirectionscanbeformallydefinedas:
(cid:34) 1 0 0 (cid:35) (cid:34) cosÎ² 0 sinÎ² (cid:35) (cid:34) cosÎ³ âˆ’sinÎ³ 0 (cid:35)
R = 0 cosÎ± âˆ’sinÎ± ,R = 0 1 0 ,R = sinÎ³ cosÎ³ 0
Î± Î² Î³
0 sinÎ± cosÎ± âˆ’sinÎ² 0 cosÎ² 0 0 1
(11)
ThuswehaveT=R R R whileemployingrotationtransformation. Besides,wehaveRâˆ’1 =
Î± Î² Î³ Î±
RT,Râˆ’1 =RT,andRâˆ’1 =RT.
Î± Î² Î² Î³ Î³
A.2 ScalingTransformation
ThescalingmatrixS canberepresentedas:
(cid:34) Î» 0 0 (cid:35) (cid:34) 1 0 0 (cid:35)
S = 0 Î» 0 =Î» 0 1 0 (12)
0 0 Î» 0 0 1
wherethescalingfactorÎ»isusedtoperformaproportionalscalingofthecoordinatesofeachpoint
inthepointcloud.
A.3 ShearTransformation
Inthethree-dimensionalspace,shearing3isrepresentedbydifferentmatrices,andthespecificform
dependsonthetypeofshearbeingperformed. Specifically,thesheartransformationmatrixH
xy
(employed in UMT) of shifting x and y by the other coordinate z, and its corresponding inverse
matrixcanbeexpressedas:
(cid:34)1 0 s(cid:35) (cid:34)1 0 âˆ’s(cid:35)
H = 0 1 t ,Hâˆ’1 = 0 1 âˆ’t (13)
xy xy
0 0 1 0 0 1
ThesheartransformationmatrixH ofshiftingxandz bytheothercoordinatey,anditscorre-
xz
spondinginversematrixcanbeexpressedas:
(cid:34)1 s 0(cid:35) (cid:34)1 âˆ’s 0(cid:35)
H = 0 1 0 ,Hâˆ’1 = 0 1 0 (14)
xz xz
0 t 1 0 âˆ’t 1
And the shear transformation matrix H of shifting y and z by the other coordinate x, and its
yz
correspondinginversematrixcanbeexpressedas:
(cid:34)1 0 0(cid:35) (cid:34) 1 0 0(cid:35)
H = s 1 0 ,Hâˆ’1 = âˆ’s 1 0 (15)
yz yz
t 0 1 âˆ’t 0 1
3https://www.mauriciopoppe.com/notes/computer-graphics/
transformation-matrices/shearing/
14Algorithm1OurproposedUMTscheme
Input: Clean3DpointclouddatasetD ={(X ,Y )}n ;numberofcategoriesN;slightranger ;
c i i i=1 s
primaryranger ;scalinglowerboundb ;scalingupperboundb ;twistinglowerboundw ;twisting
p l u l
upperboundw ;shearlowerboundh ;shearupperboundh ;spectrumoftransformationsk;matrix
u l u
setT ={R,S,H,W}.
s
Output: Unlearnable3DpointclouddatasetD ={(X ,Y )}n .
u ui i i=1
InitializetheslightanglelistsL =[],L =[],theprimaryanglelistL =[],therotationlistL =[],
Î± Î² Î³ (cid:108)âˆš (cid:109) R
thescalinglistL =[],thetwistinglistL =[],theshearlistL =[],andA = 3N ;
S W H N
forc=1tokdo
RandomlysampleatransformationmatrixV âˆˆT ;
c s
RemovetransformationmatrixV fromT ;
c s
ifV ==Rthen
c
fori=1toA do
N
forj =1toA do
N
fork =1toA do
N
L â†L âˆª{[L [i],L [j],L [k]]};
R R Î± Î² Î³
end
end
end
GettheL â†random.sample(L ,N);
R R
end
elseifV ==S then
c
fori=1toN do
RandomlysampleÎ» âˆ¼U(b ,b );
i l u
AddtothelistL â†L âˆª{Î» };
S S i
end
end
elseifV ==W then
c
fori=1toN do
RandomlysampleÏ‰ âˆ¼U(w ,w );
i l u
AddtothelistL â†L âˆª{Ï‰ };
W W i
end
end
elseifV ==Hthen
c
fori=1toN do
Randomlysampleh âˆ¼U(h ,h );
i l u
AddtothelistL â†L âˆª{h };
H H i
end
end
end
fori=1tondo
GetthetransformationmatrixT
=(cid:81)k
V bytheparameterlistsabove;
k i=1 i
GetthetransformeddataX =T Â·X ;
ui ki i
end
Return: Unlearnable3DpointclouddatasetD .
u
A.4 TwistingTransformation
The3Dtwistingtransformation[6]involvesarotationaldeformationappliedtoanobjectinthree-
dimensionalspace,creatingatwistedorspiraledeffect. Unlikesimplerotationsaroundfixedaxes,a
twistingtransformationintroducesavariablerotationthatmaychangebasedonthespatialcoordinates
oftheobject. Forinstance,consideringatwistingtransformationalongthez-axis,wheretherotation
angleisafunctionrelatedtothez-coordinate,itcanbeexpressedas:
(cid:34)cos(Î¸z) âˆ’sin(Î¸z) 0(cid:35)
W (Î¸,z)= sin(Î¸z) cos(Î¸z) 0 (16)
z
0 0 1
15whereÎ¸istheparameterofthetwistingtransformation,andzisthez-coordinateoftheobject. The
inversematrixofW (Î¸,z)is:
z
(cid:34) cos(Î¸z) sin(Î¸z) 0(cid:35)
Wâˆ’1(Î¸,z)= âˆ’sin(Î¸z) cos(Î¸z) 0 (17)
z
0 0 1
A.5 TaperingTransformation
Thetaperingtransformation[6]isalineartransformationusedtoaltertheshapeofanobject,causing
ittograduallybecomepointedorshortened. Inthree-dimensionalspace,taperingtransformationcan
adjustthedimensionsofanobjectalongoneormoreaxes,creatingataperingeffect. Thespecific
matrixrepresentationoftaperingtransformationdependsonthechosenaxisandthedesignofthe
transformation. Generally,taperingtransformationcanberepresentedbyamatrixthatismultiplied
bythecoordinatesoftheobjecttoachievetheshapeadjustment,whichisdefinedas:
(cid:34)1+Î·z 0 0(cid:35)
A (Î·,z)= 0 1+Î·z 0 (18)
z
0 0 1
wherezisthez-coordinateoftheobject. ConsideringthatÎ·zcouldindeedequal-1,insuchacase,
the3Dpointcloudsampleswouldbeprojectedontothez-plane,losingtheirpracticalsignificance
andthetaperingmatrixisalsoirreversible.
A.6 ReflectionTransformation
Reflectiontransformationisalineartransformationthatinvertsanobjectalongacertainplane. This
planeiscommonlyreferredtoasareflectionplaneormirror. Forreflectiontransformationsinthree-
dimensionalspace,wecanrepresentthemthroughamatrix. Regardingthereflectiontransformation
matricesofthexyplane,yzplane,andxzplane,wehave:
(cid:34)1 0 0 (cid:35) (cid:34)âˆ’1 0 0(cid:35) (cid:34)1 0 0(cid:35)
R = 0 1 0 ,R = 0 1 0 ,R = 0 âˆ’1 0 (19)
xy yz xz
0 0 âˆ’1 0 0 1 0 0 1
A.7 TranslationTransformation
The3Dtranslationtransformation4referstotheprocessofmovinganobjectinthree-dimensional
space. This transformation involves moving the object along the x, y, and z axes, respectively,
smoothlytransitioningitfromonepositiontoanother,whichisdefinedas:
ï£®1 0 0 t ï£¹
x
0 1 0 t
L=ï£¯ yï£º (20)
ï£°0 0 1 t ï£»
z
0 0 0 1
wheret , t , andt representsthetranslationalongthex, y, andz axes, respectively. This4Ã—4
x y z
matrixisahomogeneouscoordinatematrixthatdescribesthetranslationinthree-dimensionalspace.
Additionally, thetranslationmatrixalsocanberepresentedasaadditivematrixtooriginalpoint
x âˆˆR3Ã—3,whichcanbedefinedas:
i
(cid:34)t t t (cid:35)
x x x
L= t t t (21)
y y y
t t t
z z z
B SupplementaryExperimentalSettings
B.1 ExperimentalPlatform
Ourexperimentsareconductedonaserverrunninga64-bitUbuntu20.04.1systemwithanIntel(R)
Xeon(R)Silver4210RCPU@2.40GHzprocessor,125GBmemory,andfourNvidiaGeForceRTX
3090GPUs,eachwith24GBmemory. TheexperimentsareperformedusingthePythonlanguage,
version3.8.19,andPyTorchlibraryversion1.12.1.
4https://www.javatpoint.com/computer-graphics-3d-transformations
16Transformations Rotation Scaling Shear Twisting Tapering Translation AVG
w/o 89.32 89.32 89.32 89.32 89.32 89.32 89.32
Sample-wise 91.52 87.78 89.10 92.62 90.31 90.31 89.80
Dataset-wise 87.89 79.41 85.79 92.18 88.22 88.22 83.45
Class-wise 29.85 20.81 67.84 63.22 52.42 36.67 45.14
Table5: Thetestaccuracy(%)resultsondiversetypesoftransformationsonModelNet10trainingsetusing
PointNetclassifier.
Testsetsâ†“Transformationsâˆ’â†’ Rotation Scaling Shear Twisting Tapering Translation
Class-wisetestset 99.67 93.80 97.91 94.05 96.04 98.24
Permutedclass-wisetestset 10.68 38.22 58.37 60.68 42.51 31.94
Cleantestset 29.85 20.81 67.84 63.22 52.42 36.67
Table6:AccuracyresultsobtainedwithdifferenttestsetswhentrainingthePointNetclassifierusingclass-wise
transformedtrainingsets.
B.2 Hyper-ParameterSettings
Themodeltrainingprocessontheunlearnabledatasetandthecleandatasetremainsconsistent,using
theAdamoptimizer[20],CosineAnnealingLRscheduler[28],initiallearningrateof0.001,weight
decayof0.0001,batchsizeof16(duetoinsufficientGPUmemory,thebatchsizeissetto8when
training3DGCNonModelNet40dataset),andtrainingfor80epochs. Duetothelongertraining
processrequiredbyPCT[14],thetrainingepochsforPCTinTab.1onModelNet10,ModelNet40,
andScanObjectNNdatasetsareallsetto240.
B.3 SettingsofExploringExperiments
Weinitiallyinvestigatewhichapproachyieldsthebestunlearnableeffectamongsample-wise,dataset-
wise,andclass-wisesettingsinFig.2(a). Specificexperimentalsettingsareasfollows:
Inthedataset-wisesetting,thesameparametervaluesareappliedtotheentiredataset. Specifically,
wehaveÎ± = Î² = Î³ = 10â—¦ intherotationtransformation, thescalingfactorÎ»issetto0.8, both
shearingfactorssandtaresetto0.2. TheangleÎ¸intwistingis25â—¦. ThetaperingangleÎ·issetto
25â—¦,andtheparametersintranslationtransformationt ,t ,andt aresetto0.15.
x y z
Inthesample-wisesetting,eachsamplehasitsindependentsetofparameters,meaningtheparameter
valuesforeachsamplearerandomlygeneratedwithinacertainrange. Intherotationtransformation,
Î±,Î²,Î³areuniformlysampledfromtherangeof0â—¦to20â—¦. ThescalingfactorÎ»isuniformlysampled
from0.6to0.8, shearingfactorssandtareuniformlysampledfromtherangeof0to0.4. Both
thetwistangleÎ¸andtaperingangleÎ·aresampledfrom0â—¦to50â—¦,andtheparametersintranslation
transformationt ,t ,andt aresampledfrom0to0.3.
x y z
Intheclass-wisesetting,theparametersfortransformationsareassociatedwiththepointcloudâ€™s
class. Theselectionofparametersforeachclassisalsoobtainedbyrandomsamplingwithinafixed
range. Thechosenrangesaregenerallyconsistentwiththosedescribedforthesample-wisesetting
above. However,adifferenceliesintheconsiderationofslightanglerangeandprimaryanglerange
inthecaseofrotationtransformations,wheretheserangesare20â—¦and120â—¦,respectively.
Thespecificresultsoftestaccuracyunderdifferenttransformationmodes,includingsample-wise
(random),class-wise,anddataset-wise(universal),areprovidedinTab.5. Itcanbeseenthatunder
theclass-wisesetting,thefinalunlearnableeffectisthebest.
B.4 BenchmarkDatasets
Datasetintroduction. TheModelNet40datasetisapointclouddatasetcontaining40categories,
comprising9843trainingand2468testpointclouddata. ModelNet10isasubsetofModelNet40
dataset with 10 categories. ShapeNetPart that includes 16 categories is a subset of ShapeNet,
comprising12137trainingand2874testpointcloudsamples. ScanObjectNNisareal-worldpoint
clouddatasetwith15categories,comprising2309trainingsamplesand581testsamples. Similar
to[50,15],wesplitKITTIobjectcloudsintoclassâ€œvehicleâ€andâ€œhumanâ€containing1000training
17dataand662testdata. Theinputpointcloudobjectsforthemodelsencompass256pointsforthe
KITTIdatasetand1024pointsforotherdatasets. TheStanford3DIndoorSceneDataset(S3DIS)
datasetcontains6large-scaleindoorareaswith271rooms. Eachpointinthescenepointcloudis
annotatedwithoneofthe13semanticcategories.
Datasetlicenses. Thedatasetlicenseinformationislistedas:
â€¢ ModelNet10,ModelNet40[48]: AllCADmodelsaredownloadedfromtheInternetand
the original authors hold the copyright of the CAD models. The label of the data was
obtainedbyusviaAmazonMechanicalTurkserviceanditisprovidedfreely. Thisdataset
isprovidedfortheconvenienceofacademicresearchonly. Linkishttps://modelnet.
cs.princeton.edu/.
â€¢ ShapeNetPart[3]: WeusetheShapeNetdatabase(the"Database")atPrincetonUniversity
andStanfordUniversity. Linkishttps://www.shapenet.org/.
â€¢ ScanObjectNN[39]: ThelicenseisMITLicense: Copyright(c)2019Vision&Graphics
Group,HKUST.Permissionisherebygranted,freeofcharge,toanypersonobtaininga
copyofthissoftwareandassociateddocumentationfiles(the"Software"),todealinthe
Softwarewithoutrestriction,includingwithoutlimitationtherightstouse,copy,modify,
merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit
personstowhomtheSoftwareisfurnishedtodoso,subjecttothefollowingconditions:
The above copyright notice and this permission notice shall be included in all copies
orsubstantialportionsoftheSoftware. Linkishttps://hkust-vgd.github.io/
scanobjectnn/.
â€¢ S3DIS [2]: The copyright is from Stanford University, Patent Pending, copyright 2016.
Linkishttp://buildingparser.stanford.edu/dataset.html.
B.5 SettingsofRobustnessExperiments
Thescalingfactorinrandomscalingaugmentationissettoaminimumof0.8andamaximumof1.25.
Intherandomrotationoperation,thethreedirectionalrotationanglesareidenticalanduniformly
sampledfrom[0,2Ï€). Theperturbationsintherandomjitteraresampledfromanormaldistribution
withastandarddeviationof0.05, andtheperturbationmagnitudeisconstrainedwithin0.1. The
parameterskandÎ±inSORaresetto2and1.1,respectively. ThenumberofdroppedpointsinSRS
is500.
C SupplementaryExperimentalResults
C.1 ResultsforExecutionMechanism
WepresentthespecificexperimentalresultsofFig.2(a)inTab.5. Itcanbeobservedthatunderthe
class-wisesetting,theaveragetestaccuracyisthelowest,whiletheunlearnableconditionyieldsthe
bestperformance. Furthermore,weconductinvestigationsintotrainingthePointNetclassifierwith
class-wisetransformedtrainingsets(ModelNet10dataset),analyzingaccuracyresultsacrossdifferent
testsets,asshowninTab.6. Itisnoteworthythattheaccuracyonclass-wisetestsets(usingconsistent
transformation parameters with class-wise transformed training set) is consistently above 90%,
indicatingthatthemodelhaslearnedthemappingbetweentransformationsandlabels. Thisleadsto
correctclassificationontestsampleswiththesametransformations. However,whentheclass-wise
testsetundergoespermutation,theaccuracydropstoalevelsimilartothatofthecleantestset. This
clearlydemonstratesthatthemodelcancorrectlyclassifysamplesonlywhentheyhavecorresponding
transformations,andsampleswithouttransformationsorwithmismatchedtransformationscannotbe
correctlyclassified. Thisfurthervalidatesthatthemodellearnsaone-to-onemappingbetweenclass
transformationsandlabels.
C.2 ResultsofDiverseCombinationsofTransformations
Weinvestigatecombinationsoffourtransformations,i.e.,rotation,scaling,shear,andtwisting,and
createunlearnabledatasetsusingtheclass-wisesetting. Thetestaccuracyresultsacrossfivepoint
cloudmodelsarepresentedinTab.7. Itcanbeobservedthatthecombinationofrotationandscaling,
18Transformations PointNet PointNet++ DGCNN PointCNN PCT AVG
R 46.70 38.99 49.67 64.87 27.53 45.55
S 34.80 57.05 42.62 33.81 29.63 39.58
H 64.10 66.41 71.15 60.68 60.13 64.49
W 76.98 54.19 78.63 86.69 38.55 67.05
RS 22.25 28.08 14.10 21.48 11.89 19.56
RH 30.51 39.98 45.93 36.23 33.81 37.29
RW 34.91 44.27 29.52 46.92 32.16 37.56
SH 20.26 44.93 38.66 43.50 34.80 36.43
SW 18.28 44.27 31.39 34.25 19.93 29.62
HW 63.33 62.56 61.89 64.76 51.43 60.79
RSH 15.97 29.19 36.56 21.81 31.28 26.96
RSW 25.22 31.50 23.57 31.83 38.55 30.13
SHW 21.70 46.37 55.29 37.11 38.77 39.85
RSHW 16.52 33.37 30.51 30.73 32.49 28.72
Table7:Thetestaccuracy(%)resultsobtainedfromtrainingthepointcloudclassifiersPointNet,PointNet++,
DGCNN,PointCNN,PCTusingaModelNet10datasetgeneratedwithdiversecombinationsoftransformations
underaclass-wisesetting,whereR,S,H,andWcorrespondtorotation,scaling,shear,andtwistingrespectively.
Trainingandtestsets PointNet PointNet++ DGCNN PointCNN AVG
Cleantrainingset,cleantestset 89.32 92.95 92.73 89.54 91.14
Cleantrainingset,UMTtestset 55.51 70.93 74.78 69.71 67.73
UMTtrainingset,cleantestset 22.25 28.08 14.10 21.48 21.48
UMTtrainingset,UMTtestset 98.79 99.23 99.01 96.26 98.32
Table8:Theaccuracy(%)resultsoncleanandUMTModelNet10trainingsetandtestsetusingfourpointcloud
classifiers.Higheraccuracyvaluescorrespondtolowercross-entropylossvalues.
twotypesofrigidtransformations,achievesthemosteffectiveunlearnableeffect. Thetransformation
parametersusedinthissectionareconsistentwithAppendixB.3.
C.3 MoreResultsofUMTAgainstAdaptiveAttacks
TofurtherexploretheperformanceofUMTagainstadaptiveattacks,wesupplementtheexperimental
resultsusingfourtypesofrandomaugmentationsRSHW inTab.9,andfindthattheconclusionis
consistentwithTab.2.
ModelNet10 PointNet PointNet++ DGCNN PointCNN AVG
Cleanbaseline 89.32 92.95 92.73 89.54 91.14
UMT(k=4) 16.19 36.56 17.62 27.42 24.45
UMT(k=4)+randomRSHW 25.99 61.78 61.89 44.16 48.46
Table9:Testaccuracy(%)resultsusingUMTtrainingdataandUMTdataemployingrandomaugmentations.
C.4 ResultsofInsightfulAnalysis
Wetrainoncleantrainingsetandtestoncleantestset,trainoncleantrainingsetandtestonUMT
testset,trainonUMTtrainingsetandtestoncleantestset,andtrainonUMTtrainingsetandtest
onUMTtestsettoobtainthetestaccuracyresultsinTab.8(weensurethattheUMTparameters
fortheUMTtestsetandUMTtrainingsetareconsistent). Higheraccuracyvaluesindicatelower
cross-entropylossvalues,whileloweraccuracyvaluesrepresenthighercross-entropylossvalues.
ItcanbeobservedthatonlywhentrainedwiththeUMTtrainingset,thelossaftertestingwiththe
cleantestsetishigh(i.e.,lowaccuracy).
C.5 BoarderHyper-ParameterAnalysis
Additionally, we investigate the unlearnable effects across a wider range of hype-parameters
r ,r ,b ,b in UMT (k = 2 with RS), as shown in Tab. 10. It can be observed that our UMT
s p l u
scheme still exhibits a good unlearnable effect, and its key lies in the crucial role played by the
class-wisesetting.
19rs,rp,bl,bu PointNet DGCNN PointCNN AVG rs,rp,bl,bu PointNet DGCNN PointCNN AVG
25,120,0.6,0.8 24.78 23.35 28.19 25.44 15,120,0.75,0.8 154 38.66 40.09 31.10
25,180,0.6,0.8 26.65 25.99 20.37 234 15,120,0.6,0.7 20.59 27.53 23.68 23.93
15,90,0.6,0.8 18.39 20.59 22.03 20.34 15,120,0.6,0.8 22.25 14.10 21.48 19.28
15,240,0.6,0.8 21.26 30.51 20.70 24.16 15,120,0.6,0.9 23.90 23.90 23.35 23.72
15,120,0.6,1.2 31.50 28.63 36.78 32.30 15,120,0.6,1.0 22.58 26.54 31.61 26.91
Table 10: The test accuracy (%) results on ModelNet10 dataset with a boarder range of hype-parameters
r ,r ,b,b .
s p l u
Benchmarkdatasets Modulesâ†“Modelsâˆ’â†’ PointNet PointNet++ DGCNN PointCNN PCT AVG
Rotationmodule 53.51 44.05 57.69 49.48 43.84 49.71
ShapeNetPart[3] Scalingmodule 28.74 77.21 37.93 44.02 51.84 47.95
UMT(k=2) 15.14 41.16 26.17 32.36 44.05 31.78
Table11:Ablationmodules: Thetestaccuracy(%)resultsachievedbytrainingonunlearnabledatacreatedby
differentmodulesontheShapeNetPartdataset.
C.6 SupplementaryAblationResults
Inthissection,weconductablationexperimentsontheUMTschemeontheShapeNetPartdataset
(keepingexperimentalparametersconsistentwiththemainexperiments),asshowninTab.11. It
canbeobservedthatwhetherusingonlytherotationmoduleoronlythescalingmodule,thefinal
unlearnableeffectisnotasgoodasUMT.Thisclearlydemonstratesthateachmodulecontributesto
theoverallUMTeffectiveness.
C.7 ResultsofMixtureofClass-wiseandSample-wiseSamples
Inthissection,weinvestigatethetestaccuracyresultswhenusingamixtureofdifferentratiosofclass-
wisesamplesandsample-wise(random)samplesinthedataset,asshowninTab.12(experimentsare
conductedontheModelNet10datasetwith10categories,whereâ€œ2class-wise8sample-wise"denotes
thatsamplesfrom2categoriesundergoclass-wisetransformations,whiletheremaining8categories
undergorandomtransformations,andsoon). Wecanclearlyobservefromtheexperimentalresults
thatastheproportionofclass-wisetransformationsgraduallyincreases,thetestaccuracygradually
decreases,indicatingthattheunlearnableeffectbecomesmorepronounced. Thisstronglysuggests
thattheclass-wisesettingismoreeffectivethanthesample-wisesetting.
Clean samples
UMT
samples Â° Â°
[ğŸ’ğŸ’.ğŸ•ğŸ•Â°,ğŸğŸ.ğŸ—ğŸ—,
ğŸ—ğŸ—ğŸ•ğŸ•.ğŸ‘ğŸ‘,ğŸğŸ.ğŸ•ğŸ•ğŸ•ğŸ•]
UMT
samples Â° Â°
[ğŸ•ğŸ•.ğŸğŸÂ°,ğŸ‘ğŸ‘.ğŸğŸ,
ğŸ•ğŸ•ğŸğŸ.ğŸ•ğŸ•,ğŸğŸ.ğŸ–ğŸ–]
UMT
samples Â° Â°
[ğŸğŸğŸğŸ.ğŸ•ğŸ•Â°,ğŸ–ğŸ–.ğŸ•ğŸ•,
ğŸğŸğŸ’ğŸ’.ğŸ—ğŸ—,ğŸğŸ.ğŸ•ğŸ•ğŸ•ğŸ•] Figure7:CleanandUMTsamplesfromModelNet10dataset.
C.8 AdditionalVisualPresentationsfor3DPointCloudSamples
ModelNet10
WevisualizecleanpointcloudsamplesandUMT(k = 2usingRS)pointcloudsamplesonfour
benchmarkdatasetsModelNet10[48],ModelNet40[48],ScanObjectNN[39],andShapeNetPart[3],
as depicted in Figs. 7 to 10, respectively. The UMT parameters consist of rotation and scaling
parameters[Î±,Î²,Î³,Î»]. Itcanbeobservedthattheseunlearnablesamplesexhibitsimilarfeature
informationtonormalsamples,presentinggoodvisualeffectsandmakingitdifficulttobedetected
asabnormalities.
20ModelNet10[48] PointNet PointNet++ DGCNN PointCNN AVG
100%sample-wisedata 72.69 79.52 85.79 75.66 78.42
20%class-wisedata,80%sample-wisedata 65.64 69.05 78.19 58.04 67.73
40%class-wisedata,60%sample-wisedata 58.04 59.91 60.57 58.59 59.28
60%class-wisedata,80%sample-wisedata 50.11 48.13 60.35 46.48 51.27
80%class-wisedata,20%sample-wisedata 31.83 29.19 44.09 50.55 39.02
100%class-wisedata 22.25 28.08 14.10 21.48 21.48
Table12:Mixtureresults: Thetestaccuracy(%)resultsachievedbytrainingonthemixturedataconsistingof
class-wiseUMTsamplesandsample-wiseUMTsamples.
Clean samples
UMT
samples Â° Â°
[ğŸğŸ.ğŸ—ğŸ—Â°,ğŸ”ğŸ”.ğŸğŸ,
ğŸ—ğŸ—ğŸ—ğŸ—.ğŸ‘ğŸ‘,ğŸğŸ.ğŸ•ğŸ•ğŸ•ğŸ•]
UMT
samples Â° Â°
[ğŸ—ğŸ—ğŸğŸ.ğŸ—ğŸ—Â°,ğŸ‘ğŸ‘.ğŸ•ğŸ•,
ğŸ•ğŸ•ğŸ—ğŸ—.ğŸ—ğŸ—,ğŸğŸ.ğŸ”ğŸ”ğŸ•ğŸ•]
UMT
samples Â° Â°
[ğŸ—ğŸ—ğŸ•ğŸ•.ğŸ”ğŸ”Â°,ğŸ“ğŸ“.ğŸ—ğŸ—,
ğŸ‘ğŸ‘ğŸ”ğŸ”.ğŸ—ğŸ—,ğŸğŸ.ğŸ•ğŸ•ğŸ—ğŸ—] Figure8:CleanandUMTsamplesfromModelNet40dataset.
Clean samples
ModelNet40
UMT
samples Â° Â°
[ğŸ’ğŸ’.ğŸ”ğŸ”Â°,ğŸğŸğŸğŸ.ğŸ•ğŸ•,
ğŸğŸğŸğŸ.ğŸ’ğŸ’,ğŸğŸ.ğŸ”ğŸ”ğŸğŸ]
UMT
samples Â° Â°
[ğŸ“ğŸ“.ğŸ•ğŸ•Â°,ğŸ’ğŸ’.ğŸ–ğŸ–,
ğŸ‘ğŸ‘ğŸ”ğŸ”.ğŸ’ğŸ’,ğŸğŸ.ğŸ”ğŸ”ğŸğŸ]
UMT
samples Â° Â°
[ğŸ–ğŸ–.ğŸğŸÂ°,ğŸğŸğŸğŸ.ğŸ–ğŸ–,
ğŸ’ğŸ’ğŸğŸ.ğŸ“ğŸ“,ğŸğŸ.ğŸ•ğŸ•ğŸ’ğŸ’] Figure9:CleanandUMTsamplesfromScanObjectNNdataset.
Clean samples Scan
UMT
samples Â° Â°
[ğŸ“ğŸ“.ğŸ•ğŸ•Â°,ğŸ“ğŸ“.ğŸğŸ,
ğŸğŸğŸğŸ.ğŸ—ğŸ—,ğŸğŸ.ğŸ”ğŸ”ğŸ”ğŸ”]
UMT
samples Â° Â°
[ğŸ”ğŸ”.ğŸ”ğŸ”Â°,ğŸğŸğŸğŸ.ğŸ•ğŸ•,
ğŸ•ğŸ•ğŸ”ğŸ”.ğŸğŸ,ğŸğŸ.ğŸ”ğŸ”ğŸ“ğŸ“]
UMT
samples Â° Â°
[ğŸ•ğŸ•.ğŸğŸÂ°,ğŸğŸğŸğŸ.ğŸğŸ,
ğŸ—ğŸ—ğŸ•ğŸ•.ğŸ“ğŸ“,ğŸğŸ.ğŸ–ğŸ–ğŸğŸ] Figure10:CleanandUMTsamplesfromShapeNetPartdataset.
Shape
D ProofsforTheories
D.1 ProofforLemma3
Lemma3. TheunlearnabledatasetD generatedusingUMTonD canalsoberepresentedusinga
u c
GMM,i.e.,D âˆ¼N(yT Âµ,Î»2I).
u y y
21Proof: Assumingy =1,thenD âˆ¼N(Âµ,I),andwehave
c1
E T x=T E x=T Âµ,
(x,y)âˆ¼Dc1 1 1 (x,y)âˆ¼Dc1 1
E (T xâˆ’T Âµ)(T xâˆ’T Âµ)âŠ¤
(x,y)âˆ¼Dc1 1 1 1 1
=T E (xâˆ’Âµ)(xâˆ’Âµ)âŠ¤T âŠ¤
1 (x,y)âˆ¼Dc1 1
=T IT âŠ¤ =Î» 2R R âŠ¤I =Î» 2I
1 1 1 1 1 1
Andsimilarly,assumingy =âˆ’1,wecanobtain
E T x=âˆ’T Âµ,
(x,y)âˆ¼Dcâˆ’1 âˆ’1 âˆ’1
E (T xâˆ’T Âµ)(T xâˆ’T Âµ)âŠ¤ =Î» 2I.
(x,y)âˆ¼Dcâˆ’1 âˆ’1 âˆ’1 âˆ’1 âˆ’1 âˆ’1
Thuswehave: D âˆ¼N(yT Âµ,Î»2I).
u y y
D.2 ProofforLemma4
Lemma4. TheBayesoptimaldecisionboundaryforclassifyingD isgivenbyP (x)â‰¡AxâŠ¤x+
u u
BâŠ¤x+C =0,whereA=Î»âˆ’2âˆ’Î»âˆ’2,B =2(Î»âˆ’2T +Î»âˆ’2T )Âµ,andC =ln |Î»2 âˆ’1I| .
âˆ’1 1 âˆ’1 âˆ’1 1 1 |Î»2I|
1
Proof: AttheoptimaldecisionboundarytheprobabilitiesofanypointxâˆˆRd belongingtoclass
y = 1andy = âˆ’1modeledbyD arethesame. Similartotheoptimaldecisionboundaryofthe
u
cleandatasetD ,wehave:
c
exp[âˆ’1(xâˆ’T Âµ )âŠ¤(Î»2 I)âˆ’1(xâˆ’T Âµ )]
2 âˆ’1 âˆ’1 âˆ’1 âˆ’1 âˆ’1
(cid:113)
(2Ï€)d|Î»2 I|
âˆ’1
exp[âˆ’1(xâˆ’T Âµ )âŠ¤(Î»2I)âˆ’1(xâˆ’T Âµ )]
= 2 1 1 1 1 1
(cid:112)
(2Ï€)d|Î»2I|
1
â‡’(xâˆ’T Âµ )âŠ¤Î»âˆ’2(xâˆ’T Âµ )+ln|Î»2 I|
âˆ’1 âˆ’1 âˆ’1 âˆ’1 âˆ’1 âˆ’1
=(xâˆ’T Âµ )âŠ¤Î»âˆ’2(xâˆ’T Âµ )+ln|Î»2I|
1 1 1 1 1 1
â‡’Î»âˆ’2(xâŠ¤xâˆ’xâŠ¤T Âµ âˆ’ÂµâŠ¤ TâŠ¤ x+ÂµâŠ¤ TâŠ¤ T Âµ )
âˆ’1 âˆ’1 âˆ’1 âˆ’1 âˆ’1 âˆ’1 âˆ’1 âˆ’1 âˆ’1
|Î»2 I|
âˆ’Î»âˆ’2(xâŠ¤xâˆ’xâŠ¤T Âµ âˆ’ÂµâŠ¤TâŠ¤x+ÂµâŠ¤TâŠ¤T Âµ )+ln âˆ’1 =0
1 1 1 1 1 1 1 1 1 |Î»2I|
1
â‡’(Î»âˆ’2âˆ’Î»âˆ’2)xâŠ¤xâˆ’2(Î»âˆ’2ÂµâŠ¤ TâŠ¤ âˆ’Î»âˆ’2ÂµâŠ¤TâŠ¤)x
âˆ’1 1 âˆ’1 âˆ’1 âˆ’1 1 1 1
|Î»2 I|
+ÂµâŠ¤ Âµ âˆ’ÂµâŠ¤Âµ +ln âˆ’1 =0
âˆ’1 âˆ’1 1 1 |Î»2I|
1
|Î»2 I|
â‡’P (x)â‰¡AxâŠ¤x+2[(Î»âˆ’2T +Î»âˆ’2T )Âµ]âŠ¤x+ln âˆ’1 =0
u âˆ’1 âˆ’1 1 1 |Î»2I|
1
â‡’P (x)â‰¡AxâŠ¤x+BâŠ¤x+C =0
u
whereA=Î»âˆ’2âˆ’Î»âˆ’2,B =2(Î»âˆ’2T +Î»âˆ’2T )Âµ,andC =ln |Î»2 âˆ’1I| . Besides,notethatifP (x)
âˆ’1 1 âˆ’1 âˆ’1 1 1 |Î»2I| u
1
islessthan0,thecategoryoftheBayesianoptimalclassificationis-1;otherwise,itis1.
D.3 ProofforLemma5
Lemma5. Letz âˆ¼ N(0,I),Z = zâŠ¤z+bâŠ¤z+c,andâˆ¥Â·âˆ¥ denote2-normofvectors. Forany
2
tâ‰¥0andÎ³ âˆˆR,weuseChernoffboundtohave:
(cid:110) (cid:111)
exp t2 ||b||2âˆ’t(Î³+d)
P{Z â‰¥E[Z]+Î³}â‰¤ 2(1âˆ’2t) 2
|(1âˆ’2t)I|21
22Proof: SinceAisaconstant,wehave:
B C
P (x)â‰¡xâŠ¤x+( )âŠ¤x+ =0
u A A
â‡’P (x)â‰¡xâŠ¤x+bâŠ¤x+c=0
u
whereb= B,c= C. LetZ =zâŠ¤z+bâŠ¤z+candz âˆ¼N(0,I)âŠ‚Rd. Thuswehave:
A A
1 1 1
Z =zâŠ¤z+bâŠ¤z+c=(zâŠ¤+ bâŠ¤)(z+ b)+câˆ’ bâŠ¤b
2 2 4
1 1 1
=(z+ b)âŠ¤(z+ b)+câˆ’ bâŠ¤b
2 2 4
Foranytâ‰¥0andxâˆ¼N(0,I),wewritethemomentgeneratingfunctionforaquadraticrandom
variableY =xâŠ¤xas5:
E[exp(tY)]= 1 (cid:90) exp(cid:8) txâŠ¤x(cid:9) exp(cid:26) âˆ’1 (xâˆ’Âµ)âŠ¤(xâˆ’Âµ)(cid:27) dx
(2Ï€)d/2 2
Rd
exp(cid:8) âˆ’ÂµâŠ¤Âµ/2(cid:9)(cid:90) (cid:26)
2tâˆ’1
(cid:27)
= exp xâŠ¤x+ÂµâŠ¤x dx
(2Ï€)d/2 2
Rd
(cid:110) (cid:111)
exp(cid:8) âˆ’ÂµâŠ¤Âµ/2(cid:9)(2Ï€)d/2exp 2(1âˆ’1 2t)ÂµâŠ¤Âµ
=
(2Ï€)d/2 |Iâˆ’2tI|1
2
(cid:110) (cid:111)
exp t ÂµâŠ¤Âµ
1âˆ’2t
=
|Iâˆ’2tI|1
2
(cid:110) (cid:111)
exp t bâŠ¤b+t(câˆ’ 1bâŠ¤b) exp{ t2 bâŠ¤b+tc}
=â‡’E[exp(tZ)]= 4(1âˆ’2t) 4 = 2(1âˆ’2t)
|(1âˆ’2t)I|1
2
|(1âˆ’2t)I|21
Afterthat,weemployChernoffbound,forsomeÎ³,wehave:
E[exp(tZ)]
P{Z â‰¥E[Z]+Î³}â‰¤
exp{t[Î³+E(Z)]}
(cid:110) (cid:111)
exp t2 bâŠ¤b+tc
2(1âˆ’2t)
=
exp{t(Î³+E[zâŠ¤z]+c)}|(1âˆ’2t)I|21
(cid:110) (cid:111)
exp t2 bâŠ¤b+tc
2(1âˆ’2t)
=
exp{t(Î³+Tr(I)+E(bâŠ¤z)+c)}|(1âˆ’2t)I|21
(cid:110) (cid:111)
exp t2 bâŠ¤b+tc
2(1âˆ’2t)
=
exp{t(Î³+d+c)}|(1âˆ’2t)I|21
(cid:110) (cid:111)
exp t2 ||b||2âˆ’t(Î³+d)
2(1âˆ’2t) 2
=
|(1âˆ’2t)I|21
5[38]
23D.4 ProofforTheorem6
Theorem6. Foranyconstantt andt satisfying0â‰¤t < 1 and0â‰¤t < 1,theaccuracyofthe
1 2 1 2 2 2
unlearnabledecisionboundaryP onthedatasetD canbeupper-boundedas:
u c
Ï„ (P )â‰¤
exp(cid:110) 2(1âˆ’t2 1 2t1)||b+2Âµ||2 2+t 1(ÂµâŠ¤Âµ+bâŠ¤Âµ+c)(cid:111)
Dc u 2|(1âˆ’2t 1)I|21
+
exp(cid:110) 2(1âˆ’t2 2 2t2)||bâˆ’2Âµ||2 2âˆ’t 2(ÂµâŠ¤Âµâˆ’bâŠ¤Âµ+c+2d)(cid:111)
2|(1âˆ’2t 2)I|1 2
:=p +p
1 2
Furthermore,ifÂµâŠ¤Âµ+bâŠ¤Âµ+c+d < 0andâˆ’ÂµâŠ¤Âµ+bâŠ¤Âµâˆ’câˆ’d < 0,wehaveÏ„ (P ) < 1.
Dc u
Moreover,foranyÂµÌ¸=0âˆƒtransformationmatrixT suchthatÏ„ (P )<Ï„ (P).
i Dc u Dc
Proof: WenotethatifP (x)islessthan0,thecategoryoftheBayesianoptimalclassificationis-1;
u
otherwise,itis1. Here,x=yÂµ+zwherez âˆ¼N(0,I)andy âˆˆ{Â±1}since(x,y)âˆ¼D .
c
Ï„ (P )=E{I(y(xâŠ¤x+bâŠ¤x+c)>0)}
Dc u
=P{y(ÂµâŠ¤Âµ+zâŠ¤z+2yÂµâŠ¤z+ybâŠ¤Âµ+bâŠ¤z+c)>0}
=P(y =1)P{y(ÂµâŠ¤Âµ+zâŠ¤z+2yÂµâŠ¤z+ybâŠ¤Âµ+bâŠ¤z
+c)>0|y =1}+P(y =âˆ’1)P{y(ÂµâŠ¤Âµ+zâŠ¤z
+2yÂµâŠ¤z+ybâŠ¤Âµ+bâŠ¤z+c)>0|y =âˆ’1}
1
= P{zâŠ¤z+(b+2Âµ)âŠ¤z+ÂµâŠ¤Âµ+bâŠ¤Âµ+c>0}
2
1
+ P{âˆ’zâŠ¤zâˆ’(bâˆ’2Âµ)âŠ¤zâˆ’ÂµâŠ¤Âµ+bâŠ¤Âµâˆ’c>0}
2
:=p +p
1 2
Wecanseethat:
âˆ’Î³ :=E{zâŠ¤z+(b+2Âµ)âŠ¤z+ÂµâŠ¤Âµ+bâŠ¤Âµ+c}
1
=Tr(I)+ÂµâŠ¤Âµ+bâŠ¤Âµ+c
âˆ’Î³ :=E{âˆ’zâŠ¤zâˆ’(bâˆ’2Âµ)âŠ¤zâˆ’ÂµâŠ¤Âµ+bâŠ¤Âµâˆ’c}
2
=âˆ’Tr(I)âˆ’ÂµâŠ¤Âµ+bâŠ¤Âµâˆ’c
ApplyingLemma5,withÎ³ =Î³ ,t=t forthecomputationofp ,aswellasÎ³ =Î³ andt=t for
1 1 1 2 2
thecomputationofp ,wheret andt arespecificnon-negativeconstants,weobtain:
2 1 2
p =
exp(cid:110) 2(1âˆ’t2 1 2t1)||b+2Âµ||2 2+t 1(ÂµâŠ¤Âµ+bâŠ¤Âµ+c)(cid:111)
1 2|(1âˆ’2t 1)I|1 2
p =
exp(cid:110) 2(1âˆ’t2 2 2t2)||bâˆ’2Âµ||2 2âˆ’t 2(ÂµâŠ¤Âµâˆ’bâŠ¤Âµ+c+2d)(cid:111)
2 2|(1âˆ’2t 2)I|1 2
ThisprovidesuswiththeupperboundforÏ„ (P ). Nonetheless,toensurethatthisupperboundis
Dc u
lessthan1,additionalconditionsneedtobeaffirmed. AsÎ³ andÎ³ increase,thevaluesofp and
1 2 1
p diminish(p >0,p >0),andasÎ³ increases,Î³ decreases(sinceÎ³ +Î³ =âˆ’2bâŠ¤Âµ). Welet
2 1 2 1 2 1 2
||b+ 22Âµ||2 2 equaltoÎ± 1 â‰¥0,ÂµâŠ¤Âµ+bâŠ¤Âµ+c(alsoequalsto||Âµ||2 2+câˆ’ Î³1+ 2Î³2)equaltoÎ² 1,resulting
24in:
dp
1 =
1d[exp{ 1Î± âˆ’1 2t t2 1
1
+Î² 1t 1}/(1âˆ’2t 1)d 2]
dt 2 dt
1 1
=
exp{ 1Î± âˆ’1 2t t2 1
21
+Î² 1t 1}
[(1âˆ’2t 1)âˆ’d 2âˆ’1d
2Î± t (1âˆ’t )
+(1âˆ’2t 1)âˆ’d 2( (11 âˆ’1
2t
)21 +Î² 1)]
1
=
exp{ 1Î± âˆ’1 2t t2 1
1
+Î² 1t 1}
[
d
+
2Î± 1t 1(1âˆ’t 1)
+Î² ]
2(1âˆ’2t 1)d 2 1âˆ’2t 1 (1âˆ’2t 1)2 1
d 2Î± t (1âˆ’t )
=p ( + 1 1 1 +Î² )
1 1âˆ’2t (1âˆ’2t )2 1
1 1
2(2Î² âˆ’Î± )t2+2(Î± âˆ’2Î² âˆ’d)t +Î² +d
= 1 1 1 1 1 1 1 p
(1âˆ’2t )2 1
1
Making|(1âˆ’2t 1)I|1 2 meaningfulrequiressatisfyingthefollowingcondition:
1
1âˆ’2t >0=â‡’0â‰¤t <
1 1 2
Wenotethatp (t =0)= 1,p (t â†’ 1)=+âˆ. Whenweset dp1 =0,weobtainthat:
1 1 2 1 1 2 dt1
2(2Î² âˆ’Î± )t2+2(Î± âˆ’2Î² âˆ’d)t +d+Î² =0
1 1 1 1 1 1 1
(cid:112)
2Î² âˆ’Î± +dÂ± Î±2âˆ’2Î± Î² +d2
=â‡’t = 1 1 1 1 1
1 2(2Î² âˆ’Î± )
1 1
=â‡’Î±2âˆ’2Î² Î± +d2 â‰¥0
1 1 1
d2+Î±2 d2 Î±
=â‡’Î² â‰¤ 1 = + 1
1 2Î± 2Î± 2
1 1
d2 Î±
=â‡’Î² â‰¤( + 1) =d
1 2Î± 2 min
1
ExplanationoftheLastInequality. Assumingwedefines(Î± 1)= 2d Î±2
1
+ Î± 21. Theminimumvalue
ofthisfunctioncanbeobtainedusingtheArithmeticMean-GeometricMeanInequality(AM-GM
âˆš (cid:113)
Inequality),whichstatesthata+b â‰¥ 2 abfora,b â‰¥ 0. Thus,s(Î± 1) â‰¥ 2 2d Î±2
1
Â· Î± 21 = d. Since
Î² â‰¤s(Î± ),wecaninferthatÎ² â‰¤s(Î± ) ,whichmeansÎ² â‰¤d.
1 1 1 1 min 1
Theproductoftherootsoftheequationis: t t = d+Î²1 (weassumethatt <t ). Welet
11 12 2(2Î²1âˆ’Î±1) 11 12
f(t 1)=2(2Î² 1âˆ’Î± 1)t2 1+2(Î± 1âˆ’2Î² 1âˆ’d)t 1+d+Î² 1.Thuswehavef(0)=d+Î² 1,f(1 2)= Î± 21 >0.
Situation(i): Î² <âˆ’d. Atthistime,f(0)<0,t t >0,basedonthetrendofquadraticfunctions
1 11 12
and the distribution of roots, we can infer that 0 < t < 1, t > 1, and 2Î² < Î± . Thus we
11 2 12 2 1 1
concludethatthereexistst suchthatp (t =t )< 1.
11 1 1 11 2
Situation(ii): âˆ’d < Î² < 0. Atthistime,f(0) > 0,t t < 0,basedonthetrendofquadratic
1 11 12
functionsandthedistributionofroots,wealsocaninferthatt <0,t > 1,and2Î² <Î± . Thus
11 12 2 1 1
wehavethatp â‰¥ 1.
1 2
Situation(iii): 0<Î² <d. Atthistime,f(0)>0,thesignoft t simultaneouslydeterminesthe
1 11 12
directionoftheopeningofthequadraticfunctionf(t ). Whent t <0,t <0,t > 1,thuswe
1 11 12 11 12 2
havep â‰¥ 1;whent t >0,0<t <t < 1,theminimumpointofp isatp (t ). However,
1 2 11 12 11 12 2 1 1 12
itischallengingtocomparep (t )and 1 todeterminewhichisgreaterorsmaller.
1 12 2
Similarlyforp 2,welet ||bâˆ’ 22Âµ||2 2 equaltoÎ± 2 >0,âˆ’ÂµâŠ¤Âµ+bâŠ¤Âµâˆ’câˆ’2dequaltoÎ² 2,resultingin:
dp 2(2Î² âˆ’Î± )t2+2(Î± âˆ’2Î² âˆ’d)t +Î² +d
2 = 2 2 2 2 2 2 2 p
dt (1âˆ’2t )2 2
2 2
25Thuswehavethesimilarsituation,i.e.,Î² <âˆ’d. Atthistime,f(0)<0,t t >0,basedonthe
2 21 22
trendofquadraticfunctionsandthedistributionofroots,wecaninferthat0 < t < 1,t > 1,
21 2 22 2
and2Î² <Î± . Thusweconcludethatthereexistst suchthatp (t =t )< 1.
2 2 21 2 2 21 2
Takingintoaccounttheabovesituations,wehavethat: whenÎ² <âˆ’d,Î² <âˆ’d(i.e.,ÂµâŠ¤Âµ+bâŠ¤Âµ+
1 2
c+d < 0andâˆ’ÂµâŠ¤Âµ+bâŠ¤Âµâˆ’câˆ’d < 0),thereexistst andt respectively,makingp < 1,
âˆš 11 21 âˆš 1 2
p < 1,i.e.,p +p <1,wheret = 1 + dâˆ’ Î±2 1âˆ’2Î±1Î²1+d2 ,t = 1 + dâˆ’ Î±2 2âˆ’2Î±2Î²2+d2 .
2 2 1 2 11 2 2(2Î²1âˆ’Î±1) 21 2 2(2Î²2âˆ’Î±2)
WeknowthatforÂµÌ¸=0,Ï„ (P)=Ï•(Âµ)> 1. Therefore,weneedtointroduceadditionalconditions
Dc 2
tofurtherensurethatp < 1,andp < 1,andconsequentlyÏ„ (P )=p +p < 1 <Ï„ (P).
1 4 2 4 Dc u 1 2 2 Dc
Toletp satisfyp < 1 (Î± >0,Î² <âˆ’d,and0â‰¤t < 1),thatis,
1 1 4 1 1 1 2
exp{ Î± 1t2 1 +Î² t }< (1âˆ’2t 1)d 2
1âˆ’2t 1 1 2
1
Î± t2 d
â‡’ 1 1 +Î² t < ln(1âˆ’2t )âˆ’ln2
1âˆ’2t 1 1 2 1
1
Î± t2 d
â‡’ 1 1 +Î² t âˆ’ ln(1âˆ’2t )<âˆ’ln2=âˆ’0.693
1âˆ’2t 1 1 2 1
1
Weassumethatg(t)= 1Î± âˆ’1t 22
t
+Î² 1tâˆ’ d 2ln(1âˆ’2t). LetusassumeÎ±
1
= 1
2
andd=3for3Dpoint
clouddata,thenÎ² <âˆ’3. Uponanalyzingthefunctiong(t),weobservethatasÎ² decreases,the
1 1
minimumvalueofg(t)alsodecreases. WeutilizevariousÎ² valuesandtheircorrespondingfunction
1
valuetoprovideamoreintuitiveunderstandingasshowninTab.13.
Î² -4 -6 -8 -10 -12 -14
1
g(0.3) 0.287 -0.313 -0.913 -1.513 -2.113 -2.713
g(0.4) 1.214 0.414 -0.386 -1.186 -1.986 -2.786
Table13: DifferentÎ² valuesandcorrespondingg(t)witht = 0.3andt = 0.4. Theboldvaluesrepresent
1
caseswherep < 1 issatisfied.
1 4
Asforp ,sinceÎ± Ì¸= Î± ,weneedtoreselectappropriatevaluestodemonstratetheexistenceof
2 2 1
p < 1. Similarly,toletp satisfyp < 1 (Î± >0,Î² <âˆ’d,and0â‰¤t < 1),thatis,
2 4 2 2 4 2 2 2 2
exp{ Î± 2t2 2 +Î² t }< (1âˆ’2t 2)d 2
1âˆ’2t 2 2 2
2
Î± t2 d
â‡’ 2 2 +Î² t âˆ’ ln(1âˆ’2t )<âˆ’ln2=âˆ’0.693
1âˆ’2t 2 2 2 2
2
Weassumethath(t) = 1Î± âˆ’2t 22
t
+Î² 2tâˆ’ d 2ln(1âˆ’2t). LetusassumeÎ±
2
= 1
3
andd = 3, similarly,
we utilize various Î² values and their corresponding function value to provide a more intuitive
2
understandingasshowninTab.14.
Î² -4 -6 -8 -10 -12 -14
2
h(0.3) 0.249 -0.351 -0.951 -1.551 -2.151 -2.751
h(0.4) 1.081 0.281 -0.519 -1.319 -2.119 -2.919
Table14: DifferentÎ² valuesandcorrespondingh(t)witht = 0.3andt = 0.4. Theboldvaluesrepresent
2
caseswherep < 1 issatisfied.
2 4
Therefore,weconcludethatthereexistÎ± ,Î² ,t suchthatp < 1,Î± ,Î² ,t suchthatp < 1,i.e.,
1 1 1 1 4 2 2 2 2 4
p +p < 1.Atthesametime,weobservethatasmallervalueofÎ²makesiteasiertosatisfytheabove
1 2 2
conditions,i.e.,themorenegativeÎ² andÎ² are,themorelikelyitistosatisfytheaboveconditions.
1 2
Weformallycombineandasserttheseconditionsas,bâŠ¤Âµâ‰ª0,i.e.,ÂµâŠ¤Î»âˆ’ âˆ’2 1TâŠ¤ âˆ’1+Î»âˆ’ 12TâŠ¤
1 Âµâ‰ª0(we
Î»âˆ’2âˆ’Î»âˆ’2
âˆ’1 1
sufficientlysupportthisconditionintheempiricalresultsfromTabs.13and14). Thusweconclude
thatforanyÂµÌ¸=0âˆƒtransformationparametersT suchthatÏ„ (P )<Ï„ (P).
i Dc u Dc
26