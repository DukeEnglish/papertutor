Training-Free Model Merging for
Multi-target Domain Adaptation
Wenyi Li*1, Huan-ang Gao*1, Mingju Gao1, Beiwen Tian1, Rong Zhi2, and
Hao Zhaoâ€ 1
1 Institute for AI Industry Research (AIR), Tsinghua University
2 Mercedes-Benz Group China Ltd.
liwenyi19@mails.ucas.ac.cn, zhaohao@air.tsinghua.edu.cn
Abstract. In this paper, we study multi-target domain adaptation of
sceneunderstandingmodels.Whilepreviousmethodsachievedcommend-
ableresultsthroughinter-domainconsistencylosses,theyoftenassumed
unrealistic simultaneous access to images from all target domains, over-
lookingconstraintssuchasdatatransferbandwidthlimitationsanddata
privacy concerns. Given these challenges, we pose the question: How to
merge models adapted independently on distinct domains while bypass-
ingtheneedfordirectaccesstotrainingdata?Oursolutiontothisprob-
lem involves two components, merging model parameters and merging
model buffers (i.e., normalization layer statistics). For merging model
parameters, empirical analyses of mode connectivity surprisingly reveal
that linear merging suffices when employing the same pretrained back-
bone weights for adapting separate models. For merging model buffers,
we model the real-world distribution with a Gaussian prior and esti-
mate new statistics from the buffers of separately trained models. Our
method is simple yet effective, achieving comparable performance with
data combination training baselines, while eliminating the need for ac-
cessingtrainingdata.Projectpage:https://air-discover.github.io/
ModelMerging.
Keywords: Multi-target Domain Adaptation Â· Mode Connectivity Â·
Model Averaging
1 Introduction
Scene understanding models need to perform reliably across various domains
(i.e., diverse lighting [89], weather, and urban landscapes) to be truly useful
forautonomousdrivingworldwide.Thetypicalapproachofsupervisedlearning,
however,reliesheavilyoncostlypixel-levelannotationsbyhumans,whichsignif-
icantlyhampersthescalabilityofthesesegmentationmodels.Assuch,thestudy
of multi-target domain adaptation [20,34,55] (MTDA) is becoming increasingly
significant.Thisareaofresearchfocusesondevisingstrategiestosimultaneously
utilize large-scale unlabeled real-world data from multiple domains along with
â‹† * Indicates Equal Contribution. â€  Indicates Corresponding Author.
4202
luJ
81
]VC.sc[
1v17731.7042:viXra2 W. Li et al.
labeled synthetic data during training [78], providing a scalable approach to
enhance the robustness of these models.
MTDA presents a greater chal-
lenge compared to traditional single-
target domain adaptation (STDA, il- (a) Single Target Domain Adaptation (STDA) (c) Our Setting:
lustrated in Fig.1(a)), due to the in- Source Domain Target Domain 1 MTDA w/oaccess to training data
herent difficulty in developing a sin-
Source Domain Target Domain 2
gle model that performs effectively â€¦ â€¦
across multiple target domains. To Source Domain Target Domain N
â€¦
tackle this, previous approaches [20,
(b) Multi Target Domain Adaptation (MTDA)
34,48,55] have employed consistency with access to target data from all domains
learning among various expert mod- Target Domain 1 ?
els and online knowledge distillation SourceDomain Target â€¦Domain 2
to construct a domain-generic stu- Target Domain N
dent model. Nonetheless, a signifi-
Fig.1: Comparison of Domain Adap-
cant limitation of these methods is tation Settings. (a) Single Target Do-
their reliance on simultaneous access mainAdaptation(STDA)focusesonlever-
to all target data, as illustrated in aging labeled synthetic data and unlabeled
Fig. 1(b), which is usually impracti- data from a single target domain together
cal. This impracticality stems partly for optimal performance in that target do-
main.(b)Multi-targetDomainAdaptation
fromdatatransfercostconstraints,as
(MTDA)withdataaccessinvolvesutilizing
adatasetcomprisingthousandsofim-
datafromtargetdomainstogethertotrain
ages can balloon to hundreds of gi-
asinglemodelcapableofexcellingacrossall
gabytes. Moreover, data privacy con-
these domains. (c) MTDA without direct
cerns are significant, with potential
access to training data, employing model
restrictions on the sharing or trans- merging to enhance robustness.
ferring of such data due to policies.
Facing these challenges, in this paper, we consider a novel problem setting
illustratedinFig.1(c).Ourprimaryfocusremainsonmulti-targetdomainadap-
tation, but instead of accessing data from multiple domains, we gain access to
models that have been independently adapted to each of them. Our objective is
to integrate these models into a single, effective model for various domains.
But how to merge multiple models into one while preserving their respective
abilitiesontheirrespectivedomains?Oursolutiontothisproblemhastwomain
components.(I) Merging model parameters. Astraightforwardmethodin-
volves averaging the weights of models as a means of integration. Nevertheless,
this approach raises questions about its underlying mechanisms and its reliabil-
ity across different scenarios. In our research, we conduct careful investigation
into the conditions under which such merging is effective, and when it is not, by
examining the concept of linear mode connectivity of models. Through metic-
ulous analysis, we find that pretraining significantly contributes to enhancing
this linear connectivity among trained models. (II) Merging model buffers.
We identify the importance of model buffers, i.e., batch normalization (BN)
layerstatistics,whichcapturedomain-specificcharacteristicsinourmulti-target
domain adaptation setting. Leveraging the Gaussian prior assumption of BNTraining-Free Model Merging for Multi-target Domain Adaptation 3
layers, we estimate new means and variances for the merged layers based on the
statistics of separately trained models.
Our method, simple yet effective, demonstrates notable improvements in
performance compared to a variety of baselines. For instance, when we apply
our model merging technique to the state-of-the-art STDA method [29] using a
ResNet101[24]backbone,weobserveasubstantialincreaseof+5.6%inharmonic
mean of mIoUs for individual domains. Remarkably, this level of performance
rivals that achieved by baseline methods that involve training with multiple
combineddatasets,evenwhendataavailabilityisconstrained.Furthermore,our
technique outperforms previous top-performing multi-target domain adaptation
methods that utilize explicit consistency training by a considerable margin, un-
derscoring the critical role of exploring model connectivities. In summary, our
contributions are as follows:
â€¢ Weconductasystematicexplorationofmodeconnectivityindomain-adapted
scene parsing models, revealing the underlying conditions of when model
merging works.
â€¢ We introduce a model merging technique including parameter merging and
buffer merging for multi-target domain adaptation tasks, applicable to any
single-target domain adaptation model.
â€¢ Our approach achieves performance comparable to that of training with
multiple combined datasets, even when data availability is constrained.
2 Related Works
2.1 Domain Adaptation for Semantic Segmentation
Unsupervised domain adaptation [14] helps to reduce annotation costs in pixel-
wise semantic segmentation by utilizing unlabeled real-world images alongside
labeled synthetic samples.
Single-target Domain Adaptation. The challenge of STDA mainly in-
volves how to effectively reduce the distribution gap between labeled synthetic
dataandunlabeledsingle-domainreal-worldimages.Methodstotacklethisissue
include domain-invariant representation learning [37,46,81], adversarial train-
ing[15,21,25,26,47,62,71,75],andteacher-studentself-training[17,18,22,28â€“30].
Amongtheseapproaches,thelastonehasproventobehighlysuccessful.Itlever-
ages synthetic data [7,16,45,68] as source domains, generates pseudo labels [42]
for target domains and enforces consistency regularization over data augmenta-
tion strategies [2,8,28,31,41,51,69,86,90,91]. Despite STDA being a broadly
studiedtopic,ithaslimitationsforreal-worldapplicationslikeautonomousdriv-
ing, where vehicles encounter diverse road scenarios beyond a single domain.
Multi-target Domain Adaptation. MTDA presents greater challenges
thanSTDA,particularlyintermsofintegratingdomain-specificknowledgefrom
multiple sources into a single model. While initial efforts in this area have fo-
cused on classification tasks [20,55,83], MTDA has also been explored for se-
mantic segmentation [34,55], employing strategies like inter-domain consistency4 W. Li et al.
regularization and online knowledge distillation. Nonetheless, the application of
these explicit inter-domain regularization techniques limits the use of certain
STDA strategies, like multi-resolution training, due to the excessive GPU mem-
ory demands associated with managing multiple student models. Consequently,
these approaches [34,55] significantly underperform when compared to STDA
methodologies [28â€“30]. Moreover, they rely on the impractical assumption of
havingsimultaneousaccesstodatafromalltargetdomains.Soweasktheques-
tion: is it possible to train several models under the best STDA setting and
merge them without access to training data (i.e., in a zero-shot way [23])?
2.2 Multi-target Learning with Constrained Data Assumption
Federated Learning (FL). While FL [50] focuses on the joint training of
models, our research studies the direct merging of models post training. Rep-
resentative FL methods confront challenges including communication cost [53],
data heterogeneity [44], and support for device capabilities [27] in a distributed
setting. In contrast, our approach diverges in methodology by leveraging the
advancements in large-scale, centralized pre-training of vision models. We an-
swer the question that under which specific conditions can we simply merge
pre-trained models adapted to distinct data domains. Federated MTDA. In
this setting [82], the distributed client data is unlabeled, and a centralized la-
beled dataset is available on the server. In our STDA stage, labeled dataset is
accessible for each domain (comparable to clients). Class-Incremental Do-
main Adaptation (CIDA).CIDA[40]learnsnoveltarget-domainclassesina
domain shift-aware manner, while they do not touch multiple target domains.
2.3 Mode Connectivity for Neural Networks
Mode connectivity refers to the fact where local minima in the loss functions
of neural networks can be connected through a curve in the parameter space,
along which the performance does not experience significant deterioration [19,
67]. Linear mode connectivity is a stronger constraint requiring that the convex
combination of two minima stay in the same loss basin, which is closely related
to the lottery ticket hypothesis [12] and has direct implications for continual
learning [52]. Exploiting the property of linear mode connectivity, several works
[32,35,54,58,74,79]integrateweightsofneuralnetworksforamorerobustmerged
model. In our study, we also observe a similar phenomenon in models adapted
todifferentdomains,andweconductanempiricalanalysistotracetheoriginof
it. We also identify the importance of batch normalization layer statistics in the
multi-targetdomainadaptationcontext,asthediversecharacteristicsinherentin
multipledomainsaredistinctlycapturedinthesestatisticallayers.Differentfrom
the aforementioned work focused on language or image inputs from subsets of
thesamedataset,weproposeanovelmethodtoeffectivelymergethesestatistics.Training-Free Model Merging for Multi-target Domain Adaptation 5
3 Methodology
3.1 Overview
We introduce a novel pipeline de-
signed to address the challenging
problem of multi-target domain
Phase 1: Model Training
adaptationinsemanticsegmenta- STDA: Source â†’ Target A ğ“›ğ‘ºğ’–ğ’‘ğ’†ğ’“ğ’—ğ’Šğ’”ğ’†ğ’… S lo au br ec le STDA: Source â†’ Target B ğ“›ğ‘ºğ’–ğ’‘ğ’†ğ’“ğ’—ğ’Šğ’”ğ’†ğ’… S lo au br ec le
tion,whichisillustratedinFig.2. Source Image Source Prediction Source Image Source Prediction
What sets our approach apart
from previous methods [3,20,34, Backbone Seg. Head Backbone Seg. Head
Target A Image Target A Prediction Target A Image Target B Prediction
55,76,85]istheeliminationofthe ğ“›ğ‘«ğ’ğ’ğ’‚ğ’Šğ’ğ‘¨ğ’…ğ’‚ğ’‘ğ’•ğ’‚ğ’•ğ’Šğ’ğ’ ğ“›ğ‘«ğ’ğ’ğ’‚ğ’Šğ’ğ‘¨ğ’…ğ’‚ğ’‘ğ’•ğ’‚ğ’•ğ’Šğ’ğ’
impractical assumption that im- Phase 2: Model Merging
Parameter Merging
agesofalltargetdomainsconcur-
Buffer Merging
rently accessible during the adap-
Target A Image Target A Prediction
tationphase.Instead,ourpipeline
comprises two distinct phases: a
single-target domain adaptation Backbone Seg. Head
Target B Image Target B Prediction
phaseandamodelmergingphase.
Fig.2: Overview of Two-stage Pipeline
In the first phase, we train mod-
of Our Proposed Multi-target Domain
els adapted for individual tar-
Adaptation Solution. After training STDA
get domains separately, while the
methods on separate domains, we integrate
second phase focuses on merging models together using our proposed merging
these adapted models together to techniques.
create a robust model, without
access to any training data.
Our primary focus lies on the proposed model merging phase. In the ini-
tial phase, we simply adopt the state-of-the-art unsupervised domain adapta-
tion approach, HRDA [29], leveraging various backbone architectures such as
ResNet [24] and vision transformer [80]. Our approach encompasses two critical
components of the models: parameters (i.e., weights and biases for the learn-
able layers) and buffers (i.e., running statistics for the normalization layers).
We provide detailed explanations of the merging techniques and the underlying
motivations for these techniques in Sec. 3.2 and Sec. 3.3 for merging parameters
and buffers, respectively.
3.2 Merging Parameters
Permutation-based methods degenerate. In fact, the idea of merging the
weightsandbiasesoflearnablelayersbetweenmodelshasbeenafrontierresearch
area.Amongtheseefforts,aparticularlypromisinglineofresearchhasemerged,
knownaspermutation-basedmethods[1,11,36,57].Thesemethodsoperateunder
the assumption that, when accounting for all potential permutation symmetries
of units in hidden layers of neural networks, the loss landscapes typically form
a single basin. Therefore, when merging model parameters Î˜ and Î˜ , the
A B
primary goal of these methods is to find a set of permutation transformations6 W. Li et al.
(a) Backbone = ResNet50 (b) Backbone = ResNet101 (c) Backbone = MiT-B5
Fig.3: Results of Git Re-Basin and Mid-Point Merging on Different Back-
bones. In our domain adaptation scenario, Git Re-Basin [1] reduced to a straightfor-
ward mid-point merging approach.
Fig.4: Empirical Analysis for Linear Mode Connectivity. (a) Exploring the
linearmodeconnectivityoftwotrainedResNet101backbonestargetedattwodifferent
domains. (b-e) Ablation studies on synthetic data, self-training architecture, initial-
izaiton weights and pretrained weights to find the cause of the linear mode connectiv-
ity.
Ï€(Â·) that ensures Ï€(Î˜ ) is functionally equivalent to Î˜ , while also residing
B B
within an approximately convex basin near the reference model Î˜ . After that,
A
with a simple mid-point merging (Î» = 1 in Eq. 1), we can acquire a merged
2
model Î˜â€² that exhibits better generalization ability than single models,
Î˜â€² =Î»Î˜ +(1âˆ’Î»)Ï€(Î˜ ). (1)
A B
In our scenario, both Î˜ and Î˜ are trained during the first phase employ-
A B
ing identical network architectures [29] and utilizing the same synthetic images
and labels [60]. However, they are adapted to samples from distinct domains for
semantic segmentation [9,73]. Our initial attempt involved employing a repre-
sentative permutation-based method known as Git Re-Basin [1]. This method
transformsthetaskoffindingpermutation-symmetrictransformationsintoalin-
ear assignment problem (LAP), for which efficient and practical algorithms areTraining-Free Model Merging for Multi-target Domain Adaptation 7
available. Surprisingly, in our experimental setup, Git Re-Basinâ€™s performance
equaled that of a simple mid-point merging across all network architectures,
including ResNet50 [24], ResNet101 [24], and MiT-B5 [80], and the results of
mid-point merging (also results of Git Re-Basin [1]) are illustrated in Fig. 3.
Further investigation revealed that the permutation transformation discovered
by Git Re-Basin [1] remained identical permutation through iterations of solv-
ing LAP, suggesting that in our domain adaptation scenario, Git Re-Basin [1]
reduced to a straightforward mid-point merging approach.
Empirical analysis of linear mode connectivity.Wefurtherinvestigate
the above degeneration problem through the lens of linear mode connectivity
[13,19,52]. Specifically, we use continuous curve Ï•(Î») : [0,1] â†’ R|Î˜| to connect
Î˜ and Î˜ in the parameter space. In this specific case, we consider a linear
A B
path as follows,
Ï•(Î»)=Î»Î˜ +(1âˆ’Î»)Î˜ . (2)
A B
After defining the curve connecting the models to be merged, we traverse
along the curve and evaluate the performance of the interpolated models. To
gauge the effectiveness of these models in adapting to the two specified target
domains,denotedas D andD ,respectively,weemploytheHarmonic Meanas
1 2
our primary evaluation metric,
2Â·mIoU Â·mIoU
Harmonic Mean= D1 D2. (3)
mIoU +mIoU
D1 D2
We select harmonic mean as the metric as it gives more weight to smaller
values, which corresponds to the worst-case performance among various cities
over the world. It effectively penalizes scenarios where performance in one do-
main (e.g., in well-developed big cities) is disproportionately high while other
domains (e.g., in a rural third-world town) have low performance. We believe
this is aligned with the initial goal of the multi-target domain adaptation task.
On the contrary, in an extreme case, scoring 100% in one city and zero on all
other three cities still leads to 25% performance using arithmetic mean num-
bers, which we believe is of limited value in the multi-target domain adaptation
problem.
TheevaluationresultsoftheinterpolationaredepictedinFig.4(a).â€™CSâ€™and
â€™IDDâ€™ denotes target datasets Cityscapes [9] and Indian Driving Dataset [73],
respectively. Notably, it is evident that the two models from the first phase are
already linearly mode connected without permutation, as the harmonic mean of
the interpolated models outperforms the performance of individual models for
both domains.
Understanding the cause of linear mode connectivity. Given the
aforementioned revelation, we inquire: What is the underlying reason behind
the linear mode connectivity property observed in previous domain adaptation
methods? Subsequently, we conduct ablation experiments to investigate several
constant factors during the training of Î˜ and Î˜ in the first phase.
A B
Synthetic Data. The utilization of the same synthetic data may serve as
a bridge between the two domains. To assess this, we partition the training8 W. Li et al.
data from synthetic set [60] into two distinct non-overlapping subgroups, each
comprising30%oftheoriginaltrainingsamples.Duringthepartitioningprocess,
wegroupimageswithidenticalsceneidentifiersprovidedbythesyntheticdataset
into the same subgroup, while scenes with significant differences are placed in
separatesubgroups.Wetraintwosingle-targetdomainadaptationmodels,using
the source domain provided by these two distinct subgroups, while setting the
target domain as the CityScapes [9] dataset. We subsequently investigate the
linear mode connectivity of the two resulting models. The results, displayed
in Fig. 4(b), reveal that there is no notable decline in performance along the
linear curve connecting the two resulting models within the parameter space.
This observation suggests that the usage of the same synthetic data is not the
primary factor influencing the linear mode connectivity.
Self-training Architecture. The utilization of a teacher-student self-teaching
architecture [66] may confine resulting models to the same basin within the loss
landscape. To assess this possibility, we disable the exponential moving average
(EMA) update for the teacher models. Instead, we copy the student weights
to teacher models during each iteration. Subsequently, we proceed to train two
single-targetdomainadaptationmodels,utilizingGTA[60]asthesourcedomain
and Cityscapes [9] and IDD [73] as the target domains, respectively. We then
investigate the linear curve that connects the two resulting models within the
parameter space, and the outcomes are presented in Fig. 4(c). We can see that
the linear mode connectivity property remains intact.
Initialization and Pretraining. The practice of initializing the backbone with
thesamepretrainedweightscanpotentiallyplaceitinabasinthatischallenging
to escape from during the training process. To examine this potential scenario,
weinitializetwoseparatebackboneswithdistinctweightsandthenproceedwith
domain adaptation targeting the Cityscapes [9] and IDD [73] domains. During
theevaluationofthelinearinterpolatedmodelsbetweenthetwoconvergedmod-
els, we observe a notable deterioration in performance, as shown in Fig. 4(d).
To gain a deeper understanding of the underlying factors, we explore whether it
is the identical initial weight or the pretraining process that contributes to this
effect.Weinitiatetwobackboneswiththesameweightsbutwithoutpretraining
and conduct the experiment once more. Interestingly, we still encounter a sub-
stantial performance barrier along the linear connecting curve in the parameter
space, as reported in Fig. 4(e). This implies that it is the pretraining process
thatplaysapivotalroleinfacilitatinglinearmodeconnectivityinthefine-tuned
models.
Summary. Our empirical analysis highlight that when commencing from
identicalpretrainedweights,domainadaptationmodelscaneffectivelytransition
to diverse target domains while still maintaining linear mode connectivity in
the parameter space. Consequently, a straightforward mid-point merge between
these trained models could generate models with robustness in both domains.
3.3 Merging BuffersTraining-Free Model Merging for Multi-target Domain Adaptation 9
Buffers, namely running mean and variance for batch
normalization (BN) layers, bear a close association
with domains as they encapsulate domain-specific
characteristics. The question of how to effectively
merge buffers when merging models is usually over-
Merging Buffers
looked, as existing methods [32,35,74,79] predom-
inantly address the merging of two models trained
on separate subsets within the same domain. These
approaches are reasonable, as buffers from any given
model can be regarded as an unbiased estimation of Data points (not accessible when merging)
Gaussian distribution defined by BN statistics
the entire population, albeit solely derived from ran-
domized data subsamples. Nevertheless, in our spe- Fig.5: Illustration on
cific problem context, we are investigating the merg- Merging Statistics in
ingoftwomodelstrainedincompletelydistincttarget Batch Normalization
domains, rendering the buffer merging problem non- (BN) Layers.
trivial anymore.
Since we assume no access to any form of training
data during the merging phase of model A and model B, our available informa-
tion is confined to the set of buffers Î“={Âµ(i),Ïƒ(i),n(i)}L . Here, L represents
i=1
the number of BN layers, while Âµ(i), Ïƒ(i), and n(i) respectively signify the run-
ning statistics for mean, standard deviation, and the number of tracked batches
for the i-th layer. The statistics of the resulting BN layer is given as,
n(i)=n(i)+n(i),
A B
1
Âµ(i)= (n(i)Âµ(i)+n(i)Âµ(i)),
n(i) A A B B
(cid:32)
1 (4)
[Ïƒ(i)]2= n(i)[Ïƒ(i)]2+n(i)[Ïƒ(i)]2
n(i) A A B B
(cid:33)
+n(i)[Âµ(i)âˆ’Âµ(i)]2+n(i)[Âµ(i)âˆ’Âµ(i)]2 .
A A B B
The rationale behind Eq. 4 can be elucidated as follows. BN layers are in-
troduced to alleviate the issue of internal covariate shift [4,33,65], where the
means and variances of inputs undergo changes as they pass through internal
learnable layers. In this context, our fundamental consideration is that subse-
quent learnable layers anticipate the output of the merged BN layer to follow a
normal distribution. Since the resulting BN layer hold the the inductive bias of
inputs conforming to a Gaussian prior, we estimate Âµ(i) and [Ïƒ(i)]2 from what
we get from Î“ and Î“ . As depicted in Figure 5, we are provided with two sets
A B
of means and variances of data points sampled from this Gaussian prior, along
with the sizes of these sets. We leverage these values collectively to estimate the
parameters of this distribution.
Extending to more domains. When extending the merging method to
m(m â‰¥ 2) Gaussian distributions, the number of tracked batches n(i), the10 W. Li et al.
weighted average of the means Âµ(i) and the weighted average of the variances
can be calculated as follows.
n(i) =n(i)+n(i)+Â·Â·Â·+n(i),
1 2 M
1
Âµ(i) = (n(i)Âµ(i)+n(i)Âµ(i)+Â·Â·Â·+n(i)Âµ(i)),
n(i) 1 1 2 2 M M (5)
(cid:80)M n(i)(Ïƒi)2+(cid:80)M ni(Âµi âˆ’Âµi)2
Ïƒ2 = j=1 j j=1 j j .
(cid:80)M ni
j=1 j
4 Experiments
4.1 Datasets
In the context of multi-target domain adaptation experiments, we employ GTA
[60] and SYNTHIA [61] as the synthetic dataset and the real-world datasets of
Cityscapes [9], Indian Driving Dataset [73] (IDD), ACDC [64] and DarkZurich
[63] as the target domains. For domain adaptation methods, training involves
labeledsourcedataandunlabeledtargetdatafromdifferentdomains.Weemploy
ourmergingtechniquestoconstructamodelfromthetrainedmodels,employing
thediscussedmergingmethods,allwithouttheneedfordirectaccesstothisdata.
Synthetic Datasets. GTA[60]datasetcomprises24,966syntheticimages,
each with a resolution of 1914Ã—1052 pixels. These images are sourced from the
videogameGTA5andcomeequippedwithpixel-levelannotationsencompassing
19 categories, aligning with the annotation protocols of Cityscapes and IDD
datasets. SYNTHIA [61] dataset comprises 9,400 rendered images, each with a
resolution of 1280Ã—760 pixels, generated from a virtual city.
Real-worldDatasets. Cityscapes [9]isareal-worlddatasetfeaturingacol-
lection of 5,000 street scenes captured in various European cities. These scenes
havebeenmeticulouslylabeled,classifyingobjectsandelementsinto19distinct
categories. In our experiments, we utilize a subset of this dataset, consisting of
2,975 images for training and an additional 500 images for validation purposes.
IDD [73]offersgreaterdiversitycomparedtoCityscapes,capturingunstructured
traffic scenarios on Indian roads. It boasts a total of 10,003 images, with 6,993
designated for training, 981 for validation, and 2,029 for testing. ACDC [64]
contains 1600 training, 406 validation, and 2000 test images, evenly distributed
among adverse weather conditions, including fog, night, rain and snow. Dark-
Zurich [63]contains2416training,50validation,and151testimagesspecifically
curated for nighttime scenarios.
4.2 Implementation Details
Ourproposedmodelmergingtechniquesareeasytoimplement.Inthefirstphase
of training single-target domain adaptation model, we leverage the state-of-the-
art HRDA method [29]. We validate the effectiveness of our approach using aTraining-Free Model Merging for Multi-target Domain Adaptation 11
range of image encoders as backbones, including ResNet50 [24], ResNet101 [24],
and MiT-B5 [80], all pre-trained on ImageNet-1K [10]. Except for the backbone
encoder and the factor under examination in Fig. 4, all other training hyper-
parameters remain consistent with the original HRDA [29] implementation. In
the second phase of model merging, we work directly with the state dictionaries
of checkpoint files. For parameters, we perform a mid-point merging, and for
buffers, we apply the formula presented in Eq. 4. When examining linear mode
connectivity in Fig. 4, we evenly sample values of Î» within the range of [0, 1] in
Eq. 1, using a stride of 0.1 and including both endpoints.
Table 1: Performance Comparison of Our Method and Baselines. ThemIoU
(meanIntersection-over-Union)representstheaverageIoUacross19categories.â€˜Enc.â€™
denotes the encoder architecture, with â€˜Râ€™ representing ResNet101 and â€˜Vâ€™ indicat-
ing MiT-B5. The â€˜Metricâ€™ column specifies whether evaluation was conducted on the
Cityscapes (â€˜Câ€™) or IDD (â€˜Iâ€™) dataset. The harmonic mean (â€˜Hâ€™), representing adap-
tation ability across the two domains, is considered as the primary metric. Bold text
highlightsthebestharmonicmeanresults,whileunderlinedtextindicatesthesecond-
bestresults.â€˜â€ â€™signifiesonlymergingbackboneswhilekeepingseparatedecodeheads.
Method Enc. Metric road sidewalk building wall fence pole light sign veg. terrain sky person rider car truck bus train motor bike mIoU
C 95.3 67.5 87.7 25.8 17.3 50.6 51.8 58.9 90.2 42.8 92.9 75.2 40.4 91.9 54.5 61.6 3.9 56.0 62.7 59.3
DataComb. R I 95.8 22.3 73.0 29.2 12.8 34.2 26.0 54.6 82.2 36.2 95.3 67.0 64.4 80.3 70.2 60.9 0.0 74.8 34.1 53.3
H 95.6 33.5 79.7 27.4 14.7 40.8 34.6 56.6 86.0 39.2 94.1 70.9 49.6 85.7 61.3 61.2 0.0 64.0 44.2 56.2
C 95.9 74.4 88.9 32.4 34.0 54.6 61.2 71.1 90.3 45.1 88.9 77.8 51.2 91.3 49.6 62.8 0.0 46.5 60.5 61.9
(GTS ATD â†’A CS) R I 90.7 30.6 64.8 14.6 17.5 28.9 17.9 18.8 80.1 17.5 92.3 62.3 51.6 74.6 40.2 36.1 0.0 67.7 35.2 44.3
H 93.3 43.3 75.0 20.2 23.1 37.8 27.7 29.8 84.9 25.2 90.6 69.2 51.4 82.1 44.4 45.9 0.0 55.1 44.5 51.6
C 81.4 29.1 79.0 22.1 30.4 41.7 45.0 42.4 88.9 39.0 92.0 72.6 43.1 80.3 31.1 55.3 7.9 22.4 34.4 49.4
(GTAST â†’DA IDD) R I 96.0 34.7 75.0 33.3 22.6 33.6 24.2 54.0 84.9 39.8 95.8 69.0 66.0 80.2 69.4 63.7 0.0 76.0 28.9 55.1
H 88.1 31.6 76.9 26.6 25.9 37.2 31.5 47.5 86.8 39.4 93.8 70.8 52.1 80.2 43.0 59.2 0.0 34.6 31.4 52.1
C 91.8 60.0 86.3 39.3 31.0 47.6 53.9 53.6 89.8 46.4 91.8 75.5 48.6 89.9 54.1 60.5 25.2 31.6 39.9 58.8
Ours R I 96.4 40.0 73.4 34.9 23.3 31.7 27.9 53.6 83.7 44.8 94.5 67.8 64.1 77.6 57.6 51.3 0.0 72.8 30.0 54.0
H 94.0 48.0 79.3 37.0 26.6 38.1 36.8 53.6 86.6 45.6 93.2 71.4 55.2 83.3 55.8 55.5 0.0 44.0 34.3 56.3(â†‘4.2)
C 92.0 59.6 87.9 39.1 35.7 50.7 59.6 60.8 90.1 45.4 91.3 77.4 48.8 88.4 52.6 59.6 20.3 33.5 50.6 60.2
Oursâ€  R I 96.9 41.1 74.0 34.6 25.1 34.5 29.0 50.6 85.8 48.1 95.8 69.6 65.3 79.3 61.8 55.2 0.0 74.4 33.3 55.5
H 94.4 48.7 80.4 36.7 29.5 41.1 39.0 55.2 87.9 46.7 93.5 73.3 55.9 83.6 56.8 57.3 0.0 46.1 40.2 57.7(â†‘5.6)
C 88.9 49.7 90.2 58.8 46.6 50.9 60.9 60.3 89.7 47.4 88.8 77.7 45.0 93.5 77.7 79.4 69.0 57.6 65.8 68.3
DataComb. V I 94.0 32.2 77.9 44.6 30.3 42.6 33.2 53.0 80.6 24.8 91.2 71.0 67.2 81.2 73.4 66.1 0.0 75.6 24.6 56.0
H 91.4 39.1 83.6 50.7 36.7 46.4 43.0 56.4 84.9 32.6 90.0 74.2 53.9 86.9 75.5 72.1 0.0 65.3 35.8 61.5
C 96.4 74.2 91.0 59.4 53.5 58.0 64.9 69.4 91.6 49.9 93.8 79.2 53.7 93.4 75.2 76.3 67.6 64.3 67.1 72.6
(GTS ATD â†’A CS) V I 85.8 10.0 72.1 30.7 27.4 34.0 32.9 53.8 79.9 36.4 95.2 65.2 54.2 80.2 47.2 48.8 0.0 72.4 34.9 50.6
H 90.8 17.6 80.4 40.5 36.2 42.9 43.6 60.6 85.3 42.0 94.5 71.5 54.0 86.3 58.0 59.5 0.0 68.1 45.9 59.6
C 87.2 30.0 88.9 53.1 35.3 52.0 56.6 49.1 89.7 47.3 88.4 74.9 40.0 91.1 76.7 64.2 29.5 15.2 30.5 57.9
(GTAST â†’DA IDD) V I 93.0 4.7 78.0 42.0 24.8 44.3 25.9 59.8 79.7 24.1 91.0 62.7 59.6 78.8 71.8 75.6 0.0 63.1 16.7 52.4
H 90.0 8.2 83.1 46.9 29.1 47.8 35.5 53.9 84.4 31.9 89.7 68.3 47.9 84.5 74.2 69.4 0.0 24.5 21.6 55.0
C 93.6 57.8 89.9 58.5 41.8 55.5 58.8 56.5 90.8 52.2 92.0 77.5 46.3 93.3 76.6 75.7 53.1 44.9 57.4 67.0
Ours V I 93.6 18.9 76.1 35.2 29.4 38.7 32.7 58.1 82.2 41.2 93.9 72.5 63.3 81.0 63.7 66.6 0.0 75.2 34.9 55.6
H 93.6 28.5 82.4 44.0 34.5 45.6 42.0 57.3 86.3 46.0 92.9 74.9 53.5 86.7 69.6 70.9 0.0 56.2 43.4 60.8(â†‘1.2)
C 94.1 60.9 90.6 59.5 46.9 56.8 63.7 62.8 91.4 52.3 93.6 78.5 50.6 93.3 76.8 79.4 67.9 58.7 66.3 70.7
Oursâ€  V I 93.0 15.7 77.0 36.6 29.2 41.1 35.8 62.1 80.2 36.7 92.3 70.6 62.7 81.9 65.3 69.5 0.0 71.6 29.2 55.3
H 93.5 24.9 83.2 45.3 36.0 47.7 45.8 62.4 85.4 43.1 93.0 74.3 56.0 87.2 70.6 74.1 0.0 64.5 40.5 62.1(â†‘2.5)
4.3 Comparison with Baseline Methods
Wepresentacomparisonofourmodelmerging-basedmulti-targetdomainadap-
tation approach with several baseline methods in Tab. 1. In this experiment, we
evaluate our method using GTA [60] as the source domain and two target do-
mains, Cityscapes [9] and IDD [73]. However, our approach can easily scale to
handle a greater number of target domains, should the need arise. Additionally,12 W. Li et al.
we assess the performance when SYNTHIA [61] serves as the source domain,
and the results for this scenario are presented in the supplementary material.
Baseline methodsincludeDataCombination("DataComb.")approaches,
where a single domain adaptation model is trained on a mixture of data from
two target domains. Note that these are only presented for reference as they
contradict our considerations related to data transfer bandwidth and privacy
issues. We also include Single-Target Domain Adaptation ("STDA") baselines,
which involve training a single domain adaptation model for one domain and
assessingitsgeneralizationtobothdomains.Weevaluateourproposedmethods
(labeled as "Ours") which involve merging all models or merging only image
backbones while maintaining separate decoding heads.
Results obtained using convolutional-
based encoder architecture ResNet101 [24] Table 2: Comparison of Our
and transformer-based architecture MiT-B5 Method with State-of-the-Art
[80] are presented in Tab. 1. Our method Approaches.PriorMTDAmeth-
ods used different training meth-
demonstrates a notable improvement of
ods from ours, only for reference.
+4.2% and +1.2% in harmonic mean when
â€˜â€ â€™ signifies results reproduced by
applied to the ResNet101 [24] and MiT-
us.
B5 [80] backbones, respectively, compared to
Metric
the strongest single-target domain adapta- Setting Method Backbone CS IDD H.Mean
BDL[46] R101 41.1 - -
tionmodel.Notably,thislevelofperformance
AdaptSeg[71] R101 42.4 - -
(56.3% harmonic mean with ResNet101 [24]) CLAN[49] R101 43.2 - -
ADVENT[75] R101 43.8 - -
isalreadyonparwithdatacombinationmeth- MaxSquare[6] R101 44.3 - -
AdaptPatch[72] R101 44.9 - -
ods (56.2% harmonic mean), and we achieve CBST[92] R38 45.9 - -
STDA IntraDA[56] R101 46.3 - -
it without requiring access to any training (GTAâ†’X) DACS[70] R101 52.1 - -
DAFormer[28] R101 56.0 - -
data.Furthermore,weexploreamorerelaxed CorDA[77] R101 56.6 - -
ProDA[87] R101 57.5 - -
setting where only the encoder backbone is HRDAâ€ [29] R101 61.9 - -
DDB[5] R101 62.7 - -
merged while decoding heads are separated DAFormer[28] MiT-B5 68.3 - -
for various downstream domains. This is a HRDAâ€ [29] MiT-B5 72.6 - -
Yueetal.[84] R101 42.1 42.8 42.4
DG
feasible approach as the parameters of image Kunduetal.[39] R101 53.4 - -
MTDA-ITA[20] R101 40.3 41.2 40.8
backbones are typically orders of magnitude MTDA MT-MTDA[55] R101 43.2 44.0 43.6
CCL[34] R101 45.0 46.0 45.5
larger than those of decoding heads. Remark- Coast[88] R101 47.1 49.3 48.2
MTDA Ours R101 58.8 54.0 56.3(â†‘8.1)
ably, this configuration results in a substan- (Merging) Ours MiT-B5 67.0 55.6 60.8(â†‘12.6)
tialperformanceimprovementof +5.6%and+2.5%inharmonicmeanfortwo
backbones, respectively. We also find that our merging-based method consis-
tently achieves the best harmonic means across most categories, indicating its
ability to enhance adaptation globally instead of biasing to certain categories.
4.4 Comparison with State-of-the-Arts
We begin by comparing our method with the single-target domain adaptation
(STDA) on the GTAâ†’Cityscapes task, as shown in Tab. 2. It is worthy to note
thatourmethodcanbeappliedtoanyofthesemethods,providedtheyadaptto
differentdomainsusingthesamepretrainedweights.Thisallowsustogeneralize
to all target domains using a single model while keeping the relatively superior
performance of STDA methods. We also compare our methods with domainTraining-Free Model Merging for Multi-target Domain Adaptation 13
generalization approaches in Tab. 2, which aim to generalize a model trained on
a source domain to multiple unseen target domains. Our approach stands out
by achieving superior performance without requiring additional tricks, just by
means of exploiting parameter space mode connectivity.
In the realm of multi-target domain adaptation, our method also stands
out. We eliminate the need for explicit inter-domain consistency regularization
or knowledge distillation of multiple student models, enabling techniques from
STDAmethodslikemulti-resolutiontrainingtotransfertoMTDAtasks.There-
fore, we witness a significant improvement over the best published results of
MTDA, while eliminating the need of access to training data.
4.5 Extending to More Target Domains
In this section, we expand the application of
our model merging technique to encompass Table 3: Application of Our
four distinct target domains: Cityscapes [9], Model Merging Techniques
IDD [73], ACDC [64], and DarkZurich [63]. Across Four Target Domains.
The datasets Cityscapes [9], IDD
Each of these domains presents unique chal-
[73], ACDC [64], and DarkZurich
lenges and characteristics: Cityscapes [9] cap-
[63]arerepresentedbyâ€˜Câ€™,â€˜Iâ€™,â€˜Aâ€™,
tures European urban settings, IDD [73] fo-
and â€˜Dâ€™, respectively. The mIoU
cuses on Indian road scenes, ACDC [64] is
of each dataset and the harmonic
tailoredtoadverseweatherconditionssuchas
mean (H) is reported.
fog, rain, or snow, and DarkZurich [63] ad-
Mergingof Metric
dresses night road scenes. We conduct a thor- Model# C I A D C I A D H âˆ†
1 âœ“ 61.3 47.0 42.1 16.1 32.4 -1.0%
ough evaluation of models that are trained 2 âœ“ 51.2 55.4 37.6 16.2 31.8 -1.7%
3 âœ“ 44.9 38.5 42.4 18.6 31.9 -1.6%
separately for each domain as well as mod-
4 âœ“ 43.6 38.2 41.6 21.6 33.5 -
els created through the merging of these in- 5 âœ“ âœ“ 58.8 54.0 41.9 17.4 34.2 +0.8%
6 âœ“ âœ“ 55.1 45.2 45.2 20.8 36.2 +2.8%
dividually adapted models. The effectiveness 7 âœ“ âœ“ 57.5 45.2 45.8 24.2 38.9 +5.5%
8 âœ“ âœ“ 47.1 49.7 43.6 21.5 36.1 +2.7%
of these approaches is quantified by report- 9 âœ“ âœ“ 50.0 49.6 46.2 25.0 39.3 +5.8%
10 âœ“ âœ“ 45.5 41.3 45.3 22.0 35.2 +1.7%
ing the harmonic mean of their performance 11 âœ“ âœ“ âœ“ 53.0 49.3 44.5 20.0 35.9 +2.4%
12 âœ“ âœ“ âœ“ 55.1 49.3 45.4 21.1 37.1 +3.6%
across these diverse domains. All results are 13 âœ“ âœ“ âœ“ 51.9 43.8 46.3 22.1 36.7 +3.3%
14 âœ“ âœ“ âœ“ 47.6 46.7 46.1 22.5 36.8 +3.4%
presented in Tab. 3.
15 âœ“ âœ“ âœ“ âœ“ 51.6 47.3 45.8 21.1 36.4 +3.0%
Our proposed model merging techniques
demonstrate a significant improvement in performance, as illustrated in Tab. 3.
While we use the method with the highest harmonic mean from separately
trainedmodelsasourbaselineforcomparison,alltheapproachesbasedonmodel
mergingoutperformit,withgainsassubstantialas+5.8%.Furthermore,despite
theincreasingcomplexityinmergingmodelsfrommultiple,diversedomains,we
observe that the overall performance across all domains does not suffer any no-
table decline.
Through further analysis, we reveal that our approach is capable of simpli-
fying domain consistency complexity. While existing methods like [38,59] in-
volve O(n2) considerations for inter-domain consistency regularization and on-
line knowledge distillation, our approach reduces this to a more efficient O(n),
where n represents the number of target domains considered.14 W. Li et al.
As shown in supplementary material, we have also included Mapillary as a
target domain and compared it previous work [43].
4.6 Ablation Study
Weight Merging and Buffer Merging.
We conduct ablation studies on our proposed Table 4: Ablation Study on
parameter merging and buffer merging meth- Different Vision Backbones.
ods using ResNet101 [24] and MiT-B5 [80] as (a) ResNet101Backbone
the image encoders in the segmentation net- Setting M Pae rr ag min sg . M Be ur ffg ein rsg C IMetri Hc âˆ†
work [29], with results reported in Tab. 4(a)
STDA(Gâ†’C) - - 61.9 44.3 51.6 -0.5%
andTab.4(b),respectively.Wehaveobserved STDA(Gâ†’I) - - 49.4 55.1 52.1 -
âœ“ âœ— 58.7 51.3 54.8 +2.7%
variations in the generalization capabilities Ours âœ“ âœ“ 58.8 54.0 56.3 +4.2%
of single-target domain adaptation (STDA) (b) MiT-B5Backbone
m mo ard ie lyls aa rc isr eoss frod miffe tr he ent vad ro ym ina gin ds, ivw erh si ic tyh ap nri d- Setting M Pae rr ag min sg . M Be ur ffg ein rsg C IMetri Hc âˆ†
STDA(Gâ†’C) - - 72.6 50.6 59.6 -
quality of the target datasets used. Nonethe- STDA(Gâ†’I) - - 57.9 52.4 55.0 -4.6%
âœ“ âœ— 66.6 54.9 60.2 +0.6%
less,weselectthehigherharmonicmeansfrom Ours
âœ“ âœ“ 67.0 55.6 60.8 +1.2%
STDA models as our baseline for comparison.
The data in Tab. 4(a) and Tab. 4(b) reveal that employing a straightforward
mid-pointmergingapproachforparametersleadstoanincreaseingeneralization
abilityby+2.7%and+0.6%.Furthermore,whenbuffermergingisincorporated,
this enhancement in performance is further amplified to +4.2% and +1.2%. We
also observe an intriguing phenomenon with the MiT-B5 backbone: the merged
modeloutperformsthesingle-targetadaptedmodelwhenevaluatingintheIDD
domain. This finding implies that domain-invariant knowledge can be acquired
fromotherdomains.Theseresultssuggestthateachpartofourproposedmodel
merging technique is effective.
Different STDA Methods & Tasks. To validate the versatility of our
proposed model merging method, we conducted experiments on another STDA
MethodADVENT[75].Wealsoapplyourmethodsonimageclassificationtasks.
Details are in the supplementary material.
5 Conclusion
This paper introduces a novel model merging strategy aimed at addressing the
multi-target domain adaptation (MTDA) challenge without relying on training
data.Ourfindingsrevealthatwhenpretrainedonextensivedatasets,bothdeep
convolutional neural networks and transformer-based vision models can confine
thefinetunedmodelswiththesamebasininthelosslandscape.Wealsoempha-
size the significance of buffer merging in MTDA, as buffers are key to capturing
theuniquefeaturesofvariousdomains.Themethodsweproposearestraightfor-
wardyethighlyeffective,establishingnewstate-of-the-artresultsontheMTDA
benchmark.Weanticipatethattheconceptsandmethodologiespresentedinthis
paper will inspire future explorations in this field.Training-Free Model Merging for Multi-target Domain Adaptation 15
Acknowledgements
ThisresearchissupportedbyTsinghuaUniversityâ€“MercedesBenzInstitutefor
Sustainable Mobility.
References
1. Ainsworth, S.K., Hayase, J., Srinivasa, S.: Git re-basin: Merging models modulo
permutation symmetries. arXiv preprint arXiv:2209.04836 (2022)
2. Araslanov,N.,Roth,S.:Self-supervisedaugmentationconsistencyforadaptingse-
manticsegmentation.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 15384â€“15394 (2021)
3. Ashmore,S.,Gashler,M.:Amethodforfindingsimilaritybetweenmulti-layerper-
ceptrons by forward bipartite alignment. In: 2015 International Joint Conference
on Neural Networks (IJCNN). pp. 1â€“7. IEEE (2015)
4. Bjorck,N.,Gomes,C.P.,Selman,B.,Weinberger,K.Q.:Understandingbatchnor-
malization. Advances in neural information processing systems 31 (2018)
5. Chen, L., Wei, Z., Jin, X., Chen, H., Zheng, M., Chen, K., Jin, Y.: Deliberated
domain bridging for domain adaptive semantic segmentation. Advances in Neural
Information Processing Systems 35, 15105â€“15118 (2022)
6. Chen, M., Xue, H., Cai, D.: Domain adaptation for semantic segmentation with
maximumsquaresloss.In:ProceedingsoftheIEEE/CVFInternationalConference
on Computer Vision. pp. 2090â€“2099 (2019)
7. Chen, M., Chen, J., Ye, X., Gao, H.a., Chen, X., Fan, Z., Zhao, H.: Ultraman:
Singleimage3dhumanreconstructionwithultraspeedanddetail.arXivpreprint
arXiv:2403.12028 (2024)
8. Choi,J.,Kim,T.,Kim,C.:Self-ensemblingwithgan-baseddataaugmentationfor
domain adaptation in semantic segmentation. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 6830â€“6840 (2019)
9. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene
understanding. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 3213â€“3223 (2016)
10. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248â€“255. Ieee (2009)
11. Entezari, R., Sedghi, H., Saukh, O., Neyshabur, B.: The role of permuta-
tion invariance in linear mode connectivity of neural networks. arXiv preprint
arXiv:2110.06296 (2021)
12. Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable
neural networks. arXiv preprint arXiv:1803.03635 (2018)
13. Frankle, J., Dziugaite, G.K., Roy, D., Carbin, M.: Linear mode connectivity and
the lottery ticket hypothesis. In: International Conference on Machine Learning.
pp. 3259â€“3269. PMLR (2020)
14. Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation.
In: International conference on machine learning. pp. 1180â€“1189. PMLR (2015)
15. Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F.,
Marchand,M.,Lempitsky,V.:Domain-adversarialtrainingofneuralnetworks.The
journal of machine learning research 17(1), 2096â€“2030 (2016)16 W. Li et al.
16. Gao, H.a., Gao, M., Li, J., Li, W., Zhi, R., Tang, H., Zhao, H.: Scp-diff:
Photo-realisticsemanticimagesynthesiswithspatial-categoricaljointprior.arXiv
preprint arXiv:2403.09638 (2024)
17. Gao,H.a.,Tian,B.,Li,P.,Chen,X.,Zhao,H.,Zhou,G.,Chen,Y.,Zha,H.:From
semi-supervisedtoomni-supervisedroomlayoutestimationusingpointclouds.In:
2023 IEEE International Conference on Robotics and Automation (ICRA). pp.
2803â€“2810. IEEE (2023)
18. Gao, H.a., Tian, B., Li, P., Zhao, H., Zhou, G.: Dqs3d: Densely-matched
quantization-aware semi-supervised 3d detection. In: Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision.pp.21905â€“21915(2023)
19. Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.P., Wilson, A.G.: Loss sur-
faces, mode connectivity, and fast ensembling of dnns. Advances in neural infor-
mation processing systems 31 (2018)
20. Gholami, B., Sahu, P., Rudovic, O., Bousmalis, K., Pavlovic, V.: Unsupervised
multi-targetdomainadaptation:Aninformationtheoreticapproach.IEEETrans-
actions on Image Processing 29, 3993â€“4002 (2020)
21. Gong, R., Li, W., Chen, Y., Gool, L.V.: Dlow: Domain flow for adaptation and
generalization. In: Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. pp. 2477â€“2486 (2019)
22. Guan, T., Shen, W., Yang, X., Wang, X., Yang, X.: Bridging synthetic and real
worldsforpre-trainingscenetextdetectors.arXivpreprintarXiv:2312.05286(2023)
23. He, B., Ding, N., Qian, C., Deng, J., Cui, G., Yuan, L., Gao, H.a., Chen, H.,
Liu,Z.,Sun,M.:Zero-shotgeneralizationduringinstructiontuning:Insightsfrom
similarity and granularity. arXiv preprint arXiv:2406.11721 (2024)
24. He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 770â€“778 (2016)
25. Hoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A., Dar-
rell,T.:Cycada:Cycle-consistentadversarialdomainadaptation.In:International
conference on machine learning. pp. 1989â€“1998. Pmlr (2018)
26. Hoffman,J.,Wang,D.,Yu,F.,Darrell,T.:Fcnsinthewild:Pixel-leveladversarial
and constraint-based adaptation. arXiv preprint arXiv:1612.02649 (2016)
27. Horvath,S.,Laskaridis,S.,Almeida,M.,Leontiadis,I.,Venieris,S.,Lane,N.:Fjord:
Fair and accurate federated learning under heterogeneous targets with ordered
dropout. Advances in Neural Information Processing Systems 34, 12876â€“12889
(2021)
28. Hoyer, L., Dai, D., Van Gool, L.: Daformer: Improving network architectures and
training strategies for domain-adaptive semantic segmentation. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
9924â€“9935 (2022)
29. Hoyer, L., Dai, D., Van Gool, L.: Hrda: Context-aware high-resolution domain-
adaptive semantic segmentation. In: European Conference on Computer Vision.
pp. 372â€“391. Springer (2022)
30. Hoyer, L., Dai, D., Wang, H., Van Gool, L.: Mic: Masked image consistency for
context-enhanced domain adaptation. In: Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. pp. 11721â€“11732 (2023)
31. Hoyer, L., Dai, D., Wang, Q., Chen, Y., Van Gool, L.: Improving semi-supervised
anddomain-adaptivesemanticsegmentationwithself-superviseddepthestimation.
International Journal of Computer Vision pp. 1â€“27 (2023)
32. Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J.E., Weinberger, K.Q.: Snapshot
ensembles: Train 1, get m for free. arXiv preprint arXiv:1704.00109 (2017)Training-Free Model Merging for Multi-target Domain Adaptation 17
33. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by
reducinginternalcovariateshift.In:Internationalconferenceonmachinelearning.
pp. 448â€“456. pmlr (2015)
34. Isobe,T.,Jia,X.,Chen,S.,He,J.,Shi,Y.,Liu,J.,Lu,H.,Wang,S.:Multi-target
domainadaptation withcollaborativeconsistency learning.In:Proceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.8187â€“8196
(2021)
35. Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., Wilson, A.G.: Aver-
aging weights leads to wider optima and better generalization. arXiv preprint
arXiv:1803.05407 (2018)
36. Jordan, K., Sedghi, H., Saukh, O., Entezari, R., Neyshabur, B.: Repair:
Renormalizing permuted activations for interpolation repair. arXiv preprint
arXiv:2211.08403 (2022)
37. Kim, M., Byun, H.: Learning texture invariant representation for domain adapta-
tion of semantic segmentation. In: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition. pp. 12975â€“12984 (2020)
38. Koh, K.B., Fernando, B.: Consistency regularization for domain adaptation. In:
European Conference on Computer Vision. pp. 347â€“359. Springer (2022)
39. Kundu, J.N., Kulkarni, A., Singh, A., Jampani, V., Babu, R.V.: Generalize then
adapt:Source-freedomainadaptivesemanticsegmentation.In:Proceedingsofthe
IEEE/CVF International Conference on Computer Vision. pp. 7046â€“7056 (2021)
40. Kundu, J.N., Venkatesh, R.M., Venkat, N., Revanur, A., Babu, R.V.: Class-
incrementaldomainadaptation.In:ComputerVisionâ€“ECCV2020:16thEuropean
Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part XIII 16. pp.
53â€“69 (2020)
41. Lai, X., Tian, Z., Jiang, L., Liu, S., Zhao, H., Wang, L., Jia, J.: Semi-supervised
semanticsegmentationwithdirectionalcontext-awareconsistency.In:Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
1205â€“1214 (2021)
42. Lee, D.H., et al.: Pseudo-label: The simple and efficient semi-supervised learning
method for deep neural networks. In: Workshop on challenges in representation
learning, ICML. vol. 3, p. 896. Atlanta (2013)
43. Lee, S., Choi, W., Kim, C., Choi, M., Im, S.: Adas: A direct adaptation strategy
for multi-target domain adaptive semantic segmentation. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.19196â€“
19206 (2022)
44. Li, T., Sahu, A.K., Zaheer, M., Sanjabi, M., Talwalkar, A., Smith, V.: Federated
optimizationinheterogeneousnetworks.ProceedingsofMachinelearningandsys-
tems 2, 429â€“450 (2020)
45. Li,W.,Xu,H.,Zhang,G.,Gao,H.a.,Gao,M.,Wang,M.,Zhao,H.:Fairdiff:Fair
segmentation with point-image diffusion. arXiv preprint arXiv:2407.06250 (2024)
46. Li, Y., Yuan, L., Vasconcelos, N.: Bidirectional learning for domain adaptation of
semantic segmentation. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition. pp. 6936â€“6945 (2019)
47. Long, M., Cao, Z., Wang, J., Jordan, M.I.: Conditional adversarial domain adap-
tation. Advances in neural information processing systems 31 (2018)
48. Long,X.,Zheng,Y.,Zheng,Y.,Tian,B.,Lin,C.,Liu,L.,Zhao,H.,Zhou,G.,Wang,
W.: Adaptive surface normal constraint for geometric estimation from monocular
images. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024)18 W. Li et al.
49. Luo, Y., Zheng, L., Guan, T., Yu, J., Yang, Y.: Taking a closer look at domain
shift: Category-level adversaries for semantics consistent domain adaptation. In:
Proceedings of the IEEE/CVF conference on computer vision and pattern recog-
nition. pp. 2507â€“2516 (2019)
50. McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.:
Communication-efficient learning of deep networks from decentralized data. In:
Artificial intelligence and statistics. pp. 1273â€“1282. PMLR (2017)
51. Melas-Kyriazi, L., Manrai, A.K.: Pixmatch: Unsupervised domain adaptation via
pixelwise consistency training. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 12435â€“12445 (2021)
52. Mirzadeh, S.I., Farajtabar, M., Gorur, D., Pascanu, R., Ghasemzadeh, H.:
Linear mode connectivity in multitask and continual learning. arXiv preprint
arXiv:2010.04495 (2020)
53. Mishchenko, K., Malinovsky, G., Stich, S., RichtÃ¡rik, P.: Proxskip: Yes! local gra-
dient steps provably lead to communication acceleration! finally! In: International
Conference on Machine Learning. pp. 15750â€“15769. PMLR (2022)
54. Neyshabur,B.,Sedghi,H.,Zhang,C.:Whatisbeingtransferredintransferlearn-
ing? Advances in neural information processing systems 33, 512â€“523 (2020)
55. Nguyen-Meidine, L.T., Belal, A., Kiran, M., Dolz, J., Blais-Morin, L.A., Granger,
E.: Unsupervised multi-target domain adaptation through knowledge distillation.
In:ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputer
Vision. pp. 1339â€“1347 (2021)
56. Pan, F., Shin, I., Rameau, F., Lee, S., Kweon, I.S.: Unsupervised intra-domain
adaptation for semantic segmentation through self-supervision. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
3764â€“3773 (2020)
57. PeÃ±a, F.A.G., Medeiros, H.R., Dubail, T., Aminbeidokhti, M., Granger, E., Ped-
ersoli, M.: Re-basin via implicit sinkhorn differentiation. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.20237â€“
20246 (2023)
58. Qin, Y., Qian, C., Yi, J., Chen, W., Lin, Y., Han, X., Liu, Z., Sun, M., Zhou,
J.: Exploring mode connectivity for pre-trained language models. arXiv preprint
arXiv:2210.14102 (2022)
59. Reddy,N.,Baktashmotlagh,M.,Arora,C.:Towardsdomain-awareknowledgedis-
tillation for continual model generalization. In: Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision. pp. 696â€“707 (2024)
60. Richter,S.R.,Vineet,V.,Roth,S.,Koltun,V.:Playingfordata:Groundtruthfrom
computer games. In: Computer Visionâ€“ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14. pp.
102â€“118. Springer (2016)
61. Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia
dataset:Alargecollectionofsyntheticimagesforsemanticsegmentationofurban
scenes. In: Proceedings of the IEEE conference on computer vision and pattern
recognition. pp. 3234â€“3243 (2016)
62. Sajjadi,M.,Javanmardi,M.,Tasdizen,T.:Regularizationwithstochastictransfor-
mations and perturbations for deep semi-supervised learning. Advances in neural
information processing systems 29 (2016)
63. Sakaridis, C., Dai, D., Van Gool, L.: Map-guided curriculum domain adapta-
tionanduncertainty-awareevaluationforsemanticnighttimeimagesegmentation.
IEEETransactionsonPatternAnalysisandMachineIntelligence44(6),3139â€“3153
(2020)Training-Free Model Merging for Multi-target Domain Adaptation 19
64. Sakaridis, C., Dai, D., Van Gool, L.: Acdc: The adverse conditions dataset with
correspondences for semantic driving scene understanding. In: Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision.pp.10765â€“10775(2021)
65. Santurkar,S.,Tsipras,D.,Ilyas,A.,Madry,A.:Howdoesbatchnormalizationhelp
optimization? Advances in neural information processing systems 31 (2018)
66. Tarvainen,A.,Valpola,H.:Meanteachersarebetterrolemodels:Weight-averaged
consistency targets improve semi-supervised deep learning results. Advances in
neural information processing systems 30 (2017)
67. Tatro,N.,Chen,P.Y.,Das,P.,Melnyk,I.,Sattigeri,P.,Lai,R.:Optimizingmode
connectivity via neuron alignment. Advances in Neural Information Processing
Systems 33, 15300â€“15311 (2020)
68. Tian,B.,Gao,H.a.,Cui,L.,Zheng,Y.,Luo,L.,Wang,B.,Zhi,R.,Zhou,G.,Zhao,
H.: Latency-aware road anomaly segmentation in videos: A photorealistic dataset
and new metrics. arXiv preprint arXiv:2401.04942 (2024)
69. Tian, B., Liu, M., Gao, H.a., Li, P., Zhao, H., Zhou, G.: Unsupervised road
anomaly detection with language anchors. In: 2023 IEEE international conference
on robotics and automation (ICRA). pp. 7778â€“7785. IEEE (2023)
70. Tranheden, W., Olsson, V., Pinto, J., Svensson, L.: Dacs: Domain adaptation via
cross-domain mixed sampling. In: Proceedings of the IEEE/CVF Winter Confer-
ence on Applications of Computer Vision. pp. 1379â€“1389 (2021)
71. Tsai, Y.H., Hung, W.C., Schulter, S., Sohn, K., Yang, M.H., Chandraker, M.:
Learning to adapt structured output space for semantic segmentation. In: Pro-
ceedings of the IEEE conference on computer vision and pattern recognition. pp.
7472â€“7481 (2018)
72. Tsai, Y.H., Sohn, K., Schulter, S., Chandraker, M.: Domain adaptation for struc-
tured output via discriminative patch representations. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 1456â€“1465 (2019)
73. Varma, G., Subramanian, A., Namboodiri, A., Chandraker, M., Jawahar, C.: Idd:
Adatasetforexploringproblemsofautonomousnavigationinunconstrainedenvi-
ronments. In: 2019 IEEE Winter Conference on Applications of Computer Vision
(WACV). pp. 1743â€“1751. IEEE (2019)
74. Von Oswald, J., Kobayashi, S., Meulemans, A., Henning, C., Grewe, B.F.,
Sacramento, J.: Neural networks with late-phase weights. arXiv preprint
arXiv:2007.12927 (2020)
75. Vu, T.H., Jain, H., Bucher, M., Cord, M., PÃ©rez, P.: Advent: Adversarial entropy
minimization for domain adaptation in semantic segmentation. In: Proceedings of
theIEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.2517â€“
2526 (2019)
76. Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., Khazaeni, Y.: Federated
learning with matched averaging. arXiv preprint arXiv:2002.06440 (2020)
77. Wang, Q., Dai, D., Hoyer, L., Van Gool, L., Fink, O.: Domain adaptive seman-
tic segmentation with self-supervised depth estimation. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 8515â€“8525 (2021)
78. Wei,Y.,Wang,Z.,Lu,Y.,Xu,C.,Liu,C.,Zhao,H.,Chen,S.,Wang,Y.:Editable
scenesimulationforautonomousdrivingviacollaborativellm-agents.In:Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
pp. 15077â€“15087 (2024)
79. Wortsman, M., Ilharco, G., Kim, J.W., Li, M., Kornblith, S., Roelofs, R., Lopes,
R.G.,Hajishirzi,H.,Farhadi,A.,Namkoong,H.,etal.:Robustfine-tuningofzero-
shot models. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 7959â€“7971 (2022)20 W. Li et al.
80. Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: Segformer:
Simpleandefficientdesignforsemanticsegmentationwithtransformers.Advances
in Neural Information Processing Systems 34, 12077â€“12090 (2021)
81. Yang,J.,An,W.,Wang,S.,Zhu,X.,Yan,C.,Huang,J.:Label-drivenreconstruc-
tionfordomainadaptationinsemanticsegmentation.In:ComputerVisionâ€“ECCV
2020:16thEuropeanConference,Glasgow,UK,August23â€“28,2020,Proceedings,
Part XXVII 16. pp. 480â€“498. Springer (2020)
82. Yao,C.H.,Gong,B.,Qi,H.,Cui,Y.,Zhu,Y.,Yang,M.H.:Federatedmulti-target
domain adaptation. In: Proceedings of the IEEE/CVF Winter Conference on Ap-
plications of Computer Vision. pp. 1424â€“1433 (2022)
83. Yu, H., Hu, M., Chen, S.: Multi-target unsupervised domain adaptation without
exactly shared categories. arXiv preprint arXiv:1809.00852 (2018)
84. Yue, X., Zhang, Y., Zhao, S., Sangiovanni-Vincentelli, A., Keutzer, K., Gong, B.:
Domainrandomizationandpyramidconsistency:Simulation-to-realgeneralization
withoutaccessingtargetdomaindata.In:ProceedingsoftheIEEE/CVFInterna-
tional Conference on Computer Vision. pp. 2100â€“2110 (2019)
85. Yurochkin,M.,Agarwal,M.,Ghosh,S.,Greenewald,K.,Hoang,N.,Khazaeni,Y.:
Bayesian nonparametric federated learning of neural networks. In: International
conference on machine learning. pp. 7252â€“7261. PMLR (2019)
86. Zhang,K.,Sun,Y.,Wang,R.,Li,H.,Hu,X.:Multiplefusionadaptation:Astrong
framework for unsupervised semantic segmentation adaptation. arXiv preprint
arXiv:2112.00295 (2021)
87. Zhang,P.,Zhang,B.,Zhang,T.,Chen,D.,Wang,Y.,Wen,F.:Prototypicalpseudo
label denoising and target structure learning for domain adaptive semantic seg-
mentation. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 12414â€“12424 (2021)
88. Zhang, Y., Roy, S., Lu, H., Ricci, E., LathuiliÃ¨re, S.: Cooperative self-training for
multi-target adaptive semantic segmentation. In: Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision. pp. 5604â€“5613 (2023)
89. Zheng, Y., Zhong, C., Li, P., Gao, H.a., Zheng, Y., Jin, B., Wang, L., Zhao, H.,
Zhou, G., Zhang, Q., et al.: Steps: Joint self-supervised nighttime image enhance-
ment and depth estimation. In: 2023 IEEE International Conference on Robotics
and Automation (ICRA). pp. 4916â€“4923. IEEE (2023)
90. Zheng, Z., Yang, Y.: Rectifying pseudo label learning via uncertainty estimation
for domain adaptive semantic segmentation. International Journal of Computer
Vision 129(4), 1106â€“1120 (2021)
91. Zhou, Q., Feng, Z., Gu, Q., Cheng, G., Lu, X., Shi, J., Ma, L.: Uncertainty-aware
consistency regularization for cross-domain semantic segmentation. Computer Vi-
sion and Image Understanding 221, 103448 (2022)
92. Zou,Y.,Yu,Z.,Kumar,B.,Wang,J.:Unsuperviseddomainadaptationforseman-
tic segmentation via class-balanced self-training. In: Proceedings of the European
conference on computer vision (ECCV). pp. 289â€“305 (2018)Training-Free Model Merging for Multi-target Domain Adaptation 21
A Results of SYNTHIA as Source Domain
In this section, we show more experimental results with SYNTHIA [61] dataset
as the source domain.
Comparison with Baseline Models. Tab. 5 shows the performance com-
parisonwithbaselinemodelsusingSYNTHIA[61]asthesourcedomainandtwo
target domains, Cityscapes [9] and IDD [73]. Please note that the mIoU (mean
Intersection-over-Union)iscalculatedacross13categorieswithinstandardSYN-
THIA [61] evaluation protocol.
Table 5: Performance Comparison of Our Method and Baseline Models
(SYNTHIA â†’ X). The mIoU* (mean Intersection-over-Union) represents the aver-
age IoU across 13 categories. â€˜Enc.â€™ denotes the encoder architecture, with â€˜Râ€™ repre-
senting ResNet101 and â€˜Vâ€™ indicating MiT-B5. The â€˜Metricâ€™ column specifies whether
evaluation was conducted on the Cityscapes (â€˜Câ€™) or IDD (â€˜Iâ€™) dataset. The harmonic
mean (â€˜Hâ€™), representing adaptation ability across the two domains, is considered as
theprimarymetric.Boldtexthighlightsthebestharmonicmeanresults,whileunder-
lined text indicates the second-best results. â€˜â€ â€™ signifies only merging backbones while
keeping separate decode heads.
Method Enc.
Metric road
sidewalk building
light sign veg. sky person rider car bus motor bike
mIoU*
C 87.4 15.2 86.7 55.0 57.8 87.2 92.9 76.9 44.5 85.5 35.7 48.3 63.6 64.4
DataComb. R I 94.4 25.8 67.7 21.2 22.0 83.9 94.7 65.9 58.9 53.8 32.5 70.2 41.1 56.3
H 90.8 19.1 76.0 30.6 31.9 85.5 93.8 71.0 50.7 66.0 34.0 57.2 49.9 60.1
C 76.8 36.8 87.0 62.1 62.1 87.0 90.8 75.9 51.6 85.8 34.0 53.7 62.8 66.6
(SYNTS HT ID AA â†’CS) R I 57.5 4.1 54.1 18.8 35.4 81.3 94.6 55.8 49.7 48.6 23.4 58.7 20.4 46.3
H 65.8 7.3 66.7 28.9 45.1 84.0 92.6 64.3 50.6 62.0 27.7 56.1 30.7 54.7
C 84.5 25.3 83.8 30.6 48.6 86.4 92.4 74.6 32.7 75.8 31.4 18.8 32.3 55.2
(SYNTS HT IADA â†’IDD) R I 93.9 28.9 68.4 7.0 30.7 85.7 96.2 67.1 52.2 49.0 39.4 57.9 31.0 54.4
H 88.9 27.0 75.3 11.4 37.7 86.1 94.3 70.6 40.2 59.5 34.9 28.3 31.6 54.8
C 85.3 45.5 86.5 53.4 60.6 87.7 92.5 76.9 42.8 84.4 30.5 47.4 53.5 65.1
Ours R I 91.7 18.4 67.2 21.7 42.1 85.3 96.3 64.5 55.8 50.1 36.0 62.6 35.5 55.9
H 88.4 26.2 75.6 30.8 49.7 86.5 94.3 70.2 48.4 62.9 33.0 53.9 42.7 60.2(â†‘5.4)
C 82.7 43.0 86.3 58.8 62.2 87.8 92.5 77.7 48.9 84.5 28.3 52.4 61.5 66.7
Oursâ€  R I 93.1 26.9 67.6 20.8 33.6 85.1 96.2 65.6 54.2 49.8 38.9 61.4 35.7 56.1
H 87.6 33.1 75.8 30.7 43.6 86.4 94.3 71.1 51.5 62.6 32.8 56.5 45.2 60.9(â†‘6.1)
C 85.5 40.0 88.6 62.6 58.2 87.4 89.6 74.1 31.5 88.0 55.9 51.9 62.4 67.3
DataComb. R I 75.4 20.3 72.1 25.6 49.1 65.2 91.2 71.5 66.2 54.5 51.3 73.1 41.6 58.2
H 80.1 26.9 79.5 36.4 53.2 74.7 90.4 72.8 42.7 67.3 53.5 60.7 49.9 62.5
C 86.9 52.0 89.6 65.4 58.6 85.4 94.2 79.5 54.3 86.5 54.0 59.4 63.0 71.4
(SYNTS HT ID AA â†’CS) V I 70.0 5.0 66.2 28.1 46.8 85.0 96.2 58.1 47.3 57.6 49.4 66.8 25.5 54.0
H 77.5 9.1 76.1 39.3 52.0 85.2 95.2 67.1 50.5 69.1 51.6 62.9 36.3 61.5
C 77.7 32.6 86.9 41.9 49.5 88.5 87.9 75.3 39.8 87.8 59.8 29.5 45.7 61.8
(SYNTS HT IADA â†’IDD) V I 89.2 36.7 71.6 16.4 56.7 79.9 90.6 73.9 67.4 53.6 64.3 73.8 39.4 62.6
H 83.1 34.5 78.5 23.6 52.9 84.0 89.3 74.6 50.1 66.6 62.0 42.1 42.3 62.2
C 88.2 48.8 88.8 55.9 56.7 88.4 92.4 76.3 43.3 88.8 62.2 55.3 58.3 69.5
Ours V I 88.5 14.1 72.0 28.0 56.8 85.0 94.3 67.0 58.8 54.6 61.5 72.0 36.1 60.7
H 88.3 21.8 79.5 37.3 56.7 86.7 93.3 71.3 49.8 67.6 61.8 62.5 44.6 64.8(â†‘2.6)
C 88.4 49.4 89.2 63.0 57.9 88.8 94.1 78.3 49.6 88.7 60.9 59.0 61.0 71.4
Oursâ€  V I 87.6 14.3 72.1 27.3 56.9 82.4 92.0 70.4 63.6 54.5 63.2 74.5 38.9 61.4
H 88.0 22.2 79.7 38.1 57.4 85.5 93.0 74.2 55.7 67.6 62.0 65.9 47.5 66.0(â†‘3.8)
We can deduce similar conclusions based on the experimental results of the
SYNTHIA [61] dataset in Tab. 5, which show the the broad applicability of our22 W. Li et al.
approach. Our model merging method with SYNTHIA [61] dataset as source
domain demonstrates a notable improvement of +5.4% and +2.6% in har-
monic mean when applied to the ResNet101 [24] and MiT-B5 [80] backbones,
respectively, compared to the strongest single-target domain adaptation model.
Table 6: Comparison of Our Method with State-of-the-Art Approaches
(SYNTHIA â†’ X). We present the performance of various methods in different set-
tings,includingsingle-targetdomainadaptation(STDA),domaingeneralization(DG),
multi-targetdomainadaptationwithaccesstomultipletargetdomainssimultaneously
(MTDA), and MTDA (merging) which stands for our proposed setting. â€˜â€ â€™ signifies
results reproduced by us.
Metric
Setting Method Backbone
CS IDD H.Mean
MaxSquare[6] R101 45.8 - -
AdaptSeg[71] R101 46.7 - -
CLAN[49] R101 47.8 - -
ADVENT[75] R101 47.8 - -
IntraDA[56] R101 48.9 - -
DACS[70] R101 54.8 - -
STDA
CorDA[77] R101 62.0 - -
(SYNTHIAâ†’X)
ProDA[87] R101 62.8 - -
HRDAâ€  [29] R101 66.6 - -
HRDAâ€  [29] R101 - 54.4 -
DAFormer[28] MiT-B5 67.4 - -
HRDAâ€  [29] MiT-B5 71.4 - -
HRDAâ€  [29] MiT-B5 - 62.6 -
Yueetal.[84] R101 44.3 41.2 42.7
DG
Kunduetal.[39] R101 60.1 - -
MTDA-ITA[20] R101 42.7 39.4 41.0
MTDA MT-MTDA[55] R101 45.2 42.2 43.6
CCL[34] R101 48.1 44.0 46.0
MTDA Ours R101 65.1 55.9 60.2(â†‘14.2)
(Merging) Ours MiT-B5 69.5 60.7 64.8(â†‘18.8)
Remarkably,thelevelofperformance(60.2%harmonicmeanwithResNet101
[24])hasalreadyexceededData Combination methods(60.1%harmonicmean),
and we achieve it without requiring access to any training data. When con-
sidering the relaxed setting where only the encoder backbone is merged while
decoding heads are separated for various downstream domains, we achieve aTraining-Free Model Merging for Multi-target Domain Adaptation 23
substantialperformanceimprovementof +6.1%and+3.8%inharmonicmean
for two backbones, respectively.
ComparisonwithSoTAModels.AsshowninTab.6,ourmethodachieves
remarkable results, outperforming many approaches in both MTDA and DG.
Utilizing the ResNet101 [24] backbone, we observe a substantial improvement,
achieving a +14.2% harmonic mean increase over the best published results.
This improvement further rises to +18.8% with transformer-based backbones.
TheoutstandingperformancewithSYNTHIA[61]datasetasthesourcedomain
demonstrates the broad applicability of our merging method.
Table 7: Comparison of Our Method with Data Combination Across Four
Target Domains. The datasets Cityscapes, IDD, ACDC, and DarkZurich are repre-
sented by â€˜Câ€™, â€˜Iâ€™, â€˜Aâ€™, and â€˜Dâ€™.
Setting Encoder C I A D H
DataComb. ResNet101 57.8 53.9 38.8 13.2 29.1
Ours ResNet101 51.6 47.3 45.8 21.1 36.4(â†‘7.3)
Comparison with Data Combination Across Four Target Domains.
AsshowninTab.7,wecomparedourproposedmethodinscenarioswheretarget
domains are diverse. We used the GTA as the source dataset and selected four
datasets as target domains to compare our merging method with the approach
of combining the data from four domains for training. The results demonstrate
that our proposed model merging techniques achieve +7.3% harmonic mean
improvement in performance.
B Visualization Results
The qualitative comparison between different baselines and the proposed model
merging method are provided in Fig.6.
As illustrated in Figure 6, our proposed merging method always ensures
that the combined output retains the superior predictive aspects of two sepa-
rate models. The dotted boxes in the top two rows of this figure highlight the
STDA modelâ€™s proficiency in road classification within the Cityscapes dataset
when Cityscapes is the target, contrasting with its less effective performance on
the IDD dataset. However, the application of our method markedly enhances
the road classification results. In a similar vein, the dotted boxes in the bot-
tom two rows of Figure 6 showcase the STDA modelâ€™s adeptness in identifying
â€™ridersâ€™ and â€™busesâ€™ in the IDD dataset when IDD is the target, as opposed to
its lesser performance with Cityscapes as the target. Post-merging, the model
demonstrates significantly improved classification in these categories.24 W. Li et al.
(a) Input (b) STDA(CS) (c) STDA(IDD) (d) Model Merging (e) Ground Truth
se
p
a
c
sy
tiC
D
D
I
road sidew. build. wall fence pole tr.light tr.sign veget. terrain sky person rider car truck bus train m.bike bike n/a.
Fig.6: Visualization results for GTA to Cityscapes and IDD.(a)Testimages
from Cityscapes and IDD. We visualize results of (b) single-target domain adaptation
(STDA) trained on Cityscapes target, (c) single-target domain adaptation (STDA)
trainedonIDDtarget,(d)ourmodelmergingmethod.(e)Ground-truthsegmentation
maps.
Table 8: Model Merging Across Cityscapes(C), IDD(I) and Mapillary(M)
Metric
Method Setting
C I M H.Mean
Gâ†’C,I 45.8 46.3 - 46.0
Gâ†’C,M 45.8 - 49.2 47.4
ADAS[43]
Gâ†’I,M - 46.1 47.6 46.8
Gâ†’C,I,M 46.9 47.7 51.1 48.5
Gâ†’C,I 58.8 54.0 58.6 57.0
Gâ†’C,M 60.1 46.5 59.1 54.5
Ours
Gâ†’I,M 51.2 53.3 58.8 54.3
Gâ†’C,I,M 58.4 53.2 59.3 56.8
C Results of Mapillary as Target Domain
As shown in Table 8, we have included Mapillary as a target domain and com-
pared it to the previous work [43]. From the experimental results, it is clear
that the domain gap between C, I, and M is smaller than that of the four se-
lected datasets in main content (e.g., Ours (Gâ†’C,I) surprisingly scores 58.6 on
Mapillary despite not being adapted to it).
D Different STDA Methods
We verify if our proposed merging method works with other STDA methods by
adopting it to another STDA Method, ADVENT [75]. Different from teacher-
studentself-trainingframeworks,thismethodleveragesadversarialentropymin-Training-Free Model Merging for Multi-target Domain Adaptation 25
imization to mitigate the domain gap. According to results shown in Tab. 9, we
can deduce that our model merging technique works well with it. In fact, pre-
training, which is the key to the empirical mode connectivity, has shown to pull
the boundaries of NLP tasks [58] closer, and help in transfer learning [54]. We
report that pre-training works similarly w.r.t. generic pre-trained vision back-
bones, and naturally could apply to another STDA methods.
Table 9: Verification on another STDA Method, ADVENT [75].
Setting C I H
ADVENT(Gâ†’C) 43.533.0 37.5
ADVENT(Gâ†’I) 37.139.8 38.4
ADVENT(Merging)40.539.640.1
E Extension to Image Classification Tasks
In this section, we demonstrate the effectiveness of our model merging method
applied to image classification tasks.
By dividing the CIFAR-100 classification dataset into two distinct, non-
overlapping subsets, we independently train two ResNet50 models, labeled A
and B, on these subsets. This training was conducted either from a common
set of pretrained weights or from two sets of randomly initialized weights. The
performance outcomes for models A and B are illustrated in Fig. 7, represented
by dimmed blue and yellow lines, respectively. The results indicate that models
merged from a starting point of identical pretrained weights outperform those
trained on any single subset. Conversely, when beginning with randomly initial-
izedweights,individualmodelsexhibitlearningcapabilities,whereasthemerged
modelâ€™s performance is akin to making random guesses.
Randominitializationwouldbreakthelinearaveragingtechnique,whilesame
pre-trained backbones might work. We verified this conclusion on another pre-
trained weight. Results in Fig 8 indicate that DINO pretraining and ImageNet
pretraininghavedifferentlosslandscapesinthemodelâ€™sparameterspace.Model
merging must be conducted within the same loss landscape.26 W. Li et al.
Fig.7: Model merging results on CIFAR-100 Classification.
Fig.8: Results on CIFAR-100 Classification with ImageNet and DINO Pretrained
Weight.