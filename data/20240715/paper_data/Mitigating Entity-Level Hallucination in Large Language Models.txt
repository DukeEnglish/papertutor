Mitigating Entity-Level Hallucination in Large Language Models
WeihangSu YichenTangâˆ— QingyaoAiâ€ 
swh22@mails.tsinghua.edu.cn DepartmentofComputerScienceand aiqy@tsinghua.edu.cn
DepartmentofComputerScienceand Technology,TsinghuaUniversity, DepartmentofComputerScienceand
Technology,TsinghuaUniversity, Beijing100084,China Technology,TsinghuaUniversity,
Beijing100084,China Beijing100084,China
ChangyueWang ZhijingWu YiqunLiu
DepartmentofComputerScienceand SchoolofComputerScienceand DepartmentofComputerScienceand
Technology,TsinghuaUniversity, Technology,BeijingInstituteof Technology,TsinghuaUniversity,
Beijing100084,China Technology,Beijing100081,China Beijing100084,China
ABSTRACT 1 INTRODUCTION
TheemergenceofLargeLanguageModels(LLMs)hasrevolution- Inrecentyears,largelanguagemodels(LLMs)haveachievedre-
izedhowusersaccessinformation,shiftingfromtraditionalsearch markablesuccessinavarietyofnaturallanguageprocessing(NLP)
engines to direct question-and-answer interactions with LLMs. tasks and have already become an indispensable component of
However, the widespread adoption of LLMs has revealed a sig- manyAIapplications[2,5,32,43,50].Duetotheoutstandingca-
nificantchallengeknownashallucination,whereinLLMsgenerate pabilitiesandwideapplicationsofLLMs,theyhaverevolutionized
coherentyetfactuallyinaccurateresponses.Thishallucinationphe- howusersaccessinformation,shiftingfromtraditionalsearchen-
nomenonhasledtousersâ€™distrustininformationretrievalsystems ginestodirectquestion-and-answerinteractionswithLLMs.How-
basedonLLMs.Totacklethischallenge,thispaperproposesDy- ever,despitetheirimpressiveperformance,itiswellrecognizedthat
namicRetrievalAugmentationbasedonhallucinationDetection existingLLMsmaygeneratetextthat,whileappearingcoherent
(DRAD)asanovelmethodtodetectandmitigatehallucinations andplausibleonthesurface,isfundamentallyinaccurateorlacks
in LLMs. DRAD improves upon traditional retrieval augmenta- groundinginreality.Thisphenomenoniscommonlyreferredtoas
tionbydynamicallyadaptingtheretrievalprocessbasedonreal- LLMhallucination[13,23,28,41,51].
time hallucination detection. It features two main components: Toaddresshallucination,themostpopularmethodadoptedin
Real-timeHallucinationDetection(RHD)foridentifyingpotential existingstudiesisRetrieval-AugmentedGeneration(RAG).Inthis
hallucinationswithoutexternalmodels,andSelf-correctionbased method,relevantknowledgeisretrievedfromanexternalcorpus
on External Knowledge (SEK) for correcting these errors using andutilizedasinputforLLMs,whichhasbeenproventobeef-
externalknowledge.ExperimentresultsshowthatDRADdemon- fectiveinmanyNLPtasks[1,14,16,18,38,40].TraditionalRAG
stratessuperiorperformanceinbothdetectingandmitigatinghal- methodsoftenadoptsingle-roundretrievalthatusestheoriginal
lucinationsinLLMs.Allofourcodeanddataareopen-sourced inputofLLMasthequerytoretrieverelevantexternalinformation.
athttps://github.com/oneal2000/EntityHallucination. Althoughthismethodiseffectiveforsimpletasks,itfrequentlyfails
incomplextaskslikelong-formgenerationandmulti-hopquestion-
KEYWORDS answering.ThisisprimarilybecausetheLLMneedscontinuously
changinginformationinthesecomplextextgenerationtasks[15].
Hallucination,RetrievalAugmentedGeneration,LargeLanguage
Incontrast,multi-roundRAG[1,15,29,44]performsmultiplere-
Model
trievalsduringthegenerationprocessofLLMs.Dependingonthe
ACMReferenceFormat: timingofretrievalaugmentation,variousmethodshavebeenpro-
WeihangSu,YichenTang,QingyaoAi,ChangyueWang,ZhijingWu,andYiqun posed in this area. For example, RETRO [1] and IC-RALM [29]
Liu.2024.MitigatingEntity-LevelHallucinationinLargeLanguageMod-
triggertheretrievalmodulebasedonapre-definednumberofgen-
els.InProceedingsofPre-print.ACM,NewYork,NY,USA,9pages.https:
eratedtokens.FLARE[15]triggerstheretrievalmodulewhenever
//doi.org/10.1145/nnnnnnn.nnnnnnn
atokenâ€™spredictiveprobabilityisbelowacertainthreshold.DRA-
GIN[40]definesanempiricalmethodbasedonuncertaintyand
âˆ—Contributedequally
â€ Correspondingauthor self-attentiontodeterminewhentotriggerretrieval.
Despite these advancements, a significant oversight remains:
noneoftheexistingstudiesexplicitlyverifywhethertheretrievalis
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed triggeredatanoptimaltiming.Typically,theevaluationmethodsof
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation existingdynamicRAGapproachesonlyfocusontheperformance
onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM
indownstreamQAtasks,withoutdirectlyevaluatingwhetherthe
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora chosenmomenttoinvokeretrievalisappropriate.Inthispaper,
fee.Requestpermissionsfrompermissions@acm.org. wearguethatretrievalaugmentationwithoutidentifyingwhen
Pre-print,UnderReview,V1.0
andwherehallucinationhappensinLLMissuboptimalinterms
Â©2024AssociationforComputingMachinery.
ACMISBN978-x-xxxx-xxxx-x/YY/MM...$15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
4202
luJ
21
]LC.sc[
1v71490.7042:viXraPre-print,UnderReview,V1.0 WeihangSu,YichenTang,QingyaoAi,ChangyueWang,ZhijingWu,andYiqunLiu
ofeffectivenessandefficiency.Ontheonehand,theretrievercan- ThesemethodsallowanLLMtogeneratemultipleoutputsbased
notbeperfect,thereforeunnecessaryretrievalaugmentationmay onthesameinput.Subsequently,thelikelihoodofhallucinations
introduce irrelevant or noisy data to LLMs. On the other hand, occurringintheLLMismeasuredbasedontheconsistencyamong
callingtheretrievalmoduleduringtheLLMgenerationprocess theseoutputs.Ontheotherhand,somemethodsrequiretheintro-
willincreasetheinferencetimeandcomputationalcost.Suchcost ductionofnewmodels.Forexample,MIND[41]isanunsupervised
isunworthyifretrievalaugmentationisconductedinplaceswhere hallucinationdetectionmethodbasedontheinternalstatesofLLMs.
LLMhallucinationdoesnâ€™texist. TheMINDframeworkconsistsoftwosteps:automaticallygenerat-
Toaddresstheseconcerns,weevaluateexistingdynamicRAG ingtrainingdataandtrainingahallucinationdetectorbasedonthe
approaches on the timing of retrieval to find out if it coincides hiddenstatesoftheselectedLLM.Theinputtothehallucination
withtheoccurrenceofhallucination.Wethenproposeadynamic detectoristhehiddenstatesoftheLLM,andtheoutputisa0-1label,
retrieval-augmentedgenerationstrategybasedonhallucination representingwhethertheLLMisexperiencinghallucinations.Liu
detection.Tobespecific,weproposeDRAD,DynamicRetrieval etal.[23]specificallyfine-tunedseveralmodelssuchasBERT[6],
augmentationbasedonhallucinationDetection(DRAD,illustrated RoBERTa[24],andXLNet[47]todetecthallucinations.Theinput
inFigure1)thatsynchronizesretrievalaugmentationwithreal-time totheHallucinationDetectoristheoutputtextofanLLM,andthe
hallucinationdetectionofLLMsduringthetextgenerationprocess. outputisalsoa0-1labelindicatingwhethertheLLMisexperienc-
DRADconsistsoftwocomponents:Real-timeHallucinationDetec- inghallucinations.Ourproposedhallucinationdetectionmethod
tion(RHD)andSelf-correctionbasedonExternalKnowledge(SEK). doesnotrequiregeneratingmultipleresponsesforthesameinput
RHDisareal-timehallucinationdetectionmethodthatdoesnâ€™trely orintroducingexternalinformationormodels.Consequently,our
onanyexternalmodelsorexternalknowledge.Itdetectshalluci- approachexcelsinefficiencycomparedtoexistingworks.
nationsbyanalyzingtheuncertaintyoftheoutputentitiesofan
LLM,particularlythosewithlowprobabilityandhighentropy,that 2.2 Retrieval-augmentedGeneration
arelikelytobepotentialhallucinations.WhenRHDdetermines
Inrecentstudies,Retrieval-AugmentedGeneration(RAG)hasbeen
thatthemodelislikelytogenerateunreliabletext,SEKinvokes
widelyusedtoimprovetheperformanceofLLMs.Oneofthemost
theretrievalmoduletoretrieverelevantexternalknowledgeand
directmethodsissingle-roundRAG[1,9,12,14,16,18,34],which
helpstheLLMtomakecorrectionstoitsoutputssothatpotential
utilizetheinitialinputoftheLLMtoretrieveexternalknowledge.
hallucinationscanbeprevented.
Theretrievedexternalknowledgeissubsequentlyintegratedinto
Weconductedexperimentsonexistingbenchmarks[8,10,11,27,
themodelâ€™s input.Previousstudies,like REPLUG[34]and Uni-
35]toevaluatetheeffectivenessofourframework.Experimental
Web[22],haveexploredusingLanguageModel-basedfeedbackand
resultsshowthatRHDcanachievestate-of-the-art(SOTA)perfor-
adaptivesearchenginesforretrievalaugmentationinsingle-round
manceinhallucinationdetection,andDRADsignificantlyoutper-
retrieval,enhancingpredictionsandselectivelyincorporatingex-
formsexistingsingle-roundandmultiple-roundretrievalaugmen-
ternalknowledge.
tationmethods.
Single-roundRAGcanbequiteeffectiveforstraightforwardtasks
Tosummarize,thecontributionsofthispaperareasfollows:
orsituationswheretheuserâ€™sinformationneedsarewell-defined.
â€¢ We introduce a new retrieval-augmented framework, i.e.,
However,forthetasksthatinvolvegeneratingextensivetextsuch
DRAD1,whichdetectshallucinationsduringLLMâ€™sinfer-
aslong-formgenerationandmulti-hopQA,searchingforexternal
enceprocessandtriggersRAGonlywhenhallucinationsare
knowledgebasedontheinitialinputcannotsufficientlyaddress
detected.
theinformationneedsforLLM[15].Asaresult,researchershave
â€¢ Weproposeareal-timehallucinationdetectionmethod,i.e.,
beguntoexploremulti-roundretrievalaugmentation.Forexample,
RHD,thatachievesSOTAperformanceonexistingbench-
RETRO[1]andIC-RALM[29]triggerretrievalevery4to32tokens,
marks.
whileIRCot[44]triggersretrievalwheneveranewsentenceisgen-
â€¢ We evaluate DRAD on multiple complex QA benchmark
erated.Lookingatitfromanotherperspective,FLARE[15]triggers
datasets,andtheexperimentalresultsdemonstratethatour
retrievalwhenanytokeninthegeneratedtexthasaprobability
proposedDRADframeworksignificantlyreduceshallucina-
lowerthanacertainthreshold.However,indiscriminatelyconsid-
tionsinlargemodelsacrossthreediversetextgeneration
eringtheprobabilityofeverytokenisnottheoptimalsolutionfor
benchmarks,outperformingpreviousmethods.
multi-roundretrieval,asmanytokensarefunctionwords(such
asâ€˜amâ€™,â€˜toâ€™,â€˜inâ€™,etc)lackingsemanticmeaning.Toaddressthe
2 RELATEDWORKS
limitationsofFLARE,DRAGIN[40]optimizethetiming(whento
2.1 HallucinationDetection retrieve)andthequeryformulationmethod(whattoretrieve)ofthe
dynamicRAGframework,achievingstate-of-the-artperformance
Giventhesignificanceofhallucinationdetectionandmitigation,
ondownstreamQAtasks.
considerableresearchhasfocusedondevelopingefficientandef-
fectivemethodsforhallucinationdetection.Somemethodsrequire
themodeltogeneratemultipleoutputsforthesameinput.For 3 METHODOLOGY
instance,SelfCheckGPT[27]introducesthreemethods:SelfCheck- ThissectionwillintroducetheDRADframeworkindetail.DRAD
GPT_BERTScore,SelfCheckGPT_QA,andSelfCheckGPT_n-gram. consistsoftwocomponents:Real-timeHallucinationDetection
1Ourcodeanddataareopen-sourcedonthisanonymousGithublink:https://github. (RHD)andSelf-correctionbasedonExternalKnowledge(SEK).We
com/oneal2000/EntityHallucination. willintroduceRHDinsection3.1andSEKinsection3.2.MitigatingEntity-LevelHallucinationinLargeLanguageModels Pre-print,UnderReview,V1.0
Question:
Who is the maternal grandfather of
Antiochus X Eusebes?
LLM Antiochus X Eusebeswas a Seleucid king who ruled
Generating: from 95 BC to 92 BC. His maternal grandfather was
King Demetrius II Nicator I
Real-time HallucinationDetection (RHD) Self-correction based on External Knowledge(SEK)
Continue Name Entity Recognition QueryFormulation
Generation EntityDetected Antiochus X Eusebeswas a Seleucid king who ruled
No from 95 BC to 92 BC. His maternal grandfather was
Yes
Hallucination
Detected
Entropy of Output Entity-Level Retrieval
Distribution Probability Module
RetrievedPassage
Hallucination Judgementbased Self-correction based on
on the Threshold LLM External Knowledge
ContinueGeneration Antiochus X Eusebeswas a Seleucid king who ruled
from 95 BC to 92 BC. His maternal grandfather was
King Demetrius II NicatorPtolemy IX LathyrosI
Figure1:AnillustrationofourproposedDRADFramework,whichcomprisestwomaincomponents:RHDandSEKModule,
highlightedinthediagramwithblueandgreenframes,respectively.
3.1 Real-timeHallucinationDetection amethodforreal-timedetectionofhallucinationsinLLMâ€™sout-
3.1.1 EmpiricalInvestigation. OurapproachreferredtoasReal- puts,utilizinganentityconfidencemetricderivedfromboththe
timeHallucinationDetection(RHD,illustratedinFigure2),iscon- predictiveprobabilityandentropyofentities.
structedbasedontheassumptionthatwhenanLLMisforcedto
3.1.2 EntityProbability. Thefirstmethodtoevaluatethehalluci-
provideanswersorgeneratetextbeyonditsexistingknowledge,the
nationofanLLMinvolvesexaminingitsgenerationprobabilityfor
uncertaintyinitsoutputsignificantlyescalates[27].Forexample,
eachentityinitsoutput.Thisprocessstartswiththerecognitionof
considertheinput,"BillClintonwasbornandraisedin____".Since entitiesintheLLMoutput.Specifically,letğ‘‚representtheoutput
theLLMhasseenextensiveinformationaboutBillClintonduring
sequencegeneratedbyanLLM.Weapplyreal-timeentityrecogni-
thepre-trainingphase,itcanconfidentlyassignahighprobability tiononğ‘‚toidentifyentities.Assumeanentityğ¸isdetectedinğ‘‚,
tothetokenâ€œArkansas"tocompletethesentence.Conversely,other consistingofğ‘›tokens.Next,foreachtokenğ‘¡ ğ‘–inğ¸,where1â‰¤ğ‘– â‰¤ğ‘›,
locationssuchasâ€œAlabama"orâ€œParis"wouldbeconsideredlow letğ‘ ğ‘– denotethegenerationprobabilityofğ‘¡ ğ‘– asgivenbytheoutput
probability.However,inscenarioswheretheLLMisrequiredto layeroftheLLM.Finally,theoverallprobabilityoftheentityğ¸is
generatetextaboutunfamiliartopics,suchas"Aliceâ€™schildhood
computedasanaggregationoftheprobabilitiesofitsconstituent
neighbornowlivesin____,"themodelfacesachallenge.Itlacks
tokens.Thiscanbeexpressedas:
specificknowledgetoconfidentlychooseawordtofillthegap,
leadingtoaflatprobabilitydistributionacrossalltokensrelated ğ‘ƒ(ğ¸)=ğ‘“(ğ‘ 1,ğ‘ 2,...,ğ‘ ğ‘›), (1)
toplacenamesinitsvocabulary.Thisuncertaintyinchoosingthe
where ğ‘“ isanaggregationfunction(e.g.,product,sum,average,
righttokencancausethemodeltoselectarandomentityduring
textgenerationandcausehallucinations.
min,max)appliedtotheprobabilitiesğ‘ 1,ğ‘ 2,...,ğ‘ ğ‘›ofthetokensin
ğ¸.
Thisinsightleadsustoestablishaconnectionbetweenuncer-
taintymetricsandhallucinationdetectionintextgeneration.When 3.1.3 EntityEntropy. Inadditiontoprobability,entropyiswidely
amodelâ€™sinternalparameterscannotreliablychoosethecorrect usedtoevaluatetheuncertaintyinherentinthemodelâ€™soutput
responsefrommultipleoptions,itsoutputstendtoincludetokens distribution.Givenanentityğ¸comprisedofğ‘›tokens,weassess
withlowprobabilitiesandhighentropy.Therefore,wepropose the entropy for each token in the LLM output distribution. For
eachtokenğ‘¡ ğ‘– intheentityğ¸,theentropyHğ‘– intheLLMoutputPre-print,UnderReview,V1.0 WeihangSu,YichenTang,QingyaoAi,ChangyueWang,ZhijingWu,andYiqunLiu
Question: 3.2 Self-correctionbasedonExternal
Knowledge
Who is the maternal grandfather of
Antiochus X Eusebes? TheobjectiveoftheDRADframeworkistomitigatehallucinations
inLLMusingaDetect-Retrieve-Reviseparadigm.TheRHDmod-
Antiochus X Eusebes, also known as Antiochus uleperformsdetection,whichinvolvesreal-timeidentificationof
X Alexander, was a Seleucid king who ruled hallucinations.Followinghallucinationdetection,DRADretrieves
from 95 BC to 92 BC. His maternal grandfather relevantexternalknowledgebasedonthecontextsurroundingthe
was King Demetrius II Nicator I hallucinationâ€™soccurrenceandthenrevisesthehallucination.This
Retrieve-ReviseprocessisimplementedthroughtheSEKmethod
(Self-correctionbasedonExternalKnowledge).TheSEKmethod
Continue Name Entity Recognition comprisesthreemaincomponents:queryformulation,relevant
Generation
EntityDetected knowledgeretrieval,andself-correction.Thissectionprovidesa
No
detaileddescriptionoftheSEKmethod.
Yes
3.2.1 FormulatingSearchQueries. GiventheoutputofanLLM
ğ‘‚ comprisingtokensğ‘¡ 1,ğ‘¡ 2,...,ğ‘¡ ğ‘› andanentityğ¸ identifiedasa
Ent Dro ip sty r io bf u tO iou ntput E Pn rt oit by a- bL ie liv te yl hallucinationbytheRHDmodule,wherethetokenspanofğ¸ is
fromğ‘¡
ğ‘˜
toğ‘¡ ğ‘˜+ğ‘–,theSEKqueryğ‘„isformulatedas:
Hallucination Judgement
ğ‘„ =ğ‘“(ğ‘‚,ğ‘¡ ğ‘˜,ğ‘–)=concat(ğ‘¡ ğ‘˜âˆ’ğ‘š,...,ğ‘¡ ğ‘˜âˆ’1,ğ‘¡ ğ‘˜+ğ‘–+1,...,ğ‘¡ ğ‘˜+ğ‘š). (5)
based on the Threshold
In this equation,ğ‘„ represents the formulated query,ğ‘‚ denotes
theoutputoftheLLMconsistingofaseriesoftokens,ğ‘¡
ğ‘˜
toğ‘¡
ğ‘˜+ğ‘–
Figure2:AnillustrationofourproposedReal-timeHalluci- definethehallucinationentityspanwithinthecontext,andconcat
nationDetection(RHD)framework. istheconcatenationfunctionthatassemblestokenssurrounding
theentityspantoconstructthequery.Thefunctionğ‘“ representsthe
processofquerygenerationbasedonthegivencontextandentity
distributionisdefinedas: span.Theparameterğ‘šisaflexiblehyperparameterthatdetermines
âˆ‘ï¸ therangeoftokensincludedintheconcatenationprocess.
Hğ‘– =âˆ’ ğ‘ ğ‘–(ğ‘¤Ëœ)logğ‘ ğ‘–(ğ‘¤Ëœ), (2)
ğ‘¤ËœâˆˆW 3.2.2 ExternalKnowledgeRetrieval. Givenanexternalknowledge
whereğ‘ ğ‘–(ğ‘¤Ëœ)representsthelikelihoodofgeneratingwordğ‘¤Ëœ,and corpusD,composedofasetofdocuments{ğ‘‘ 1,ğ‘‘ 2,...,ğ‘‘ ğ‘›},anda
WisthevocabularyoftheLLM.Tocomputetheoverallentropy retrievalfunctionR,theprocessofexternalknowledgeretrieval
oftheentityğ¸,weaggregatetheentropiesofitsconstituenttokens. foragivenqueryğ‘ismathematicallyformalizedasfollows:
Thiscanbedoneusingapoolingfunctionğ‘“ thattakestheentropies
ofindividualtokensandcombinesthemtoyieldtheentropyofthe S={R(ğ‘,ğ‘‘ ğ‘–)|ğ‘‘ ğ‘– âˆˆD}. (6)
entity:
Inthisequation,Srepresentsascorelistwhereeachscoreiscom-
H(ğ¸)=ğ‘“(H 1,H 2,...,Hğ‘›). (3) puted by the retrieval function R for the query-document pair
(ğ‘,ğ‘‘ ğ‘–),assessingtheirrelevance.
Thisapproach,usingentropyasameasure,offersanadditional
dimensiontoevaluatetheuncertaintyinanLLMâ€™soutput,particu-
larlyinitsabilitytogeneratespecificentities. S sorted=sortğ‘‘ğ‘’ğ‘ ğ‘(S), (7)
3.1.4 ThresholdforHallucinationDetection. Toexplicitlydeter- where S is the list of scores S sorted in descending order
sorted
minewhetheranentitygenerationshouldbeconsideredasahallu- basedonrelevance.Finally,thetopğ‘˜documentsfromS are
sorted
cinationornot,wedefinetwothresholds,ğœƒ 1andğœƒ 2. selectedasthesetofretrievedrelevantdocuments:
(cid:40)
EntityHallucination=
yes ifğ‘ƒ(ğ¸) <ğœƒ 1orH(ğ¸) >ğœƒ 2
(4)
D retrieved={ğ‘‘ ğ‘–|(ğ‘,ğ‘‘ ğ‘–,ğ‘  ğ‘–) âˆˆS sortedâˆ§ğ‘– â‰¤ğ‘˜}. (8)
no otherwise
Inthisequation,D denotesthesubsetofdocumentsfrom
retrieved
Notethatğœƒdeterminesthefrequencyofcallingtheretrievalmod- Dthataremostrelevanttothequeryğ‘,basedonthetopğ‘˜scores
ule.Thespecificvalueofğœƒ canbeadjustedbasedonthedemands inS .
sorted
oftheactualapplication,therebyofferingflexibilityindetermin- Note that in the field of information retrieval, various meth-
inghowoftentheretrievalmoduleisconsulted.Weconducted ods have been explored for the retrieval function ğ‘…. These in-
adetailedexperimenttoexploretheimpactofthethresholdon cludevocabulary-basedapproachessuchassuchasTF-IDF[30],
efficiencyandeffectiveness.Theexperimentalresultscanbefound BM25[31],andQueryLikelihood[49],aswellasneuralnetwork-
insection5.2. basedmethods[3,4,7,19â€“21,25,26,36,37,39,48].MitigatingEntity-LevelHallucinationinLargeLanguageModels Pre-print,UnderReview,V1.0
3.2.3 Self-Correction. TheRHDmethoddetectsthespecificposi- â€¢ NQ[17].WeusetheNQdatasettoassesstheDRADmodelâ€™s
tioninanLLMâ€™soutputwherehallucinationoccurs.Itisimportant capabilitytoanswerfactualquestions.NQisalarge-scale
tonotethatthetextprecedingthishallucinationpositionistypi- question-answering dataset derived from real queries of
callyaccurateanddoesnotrequiremodification.Thus,torevise GoogleSearch.
thehallucinationtokens,thefirststepofSelf-correctionistrun-
catingtheoutputofanLLMatthepositionofhallucination.This 4.2 SettingsforeachDataset
truncationprocessismathematicallyrepresentedas: Oursettingsonvariousbenchmarksareasfollows:
ğ‘‚â€² =truncate(ğ‘‚,ğ‘¡ ğ‘˜), (9) â€¢ 2WikiMultihopQA[11].Wefollowtheprompttemplate
whereğ‘‚â€² denotesthetruncatedoutput,ğ‘‚ istheoriginaloutput ofWangetal.[45]thatinstructstheselectedLLMtogener-
oftheLLM,andğ‘¡ ğ‘˜ denotesthefirsttokenofthehallucinateden- atethechain-of-thoughtreasoningprocess.Followingthe
prompttemplateof[44]and[15],weadd8examplequery-
tity.Followingthis,theretrievedexternalknowledgesetD
retrieved
answerpairstotheinputasprompts.
(definedinEq 8)isinputintotheLLMusingaspecificprompt
template2.Aftertheexternalknowledgeisconcatenated,theLLM Fortheevaluationmetrics,weusepattern-matchingtech-
niquestoextractthefinalanswerfromtheoutputofthe
regeneratesthecontentatthepositionofthehallucination,utiliz-
LLM.Thisextractedanswerissubsequentlycomparedtothe
ingtheexternalknowledge.Theregeneratedcontentrepresents
referenceanswer.Weemployvariousmethodsforthiscom-
theLLMâ€™sself-correctionbasedonexternalknowledge,aimingto
parison,includingtheexactmatch(EM)metricattheanswer
addressthedetectedhallucinatedentitiesinitsoutput.
levelandtoken-levelassessmentsofF1score,precision,and
3.3 Discussion recall.
â€¢ StrategyQA[8].WefollowthesettingofWeietal.[46]to
Inthissubsection,wediscussthelimitationsoftheRHDmethod.
instructtheLLMtogeneratethechain-of-thoughtreasoning
While the RHD method is effective at detecting hallucinations
process.Followingtheprompttemplateof[46]and[15],we
caused by gaps in the knowledge of LLMs, it is less successful
add8examplequery-answerpairstotheinputasprompts.
atidentifyinghallucinationsthatareduetoincorrectinformation
Sincethisdatasetonlycontainstrue/falsequestions,wedi-
acquiredduringthemodelâ€™spre-trainingphase.Hallucinationsin
rectlyreportedtheaccuracy.
LLMsprimarilyarisefromtwosources:first,theabsenceofrelevant
â€¢ NQ[17].SincethequestionsinNQarerelativelysimple,we
knowledge,whichleadstheLLMtogeneratethemostprobablebut
didnotincludeexamplequery-answerpairsaspromptsin
potentiallyincorrecttoken;second,thelearningofincorrectknowl-
theinputtoLLM.
edgeduringpre-training.TheRHDmethodutilizesuncertainty
Fortheevaluationmetrics,weusethesameparametersas
forhallucinationdetection,makingitproficientatrecognizingthe
wedidinour2WikiMultihopQAexperiment.
firsttypeofhallucination,whereLLMsoftenshowuncertainty.
Forallthreedatasets,wechoseBM25asourretrievalmodelbased
However,itstruggleswiththesecondtype,whereLLMsmight
onfindingsfrom[29],whichdemonstrateditssuperiorperformance
deliverincorrectresponseswithhighconfidence.
inRAG,evenoutperformingSOTAdenseretrievalmodels.Forthe
externalcorpus,weuseWikipediapassages.TheTop3relevant
4 EXPERIMENTALSETUP
passagesretrievedbyBM25wereaddedtotheprompt.
4.1 Datasets
4.3 Baselines
4.1.1 HallucinationDetection. Weevaluateourproposedreal-time
hallucinationdetectionmethodonWikiBioGPT-3dataset[27].This 4.3.1 HallucinationDetection. Wechosethefollowinghallucina-
datasetcomprisestextgeneratedbyGPT-3basedonvarioustopics, tiondetectionmethodsasbaselines:
alongwithmanuallyannotatedlabelsindicatingwhetherthese â€¢ SCG-BERTScore[27]usestheLLMtoproducemultiple
passagescontainhallucinationsfromafactualperspective.
samplesforasingleinput.BERTScorecalculatestheaverage
4.1.2 Retrieval-augmentedTextGeneration. Weevaluatethetext similaritybetweenasentenceanditsmostsimilarcounter-
generationperformanceofDRADonthefollowingdatasets. partinvarioussamples.Ifasentenceishighlysimilarto
many samples, itâ€™s probably factual. If not, it might be a
â€¢ 2WikiMultihopQA [11]. We use the 2WikiMultihopQA
fabricateddetailor"hallucination".
datasettoassesstheDRADmodelâ€™sabilitytoanswercom-
â€¢ SCG-QA [27] generates multiple samples from the LLM
plexquestions.2WikiMultihopQAcomprisescomplexques-
forasingleinput.Basedononeofthesesamples,aquery
tionsthatrequiretwohopsofinformationfromWikipedia
generationsystemcreatesmultiple-choicequestions.Thena
articles.Answeringthesequestionstypicallyinvolvescom-
QAsystemusestheothersamplestoanswerthequestion.
posing,comparing,orinferringmultiplepiecesofinforma-
Theconsistencyoftheseanswersisusedtomeasurethe
tiontoprovideanaccurateresponse.
probabilityofhallucination.
â€¢ StrategyQA[8].WeutilizetheStrategyQAdatasettoassess
â€¢ SCG-Ngram[27]involvestrainingann-grammodel,on
thecommonsensereasoningcapabilitiesofDRAD.Strate-
samplesgeneratedbytheselectedLLM.Asthesamplesize
gyQAcomprisesadiverserangeofcrowdsourcedyes/no
escalates,thebehaviorofthisnewmodelprogressivelyap-
questions.
proximatesthatoftheoriginalLLM.Subsequently,theaver-
2TheprompttemplateisdetailedinSection4.2 agelogprobabilitiesforagivenresponse,R,arecomputedPre-print,UnderReview,V1.0 WeihangSu,YichenTang,QingyaoAi,ChangyueWang,ZhijingWu,andYiqunLiu
Table1:AcomparativeoverviewofourselectedRetrieval- Table2:Experimentalresultsofourhallucinationdetection
AugmentedGenerationbaselines. techniquecomparedtootherbaselinemethodsontheSe-
flCheckGPTdataset.Thebestperformancesarehighlighted
inbold.TheperformanceofeachSeflCheckGPTmethodis
TimingforRetrieval QueryFormulation
directlysourcedfromtheiroriginalpaper.
SRR BeforeGeneration InitialInput
FLR PerSentence LastGeneratedSentence AUC
AnyTokenProbability LastGeneratedSentence SCG_BERTScore 81.96
TPR
BelowThreshold ExcludeUncertainTokens SCG_QA 84.26
MultiGeneration
Entity-based SCG_n-gram 85.63
DRAD HallucinationDetection ContextofHallucination SCG_ensemble 87.33
AvgTokenEntropy 80.73
AvgTokenProb 83.21
basedonthen-grammodel.Thisaverageoflogprobabilities SingleGeneration MaxTokenEntropy 85.75
isthenemployedasametrictoquantifythelikelihoodof MinTokenProb 87.51
hallucination. RHD(ours) 89.31
â€¢ SCG-Ensemble[27]isasimplecombinationofthenormal-
izedscoresofSCG-BERTScore,SCG-QA,andSCG-Ngram.
â€¢ PredictiveProbability[15]involvesleveragingtheproba- Table3:Comparativeexperimentalresultsforvarioustoken
bilitiesoftokensgeneratedbyLLMsasametrictomeasure poolingmethodsofRHD.Sincelowprobabilityandhigh
hallucination.FLARE[15]adoptsthissimplemethodology entropyindicatelowconfidence,weemployminpoolingfor
todeterminetheappropriatetimingforinvokingretrieval probabilityandmaxpoolingforentropy.
augmentation.
â€¢ PredictiveEntropyutilizestheentropyoftheoutputdis-
AUC
tributionoftokensgeneratedinthetextbylargemodelsas
ametrictogaugehallucination. PoolingMethod Probability Entropy
4.3.2 TextGeneration. WechoosethefollowingTextGeneration Max - 88.00
baselinesforcomparison.Toensureafaircomparison,eachsen- Min 88.75 -
tenceineverymulti-roundRAGmethodcanonlyberevisedonce. First 87.60 87.46
Average 89.31 87.91
â€¢ NOR(NORetrieval).Directlygeneratinganswerswithout
retrievalaugmentation.
â€¢ SRR(Single-RoundRetrieval).Inthissetting,weretrieve
relevantexternaldocumentsoncebasedontheinitialques- theGPT-3.5languagemodel,specificallythe"text-davinci-
tion.Mostofthecurrentretrieval-augmentedLLMsadopt 003"variant,byiterativelyqueryingitsAPI3.Notably,this
thissetting. APIallowsaccesstotheprobabilitiesofthegeneratedtokens.
â€¢ FLR(FixLengthRetrieval)[44].Amulti-roundretrieval â€¢ NameEntityRecognition(NER):FortheNamedEntity
augmentationmethodthattriggerstheretrievalmodulefor Recognition(NER)componentofRHD,wefollowthemethod-
everysentence. ologiesinpriorstudies[23,42].Specifically,weutilizedthe
â€¢ TPR(TokenProbabilityRetrieval)[15].Amulti-roundre- Spacylibrary,atoolrecognizedforitsefficacyinNERas
trievalaugmentationmethodthattriggersretrievaleachtime evidencedbypreviousresearch[33].
itencountersanuncertaintoken. â€¢ ExternalKnowledgeCorpus:WeadoptWikipediaasour
Formulti-roundRetrieval-AugmentedGeneration,thetwomost externalknowledgecorpus.Eacharticleissegmentedinto
criticalaspectsarethetimingforretrievalandthemethodofquery 100-tokenpassages.
formationwhentriggeringretrieval.Tovisuallydemonstratethe
differencesbetweenourselectedbaselines,wepresentthetiming 5 EXPERIMENTALRESULTS
forretrievalandthemethodofqueryformationforeachbaseline
5.1 HallucinationDetection
inTable1.
Inthissection,wepresenttheexperimentalfindingsobtainedfrom
4.4 ImplementationDetails theWikiBioGPT-3dataset.Thisdatasethasbeenspecificallycrafted
Inthissubsection,weprovideacomprehensiveoverviewofour forabinaryclassificationtask,withtheobjectiveofmeasuringthe
implementation details for the major components of our study: likelihoodofhallucinatedtextproducedbytheGPT3(text-davinci-
configurationsforLLMs,NamedEntityRecognition(NER),andthe 003)model.TheAUC(AreaUndertheCurve)metricwasadoptedto
ExternalKnowledgeCorpus. evaluatetheperformanceofeachhallucinationdetectionmethod.
â€¢ LLMConfiguration:WefollowsettingsofJiangetal.[15]
andvalidateourretrievalaugmentationapproachbasedon 3https://api.openai.com/v1/chat/completionsMitigatingEntity-LevelHallucinationinLargeLanguageModels Pre-print,UnderReview,V1.0
Table4:ExperimentalresultsofDRADandotherbaselineson2WikiMultihopQA,NQ,andStrategyQA.Thebestresultsarein
bold.#Numindicatesthetimesofretrievalmodulecalls,smallermeansmoreefficient.
2WikiMultihopQA NQ StrategyQA
F1 EM Prec. Recall #Num F1 #Num Accuracy #Num
WithoutRetrieval NOR 0.2904 0.22 0.2879 0.3001 0 0.283 0 0.64 0
Single-roundRetrieval SRR 0.3574 0.26 0.3622 0.3927 1 0.293 1 0.65 1
FLR 0.4704 0.38 0.4693 0.4886 9.61 0.291 3.41 0.62 5.48
Multi-roundRetrieval TPR 0.3734 0.24 0.3681 0.4071 5.29 0.252 1.67 0.60 2.25
DRAD 0.4732 0.39 0.4741 0.4976 1.40 0.339 0.48 0.76 0.53
5.1.1 OverallResults. TheexperimentresultsofRHDandother
baselinesareshowninTable2.Wehavethefollowingobservations.
(1)Firstly,methodsbasedontokenprobabilityareshowneffective
indetectinghallucinations.Amongthese,theminimumtoken-level
probabilityhassuperiorperformanceamongalltoken-generation
probabilitytechniques.However,theperformanceoftoken-level
hallucinationdetectionmethodsgenerallyfallsshortwhencom-
paredtoRHD.Thisalsoconfirmsourassumptionthatindis-
criminatelyconsideringtheprobabilityofeverytokento
detecthallucinationsisunreasonable,asmanytokensare
meaninglesswords.(2)Furthermore,theSCG_ensemblemethod,
whileeffectiveamongmulti-roundgenerationtechniques,hasdraw-
backsduetotheneedforLLMstocreatemultipleresponses,making
itimpracticalforreal-timeuse.Despiteitshighcomputationalcom-
plexity,itsperformanceadvantageoverprobability-basedmethods Figure3:VisualcomparisonbetweenDRADandbaselines
isnotsubstantial.Thus,werecommendusingthetokenprobability acrossalldatasets.Fortheevaluationmetric,wechooseAc-
methodswhenpossible,duetotheirefficiencyandeffectiveness.(3) curacyforStrategyQA,EMfor2WikiMultihopQA,andF1for
Lastly,ourproposedhallucinationdetectionmethodRHDoutper- NQ.
formsallotherbaselines.Thismethodenablesreal-timedetection
ofhallucinationsandexhibitsexcellentperformance,providinga
solidfoundationforourproposedDetect-Retrieve-Reviseparadigm. methodsacrossvariousbenchmarks.Specifically,inthecontext
ofStrategyQAandNQdatasets,DRADdemonstratesaremark-
5.1.2 AblationStudyonPoolingMethods. Intheoutputgenerated ableimprovementinperformancerelativetoprecedingretrieval-
byLLM,numerousentitiesarecomposedofmultipletokens.Thus, augmentedmodels.Forthe2WikiMultihopQAdataset,whilethe
weexplorediversepoolingstrategiesfortheseentitiesthatencom- resultsarecommensuratewiththoseofFLR,itisnoteworthythat
passmultipletokenswhichareshowninTable3.Throughtheex- the number of retrieval invocations for DRAD constitutes only
perimentalresults,wehavethefollowingobservations:Firstly,dif- 14.7%ofthosenecessitatedbyFLR.FortheFLRmethod,thoughthe
ferentpoolingmethodshaveminimalimpactonourRHDmethod, retrievalmoduleisinvokedforeveryindividualsentence,ourexper-
suggestingthatRHDisnotsensitivetopoolingstrategies.Addition- imentalresultssuggestthatsuchanapproachdoesnotinvariably
ally,theaveragepoolingofProbabilityachievesthehighestAUC enhancetheperformanceofLanguageLearningModels(LLMs).
withascoreof89.31,indicatingitâ€™sthemosteffectivemethodfor Particularly,whentestedontheStrategyQAdataset,theperfor-
representinganentity.Thefollowingexperimentsinthissection manceoftheFLRmethodwasfoundtobesubparincomparisonto
allfollowthissetting. thesingle-roundretrievalmethods.Additionally,despiteutilizing
theidenticalhyperparametersasdelineatedintheoriginalpaper
5.2 DownstreamQuestion-AnsweringTasks
onTPR,theperformanceoftheTPRmodelwasobservedtobe
5.2.1 Overall Performance. The overall experimental results of inferiortootherbaselinemodels.Onanothernote,ondatasetssuch
DRADandotherbaselines4arepresentedinTable4andFigure3. asNQandStrategyQA,theDRADmodelnecessitatedevenfewer
TheexperimentalresultsshowthatDRADoutperformsallbaseline invocationsoftheretrievalmodulethanmethodssupplemented
by single-round retrieval. This observation emphaticallyunder-
4TheimplementationmethodoftheTPR(FLARE)officialGithubrepositoryisas scoresthesuperiorefficiencyandeffectivenessofourproposed
follows:aslongastheprobabilityofallthetokensgeneratedbyLLMisnothigher methodology.
thanthethreshold,retrievaliscontinuouslytriggeredtomodifythissentenceuntil
theconditionismet.Toensureafaircomparison,westipulatethateachsentencein
5.2.2 AblationStudiesonThreshold. Thissectionpresentsthere-
everymulti-roundRAGmethodcanonlyberevisedonce.Thatâ€™swhytheperformance
reportedintheoriginalFLAREpaperishigherthanwereproduce. sultsofourablationstudyondifferentthresholds.ThecomparisonPre-print,UnderReview,V1.0 WeihangSu,YichenTang,QingyaoAi,ChangyueWang,ZhijingWu,andYiqunLiu
Table5:ComparasionbetweendifferentthresholdsofRHD DRADâ€™ssuperiorityinhallucinationdetectionanditseffectiveness
on2WikiMultihopQAandStrategyQA.Thebestresultsare comparedtootherretrievalaugmentationmethods.Weacknowl-
inbold. edgethelimitationsofthispaper,notablythatthereal-timehalluci-
nationdetectionmethod(RHD)dependsontokenprobabilitydata,
2WikiMultihopQA StrategyQA whichisunavailablefromcertainAPIs.Futureworkaimstode-
velopmorereal-timehallucinationdetectionmethodstoovercome
Threshold F1 EM Accuracy
thisconstraint.
0.40 0.4732 0.39 0.76
0.42 0.4708 0.39 0.75
REFERENCES
0.44 0.4682 0.39 0.75
[1] SebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRuther-
0.46 0.4715 0.38 0.76
ford,KatieMillican,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,Bog-
0.48 0.4689 0.38 0.75 danDamoc,AidanClark,etal.2022.Improvinglanguagemodelsbyretrieving
0.50 0.4654 0.38 0.75 fromtrillionsoftokens.InInternationalconferenceonmachinelearning.PMLR,
2206â€“2240.
[2] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda
Table6:ComparisonbetweentheefficiencyofDRADand Askell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneural
otherbaselinesacrossalldatasets.Themostefficientresults informationprocessingsystems33(2020),1877â€“1901.
areinbold.2WMQAindicatesthe2WikiMultihopQAdataset. [3] JiaChen,HaitaoLi,WeihangSu,QingyaoAi,andYiqunLiu.2023. THUIR
at WSDM Cup 2023 Task 1: Unbiased Learning to Rank. arXiv preprint
arXiv:2304.12650(2023).
[4] XuesongChen,ZiyiYe,XiaohuiXie,YiqunLiu,XiaorongGao,WeihangSu,
#Numofretrieval
ShuqiZhu,YikeSun,MinZhang,andShaopingMa.2022. Websearchviaan
efficientandeffectivebrain-machineinterface.InProceedingsofthefifteenthACM
Dataset FLR TPR DRAD SRR internationalconferenceonwebsearchanddatamining.1569â€“1572.
NQ 3.41 1.67 0.48 1 [5] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,Gaurav
Mishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,Se-
2WMQA 9.61 5.29 1.40 1
bastianGehrmann,etal.2022.Palm:Scalinglanguagemodelingwithpathways.
StrategyQA 5.48 2.25 0.53 1 arXivpreprintarXiv:2204.02311(2022).
[6] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:
Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.arXiv
preprintarXiv:1810.04805(2018).
betweendifferentthresholdsofRHD(ğœƒ 1)on2WikiMultihopQAand [7] YanFang,JingtaoZhan,QingyaoAi,JiaxinMao,WeihangSu,JiaChen,andYiqun
StrategyQAisshowninTable5.Theexperimentalresultsdemon- Liu.2024. ScalingLawsForDenseRetrieval. arXivpreprintarXiv:2403.18684
(2024).
stratethatourmethodisnotsensitivetohyperparameters.There [8] MorGeva,DanielKhashabi,EladSegal,TusharKhot,DanRoth,andJonathan
isnosignificantperformancedifferenceintherangeof0.4to0.5. Berant.2021.Didaristotleusealaptop?aquestionansweringbenchmarkwith
implicitreasoningstrategies.TransactionsoftheAssociationforComputational
Itâ€™simportanttonotethatthethresholddirectlydeterminesthe Linguistics9(2021),346â€“361.
frequencyofinvokingtheretrievalmodule.Ahigherthreshold [9] KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.2020.
resultsinfewercallstotheretrieval.Inpracticalapplications,the
Retrievalaugmentedlanguagemodelpre-training.InInternationalconferenceon
machinelearning.PMLR,3929â€“3938.
threshold can be adjusted based on real-world requirements to
[10] HiroakiHayashi,PrashantBudania,PengWang,ChrisAckerson,RajNeervannan,
balanceefficiencyandeffectiveness. andGrahamNeubig.2021.WikiAsp:Adatasetformulti-domainaspect-based
summarization.TransactionsoftheAssociationforComputationalLinguistics9
5.2.3 EfficiencyofDRAD. Thissectionpresentstheexperimental (2021),211â€“225.
[11] XanhHo,Anh-KhoaDuongNguyen,SakuSugawara,andAkikoAizawa.2020.
resultsregardingtheefficiencyofvariousmulti-roundretrieval-
Constructingamulti-hopQAdatasetforcomprehensiveevaluationofreasoning
augmentedLLMsasshowninTable6.Wecomparedthenumberof steps.arXivpreprintarXiv:2011.01060(2020).
retrievalcallsmadebySRR,FLR,TPR,andDRADacrossdifferent [12] GautierIzacardandEdouardGrave.2020. Leveragingpassageretrievalwith
generative models for open domain question answering. arXiv preprint
datasets.TheresultsindicatethatFLR,duetoitsrequirementof arXiv:2007.01282(2020).
invokingretrievalforeverysentence,istheleastefficient.ForTPRâ€™s [13] ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,DanSu,YanXu,EtsukoIshii,
efficiencyliesbetweenthatofDRADandFLR.DRAD,however, YeJinBang,AndreaMadotto,andPascaleFung.2023.Surveyofhallucinationin
naturallanguagegeneration.Comput.Surveys55,12(2023),1â€“38.
demonstratesthefewestretrievalcalls,makingitthemostefficient. [14] ZhengbaoJiang,LuyuGao,JunAraki,HaiboDing,ZhiruoWang,JamieCallan,
ItisnoteworthythatDRADmakesfewerthanoneretrievalcall andGrahamNeubig.2022.Retrievalasattention:End-to-endlearningofretrieval
andreadingwithinasingletransformer.arXivpreprintarXiv:2212.02027(2022).
ontheNQandStrategyQAdatasets,yetitsperformancesurpasses
[15] ZhengbaoJiang,FrankFXu,LuyuGao,ZhiqingSun,QianLiu,JaneDwivedi-Yu,
thatofSingle-timeretrieval.Thisunderscoresthesuperiorityofthe YimingYang,JamieCallan,andGrahamNeubig.2023.Activeretrievalaugmented
DRADapproachoverotherretrieval-augmentedmodelsinboth generation.arXivpreprintarXiv:2305.06983(2023).
[16] UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMike
efficiencyandeffectiveness.
Lewis.2019.Generalizationthroughmemorization:Nearestneighborlanguage
models.arXivpreprintarXiv:1911.00172(2019).
6 CONCLUSIONSANDFUTUREWORKS [17] TomKwiatkowski,JennimariaPalomaki,OliviaRedfield,MichaelCollins,Ankur
Parikh,ChrisAlberti,DanielleEpstein,IlliaPolosukhin,JacobDevlin,Kenton
Inthisstudy,weintroduceDRADtomitigatehallucinationsof Lee,etal.2019.Naturalquestions:abenchmarkforquestionansweringresearch.
LLMsbyintegratingreal-timehallucinationdetection(RHD)and
TransactionsoftheAssociationforComputationalLinguistics7(2019),453â€“466.
[18] PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,
self-correction based on external knowledge (SEK). RHD moni- NamanGoyal,HeinrichKÃ¼ttler,MikeLewis,Wen-tauYih,TimRocktÃ¤schel,
torstheLLMâ€™soutputforpotentialhallucinationswithoutexternal etal.2020.Retrieval-augmentedgenerationforknowledge-intensivenlptasks.
AdvancesinNeuralInformationProcessingSystems33(2020),9459â€“9474.
models, while SEK retrieves relevant information to adjust the
[19] HaitaoLi,JiaChen,WeihangSu,QingyaoAi,andYiqunLiu.2023. Towards
LLMâ€™soutput,preventinghallucinations.Experimentsdemonstrate betterwebsearchperformance:pre-training,fine-tuningandlearningtorank.MitigatingEntity-LevelHallucinationinLargeLanguageModels Pre-print,UnderReview,V1.0
arXivpreprintarXiv:2303.04710(2023). [44] HarshTrivedi,NiranjanBalasubramanian,TusharKhot,andAshishSabharwal.
[20] HaitaoLi,WeihangSu,ChangyueWang,YueyueWu,QingyaoAi,andYiqunLiu. 2022. Interleavingretrievalwithchain-of-thoughtreasoningforknowledge-
2023.Thuir@coliee2023:Incorporatingstructuralknowledgeintopre-trained intensivemulti-stepquestions.arXivpreprintarXiv:2212.10509(2022).
languagemodelsforlegalcaseretrieval.arXivpreprintarXiv:2305.06812(2023). [45] XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,
[21] HaitaoLi,ChangyueWang,WeihangSu,YueyueWu,QingyaoAi,andYiqunLiu. AakankshaChowdhery,andDennyZhou.2022.Self-consistencyimproveschain
2023.THUIR@COLIEE2023:moreparametersandlegalknowledgeforlegal ofthoughtreasoninginlanguagemodels.arXivpreprintarXiv:2203.11171(2022).
caseentailment.arXivpreprintarXiv:2305.06817(2023). [46] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,
[22] JunyiLi,TianyiTang,WayneXinZhao,JingyuanWang,Jian-YunNie,andJi- QuocVLe,DennyZhou,etal.2022.Chain-of-thoughtpromptingelicitsreasoning
RongWen.2023.TheWebCanBeYourOysterforImprovingLargeLanguage inlargelanguagemodels.AdvancesinNeuralInformationProcessingSystems35
Models.arXivpreprintarXiv:2305.10998(2023). (2022),24824â€“24837.
[23] TianyuLiu,YizheZhang,ChrisBrockett,YiMao,ZhifangSui,WeizhuChen,and [47] ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RussRSalakhutdinov,
BillDolan.2021.Atoken-levelreference-freehallucinationdetectionbenchmark andQuocVLe.2019.Xlnet:Generalizedautoregressivepretrainingforlanguage
forfree-formtextgeneration.arXivpreprintarXiv:2104.08704(2021). understanding.Advancesinneuralinformationprocessingsystems32(2019).
[24] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer [48] ZiyiYe,XiaohuiXie,QingyaoAi,YiqunLiu,ZhihongWang,WeihangSu,and
Levy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019. Roberta:A MinZhang.2024.RelevanceFeedbackwithBrainSignals.ACMTransactionson
robustlyoptimizedbertpretrainingapproach.arXivpreprintarXiv:1907.11692 InformationSystems42,4(2024),1â€“37.
(2019). [49] ChengXiangZhai.2008.Statisticallanguagemodelsforinformationretrieval.
[25] XinyuMa,JiafengGuo,RuqingZhang,YixingFan,XiangJi,andXueqiCheng. Synthesislecturesonhumanlanguagetechnologies1,1(2008),1â€“141.
2021.Prop:Pre-trainingwithrepresentativewordspredictionforad-hocretrieval. [50] SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,Shuohui
InProceedingsofthe14thACMInternationalConferenceonWebSearchandData Chen,ChristopherDewan,MonaDiab,XianLi,XiVictoriaLin,etal.2022.Opt:
Mining.283â€“291. Openpre-trainedtransformerlanguagemodels.arXivpreprintarXiv:2205.01068
[26] YixiaoMa,YueyueWu,WeihangSu,QingyaoAi,andYiqunLiu.2023.CaseEn- (2022).
coder:AKnowledge-enhancedPre-trainedModelforLegalCaseEncoding.arXiv [51] ChuntingZhou,GrahamNeubig,JiataoGu,MonaDiab,PacoGuzman,Luke
preprintarXiv:2305.05393(2023). Zettlemoyer,andMarjanGhazvininejad.2020.Detectinghallucinatedcontentin
[27] PotsaweeManakul,AdianLiusie,andMarkJFGales.2023.Selfcheckgpt:Zero- conditionalneuralsequencegeneration.arXivpreprintarXiv:2011.02593(2020).
resourceblack-boxhallucinationdetectionforgenerativelargelanguagemodels.
arXivpreprintarXiv:2303.08896(2023).
[28] JoshuaMaynez,ShashiNarayan,BerndBohnet,andRyanMcDonald.2020.
Onfaithfulnessandfactualityinabstractivesummarization. arXivpreprint
arXiv:2005.00661(2020).
[29] OriRam,YoavLevine,ItayDalmedigos,DorMuhlgay,AmnonShashua,Kevin
Leyton-Brown,andYoavShoham.2023.In-contextretrieval-augmentedlanguage
models.arXivpreprintarXiv:2302.00083(2023).
[30] JuanRamosetal.2003.Usingtf-idftodeterminewordrelevanceindocument
queries.InProceedingsofthefirstinstructionalconferenceonmachinelearning,
Vol.242.Citeseer,29â€“48.
[31] StephenRobertson,HugoZaragoza,etal.2009. Theprobabilisticrelevance
framework:BM25andbeyond.FoundationsandTrendsÂ®inInformationRetrieval
3,4(2009),333â€“389.
[32] TevenLeScao,AngelaFan,ChristopherAkiki,ElliePavlick,SuzanaIliÄ‡,Daniel
Hesslow,RomanCastagnÃ©,AlexandraSashaLuccioni,FranÃ§oisYvon,Matthias
GallÃ©,etal.2022.Bloom:A176b-parameteropen-accessmultilinguallanguage
model.arXivpreprintarXiv:2211.05100(2022).
[33] HemlataShelar,GagandeepKaur,NehaHeda,andPoorvaAgrawal.2020.Named
entityrecognitionapproachesandtheircomparisonforcustomnermodel.Science
&TechnologyLibraries39,3(2020),324â€“337.
[34] WeijiaShi,SewonMin,MichihiroYasunaga,MinjoonSeo,RichJames,Mike
Lewis,LukeZettlemoyer,andWen-tauYih.2023.Replug:Retrieval-augmented
black-boxlanguagemodels.arXivpreprintarXiv:2301.12652(2023).
[35] IvanStelmakh,YiLuan,BhuwanDhingra,andMing-WeiChang.2022. Asqa:
Factoidquestionsmeetlong-formanswers. arXivpreprintarXiv:2204.06092
(2022).
[36] WeihangSu,QingyaoAi,XiangshengLi,JiaChen,YiqunLiu,XiaolongWu,and
ShengluanHou.2023.Wikiformer:Pre-trainingwithStructuredInformationof
WikipediaforAd-hocRetrieval.arXivpreprintarXiv:2312.10661(2023).
[37] WeihangSu,QingyaoAi,YueyueWu,YixiaoMa,HaitaoLi,andYiqunLiu.2023.
Caseformer:Pre-trainingforLegalCaseRetrieval.arXivpreprintarXiv:2311.00333
(2023).
[38] WeihangSu,YiranHu,AnzheXie,QingyaoAi,ZibingQue,NingZheng,YunLiu,
WeixingShen,andYiqunLiu.2024.STARD:AChineseStatuteRetrievalDataset
withRealQueriesIssuedbyNon-professionals.arXivpreprintarXiv:2406.15313
(2024).
[39] WeihangSu,XiangshengLi,YiqunLiu,MinZhang,andShaopingMa.2023.
Thuir2atntcir-16sessionsearch(ss)task.arXivpreprintarXiv:2307.00250(2023).
[40] WeihangSu,YichenTang,QingyaoAi,ZhijingWu,andYiqunLiu.2024.Dragin:
Dynamicretrievalaugmentedgenerationbasedonthereal-timeinformation
needsoflargelanguagemodels.arXivpreprintarXiv:2403.10081(2024).
[41] WeihangSu,ChangyueWang,QingyaoAi,YiranHu,ZhijingWu,YujiaZhou,
andYiqunLiu.2024.Unsupervisedreal-timehallucinationdetectionbasedonthe
internalstatesoflargelanguagemodels.arXivpreprintarXiv:2403.06448(2024).
[42] AmoghKamatTarcar,AashisTiwari,VineetNaiqueDhaimodker,PenjoRebelo,
RahulDesai,andDattarajRao.2019. HealthcareNERmodelsusinglanguage
modelpretraining.arXivpreprintarXiv:1910.11241(2019).
[43] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne
Lachaux,TimothÃ©eLacroix,BaptisteRoziÃ¨re,NamanGoyal,EricHambro,Faisal
Azhar,etal.2023.Llama:Openandefficientfoundationlanguagemodels.arXiv
preprintarXiv:2302.13971(2023).