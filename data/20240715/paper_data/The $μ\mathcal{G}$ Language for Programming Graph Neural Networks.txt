The ğœ‡G LanguageforProgrammingGraphNeuralNetworks
MATTEOBELENCHIA,FLAVIOCORRADINI,MICHELAQUADRINI,andMICHELELORETI,Uni-
versityofCamerino,Italy
Graphneuralnetworksformaclassofdeeplearningarchitecturesspecificallydesignedtoworkwithgraph-structureddata.Assuch,
theysharetheinherentlimitationsandproblemsofdeeplearning,especiallyregardingtheissuesofexplainabilityandtrustworthiness.
Weproposeğœ‡G,anoriginaldomain-specificlanguageforthespecificationofgraphneuralnetworksthataimstoovercometheseissues.
Thelanguageâ€™ssyntaxisintroduced,anditsmeaningisrigorouslydefinedbyadenotationalsemantics.Anequivalentcharacterization
intheformofanoperationalsemanticsisalsoprovidedand,togetherwithatypesystem,isusedtoprovethetypesoundnessofğœ‡G.
Weshowhowğœ‡Gprogramscanberepresentedinamoreuser-friendlygraphicalvisualization,andprovideexamplesofitsgenerality
byshowinghowitcanbeusedtodefinesomeofthemostpopulargraphneuralnetworkmodels,ortodevelopanycustomgraph
processingapplication.
CCSConcepts:â€¢Computingmethodologiesâ†’Neuralnetworks;Parallelprogramminglanguages;â€¢Mathematicsofcomputing
â†’Graphtheory;â€¢Theoryofcomputationâ†’Programsemantics;Typetheory;â€¢Softwareanditsengineeringâ†’Formalmethods;
Formallanguagedefinitions.
AdditionalKeyWordsandPhrases:GraphNeuralNetworks,Domain-SpecificLanguage,GraphDeepLearning
ACMReferenceFormat:
MatteoBelenchia,FlavioCorradini,MichelaQuadrini,andMicheleLoreti.2018.Theğœ‡GLanguageforProgrammingGraphNeural
Networks.J.ACM37,4,Article111(August2018),32pages.https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Deeplearningmodelsareattheforefrontofartificialintelligenceresearchtoday.Amongthem,artificialneuralnetworks
arethemostcommonlyusedclassofmodelsforawiderangeofdifferenttasks,includingnaturallanguageprocessing,
computervision,softwareengineering,andmanymore[29].Forapplicationswheretheinputscanberepresentedas
graphs,thegraphneuralnetwork[10](GNN)modelhasbeenthearchitectureofchoice,andhasachievedstate-of-the-art
performanceonmanytasks.
Despitethesepromisingadvancements,deeplearningmodels,includinggraphneuralnetworks,faceanumberof
issues.Thesesystemsaredifficulttoengineerwith,andarenotoriouslyhardertodebugandinterpretcomparedto
traditionalsoftwaresystems[23].Anotherissueisthelackofguaranteesthatthesesystemsofferregardingtheiroutputs,
whichtimeandagainhavebeenshowntobeeasilyfoolable,notonlyusingso-calledâ€œadversarialexamplesâ€[17],
butalsomoregenerallyinunpredictableandsurprisingways[3,12].Furthermore,thesesystemsactlikeablack-box
andareopaque,inthesensethathumanusersareunabletounderstandhowtheyhavereachedtheirconclusions[4].
Authorsâ€™address:MatteoBelenchia,matteo.belenchia@unicam.it;FlavioCorradini,flavio.corradini@unicam.it;MichelaQuadrini,michela.quadrini@
unicam.it;MicheleLoreti,michele.loreti@unicam.it,UniversityofCamerino,ViaAndreaDâ€™Accorso16,Camerino,Macerata,Italy,62032.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenot
madeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforcomponents
ofthisworkownedbyothersthantheauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,toposton
serversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
Â©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ManuscriptsubmittedtoACM
ManuscriptsubmittedtoACM 1
4202
luJ
21
]LF.sc[
1v14490.7042:viXra2 Belenchiaetal.
Finally,thereisaverystrongbiasindeeplearningresearchagainsttheusageofpriorknowledgeevenwhensuch
usageiswarranted,andeventhen,itisdifficulttofigureouthowtointegratesuchknowledgeinthesesystems[24].
Inthispaper,wetacklealltheseissuesbyproposinganewgraphneuralnetworkspecificationlanguage,calledğœ‡G
(pronouncedasâ€œmee-geeâ€).Agraphneuralnetworkinğœ‡GisbuiltupasacompositionofsimplerGNNsfollowing
theformalrulesofthelanguage.TheseGNNsarebuiltupfromthebaseterms,specifyingfunctionstobeappliedto
thesinglenodesintermsofthemselvesand/ortheirneighbors,whichcanthenbecomposedsequentially,inparallel,
selectedaccordingtoaBooleancondition,oriterated.
Thelanguageâ€™ssyntaxisintroducedusingacontext-freegrammar,andthemeaningofeachtermisformallyand
rigorouslydefinedbyadenotationalsemantics.Lateron,wealsointroduceastructuraloperationalsemantics,andprove
thatbothsemanticsareequivalent,i.e.,theydefinethesamegraphneuralnetworkoperations.Usingtheoperational
semanticsandafterhavingdefinedatypesystemforğœ‡G,weprovethetypesoundnessofthelanguage.Thetype
soundnessofğœ‡Gguaranteesthateverywell-typedğœ‡GprogramproperlydefinesaGNNwiththecorrectoutputtype.
Thelanguagecanbeusedbothintextualformandingraphicalform.Thegraphicalrepresentationofğœ‡Gprograms
isintroducedasamoreuser-friendlyapproachtousethelanguage,asitkeepstheflowofinformationandthetypeof
labelseasiertofollowandreasonabout.
Ourlanguageisframeworkandhardware-agnostic,asğœ‡Gcouldbeimplementedindifferentways.Weoptedto
implementğœ‡GinTensorFlowsothatitcanuseitsautomaticdifferentiationcapabilitiesandallowtheGNNsweprogram
tobeexecutableonCPUs,GPUs,orTPUswithoutchangingtheirimplementation.Furthermore,theseGNNscan
interoperatewithanyotherTensorFlowfeatureasiftheywereTensorFlowmodelsthemselves.
Weclaimthatğœ‡Ghelpswiththeaforementionedproblemsofdeeplearning.Forthematterofexplainabilityand
interpretability,usingğœ‡G helpsbymakingthecomputationsperformedmoreexplicit,bytakingtheformofağœ‡G
expressionwhichhasclearlydefinitesemanticsandtypes.Indeed,theusermightevendefinefunctionsandcomputations
usingthesameterminologyofthespecificdomaininwhichtheGNNisgoingtobeusedin,makingthepurposeof
eachtermeasiertounderstand.Some,orall,partsoftheGNNcanbedefinedtobeinherentlyinterpretable,byvirtue
ofbeingdefinedexplicitlybasedontheavailabledomainknowledge.Furthermore,determiningthenodes,edges,or
labelsthatcontributedtoaspecificpredictionbecomesamenabletostaticanalysistechniques[7].
Asfortheissuesoftrustworthiness,theuseofaformallanguagelikeğœ‡GallowstheformalverificationofGNNs
similarlytothatofotherprogramminglanguages,e.g.,byabstractinterpretation,symbolicexecution,data-flowanalysis,
andsoon.Inparticular,weareinterestedintheproblemofformallyverifyingsafetypropertiesofGNNs,e.g.output
reachabilityproperties[18].Theverificationofanoutputreachabilitypropertyforafunctionğ‘“ givenaninputregion
ğ‘‹ andanoutputregionğ‘Œ consistsincheckingwhetherforallinputsğ‘¥ âˆˆğ‘‹,ğ‘“(ğ‘¥) âˆˆğ‘Œ.Asfarasweknow,nosolution
forthisproblemhasbeenproposedforgraphneuralnetworksspecifically[33].Theverificationofpropertiesofthis
kindcanbeusedtoprovetherobustnessofaGNNagainstadversarialexamples,whichiscriticalwhensuchmodels
aredeployedinsafety-criticalsystems,whereguaranteesofcorrectfunctioningareparamount.
Finally,itiseasytoincludepriorknowledgewhendefiningaGNNinğœ‡G.Thebasictermsofthelanguageneednot
makeuseofneuralnetworklayersasistypicallythecaseforthemorepopulargraphneuralnetworkmodels,butcanin
generaluseanykindoffunction,notnecessarilydependingontrainableparameters.Thiswayisalsopossibletodefine
aGNNwhichdoesnotrequiretrainingatall,aswedidinapreviousworkwhereweusedğœ‡Gformodelchecking[6].
Prior,orinnate,knowledgecanbefreelymixedwithoptimizablefunctioninordertobuildhybrid,neural-symbolic
systems[22,25,32].
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 3
Contributions. Ourmaincontributionsare:
â€¢ Wedefinethesyntaxğœ‡Glanguage,andprovideitsdenotationalandoperationalsemantics.
â€¢ Weprovetheequivalenceofthedenotationalandoperationalsemanticsofğœ‡G.
â€¢ Wedefinethetypingrulesofğœ‡Gandproveitstypesoundness.
â€¢ Weshowhowğœ‡Gprogramscanberepresentedgraphically.
â€¢ Wedemonstratehowtouseğœ‡Gtoprogramsomeofthemostpopulargraphneuralnetworkmodels.
Structureofthepaper. WesurveytherelatedworkinSection2,whileweintroducethenecessarynotationand
preliminaryinformationinSection3and4.Thentheğœ‡GlanguageisdescribedindetailinSection5byshowingits
syntaxandsemantics.Aproofofequivalenceofthedenotationalandoperationalsemanticsofğœ‡G isinSection6,
followedbyaproofoftypesoundnessinSection7.Thegraphicalrepresentationofğœ‡Gprogramsisintroducedin
Section8.Abriefdiscussionontheimplementationofğœ‡G isinSection9.Finally,weevaluateğœ‡G byshowingits
applicationinCTLmodelcheckinganditsusagetodefinesomegraphneuralnetworkmodelsinSection10,and
delineatethefuturedirectionofourworkintheconclusions.
2 RELATEDWORK
Manydomainspecificlanguages(DSL)havebeendevelopedovertheyearstoeasethedevelopmentofmachinelearning
applications[28].Farfrombeingacompletesurvey,wewilldiscusshereonlythemostrecentonesorthosethatare
mostrelevantforourwork.
DeepDSL[37]isalanguagebasedonScalathatallowsthedefinitionofneuralnetworksthroughvariableassignments.
Variablescanbeassignedcomputationalmodulessuchasconvolutionallayers,poolinglayers,activationfunctionsand
soon.Neuralnetworksaredefinedbyvariableassignmentswheretheright-handsideisthefunctioncompositionof
othercomputationalmodules.ThecodeiscompiledtoJavaandusesJCudatocommunicatewithCUDAandcuDNN.
AiDSL [13] is a language for the specification of supervised feed-forward neural networks using a model driven
engineeringapproach.ThecodeiscompiledintoEncog,amachinelearningframeworkinJava.SEMKIS-DSL[19]is
anotherlanguagedevelopedusingamodeldrivenengineeringapproach,wherethemainfocusisshiftedfromthe
specificationoftheneuralnetworkarchitecturetotherequirementsthattheinputdatasetandtheneuralnetworkâ€™s
outputsmustsatisfy.Forthetimebeingtherearenocompilationtargetsforthelanguage,soitisnotpossibleto
automaticallyproduceaneuralnetworkfromspecification.Ontheotherhand,Diesel[11]actsatafinerlevelofdetail,
allowingthespecificationofcommonlinearalgebraandneuralnetworkcomputationsthataretypicallyimplemented
bylibrariessuchascuBLASandcuDNN.Itusesapolyhedralmodeltoscheduleoperationsandperformoptimizations,
andthesourcecodeiscompileddirectlyintoCUDAcode.Sofar,noneoftheselanguagestakeintoconsiderationthe
definitionofgraph-structuredinputsorthetypicaloperationsthatcharacterizegraphneuralnetworks.
ADSLwheregraphsarefirst-classobjectsisOptiML[31],whichsupportsthespecificationofmachinelearning
modelstoberunonheterogenoushardware(i.e.theCPUorGPU).Thelanguagecanbeusedtoimplementanymachine
learningmodelthatcanbeexpressedusingtheStatisticalQueryModel[20],andthecodecanbecompiledtoScala,
C++,orCUDAcode.GraphobjectsinOptiMLsupportoperationsexpressedintermsofverticesandedges,andgraph
operationsarespecifiedintermsofverticesandtheirneighboursinthegraph.Furthermore,OptiMLhasafixedpoint
operatorwithacustomizablethresholdvaluejustlikeğœ‡G.AnotherDSLthatsupportsgraphsisStreamBrain[27],which
isalanguageforthespecificationofBayesianConfidencePropagationNeuralNetworks.Thismodeltakesininputa
graphwhereeverynodeisarandomvariableandedgesrepresentthedependenciesbetweenvariables.StreamBrain
ManuscriptsubmittedtoACM4 Belenchiaetal.
hasaPythoninterfaceanditssyntaxissimilartothatofKeras,wherethemodelisseenasastackoflayers,anditcan
compiletoOpenMP,OpenCL,andCUDA.StreambrainisbasedonNumPyandsupportsCPUs,GPUsandFPGAs.
Despitethenativesupportforgraphs,neitherOptiMLnorStreamBrainfullysupportthegraphneuralnetwork
model,withthefirstbeinglimitedtostatisticalinferenceproblems,andthelattertoaspecifickindofneuralarchitecture
andgraphlabeling.Therefore,wecanconcludethat,atthetimeofwriting,nodomainspecificlanguagehasbeen
developedspecificallyforgraphneuralnetworkmodelsandtasks.
3 BACKGROUNDANDNOTATION
Graphsandtheirlabelings. AdirectedgraphisapairG=(V,E)whereVisacollectionofvertices(whichwealso
refertoasnodes)andEisacollectionoforderedpairsofvertices,callededges.Foranyğ‘¢,ğ‘£ âˆˆV,whenever(ğ‘¢,ğ‘£) âˆˆE
wesaythatğ‘¢isapredecessorofğ‘£ andthatğ‘£ isasuccessorofğ‘¢.Wealsosaythat(ğ‘¢,ğ‘£)isanincomingedgeforğ‘£ and
â†âˆ’ â†âˆ’
anoutgoingedgeforğ‘¢.Moreover,letğ‘ G(ğ‘£)denotethesetofpredecessorsofğ‘£,formallyğ‘ G(ğ‘£) = {ğ‘¢ | (ğ‘¢,ğ‘£) âˆˆ E},
â†’âˆ’ â†’âˆ’
whileğ‘ G(ğ‘¢)denotesthesetofsuccessorsofğ‘¢,namelyğ‘ G(ğ‘¢)={ğ‘£ | (ğ‘¢,ğ‘£) âˆˆE}.Similarly,letğ¼ G(ğ‘£)denotethesetof
incomingedgesforğ‘£,formallyğ¼ G(ğ‘£)={(ğ‘¢,ğ‘£) |ğ‘¢,ğ‘£ âˆˆV},andletğ‘‚ G(ğ‘¢)denotethesetofoutgoingedgesforğ‘¢,formally
ğ‘‚ G(ğ‘¢)={(ğ‘¢,ğ‘£) |ğ‘¢,ğ‘£ âˆˆV}.
Sometimesitisusefultoassociatenodesandedgeswithvalues.GivenagraphG = (V,E),wecanconsiderits
node-labelingandedge-labeling.Theformerisafunctionğœ‚ :V â†’ğ‘‡
V
associatingeachnodeğ‘£ âˆˆVwithavalueinthe
setğ‘‡ V.Similarly,anedge-labelingisafunctionğœ‰ : E â†’ğ‘‡
E
thatmapsanedgetoitslabelinthesetğ‘‡ E.Letğ‘‰ âŠ† V
(resp.ğ¸ âŠ†E),weletğœ‚(ğ‘‰)(resp.ğœ‰(ğ¸))denotethemulti-setoflabelsassociatedtotheelementsofğ‘‰ (resp.ğ¸)byğœ‚(resp.
ğœ‰).Likewise,welet(ğœ‚,ğœ‰)(ğ¸)denotethemulti-setoftuples(ğœ‚(ğ‘¢),ğœ‰((ğ‘¢,ğ‘£)),ğœ‚(ğ‘£))foreach(ğ‘¢,ğ‘£) âˆˆğ¸.
Encodingofgraphsandlabels. Onewaytorepresentgraphsandtheirlabelingfunctionsonacomputerisintheform
ofamatrixofnodefeaturesX,anadjacencymatrixAandamatrixofedgefeaturesE.ThenodefeaturesmatrixX
storesthefeaturesassociatedwitheachnodeinthegraph.Theğ‘–-throwofthematrixcontainsavalueğ‘¥
ğ‘–
âˆˆğ‘‡
V
that
representstheinformationassociatedwiththeğ‘–-thvertexofthegraphinagivenordering.TheadjacencymatrixA
encodesthearchitectureofthegraph,witheachnon-zeroelementğ‘
ğ‘–ğ‘—
âˆˆRdenotingthepresenceofanedgefromanode
ğ‘–toanodeğ‘—.TheedgefeaturesmatrixEstoresthefeaturesassociatedwitheachedge,likethenodefeaturesmatrix.
Theğ‘–-throwofEisavalueğ‘’
ğ‘–
âˆˆğ‘‡
E
thatrepresentstheinformationassociatedwiththeğ‘–-thedgeintherow-major(or
anyother)orderingoftheedgesinA.
GraphNeuralNetworks. Agraphneuralnetworkisadeeplearningmodelthatoperatesongraph-structureddata.
IntroducedbyScarsellietal.[30],itemergedtoovercomethelimitationsofgraphkernelmethods[26].GNNsgeneralize
manyotherclassesofdeeplearningarchitectures,asotherdeeplearningmodelscanbeseenasaparticularcase
ofgraphneuralnetworks[8].ConvolutionalNeuralNetworks(CNNs),forexample,canbeseenasagraphneural
networkswhereinputscanonlybe1-dimensionalsequences(suchastexts)or2-dimensionalgrids(suchasimages),
whicharebothparticularinstancesofgraphs.Graphneuralnetworks,moregenerally,canlearnonanynon-Euclidean
structurerepresentableasgraphs,whichcontrarytogridssuchasimages,canhavedifferingshapes,numberofnodes
andconnectivityproperties.
Graphneuralnetworkscanbeusedformanytasks,whichcanberoughlycategorizedaccordingtothesubjectof
prediction:nodes,edges,orentiregraphs.Aswithothermachinelearningmodels,therearetwomainkindsoftasks,
namelyclassificationandregression,wherethegoalistopredicttheclassorsomenumericalquantityassociatedwith
thenodesinthegraph,edges,ortheentiregraph.
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 5
Overtheyears,manytypesofgraphneuralnetworkvariantshavebeendeveloped[38].Amongthese,wemention
GraphConvolutionalNetworks[21],GraphIsomorphismNetworks[36],andGraphAttentionNetworks[34].Graph
neuralnetworkshavebeenappliedtothemostdisparatedomains[38],spanningcomputervision,naturallanguage
processing,particlephysics,chemistry,combinatorialoptimization,recommendersystems,trafficforecastingandgraph
mining.
Themostgeneralformofgraphneuralnetwork,whichsubsumesmostoftheGNNvariantsthathavebeendeveloped
overtheyears[8],isthemessagepassingneuralnetwork[5,14](MPNN).AtypicalMPNNcomprisesafixednumber
ofconvolutionallayersstackedinsequence,eachofthemupdatingthelabelsofthenodes.Aconvolutionoperation
computesthenewnodelabelsğ‘¥â€²fromthecurrentnodelabelsğ‘¥ accordingtothefollowingequation:
ğ‘– ğ‘–
(cid:16) (cid:17)
ğ‘¥ ğ‘–â€² =ğœ“ ğ‘¥ ğ‘–,ğœ ğ‘—âˆˆğ‘ G(ğ‘–)ğœ‘(ğ‘¥ ğ‘—,ğ‘’ ğ‘—ğ‘–,ğ‘¥ ğ‘–) (1)
whereğ‘ (ğ‘–)isthesetofneighborsofnodeğ‘–,ğœisapermutation-invariantfunctionandğœ“,ğœ‘are(usually)optimizable
G
functions.Thefunctionğœ‘generatesthemessagesentfromnodeğ‘—tonodeğ‘–,whichinthesimplestcasemightsimplybe
ğ‘¥ .Thefunctionğœaggregatesthemultisetofmessagesobtainedbyğœ‘inawaythatisindependentoftheordertheyare
ğ‘—
evaluatedandisusuallyanon-learnablefunctionsuchasthesum,productormean,but,moregenerally,itcanalsobe
definedasaneuralnetwork.Thefunctionğœ“ updatesthelabelofnodeğ‘–byconsideringboththecurrentlabelğ‘¥ andthe
ğ‘–
valuecomputedbyğœ.
GraphneuralnetworkscomposedbylayersdescribedbyEquation1arepermutation-equivariantfunctionsthattake
ininputagraphG,anedge-labelingğœ‰,andanode-labelingğœ‚toreturnanewnode-labelingğœ‚â€².Thischaracterization
leavesoutsometypesofgraphneuralnetworks,namely,thegraphneuralnetworksthatusepoolinglayers(which
thereforechangethearchitectureofthegraphbyremovingelementsfromVandE)andthegraphneuralnetworks
thatlearnedgefeaturesratherthannodefeatures(whichthereforeproduceanewedge-labelingğœ‰â€²instead).Atany
rate,thestandardgraphneuralnetworkmodelwehavejustdescribedisgeneralenoughtoencompassthesetwo
specialcasesaswell.Agraphneuralnetworkthatusespoolinglayerscanberepresentedbyallowingthenode-labeling
functiontolabeleachnodewithanadditionalBooleanvaluethatspecifieswhetherthenodehasbeendeletedornot
andbyrulingthatanedgeisvalidifandonlyifboththesourceanddestinationnodeshavenotbeendeleted.Agraph
neuralnetworkthatlearnsedgefeaturescaninsteadbemodeledbyreifyingedgesintonodesandbyhavingthegraphâ€™s
node-labelingfunctionlabeleachnodewithanadditionalBooleanvaluethatspecifieswhetherthenoderepresentsan
edgeoranodeoftheoriginalgraph.
4 THEDOMAINOFGRAPHNEURALNETWORKS
Inthissection,weintroducethenecessarydefinitionsandtheoremsthatcharacterizegraphneuralnetworksinour
work.Weelaboratefurtheronthenotionofnode-labelingfunctionandformalizethegraphneuralnetworkasa
transformationbetweennode-labelingfunctions,thenmoveontoprovethatthesetofgraphneuralnetworksisa
chaincompletepartiallyorderedset.
4.1 Node-labelingfunctions
Wedenotethesetofalllabelingfunctionswithco-domainğ‘‡ asğ»[ğ‘‡],andwesaythatthissetspecifiesitstype.As
examples,labelingfunctionsoftypeğ»[B]mapverticestoBooleanvalues,whilefunctionsoftypeğ»[Nğ‘˜]withğ‘˜ âˆˆN
mapverticestoğ‘˜-tuplesofnaturalnumbers.Eachtypealsoincludesabottomnode-labelingfunctionâŠ¥ğ» =ğœ†ğ‘£.undef
ManuscriptsubmittedtoACM6 Belenchiaetal.
thatisundefinedforeverynode.Then(ğ»[ğ‘‡],âŠ‘ğ»)isapartiallyorderedset,whereâŠ¥ğ» isthebottomelementandâŠ‘ğ»
isthepointwiseorderingrelationsuchthat,forallğ‘£ âˆˆV
ğœ‚1 âŠ‘ğ» ğœ‚2 â‡â‡’ ğœ‚1(ğ‘£)=ğ‘¡ =â‡’ ğœ‚2(ğ‘£)=ğ‘¡
Wealsoconsidertheparallelcompositionofnode-labelingfunctionsğœ‚1andğœ‚2,andwedenoteitasğœ‚1|ğœ‚2,tospecify
thenode-labelingfunctionğœ†ğ‘£.(ğœ‚1(ğ‘£),ğœ‚2(ğ‘£))thatmapsverticesto(possiblynested)pairsofnodelabels.Theinverse
operationisgivenbytheprojectionfunctionsğœ‹ ğ¿,ğœ‹
ğ‘…
:ğ»[ğ‘‡ Ã—ğ‘‡] â†’ğ»[ğ‘‡]suchthat
ğœ‹ ğ¿(ğœ‚1|ğœ‚2)=ğœ‚1
ğœ‹ ğ‘…(ğœ‚1|ğœ‚2)=ğœ‚2
Wecallğœ‹ ğ¿theleftprojectionandğœ‹ ğ‘…therightprojection.Adequatecompositionofthesetwoprojectionfunctionscanbe
usedtoobtainanynestednode-labelingfunction.
4.2 Graphneuralnetworks
Given a graph G together with its edge-labeling function ğœ‰, we define a graph neural network to be a function
ğœ™
G,ğœ‰
:ğ»[ğ‘‡1] â†’ğ»[ğ‘‡2]thatmapsanode-labelingfunctiontoanothernode-labelingfunction.Wedenotethesetofsuch
graphneuralnetworksasÎ¦ G,ğœ‰[ğ‘‡1,ğ‘‡2]or,moresuccinctly,asÎ¦ G,ğœ‰.ThesubscriptG,ğœ‰indicatesthateachgraphneural
networkisparametrizedbyagraphandanedge-labelingfunction.Likewiseforthenode-labelingfunctions,thesetof
graphneuralnetworksÎ¦ G,ğœ‰[ğ‘‡1,ğ‘‡2]isapartiallyorderedsetwithapoint-wiseorderingrelationâŠ‘Î¦
G,ğœ‰[ğ‘‡1,ğ‘‡2]
suchthat,
forallğœ‚ âˆˆğ»[ğ‘‡1]
ğœ™1 âŠ‘Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2] ğœ™2 â‡â‡’ ğœ™1(ğœ‚)=ğœ‚â€² =â‡’ ğœ™2(ğœ‚)=ğœ‚â€²
Whenthetypesareclearfromthecontextornotrevelant,wedropthesubscriptandsimplywriteâŠ‘Î¦ .
G,ğœ‰
Next,wedefinetheunderlyingrelationofaGNNasthesetofinput-outputtupleswhichcharacterizetheGNN.
Buildingonthisconcept,wealsospecifytheirsequentialandparallelcomposition.
Definition4.1. Givenagraphneuralnetworkğœ™ :Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2]wedefineitsunderlyingrelation,denotedbyğ‘Ÿğ‘’ğ‘™(ğœ™),as
ğ‘Ÿğ‘’ğ‘™(ğœ™)={(ğœ‚1,ğœ‚2) âˆˆğ»[ğ‘‡1]Ã—ğ»[ğ‘‡2] |ğœ™(ğœ‚1)=ğœ‚2}
Thesequentialcomposition,orsimplycomposition,ofrelationsğ´:ğ»[ğ‘‡1]Ã—ğ»[ğ‘‡2]andğµ:ğ»[ğ‘‡2]Ã—ğ»[ğ‘‡3],denotedby
ğ´âˆ—ğµ,isdefinedas
ğ´âˆ—ğµ={(ğœ‚1,ğœ‚3) |âˆƒğœ‚2 âˆˆğ»[ğ‘‡2] :(ğœ‚1,ğœ‚2) âˆˆğ´âˆ§(ğœ‚2,ğœ‚3) âˆˆğµ}
Theparallelcomposition,orconcatenation,ofrelationsğ´:ğ»[ğ‘‡1]Ã—ğ»[ğ‘‡2]andğµ:ğ»[ğ‘‡1]Ã—ğ»[ğ‘‡3],denotedbyğ´âŒ¢ğµ,is
definedas
ğ´âŒ¢ğµ={(ğœ‚1,(ğœ‚2,ğœ‚3)) | (ğœ‚1,ğœ‚2) âˆˆğ´âˆ§(ğœ‚1,ğœ‚3) âˆˆğµ}
ThefollowinglemmadefinestherelationshipbetweentherelationsandtheorderingofGNNs.
Lemma4.2. GiventwoGNNsğœ™1,ğœ™2wehave
ğœ™1 âŠ‘Î¦ ğœ™2 â‡â‡’ ğ‘Ÿğ‘’ğ‘™(ğœ™1) âŠ†ğ‘Ÿğ‘’ğ‘™(ğœ™2)
G,ğœ‰
Proof. First we show thatğœ™1 âŠ‘Î¦ ğœ™2 =â‡’ ğ‘Ÿğ‘’ğ‘™(ğœ™1) âŠ† ğ‘Ÿğ‘’ğ‘™(ğœ™2). Sinceğœ™1(ğœ‚) = ğœ‚â€² =â‡’ ğœ™2(ğœ‚) = ğœ‚â€², any
G,ğœ‰
(ğœ‚,ğœ‚â€²) âˆˆğ‘Ÿğ‘’ğ‘™(ğœ™1)isalsoamemberofğ‘Ÿğ‘’ğ‘™(ğœ™2).
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 7
Fromtheotherdirection,supposethatğ‘Ÿğ‘’ğ‘™(ğœ™1) âŠ†ğ‘Ÿğ‘’ğ‘™(ğœ™2).Since(ğœ‚,ğœ‚â€²) âˆˆğ‘Ÿğ‘’ğ‘™(ğœ™1) =â‡’ (ğœ‚,ğœ‚â€²) âˆˆğ‘Ÿğ‘’ğ‘™(ğœ™2),wehave
thatâˆ€(ğœ‚,ğœ‚â€²) âˆˆğ‘Ÿğ‘’ğ‘™(ğœ™1),ğœ™1(ğœ‚)=ğœ‚â€² =â‡’ ğœ™2(ğœ‚)=ğœ‚â€². â–¡
Aswasthecaseforthenode-labelingfunctions,weconsidertheparallelcompositionofgraphneuralnetworks
ğœ™1:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡1]andğœ™2:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡2],andwedenoteitasğœ™1Ã—ğœ™2,todefinethegraphneuralnetwork
ğœ†ğ‘’.ğœ†ğ‘£.(ğœ™1(ğ‘’)(ğ‘£),ğœ™2(ğ‘’)(ğ‘£))
withtypeÎ¦ G,ğœ‰[ğ‘‡,ğ‘‡1Ã—ğ‘‡2]thatconcatenatesthenode-labelingsgeneratedbyğœ™1andğœ™2.Thentherelationshipbetween
theparallelcompositionoflabelingfunctionsandtheparallelcompositionofgraphneuralnetworksishighlightedby
theequation:
ğœ™1Ã—ğœ™2(ğœ‚)=ğœ™1(ğœ‚)|ğœ™2(ğœ‚)
Next,weprovethatthesetofgraphneuralnetworksofagiventypeÎ¦ G,ğœ‰[ğ‘‡1,ğ‘‡2]withtheorderingrelationâŠ‘Î¦
G,ğœ‰
is
achaincompletepartiallyorderedset(ccpo).
Theorem4.3. Thesetofgraphneuralnetworksisachaincompletepartiallyorderedset (Î¦ G,ğœ‰,âŠ‘Î¦ G,ğœ‰),withleast
elementâŠ¥Î¦ :ğ»[ğ‘‡1] â†’ğ»[ğ‘‡2]definedby
G,ğœ‰
âŠ¥Î¦ (ğœ‚)=undef
G,ğœ‰
Proof. FirstweshowthatâŠ‘Î¦ fulfilstherequirementstoapartialorder.Wehavethatğœ™ âŠ‘Î¦ ğœ™becauseğœ™(ğœ‚)=ğœ‚â€²
G,ğœ‰ G,ğœ‰
impliesğœ™(ğœ‚)=ğœ‚â€²,thereforewesatisfythereflexivityrequirement.Thentoprovetransitivity,weassumethatğœ™1 âŠ‘Î¦ ğœ™2
G,ğœ‰
andğœ™2 âŠ‘Î¦ ğœ™3andshowthatğœ™1 âŠ‘Î¦ ğœ™3.Ifğœ™1(ğœ‚)=ğœ‚â€²,wegetthatğœ™2(ğœ‚)=ğœ‚â€²fromğœ™1 âŠ‘Î¦ ğœ™2.Thenwealsohave
G,ğœ‰ G,ğœ‰ G,ğœ‰
ğœ™3(ğœ‚) =ğœ‚â€² fromğœ™2 âŠ‘Î¦ ğœ™3.Lastly,toprovethattheorderingisanti-symmetric,weassumethatğœ™1 âŠ‘Î¦ ğœ™2and
G,ğœ‰ G,ğœ‰
ğœ™2 âŠ‘Î¦ ğœ™1andweneedtoshowthatğœ™1=ğœ™2.Ifğœ™1(ğœ‚)=ğœ‚â€²,thenbyğœ™1 âŠ‘Î¦ ğœ™2wehavethatğœ™2(ğœ‚)=ğœ‚â€².Likewiseif
G,ğœ‰ G,ğœ‰
ğœ™1(ğœ‚)isundefined,thenğœ™2(ğœ‚)mustbeundefinedaswell,otherwiseğœ™2(ğœ‚)=ğœ‚â€²andğœ™2 âŠ‘Î¦ ğœ™1areincontradiction.
G,ğœ‰
Thereforeğœ™1andğœ™2areequalonanyğœ‚.
Toseethat (Î¦ G,ğœ‰,âŠ‘Î¦ G,ğœ‰) isaccpo,weneedtoshowthatforallchainsğ‘Œ,theleastupperbound(cid:195)ğ‘Œ exists.Let
ğ‘Ÿğ‘’ğ‘™((cid:195)ğ‘Œ)=(cid:208){ğ‘Ÿğ‘’ğ‘™(ğœ™) |ğœ™ âˆˆğ‘Œ}.Wefirstshowthat(cid:208){ğ‘Ÿğ‘’ğ‘™(ğœ™) |ğœ™ âˆˆğ‘Œ}indeedspecifiesagraphneuralnetwork.Thatis,
whenever(ğœ‚,ğœ‚â€²)and(ğœ‚,ğœ‚â€²â€²)aremembersofğ‘‹ =(cid:208){ğ‘Ÿğ‘’ğ‘™(ğœ™) |ğœ™ âˆˆğ‘Œ},thenğœ‚â€² =ğœ‚â€²â€².If(ğœ‚,ğœ‚â€²) âˆˆğ‘‹,thentheremustbe
ağœ™ âˆˆğ‘Œ suchthatğœ™(ğœ‚) =ğœ‚â€²,andsimilarlyfor(ğœ‚,ğœ‚â€²â€²) âˆˆğ‘‹,theremustbeağœ™â€² âˆˆğ‘Œ suchthatğœ™â€²(ğœ‚) =ğœ‚â€²â€².Sinceğ‘Œ isa
chain,theneitherğœ™ âŠ‘Î¦ ğœ™â€²orğœ™â€² âŠ‘Î¦ ğœ™.Inanycase,thismeansthatğœ™(ğœ‚)=ğœ™â€²(ğœ‚)andğœ‚â€² =ğœ‚â€²â€².Nextweshowthat
G,ğœ‰ G,ğœ‰
(cid:195)ğ‘Œ aswedefineditisanupperboundofğ‘Œ.Letğœ™beamemberofğ‘Œ.Clearly,ğœ™ âŠ‘Î¦ (cid:195)ğ‘Œ,becauseğ‘Ÿğ‘’ğ‘™(ğœ™) âŠ†ğ‘Ÿğ‘’ğ‘™((cid:195)ğ‘Œ).
G,ğœ‰
Lastly,weprovethat(cid:195)ğ‘Œ istheleastupperboundofğ‘Œ.Letğœ™1 beanupperboundofğ‘Œ.Bydefinition,ğœ™ âŠ‘Î¦ ğœ™1
G,ğœ‰
forallğœ™ âˆˆğ‘Œ,andğ‘Ÿğ‘’ğ‘™(ğœ™) âŠ† ğ‘Ÿğ‘’ğ‘™(ğœ™1).Thenitmustalsobethecasethat(cid:208){ğ‘Ÿğ‘’ğ‘™(ğœ™) | ğœ™ âˆˆğ‘Œ} âŠ† ğ‘Ÿğ‘’ğ‘™(ğœ™1),andtherefore
ğ‘Ÿğ‘’ğ‘™((cid:195)ğ‘Œ) âŠ†ğ‘Ÿğ‘’ğ‘™(ğœ™1).Then(cid:195)ğ‘Œ âŠ‘Î¦ ğœ™1anditistheleastupperboundofğ‘Œ.
G,ğœ‰
ThelaststepoftheproofistoshowthatâŠ¥Î¦
G,ğœ‰
istheleastelementofÎ¦ G,ğœ‰.ItisindeedamemberofÎ¦
G,ğœ‰
and
âŠ¥Î¦ âŠ‘Î¦ ğœ™ forallğœ™,sinceâŠ¥Î¦ (ğœ‚)=ğœ‚â€²implies(vacuously)thatğœ™(ğœ‚)=ğœ‚â€². â–¡
G,ğœ‰ G,ğœ‰ G,ğœ‰
5 THEğœ‡GLANGUAGEFORGRAPHNEURALNETWORKS
Inthissection,weintroducethesyntaxofğœ‡Gasaprogramminglanguageforthedefinitionofgraphneuralnetworks.
Afterspecifyingitssyntax(Definition5.1),weshowitsdenotationalsemantics(Definition5.2),andstructuraloperational
semantics(Definition5.8).
ManuscriptsubmittedtoACM8 Belenchiaetal.
Definition5.1(Syntaxofğœ‡G). GivenasetSoffunctionsymbols,wedefineanalgebraofgraphneuralnetworks
withthefollowingabstractsyntax:
N ::=ğœ„ |ğœ“ |â—ğœ‘ ğœ |â–·ğœ‘ ğœ |N1;N2 |N1||N2 |N1âŠ•N2 |Nâ˜…
withğœ‘,ğœ,ğœ“ âˆˆS.Theoperatorprecedencerulesgivenbyâ˜…>; > || >âŠ•andparenthesesareintroducedtothesyntax
tomakethemeaningofexpressionsunambiguous.
GivenagraphGandanedge-labelingğœ‰,themeaningofağœ‡Gexpressionisagraphneuralnetwork,afunction
betweennode-labelingfunctions.Thetermğœ„ representstheapplicationoftheidentity GNNthatleavesthenode
labelsunaltered.Anotherofthebasicğœ‡Gtermsisthefunctionapplicationğœ“.ThisrepresentstheGNNthatappliesthe
ğœ‘ ğœ‘
functionreferencedbyğœ“.Moreover,thepre-imagetermâ—
ğœ
andthepost-imagetermâ–·
ğœ
defineaGNNthatcomputesthe
labelingofanodeintermsofthelabelsofitspredecessorsandsuccessors,respectively.TwoGNNscanbecomposedby
sequentialcompositionN1;N2andparallelcompositionN1||N2.ThechoiceoperatorN1âŠ•N2allowstorundifferent
GNNsaccordingtothevaluesofanode-labelingfunction.Finally,thestaroperatorNâ˜…isusedtoprogramrecursive
behavior.
5.1 Denotationalsemantics
Havingdefinedthesyntax,wearenowreadytointroducethedenotationalsemanticsofğœ‡G.
Definition5.2. (Denotationalsemantics)GivenagraphGandanedge-labelingfunctionğœ‰,wedefinethesemantic
interpretationfunctionSğ‘‘ğ‘ [[Â·]]G,ğœ‰ :N â†’Î¦
G,ğœ‰
onğœ‡GformulasN byinductioninthefollowingway:
Sğ‘‘ğ‘ [[ğœ„]]G,ğœ‰ =ğ‘–ğ‘‘
Sğ‘‘ğ‘ [[ğœ“]]G,ğœ‰ (ğœ‚)=ğœ†ğ‘£.ğ‘“ ğœ“(ğœ‚(V),ğœ‚(ğ‘£))
Sğ‘‘ğ‘ [[â—ğœ‘ ğœ]]G,ğœ‰ (ğœ‚)=ğœ†ğ‘£.ğ‘“ ğœ(ğ‘“ ğœ‘((ğœ‚,ğœ‰)(ğ¼ G(ğ‘£))),ğœ‚(ğ‘£))
Sğ‘‘ğ‘ [[â–·ğœ‘ ğœ]]G,ğœ‰ (ğœ‚)=ğœ†ğ‘£.ğ‘“ ğœ(ğ‘“ ğœ‘((ğœ‚,ğœ‰)(ğ‘‚ G(ğ‘£))),ğœ‚(ğ‘£))
Sğ‘‘ğ‘ [[N1;N2]]G,ğœ‰ =Sğ‘‘ğ‘ [[N2]]G,ğœ‰ â—¦Sğ‘‘ğ‘ [[N1]]G,ğœ‰
Sğ‘‘ğ‘ [[N1||N2]]G,ğœ‰ =Sğ‘‘ğ‘ [[N1]]G,ğœ‰ Ã—Sğ‘‘ğ‘ [[N2]]G,ğœ‰
Sğ‘‘ğ‘ [[N1âŠ•N2]]G,ğœ‰ =ğ‘ğ‘œğ‘›ğ‘‘(ğœ‹ ğ¿,Sğ‘‘ğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ‘…,Sğ‘‘ğ‘ [[N2]]G,ğœ‰ â—¦ğœ‹ ğ‘…)
Sğ‘‘ğ‘ [[Nâ˜… ]]G,ğœ‰ =ğ¹ğ¼ğ‘‹(ğœ†ğœ™.ğ‘ğ‘œğ‘›ğ‘‘(ğœ†ğ‘’.ğœ†ğ‘£.Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğ‘’)(ğ‘£)â‰ƒğœ– ğ‘’(ğ‘£),ğ‘–ğ‘‘,ğœ™â—¦Sğ‘‘ğ‘ [[N]]G,ğœ‰ ))
foranyğœ“,ğœ‘,ğœ âˆˆS.Thefunctionsğ‘ğ‘œğ‘›ğ‘‘andğ¹ğ¼ğ‘‹ aredefinedas:
ğ‘ğ‘œğ‘›ğ‘‘(ğ‘¡,ğ‘“1,ğ‘“2)(ğœ‚)=ï£±ï£´ï£´ï£²ğ‘“1(ğœ‚) ifğ‘¡(ğœ‚)=ğœ†ğ‘£.True
ï£´ï£´ğ‘“2(ğœ‚) otherwise
ï£³
ğ¹ğ¼ğ‘‹(ğ‘“)=(cid:196) {ğ‘“ğ‘› (âŠ¥Î¦ ) |ğ‘› â‰¥0}
G,ğœ‰
whereğ‘“0 =ğ‘–ğ‘‘andğ‘“ğ‘›+1 =ğ‘“ â—¦ğ‘“ğ‘›.Foranylabeltypeğ‘‡ andanyrealvalueğœ– âˆˆR,wedefineabinarypredicateâ‰ƒğœ– such
that
ğ‘¥ â‰ƒğœ– ğ‘¦ â‡â‡’ |ğ‘¥âˆ’ğ‘¦| â‰¤ğœ–
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 9
Additionally,weprovidethesemanticsforatermoftheformN1âŠ—N2whichwillbeusefullaterintheproofofthe
equivalencewiththestructuraloperationalsemantics
Sğ‘‘ğ‘ [[N1âŠ—N2]]G,ğœ‰ =(Sğ‘‘ğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ¿)Ã—(Sğ‘‘ğ‘ [[N2]]G,ğœ‰ â—¦ğœ‹ ğ‘…)
Inthefollowingparagraphs,weclarifythemeaningofğœ‡Gexpressions.
Identityapplication. Thetermğœ„isevaluatedastheidentitygraphneuralnetworkthatreturnstheinputnode-labeling
functionasis.
Function application. A function symbolğœ“1,ğœ“2,... âˆˆ S is evaluated as the graph neural network that maps a
node-labelingtoanewnode-labelingbyapplyingthecorrespondingfunctionğ‘“1,ğ‘“2,...onbothlocalandglobalnode
information.Thelocalinformationisthelabelofeachindividualnode,whiletheglobalinformationisthemultisetof
thelabelsofallthenodesinthegraph.Thegraphneuralnetworkweobtainappliesa(possiblytrainable)functionğ‘“ to
thesetwopiecesofinformation.Twoparticularcasesariseifthefunctionignoreseitherofthetwoinputs.Ifğ‘“ ignores
theglobalinformation,theGNNreturnsanode-labelingfunctionğœ‚ ,apurelylocaltransformationofthenodelabels.
ğ‘™
Ontheotherhand,ifğ‘“ ignoresthelocalinformation,theGNNreturnsanode-labelingfunctionğœ‚ thatassignsto
ğ‘”
eachnodealabelthatsummarizestheentiregraph,emulatingwhatintheGNNliteratureisknownasaglobalpooling
operator[16].
Pre-imageandPost-Image. Thepre-imageâ—andthepost-imageâ–·,togetherwithfunctionsymbolsğœ‘,ğœ âˆˆ Sare
evaluatedasthegraphneuralnetworksSğ‘‘ğ‘ [[â—ğœ‘ ğœ]]G,ğœ‰ andSğ‘‘ğ‘ [[â–·ğœ‘ ğœ]]G,ğœ‰.Inthecaseofthepre-image,foranysymbol
ğœ‘ âˆˆSthecorrespondingfunctionğ‘“ generatesamessagefromtuples(ğœ‚(ğ‘¢),ğœ‰((ğ‘¢,ğ‘£)),ğœ‚(ğ‘£))foreach(ğ‘¢,ğ‘£) âˆˆğ¼ G(ğ‘£).Then
foranysymbolğœ âˆˆSthecorrespondingfunctionğ‘”generatesanewlabelforanodeğ‘£fromthemultisetofincoming
messagesforğ‘£ obtainedfromğ‘“ andthecurrentlabelğœ‚(ğ‘£).Thefunctionsğ‘“ andğ‘”maybetrainable.Thecaseofthe
post-imageisanalogous,withthedifferencethatğ‘“ isappliedtotuples(ğœ‚(ğ‘£),ğœ‰((ğ‘£,ğ‘¢)),ğœ‚(ğ‘¢))foreach(ğ‘£,ğ‘¢) âˆˆğ‘‚ (ğ‘£)
G
instead.
Sequentialcomposition. AnexpressionoftheformN1;N2isevaluatedasthegraphneuralnetworkresultingfrom
thefunctioncompositionofSğ‘‘ğ‘ [[N2]]G,ğœ‰ andSğ‘‘ğ‘ [[N1]]G,ğœ‰.
Parallel composition. An expression of the form N1||N2 is evaluated as the graph neural network that maps a
node-labelingfunctiontothenode-labelingfunctionobtainedfromtheparallelcompositionofSğ‘‘ğ‘ [[N1]]G,ğœ‰ and
Sğ‘‘ğ‘ [[N2]]G,ğœ‰.
Choice. Thechoice operatorN1 âŠ•N2 appliedto ğœ‡G formulasN1,N2 isevaluatedasthegraphneuralnetwork
Sğ‘‘ğ‘ [[N1]]G,ğœ‰ iftheleftprojectionoftheinputnode-labelingğœ‚â€² =ğœ‹ ğ¿(ğœ‚)isanode-labelingfunctionsuchthatâˆ€ğ‘£ âˆˆ
G,ğœ‚â€²(ğ‘£)=True.Otherwise,itisevaluatedasthegraphneuralnetworkSğ‘‘ğ‘ [[N2]]G,ğœ‰.Inanycase,theselectedGNNis
givenininputtherightprojectionoftheinputnode-labelingfunction.
Fixedpoints. ThestaroperatorNâ˜…,orthefixedpointoperator,appliedtoağœ‡GformulaN isevaluatedasthegraph
neuralnetworkthatthatmapsanode-labelingfunctionğœ‚toanewnode-labelingfunctionğœ‚â€²thatisthefixedpointof
N computedstartingbyğœ‚.Inotherwords,thesequence
ManuscriptsubmittedtoACM10 Belenchiaetal.
ğœ‚0=ğœ‚
ğœ‚1=Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğœ‚0)
ğœ‚2=Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğœ‚1)
.
.
.
ğœ‚ ğ‘– =Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğœ‚ ğ‘–âˆ’1)
iscomputeduntilweobtainalabelingğœ‚â€²suchthatSğ‘‘ğ‘ [[N]]G,ğœ‰ (ğœ‚â€²)=ğœ‚â€².Theğ¹ğ¼ğ‘‹ functionusedinthedefinition
requiresthattheinputfunctionalisacontinuousfunction.Inordertoprovethat,wefirsthavetoshowthatfunction
compositionâ—¦andthefunctionğ‘ğ‘œğ‘›ğ‘‘arecontinuous.Later,inSection6,wewillalsorequirethecontinuityofparallel
compositionÃ—toprovetheequivalenceofthedenotationalandoperationalsemantics.Thefollowinglemmasproveall
theseresults.
Lemma5.3. Functioncompositionâ—¦iscontinuousinbothitsarguments.
Proof. Weprovethatâ—¦iscontinuousinthefirstargument.Theproofofcontinuityinthesecondargumentis
analogous.Letğœ™0beagraphneuralnetworkandletğ¹(ğœ™)=ğœ™â—¦ğœ™0.Westartbyprovingthatğ¹ismonotone.Ifğœ™1 âŠ‘Î¦ ğœ™2,
G,ğœ‰
thenğ‘Ÿğ‘’ğ‘™(ğœ™1) âŠ†ğ‘Ÿğ‘’ğ‘™(ğœ™2),andwecanconcludethat
ğ‘Ÿğ‘’ğ‘™(ğœ™0)âˆ—ğ‘Ÿğ‘’ğ‘™(ğœ™1) âŠ†ğ‘Ÿğ‘’ğ‘™(ğœ™0)âˆ—ğ‘Ÿğ‘’ğ‘™(ğœ™2)
andthereforeğ¹(ğœ™1) âŠ‘Î¦ ğ¹(ğœ™2).
G,ğœ‰
Nextweprovethecontinuityofğ¹.Letğ‘Œ beanon-emptychain,then
(cid:196) (cid:196)
ğ‘Ÿğ‘’ğ‘™(ğ¹( ğ‘Œ))=ğ‘Ÿğ‘’ğ‘™(( ğ‘Œ)â—¦ğœ™0)
(cid:196)
=ğ‘Ÿğ‘’ğ‘™(ğœ™0)âˆ—ğ‘Ÿğ‘’ğ‘™( ğ‘Œ)
(cid:216)
=ğ‘Ÿğ‘’ğ‘™(ğœ™0)âˆ— {ğ‘Ÿğ‘’ğ‘™(ğœ™) |ğœ™ âˆˆğ‘Œ}
(cid:216)
= {ğ‘Ÿğ‘’ğ‘™(ğœ™0)âˆ—ğ‘Ÿğ‘’ğ‘™(ğœ™) |ğœ™ âˆˆğ‘Œ}
(cid:196)
=ğ‘Ÿğ‘’ğ‘™( {ğ¹(ğœ™) |ğœ™ âˆˆğ‘Œ})
Thusğ¹((cid:195)ğ‘Œ)=(cid:195){ğ¹(ğœ™) |ğœ™ âˆˆğ‘Œ}. â–¡
Lemma5.4. ParallelcompositionÃ—iscontinuousinbothitsarguments.
Proof. WeprovethatÃ—iscontinuousinthefirstargument.Theproofofcontinuityinthesecondargumentis
analogous. Letğœ™0 be a graph neural network and let ğ¹(ğœ™) = ğœ™ Ã—ğœ™0. We start by proving that ğ¹ is monotone. If
ğœ™1 âŠ‘Î¦ ğœ™2,thenğ‘Ÿğ‘’ğ‘™(ğœ™1) âŠ†ğ‘Ÿğ‘’ğ‘™(ğœ™2),andwecanconcludethat
G,ğœ‰
ğ‘Ÿğ‘’ğ‘™(ğœ™0)âŒ¢ğ‘Ÿğ‘’ğ‘™(ğœ™1) âŠ†ğ‘Ÿğ‘’ğ‘™(ğœ™0)âŒ¢ğ‘Ÿğ‘’ğ‘™(ğœ™2)
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 11
andthereforeğ¹(ğœ™1) âŠ‘Î¦ ğ¹(ğœ™2).Next,weprovethecontinuityofğ¹.Letğ‘Œ beanon-emptychain,then
G,ğœ‰
(cid:196) (cid:196)
ğ‘Ÿğ‘’ğ‘™(ğ¹( ğ‘Œ))=ğ‘Ÿğ‘’ğ‘™(( ğ‘Œ)|ğœ™0)
(cid:196)
=ğ‘Ÿğ‘’ğ‘™( ğ‘Œ)âŒ¢ğ‘Ÿğ‘’ğ‘™(ğœ™0)
(cid:216)
= {ğ‘Ÿğ‘’ğ‘™(ğœ™) |ğœ™ âˆˆğ‘Œ}âŒ¢ğ‘Ÿğ‘’ğ‘™(ğœ™0)
(cid:216)
= {ğ‘Ÿğ‘’ğ‘™(ğœ™)âŒ¢ğ‘Ÿğ‘’ğ‘™(ğœ™0) |ğœ™ âˆˆğ‘Œ}
(cid:196)
=ğ‘Ÿğ‘’ğ‘™( {ğ¹(ğœ™) |ğœ™ âˆˆğ‘Œ})
Thusğ¹((cid:195)ğ‘Œ)=(cid:195){ğ¹(ğœ™) |ğœ™ âˆˆğ‘Œ}.
â–¡
Lemma5.5. Thefunctionğ‘ğ‘œğ‘›ğ‘‘iscontinuousinitsfirstargument.
Proof. Letğœ™0,ğœ™1 beGNNs,andletğ¹(ğœ™) =ğ‘ğ‘œğ‘›ğ‘‘(ğœ™,ğœ™0,ğœ™1).Westartbyprovingthatğ¹ ismonotone.Wehaveto
showthatifğœ™2 âŠ‘Î¦ ğœ™3,thenğ¹(ğœ™2) âŠ‘ğ¹(ğœ™3).Weconsideranarbitrarynode-labelingfunctionğœ‚andshowthat
G,ğœ‰
ğ¹(ğœ™2)(ğœ‚)=ğœ‚â€² =â‡’ ğ¹(ğœ™3)(ğœ‚)=ğœ‚â€²
Ifğœ™2(ğœ‚) =ğœ†ğ‘£.True,thenğ¹(ğœ™2)(ğœ‚) =ğœ™0(ğœ‚),andfromğœ™2 âŠ‘Î¦ ğœ™3,wegetthatğ¹(ğœ™3)(ğœ‚) =ğœ™0(ğœ‚).Letâ€™sconsiderthe
G,ğœ‰
caseğœ™2(ğœ‚)â‰ ğœ†ğ‘£.True.Thenğ¹(ğœ™2)(ğœ‚)=ğœ™1(ğœ‚)andthesameistrueforğ¹(ğœ™3)(ğœ‚)sotheresultisimmediate.
Toprovethecontinuityofğ¹.Letğ‘Œ beanon-emptychain,andwewillonlyshowthatğ¹((cid:195)ğ‘Œ) âŠ‘Î¦ (cid:195){ğ¹(ğœ™) |ğœ™ âˆˆğ‘Œ}
G,ğœ‰
sincefromthemonotonicityofğ¹ wejustprovedwecanconclude(cid:195){ğ¹(ğœ™) |ğœ™ âˆˆğ‘Œ}âŠ‘Î¦ ğ¹((cid:195)ğ‘Œ).
G,ğœ‰
Thenletâ€™sassumeğ¹((cid:195)ğ‘Œ)(ğœ‚)=ğœ‚â€²andwehavetodetermineağœ™ âˆˆğ‘Œ suchthatğ¹(ğœ™)(ğœ‚)=ğœ‚â€².If((cid:195)ğ‘Œ)(ğœ‚)=ğœ†ğ‘£.True,
wehaveğ¹((cid:195)ğ‘Œ)(ğœ‚) =ğœ™1(ğœ‚)anditmustbethecasethat(ğœ‚,ğœ†ğ‘£.True) âˆˆğ‘Ÿğ‘’ğ‘™((cid:195)ğ‘Œ).Butsinceğ‘Ÿğ‘’ğ‘™((cid:195)ğ‘Œ) = (cid:208){ğ‘Ÿğ‘’ğ‘™(ğœ™) |
ğœ™ âˆˆğ‘Œ}theremustexistağœ™ âˆˆğ‘Œ suchthatğœ™(ğœ‚)=ğœ†ğ‘£.Trueandğ¹(ğœ™)(ğœ‚)=ğœ™1(ğœ‚).Thecasewhere((cid:195)ğ‘Œ)(ğœ‚)â‰ ğœ†ğ‘£.True
isanalogous. â–¡
Lemma5.6. Thefunctionğ‘ğ‘œğ‘›ğ‘‘iscontinuousinitssecondandthirdarguments.
Proof. Weprovethatğ‘ğ‘œğ‘›ğ‘‘iscontinuousinthesecondargument.Theproofofcontinuityinthethirdargumentis
analogous.Letğœ™ ğ‘¡,ğœ™0beGNNs,andletğ¹(ğœ™)=ğ‘ğ‘œğ‘›ğ‘‘(ğœ™ ğ‘¡,ğœ™,ğœ™0).Westartbyprovingthatğ¹ ismonotone.
Ifğœ™ ğ‘¡(ğœ‚)=ğœ†ğ‘£.True,thenğ¹(ğœ™1)(ğœ‚)=ğœ™1(ğœ‚),andfromğœ™1 âŠ‘Î¦ ğœ™2,wegetthatğœ™1(ğœ‚)=ğœ‚â€² =â‡’ ğœ™2(ğœ‚)=ğœ‚â€².Therefore
G,ğœ‰
ğ¹(ğœ™2)(ğœ‚)=ğœ™2(ğœ‚)=ğœ‚â€²provesourresult.Letâ€™sconsiderthecaseğœ™ ğ‘¡(ğœ‚)â‰ ğœ†ğ‘£.True.Thenğ¹(ğœ™1)(ğœ‚)=ğœ™0(ğœ‚)andthesame
istrueforğ¹(ğœ™2)(ğœ‚)sotheresultisimmediate.
Toprovethecontinuityofğ¹.Letğ‘Œ beanon-emptychain,andwewillonlyshowthatğ¹((cid:195)ğ‘Œ) âŠ‘Î¦ (cid:195){ğ¹(ğœ™) |ğœ™ âˆˆğ‘Œ}
G,ğœ‰
sincefromthemonotonicityofğ¹ wejustprovedwecanconclude(cid:195){ğ¹(ğœ™) |ğœ™ âˆˆğ‘Œ}âŠ‘Î¦ ğ¹((cid:195)ğ‘Œ).
G,ğœ‰
Thenletâ€™sassumeğ¹((cid:195)ğ‘Œ)(ğœ‚) =ğœ‚â€²andwehavetodetermineağœ™ âˆˆğ‘Œ suchthatğ¹(ğœ™)(ğœ‚) =ğœ‚â€².Ifğœ™ ğ‘¡(ğœ‚) â‰ ğœ†ğ‘£.True,
wehaveğ¹((cid:195)ğ‘Œ)(ğœ‚) =ğœ™0(ğœ‚)andforallğœ™ âˆˆğ‘Œ wehaveğ¹(ğœ™)(ğœ‚) =ğœ™0(ğœ‚).Ifğœ™ ğ‘¡(ğœ‚) = ğœ†ğ‘£.True,wehaveğ¹((cid:195)ğ‘Œ)(ğœ‚) =
((cid:195)ğ‘Œ)(ğœ‚) =ğœ‚â€² andtherefore (ğœ‚,ğœ‚â€²) âˆˆ ğ‘Ÿğ‘’ğ‘™((cid:195)ğ‘Œ).Butthen,sinceğ‘Ÿğ‘’ğ‘™((cid:195)ğ‘Œ) = (cid:208){ğ‘Ÿğ‘’ğ‘™(ğœ™) | ğœ™ âˆˆ ğ‘Œ},theremustexista
ğœ™ âˆˆğ‘Œ suchthatğœ™(ğœ‚)=ğœ‚â€²andthenğ¹(ğœ™)(ğœ‚)=ğœ‚â€². â–¡
Finally,wecanshowthatthefunctionalwepasstoğ¹ğ¼ğ‘‹ isindeedacontinuousfunction.
Theorem5.7. Thefunctionalğ¹(ğœ™)=ğ‘ğ‘œğ‘›ğ‘‘(ğœ†ğ‘’.Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğ‘’) (cid:27)ğ‘’,ğ‘–ğ‘‘,ğœ™â—¦Sğ‘‘ğ‘ [[N]]G,ğœ‰ )iscontinuous.
ManuscriptsubmittedtoACM12 Belenchiaetal.
Proof. Wecanseeğ¹ asthecompositionoftwofunctions
ğ¹ =ğœ†ğ‘“.ğ‘ğ‘œğ‘›ğ‘‘(ğœ†ğ‘’.Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğ‘’) (cid:27)ğ‘’,ğ‘“,ğ‘–ğ‘‘)â—¦ğœ†ğ‘“.ğ‘“ â—¦Sğ‘‘ğ‘ [[N]]G,ğœ‰
andsincethefunctioncompositionofcontinuousfunctionsisitselfcontinuous,weconcludeitscontinuityfromthe
continuityoffunctioncomposition(Lemma5.3)andğ‘ğ‘œğ‘›ğ‘‘(Lemma5.6). â–¡
5.2 Operationalsemantics
Forthestructuraloperationalsemantics,weconsidercomputationsoftheform
âŸ¨N,ğœ‚âŸ©
whereN isağœ‡Gexpressionandğœ‚isanode-labelingfunction.ThetransitionrulesareshowninTable1.Foranylabel
typeğ‘‡ andanyrealvalueğœ– âˆˆR,thefunctionğ‘“
â‰ƒğœ–
:(ğ‘‡ Ã—ğ‘‡)â˜…Ã—(ğ‘‡ Ã—ğ‘‡)â†’Bassociatedtoğœ“
â‰ƒğœ–
inTable1isdefinedas
ğ‘“ â‰ƒğœ–(ğ‘‹,ğ‘¥)=ğœ‹ ğ¿(ğ‘¥)â‰ƒğœ– ğœ‹ ğ‘…(ğ‘¥)
Finally,wecandefinethesemanticinterpretationfunctiononthestructuraloperationalsemantics.
Definition5.8(Structuraloperationalsemantics). GivenagraphGandanedge-labelingfunctionğœ‰,wedefinethe
semanticinterpretationfunctionSğ‘ ğ‘œğ‘ [[Â·]]G,ğœ‰ :N â†’Î¦
G,ğœ‰
suchthat
Sğ‘ ğ‘œğ‘ [[N]]G,ğœ‰
(ğœ‚)=ï£±ï£´ï£´ï£²ğœ‚â€² ifâŸ¨N,ğœ‚âŸ©â†’â˜…
G,ğœ‰
ğœ‚â€²
ï£´ï£´undef otherwise
ï£³
TherulesPAR1andPAR2inTable1maketheevaluationofğœ‡Gexpressionsnon-deterministic.Thenexttheorem
showsthatnomattertheorderofapplicationofthestructuralsemanticsrules,theobtainedGNNisthesame.
Theorem5.9(Confluence). IfâŸ¨N,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N1,ğœ‚1âŸ©andâŸ¨N,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N2,ğœ‚2âŸ©,theneitherN1=N2andğœ‚1=ğœ‚2or
thereexistsatermNâ€²andalabelingfunctionğœ‚â€²suchthatâŸ¨N1,ğœ‚1âŸ©â†’
G,ğœ‰
âŸ¨Nâ€²,ğœ‚â€²âŸ©andâŸ¨N2,ğœ‚2âŸ©â†’
G,ğœ‰
âŸ¨Nâ€²,ğœ‚â€²âŸ©.
Proof. ByinductiononthestructureofN.TheonlyinterestingcaseisthatforexpressionsoftheformN1âŠ—N2,
whererulesPAR1 andPAR2 canbeappliedinanyorder.Supposethat âŸ¨N1 âŠ—N2,ğœ‚âŸ© â†’
G,ğœ‰
âŸ¨N 1â€² âŠ—N2,ğœ‚1|ğœ‹ ğ‘…(ğœ‚)âŸ© by
rulePAR1andthatâŸ¨N1âŠ—N2,ğœ‚âŸ© â†’
G,ğœ‰
âŸ¨N1âŠ—N 2â€²,ğœ‹ ğ¿(ğœ‚)|ğœ‚2âŸ©byrulePAR2.ThentherequiredtermisNâ€² =N 1â€²âŠ—N 2â€²
and the labeling function isğœ‚â€² = ğœ‚1|ğœ‚2, because âŸ¨N1 âŠ— N 2â€²,ğœ‹ ğ¿(ğœ‚)|ğœ‚2âŸ© â†’
G,ğœ‰
âŸ¨N 1â€² âŠ— N 2â€²,ğœ‚1|ğœ‚2âŸ© by rule PAR1 and
âŸ¨N 1â€²âŠ—N2,ğœ‚1|ğœ‹ ğ‘…(ğœ‚)âŸ©â†’
G,ğœ‰
âŸ¨N 1â€²âŠ—N 2â€²,ğœ‚1|ğœ‚2âŸ©byrulePAR2. â–¡
5.3 Languagemacros
Wecanenrichğœ‡Gwithanumberofmacrosthatsimplifiesthejobofprogrammingagraphneuralnetwork.Inthis
section,wedescribethemeanstodefinevariables,functions,andshortcutsforthedefinitionofif-then-elseandwhile
loopexpressions.SomeoftheseextensionsrequireustointroduceasetofvariablesymbolsX::=ğ‘‹ |ğ‘Œ |ğ‘ |Â·Â·Â· tothe
languageâ€™ssyntax.Wewillalsousefunctionsymbolsğ‘ andğ‘ todenoteleftandrightprojections.
ğ¿ ğ‘…
Variableassignments. Itisusefulsometimestoassignanentireexpressiontoasinglelabel,sothatwheneverthatlabel
occursinaprogram,thereferredexpressionissubstitutedtoit.Forthispurpose,weintroducevariableassignments
intheformof letexpressions.Aletexpressionhastheformlet X = N in Nâ€².Theintuitivemeaningofsuch
expressionisthatalloccurrencesofthevariablesymbolXintheexpressionNâ€²aresubstitutedwiththeexpression
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 13
[ID] âŸ¨ğœ„,ğœ‚âŸ©â†’ ğœ‚
G,ğœ‰
[APPLY] âŸ¨ğœ“,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨ğœ„,ğœ†ğ‘£.ğ‘“ ğœ“([ğœ‚(ğ‘¢) |ğ‘¢ âˆˆV],ğœ‚(ğ‘£))âŸ©
ğœ‘ â†’âˆ’
[PREIMG] âŸ¨â— ğœ,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨ğœ„,ğœ†ğ‘£.ğ‘“ ğœ([ğ‘“ ğœ‘(ğœ‚(ğ‘¢),ğœ‰((ğ‘£,ğ‘¢)),ğœ‚(ğ‘£)) |ğ‘¢ âˆˆ ğ‘(ğ‘£)],ğœ‚(ğ‘£))âŸ©
ğœ‘ â†âˆ’
[POSTIMG] âŸ¨â–· ğœ,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨ğœ„,ğœ†ğ‘£.ğ‘“ ğœ([ğ‘“ ğœ‘(ğœ‚(ğ‘¢),ğœ‰((ğ‘¢,ğ‘£)),ğœ‚(ğ‘£)) |ğ‘¢ âˆˆ ğ‘(ğ‘£)],ğœ‚(ğ‘£))âŸ©
âŸ¨N1,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N 1â€²,ğœ‚â€²âŸ©
[SEQ1]
âŸ¨N1;N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N 1â€²;N2,ğœ‚â€²âŸ©
[SEQ2] âŸ¨ğœ„;N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N2,ğœ‚âŸ©
[SPLIT] âŸ¨N1||N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N1âŠ—N2,ğœ‚|ğœ‚âŸ©
âŸ¨N1,ğœ‹ ğ¿(ğœ‚)âŸ©â†’
G,ğœ‰
âŸ¨N 1â€²,ğœ‚â€²âŸ©
[PAR1]
âŸ¨N1âŠ—N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N 1â€²âŠ—N2,ğœ‚â€²|ğœ‹ ğ‘…(ğœ‚)âŸ©
âŸ¨N2,ğœ‹ ğ‘…(ğœ‚)âŸ©â†’
G,ğœ‰
âŸ¨N 2â€²,ğœ‚â€²âŸ©
[PAR2]
âŸ¨N1âŠ—N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N1âŠ—N 2â€²,ğœ‹ ğ¿(ğœ‚)|ğœ‚â€²âŸ©
[MERGE] âŸ¨ğœ„âŠ—ğœ„,ğœ‚âŸ©â†’ âŸ¨ğœ„,ğœ‚âŸ©
G,ğœ‰
[CHOICE1] âŸ¨N1âŠ•N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N1,ğœ‹ ğ‘…(ğœ‚)âŸ©ifğœ‹ ğ¿(ğœ‚)=ğœ†ğ‘£.True
[CHOICE2] âŸ¨N1âŠ•N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N2,ğœ‹ ğ‘…(ğœ‚)âŸ©ifğœ‹ ğ¿(ğœ‚)=ğœ†ğ‘£.True
[STAR] âŸ¨Nâ˜…,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨((ğœ„||N);ğœ“ â‰ƒğœ–||ğœ„);(ğœ„âŠ•N;Nâ˜…),ğœ‚âŸ©
Table1. Structuraloperationalsemanticsofğœ‡G
N.Thissubstitutionispurelysyntactical,andletexpressionsaresimplyrewrittenasNâ€² .Clearly,multiplelet
[N/X]
expressionscanbechained,sothatwecanwritelet X1 = N1,X2 = N2,...,Xğ‘˜ = Nğ‘˜ in N asashorthandfor
let X1=N1 in (let X2=N2 in (...(let Xğ‘˜ =Nğ‘˜ in N)...)).
Functiondefinitions. Similarlytovariableassignments,wealsoconsiderthedefinitionoffunctions.Adefexpression
hastheformdef X(X1,...Xğ‘˜){N} in Nâ€².ItsintuitivemeaningisthatwheneverthevariablesymbolXfollowedby
aparenthesizedlistofvalues(N1,...,Nğ‘˜)(whichfromnowonwerefertoasacallexpression)occursinNâ€²theentire
callexpressionissubstitutedwiththeexpressionN inwhicheachvariablesymbolX1,...,Xğ‘˜ hasbeensubstituted
withthecorrespondingexpressionN1,...,Nğ‘˜.Asinthecaseofvariables,thissubstitutionispurelysyntacticalis
rewrittenas
Nâ€²
[X(N1,...,Nğ‘˜)/let X1=N1,...,Xğ‘˜=Nğ‘˜ in N]
ManuscriptsubmittedtoACM14 Belenchiaetal.
If-then-elseselection. Atypicalprogramminglanguageconstructistheif-then-elseselectionoperatorthatevaluatesa
Booleanconditionandexecutesoneoutoftwobranchesaccordingly.Inğœ‡Gthisoperatorcanbeimplementedasa
macrousingthechoiceoperator.Weconsideragraphneuralnetworkğœ™1:Î¦ G,ğœ‰[ğ‘‡1,B]denotedbyanexpressionN1to
provideaBooleannode-labelingfunction,andthenexecutethegraphneuralnetworkdenotedbyN2iftheBoolean
labelingisTrueforeverynode,otherwisetheGNNdenotedbyN3isexecutedinstead.Thenwecanintroducetheterm
if N1 then N2 else N3asashorthandfor(N1||ğœ„);(N2âŠ•N3).
Fixpointswithvariables. Oftentimeswemightwanttoincludeconstanttermsinourfixpointcomputations,thatis,
includeGNNswhoseoutputsdonotdependonthecurrentiteratesolution,butonlyontheinitialnodelabelsasreceived
bythestaroperator.Tothisend,weintroduceamacroexpressionfix X =N,let Y1 =N1,...,Yğ‘˜ =Nğ‘˜ in Nâ€²
wherethevariablesymbolXisthefixpointvariableandN istheGNNthatcomputestheinitialvalueforthefixpoint
computation.ThevariablesYğ‘– forğ‘– âˆˆ1,...,ğ‘˜denotethevalueswewanttopre-computeandstayconstantduringthe
computation.Toclarifythisbehaviorformally,weshowhowthismacrocanbeexpressedinthelanguage.Werestrict
theplacementofvariablesymbolsX,Ytonotappearontheright-handsideofasequentialcompositionexpression.
Then,thismacrocanbeimplementedas
((N1||N2||Â·Â·Â·||Nğ‘˜)||N);
((ğ‘1||ğ‘2||Â·Â·Â·||ğ‘ ğ‘˜)||N [â€² ğ‘1/Y1,...,ğ‘ğ‘˜/Yğ‘˜,ğ‘ğ‘…/X])â˜…;
ğ‘
ğ‘…
wherethetermsğ‘ denoteacompositionofleftandrightprojectionsfunctionstoobtainthenode-labelingproducedby
ğ‘–
Sğ‘‘ğ‘ [[Nğ‘–]]G,ğœ‰ intheparallelcompositionofNğ‘– forğ‘– âˆˆ1,...,ğ‘˜.Indeed,itisevenpossibletostaticallyinferthelargest
possiblesetofsub-expressionsNğ‘– thatcanbepre-computed,andthereforewecansimplywritefix X=N in Nâ€²to
havethesameeffect.Thismacrothenessentiallyimplementsğœ‡-calculusstylefixpointcomputations,withtheadded
flexibilityofbeingabletosetanyinitialvalueforthefixpointcomputation.
Inthecasethereisnosuchsub-expressionNğ‘–,themacroreducesto
N;Nâ˜…
[ğœ„/X]
Fixediterationloops. Thebasicfixpointoperatoriteratesuntiltheoutputlabelingisconsideredequaltotheinput
labeling.Attimestheprogrammermightwanttoinsteaditerateforafixednumberofsteps,thatis,executethebody
ofthefixpointexpressiononlyforğ‘˜ â‰¥ 1steps.Forthispurpose,weexpandonthepreviousmacrotointroduce
repeat X=N in Nâ€² for ğ‘˜withğ‘˜ âˆˆN+thatisashorthandfortheğœ‡Gexpression
(fix X=ğ‘§ğ‘’ğ‘Ÿğ‘œ||N in
if X;ğ‘ ğ¿;< ğ‘˜ then
(X;ğ‘ ğ¿;ğ‘ ğ‘¢ğ‘ğ‘)||N [â€²
X;ğ‘ğ‘…/X]
else
X);ğ‘
ğ‘…
whereğ‘§ğ‘’ğ‘Ÿğ‘œdenotesaconstantfunctionthatmapsallnodelabelsto0,< denotesaBooleanfunctionthatmapsnodes
ğ‘˜
toTrueiftheir(integer)labelissmallerthanğ‘˜,andğ‘ ğ‘¢ğ‘ğ‘denotesthesuccessorfunctionovernaturalnumbers.Clearly,
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 15
wecanalsohavearepeatmacrowithouthavingtospecifyavariableandaninitialcondition:repeat Nâ€² for ğ‘˜can
berewrittenasrepeat X=ğœ„ in Nâ€² for ğ‘˜.
6 EQUIVALENCEOFOPERATIONALANDDENOTATIONALSEMANTICS
Boththedenotationalsemanticsandoperationalsemanticsofğœ‡Gdefineamappingfromnode-labelingfunctionsto
node-labelingfunctions,i.e.,agraphneuralnetwork.Inthenexttheorem,weshowthattheydefinethesamegraph
neuralnetwork.
Theorem6.1(Eqivalenceofdenotationalandstructuraloperationalsemantics). Foranyğœ‡Gexpression
N,anygraphGandanyedge-labelingfunctionğœ‰,wehavethat
Sğ‘‘ğ‘ [[N]]G,ğœ‰ =Sğ‘ ğ‘œğ‘ [[N]]G,ğœ‰
Proof. SincebothfunctionsreturnGNNs,whicharemembersofapartiallyorderedsetÎ¦ withorderingrelation
G,ğœ‰
âŠ‘Î¦ ,itissufficienttoshowthatforanyğœ‡GexpressionN:
G,ğœ‰
â€¢
Sğ‘ ğ‘œğ‘ [[N]]G,ğœ‰
âŠ‘Î¦
G,ğœ‰
Sğ‘‘ğ‘ [[N]]G,ğœ‰
â€¢
Sğ‘‘ğ‘ [[N]]G,ğœ‰
âŠ‘Î¦
G,ğœ‰
Sğ‘ ğ‘œğ‘ [[N]]G,ğœ‰
Lemma6.2andLemma6.3provethatboththeseconditionsaresatisfiedbythesemanticsofğœ‡G. â–¡
Lemma6.2. ForeveryexpressionN ofğœ‡G,wehaveSğ‘ ğ‘œğ‘ [[N]]G,ğœ‰ âŠ‘Î¦
G,ğœ‰
Sğ‘‘ğ‘ [[N]]G,ğœ‰ .
Proof. WeshallprovethatforanyexpressionN andanynode-labelingfunctionğœ‚,ğœ‚â€²
âŸ¨N,ğœ‚âŸ©â†’â˜…
G,ğœ‰
ğœ‚â€² =â‡’ Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğœ‚)=ğœ‚â€² (2)
Inordertodothat,wewillshowthat
âŸ¨N,ğœ‚âŸ©â†’
G,ğœ‰
ğœ‚â€² =â‡’ Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğœ‚)=ğœ‚â€² (3)
âŸ¨N,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨Nâ€²,ğœ‚â€²âŸ© =â‡’ Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğœ‚)=Sğ‘‘ğ‘ [[Nâ€²]]G,ğœ‰ (ğœ‚â€²) (4)
IfEquation3andEquation4hold,theproofofEquation2isastraightforwardinductiononthelengthğ‘˜ ofthe
derivationsequence âŸ¨N,ğœ‚âŸ© â†’ğ‘˜ ğœ‚â€².TheproofofEquation3andEquation4isbyinductionontheshapeofthe
G,ğœ‰
derivationtrees.
ThecaseID: WehaveâŸ¨ğœ„,ğœ‚âŸ©â†’
G,ğœ‰
ğœ‚,andsinceSğ‘‘ğ‘ [[ğœ„]]G,ğœ‰ (ğœ‚)=ğœ‚,theresultfollows.
ThecaseAPPLY: WehaveâŸ¨ğœ“,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨ğœ„,ğœ†ğ‘£.ğ‘“ ğœ“([ğœ‚(ğ‘¢) |ğ‘¢ âˆˆV],ğœ‚(ğ‘£))âŸ©,andsince
Sğ‘‘ğ‘ [[ğœ“]]G,ğœ‰ (ğœ‚)=ğœ†ğ‘£.ğ‘“ ğœ“([ğœ‚(ğ‘¢) |ğ‘¢ âˆˆV],ğœ‚(ğ‘£))
=Sğ‘‘ğ‘ [[ğœ„]]G,ğœ‰ (ğœ†ğ‘£.ğ‘“ ğœ“([ğœ‚(ğ‘¢) |ğ‘¢ âˆˆV],ğœ‚(ğ‘£)))
theresultfollows.
ThecasePREIMG: AnalogoustocaseAPPLY.
ThecasePOSTIMG: AnalogoustocaseAPPLY.
ManuscriptsubmittedtoACM16 Belenchiaetal.
ThecaseSEQ1: AssumethatâŸ¨N1;N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N 1â€²;N2,ğœ‚â€²âŸ©becauseâŸ¨N1,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N 1â€²,ğœ‚â€²âŸ©.Byinductionhypoth-
esiswehavethatSğ‘‘ğ‘ [[N1]]G,ğœ‰ (ğœ‚)=Sğ‘‘ğ‘ [[N 1â€²]]G,ğœ‰ (ğœ‚â€²).Thenweget
Sğ‘‘ğ‘ [[N1;N2]]G,ğœ‰ (ğœ‚)=Sğ‘‘ğ‘ [[N2]]G,ğœ‰ (Sğ‘‘ğ‘ [[N1]]G,ğœ‰ (ğœ‚))
=Sğ‘‘ğ‘ [[N2]]G,ğœ‰ (Sğ‘‘ğ‘ [[N 1â€²]]G,ğœ‰ (ğœ‚â€²))
=Sğ‘‘ğ‘ [[N 1â€²;N2]]G,ğœ‰ (ğœ‚â€²)
ThecaseSEQ2: WehavethatâŸ¨ğœ„;N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N2,ğœ‚âŸ©.Thenweget
Sğ‘‘ğ‘ [[ğœ„;N2]]G,ğœ‰ (ğœ‚)=Sğ‘‘ğ‘ [[N2]]G,ğœ‰ (Sğ‘‘ğ‘ [[ğœ„]]G,ğœ‰ (ğœ‚))
=Sğ‘‘ğ‘ [[N2]]G,ğœ‰ (ğœ‚)
ThecaseSPLIT: WehaveâŸ¨N1||N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N1âŠ—N2,ğœ‚|ğœ‚âŸ©.Thenweget
Sğ‘‘ğ‘ [[N1||N2]]G,ğœ‰ (ğœ‚)=Sğ‘‘ğ‘ [[N1]]G,ğœ‰ Ã—Sğ‘‘ğ‘ [[N2]]G,ğœ‰ (ğœ‚)
=(Sğ‘‘ğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ¿)Ã—(Sğ‘‘ğ‘ [[N2]]G,ğœ‰ â—¦ğœ‹ ğ‘…)(ğœ‚|ğœ‚)
=Sğ‘‘ğ‘ [[N1âŠ—N2]]G,ğœ‰ (ğœ‚|ğœ‚)
ThecasePAR1: WehaveâŸ¨N1âŠ—N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N 1â€²âŠ—N2,ğœ‚â€²|ğœ‹ ğ‘…(ğœ‚)âŸ©becauseâŸ¨N1,ğœ‹ ğ¿(ğœ‚)âŸ©â†’
G,ğœ‰
âŸ¨N 1â€²,ğœ‚â€²âŸ©.Byinduction
hypothesiswehavethatSğ‘‘ğ‘ [[N1]]G,ğœ‰ (ğœ‹ ğ¿(ğœ‚))=Sğ‘‘ğ‘ [[N 1â€²]]G,ğœ‰ (ğœ‚â€²).Thenweget
Sğ‘‘ğ‘ [[N1âŠ—N2]]G,ğœ‰ (ğœ‚)=(Sğ‘‘ğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ¿)Ã—(Sğ‘‘ğ‘ [[N2]]G,ğœ‰ â—¦ğœ‹ ğ‘…)(ğœ‚)
=Sğ‘‘ğ‘ [[N1]]G,ğœ‰ (ğœ‹ ğ¿(ğœ‚))|Sğ‘‘ğ‘ [[N2]]G,ğœ‰ (ğœ‹ ğ‘…(ğœ‚))
=Sğ‘‘ğ‘ [[N 1â€²]]G,ğœ‰ (ğœ‚â€²)|Sğ‘‘ğ‘ [[N2]]G,ğœ‰ (ğœ‹ ğ‘…(ğœ‚))
=(Sğ‘‘ğ‘ [[N 1â€²]]G,ğœ‰ â—¦ğœ‹ ğ¿)Ã—(Sğ‘‘ğ‘ [[N2]]G,ğœ‰ â—¦ğœ‹ ğ‘…)(ğœ‚â€²|ğœ‹ ğ‘…(ğœ‚))
=Sğ‘‘ğ‘ [[N 1â€²âŠ—N2]]G,ğœ‰ (ğœ‚â€²|ğœ‹ ğ‘…(ğœ‚))
ThecasePAR2: AnalogoustocasePAR1.
ThecaseMERGE: WehavethatâŸ¨ğœ„âŠ—ğœ„,ğœ‚âŸ©â†’ âŸ¨ğœ„,ğœ‚âŸ©.Thenweget:
G,ğœ‰
Sğ‘‘ğ‘ [[ğœ„âŠ—ğœ„]]G,ğœ‰ (ğœ‚)=(Sğ‘‘ğ‘ [[ğœ„]]G,ğœ‰ â—¦ğœ‹ ğ¿)Ã—(Sğ‘‘ğ‘ [[ğœ„]]G,ğœ‰ â—¦ğœ‹ ğ‘…)(ğœ‚)
=Sğ‘‘ğ‘ [[ğœ„]]G,ğœ‰ (ğœ‹ ğ¿(ğœ‚))|Sğ‘‘ğ‘ [[ğœ„]]G,ğœ‰ (ğœ‹ ğ‘…(ğœ‚))
=ğœ‹ ğ¿(ğœ‚)|ğœ‹ ğ‘…(ğœ‚)
=ğœ‚
=Sğ‘‘ğ‘ [[ğœ„]]G,ğœ‰ (ğœ‚)
ThecaseCHOICE1: WehaveâŸ¨N1âŠ•N2,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨N1,ğœ‹ ğ‘…(ğœ‚)âŸ©becauseğœ‹ ğ¿(ğœ‚)=ğœ†ğ‘£.True.Thenweget
Sğ‘‘ğ‘ [[N1âŠ•N2]]G,ğœ‰ (ğœ‚)=ğ‘ğ‘œğ‘›ğ‘‘(ğœ‹ ğ¿,Sğ‘‘ğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ‘…,Sğ‘‘ğ‘ [[N2]]G,ğœ‰ â—¦ğœ‹ ğ‘…)(ğœ‚)
=Sğ‘‘ğ‘ [[N1]]G,ğœ‰ (ğœ‹ ğ‘…(ğœ‚))
ThecaseCHOICE2: AnalogoustoCHOICE1.
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 17
ThecaseSTAR: Wehave
âŸ¨Nâ˜…,ğœ‚âŸ©â†’
G,ğœ‰
âŸ¨((ğœ„||N);ğœ“ â‰ƒğœ–||ğœ„);(ğœ„âŠ•N;Nâ˜… ),ğœ‚âŸ©
Letğ¹ =ğœ†ğœ™.ğ‘ğ‘œğ‘›ğ‘‘(ğœ†ğ‘’.ğœ†ğ‘£.Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğ‘’)(ğ‘£)â‰ƒğœ– ğ‘’(ğ‘£),ğ‘–ğ‘‘,ğœ™â—¦Sğ‘‘ğ‘ [[N]]G,ğœ‰ ),thenweget
Sğ‘‘ğ‘ [[Nâ˜… ]]G,ğœ‰ (ğœ‚)=ğ¹ğ¼ğ‘‹(ğ¹)(ğœ‚)
=ğ¹(ğ¹ğ¼ğ‘‹(ğ¹))(ğœ‚)
=ğ‘ğ‘œğ‘›ğ‘‘(ğœ†ğ‘’.ğœ†ğ‘£.Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğ‘’)(ğ‘£)â‰ƒğœ– ğ‘’(ğ‘£),ğ‘–ğ‘‘,ğ¹ğ¼ğ‘‹(ğ¹)â—¦Sğ‘‘ğ‘ [[N]]G,ğœ‰ )(ğœ‚)
=ğ‘ğ‘œğ‘›ğ‘‘(ğœ†ğ‘’.ğœ†ğ‘£.Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğ‘’)(ğ‘£)â‰ƒğœ– ğ‘’(ğ‘£),ğ‘–ğ‘‘,Sğ‘‘ğ‘ [[Nâ˜… ]]G,ğœ‰ â—¦Sğ‘‘ğ‘ [[N]]G,ğœ‰ )(ğœ‚)
=ğ‘ğ‘œğ‘›ğ‘‘(Sğ‘‘ğ‘ [[(N||ğœ„);ğœ“ â‰ƒğœ–]]G,ğœ‰,Sğ‘‘ğ‘ [[ğœ„]]G,ğœ‰,Sğ‘‘ğ‘ [[N;Nâ˜… ]]G,ğœ‰ )
=ğ‘ğ‘œğ‘›ğ‘‘(ğœ‹ ğ¿,Sğ‘‘ğ‘ [[ğœ„]]G,ğœ‰ â—¦ğœ‹ ğ‘…,Sğ‘‘ğ‘ [[N;Nâ˜… ]]G,ğœ‰ â—¦ğœ‹ ğ‘…)â—¦Sğ‘‘ğ‘ [[(N||ğœ„);ğœ“ â‰ƒğœ–||ğœ„]]G,ğœ‰ (ğœ‚)
=Sğ‘‘ğ‘ [[ğœ„âŠ•N;Nâ˜… ]]G,ğœ‰ â—¦Sğ‘‘ğ‘ [[((ğœ„||N);ğœ“ â‰ƒğœ–||ğœ„)]]G,ğœ‰ (ğœ‚)
=Sğ‘‘ğ‘ [[((ğœ„||N);ğœ“ â‰ƒğœ–||ğœ„);(ğœ„âŠ•N;Nâ˜… )]]G,ğœ‰ (ğœ‚)
ThiscompletesourproofofLemma6.2. â–¡
Lemma6.3. ForeveryexpressionN ofğœ‡G,wehaveSğ‘‘ğ‘ [[N]]G,ğœ‰ âŠ‘Î¦
G,ğœ‰
Sğ‘ ğ‘œğ‘ [[N]]G,ğœ‰ .
Proof. WeproceedbystructuralinductionontheexpressionN.
Thecaseğœ„: Immediate,asSğ‘‘ğ‘ [[ğœ„]]G,ğœ‰ (ğœ‚)=Sğ‘ ğ‘œğ‘ [[ğœ„]]G,ğœ‰ (ğœ‚).
Thecaseğœ“: Thiscaseissimpleaswell,sinceSğ‘‘ğ‘ [[ğœ“]]G,ğœ‰ (ğœ‚)=Sğ‘ ğ‘œğ‘ [[ğœ“]]G,ğœ‰ (ğœ‚)byapplyingruleAPPLYfollowed
byruleID.
ğœ‘
Thecaseâ— ğœ: Analogoustocaseğœ“,butinsteadusingtherulesPREIMGandID.
ğœ‘
Thecaseâ–· ğœ: Analogoustocaseğœ“,butinsteadusingtherulesPOSTIMGandID.
ThecaseN1;N2: ByinductionhypothesiswehaveSğ‘‘ğ‘ [[N1]]G,ğœ‰ âŠ‘Î¦
G,ğœ‰
Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰ andSğ‘‘ğ‘ [[N2]]G,ğœ‰ âŠ‘Î¦
G,ğœ‰
Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰.Bythemonotonicityofâ—¦inbothitsarguments(Lemma5.3)andtheinductionhypothesisweget
that
Sğ‘‘ğ‘ [[N1;N2]]G,ğœ‰ =Sğ‘‘ğ‘ [[N2]]G,ğœ‰ â—¦Sğ‘‘ğ‘ [[N1]]G,ğœ‰
âŠ‘Î¦
Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰ â—¦Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰
G,ğœ‰
Tocontinuetheproof,weneedtoprovethatif âŸ¨N1,ğœ‚âŸ© â†’ğ‘˜
G,ğœ‰
ğœ‚â€² then âŸ¨N1;N2,ğœ‚âŸ© â†’ğ‘˜
G,ğœ‰
âŸ¨N2,ğœ‚â€²âŸ©.Theproof
isbyinductiononthederivationlengthğ‘˜.Forğ‘˜ = 1wehave âŸ¨N1,ğœ‚âŸ© â†’
G,ğœ‰
ğœ‚â€²,whichmeansthatN1 = ğœ„
andğœ‚ = ğœ‚â€². Therefore, by applying rule SEQ2 we get âŸ¨ğœ„;N2,ğœ‚âŸ© â†’
G,ğœ‰
âŸ¨N2,ğœ‚â€²âŸ©. For the inductive step, letâ€™s
assumethelemmaholdsforğ‘˜andprovethatitholdsforğ‘˜+1.WeassumethatâŸ¨N1,ğœ‚âŸ©â†’ğ‘˜ G+ ,ğœ‰1ğœ‚â€²,whichmeans
thatâŸ¨N1,ğœ‚âŸ© â†’
G,ğœ‰
âŸ¨N 1â€²,ğœ‚1âŸ©forsomeN 1â€²,ğœ‚1,andâŸ¨N 1â€²,ğœ‚1âŸ© â†’ğ‘˜
G,ğœ‰
ğœ‚â€².Thereforethefirstrulewecanapplyfor
thesequentialcompositionofN1 andN2 isSEQ1 withâŸ¨N1,ğœ‚âŸ© â†’
G,ğœ‰
âŸ¨N 1â€²,ğœ‚1âŸ©asitspremise,andwegetthe
transition âŸ¨N1;N2,ğœ‚âŸ© â†’
G,ğœ‰
âŸ¨N 1â€²;N2,ğœ‚1âŸ©. Now we apply the induction hypothesis âŸ¨N 1â€²,ğœ‚1âŸ© â†’ğ‘˜
G,ğœ‰
ğœ‚â€² =â‡’
âŸ¨N 1â€²;N2,ğœ‚1âŸ©â†’ğ‘˜
G,ğœ‰
âŸ¨N2,ğœ‚â€²âŸ©toobtainthedesiredresult,havingperformedğ‘˜+1stepsintotal.Usingthisresult,
ManuscriptsubmittedtoACM18 Belenchiaetal.
wecanconcludethat
Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰ â—¦Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰
âŠ‘Î¦
Sğ‘ ğ‘œğ‘ [[N1;N2]]G,ğœ‰
G,ğœ‰
asrequired.
ThecaseN1||N2:
ByinductionhypothesiswehaveSğ‘‘ğ‘ [[N1]]G,ğœ‰
âŠ‘Î¦
G,ğœ‰
Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰ andSğ‘‘ğ‘ [[N2]]G,ğœ‰
âŠ‘Î¦
G,ğœ‰
Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰.BythemonotonicityofÃ—inbothitsarguments(Lemma5.4)andtheinductionhypothesis,we
getthat
Sğ‘‘ğ‘ [[N1||N2]]G,ğœ‰ =Sğ‘‘ğ‘ [[N1]]G,ğœ‰ Ã—Sğ‘‘ğ‘ [[N2]]G,ğœ‰
âŠ‘Î¦
Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰ Ã—Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰
G,ğœ‰
Tocontinuetheproof,wehavetoshowthatifâŸ¨N1,ğœ‚1âŸ©â†’ğ‘˜ G1
,ğœ‰
ğœ‚ 1â€²andâŸ¨N2,ğœ‚2âŸ©â†’ğ‘˜ G2
,ğœ‰
ğœ‚ 2â€²,thenâŸ¨N1âŠ—N2,ğœ‚1|ğœ‚2âŸ©â†’ğ‘˜ G1 ,+ ğœ‰ğ‘˜2
ğœ‚ 1â€²|ğœ‚ 2â€².Forğ‘˜1 = ğ‘˜2 = 1,wehavethatN1 = N2 = ğœ„,ğœ‚1 = ğœ‚ 1â€² andğœ‚2 = ğœ‚ 2â€².Hence,weget âŸ¨ğœ„ âŠ—ğœ„,ğœ‚1|ğœ‚2âŸ© â†’
G,ğœ‰
âŸ¨ğœ„,ğœ‚1|ğœ‚2âŸ© â†’
G,ğœ‰
ğœ‚1|ğœ‚2 by applying rule MERGE followed by rule ID and performing 1 + 1 = 2 steps. For
the inductive step, letâ€™s assume the implication holds for ğ‘˜1,ğ‘˜2 and prove it for ğ‘˜1 + 1,ğ‘˜2 + 1. We have
âŸ¨N1,ğœ‚1âŸ© â†’
G,ğœ‰
âŸ¨N 1â€²,ğœ‚ 1â€²â€²âŸ© â†’ğ‘˜ G1
,ğœ‰
ğœ‚ 1â€² and âŸ¨N2,ğœ‚2âŸ© â†’
G,ğœ‰
âŸ¨N 2â€²,ğœ‚ 2â€²â€²âŸ© â†’ğ‘˜ G2
,ğœ‰
ğœ‚ 2â€² forsomeN 1â€²,N 2â€²,ğœ‚ 1â€²â€²,ğœ‚ 2â€²â€².Thenwe
canperformtwostepsâŸ¨N1 âŠ—N2,ğœ‚1|ğœ‚2âŸ© â†’
G,ğœ‰
âŸ¨N 1â€² âŠ—N2,ğœ‚ 1â€²â€²|ğœ‚2âŸ© â†’
G,ğœ‰
âŸ¨N 1â€² âŠ—N 2â€²,ğœ‚ 1â€²â€²|ğœ‚ 2â€²â€²âŸ©byapplyingrules
[PAR1]and[PAR2].ThenbyapplyingtheinductionhypothesiswegetourresultâŸ¨N 1â€²âŠ—N 2â€²,ğœ‚ 1â€²â€²|ğœ‚ 2â€²â€²âŸ©â†’ğ‘˜ G1 ,+ ğœ‰ğ‘˜2 ğœ‚ 1â€²|ğœ‚ 2â€²
in2+ğ‘˜1+ğ‘˜2=(ğ‘˜1+1)+(ğ‘˜2+1)steps.Usingthisresult,wecanconcludethat
Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰ Ã—Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰ âŠ‘Î¦ Sğ‘ ğ‘œğ‘ [[N1âŠ—N2]]G,ğœ‰ â—¦(ğ‘–ğ‘‘Ã—ğ‘–ğ‘‘)
G,ğœ‰
=Sğ‘ ğ‘œğ‘ [[N1||N2]]G,ğœ‰
asrequired.
ThecaseN1âŠ—N2:
ByinductionhypothesiswehaveSğ‘‘ğ‘ [[N1]]G,ğœ‰
âŠ‘Î¦
G,ğœ‰
Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰ andSğ‘‘ğ‘ [[N2]]G,ğœ‰
âŠ‘Î¦
G,ğœ‰
Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰.Bythemonotonicityofâ—¦(Lemma5.3)andthatofÃ—inbothitsarguments(Lemma5.4),andthe
inductionhypothesis,wegetthat
Sğ‘‘ğ‘ [[N1âŠ—N2]]G,ğœ‰ =(Sğ‘‘ğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ¿)Ã—(Sğ‘‘ğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ‘…)
âŠ‘Î¦
G,ğœ‰
(Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ¿)Ã—(Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰ â—¦ğœ‹ ğ‘…)
=Sğ‘ ğ‘œğ‘ [[N1âŠ—N2]]G,ğœ‰
ThecaseN1âŠ•N2: WehavethatSğ‘‘ğ‘ [[N1âŠ•N2]]G,ğœ‰ =ğ‘ğ‘œğ‘›ğ‘‘(ğœ‹ ğ¿,Sğ‘‘ğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ‘…,Sğ‘‘ğ‘ [[N2]]G,ğœ‰ â—¦ğœ‹ ğ‘…).Byinduction
hypothesis we have that Sğ‘‘ğ‘ [[N1]]G,ğœ‰ âŠ‘Î¦
G,ğœ‰
Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰ and Sğ‘‘ğ‘ [[N2]]G,ğœ‰ âŠ‘Î¦
G,ğœ‰
Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰. By the
monotonicityofâ—¦(Lemma5.3)andthatofğ‘ğ‘œğ‘›ğ‘‘initssecondandthirdarguments(Lemma5.6),andtheinduction
hypothesiswehavethat
Sğ‘‘ğ‘ [[N1âŠ•N2]]G,ğœ‰ =ğ‘ğ‘œğ‘›ğ‘‘(ğœ‹ ğ¿,Sğ‘‘ğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ‘…,Sğ‘‘ğ‘ [[N2]]G,ğœ‰ â—¦ğœ‹ ğ‘…)
âŠ‘Î¦
G,ğœ‰
ğ‘ğ‘œğ‘›ğ‘‘(ğœ‹ ğ¿,Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ‘…,Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰ â—¦ğœ‹ ğ‘…)
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 19
NowfromtherulesCHOICE1andCHOICE2itfollowsthat
Sğ‘ ğ‘œğ‘ [[N1âŠ•N2]]G,ğœ‰ (ğœ‚)=Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰ (ğœ‹ ğ‘…(ğœ‚))ifğœ‹ ğ¿(ğœ‚)=ğœ†ğ‘£.True
Sğ‘ ğ‘œğ‘ [[N1âŠ•N2]]G,ğœ‰ (ğœ‚)=Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰ (ğœ‹ ğ‘…(ğœ‚))ifğœ‹ ğ¿(ğœ‚)â‰ ğœ†ğ‘£.True
therefore
ğ‘ğ‘œğ‘›ğ‘‘(ğœ‹ ğ¿,Sğ‘ ğ‘œğ‘ [[N1]]G,ğœ‰ â—¦ğœ‹ ğ‘…,Sğ‘ ğ‘œğ‘ [[N2]]G,ğœ‰ â—¦ğœ‹ ğ‘…)=Sğ‘ ğ‘œğ‘ [[N1âŠ•N2]]G,ğœ‰
ThecaseNâ˜… : WehavethatSğ‘‘ğ‘ [[Nâ˜…]]G,ğœ‰ =ğ¹ğ¼ğ‘‹(ğ¹),where
ğ¹ =ğœ†ğœ™.ğ‘ğ‘œğ‘›ğ‘‘(ğœ†ğ‘’.ğœ†ğ‘£.Sğ‘‘ğ‘ [[N]]G,ğœ‰ (ğ‘’)(ğ‘£)â‰ƒğœ– ğ‘’(ğ‘£),ğ‘–ğ‘‘,ğœ™â—¦Sğ‘‘ğ‘ [[N]]G,ğœ‰ )
Westartbyprovingthatif ğ‘“ : ğ· â†’ ğ· isacontinuousfunctiononaccpo (ğ·,âŠ‘) andifğ‘‘ âˆˆ ğ· issuchthat
ğ‘“(ğ‘‘) âŠ‘ ğ‘‘,thenğ¹ğ¼ğ‘‹(ğ‘“) âŠ‘ ğ‘‘.Toprovethis,werecallthatbythemonotonicityof ğ‘“ itmustbethecasethat
ğ‘“(ğ‘“(ğ‘‘)) âŠ‘ ğ‘“(ğ‘‘),andmoregenerally,thatğ‘“ğ‘›+1(ğ‘‘) âŠ‘ ğ‘“ğ‘›(ğ‘‘),âˆ€ğ‘› â‰¥0.Thereforeğ‘‘isanupperboundofthechain
{ğ‘“ğ‘›(ğ‘‘) |ğ‘› â‰¥0}.Then,sinceâŠ¥âŠ‘ğ‘‘,wehavethatğ‘“ğ‘›(âŠ¥) âŠ‘ ğ‘“ğ‘›(ğ‘‘),âˆ€ğ‘› â‰¥0andğ‘‘isalsoanupperboundforthe
chain{ğ‘“ğ‘›(âŠ¥) |ğ‘› â‰¥0}.Thenğ¹ğ¼ğ‘‹(ğ‘“)=(cid:195){ğ‘“ğ‘›(âŠ¥) |ğ‘› â‰¥0}âŠ‘ğ‘‘sincetheleastfixedpointofğ‘“ istheleastupper
boundofthatchain.
Thisresultprovesthatğ¹ğ¼ğ‘‹(ğ¹) âŠ‘ Sğ‘ ğ‘œğ‘ [[Nâ˜…]]G,ğœ‰,ontheconditionthatweshowthatğ¹(Sğ‘ ğ‘œğ‘ [[Nâ˜…]]G,ğœ‰ ) âŠ‘
Sğ‘ ğ‘œğ‘ [[Nâ˜…]]G,ğœ‰.Wecanprovethisbyfirstshowingthat
Sğ‘ ğ‘œğ‘ [[Nâ˜… ]]G,ğœ‰ =Sğ‘ ğ‘œğ‘ [[((N||ğœ„);ğœ“ â‰ƒğœ–||ğœ„);(ğœ„âŠ•N;Nâ˜… )]]G,ğœ‰
âŠ’Sğ‘ ğ‘œğ‘ [[(ğœ„âŠ•N;Nâ˜… )]]G,ğœ‰ â—¦Sğ‘ ğ‘œğ‘ [[((N||ğœ„);ğœ“ â‰ƒğœ–||ğœ„)]]G,ğœ‰
=ğ‘ğ‘œğ‘›ğ‘‘(ğœ‹ ğ¿,Sğ‘ ğ‘œğ‘ [[ğœ„]]G,ğœ‰ â—¦ğœ‹ ğ‘…,Sğ‘ ğ‘œğ‘ [[N;Nâ˜… ]]G,ğœ‰ â—¦ğœ‹ ğ‘…)â—¦Sğ‘ ğ‘œğ‘ [[((N||ğœ„);ğœ“ â‰ƒğœ–||ğœ„)]]G,ğœ‰
=ğ‘ğ‘œğ‘›ğ‘‘(Sğ‘ ğ‘œğ‘ [[(N||ğœ„);ğœ“ â‰ƒğœ–]]G,ğœ‰,Sğ‘ ğ‘œğ‘ [[ğœ„]]G,ğœ‰,Sğ‘ ğ‘œğ‘ [[N;Nâ˜… ]]G,ğœ‰ )
âŠ’ğ‘ğ‘œğ‘›ğ‘‘(Sğ‘ ğ‘œğ‘ [[(N||ğœ„);ğœ“ â‰ƒğœ–]]G,ğœ‰,Sğ‘ ğ‘œğ‘ [[ğœ„]]G,ğœ‰,Sğ‘ ğ‘œğ‘ [[Nâ˜… ]]G,ğœ‰ â—¦Sğ‘ ğ‘œğ‘ [[N]]G,ğœ‰ )
TheinductionhypothesisgivesusthatSğ‘‘ğ‘ [[N]]G,ğœ‰ âŠ‘Sğ‘ ğ‘œğ‘ [[N]]G,ğœ‰,andbythemonotonicityofâ—¦(Lemma5.3)
andğ‘ğ‘œğ‘›ğ‘‘(Lemmas5.5and5.6)weget
Sğ‘ ğ‘œğ‘ [[Nâ˜… ]]G,ğœ‰ âŠ’ğ‘ğ‘œğ‘›ğ‘‘(Sğ‘ ğ‘œğ‘ [[(N||ğœ„);ğœ“ â‰ƒğœ–]]G,ğœ‰,Sğ‘ ğ‘œğ‘ [[ğœ„]]G,ğœ‰,Sğ‘ ğ‘œğ‘ [[Nâ˜… ]]G,ğœ‰ â—¦Sğ‘ ğ‘œğ‘ [[N]]G,ğœ‰ )
âŠ’ğ‘ğ‘œğ‘›ğ‘‘(Sğ‘‘ğ‘ [[(N||ğœ„);ğœ“ â‰ƒğœ–]]G,ğœ‰,ğ‘–ğ‘‘,Sğ‘ ğ‘œğ‘ [[Nâ˜… ]]G,ğœ‰ â—¦Sğ‘‘ğ‘ [[N]]G,ğœ‰ )
=ğ¹(Sğ‘ ğ‘œğ‘ [[Nâ˜… ]]G,ğœ‰
)
ThiscompletesourproofofLemma6.3.
â–¡
7 TYPESOUNDNESS
Wesaythatağœ‡Gtermiswell-typedwheneveritcanbetypedwiththerulesofTable2.Theserulesguaranteethat
anywell-typedğœ‡Gtermdefinesagraphneuralnetwork.Next,wewillbeusingthesetypingrulestoprovethetype
soundnessoftheoperationalsemanticsofğœ‡G.Provingthetypesafetyofğœ‡GensuresthattheGNNsweprogramwithit
handlethedatatypescorrectlyandproduceresultsoftheexpectedtype.Concretely,thismeansthatwell-typedGNNs
canalwaysprogresstothenextstepofcomputationandthataftereachstepthetypeoftheoutputispreserved.
ManuscriptsubmittedtoACM20 Belenchiaetal.
[T-ID] ğœ„:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡]
Î“âŠ¢ğ‘“
ğœ“
:ğ‘‡ 1âˆ—Ã—ğ‘‡1â†’ğ‘‡2
[T-PSI]
ğœ“ :Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2]
Î“âŠ¢ğ‘“
ğœ‘
:ğ‘‡1Ã—ğ‘‡
ğ‘’
Ã—ğ‘‡1â†’ğ‘‡2 Î“âŠ¢ğ‘“
ğœ
:ğ‘‡ 2âˆ—Ã—ğ‘‡1â†’ğ‘‡3
[T-PRE]
ğœ‘
â—
ğœ
:Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡3]
Î“âŠ¢ğ‘“
ğœ‘
:ğ‘‡1Ã—ğ‘‡
ğ‘’
Ã—ğ‘‡1â†’ğ‘‡2 Î“âŠ¢ğ‘“
ğœ
:ğ‘‡ 2âˆ—Ã—ğ‘‡1â†’ğ‘‡3
[T-POST]
ğœ‘
â–·
ğœ
:Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡3]
N1:Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2] N2:Î¦ G,ğœ‰[ğ‘‡2,ğ‘‡3]
[T-SEQ]
N1;N2:Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡3]
N1:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡1] N2:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡2]
[T-PAR]
N1||N2:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡1Ã—ğ‘‡2]
N1:Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡ 1â€²] N2:Î¦ G,ğœ‰[ğ‘‡2,ğ‘‡ 2â€²]
[T-PROD]
N1âŠ—N2:Î¦ G,ğœ‰[ğ‘‡1Ã—ğ‘‡2,ğ‘‡ 1â€²Ã—ğ‘‡ 2â€²]
N1:Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2] N2:Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2]
[T-CHOICE]
N1âŠ•N2:Î¦ G,ğœ‰[BÃ—ğ‘‡1,ğ‘‡2]
N :Î¦ G,ğœ‰[ğ‘‡,ğ‘‡]
[T-STAR]
Nâ˜…:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡]
Table2. Typingrulesofğœ‡G.
Theorem7.1(Typesoundness). IfN :Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2],ğœ‚ :ğ»[ğ‘‡1]andâŸ¨N,ğœ‚âŸ©â†’â˜…
G,ğœ‰
ğœ‚â€²,thenğœ‚â€² :ğ»[ğ‘‡2].
Proof. FollowsfromLemma7.2(Progress)andLemma7.3(Preservation). â–¡
Lemma7.2(Progress). SupposethatN isawell-typedtermN :Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2]andthatğœ‚ :ğ»[ğ‘‡1],theneitherN =ğœ„or
thereexistsNâ€²,ğœ‚â€²suchthatâŸ¨N,ğœ‚âŸ©â†’ âŸ¨Nâ€²,ğœ‚â€²âŸ©.
G,ğœ‰
Proof. Byinductionontypingderivations.ItsufficestoshowthateitherN =ğœ„orthereexistsanoperationalrule
thatcanbeapplied.
CaseT-ID: Immediate,asN =ğœ„.
CaseT-PSI: WehaveN =ğœ“ andthereforecanuseruleAPPLY.
ğœ‘
CaseT-PRE: WehaveN =â— ğœ andthereforecanuserulePREIMG.
ğœ‘
CaseT-POST: WehaveN =â–· ğœ andthereforecanuserulePOSTIMG.
CaseT-SEQ: WehaveN =N1;N2whereN1 :Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡]andN2 :Î¦ G,ğœ‰[ğ‘‡,ğ‘‡2].ByinductionhypothesisN1is
eitherğœ„oritispossibletoapplyatransitionrule.Intheformercase,wecanapplyruleSEQ2,whileinthelatter
casewecanapplyruleSEQ1.
CaseT-PAR: WehaveN = N1||N2 whereN1 : Î¦ G,ğœ‰[ğ‘‡,ğ‘‡1] andN2 : Î¦ G,ğœ‰[ğ‘‡,ğ‘‡2].Inthiscase,wecanuserule
SPLIT.
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 21
CaseT-PROD: WehaveN =N1âŠ—N2whereN1:Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡ 1â€²]andN2:Î¦ G,ğœ‰[ğ‘‡2,ğ‘‡ 2â€²].ByinductionhypothesisN1
andN2areeitherğœ„orisitpossibletoapplyatransitionrule.Then,
â€¢ ifN1=ğœ„andN2=ğœ„,wecanapplyruleMERGE.
â€¢ ifN1â‰ ğœ„andN2=ğœ„,wecanapplyrulePAR1.
â€¢ ifN1=ğœ„andN2â‰ ğœ„,wecanapplyrulePAR2.
â€¢ ifN1â‰ ğœ„andN2â‰ ğœ„,wecanapplyeitherrulePAR1orrulePAR2.
CaseT-CHOICE: We have N = N1 âŠ• N2 : Î¦ G,ğœ‰[BÃ—ğ‘‡1,ğ‘‡2] where N1 : Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2] and N2 : Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2].
Furthermore,wehavethatğœ‚ :ğ»[BÃ—ğ‘‡1].Ifğœ‹ ğ¿(ğœ‚)istheconstantnode-labelingfunctionthatmapseverynode
toTrue,weapplyruleCHOICE1.InanyothercaseweapplyruleCHOICE2.
CaseT-STAR: WehaveN =N 1â˜…whereN1:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡].WecanapplyruleSTAR.
â–¡
Lemma7.3(Preservation). IfN :Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2],ğœ‚ :ğ»[ğ‘‡1],andâŸ¨N,ğœ‚âŸ©â†’
G,ğœ‰
ğ›¾
â€¢ ifğ›¾ =âŸ¨Nâ€²,ğœ‚â€²âŸ©,thenNâ€² :Î¦ G,ğœ‰[ğ‘‡â€²,ğ‘‡2]andğœ‚â€² :ğ»[ğ‘‡â€²].
â€¢ ifğ›¾ =ğœ‚â€²,thenğœ‚â€² :ğ»[ğ‘‡2].
Proof. Byinductionontypingderivations.
CaseT-ID: WehaveN =ğœ„:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡]andğœ‚ :ğ»[ğ‘‡].TheonlyapplicableruleisIDbywhichâŸ¨ğœ„,ğœ‚âŸ©â†’
G,ğœ‰
ğœ‚ :ğ»[ğ‘‡].
CaseT-PSI: Wehaveğœ“ :Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2]andğœ‚ :ğ»[ğ‘‡1].TheonlyrulethatcanbeappliedisAPPLYwhichgivesNâ€² =ğœ„
andğœ‚â€² =ğœ†ğ‘£.ğ‘“ ğœ“([ğœ‚(ğ‘¢) |ğ‘¢ âˆˆV],ğœ‚(ğ‘£)).Thenğœ‚â€²hastypeğ»[ğ‘‡2]andğœ„hastypeÎ¦ G,ğœ‰[ğ‘‡2,ğ‘‡2]byruleT-ID.
ğœ‘
CaseT-PRE: Wehaveâ— ğœ :Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡3]andğœ‚ :ğ»[ğ‘‡1].TheonlyrulethatcanbeappliedisPREIMGwhichgives
â†’âˆ’
Nâ€² = ğœ„ andğœ‚â€² = ğœ†ğ‘£.ğ‘“ ğœ([ğ‘“ ğœ‘(ğœ‚(ğ‘¢),ğœ‰((ğ‘£,ğ‘¢)),ğœ‚(ğ‘£)) | ğ‘¢ âˆˆ ğ‘(ğ‘£)],ğœ‚(ğ‘£)).Thenğœ‚â€² hastypeğ»[ğ‘‡3] andğœ„ hastype
Î¦ G,ğœ‰[ğ‘‡3,ğ‘‡3]byruleT-ID.
CaseT-POST: AnalogoustoT-PRE,butusingrulePOSTIMGinstead.
CaseT-SEQ: WehaveN1;N2 : Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡3] andğœ‚ : ğ»[ğ‘‡1].Wealsohavesub-derivationswithconclusionsthat
N1:Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2]andN2:Î¦ G,ğœ‰[ğ‘‡2,ğ‘‡3].Therearetwopossiblerulesthatcanbeapplied,SEQ1andSEQ2.
â€¢ InthecaseofSEQ1,wehaveNâ€² =N 1â€²;N2.ByapplyingtheinductionhypothesisonN1wegetthatğœ‚â€² :ğ»[ğ‘‡]
andN 1â€² :Î¦ G,ğœ‰[ğ‘‡,ğ‘‡2]forsomenode-labelingtypeğ»[ğ‘‡].ThenwecanapplyruleT-SEQonN 1â€²;N2togetits
typeÎ¦ G,ğœ‰[ğ‘‡,ğ‘‡3].
â€¢ InthecaseofSEQ2,thenN1 =ğœ„,ğ‘‡2 =ğ‘‡1andwehaveNâ€² =N2whichweknowhastypeÎ¦ G,ğœ‰[ğ‘‡1,ğ‘‡3],and
ğœ‚â€² =ğœ‚whichweknowhastypeğ»[ğ‘‡1].
CaseT-PAR: WehaveN1||N2:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡1Ã—ğ‘‡2]andğœ‚ :ğ»[ğ‘‡].Wealsohavesub-derivationswithconclusionsthat
N1:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡1]andN2:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡2].TheonlyrulethatcanbeappliedisSPLIT.Thenğœ‚â€² =ğœ‚|ğœ‚ :ğ»[ğ‘‡ Ã—ğ‘‡]and
Nâ€² =N1âŠ—N2whichbyruleT-PRODhastypeÎ¦ G,ğœ‰[ğ‘‡ Ã—ğ‘‡,ğ‘‡1Ã—ğ‘‡2].
CaseT-PROD: WehaveN1âŠ—N2 :Î¦ G,ğœ‰[ğ‘‡1Ã—ğ‘‡2,ğ‘‡ 1â€²Ã—ğ‘‡ 2â€²]andğœ‚ :ğ»[ğ‘‡1Ã—ğ‘‡2].Wealsohavesub-derivationswith
conclusionsthatN1 : Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡ 1â€²] andN2 : Î¦ G,ğœ‰[ğ‘‡2,ğ‘‡ 2â€²].Therearethreepossiblerulesthatcanbeapplied,
PAR1,PAR2,andMERGE.
â€¢ InthecaseofPAR1,wehaveNâ€² =N 1â€²âŠ—N2andğœ‚â€² =ğœ‚1|ğœ‹ ğ‘…(ğœ‚).ByapplyingtheinductionhypothesisonN1
wegetthatğœ‚1:ğ»[ğ‘‡]andN 1â€² :Î¦ G,ğœ‰[ğ‘‡,ğ‘‡ 1â€²]forsomenode-labelingtypeğ»[ğ‘‡].ThenwecanapplyruleT-Prod
onN 1â€²âŠ—N2togetitstypeÎ¦ G,ğœ‰[ğ‘‡ Ã—ğ‘‡2,ğ‘‡ 1â€²Ã—ğ‘‡ 2â€²],andğœ‚â€² :ğ»[ğ‘‡ Ã—ğ‘‡2].
â€¢ ThecaseofPAR2isanalogoustothatofPAR1.
ManuscriptsubmittedtoACM22 Belenchiaetal.
â€¢ InthecaseofMERGE,wehaveNâ€² = ğœ„ andğœ‚â€² = ğœ‚.Furthermore,thisrulecouldhavebeenappliedonlyif
N1=N2=ğœ„,whichmeansthatğ‘‡1=ğ‘‡ 1â€²andğ‘‡2=ğ‘‡ 2â€².ThenbyruleT-Idwegetthatğœ„:Î¦ G,ğœ‰[ğ‘‡ 1â€²Ã—ğ‘‡ 2â€²,ğ‘‡ 1â€²Ã—ğ‘‡ 2â€²].
CaseT-CHOICE: We have N1 âŠ•N2 : Î¦ G,ğœ‰[BÃ—ğ‘‡1,ğ‘‡2] andğœ‚ : ğ»[BÃ—ğ‘‡1]. We also have sub-derivations with
conclusionsthatN1 : Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2] andN2 : Î¦ G,ğœ‰[ğ‘‡1,ğ‘‡2].Therearetwopossiblerulesthatcanbeapplied,
CHOICE1andCHOICE2.
â€¢ IncaseCHOICE1,wehaveğœ‚â€² =ğœ‹ ğ‘…(ğœ‚):ğ»[ğ‘‡1]andNâ€² =N1whichweknowtohavetypeÎ¦ G,ğœ‰[ğ‘‡1,ğ‘‡2].
â€¢ IncaseCHOICE2,wehaveğœ‚â€² =ğœ‹ ğ‘…(ğœ‚):ğ»[ğ‘‡1]andNâ€² =N2whichweknowtohavetypeÎ¦ G,ğœ‰[ğ‘‡1,ğ‘‡2].
CaseT-STAR: WehaveNâ˜…:Î¦ G,ğœ‰[ğ‘‡,ğ‘‡]andğœ‚ :ğ»[ğ‘‡].WealsohaveasubderivationwiththeconclusionthatN :
Î¦ G,ğœ‰[ğ‘‡,ğ‘‡].TheonlyrulethatcanbeappliedisSTAR.Thenğœ‚â€² =ğœ‚ :ğ»[ğ‘‡]andNâ€² =((ğœ„||N);ğœ“(cid:27)||ğœ„);(ğœ„âŠ•N;Nâ˜…).
ByapplyingthetypingruleswecanconcludethatthistermalsohastypeÎ¦ G,ğœ‰[ğ‘‡,ğ‘‡].
â–¡
8 AGRAPHICALREPRESENTATION
Inthissection,wepresentagraphicalrepresentationforğœ‡Gprogramsandexpressions.Usingsuchgraphicalnotation
makesiteasiertounderstandandcommunicatethestructureandpurposeofağœ‡Gprogram,astheprogrammermight
easilylosetrackoflabeltypesandsizeswhendevelopingintextualform.Thebasictermsofğœ‡Gcanberepresented
graphicallyasboxes,asshowninFigure1,whilethecompositetermsofthelanguageareshowninFigure2.Eachterm
hasexactlyoneinputnode-labelingandoneoutputnode-labeling,andthegraphicalrepresentationisbuilttop-down
startingfromthemain(usuallycomposite)termandsubstitutingeachboxwitheitherabasictermoracompositeone;
inthislastcase,thisprocedureisrepeatedrecursivelyuntilallboxesinthefigurearebasicterms.Asanillustrative
example,letğœ“1,ğœ“2,ğœ“3,ğœ‘,ğœbefunctionsymbols.Figure3showshowwecanrepresentgraphicallytheğœ‡Gexpression
ğœ‘
(ğœ“1||ğœ“2);ğœ“3;â— ğœ.
Fig.1. Graphicalrepresentationofbasicğœ‡Gexpressions.
Inthegraphicalrepresentationforğœ‡Gexpressionswehaveusedsomeauxiliaryfunctionswhichwedescribeinturn:
â€¢ Thefunctionâ€¢:ğ»[ğ‘‡1] â†’ğ»[ğ‘‡1]Ã—ğ»[ğ‘‡1],â€¢(ğœ‚)=(ğœ‚,ğœ‚)mapsanode-labelingfunctiontoatuplecontainingtwo
copiesofitself.
â€¢ Thefunctionâ—¦:ğ»[ğ‘‡1]Ã—ğ»[ğ‘‡2] â†’ğ»[ğ‘‡1Ã—ğ‘‡2],
â—¦(ğœ‚1,ğœ‚2)=ï£±ï£´ï£´ï£´ï£´ï£´ï£²ğœ‚ ğœ‚1
2
i if fğœ‚ ğœ‚2 1= =ğœ‚ ğœ‚âŠ¥
âŠ¥
ï£´ï£´ï£´ï£´ï£´ğœ‚1|ğœ‚2
otherwise
ï£³
mapstwonode-labelingfunctionstoanewnode-labelingfunctionwhichlabelseachnodewiththeconcatenation
oftheirlabels.Ifoneoftheinputlabelingfunctionsistheemptylabeling,â—¦simplyreturnstheotherfunction.
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 23
(a) The sequential composition of (b)Theparallelcompositionofgraphneu-
graphneuralnetworksğœ™1andğœ™2. ralnetworksğœ™1andğœ™2.
(c)Thechoiceoperatorforthegraphneuralnet-
worksğœ™1andğœ™2.
(d)Thestargraphneuralnetworkthatcomputesthefixedpointofğœ™1.
Fig.2. Graphicalrepresentationofthepossiblecompositionsofğœ‡Gexpressions.
Fig.3.
Examplegraphicalrepresentationoftheğœ‡Gexpression(ğœ“1||ğœ“2);ğœ“3;â—ğœ‘
ğœ.
â€¢ Thefunctionâ‹„:ğ»[BÃ—ğ‘‡1] â†’ğ»[ğ‘‡1]Ã—ğ»[ğ‘‡1],
â‹„(ğœ‚)=ï£±ï£´ï£´ï£²(ğœ‹ ğ‘…(ğœ‚),ğœ‚ âŠ¥) ifğœ‹ ğ¿(ğœ‚)=ğœ†ğ‘£.True
ï£´ï£´(ğœ‚ âŠ¥,ğœ‹ ğ‘…(ğœ‚)) otherwise
ï£³
takesaBooleannode-labelingfunctionandanotherlabelingfunctionandpropagatesitinoutputasthefirstor
secondelementofatuple.IftheBooleanlabelingfunctionmapseverynodetoatruevalue,theotherlabeling
functionispropagatedasthefirstelementofthetuple(tobereceivedininputbytheGNNtobeexecutedwhen
thetestissatisfied),otherwiseitisreturnedasthesecondelementofthetuple.
ManuscriptsubmittedtoACM24 Belenchiaetal.
9 IMPLEMENTATION
Theğœ‡GlanguagehasbeenimplementedasaPythonlibrarycalledlibmg,whichwedescribedindetailinaprevious
work[7].ThelibrarywasdevelopedontopofSpektral[15],agraphneuralnetworklibrarybasedonTensorflow[1].
ThemainadvantageofusingaframeworksuchasTensorFlowisthatğœ‡G programsarecompiledtographneural
networksthatcanbeexecutedseamlesslyonCPUs,GPUs,orinadistributedsettingwithoutchangingthesourcecode.
The functionalities of libmg include the typical language support tools in the form of a parser, unparser and
normalizerforğœ‡Gexpressions.Themainfunctionalitythatisprovidedconsistsintheğœ‡Gcompilerthattransformsa
ğœ‡GprogramintoaTensorFlowModelinstance.Thecompilercanbesetuptoautomaticallymemoizesub-expressions
thatoccurmultipletimesinthesameprogram.Thelibraryalsoimplementsbasicgraphvisualizationfunctions,that
allowstoviewinanywebbrowsertheinputoroutputgraphs,orintermediaterepresentationproducedbyağœ‡Gmodel.
Finally,asimpleexplanatoryalgorithmisprovidedthatcomputesthesub-graphthatwasusedbyağœ‡G modelto
computethelabelofagivennode.
Usinglibmg,theğœ“,ğœ‘,andğœ functionsaredefinedbyinstantiatingthecorrespondingclassesandarestoredin
dictionaries.Thesedictionaries,whichcontainamappingbetweenfunctionnamesandthefunctionsthemselves,are
usedtocreateağœ‡Gcompilerinstance.Theğœ‡Gcompilercanthenparseğœ‡Gprogramscontainingthefunctionnames
definedinitsdictionaries.
ThemodelsgeneratedbythecompilercanbetrainedbyTensorFlowasusual.Onlyinthecaseofthestaroperator,
thecorrespondinglayerimplementsacustomgradientcomputation.Inthatcaseimplicitdifferentiationisusedasto
makethecomputationofthegradienttractable.
10 EVALUATION
Weevaluateğœ‡Gbothintermsofitsexpressiveness,meaningthatitcanbeusedtodefinemost,ifnotall,ofthegraph
neuralnetworkarchitecturesthathavebeendevelopedovertheyears,andalsoasaspecificationlanguageforthe
developmentofgraphneuralnetworksforspecifictasks.InSection10.1wedescribeanupdatedversionoftheCTL
modelcheckerdevelopedusingğœ‡Gintroducedinourpreviouswork[6,7].Then,inSection10.2,weshowhowto
definesomeofthemorewell-knownGNNarchitecturesusingourlanguage.
10.1 ModelChecking
Weevaluatedğœ‡GinapreviousworkbyusingittodefineaGNNthatperformsCTLmodelcheckingonKripkestructures.
Forthatusecase,thesetoffunctionsthatweusedisshowninTable3,whileinTable4weshowthetranslationfunction
fromCTLtotheğœ‡GexpressiondenotingtheGNNthatverifiesit.
Theğœ‡Gmodelcheckerimplementationwascomparedtotwootherexplicit-statemodelcheckers,pyModelChecking1
andmCRL2[9].WeusedsomeofthebenchmarksfromtheModelCheckingContest2022[2],namelythoseforwhich
wecouldexplicitlygeneratethereachabilitygraphonourmachine,showninTable5.Theğœ‡Gmodelcheckerwas
setuptoeitherverifyalltheformulasinparallel(thefullsetup),oronebyoneaswouldhavebeenthecaseforthe
othermodelcheckers(thesplitsetup).Intheformercase,therewouldhavebeenadditionaladvantagesduetothe
memoizationofcommonsub-formulasacrossmultipleformulas.TheexperimentalresultsareshowninFigure4,and
wenoticethatforthelargerKripkestructuresthereisanevidentspeedadvantageofğœ‡Gcomparedtotheothermodel
checkers(insomeinstances,byanorderofmagnitude).
1https://github.com/albertocasagrande/pyModelChecking
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 25
(cid:40)
True ifğ‘ âˆˆğ‘¥
ğœ“
ğ‘
:(2ğ´ğ‘ƒ)â˜…Ã—2ğ´ğ‘ƒ â†’B ğœ“ ğ‘(ğ‘‹,ğ‘¥)= (âˆ€ğ‘ âˆˆğ´ğ‘ƒ)
False otherwise
ğœ“
ğ‘¡ğ‘¡
:(2ğ´ğ‘ƒ)â˜…Ã—2ğ´ğ‘ƒ â†’B ğœ“ ğ‘¡ğ‘¡(ğ‘‹,ğ‘¥)=True
ğœ“
ğ‘“ğ‘“
:(2ğ´ğ‘ƒ)â˜…Ã—2ğ´ğ‘ƒ â†’B ğœ“ ğ‘“ğ‘“(ğ‘‹,ğ‘¥)=False
ğœ“ Â¬:(B)â˜…Ã—Bâ†’B ğœ“ Â¬(ğ‘‹,ğ‘¥)=Â¬ğ‘¥
ğœ“ âˆ§:(B2)â˜…Ã—B2â†’B ğœ“ âˆ§(ğ‘‹,ğ‘¥)=ğ‘¥1âˆ§ğ‘¥2
ğœ“ âˆ¨:(B2)â˜…Ã—B2â†’B ğœ“ âˆ¨(ğ‘‹,ğ‘¥)=ğ‘¥1âˆ¨ğ‘¥2
ğœ‘
ğ‘–ğ‘‘
:BÃ—()Ã—Bâ†’B ğœ‘ ğ‘–ğ‘‘(ğ‘–,ğ‘’,ğ‘—)=ğ‘–
ğœ âˆ¨:Bâ˜…Ã—Bâ†’B ğœ âˆ¨(ğ‘€,ğ‘¥)=(cid:212) ğ‘–ğ‘€
ğ‘–
Table3. Theğœ“,ğœ‘,andğœfunctionsusedforCTLmodelchecking.Thesubscriptsindicatethefunctionsymbolsthatwillbeusedinthe
ğœ‡Gexpressions.Foreachğ‘ âˆˆğ´ğ‘ƒ,weconsideracorrespondingfunctionğœ“ğ‘.
M(ğ‘)=ğ‘
M(Â¬ğœ™)=M(ğœ™);Â¬
M(ğœ™1âˆ§ğœ™2)=(M(ğœ™1)||M(ğœ™2));âˆ§
M(EXğœ™)=M(ğœ™);â–·ğ‘–ğ‘‘
âˆ¨
M(EGğœ™)=fix ğ‘‹ =ğ‘¡ğ‘¡ in (M(ğœ™)||ğ‘‹;â–·ğ‘–ğ‘‘ );âˆ§
âˆ¨
M(Eğœ™1Uğœ™2)=fix ğ‘‹ =ğ‘“ğ‘“ in (M(ğœ™2)||(M(ğœ™1)||ğ‘‹;â–·ğ‘– âˆ¨ğ‘‘ );âˆ¨);âˆ§
Table4. RecursivedefinitionofthetranslationfunctionM:ğœ™â†’N.
ID PetriNet #nodesinRG #edgesinRG
1 RobotManipulation-PT-00001 110 274
2 TokenRing-PT-005 166 365
3 Philosophers-PT-000005 243 945
4 RobotManipulation-PT-00002 1430 5500
5 Dekker-PT-010 6144 171530
6 BART-PT-002 17424 53328
7 ClientsAndServers-PT-N0001P0 27576 113316
8 Philosophers-PT-000010 59049 459270
9 Referendum-PT-0010 59050 393661
10 SatelliteMemory-PT-X00100Y0003 76358 209484
11 RobotManipulation-PT-00005 184756 1137708
12 Dekker-PT-015 278528 16834575
13 HouseConstruction-PT-00005 1187984 7191110
Table5. Petrinetmodelsusedfortheexperiments,forwhichwereportedherethenumberofnodesandedgesinthereachability
graph(RG).
10.2 SpecificationofGraphNeuralNetworks
Inthissection,weshowhowtodefinesomeofthemostpopulargraphneuralnetworkmodelsinğœ‡G.Asisthecasefor
anyprogramminglanguage,therearemanydifferentwaystoobtainthesameresult,thereforetheimplementations
ManuscriptsubmittedtoACM26 Belenchiaetal.
Fig.4. Executiontimesofğœ‡G(splitandfullsetups),pyModelChecking,andmCRL2across13PetrinetmodelsfromtheModel
CheckingContest2022.They-axisisonalogarithmicscale.OnthetwelfthPetrinet,theredbarindicatesthattheğœ‡Gmodelranout
ofmemoryafterthepre-processing(tracing)step.
thatweshowhereareonlyintendedtohighlightthegeneralityofthelanguage.Forspecificusecases,itisuptothe
programmertochoosethemostusefulğœ‡Gimplementation.
Graph Convolutional Networks. A Graph Convolutional Network [21] (GCN) performs a graph convolution by
multiplyingthenodefeatureswithamatrixofweights,normalizedaccordingtothedegreeofthenodesandassigning
greaterimportancetothefeaturesreceivedfromnodeswithfewerneighbors.Thenewnodelabelsğ‘¥â€² âˆˆ Rğ‘š are
ğ‘–
computedfromthecurrentnodelabelsğ‘¥
ğ‘–
âˆˆRğ‘›accordingtotheequation
ğ‘¥ Î˜
ğ‘¥ ğ‘–â€² =ğ‘“ (cid:169)
(cid:173)
âˆ‘ï¸ âˆšï¸ğ‘‘ğ‘’ğ‘”(ğ‘–)ğ‘— âˆšï¸ğ‘‘ğ‘’ğ‘”(ğ‘—)(cid:170)
(cid:174)
(cid:171)ğ‘—âˆˆğ‘ G(ğ‘–)âˆª{ğ‘–}
(cid:172)
whereğ‘‘ğ‘’ğ‘”(ğ‘–)isthedegreeofnodeğ‘–,Î˜âˆˆRğ‘›Ã—ğ‘šisamatrixoftrainableweights,andğ‘“ isanactivationfunction.Theset
ofneighborsofanodeincludeboththepredecessorsandthesuccessors,astheGCNwasoriginallythoughtforthe
caseofundirectedgraphs.ToimplementtheGCN,wemakeuseofthefunctionsshowninTable6.Westartbydefining
ağœ‡Gexpressiontocomputethedegreeofeachnode.Werecallthatthedegreeofanodeisthenumberofitsoutgoing
edges(theoutdegree)plusthenumberofincomingedges(theindegree),withselfloopscountingasboth.Itiseasyto
computethisnumberinğœ‡G:weuseâ—1 tocomputetheindegreeandâ–·1 tocomputetheoutdegree,thenweaddthem.
+ +
(â—1 +||â–·1 +);+2
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 27
ğœ“
ğ‘ğ‘
:(Rğ‘›)â˜…Ã—Rğ‘› â†’Rğ‘š ğœ“ ğ‘ğ‘(ğ‘‹,ğ‘¥)=ğ‘“(ğ‘¥ğ‘‡Î˜)
ğœ“
+2
:(R2)â˜…Ã—R2â†’R ğœ“ +2(ğ‘‹,ğ‘¥)=ğ‘¥1+ğ‘¥2
ğœ“
+2ğ‘›
:(R2ğ‘›)â˜…Ã—R2ğ‘› â†’Rğ‘› ğœ“ +2ğ‘›(ğ‘‹,ğ‘¥)=ğ‘¥1+ğ‘¥2
ğœ‘1:Rğ‘›Ã—RÃ—Rğ‘› â†’R ğœ‘1(ğ‘–,ğ‘’,ğ‘—)=1
ğœ‘ ğ‘‘ğ‘”ğ‘› :Rğ‘›+1Ã—RÃ—Rğ‘›+1â†’Rğ‘› ğœ‘ ğ‘‘ğ‘”ğ‘›(ğ‘–,ğ‘’,ğ‘—)= âˆš ğ‘–1ğ‘–2âˆš ğ‘—1
ğœ +:Râ˜…Ã—Rğ‘› â†’R ğœ +(ğ‘€,ğ‘¥)=1+(cid:205) ğ‘–ğ‘€
ğ‘–
ğœ ğ‘‘ğ‘”+:(Rğ‘›)â˜…Ã—Rğ‘›+1â†’Rğ‘› ğœ ğ‘‘ğ‘”+(ğ‘€,ğ‘¥)= ğ‘¥ğ‘¥ 12 +(cid:205) ğ‘–ğ‘€ ğ‘–
Table6. FunctionsusedinthedefinitionoftheGCN.
Inthestepsthatfollow,weneedthedegreeofnodesbutalsotheoriginallabels,thereforeweuseparallelcomposition
withğœ„tohaveboththesevalues.
(â—1 +||â–·1 +);+2||ğœ„
Thenwecompute(cid:205) ğ‘—âˆˆğ‘ G(ğ‘–)âˆª{ğ‘–} âˆš ğ‘‘ğ‘’ğ‘”(ğ‘–ğ‘¥ )âˆšğ‘—
ğ‘‘ğ‘’ğ‘”(ğ‘—)
bysummingthepre-imageandthepost-imageusingğ‘‘ğ‘”ğ‘›andğ‘‘ğ‘”+
((â—1 +||â–·1 +);+2||ğœ„);(â—ğ‘‘ ğ‘‘ğ‘” ğ‘”ğ‘› +||â–·ğ‘‘ ğ‘‘ğ‘” ğ‘”ğ‘› +);+2ğ‘›
Notethatbothğœ‘
ğ‘‘ğ‘”ğ‘›
andğœ ğ‘‘ğ‘”+expectnodelabelsğ‘™ âˆˆRğ‘›+1,whereğ‘™1 âˆˆRisthedegreeofthenodeandğ‘™2 âˆˆRğ‘› isthe
actuallabelofthenode.Finally,wemultiplythenodelabelswiththeweightmatrixÎ˜followedbytheapplicationof
theactivationfunctionğ‘“,i.e.,theclassicdenseneuralnetworklayer.Puttingallofthistogether,theğœ‡Gexpressionthat
implementsaGCNlayeris
((â—1 +||â–·1 +);+2||ğœ„);(â—ğ‘‘ ğ‘‘ğ‘” ğ‘”ğ‘› +||â–·ğ‘‘ ğ‘‘ğ‘” ğ‘”ğ‘› ğ‘Ÿ+);+2ğ‘›;ğ‘ğ‘
GraphAttentionNetworks. AGraphAttentionNetwork[34](GAT),liketheGCN,transformsthenodelabelsusinga
learnableweightmatrixÎ˜âˆˆRğ‘›Ã—ğ‘š.Thenitemploysaself-attentionmechanismğ‘:Rğ‘šÃ—Rğ‘š â†’R,sharedbyallthe
nodes,thatgiventhelabelsofnodeğ‘— andnodeğ‘–suchthat(ğ‘—,ğ‘–) âˆˆE,itindicatestheimportanceofnodeğ‘—â€™slabelsto
nodeğ‘–.Thisvalueisthennormalizedusingthesoftmaxfunction.Intheoriginalformulation,theattentionmechanismis
asingle-layerfeedforwardneuralnetworkwhichusesaweightvectoraâˆˆR2ğ‘šandtheLeakyReLUactivationfunction.
Therefore,theattentioncoefficientsğ›¼ aregivenby
ğ‘—ğ‘–
ğ‘’ğ‘¥ğ‘(cid:16) ğ¿ğ‘’ğ‘ğ‘˜ğ‘¦ğ‘…ğ‘’ğ¿ğ‘ˆ (cid:16) ağ‘‡(ğ‘¥ ğ‘–Î˜,ğ‘¥ ğ‘—Î˜)(cid:17)(cid:17)
ğ›¼ ğ‘—ğ‘– = (cid:205) ğ‘˜âˆˆâ† ğ‘âˆ’ G(ğ‘–)âˆª{ğ‘–}ğ‘’ğ‘¥ğ‘(cid:0)ğ¿ğ‘’ğ‘ğ‘˜ğ‘¦ğ‘…ğ‘’ğ¿ğ‘ˆ (cid:0) ağ‘‡(ğ‘¥ ğ‘–Î˜,ğ‘¥ ğ‘˜Î˜)(cid:1)(cid:1)
Giventheseattentioncoefficients,theGATlayercomputesthenewnodelabelsğ‘¥ ğ‘–â€² âˆˆRğ‘šfromthecurrentlabelsğ‘¥
ğ‘–
âˆˆRğ‘›
accordingtotheequation
ğ‘¥ ğ‘–â€² =ğ‘“ (cid:169)
(cid:173)
âˆ‘ï¸ ğ›¼ ğ‘—ğ‘–ğ‘¥ ğ‘—Î˜(cid:170)
(cid:174)
(cid:173) (cid:174)
â†âˆ’
(cid:171)ğ‘—âˆˆğ‘ G(ğ‘–)âˆª{ğ‘–}
(cid:172)
whereğ‘“ istheactivationfunction.Theauthorsthenextendthisbasicmechanismbyemployingmulti-headattention,
thatis,theydefineğ¾ independentattentionheadsandweightmatriceswhoseoutputsarethenconcatenated.Thus,we
get
ğ‘¥ ğ‘–â€² =(cid:13) (cid:13) (cid:13)ğ‘˜ğ¾ =1ğ‘“ (cid:169) (cid:173)
(cid:173)
âˆ‘ï¸ ğ›¼ğ‘˜ ğ‘—ğ‘–ğ‘¥ ğ‘—Î˜ğ‘˜(cid:170) (cid:174)
(cid:174)
â†âˆ’
(cid:171)ğ‘—âˆˆğ‘ G(ğ‘–)âˆª{ğ‘–}
(cid:172)
ManuscriptsubmittedtoACM28 Belenchiaetal.
ğœ“ğœ“ ğ‘ :ğ‘ (ğ‘– R: ğ‘›( +R 1ğ‘› )â˜…)â˜… Ã—Ã— RR ğ‘›ğ‘› +1â†’ â†’R Rğ‘š
ğ‘›
ğœ“ ğœ“ğ‘ (ğ‘ ğ‘‹ğ‘– ,( ğ‘¥ğ‘‹ ), =ğ‘¥) ğ‘¥=
1
ğ‘“(ğ‘¥Î˜ğ‘–)
/ / ğ‘¥2
ğœ‘
ğ‘ğ‘¡ğ‘¡âˆ—ğ‘–
:Rğ‘›Ã—RÃ—Rğ‘› â†’Rğ‘› ğœ‘ ğ‘ğ‘¡ğ‘¡âˆ—ğ‘–(ğ‘–,ğ‘’,ğ‘—)=ğ‘’ğ‘¥ğ‘(ğ¿ğ‘’ğ‘ğ‘˜ğ‘¦ğ‘…ğ‘’ğ¿ğ‘ˆ(ağ‘– Â·(ğ‘–ğ‘‡Î˜ğ‘–,ğ‘—ğ‘‡Î˜ğ‘–)))Â·ğ‘–
ğœ‘
ğ‘ğ‘¡ğ‘¡ğ‘–
:Rğ‘›Ã—RÃ—Rğ‘› â†’R ğœ‘ ğ‘ğ‘¡ğ‘¡ğ‘–(ğ‘–,ğ‘’,ğ‘—)=ğ‘’ğ‘¥ğ‘(ğ¿ğ‘’ğ‘ğ‘˜ğ‘¦ğ‘…ğ‘’ğ¿ğ‘ˆ(ağ‘– Â·(ğ‘–ğ‘‡Î˜ğ‘–,ğ‘—ğ‘‡Î˜ğ‘–)))
ğœ
ğ‘¡âˆ’ğ‘ğ‘¡ğ‘¡ğ‘–
:Râ˜…Ã—Rğ‘› â†’R ğœ ğ‘¡âˆ’ğ‘ğ‘¡ğ‘¡ğ‘–(ğ‘€,ğ‘¥)=ğ‘’ğ‘¥ğ‘(ğ¿ğ‘’ğ‘ğ‘˜ğ‘¦ğ‘…ğ‘’ğ¿ğ‘ˆ(ağ‘– Â·(ğ‘¥ğ‘‡Î˜ğ‘–,ğ‘¥ğ‘‡Î˜ğ‘–)))+(cid:205) ğ‘—ğ‘€
ğ‘—
ğœ
ğ‘›âˆ’ğ‘ğ‘¡ğ‘¡ğ‘–
:(Rğ‘›)â˜…Ã—Rğ‘› â†’Rğ‘› ğœ ğ‘›âˆ’ğ‘ğ‘¡ğ‘¡ğ‘–(ğ‘€,ğ‘¥)=ğ‘’ğ‘¥ğ‘(ğ¿ğ‘’ğ‘ğ‘˜ğ‘¦ğ‘…ğ‘’ğ¿ğ‘ˆ(ağ‘– Â·(ğ‘¥ğ‘‡Î˜ğ‘–,ğ‘¥ğ‘‡Î˜ğ‘–)))Â·ğ‘¥+(cid:205) ğ‘—ğ‘€
ğ‘—
Table7. FunctionsusedinthedefinitionofGATconvolutions.
ThefunctionsneededtoimplementtheGATlayerareshowninTable7.First,wecomputetheunnormalizedattention
coefficientsandmultiplythemwiththenodelabels,usingthetermâ—ğ‘ğ‘¡ğ‘¡âˆ—1 .Thentonormalizethem,wecompute
ğ‘›âˆ’ğ‘ğ‘¡ğ‘¡1
inparallelthetotalattentionvaluesforincomingedgesateachnodeusingâ—ğ‘ğ‘¡ğ‘¡1 ,anddividethelabelsofthefirst
ğ‘¡âˆ’ğ‘ğ‘¡ğ‘¡1
expressionbythisexpression
(â—ğ‘ğ‘¡ğ‘¡âˆ—1 ||â—ğ‘ğ‘¡ğ‘¡1 );/
ğ‘›âˆ’ğ‘ğ‘¡ğ‘¡1 ğ‘¡âˆ’ğ‘ğ‘¡ğ‘¡1
Atthispoint,wesimplyhavetomultiplybytheweightmatrixandapplytheactivationfunctiontogetthebasicGAT
layer
(â— ğ‘›ğ‘ğ‘¡ âˆ’ğ‘¡ ğ‘âˆ— ğ‘¡1 ğ‘¡1||â— ğ‘¡ğ‘ âˆ’ğ‘¡ğ‘¡ ğ‘1 ğ‘¡ğ‘¡1);/;ğ‘ğ‘1
Nowweshowhowtousemulti-headattention.Wesavethepreviousexpressioninğ‘˜ differentvariablesusinglet
expressions,suchthateachexpressiongetsitsownattentionmechanismandweightmatrix.Thenthebodyofthelet
expressionissimplytheparallelcompositionofthesevariables
let ğºğ´ğ‘‡1=(â— ğ‘›ğ‘ğ‘¡ âˆ’ğ‘¡ ğ‘âˆ— ğ‘¡1 ğ‘¡1||â— ğ‘¡ğ‘ âˆ’ğ‘¡ğ‘¡ ğ‘1 ğ‘¡ğ‘¡1);/;ğ‘ğ‘1,
.
.
.
ğºğ´ğ‘‡ ğ‘˜ =(â— ğ‘›ğ‘ğ‘¡ âˆ’ğ‘¡ ğ‘âˆ— ğ‘¡ğ‘˜ ğ‘¡ğ‘˜||â— ğ‘¡ğ‘ âˆ’ğ‘¡ğ‘¡ ğ‘ğ‘˜ ğ‘¡ğ‘¡ğ‘˜);/;ğ‘ğ‘ ğ‘˜ in
ğºğ´ğ‘‡1||ğºğ´ğ‘‡2||Â·Â·Â·||ğºğ´ğ‘‡
ğ‘˜
GraphIsomorphismNetworks. AGraphIsomorphismNetwork[36](GIN)generalizestheWeisfeiler-Lehmantest[35]
andthushasthegreatestdiscriminativepoweramongGNNsfordeterminingwhethertwographsarenotisomorphic.
TheGINconvolutionconsistsinmultiplyingeachnodelabelby(1+ğœ–),whereğœ– âˆˆRisatrainableparameter,thenadding
thesumofthelabelsoftheneighbornodes.Theobtainedvaluesarethenpassedininputtoamultilayerperceptron
(MLP),thatis,itgetsmultipliedwithalearnableweightmatrixÎ˜ğ‘– followedbytheapplicationofanactivationfunction
ğ‘“
ğ‘–
twoormoretimesinsequence.Formally,theGINcomputesthenewnodelabelsğ‘¥ ğ‘–â€² âˆˆRğ‘š fromthecurrentlabels
ğ‘¥
ğ‘–
âˆˆRğ‘›as
ğ‘¥ ğ‘–â€² =ğ‘€ğ¿ğ‘ƒ((1+ğœ–)Â·ğ‘¥ ğ‘– + âˆ‘ï¸ ğ‘¥ ğ‘—)
â†âˆ’
ğ‘—âˆˆğ‘ G(ğ‘–)
InTable8weshowthefunctionsweneedtoimplementtheGINlayerinğœ‡G.Weuseparallelcompositiontocompute
theproductofthenodelabelswith1+ğœ–andthesumofthelabelsoftheneighborsofeachnode,thenweaddthem
using+.
(âˆ—ğœ–||â—ğ‘–ğ‘‘ );+
+
Finally,weputinsequencetwodenseneuralnetworklayerswithweightmatricesÎ˜1 âˆˆ Rğ‘›Ã—ğ‘›â€²,Î˜2 âˆˆ Rğ‘›â€²Ã—ğ‘š and
activationfunctionsğ‘“1,ğ‘“2,asinthecaseoftheoriginalGINarticle.Therefore,theğœ‡Gexpressionthatimplementsa
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 29
ğœ“
ğ‘ğ‘1
:(Rğ‘›)â˜…Ã—Rğ‘› â†’Rğ‘›â€² ğœ“ ğ‘ğ‘1(ğ‘‹,ğ‘¥)=ğ‘“1(ğ‘¥Î˜1)
ğœ“
ğ‘ğ‘2
:(Rğ‘›â€²)â˜…Ã—Rğ‘›â€² â†’Rğ‘š ğœ“ ğ‘ğ‘2(ğ‘‹,ğ‘¥)=ğ‘“2(ğ‘¥Î˜2)
ğœ“
âˆ—ğœ–
:(Rğ‘›)â˜…Ã—Rğ‘› â†’Rğ‘› ğœ“ âˆ—ğœ–(ğ‘‹,ğ‘¥)=(ğ‘¤
ğœ–
+1)Â·ğ‘¥
ğœ“ +:(R2ğ‘›)â˜…Ã—R2ğ‘› â†’Rğ‘› ğœ“ +(ğ‘‹,ğ‘¥)=ğ‘¥1+ğ‘¥2
ğœ‘
ğ‘–ğ‘‘
:Rğ‘›Ã—RÃ—Rğ‘› â†’Rğ‘› ğœ‘ âˆ—(ğ‘–,ğ‘’,ğ‘—)=ğ‘–
ğœ +:(Rğ‘›)â˜…Ã—Rğ‘› â†’Rğ‘› ğœ +(ğ‘€,ğ‘¥)=(cid:205) ğ‘–ğ‘€
ğ‘–
Table8. FunctionsusedinthedefinitionofGINconvolutions.
ğœ“
ğ‘€ğ¿ğ‘ƒ2
:(Rğ‘›+ğ‘˜)â˜…Ã—Rğ‘›+ğ‘˜ â†’Rğ‘š ğœ“ ğ‘€ğ¿ğ‘ƒ2(ğ‘‹,ğ‘¥)=ğ‘“4(ğ‘“3(ğ‘¥Î˜3)Î˜4)
ğœ“0:(Rğ‘›)â˜…Ã—Rğ‘› â†’Rğ‘˜ ğœ“0(ğ‘‹,ğ‘¥)=0ğ‘˜
ğœ‘
ğ‘€ğ¿ğ‘ƒ1
:Rğ‘›+ğ‘˜ Ã—RÃ—Rğ‘›+ğ‘˜ â†’Rğ‘˜ ğœ‘ ğ‘€ğ¿ğ‘ƒ1(ğ‘–,ğ‘’,ğ‘—)=ğ‘“2(ğ‘“1((ğœ‹ ğ¿(ğ‘—),ğ‘’,ğ‘–)Î˜1)Î˜2)
ğœ +:(Rğ‘˜)â˜…Ã—Rğ‘˜ â†’Rğ‘˜ ğœ +(ğ‘€,ğ‘¥)=(cid:205) ğ‘–ğ‘€
ğ‘–
Table9. FunctionsusedinthedefinitionoftheoriginalGNNconvolution.
GINis
(âˆ—ğœ–||â—ğ‘– +ğ‘‘ );+;ğ‘ğ‘1;ğ‘ğ‘2
TheOriginalGraphNeuralNetworkModel. ThegraphneuralnetworkmodelwasfirstintroducedbyScarsellietal.
[30].Thenewnodelabelsğ‘¥â€² âˆˆRğ‘š oftheoriginalGNNmodelarecomputedasatwo-stepprocess.Inthefirststepthe
ğ‘–
newâ€œstateâ€ğ‘  ğ‘–â€² âˆˆRğ‘˜ ofeachnodeiscomputedfromthecurrentnodelabelsğ‘¥
ğ‘–
âˆˆRğ‘›,theedgelabelsğ‘’
ğ‘—ğ‘–
âˆˆRğ‘™,andthe
neighbornodesâ€™labelsğ‘¥
ğ‘—
âˆˆRğ‘›andstatesğ‘ 
ğ‘—
âˆˆRğ‘˜ asthefixedpointoftheequation
âˆ‘ï¸
ğ‘  ğ‘– = â„(ğ‘¥ ğ‘–,ğ‘’ ğ‘—ğ‘–,ğ‘¥ ğ‘—,ğ‘  ğ‘—)
â†âˆ’
ğ‘—âˆˆğ‘ G(ğ‘–)
whereâ„ istypicallyatwo-layerperceptronwithweightmatricesÎ˜1 âˆˆ R(2ğ‘›+ğ‘™+ğ‘˜)Ã—ğ‘›â€²,Î˜2 âˆˆ Rğ‘›â€²Ã—ğ‘˜ andactivation
functionsğ‘“1,ğ‘“2.Then,thesecondstepconsistsintheapplicationofafunctionğ‘”tothestateandlabelsofeachnodeto
obtainthefinalnodelabels
ğ‘¥ ğ‘–â€² =ğ‘”(ğ‘¥ ğ‘–,ğ‘  ğ‘–)
whereinthiscasetooğ‘”isatwo-layerperceptron,withweightmatricesÎ˜3 âˆˆR(ğ‘›+ğ‘˜)Ã—ğ‘šâ€²,Î˜4 âˆˆRğ‘šâ€²Ã—ğ‘š andactivation
functions ğ‘“3,ğ‘“4.InTable9weshowthefunctionsrequiredtoimplementthisGNNlayer.Thefirststepconsistsin
initializingthenodestates.Forthisexample,weinitializethenodestatestoanarrayofğ‘˜zerosandweappendittothe
currentnodelabelsusingtheidentitytermandparallelcomposition
(ğœ„||0)
After this step, we compute the fixed point of the states using a two-layer perceptron to generate messages and
aggregatingthemthroughsummation
(ğœ„||0);(ğœ„||â—ğ‘€ğ¿ğ‘ƒ1)â˜…
+
Onceafixedpointforthestatesisreached,weuseanothermulti-layerperceptrontoobtainthefinalnodelabelsand
obtain
(ğœ„||0);(ğœ„||â—ğ‘€ +ğ¿ğ‘ƒ1)â˜…;ğ‘€ğ¿ğ‘ƒ2
ManuscriptsubmittedtoACM30 Belenchiaetal.
11 CONCLUSION
Wepresentedthesyntaxandsemanticsof ğœ‡G,anovelprogramminglanguageforthedefinitionofgraphneural
networks,whereagraphneuralnetworkischaracterizedasahigher-orderfunctionğœ™ :ğ»[ğ‘‡1] â†’ğ»[ğ‘‡2].Weproved
thetypesoundnessofğœ‡Gandshownagraphicalformalismfortherepresentationofğœ‡Gprograms.Thelanguagewas
implementedasaPythonlibrarybuiltontheTensorFlowframework.Weevaluatedğœ‡GontheapplicationofCTLmodel
checkingandasameanstospecifysomeofthemostpopulargraphneuralnetworkmodels.
Infutureimplementations,weplantoexpandthelanguageinanumberofways.Atypicalgraphneuralnetwork
operationthatisstilloverlookedinğœ‡Gispooling.Apoolingoperatorchangestheunderlyinggraphbyreducingthe
numberofnodes,producingacoarsenedversionofagraph.Graphpoolingoperatorscanbegenerallydescribedin
termsoftheoperationsofselection,reduction,andconnection[16].Theselectionoperatorgroupsnodestogether,the
reductionoperatoraggregatesthenodesineachgroupintoasinglenode,andfinallytheconnectionoperatorgenerates
theedgestolinkthenewlygeneratednodes.Theintroductionofpoolingoperatorsrequirescarefulconsiderationof
howtheycanbecomposedwiththeexistingterms,butitwouldallowğœ‡Gtotacklegraph-leveltasksexplicitly.
Themainpurposefordevelopingğœ‡Gistobeeventuallyabletoperformformalverificationofgraphneuralnetworks
andgenerateexplanationsfortheiroutputs.Forthetimebeing,therearestillnoproceduresthatallowtheverification
ofoutputreachabilitypropertiesofGNNs[33].Inthefuture,weaimtodefineanabstractinterpretationapproachfor
thesetasks.
REFERENCES
[1] MartÃ­nAbadi,AshishAgarwal,PaulBarham,EugeneBrevdo,ZhifengChen,CraigCitro,GregS.Corrado,AndyDavis,JeffreyDean,MatthieuDevin,
SanjayGhemawat,IanGoodfellow,AndrewHarp,GeoffreyIrving,MichaelIsard,YangqingJia,RafalJozefowicz,LukaszKaiser,ManjunathKudlur,
JoshLevenberg,DandelionManÃ©,RajatMonga,SherryMoore,DerekMurray,ChrisOlah,MikeSchuster,JonathonShlens,BenoitSteiner,Ilya
Sutskever,KunalTalwar,PaulTucker,VincentVanhoucke,VijayVasudevan,FernandaViÃ©gas,OriolVinyals,PeteWarden,MartinWattenberg,Martin
Wicke,YuanYu,andXiaoqiangZheng.2015.TensorFlow:Large-ScaleMachineLearningonHeterogeneousSystems. https://www.tensorflow.org/
Softwareavailablefromtensorflow.org.
[2] ElvioAmparore,BernardBerthomieu,GianfrancoCiardo,SilvanoDalZilio,FrancescoGallÃ ,LomMessanHillah,FrancisHulin-Hubard,PeterGjÃ¸l
Jensen,LoÃ¯gJezequel,FabriceKordon,DidierLeBotlan,TorstenLiebke,JeroenMeijer,AndrewMiner,EmmanuelPaviot-Adet,JiÅ™Ã­Srba,Yann
Thierry-Mieg,TomvanDijk,andKarstenWolf.2019.Presentationofthe9thEditionoftheModelCheckingContest.InToolsandAlgorithmsforthe
ConstructionandAnalysisofSystems,DirkBeyer,MariekeHuisman,FabriceKordon,andBernhardSteffen(Eds.).SpringerInternationalPublishing,
Cham,50â€“68. https://doi.org/10.1007/978-3-030-17502-3_4
[3] AnishAthalye,LoganEngstrom,AndrewIlyas,andKevinKwok.2018.SynthesizingRobustAdversarialExamples. arXiv:1707.07397[cs.CV]
[4] EnricoBarbieratoandA.Gatti.2024.TheChallengesofMachineLearning:ACriticalReview.ELECTRONICS13(2024),1â€“30. https://doi.org/10.
3390/electronics13020416
[5] PeterW.Battaglia,JessicaB.Hamrick,VictorBapst,AlvaroSanchez-Gonzalez,ViniciusZambaldi,MateuszMalinowski,AndreaTacchetti,David
Raposo,AdamSantoro,RyanFaulkner,CaglarGulcehre,FrancisSong,AndrewBallard,JustinGilmer,GeorgeDahl,AshishVaswani,KelseyAllen,
CharlesNash,VictoriaLangston,ChrisDyer,NicolasHeess,DaanWierstra,PushmeetKohli,MattBotvinick,OriolVinyals,YujiaLi,andRazvan
Pascanu.2018.Relationalinductivebiases,deeplearning,andgraphnetworks. https://doi.org/10.48550/arXiv.1806.01261arXiv:1806.01261[cs,stat].
[6] MatteoBelenchia,FlavioCorradini,MichelaQuadrini,andMicheleLoreti.2023. ImplementingaCTLModelCheckerwithğœ‡G,aLanguage
forProgrammingGraphNeuralNetworks.InFormalTechniquesforDistributedObjects,Components,andSystems(LectureNotesinComputerScience),
MariekeHuismanandAntÃ³nioRavara(Eds.).SpringerNatureSwitzerland,Cham,37â€“54. https://doi.org/10.1007/978-3-031-35355-0_4
[7] MatteoBelenchia,FlavioCorradini,MichelaQuadrini,andMicheleLoreti.2024.libmg:aPythonLibraryforProgrammingGraphNeuralNetworks
inÂµG.(2024). AcceptedwithminorrevisionsforpublicationtoScienceofComputerProgramming.
[8] MichaelM.Bronstein,JoanBruna,TacoCohen,andPetarVeliÄkoviÄ‡.2021. GeometricDeepLearning:Grids,Groups,Graphs,Geodesics,and
Gauges. arXiv:2104.13478[cs.LG]
[9] OlavBunte,JanFrisoGroote,JeroenJ.A.Keiren,MauriceLaveaux,ThomasNeele,ErikP.deVink,WiegerWesselink,AntonWijs,andTimA.C.
Willemse.2019.ThemCRL2ToolsetforAnalysingConcurrentSystems.InToolsandAlgorithmsfortheConstructionandAnalysisofSystems,TomÃ¡Å¡
VojnarandLijunZhang(Eds.).SpringerInternationalPublishing,Cham,21â€“39. https://doi.org/10.1007/978-3-030-17465-1_2
ManuscriptsubmittedtoACMTheğœ‡GLanguageforProgrammingGraphNeuralNetworks 31
[10] GabrieleCorso,HannesStark,StefanieJegelka,TommiJaakkola,andReginaBarzilay.2024.Graphneuralnetworks.NatureReviewsMethodsPrimers
4,1(07Mar2024),17. https://doi.org/10.1038/s43586-024-00294-7
[11] VenmugilElango,NormRubin,MaheshRavishankar,HariharanSandanagobalane,andVinodGrover.2018.Diesel:DSLforlinearalgebraand
neuralnetcomputationsonGPUs.InProceedingsofthe2ndACMSIGPLANInternationalWorkshoponMachineLearningandProgrammingLanguages
(MAPL2018).AssociationforComputingMachinery,NewYork,NY,USA,42â€“51. https://doi.org/10.1145/3211346.3211354
[12] KevinEykholt,IvanEvtimov,EarlenceFernandes,BoLi,AmirRahmati,ChaoweiXiao,AtulPrakash,TadayoshiKohno,andDawnSong.2018.
RobustPhysical-WorldAttacksonDeepLearningModels. arXiv:1707.08945[cs.CR]
[13] VicenteGarcÃ­aDÃ­az,JordÃ¡nEspada,B.PelayoGarcÃ­a-Bustelo,andJuanCuevaLovelle.2015.TowardsaStandard-basedDomain-specificPlatform
toSolveMachineLearning-basedProblems.InternationalJournalofArtificialIntelligenceandInteractiveMultimedia3(Jan.2015),6â€“12. https:
//doi.org/10.9781/ijimai.2015.351
[14] JustinGilmer,SamuelS.Schoenholz,PatrickF.Riley,OriolVinyals,andGeorgeE.Dahl.2017.NeuralmessagepassingforQuantumchemistry.In
Proceedingsofthe34thInternationalConferenceonMachineLearning-Volume70(ICMLâ€™17).JMLR.org,Sydney,NSW,Australia,1263â€“1272.
[15] DanieleGrattarolaandCesareAlippi.2020.GraphNeuralNetworksinTensorFlowandKeraswithSpektral.IEEEComput.Intell.Mag.16(2020),
99â€“106. https://doi.org/10.1109/mci.2020.3039072
[16] DanieleGrattarola,DanieleZambon,FilippoMariaBianchi,andCesareAlippi.2022.UnderstandingPoolinginGraphNeuralNetworks.IEEE
TransactionsonNeuralNetworksandLearningSystems35,2(July2022),2708â€“2718. https://doi.org/10.1109/TNNLS.2022.3190922ConferenceName:
IEEETransactionsonNeuralNetworksandLearningSystems.
[17] SicongHan,ChenhaoLin,ChaoShen,QianWang,andXiaohongGuan.2023.InterpretingAdversarialExamplesinDeepLearning:AReview.ACM
Comput.Surv.55,14s,Article328(jul2023),38pages. https://doi.org/10.1145/3594869
[18] XiaoweiHuang,DanielKroening,WenjieRuan,JamesSharp,YouchengSun,EmeseThamo,MinWu,andXinpingYi.2020.ASurveyofSafetyand
TrustworthinessofDeepNeuralNetworks:Verification,Testing,AdversarialAttackandDefence,andInterpretability. http://arxiv.org/abs/1812.08342
arXiv:1812.08342[cs].
[19] BenjaminJahiÄ‡,NicolasGuelfi,andBenoÃ®tRies.2023.SEMKIS-DSL:ADomain-SpecificLanguagetoSupportRequirementsEngineeringofDatasets
andNeuralNetworkRecognition.Information14,4(April2023),213. https://doi.org/10.3390/info14040213Number:4Publisher:Multidisciplinary
DigitalPublishingInstitute.
[20] MichaelKearns.1998.Efficientnoise-tolerantlearningfromstatisticalqueries.J.ACM45,6(Nov.1998),983â€“1006. https://doi.org/10.1145/293347.
293351
[21] ThomasN.KipfandMaxWelling.2017.Semi-SupervisedClassificationwithGraphConvolutionalNetworks. arXiv:1609.02907[cs.LG]
[22] LuÃ­sC.Lamb,Arturdâ€™AvilaGarcez,MarcoGori,MarceloO.R.Prates,PedroH.C.Avelar,andMosheY.Vardi.2020.GraphNeuralNetworksMeet
Neural-SymbolicComputing:ASurveyandPerspective.InProceedingsoftheTwenty-NinthInternationalJointConferenceonArtificialIntelligence,
IJCAI-20,ChristianBessiere(Ed.).InternationalJointConferencesonArtificialIntelligenceOrganization,Yokohama,Yokohama,Japan,4877â€“4884.
https://doi.org/10.24963/ijcai.2020/679Surveytrack.
[23] GaryMarcus.2018.DeepLearning:ACriticalAppraisal. arXiv:1801.00631 http://arxiv.org/abs/1801.00631
[24] GaryMarcus.2018.Innateness,AlphaZero,andArtificialIntelligence. arXiv:1801.05667[cs.AI]
[25] GaryMarcus.2020.TheNextDecadeinAI:FourStepsTowardsRobustArtificialIntelligence. https://arxiv.org/abs/2002.06177v3arXiv:2002.06177.
[26] ChristopherMorris,MatthiasFey,andNilsKriege.2021.ThePoweroftheWeisfeiler-LemanAlgorithmforMachineLearningwithGraphs.In
ProceedingsoftheThirtiethInternationalJointConferenceonArtificialIntelligence.ijcai.org,Montreal,Canada,4543â€“4550. https://doi.org/10.24963/
ijcai.2021/618ConferenceName:ThirtiethInternationalJointConferenceonArtificialIntelligence{IJCAI-21}ISBN:9780999241196Place:Montreal,
CanadaPublisher:InternationalJointConferencesonArtificialIntelligenceOrganization.
[27] ArturPodobas,MartinSvedin,StevenW.D.Chien,IvyB.Peng,NareshBalajiRavichandran,PawelHerman,AndersLansner,andStefanoMarkidis.
2021. StreamBrain:AnHPCFrameworkforBrain-likeNeuralNetworksonCPUs,GPUsandFPGAs.InProceedingsofthe11thInternational
SymposiumonHighlyEfficientAcceleratorsandReconfigurableTechnologies(Online,Germany)(HEARTâ€™21).AssociationforComputingMachinery,
NewYork,NY,USA,Article8,6pages. https://doi.org/10.1145/3468044.3468052
[28] IvensPortugal,PauloAlencar,andDonaldCowan.2016. APreliminarySurveyonDomain-SpecificLanguagesforMachineLearninginBig
Data.In2016IEEEInternationalConferenceonSoftwareScience,TechnologyandEngineering(SWSTE).IEEE,BeerSheva,Israel,108â€“110. https:
//doi.org/10.1109/SWSTE.2016.23
[29] IqbalH.Sarker.2021.DeepLearning:AComprehensiveOverviewonTechniques,Taxonomy,ApplicationsandResearchDirections.SNComputer
Science2,6(18Aug2021),420. https://doi.org/10.1007/s42979-021-00815-1
[30] FrancoScarselli,MarcoGori,AhChungTsoi,MarkusHagenbuchner,andGabrieleMonfardini.2009.TheGraphNeuralNetworkModel.IEEE
TransactionsonNeuralNetworks20,1(Jan.2009),61â€“80. https://doi.org/10.1109/TNN.2008.2005605ConferenceName:IEEETransactionsonNeural
Networks.
[31] ArvindK.Sujeeth,HyoukJoongLee,KevinJ.Brown,HassanChafi,MichaelWu,AnandR.Atreya,KunleOlukotun,TiarkRompf,andMartin
Odersky.2011.OptiML:animplicitlyparalleldomain-specificlanguageformachinelearning.InProceedingsofthe28thInternationalConferenceon
InternationalConferenceonMachineLearning(ICMLâ€™11).Omnipress,Madison,WI,USA,609â€“616.
[32] ZacharySusskind,BryceArden,LizyK.John,PatrickStockton,andEugeneB.John.2021.Neuro-SymbolicAI:AnEmergingClassofAIWorkloads
andtheirCharacterization. arXiv:2109.06133 https://arxiv.org/abs/2109.06133
ManuscriptsubmittedtoACM32 Belenchiaetal.
[33] MarcoSÃ¤lzerandMartinLange.2022.FundamentalLimitsinFormalVerificationofMessage-PassingNeuralNetworks. https://openreview.net/
forum?id=WlbG820mRH-
[34] PetarVeliÄkoviÄ‡,GuillemCucurull,ArantxaCasanova,AdrianaRomero,PietroLiÃ²,andYoshuaBengio.2018. GraphAttentionNetworks.
arXiv:1710.10903[stat.ML]
[35] BorisYulievichWeisfeilerandAndreyAleksandrovichLeman.1968.TheReductionofaGraphtoCanonicalFormandtheAlgebrawhichAppears
therein.Nauchno-TechnicheskayaInformatsia9(1968),12â€“16.
[36] KeyuluXu,WeihuaHu,JureLeskovec,andStefanieJegelka.2019.HowPowerfulareGraphNeuralNetworks? arXiv:1810.00826[cs.LG]
[37] Tian Zhao, Xiaobing Huang, and Yu Cao. 2017. DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning.
arXiv:1701.02284[cs.PL]
[38] JieZhou,GanquCui,ShengdingHu,ZhengyanZhang,ChengYang,ZhiyuanLiu,LifengWang,ChangchengLi,andMaosongSun.2020.Graph
neuralnetworks:Areviewofmethodsandapplications.AIOpen1(Jan.2020),57â€“81. https://doi.org/10.1016/j.aiopen.2021.01.001
Received20February2007;revised12March2009;accepted5June2009
ManuscriptsubmittedtoACM