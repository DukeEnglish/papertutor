Simulating the economic impact of rationality through
reinforcement learning and agent-based modelling
SimoneBrusatina,TommasoPadoana,AndreaColettab,DomenicoDelliGattic andAldoGlielmob,*
aUniversit√†diTrieste,Italy
bBancad‚ÄôItalia,Italy‚Ä†
cUniversit√†CattolicadelSacroCuore,Italy
Abstract. GE ABM ABM+RL
Agent-basedmodels(ABMs)aresimulationmodelsusedineco- Heterogeneity ‚àº ‚úì ‚úì
nomics to overcome some of the limitations of traditional frame- RationalExpectations ‚úì √ó ‚úì
BoundedRationality √ó ‚úì ‚úì
works based on general equilibrium assumptions. However, agents
ImperfectInformation √ó ‚úì ‚úì
withinanABMfollowpredetermined,notfullyrational,behavioural ImperfectMarkets √ó ‚úì ‚úì
rules which can be cumbersome to design and difficult to justify. Learning √ó √ó ‚úì
Here we leverage multi-agent reinforcement learning (RL) to ex- Easeofdesign ‚úì √ó ‚àº
pandthecapabilitiesofABMswiththeintroductionoffullyrational Table 1: A simplified overview of the advantages of extending
agentsthatlearntheirpolicybyinteractingwiththeenvironmentand ABMs with RL. While ABMs are more flexible than traditional
maximisingarewardfunction.Specifically,weproposea‚ÄòRational models based on general equilibrium assumptions, they typically
macro ABM‚Äô (R-MABM) framework by extending a paradigmatic lack a framework for learning rational behaviour, which also in-
macroABMfromtheeconomicliterature.Weshowthatgradually creases modelling complexity. Multi-agent RL has the potential of
substitutingABMfirmsinthemodelwithRLagents,trainedtomax- providingthisfeatureinexistingABMs,thusimprovingtheircapa-
imiseprofits,allowsforathoroughstudyoftheimpactofrationality bilitiesandfacilitatingmodeldesignatthesametime.
onthe economy.Wefind thatRLagents spontaneouslylearnthree
havegraduallybeenadoptedandgainedpopularityineconomicsun-
distinct strategies for maximising profits, with the optimal strategy
der the name of ‚Äúagent-based models‚Äù (ABMs) [14, 12, 6]. ABMs
depending on the level of market competition and rationality. We
can model economic systems by simulating a number of individ-
alsofindthatRLagentswithindependentpolicies,andwithoutthe
ualeconomicagents,representingdecisionmakersandinstitutions,
abilitytocommunicatewitheachother,spontaneouslylearntoseg-
whichcanbedefinedtobeheterogeneousandactwithboundedra-
regateintodifferentstrategicgroups,thusincreasingmarketpower
tionalityinnon-equilibriumenvironments[29,28].Agentstypically
andoverallprofits.Finally,wefindthatahigherdegreeofrationality
relyonapoolofhand-craftedbehaviouralrules,thataredefinedin
intheeconomyalwaysimprovesthemacroeconomicenvironmentas
advancebyeconomiststomimicthecomplexityofrealeconomies.
measuredbytotaloutput,dependingonthespecificrationalpolicy,
While this approach allows addressing some of the limitations de-
thiscancomeatthecostofhigherinstability.OurR-MABMframe-
scribed,italsoposesnewchallenges,suchasthedifficultyinmod-
workisgeneral,itallowsforstablemulti-agentlearning,andrepre-
ellingrealisticagents,andthecompleteabsenceoflearningandof
sentsaprincipledandrobustdirectiontoextendexistingeconomic
‚Äòrationalexpectations‚Äôintheagents‚Äôbehaviour.
simulators.
ReinforcementLearning(RL)[27]canofferasolutiontoclassi-
cal ABMs limitations as, in principle, it can allow for a grounded
1 Introduction introductionofrationalexpectationandlearninginsimulationmod-
els,withouttheneedoftheover-simplisticassumptionsoftraditional
Traditionaleconomicsimulationmodelsaremostlybasedonthedy-
economicmodellingframeworks.Moreover,RLcanfacilitatethede-
namicstochasticgeneralequilibrium(DSGE)framework,andarefar
signofaccurateeconomicABMs,asitcaneliminatetheburdenof
from being a perfect representation of real economies, due to their
programmingdetailedbehaviouralrulesinfavouroftheeasiertask
strong assumptions, which typically do not allow for agent hetero-
ofchoosingameaningfulrewardfunction,typicallyknownas‚Äòutil-
geneity,boundedrationalityornonequilibriumdynamics[14,12].
ityfunction‚Äôintheeconomicliterature.Therecentbreakthroughsin
In today‚Äôs tech age, advanced computational methods hold the
deep-RLandmulti-agentRL(MARL)[17,1]alongwiththeever-
promiseofsolvingsomeofthesekeylimitations[25,8].Specifically,
growingavailabilityofcomputationalpower,arenowallowingRL
multi-agent systems (MAS) [33] may offer a bottom-up approach
andMARLtoemergeasattractivepowerfultoolstoexpandtheca-
tomodeleconomicsystemsthatnaturallyaddressestheabovemen-
pabilitiesoftraditionalABMsandpavethewaytoanewgeneration
tionedlimitationsoftraditionalmodels.Sincetheearly2010s,MAS
ofeconomicsimulators[10,2].Table1summarisesthemainadvan-
‚àóaldo.glielmo@bancaditalia.it. tagesofcombiningRLwiththeABMmethodology.
‚Ä†Theviewsandopinionsexpressedinthispaperarethoseoftheauthorsand Existing work has investigated the use of RL towards economic
donotnecessarilyreflecttheofficialpolicyorpositionofBancad‚ÄôItalia. modellingofdifferentenvironments[9,19,21,4,16,18].Arecent
4202
yaM
3
]GL.sc[
1v16120.5042:viXraexampleistheworkofJohansonetal.[20],showingthatbasicphe- al[3].Foramoredetaileddescriptionofthemodelanditsparameters
nomenaofmicroeconomicscanemergespontaneouslyfromlearning werefertotheoriginalmanuscript.
agentsinarealisticenvironment:heterogeneousagentslearntopro-
Agenttypes. Thebasismodelcomprisesthreetypesofagentsde-
duce,tradeandconsumeresourcesbasedontheirpreferences.Other
finedasfollows.
worksfocusonrealbusinesscycle(RBC)modelsshowingthatopti-
malpoliciescanbelearntthroughMARL[34,11,24,22,5,13],us- ‚Ä¢ Households consist of workers and capitalists. Workers supply
ingdifferentlearningtechniquesincludingcurriculumlearning[7]. labour,receivingawage.Capitalistsreceivedividendsanddonot
In[24],Mietal.showcasetheadvantagesofMARLsystemsover work.Bothworkersandcapitalistsdepositaccumulatedwealthin
traditional models in the study the impact of taxation policies. Fi- banks.
nally,otherlinesofworkfocusmoreonalgorithmicpricing [9]and ‚Ä¢ Banks receive deposits from households and extend loans to
financialmarketssimulations[30]. firms.
Inthispaper,wecontributetowardsthisemerginginterdisciplinary ‚Ä¢ FirmsconsistofK-firmsandC-firms.K-firmsrequirelabourfor
challenge by proposing the ‚ÄòRational macro ABM‚Äô (R-MABM) their production, and produce capital goods i.e., machinery and
framework.TheR-MABMframeworkleveragesmulti-agentRLto equipment,thattheyselltoC-firms.C-firmsrequirebothlabour
expandthecapabilitiesofclassicalmacroABMswiththeintroduc- andcapitalgoodsfortheirproductionandproduceconsumption
tionoffullyrationalagents,whichlearntheirpolicybyinteracting goodsthattheyselltohouseholds.
withtheenvironmentandmaximisingtheirrewards.Specifically,we
Figure1summarisestheagentsandtheirinteractions.Forthesakeof
extendandstudythewell-knownmacroeconomicABMwithcapi-
brevity,inthefollowingweprovideextradetailsonlyonthespecific
talandcredit(‚ÄòCC-MABM‚Äô)fromAssenzaetal.[3].Suchamodel
behaviouralrulesofhouseholdsandC-firmsthataremostrelevantto
considersaneconomypopulatedbyhouseholds,C-firms,K-firm,and
ourlaterexperiments.
banks.Householdssupplylabourtofirms,consumegoods,andsave
moneythroughdepositsatthebanks.Banksprovideloanstofirms Markets. All of the households aim to buy consumption goods,
for investment purposes. K-firms produce capital goods which are andthereforeparticipateinasearchandmatchingconsumptionmar-
thenusedbyC-firmstoproduceconsumptiongoods.Ourextended ket,definedasfollows.
model substitutes a variable number of C-firms with RL agents,
‚Ä¢ Atthestartofperiodt,eachconsumerdeterminestheirconsump-
whichrationallymaximiseprofits,allowingthestudyoftheimpact
tionbudget,basedontheirincomeandbankdeposits.
ofrationalityintheeconomy.Figure1showsavisualisationofthe
‚Ä¢ Todecidewheretobuythegoods,eachconsumervisitsz ran-
c
R-MAMB model, and a sample of the learning process of rational
domlyselectedC-firmsandsortsthembytheirretailpricefrom
firms.
lowesttohighest.
Weexperimentallyevaluateourframeworkwithextensivesimula-
‚Ä¢ Theconsumerstartsbuyinggoodsfromthefirstfirmand,ifthe
tions,consideringdifferentlevelsofmarketcompetition(controlled
consumptionbudgetisnotexhausted,theconsumermovesonto
bythenumberoffirmsvisibletoeachconsumer),rationality(con-
thesecondfirmintheorder,andsoon.
trolledbythenumberofRLagents),andagentlearningwithshared
orindependentpolicies.WeshowthatRLagentsspontaneouslylearn Asimilarmechanismisinplacewithinthecapitalgoodsmarket:
threedistinctstrategiesformaximisingprofits,withtheoptimalstrat- eachC-firmrandomlyvisitsz kK-firmsandbuystherequiredamount
egy depending on the level of market competition and rationality. ofcapitalgoodsstartingfromthefirmofferingthelowestprice.Note
We also find that agents with independent policies spontaneously that,ahigher(lower)valueofthez kandz cparameters,impliesthat
learntosegregateintodifferentstrategicgroups,thusincreasingmar- eachcustomerwillbeabletocomparethepricesofahigher(lower)
ketpowerandoverallprofits,withoutexplicitlycommunicationwith numberoffirms.Hence,importantly,wecanconsiderthosetwopa-
eachother.Finally,weassessthemacroeconomicimpactofhigher rameters as effective controllers of the level of competition of the
degrees of rationality by measuring the direct effects on GDP and searchandmatchingmarketsdescribed.
onitsvolatility.Tothebestofourknowledge,ourR-MABMframe- Priceandquantitydecisionsoffirms. Themodelassumesthatat
workisthefirstattemptatextendingmacroeconomicABMswiththe eachtimestept,afirmidecidesapriceP atwhichtosellitsgoods,
i,t
introductionofrationalagentsusingRL.Ourresultsamplydemon- andatargetquantityY‚àó ofgoodsitaimstoproduce.Basedonthe
i,t
stratetherobustnessoftheR-MABMframeworkanditsrelevance desiredproductionY‚àó ,thefirmdecideshowmanyworkerstoem-
i,t
toavastcommunityattheintersectionbetweeneconomicsandcom- ployN andhowmuchcapitalK isrequired.Thefirmwillthen
i,t i,t
puterscience,thuspavingthewaytonumerousapplicationsandfu- attempttoacquirelabourandcapitalinthecorrespondingmarkets.
tureinvestigations. Finally,theproductionfunctionfollowsaLeontieftechnology,i.e.,
Y = min (Œ± N ,Œ± K ) where Œ± and Œ± are labour and
i,t N i,t K i,t N K
2 TheRational-MABMmodel capitalproductivity,respectivelyandY i,tisthequantityofgoodsac-
tuallyproduced.Noticethatconsumptiongoodsarenon-storable,so
Our work expands a traditional macroeconomic ABM introducing unsoldgoodsdonotcarryovertothenextperiod.
fully rational agents through reinforcement learning. This section Fortherestofthiswork,wewilloftencalladefinedpolicytoset
firstprovidesanoverviewoftheMABMthatformsthebasisofour P andY‚àó a‚Äòprice-quantitystrategy‚Äô,orsimplya‚Äòstrategy‚Äô.The
i,t i,t
framework,whichwealsocallthe‚Äòbasismodel‚Äô,andthendetailsthe price-quantitystrategyofC-firmsinthebasismodelisbasedonthe
multi-agentRLschemewedeveloptoextendit. followingmarketsignals.Attheendoftheperiodt,thefirmiob-
servestheamountofgoodssoldYs = min(cid:0) Y ,Yd(cid:1) ,whereYd
i,t i,t i,t i,t
istheactualdemand,andtheaveragepriceP chargedbycompeti-
2.1 Thebasismodel t
tors.
WeherebrieflydescribethebasismacroABM,namelythemacroe- To set future prices and quantities, firms use the past step firm
conomicABMwithcapitalandcredit(CC-MABM)fromAssenzaet stock‚àÜY i,t =Y i,t‚àíY id ,tandpricedelta‚àÜP i,t =P i,t‚àíP t.Thefirm&
Firms ""
‚Ä¶
Bank Credit K-firms # " !! profit maximisers
" !
Capital !
maxùîº
[‚àë t
Œ≥ tœÄ
t ]
goods
fully rational firms
Deposits
C-firms
bounded rational firms
Labor
$%
$%
Households Con gs ou om dp stion ! ! !$% trend followers
( ' ‚Ä¶
)
Figure1: The R-MABM model. The left panel shows a schematic diagram of the basis model, in which 4 types of agents (green ovals)
exchangegoods(yellowrectangles).Thearrowsrepresenttheflowofthespecificgood,fromprovidertoreceiver.Themiddlepanelillustrates
theextensionofthebasismodelweimplementwithourR-MABMframework.Consumption-goodproducingfirms‚ÄòC-firms‚Äôinthestandard
modelareexclusively‚Äòboundedrational‚Äôand‚Äòtrend-follower‚Äôandtakedecisionsusingsimpleheuristicrules.Theseareaugmentedwith‚Äòfully
rational‚ÄôRLagentsthattakedecisionsinordertomaximiseprofits.TherightpanelshowsatypicallearningcurvewherefullyrationalRL
agentslearntoaccumulatehigherprofitsthanboundedrationalagentsasthenumberoflearningepisodesprogresses.
stockisthedifferencebetweentheamountofgoodsproduced,and TheaimofeachRLagentistomaximisethesumofitsdiscounted
theamountofgoodsdemanded.Ifpositive,itamountstotheunsold rewards
‚Äòstock‚Äôofgoodspiledupbythefirminagivenstep.Ifnegative,its
S
=(cid:88)tsim
Œ≥tr . (3)
absolutevalueequalsthequantityofextragoodsthatcouldhavebeen i i,t
producedtomeetthedemand.Thepricedeltaisthedistancebetween t=1
Themaximisationofrewards(tobelaterdefinedasfirmsprofits)is
the firm‚Äôs price and the average market price, positive if above the
thefeatureoftheRLagentsthatallowsthemtobefullyrational,in
average or negative if below it. C-firms in the basis model adjust
contrastwithstandardfirmsofthebasismodelthatsimplyfollowa
pricesandtargetquantitiesfollowingthesimpleheuristicpicturedin
behaviouraltrend-followingheuristic.
Figure1anddescribedbythefollowingequations
WetraineachRLagentusingastandardasynchronousQ-learning
Ô£± P (1+Œ∑ ) if‚àÜY ‚â§0‚àß‚àÜP <0 algorithm,likethatintroducedin[32],withœµgreedypolicy.There-
Ô£¥Ô£¥Ô£≤ i,t i,t+1 i,t i,t
fore, each RL agent considers discrete state and action spaces,
P i,t+1 = P i,t(1‚àíŒ∑ i,t+1) if‚àÜY i,t >0‚àß‚àÜP i,t ‚â•0 (1) namely S and A, and learns a value function Q : S √óA ‚Üí R,
Ô£¥Ô£¥Ô£≥P
otherwise
throughthewell-knownBellmanequation,toestimatetheexpected
i,t
rewardrforeachstate-actionpair(s,a).Notethat,asexplainedbe-
Ô£± Y +œÅ|‚àÜY | if‚àÜY ‚â§0‚àß‚àÜP ‚â•0
Ô£¥Ô£¥Ô£≤ i,t i,t i,t i,t low,theagentsdonothavefullknowledgeofthestateofthemodel,
Y‚àó = Y ‚àíœÅ|‚àÜY | if‚àÜY >0‚àß‚àÜP <0 , (2) norcantheyfullyobservetheactionsperformedbytheotheragents.
i,t+1 i,t i,t i,t i,t
Ô£¥Ô£¥Ô£≥Y
i‚àó
,t
otherwise
B gae rc da su ts he eo of thth ei rs s, aw se pr ae rs to or ft tt ho es eu nc vh ira onte mch en ni tq .ue,whereeachagentre-
State space. Similarly to standard (bounded rational) firms, RL
whereœÅisthequantityadjustmentparameterandŒ∑ ‚àº U(0,Œ∑¬Ø)isa
agentsusethepricedelta‚àÜÀúP andthefirmstock‚àÜÀúY valuesasstate
uniform random variable whose range is fixed by the price adjust- i,t i,t
observationsS.Thismeansthatthecurrentstateoftheunderlying
mentparameterŒ∑¬Ø.
stochasticgamemodelisonlypartiallyobservablebyagents.Inthe
Basically,C-firmsofthebasismodelonlyadjusttheirsellingprice
caseofRLagents,thetwoquantitiesaredefinedbyfirsttakingthe
towardstheaverageofthemarket,andproductiontocompensatefor
logarithmsofthecorrespondingquantities
currentdemand.Inthissense,thesefirmscanbeconsideredbounded
rationalagentsthatactastrendfollowers. ‚àÜÀúP =log(P )‚àílog(P )=log(P /P ) (4)
i,t i,t t i,t t
‚àÜÀúY =log(Y )‚àílog(Yd)=log(Y /Yd). (5)
i,t i,t i,t i,t i,t
2.2 TheRLagents
Since tabular Q-learning requires a finite observation space, these
quantitiesarebinned.Tothisend,aminimumandmaximumvalue
WeexpandthebasismodelbyintroducingfullyrationalC-firmsas
areset,S andS respectively.Then,theintervaldelimitedby
RLagentsthatlearntheirprice-quantitystrategiesbyinteractingwith min max
suchvaluesisuniformlydividedinn poles.Eachreal-valuedob-
theenvironmentandmaximisingarewardfunction.Inparticular,the S
servationisthenassignedtheindexofthenearestpole.Inthisway,
underlyingdecisionmodelforasubsetN offullyrationalC-firms
theobservationspaceisdiscretisedoverafinitenumberofequally
correspondstoastochasticgame(seee.g.[26,31])withN players,
spacedbins.Thelogarithmictransformationinthestatespacedef-
i.e. the RL agents, or, in the special case when N = 1, a Markov
initionaboveallowsustointerpretthetwostatesasthepercentage
decision process. The remaining bounded rational agents, those of
differencesofthepastchoiceswithrespecttopastmarketsignals.
thebasismodel,arenotseenasplayersofthegamesincetheyallact
following the same fixed behavioural strategy, and so their actions Action space. Based on the current state observations, each RL
canjustbeseenaspartoftheprobabilistictransitionofthestochastic agenticanmakeprice-quantitydecisionsforthenextstep,thusset-
game. tingP i,t+1andY i‚àó ,t+1,respectively.Tochangethesevaluestheagentselectsanactionseta i,t = (aY i,t,aP i,t).Giventheselectedactions, Symbol Description Value
pricesandquantitiesareadjustedas
Ttrain Numberoftrainingepisodes 100
P i,t+1
=elog(Pi,t)+aP
i,t (6)
T tst ie mst NN uu mm bb ee rr oo ff ste tes pt sep fois ro ed ae cs
hsimulation
52 00
00
Y i‚àó ,t+1 =elog(Yi‚àó ,t)+aY i,t. (7) t Hburn-in N Nu um mb be er ro of fb wu or rn k- ein rssteps 3 10 00 00
Fc NumberofC-firms 100
Similarlytothestateobservations,theactionsarerequiredtobedis- F k NumberofK-firms 20
crete. We select a number n of discrete actions to be uniformly œÅ Quantityadjustmentparameter 0.9
A
Œ∑¬Ø Priceadjustmentparameter 0.1
spacedbetweenaminimumandmaximumvalue,A andA
min max zc NumberofC-firmsvisitedbyaconsumer {2,...,10}
respectively. z k NumberofK-firmsvisitedbyaC-firm 2
N NumberofRLagents {1,...,20}
Reward. At each step t, each RL agent observes s =
(cid:16) ‚àÜÀúP i,t,‚àÜÀúY i,t,(cid:17) performsactiona i,t = (cid:0) aP i,t,aY i,t(cid:1) ,andobtaini s,t are- Œ≥ Œ± D Leis ac rno iu nn gt rfa ac tetor 0 0. .9 15
wardr givenby Smin Observationlowerbound ‚àí2
i,t Smax Observationupperbound 2
(cid:40)
œÄ ifA >0
nS Numberofobservations 11
r =
i,t i,t
, (8)
Amin Actionslowerbound ‚àí0.3
i,t ‚àí100 otherwise Amax Actionsupperbound 0.3
nA Numberofdiscreteactions 11
whereœÄ i,tarethefirm‚ÄôsprofitsandA i,tarethefirm‚Äôsassets.Profits Table2:Experimentalparametersused.Listofparametersusedin
arecomputedastotalrevenuesi.e.,thequantityofsoldgoodsYs thesimulations.Thevaluesofz andN rangefrom2to10andfrom
i,t c
timestheretailpriceP ,minustotalcosts,consistingoflabour,in- 1to20respectively,thespecificvaluesusedcanbereaddirectlyin
it
vestments,debtinstalmentsanddividends.Allquantitiesareadjusted thefiguresofSection4.
forinflationandhenceexpressedinrealterms.Basically,therewards
oftheRLagentaretheprofitsunlessthefirmgoesbankrupt,where 3 Experimentalsetup
itgetspenalisedmoreheavily.Noticethat,asweconsideramulti-
agent setting, each agent‚Äôs reward actually depends on the current
We simulate the macroeconomic model considering a single bank,
stateandthejointactionsofalltheagents.
1000workers,20K-firms,and100C-firms.Inourexperiments,we
Fromagame-theoreticalstandpoint,onecouldalsothinkofdeter-
varythelevelsofcompetition(i.e.,z )from2to10,andthenum-
miningtheunderlyingstochasticgameandcomputethecorrespond- c
ber N of RL agents, representing fully rational C-firms, from 1 to
ingoptimalpoliciesfortheRLagents,givensomesolutionconcept
20. Since we focus exclusively on C-firms in our experiment, we
suchasMarkovperfectequilibrium[23].Whilethiscouldtheoret-
will refer to them simply as ‚Äòfirms‚Äô for the remainder of the work.
ically be done as the basis model can provide a full definition of
AllsimulationparametersaredescribedinTable2,whileadditional
thestates,actions,transitionandrewards,itwouldnotbefeasiblein
parametersofthebasismodelarethoseofAssenzaetal.[3].
practiceasthesizeofthemodelandofthestrategyspacearehighly
We train the RL agents for each model configuration, consider-
prohibitive.Moreover,thepossibleuseofinformationextractedfrom
ingbothsharedandindependentpolicies.WeconsiderT = 100
thegamemodelwouldnotrepresentarealisticassumptionsince,in train
episodes for the training phase, while the test phase consists of
the real world, agents are not able to fully observe their environ-
T = 20episodes.Unlessotherwisespecified,wereportaverages
ment. On the other hand, the adopted Q-learning procedure allows test
ofthesimulationresultsoverthe20testingepisodes,witherrorbars
each agent to learn a policy which approximates the best response
representingonestandarddeviation.
forthebehaviourofboundedrationalagentsand,possibly,theother
SimilarlytosomeexistingMARLframeworksforeconomicmod-
RLagentsaswell.
elling,weconsiderasortofcurriculumlearning[7]:thesimulation
Training. DuringthetrainingeachagentiupdatestheQ-function startswithouttheRLagents,whichareintroducedaftert steps.
burn-in
accordingtothefollowingBellmanequation Then, the RL agents assume control of N firms for additional t
sim
steps.Atthestartofthetrainingphase,RLagentsareinitialisedwith
Q(si,t,ai,t)‚Üê(1‚àíŒ±)Q(si,t,ai,t)+Œ±(ri,t+Œ≥maxaQ(si,t+1,a)), (9) an empty Q-matrix i.e. all values are zero. During training, and at
eachepisodeœÑ =0,...,T ‚àí1,theagentsupdatetheexploration
where Œ± is the learning rate, Œ≥ is the discount factor and s is train
i,t+1 rate œµ of the œµ-greedy policy in Eq. (10) as œµ‚Üêmax(0.9œÑ,0.01).
thenextstateobservedbytheRLagent.Consideringthemulti-agent
Simulationresultsarepresentedinthenextsection.
setting, we differentiate between training with shared policies and
withindependentpolicies.Intheformercase,theagentsshareand
update the same Q-matrix. In the latter case, each agent i will use
4 Results
andupdateitsownmatrixQ .
i
During the training phase each agent follows an œµ-greedy action
selection Inthissectionwediscussthemacroeconomicsimulationsobtained
withourR-MABMframework,wherewegraduallysubstitutestan-
(cid:40)
randomaction withprobabilityœµ
dardfirmswithRLagentstrainedtomaximiseprofits.Inparticular,
a = , (10)
i,t argmaxQ(s i,t,a) otherwise weshowcasehowourframeworkallowsforadetailedstudyofthe
a
impactofrationalityinacomplexeconomicsystem.Wedivideour
with œµ slowly decaying for progressing training episodes. Once resultsintomicroeconomiceffects(Section4.1)andmacroeconomic
trained,eachagentfollowsthelearnedpolicyandchoosestheaction effects(Section4.2).Tothebestofourknowledge,ourframeworkis
forwhichQ-matrixismaximisedi.e.,argmax a‚ààAQ(s,a). thefirsttoallowforsimilaranalyses.Number of RL agents ( )
Figure2:Differentemergingstrategiesfordifferenteconomicenvironments.Themiddlepanel(B)showsthetimeseriesofobservedsales
(Ys)andprice-deltas(‚àÜÀúP)forasampleboundedrationalagent(dashedline)andRLagent(solidline)under3differentcombinationsof
marketcompetition(z )anddegreeofrationality(N)asindicatedbythearrows.The3combinationsgiveriseto3differentstrategiesforthe
c
RLagents.Fromtoptobottom,wefind‚Äòperfectcompetition‚Äô,‚Äòdumping‚Äôand‚Äòmarketpower‚Äôstrategies(seemaintextformoredetails).The
changeintheemergingstrategyfordifferenteconomicenvironmentsishighlightedintheleft(A)andright(C)panels,whichplottheaverage
valueofprice-delta(leftaxes)andsales(rightaxes)asafunctionofz andN respectively.
c
4.1 Themicroeconomicimpactofrationalagents adaptivelychangesthepricesofgoodsinaverysophisticatedman-
nertokeepthemjustslightlybelowthecompetitors‚Äôprices.Thisis
We first evaluate the microeconomic impact of RL agents, which evident from the fact that the solid blue line, representing the RL
learntoefficientlyproduceandpricegoodsincompetitionwitheach agentprice,isalwaysbelowthedottedlightblueline,representing
otherandwithboundedrationalfirms.Inparticular,foreachfirmwe thepricechargedbyasampleboundedrationalfirm.Thelowprices
measureitssalesYs ofconsumptiongoodsanditspricedelta‚àÜÀúP, allowforhighsales,asdemonstratedbythesolidorangelinebeing
toassessthedistancebetweenthefirm‚Äôspriceandtheaveragemarket alwaysconsiderablyhigherthanthedashedorangeline.
price. Forbothprice-quantitystrategies,theRLagenthasalsosuccess-
Ingeneral,wefindthatRLagentschooseandadapttheirstrategy fullylearnedtoavoidbankruptcy.Thiscanbebestobservedfromthe
according to the level of market competition z and rationality N, timeseriesofsalesforthedumpingstrategy,sincethesalesoftheRL
c
outperforming the profits of bounded rational firms from the basis agentsremainhighfortheentiretimewhilethesalesofthebounded
model. rationalfirmsometimesdroptozeroforacertainnumberofsteps,
indicatingasuddenbankruptcyandasubsequentslowrecover.
Differentemergingstrategiesfordifferentlevelsofcompetition.
Wefirstevaluatetheimpactofdifferentlevelsofcompetitionthrough Emergingperfectcompetitionformultiplefullyrationalagents
themodelparameterz ,consideringasimulationwithasingleRL withasharedpolicy. InFigure2.Cweinvestigatethecaseofmul-
c
agent(N = 1).Figure2.Ashowstheagentpolicyintermsofprice tipleRLagents,withcompetitionlevelfixedatz = 5.Inthisset-
c
deltaandsales.Weobservethatatincreasinglevelsofmarketcom- ting,wetrainNRLagentsusingasharedpolicy,i.e.,allagentsshare
petition,theRLagentexhibitsdifferentstrategies. andupdatethesameQ-matrix.
When the competition is low, for z 2 or 3, the RL agent learns While for N = 1,2 the RL agents learn the dumping strategy
c
tomaximiseprofitsbyproducingverylowquantitiesofgoodsand discussed in the previous paragraph, a new transition appears after
sellingthematskyrocketingprices.Wecallthisstrategya‚Äúmarket N =2andthestrategylearnedbythreeormoreRLagentsisentirely
power‚ÄùstrategysincetheRLagenthaslearnedthathecancharge new.Inthisscenario,theevenhigherlevelofcompetitioncausedby
any desired price on the goods sold and always find buyers since thepresenceofmultiplefullyrationalfirmsintheeconomicsystem
consumersareforcedtovisitonlyasmallnumberoffirmstocover drivestheRLagentstolearntheveryconservativestrategyofpro-
theirconsumptionneeds. ducingmoderateamountsandsellingthematapricewhichisinline
Whenthecompetitioninthemarketgetshigher,forz 3orhigher, with the market. By stretching the technical meaning of the term,
c
the market power strategy becomes less profitable, and the learned wecallthisstrategy‚Äòperfectcompetition‚Äô,sinceunderthisstrategy
price-quantitystrategychangesabruptly.Inthesecircumstances,the RLfirmshavenopowertoaffectprices.Theyare‚Äòpricetakers‚Äôand
RLagentlearnstodroptheretailpricebelowmarketlevelinorder closelyfollowconsumerdemand.Notably,perfectcompetitionisnot
to undercut the competition, and it further learns that by charging optimalforlowlevelsofcompetition(z )andforonlyafewfullyra-
c
lowerpricesitcanproduceandsellhigherquantities.This,inturn, tionalfirms(N),wheninsteaddumpingormarketpowerstrategies
allowstheRLagenttosecurealargeportionofthemarketshareand aremuchmoreprofitable;butitemergesastheonlyviablestrategy
accumulatehighprofits.Wecallthisstrategya‚Äúdumping‚Äùstrategy. whentheeconomicsystemiscomposedofmanyfullyrationalfirms
Figure2.Bshowssomeinsightsfromsimulationswithz =2and competingfiercelyagainsteachotherforsustainedprofitsandavoid-
c
z = 5,comparingtheRLagentstrategiesagainstaboundedratio- anceofbankruptcy.
c
nalfirm.Forthe‚Äúmarketpower‚Äùstrategy(z = 2),weclearlysee InFigure2.BweshowaninsightintothesimulationwithN =5,
c
thattheRLagentchargesmuchhigherretailprices,andthisimplies showing the strategy of a sample RL agent, which typically sells
muchlowersalesthantheboundedrationalfirm.Inspiteofthevery fewergoodsthanaboundedrationalagent,atsimilarorlowerprices.
lowsales,thestrategygivesrisetohightotalrevenues(P ¬∑Ys)and It is again interesting to notice the adaptive and sophisticated be-
t t
concomitantlytolowtotalcosts,yieldinghighprofits. haviouroftheRLagentinpricinggoodsfromthetopinsetofFig-
ThefigureshowsanevenmoreinterestingbehaviourfortheRL ure2.B.
agentwiththe‚Äúdumping‚Äùstrategy(z = 5).Infact,theRLagent Finally, we note that average profits of RL agents diminish sub-
c(A) (B) (C)
Figure3:IndependentRL-agentsspontaneouslysegregateintostrategicgroupsincreasingoverallprofits.Theleftpanel(A)showsthe
total mean cumulative rewards (i.e., profits) for a varying number of agents, with z = 5, and for RL agents with shared or independent
c
policies.Agentswithindependentpoliciesalwaysachievehigheroverallrewardsasaresultofspontaneoussegregationintostrategicgroups.
Thisphenomenoncanbeclearlyobservedinthemiddleandrightpanels(B)and(C),showingthevalueofagent-specificpricedeltaandsales
asafunctionofagrowingnumberofrationalagents,withz =5.Asmallnoisewasaddedtothex-axistobetterresolveverynearbypoints.
c
Thedifferentstrategicgroupsareparticularlyeasytospotwhenplottingpricedeltaagainstsales,asdoneintheinsetof(A)forN =20.
stantiallyinthetransitionfromalowrationalityenvironment(N = substantially over the shared policy scenario since certain strategic
1,2)whichcanbeexploitedwithadumpingstrategy,toahighra- groupscanstillcarryonexploitativebehaviourwhichwouldnotbe
tionalityenvironment(N ‚â•3)whereRLagentsareforcedintoper- sustainableforallRLagentsatthesametime.Thespontaneousseg-
fectcompetitiontostayprofitableandbankruptcy-free.Thisabrupt regationofagentsisparticularlyclearfromtheinsetofFigure3.A,
changeinprofitsisreportedinFigure3.A.Thefigurealsoillustrates wherethe20RLagentsareseentoclusterintoallthreediscovered
thatprofitskeepdecreasingalthoughmoregraduallyalsoforN ‚â•3, strategiesaccordingtotheirmediansalesandprice.
inlinewithclassicalmicroeconomictheory.Althoughwedonotex- Panels B and C of Figure 3 show the gradual emergence of the
plicitlyshowtheprofits/rewardsofboundedrationalfirms,wenote segregationintostrategicgroups,whichbeginsatN =4withasep-
herethattheseareconsistentlylowerthanthoseofRLagentsasil- arationbetweendumpingandperfectcompetition,andcontinuesat
lustratedforasinglerunintherightpanelofFigure1. N = 15withtheadditionofthemarketpowergroup.Interestingly,
In summary, RL agents spontaneously find the following three while RL agents with a shared policy transition to perfect compe-
strategiestomaximiseprofits: titionalreadyforN > 2,RLagentswithindependentpoliciesare
‚Ä¢ Marketpower.Firmsmaximiseprofitsbyexploitingtheimper- abletocontinueleveragingthedumpingpolicyevenwithN =3,as
fectionofthemarket,withhouseholdsbeingabletovisitonlya a result of subtle tacit coordination in the three dumping strategies
smallnumberoffirmsfortheirconsumption.Theyproduceand enacted. This results in drastically higher profits as clearly observ-
sell small amounts of goods but they charge a very high price, ablefromFigure3.A.
thusatthesametimemaximisingrevenuesandminimisingcosts.
‚Ä¢ Dumping.Firmsmaximiseprofitsbydroppingtheirpricesbelow
4.2 Themacroeconomicimpactofrationalagents
market level to eliminate competition and at the same time pro-
ducingasmuchaspossibletoincreasesalesandgainasignificant
Inthissection,weinvestigatethemacroeconomiceffectsofintroduc-
marketshare.
ingRLagentsintoourR-MABM.Ingeneral,wefindthatahigher
‚Ä¢ Perfectcompetition.Firmsachievesustainableprofitsandavoid
degreeofrationalityintheeconomyalwaysimprovesthemacroeco-
bankruptcybyaconservativestrategythatinvolvesproducingand
nomicenvironmentasmeasuredbytotaloutput,butthatthemacroe-
selling moderate amounts of goods, and charging market level
conomicstabilityofthesystemdependsonthespecificstrategyput
prices.
inplacebytherationalfirms.
Independent RL agents increase profits over perfect competi-
Increasedrationalityimplieshigheroveralloutput. Figure4.A
tionbysegregation. WenowinvestigatethecaseofmultipleRL
shows the average Gross domestic product (GDP) of simulations
agentstrainedwithindependentQ-matrices,andhencefreetolearn
with RL agents with shared or independent policies for increasing
differentpolicies.Figure3.Ashowsthecumulativeaveragerewards
N.Thefigureclearlyshowsthat,independentlyofthetypeofpol-
achievedbyanincreasingnumberN ofRLagentsatfixedz = 5,
c icy, any level of increased rationality N gives rise to a significant
fortrainingwithbothindependentandsharedpolicies.Thefirstim-
increase in the overall output of the economic system with respect
mediately visible emergent phenomenon is the fact that RL agents
to the basis model. This is not surprising since rational firms have
with individual policies achieve higher total profits/rewards on av-
agreaterincentivetoincreaseproductionlevelstoincreaseprofits.
erageascomparedwithRLagentsthatareconstrainedtothesame
Thishappensbothin‚Äòexploitable‚Äôeconomicenvironmentsthatallow
commonpolicy.Whilethismayseemcounterintuitive,sinceanagent
fewRLagentstogainahighmarketshareviadumping(N = 1,2)
wouldmaximiseitsindividualrewardsattheexpenseoftheothers,
andin‚Äòunexploitable‚ÄômarketswhereRLagentsareforcedintoper-
werecallthatourmodelisnotazero-sumgame.Independentagents
fectcompetitionandachievehigheroutputscollectively.
can achieve higher profits by adjusting their strategies and holding
differentmarketniches.Specifically,wehereobserveaninteresting Increasedrationalityimplieshighereconomicstabilityonlywhen
phenomenonofspontaneoussegregationoftheRLagentsintodiffer- it also implies perfect competition. Figure 4.B shows the eco-
entstrategicgroups,whereagentswithineachgroupfollowoneof nomicinstabilityoftheR-MABM,measuredbythestandarddevi-
the three strategies described above. This, in turn, increases profits ationoftheGDP,foranincreasingnumberN ofRLagentstrainedFigure4: RLagentsalwaysincreasetotalmacroeconomicoutput,butonlyperfectcompetitionalsoincreaseseconomicstability.Mean
(A),andstandarddeviation(B)oftheGDPinthelast1000stepsofsimulationswithagrowingnumberofrationalagents,trainedeitherwith
sharedorindependentpolicies.Errorbarsrepresentthestandarderroronaveragequantities.Thelastpanel(C)showstheresponsefunctionof
consumption,GDPandGDPdeflatortoapositiveshockinthepropensitytoconsumeofallhouseholds.Theimpulseresponsesareshownfor
thebasismodelandforthemodelswithsharedandindependentpolicieswithN =7.Allresultsofthefigureareobtainedwithz =5.
c
withsharedorindependentpolicies.Whileoveralloutputalwaysin- fullyadoptedonlybysharedpolicyRLagents.
creaseswithincreasingrationality,economicstabilitydoesnotnec-
essarilyincrease.
Specifically,wefindthatrationalityincreaseseconomicstability 5 Conclusions
only when it also implies the adoption of the perfect competition
strategybyrationalfirms.Onthecontrary,rationalfirmsadoptingan We introduced an innovative and promising application for multi-
exploitativebehaviour,likethedumpingstrategy,capitaliseonfirms agentreinforcementlearning:theextensionoftraditionalmacroeco-
withlimitedrationalitymakingthemmoresusceptibletobankruptcy, nomicagent-basedmodels(ABMs)with‚Äòfullyrational‚Äôagentsthat
thusincreasingoveralleconomicinstability. optimisetheirbehaviourtomaximisearewardfunction.Specifically,
The effect can be clearly seen by noticing how the instability inthisworkweapplythisideatoaclassicalmacroeconomicABM,
of share policy RL agents is higher than in the basis model for proposing a ‚ÄòRational macro ABM‚Äô (R-MABM) by substituting
N = 1,2,whendampingistheoptimalstrategy,andhowitdrops boundedrationalfirmsintheclassicalmodelwithprofit-maximising
rapidlytobelowbaselineforN ‚â•3,whenRLagentsareforcedinto reinforcementlearning(RL)agents.OurR-MABMmodelallowsthe
perfectcompetition.Moreover,independentpolicyRLagentskeep studyofavarietyofissuesineconomicsthatcannotbeinvestigated
exploitingthedumpingstrategyasdescribedintheprevioussection, withtraditionalmacroABMs,whereagentssimplyfollowfixedbe-
andhenceconsistentlygiverisetohighereconomicinstabilitythan haviouralrules.WeshowcasehowtheR-MABMallowsathorough
sharedpolicyRLagents.Alsointhecaseofindependentpoliciesthe examinationoftheimpactofincreasedlevelsofrationality(modelled
instabilityisseentodecreaseforlargervaluesofN,butinamore asthenumberofRLagents)intheeconomy.Wefindanumberof
gradualfashion,asaresultofperfectcompetitiongraduallybecom- empiricalresultsthatcanbewellconnectedwithideasandconcepts
ingtheprevalentstrategy. fromeconomictheory.
Onthemicroeconomiclevel,wefindthatRLagents(i.e.,fullyra-
Perfectcompetitiongivesrisetothehighestoveralloutputandto tionalfirms)spontaneouslylearnthreedistinctprice-quantitystrate-
efficientresponsestoaggregateshocks. Thehighestoverallout- gies to maximise profits depending on the economic environment
putsareachievedatN =7,10,15forRLagentswithsharedpolicy they are immersed in. When competition is very low, they learn a
followingaperfectcompetitionstrategy.Thisresultisnottrivial,as ‚Äòmarketpower‚Äôstrategyconsistingofchargingveryhighpricesand
onemightimaginethedumpingstrategytoleadtogreateroutputsas producing very little, relying on the fact that with low competition
itimpliesthehighestproductionlevelsforrationalfirms.Theperfect thereisalwaysdemandfortheirgoods.Whencompetitionishigher
competitionstrategyalsoappearstogiverisetoquickandeffective buttherearefewrationalfirms,theylearna‚Äòdumping‚Äôstrategycon-
responsestoaggregateshocks,asexemplifiedinFigure4.C.Thefig- sistingoffloodingthemarketwithhighquantitiesofgoodssoldat
ureshowstheimpulseresponsefunctionsforthreemacroeconomic verylowprices,thusundercuttingcompetitionfromtheboundedra-
variables:consumption,realGDP,andGDPdeflator(ameasureof tionalagents.Finally,whenbothcompetitionandthenumberofra-
inflation).Theimpulse(or‚Äòshock‚Äô)consistsofaninstantaneous30% tional firms in the economy are high, RL agents are forced into a
increaseinthepropensitytoconsumeofallhouseholds,andiscom- ‚Äòperfectcompetition‚Äôstrategyconsistingofproducingenoughgoods
putedforthebaselinemodelandforR-MABMmodelswithN =7 to satisfy the demand and selling them at market prices. Notably,
andshared/independentpolicies.Thecurvesareobtainedfollowing wealsofindthatwhenRLagentsareallowedtofollowindependent
thetechniquedescribedin[15]. policiestheyspontaneouslylearntosegregateintodifferentstrategic
Notably,therationalmodelsareabletomorequicklysatisfythe groupsthusincreasingprofitsforall.
increaseddemandasindicatedbythemorerapidconvergenceofthe Onthemacroeconomiclevel,wefindthatrationalityalwaysim-
shocked consumption variable to the pre-shock value (zero in the provesoveralleconomicoutput,butthatmacroeconomicstabilityis
graphs).Atthesametime,theyalsogeneratelowerinflationasmea- improvedonlywhenRLagentsareforcedintoastrategyofperfect
suredbytheGDPdeflatorvariables.Theefficiencyintheeconomic competition. We further find that RL agents in perfect competition
responseisparticularlystrongforthesharedpolicymodelandcould strategy typically give rise to the greatest economic output and to
beadirectconsequenceoftheperfectcompetitionstrategywhichis higheconomicstabilityandresiliencetoshocks.Thisworkrepresentsanimportantfirststepintheinvestigationof [20] M.B.Johanson,E.Hughes,F.Timbers,andJ.Z.Leibo.Emergentbar-
the use of RL in traditional economic ABMs and, more generally, teringbehaviourinmulti-agentreinforcementlearning. arXivpreprint
arXiv:2205.06760,2022.
animportantsteptowardstheintegrationofartificialintelligencein
[21] A.Kuriksha.Aneconomyofneuralnetworks:Learningfromheteroge-
economicsimulationmodels.OurR-MABMframeworkallowsfor neousexperiences.arXivpreprintarXiv:2110.11582,2021.
stable multi-agent learning, and thus it provides a solid foundation [22] E.Lindau.Optimaltaxationbytwo-agentreinforcementlearning,2023.
[23] E.MaskinandJ.Tirole. Markovperfectequilibrium:I.observableac-
formanypossibleextensions.Futureworkcouldinvolveincreasing
tions.JournalofEconomicTheory,100(2):191‚Äì219,2001.
the action space of rational firms to include investment choices, or [24] Q.Mi,S.Xia,Y.Song,H.Zhang,S.Zhu,andJ.Wang. Taxai:Ady-
studyingtheeffectoftheintroductionofRLagentsinplaceofother namic economic simulator and benchmark for multi-agent reinforce-
agentclassesinthebasismodel,suchasbanksandhouseholds.Fi- mentlearning.arXivpreprintarXiv:2309.16307,2023.
[25] J.Rotheetal.Economicsandcomputation,volume4.Springer,2015.
nally, it would be interesting to add an RL government or central
[26] L.S.Shapley.Stochasticgames.ProceedingsoftheNationalAcademy
banktostudytheemergenceofoptimalfiscalormonetarypolicies. ofSciences,39(10):1095‚Äì1100,1953.
[27] R.S.SuttonandA.G.Barto.Reinforcementlearning:Anintroduction.
MITpress,2018.
Codeavailability
[28] L.Tesfatsion. Agent-basedcomputationaleconomics:Aconstructive
approachtoeconomictheory. Handbookofcomputationaleconomics,
Intheinterestofreproducibility,thecodetosimulatetheR-MABM 2:831‚Äì880,2006.
framework will be released in open source soon. In the meantime, [29] L.Tesfatsion. Agent-basedcomputationaleconomics:Overviewand
pleasedonothesitatetogetintouchifyouareinterested. briefhistory.ArtificialIntelligence,LearningandComputationinEco-
nomicsandFinance,pages41‚Äì58,2023.
[30] N. Vadori, L. Ardon, S. Ganesh, T. Spooner, S. Amrouni, J. Vann,
References M.Xu,Z.Zheng,T.Balch,andM.Veloso. Towardsmulti-agentrein-
forcementlearning-drivenover-the-countermarketsimulations. Math-
[1] S.V.Albrecht,F.Christianos,andL.Sch√§fer. Multi-agentreinforce- ematicalFinance,34(2):262‚Äì347,2024.
mentlearning:Foundationsandmodernapproaches.MassachusettsIn- [31] J.vanderWal.Stochasticdynamicprogramming.1981.
stituteofTechnology:Cambridge,MA,USA,2023. [32] C.WatkinsandP.Dayan. Q-learning. MachineLearning,8:279‚Äì292,
[2] L.Ardon,J.Vann,D.Garg,T.Spooner,andS.Ganesh. Phantom-a 1992.
rl-drivenmulti-agentframeworktomodelcomplexsystems. InPro- [33] M.Wooldridge. Anintroductiontomultiagentsystems. Johnwiley&
ceedingsofthe2023InternationalConferenceonAutonomousAgents sons,2009.
andMultiagentSystems,pages2742‚Äì2744,2023. [34] S.Zheng,A.Trott,S.Srinivasa,D.C.Parkes,andR.Socher. Theai
[3] T. Assenza, D. D. Gatti, and J. Grazzini. Emergent dynamics of a economist:Taxationpolicydesignviatwo-leveldeepmultiagentrein-
macroeconomicagentbasedmodelwithcapitalandcredit. Journalof forcementlearning.Scienceadvances,8(18):eabk2607,2022.
EconomicDynamicsandControl,50:5‚Äì28,2015.
[4] T. Atashbar and R. A. Shi. Deep reinforcement learning: emerging
trendsinmacroeconomicsandfutureprospects.2022.
[5] T. Atashbar and R. A. Shi. AI and macroeconomic modeling: Deep
reinforcementlearninginanRBCmodel.InternationalMonetaryFund,
2023.
[6] R. L. Axtell and J. D. Farmer. Agent-based modeling in economics
andfinance:Past,present,andfuture. JournalofEconomicLiterature,
2022.
[7] Y.Bengio,J.Louradour,R.Collobert,andJ.Weston.Curriculumlearn-
ing.InProceedingsofthe26thannualinternationalconferenceonma-
chinelearning,pages41‚Äì48,2009.
[8] L.Blume,D.Easley,J.Kleinberg,R.Kleinberg,and√â.Tardos. Intro-
ductiontocomputerscienceandeconomictheory.JournalofEconomic
Theory,156:1‚Äì13,2015.
[9] E.Calvano,G.Calzolari,V.Denicolo,andS.Pastorello. Artificialin-
telligence,algorithmicpricing,andcollusion. AmericanEconomicRe-
view,110(10):3267‚Äì3297,2020.
[10] A.Charpentier,R.Elie,andC.Remlinger. Reinforcementlearningin
economicsandfinance.ComputationalEconomics,pages1‚Äì38,2021.
[11] M. Curry, A. Trott, S. Phade, Y. Bai, and S. Zheng. Learning solu-
tionsinlargeeconomicnetworksusingdeepmulti-agentreinforcement
learning.InAAMAS,pages2760‚Äì2762,2023.
[12] H.DawidandD.D.Gatti. Agent-basedmacroeconomics. Handbook
ofcomputationaleconomics,4:63‚Äì156,2018.
[13] K. Dwarakanath, S. Vyetrenko, P. Tavallali, and T. Balch. Abides-
economist:Agent-basedsimulationofeconomicsystemswithlearning
agents.arXivpreprintarXiv:2402.09563,2024.
[14] J.D.FarmerandD.Foley.Theeconomyneedsagent-basedmodelling.
Nature,460(7256):685‚Äì686,2009.
[15] D.D.GattiandJ.Grazzini. Risingtothechallenge:Bayesianestima-
tionandforecastingtechniquesformacroeconomicagentbasedmodels.
JournalofEconomicBehavior&Organization,178:875‚Äì902,2020.
[16] A.Glielmo,M.Favorito,D.Chanda,andD.DelliGatti.Reinforcement
learningforcombiningsearchmethodsinthecalibrationofeconomic
abms. InProceedingsoftheFourthACMInternationalConferenceon
AIinFinance,pages305‚Äì313,2023.
[17] S.GronauerandK.Diepold. Multi-agentdeepreinforcementlearning:
asurvey.ArtificialIntelligenceReview,55(2):895‚Äì943,2022.
[18] E.Hill,M.Bardoscia,andA.Turrell. Solvingheterogeneousgeneral
equilibriumeconomicmodelswithdeepreinforcementlearning. arXiv
preprintarXiv:2103.16977,2021.
[19] N.HinterlangandA.T√§nzer.Optimalmonetarypolicyusingreinforce-
mentlearning.2021.