Are We in The Zone? Exploring The Features and Method of Detecting
Simultaneous Flow Experiences Based on EEG Signals
BAIQIAOZHANG,
ShandongUniversity,China
XIANGXIANLI,
ShandongUniversity,China
YUNFANZHOU,
ZhejiangUniversity,China
JUANLIU,
ShandongUniversity,China
WEIYINGLIU,
ShandongUniversity,China
CHAOZHOU,
InstituteofSoftwareChineseAcadamyofSciences,China
YULONGBIANâˆ—,
ShandongUniversity,China
Fig.1. Detectingteammembersâ€™simultaneousflowexperienceduringatwo-usercollaborativetasksbasedonmulti-channel
electroencephalogram(EEG)signals.
Whenexecutinginterdependentpersonaltasksfortheteamâ€™spurpose,simultaneousindividualflow(simultaneousflow)is
theantecedentconditionofachievingsharedteamflow.Detectingsimultaneousflowhelpsbetterunderstandingthestatusof
teammembers,whichisthusimportantforoptimizingmulti-userinteractionsystems.However,thereiscurrentlyalack
explorationonobjectivefeaturesandmethodsfordetectingsimultaneousflow.Basedonbrainmechanismofflowinteamwork
andpreviousstudiesonelectroencephalogram(EEG)-basedindividualflowdetection,thisstudyaimstoexplorethesignificant
âˆ—YulongBianisthecorrespondingauthor.
Authorsâ€™addresses:BaiqiaoZhang,baiqiao@mail.sdu.edu.cn,ShandongUniversity,Weihai,China;XiangxianLi,ShandongUniversity,
Jinan,China,larst@affiliation.org;YunfanZhou,ZhejiangUniversity,Hangzhou,China,zhouyunfan00@163.com;JuanLiu,Shandong
University,Weihai,China,zzzliujuan@sdu.edu.cn;WeiyingLiu,ShandongUniversity,Weihai,China,2202237565@mail.sdu.edu.cn;Chao
Zhou,InstituteofSoftwareChineseAcadamyofSciences,Beijing,China,zhouchao@iscas.ac.cn;YulongBian,ShandongUniversity,Weihai,
China,bianyulong@sdu.edu.cn.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthat
copiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.
Copyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopy
otherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrom
permissions@acm.org.
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
2474-9567/2024/0-ART0$15.00
https://doi.org/XXXXXXX.XXXXXXX
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.
4202
yaM
3
]CH.sc[
1v54020.5042:viXra0:2 â€¢ Zhangetal.
EEGfeaturesrelatedtosimultaneousflow,aswellaseffectivedetectionmethodsbasedonEEGsignals.First,atwo-player
simultaneousflowtaskisdesigned,basedonwhichweconstructthefirstmulti-EEGsignalsdatasetofsimultaneousflow.
Then,weexplorethepotentialEEGsignalfeaturesthatmayberelatedtoindividualandsimultaneousflowandvalidatetheir
effectivenessinsimultaneousflowdetectionwithvariousmachinelearningmodels.Theresultsshowthat1)theinter-brain
synchronyfeaturesarerelevanttosimultaneousflowduetoenhancingthemodelsâ€™performanceindetectingdifferenttypesof
simultaneousflow;2)thefeaturesfromthefrontallobeareaseemtobegivenpriorityattentionwhendetectingsimultaneous
flows;3)RandomForestsperformedbestinbinaryclassificationwhileNeuralNetworkandDeepNeuralNetwork3performed
bestinternaryclassification.
CCSConcepts:â€¢Human-centeredcomputingâ†’HCIdesignandevaluationmethods.
AdditionalKeyWordsandPhrases:Teamflowexperience,EEGsignals,Evaluationmethod,Dataset
ACMReferenceFormat:
BaiqiaoZhang,XiangxianLi,YunfanZhou,JuanLiu,WeiyingLiu,ChaoZhou,andYulongBian.2024.AreWeinTheZone?
ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals.Proc.ACMInteract.
Mob.WearableUbiquitousTechnol.0,0,Article0(2024),36pages.https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Theflowexperience(shortasflow)isanimportantuserexperienceduringhuman-computerinteraction[8].It
isahighlyenjoyablementalstateinwhichtheindividualisfullyimmersedandengagedinactivities,andis
thusconsideredtheoptimalexperience[22][23][69].Flowexistsnotonlyinperformingindividualactivities,but
alsoinperformingcollaborativeactivities.Teammembersmaysimultaneouslyexperienceflowandpositive
collectiveexperienceswhileperformingtasksfortheteamâ€™spurpose,reflectingtheunifiedandcoordinated
feelingsgeneratedamongteammembersastheystrivetowardacommongoal,whichisknownasteamflowor
groupflow[64].Teamflowisconsideredtobethegoalofoptimalcollaboration[73].Studieshaveshownthatthe
experienceofteamflowisimportantforoptimizingteamperformance:itcanfostercloseremotionalconnections
betweenteammembersandtheworkenvironment,increaseteamexperienceandwell-beingattheteamlevel,and
therebystimulatehighermotivation,creativity,andbothteamandindividualperformance[71][78][12][39][73].
Asmulti-usercollaborativeapplicationsbecomemorecommon,theteamflowexperienceisincreasinglyfocused
onandconsideredanimportantuserexperiencegoalincollaborationandcooperation.
Althoughsomeresearchersconsiderteamflowasasolelygroupphenomenon[64],vandenHoutetaldefined
itasaconcatenativeexperienceofteammembersâ€™individualflow[72].Simultaneousindividualflowwhile
executing personal tasks for the teamâ€™s purpose may be the antecedent condition of achieving shared team
flow[10]. Consideringshared teamflow isa complex stateand hardto operationalize,according tovan den
Houtetal.â€™sconcatenativeidea,wefirstfocusontheantecedentconditionanddescribeitas"simultaneous
flowexperienceincollaborativetask"(shortassimultaneousflow),whichmeansteammembersexperiencing
individualflowatthesametime[10].Therefore,methodsofdetectingsimultaneousflowarecriticalforbetter
understandingteammembersâ€™status,whichisimportantforthedynamicoptimizationofmulti-usercollaborative
systems.
Thecurrentprimarymethodstoevaluateflowarequestionnaires,scales,interviewandtheexperiencesampling
method(ESM)[24][36][53].Thesemethodsareretrospectiveandhavesubjectiveresponsebias[79].Bycontrast,
physiologicalmeasurementshaveshowngreaterpotentialinevaluatingflowinamoreobjectiveandreal-time
mannerwithoutdisruptingtheongoingexperienceprocess[7].Forindividualflow,recentstudieshavefound
correlations between flow and neural activity signals (e.g. frontal lobe activity)[15][49], and explored flow
detectionmethodsbasedonelectroencephalogram(EEG)signals[17][37][57][76].Thesestudiespreliminarily
demonstratetheeffectivenessofthismethod.However,themethodsofdetectingsimultaneousflow/teamflow
havenotbeeninvestigatedingreaterdetail.Therefore,thisstudyfocusonthesimultaneousflow,exploringits
relatedfeaturesandpotentialdetectmethodsbasedonEEGsignals.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:3
Wefirstconsideredthesmallestteamsizeanddesignedatwo-usergametoinducesimultaneousflow.Based
onthetask,werecordedthepairedparticipantsâ€™multichannelEEGsignalsfrombrainregionsrelatedtoflow
experience.Afterlabeling,slicing,andprocessingthedata,weconstructedthefirstmultichannelEEGdataset
withmultiplelabelsofflowexperiencesincollaborativetasks.Next,wefocusedoninter-brainsynchronyfeatures
fromdifferentbrainregionsâ€™EEGsignalsandexploredtheireffectivenessindetectingsimultaneousflowby
usingseveralclassifiers,includingLogisticRegression(LR),SupportVectorMachines(SVM),DecisionTrees(DT),
RandomForests(RF)andseveralNeuralNetwork(NN)models.
Themaincontributionsofthisresearchinclude:
â€¢ Wedesignedatwo-usertaskforcontinuouslymeasuringsimultaneousflowincollaborationbasedon
whichweconstructedthefirstmultichannelEEGdatasetofsimultaneousflow.
â€¢ Weproposedinter-brainsynchronyfeaturesfromEEGsignalswhichpotentiallyrelevanttosimultaneous
flowforthefirsttime.Basedoncomparisonexperiments,featureimportanceexperimentsandablation
study,wevalidatedtheeffectivenessofinter-brainsynchronyfeaturesindetectingsimultaneousflow,and
foundfeaturesfromfrontallobeareaweregivenpriorityattentionbythemodels.
â€¢ WemakethefirstattempttorecognizesimultaneousflowbasedonEEGsignalsbyusingmultiplemachine
learningmethods.Inthisstudy,RFachievedthehighestaccuracyinbinaryclassification(83.9%);NNand
DNN3performedbestinternaryclassification(87.2%).
2 RELATEDWORK
2.1 WhatareTeamFlowandSimultaneousFlow?
Individualflowreferstoamentalstatecharacterizedbyperceivingoneselftobeentirelyabsorbedinthecurrent
personalactivity[9][21].Comparedtoindividualflow,teamflow(orgroupflow)placesgreateremphasison
theinteractionsamongteammembersandachievementofanenjoyableexperienceinteamlevel.Sawyer[64]
wasoneofthefirstresearcherstoproposetheconceptofteamflow.Hearguedthatteamflowdependsonthe
interactionsbetweenteammembersandmaymanifestthroughsuchinteractions.Comparedwithindividualflow,
threemajordifferenceswereidentified:(1)Teamflowrequiresbehavioral,cognitive,andemotionalinteractions
amongteammembers,whereasindividualflowisgeneratedfromasoloactivity[71];(2)Inteamflow,collective
goalstakeprecedenceoverotherelementsofthegroup,whereasindividualflowismorefocusedonpersonal
goalsandoutcomes;(3)Teamflowrequiresahigherlevelofskillamongteammembers[58].Sharedteamflowis
acomplexstateofandmore.
Inmorerecentstudies,whilevandenHoutetal.concededthatagroupcanattaina"collectivestateofmind",
theydisagreewiththeviewthatteamflowallowsforindividualsnottoexperienceflow[72].VandenHoutetal.
definedteamflowasasasharedexperienceofflowderivedfromanoptimizedteamdynamicduringtheexecution
ofinterdependentpersonaltasks.Inthisdefinition,"shared"meansthatindividualteammembersareexperiencing
flowsimultaneouslyandcollectivelywhileexecutingtheirpersonaltasksfortheteamâ€™spurpose(s)[72].Inother
words,theyviewteamflowasaconcatenativeexperience,proposingthatsimultaneousindividualflowwhile
executingpersonaltasksfortheteamâ€™spurposemaybetheantecedentconditionofachievingsharedteamflow.
As we explained in the Section 1, we introduce the term "simultaneous flow" to describe simultaneous flow
experienceincollaborativetask,representingthestatethatteammembersexperiencingindividualflowatthe
sametime[10].Inthispaper,wefocuson"simultaneousflow"forthefirststep,andpreliminaryexplorewhether
ithassimilaritieswithsharedteamflow.
2.2 FlowDetectionbasedonEEGSignals
Because traditional flow evaluation methods (such as questionnaires and interviews) are retrospective and
subjective,researchershavepaidincreasingattentiontothepotentialofobjectivephysiologicalsignalsinflow
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:4 â€¢ Zhangetal.
experienceevaluations[7][20][79].Currentstudiesonthistopicprimarilyfocusonevaluatingindividualflow
experiencesusingphysiologicalsignals,suchaselectrocardiograms(ECG),electrodermalactivity(EDA),and
electroencephalograms (EEG). Based on these physiological signals with flow experience labels, supervised
learning(throughtraditionalmachinelearningordeeplearningmodels)hasbeenusedtorecognizeanddetect
differentflowexperiences.Yeetal.[79]constructedseveralmachinelearningmodelstodetectusergamingflow
experiencesusingskinconductanceandheartratesignals.Theyachievedanaccuracyrateof90%inbinary
classificationtasksbyusingasupportvectormachineclassifierandachievedanaccuracyrateof61%internary
classificationtasksbyusingrandomforestalgorithm.Intermsofdeeplearning,Maieretal.[45]proposeda
DeepFlowmodelbasedondeeplearningtoclassifyhigh-levelandlow-levelflow,aswellastoclassifydifferent
experiencesofflow,boredom,andstress.Cherepetal.[20]usedwearabledevicestocollectEEGsignalsand
usedthedeeplearningmodelEEGNettoidentifyflowexperienceswithanaccuracyofover65%.EEGNetuses
rawmultichannelEEGsignalsasinputswithoutpre-extractedfeatures.Accordingtoresearchontheneural
mechanisms of the team flow experience[65], EEG signals contain valuable information regarding the flow
experience.Existingresearchonflowmetricsfocusesprimarilyontheindividualtasks,andlacksexplorationof
flowmetricsatthecollaborativetasks.Therefore,thisstudyfocusesondetectingflowexperienceincollaborative
tasksbasedonEEGsignals.
2.3 PhysiologicalSynchrony
Chatterjeeetal.pointedoutthatthephenomenoninwhichthephysiologicalresponsesoftwoindividualsbecome
moresimilariscalled"physiologicalsynchrony"[18].Researchontheneuralactivitiesofteamflowshowedthat
teamflowischaracterizedbyhigherinter-brainsynchronyinthelefttemporalcortex[65],revealingtheneural
activitymechanismsofteamflowincollaborativetasksbasedonneuralphysiologicalsignals.Currentresearch
onphysiologicalsynchronyindetectingteamflow/simultaneousflowhasnotyetbeenconducted.Therefore,
webeginbyreferringtorelevantresearchinaffectivecomputingandsummarizethefollowingthreeimportant
aspects.
2.3.1 ExperimentalTasksforPhysiologicalSynchrony. Intermsofexperimentaldesign,studiesonphysiological
synchronycommonlyrequirethegroupparticipantstoengageincollaborativetasks.Kevinetal.askedparticipants
toplaydifferentrolesasateaminaflightoperationtaskandcollectedtheirEEGsignalstodetectthecognitive
loadandcooperationlevelamongtheteammembers[74].Darzietal.measuredeachparticipantâ€™sphysiological
response,physiologicalconnectivityindices,andtaskperformanceunderdifferentconditions[25].Theysuggested
thatinmulti-usersettings,physiologicalsynchronyinformationrelatedtothespecificityofinteractionsbetween
participantscanbeobtainedbyexaminingthesimilaritiesintheirphysiologicalresponses.Thedegreeoflinkage
increaseswiththenumberofcollaborations,theintensityofcompetition,andthesheeramountofjointattention.
Thesefindingscanserveasareferencefordesigningsimultaneous/teamflowtasks.
2.3.2 FeaturesofPhysiologicalSynchrony. Intermsoffeatureselection,theexplorationofinter-brainsynchrony
featuresmainlyfocusedonneuralresponseconsistencyacrossparticipants.Chatterjeeetal.categorizedfeatures
intoindividualfeatures,computedfromthephysiologicaldataofasingleparticipant,andsynchronyfeatures,
computedfromthephysiologicaldataoftwoparticipants[18].Darzietal.constructedafeaturesetbycombining
individual physiological features with physiological connectivity features[25]. Javier et al.[32] found that a
combination of features extracted from the childâ€™s EDA activity and physiological synchrony between the
childandtheadultresultedinthehighestclassificationaccuracy.Shkurtaetal.[29]computedthesynchrony
featuresfromEDAsignalsusingdynamictimewarping(DTW)algorithm.Theresultsshowthatanincrementin
physiologicalsynchronyamongstudentsduringalectureisrelatedtoanimprovementinstudentsâ€™emotional
state.Studiesusingmulti-personEEGdatashowthatinter-braincorrelationsinthetaandalphawaveamplitudes
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:5
intherighttemporoparietaljunctionandalphaandbetawaveamplitudesinthefrontalregionareassociatedwith
understandingothersâ€™intentionsandhigh-levelcooperativestrategies[65].Dingetal.[27]arguedthatinter-brain
amplitudefeaturesandacombinationofallinter-brainfeaturesoutperformedsingle-brainfeaturesinthemost
competentparticipant.Inter-brainsynchronyfeaturesshouldbeconsideredinthisstudy.
2.3.3 RecognitionPerformanceBasedonPhysiologicalSynchronyFeatures. Dingetal.[27]usedamethodthat
combinesphysiologicalsynchronyindicesandclassificationalgorithms.Usingmachinelearningtechniques
suchaslineardiscriminantanalysisandrandomforests,theyperformedafour-categoryclassificationtaskand
ultimatelyachievedaclassificationaccuracyof75%.Whenthesynchronyfeatureswereremoved,theaccuracy
decreasedto65.6%.Darzietal.[25]usedthephysiologicalresponsesfrompairedparticipantstoautomatically
classifyemotionswithinacompetitivecontext.Theyusedlinearkernelsupportvectormachinesandensemble
decisiontreestoclassifytheparticipantsâ€™psychologicalstatesusingmultiplephysiologicalsignalssuchasskin
conductance,achievingabinaryclassificationaccuracyof84.3%andaternaryclassificationaccuracyof60.5%.
Kevinetal.[74]assigneddifferentrolesandtaskstoparticipantswithinthesamescenarioandconstructeda
lineardiscriminantclassifierbasedonspectralEEGfeaturestocategorizedifferentpsychologicalstates,achieving
aclassificationaccuracyof60%forteamcooperationlevels.Therefore,inter-brainsynchronyfeaturesshouldbe
includedinsimultaneousflowdetection.
Insummary,neural-activitysignalsreflectflowexperiencesandcanbeeffectivelyusedtoobjectivelyevaluate
them.ThisresearchfocusesonEEGsignals,fullyconsideringthephysiologicalsynchronyfeaturesofteam
members,andexploressimultaneousflowdetectionmethodsbasedonEEGsignals.Asthereiscurrentlyno
specialized,labeledEEGdatasetforsimultaneousflow,webeginbyconstructinganEEGdatasetforsimultaneous
flow.
3 CONSTRUCTIONOFSIMULTANEOUSFLOWEEGDATASET
Inthisstudy,westartwiththesmallestteamsize(twoteammembers)toconstructthefirstEEGdatasetfor
simultaneousflow.Beforecollectingdata,weneedtofirstdesignatwo-usercollaborativeexperimentaltaskto
inducesimultaneousflow.
3.1 DesignofAnExperimentaltaskforInducingSimultaneousFlow
3.1.1 DesignRationale. Currently,nounifiedtaskparadigmisspecificallydesignedtoinducesimultaneousflow.
Toconstructasimultaneousflowtask,wefirstsummarizedthecommonlyusedtasksininducingindividualflow,
including"Whack-a-Mole,""Tetris,"andothergametasks[6][7][30][79],aswellasmentalarithmetic,chess,and
learningquizzes,amongothers[3][37][76].Afteracomprehensivecomparisonoftheseflow-inducingtasks,we
designedthe"Two-playerCollaborativeWhack-a-MoleGame"basedonthetraditionalsingle-playerWhack-a-
Moletask.Themainselectioncriteriawereasfollows:
â€¢ Gamerulesandoperationsaresimpleandprovidehighaccessibilitytovarioususergroups.
â€¢ Thegamecaneasilybedesignedasacollaborativetaskinvolvinginterdependentpersonaltasks.
â€¢ Thegamecanensurehighfluencyinanoperatingtaskbecauseitislesssusceptibletorandomoperational
errors(e.g.,unlikeTetris,inwhichasmallerrorcanaccumulatealargeeffect).
â€¢ Thegamerequirestheparticipantstobefastandhighlyfocused,facilitatingtherapidinductionoftheflow
experience.
â€¢ Thedifficultyoftaskcanbedynamicallyadjustedinaneasyway.
Finally,wedevelopedanexperimentalsystemusingthewidelyusedgameengineUnity3Dbasedonthe
designdescribedinsubsequentsections.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:6 â€¢ Zhangetal.
3.1.2 Storyline. Previousstudiesdemonstratedthattheuseofthemesandstorylinescanenhanceusersâ€™involve-
mentandintentionalengagement[5][11].Thestorylineforthisgameisasfollows:
Youandyourpartnerrunafarmasateambyproducingandsellingagriculturalproductssuchasvegetables.A
groupofmolesistryingtostealagriculturalproducts.Youandyourteammatesneedtoworktogethertoprotect
youragriculturalproductsandfightthemolestominimizelossesandmaximizeprofits.
3.1.3 TaskGoalandCollaborationPattern. AccordingtovandenHoutetalâ€™sconceptualizationofflow[72][73],
thetaskGoalandcollaborationPatternweredesignedasfollows.
TaskGoal:Whenteammembersaimtoachieveacommonteamgoalandpursueoptimalteamperformance,
theycooperateandcollectivelyfocusontheirownpersonaltask,therebyinducingsimultaneousfloworevent
teamflow.Thegoaloftheteamtaskistocollaborativelyprotectasmanyvegetablesandeliminateasmanymoles
aspossibleinalimitedamountoftimetomaximizeprofits.
CollaborationPattern:User1isresponsibleforhittingasmanymolesaspossibletorecapturethevegetables
(Fig.2a),whereasuser2isresponsibleforhittingasmanymolesaspossibletocatchthem(Fig.2b).Althoughthe
divisionoflaborinthegamesisdifferent,theoperationalmethodsareidentical.Individualperformancesare
displayedonauserinterfaceinrealtime.
Hitting targets continuously as much as possible can result in additional rewards for team profits. Team
performanceandprofitsarealsodisplayedontheuserinterfaceinrealtime(Fig.2).Eachuserâ€™sperformancein
thepersonaltaskcontributestotheteamâ€™sperformance,whichmakesthemcollaborateasateam.
Fig.2. Screenshotsofthecollaborativegameinterfaceforuser1(a)anduser2(b).Theinterfacedisplaysvariousparameters
relatedtoteamtasks,individualperformance,andteamperformanceinreal-time.
3.1.4 FlowExperienceSampling. Differentfrompreviousstudieswhichmeasuredflowonlyonceafterthetask,
thisstudyaimedtocapturemoreimmediateinstancesofflow.Therefore,weemployedtheExperienceSampling
Method(ESM)tosampleflowexperiencemultipletimesduringthetask,whichisacommonmethodinflow
research[53].Eachroundofthegamelastsfor5min.Afterthegamestarts,theuserexperienceissampledevery1
minute(fivetimesintotal).Thesystemwasdesignedtopopoutanevaluationinterfacebyaskingparticipantsto
scoretheirindividualflowexperiences.Moreover,thegamescreenremainsvisiblewhentheevaluationinterface
isdisplayed,whichhelpsminimizedisruptiontothecontinuityofgameexperience.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:7
Astothemeasureofflow,whenimplementingESM,repeatedlyansweringlengthyflowquestionnaireswill
interrupt the flow state during the playing session and decrease the validity of the measurements[54], thus
single-itemmeasureisaneffectivewaytominimizedisruptiontothecontinuityofflowexperience[40].The
effectivenessofthesingle-itemmeasurehasbeensupportedbypreviousstudies[60][61].Inthisstudy,weadopted
asingle-itemscaletoevaluatetheflowexperienceonaLikertscalerangingfrom0(noflow)to3(highdegreeof
flow)(seeFig.3).TheitemwasdevelopedfromascientificquestionnairebyCsikszentmihalyitomeasureflow
experiences[53][62][54][40].Accordingtotheestablishedprocedureforusingthisquestionnaire,weprovided
participantswithaone-paragraphdescriptionoftheflowconceptandexamplestoensuretheircomprehensive
understandingbeforeenteringthegame.
Toenhancetherationalityandeffectivenessofthesingle-itemmeasureinESM,incorporatingatest-retest
procedureisbeneficialtominimizethepotentialmeasurementerrorofusingsingle-itemmeasures[26].Accord-
ingly,thisstudyincludedaprocedurewhereusersreviewedthevideorecordingimmediatelyaftercompleting
thegametoverifytheaccuracyofthescoring.
Fig.3. Duringthetask,flowexperiencesaresampledatintervalsbypoppingoutanevaluationinterface(a).Followingthe
sampling,thegamedifficultycanbeadjustedaccordingtothetwoplayerâ€™sskillbypoppingoutanadjustmentinterface(b).
3.1.5 Adjustment of Game Difficulty. According to the classic flow-channel model, achieving flow experi-
encerequiresamatchbetweentheindividualâ€™sskillandtheleveloftaskchallenge(optimalchallengeâ€“skill
balance)[23][79].Appropriatedifficultyisacriticalfactorforinducingaflowexperience.Therefore,amechanism
foradjustingthedifficultyofflow-inducinggametasksisnecessary.
Toourknowledge,thereiscurrentlynoobjectivemethodthatcanaccuratelyandadaptivelyadjustthedifficulty
forpairedparticipants.Therefore,consideringtheexistingmethods,quicklycommunicatingandreachingan
agreementbetweentwoplayerscouldbeareliableapproach.Inthisgame,theoptimaltaskdifficultyisachieved
byallowingthetwouserstoactivelyadjustthespeedsofthemoles(timeofeachoccurrence).Therearefour
molesperoccurrenceandthespeedissetatfivelevels.Duringeachuserexperiencesampling,theparticipants
canquicklycommunicatewhethertoadjustthedifficultylevel,asshowninFig.3(b).
3.1.6 Recording of Game Data. The system continuously records data for both users throughout the game,
including:(1)thetimepointsofgamestart,experiencesampling,andgameover;(2)responsetimesfortemporal
alignmentandsegmentationwithEEGsignals;(3)ratingscoresofflowexperiencefromsampling;and(4)game
performancedata.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:8 â€¢ Zhangetal.
3.2 AcquisitionofEEGData
3.2.1 Participants. 52pairsofparticipantswererecruitedfromalocaluniversitytoparticipatethisstudy(mean
age:18.35Â±1.81years).Noneoftheparticipantsexperiencedalcoholconsumption,excessivefatigue,medication
use,orillnessfromthedaybeforetheexperimentuntilthedayoftesting.Eachpairofparticipantswasrandomly
assignedtooneoftworoles:recapturingvegetablesorcatchingmoles.Atotalof97.6%oftheparticipantswere
right-handed,96.97%hadpreviouslyplayedvideogames,and84.69%werefamiliarwiththeirteammates.The
study was conducted in accordance with the guidelines of the Declaration of Helsinki and approved by the
HumanResearchEthicsCommitteeofthelocalhospital.Informedconsentwasobtainedfromeachparticipant.
Note:Inthispaper,weuseddifferentwordstodescribeparticipants,suchas"user","player"and"teammember".
Theirmeaningsarethesameandweonlyusedifferentexpressionsindifferentcontexts.
3.2.2 Data Acquisition Apparatus and Environment. To acquire EEG data of simultaneous flow, each pair of
participantswasrequiredtoperformthecollaborativetasksimultaneously.TheyusedtwoPCs(LegionTower7i
Gen7withRTX3070andDellmonitors)withthesamemodeltoplaythegame(Fig.4),andperformedthetasks
usingmiceofthesamemodel(RazerDeathAdderV3Pro).
EachparticipantworeanEEGrecordingdevicetoacquiretheEEGsignalsthroughoutthecollaborativetask.
TheEEGrecordingdevicesadoptedinthisstudyweretwosetsofEmotivEpoc+,whichsupportstheacquisition
ofupto14channelsofEEGsignalsatasamplingrateof256Hz[43].Thesoftwareusedfordataacquisitionis
EmotivePRO.Theelectrodedistributionforthe14-channelEEGsignalsfollowsthe10-20internationalsystem,
asshowninFig.5.WechosetheEmotivEpoc+becauseitiseasytousewithincreasedpracticabilityandless
physicalrestriction.Moreover,ithasbeeneffectivelyusedinbuildingEEGdatasetsforflowexperience,emotions,
andpsychologicalbehaviors[47][82][38][56][70].
Fig.4. Dataacquisitionapparatusesandexperimentalenvironment.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:9
Fig.5. ElectrodeplacementdistributionforEEGdataacquisition.
3.2.3 EEGDataAcquisitionProcedure. ThedataacquisitionprocessisshowninFig.6.
First,theresearcherexplainedthegamedetailstoeachparticipantpair.Subsequently,eachpairofparticipants
practicedtheoperationsinapracticescene.TheythenworetheEEGacquisitiondevicesandenteredarelaxed
state.Duringthisperiod,theirbaselineEEGdatawerecollectedforapproximately1min.Theparticipantsthen
playedthreeroundsofthegame,withsufficientrestbetweenrounds.Tocapturevariousflow-relatedexperiences,
thestartingdifficultylevelsforthethreeroundswerevaried.Flowexperiencesampling(introducedinSection
3.1.4)wasperformedduringthisperiod.Duringthegame,theparticipantswereaskedtoavoidheadmovements
andfacialexpressionstoreduceartifactsintheEEGsignals.Theresearchermonitoredtheparticipantsâ€™EEG
dataonaseparatedisplaythroughoutthegame.Ifthereweresignalabnormalitiesduringtheprocess,theywere
recordedandconsideredtoexcludethecorrespondingdatafromsubsequentprocessing.Ittookapproximately
30â€“40 minutes for each pair of participants to complete the EEG data acquisition process. Each participant
obtainedacorrespondingcoursecreditasareward.Finally,welabeledandpreparedtherecordedEEGsample
dataforfurtherdatasetorganizationbasedontheflowscoresobtainedduringuserexperiencesampling.
3.3 DatasetOrganization
3.3.1 Selection of EEG Channels. It has been demonstrated that the flow experience is associated with the
activityinthefrontallobeofthebrain[37][53].Moreover,Shehataetal.[65]foundthatthelefttemporallobeacts
downstreamincausalinformationrelationships,receivingandintegratinginformationfrombrainregionsrelated
toindividualflowandsocialinteraction,andparticipatinginhigher-orderinter-brainneuralsynchrony[24].This
areaisactivatedduringtheemergenceofteamflowandplaysaroleingeneratingsharedflowexperiences[65].
Therefore,wehaveselectedtheEEGsignalsfromhefrontalandlefttemporallobestoconstructthesimultaneous
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:10 â€¢ Zhangetal.
Fig.6. ProcessDiagramofSimultaneousFlowCollectionSystem.
flowdataset.Finally,accordingtothedistributionoftheelectrodesshowninFig.5,eight-channelEEGsignals
wereselected:F3,F4,F7,F8,AF3,AF4,T7,andP7.
3.3.2 EEGSignalSlicing. Foreachparticipant,continuousEEGsignalswererecordedduringeachroundof
thegamefor5min.Thedatafromthe6-secondintervalsprecedingeachsamplingpointwereusedforEEG
dataslicing.TheformofdatastoredineachslicedsignalfilewaschannelsÃ—datapoints.Thesamplingrateis
256Hz,thenumberofdatapointsineachfileis1536.Weselected6-secondsegmentsofEEGsignals,duetoa
6-secondtimewindowsizestrikesabalancebetweenaccuracyandcomputationalefficiency.Itislongenoughto
capturesufficientinformationofasubjectivestate[82],andwhilebeingshortenoughtoreducecomputational
complexityfordynamiccomputing[16].
3.3.3 DatasetComposition. Although52pairsofparticipantsparticipatedinEEGdataacquisition,47pairswere
includedinthefinaldatasetafterexcludinggroupswithdataacquisitionerrorsandoperationalfailures.Thesize
ofthedatasetwasapproximately34.4GB,andcontainedEEGsignalsrecorded6sbeforeeachflowexperience
sampling.AlldataweresavedasCSVfiles,eachofwhichwasamatrixofdimensions[14,1536].Thesedata
represent14-channelEEGdatawith1536samplepoints;thesamplingrateoftheEEGsignalsis256Hzi.e.,each
CSVfilehasadurationof6seconds.
Duringthegame,theEEGsignalscollectedfromeachpairofparticipantswereorganizedby"groupnumber-
participantnumber-gamesetnumber.â€Eachparticipantâ€™sfoldercontainedEEGdataslicedfromfivegamerounds.
Thedataandcodeofthispaperisreleasedat:https://anonymous.4open.science/r/Team-Flow-Dataset-494E
anonymously.Detailedinformationaboutthedataisalsosummarizedinthe"README.md"file.
4 FEATUREEXTRACTIONFORINDIVIDUALANDSIMULTANEOUSFLOW
Accordingtopreviousstudies,EEGsignalscontainbothindividual[17][37][76]andsimultaneousflowfeatures[65].
Basedonpreviousstudies,wehavesummarizedtheEEGsignalfeaturesrelatedtoindividualflowandproposed
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:11
featuresthatmayreflectinter-brainsynchronyamongteammembersforsimultaneousflow.Methodsfordata
denoisingandfeatureextractionareintroducedinthissection.
4.1 DataDenoising
RawEEGdatacontainnoiseandredundancy,thusrequiringinitialpreprocessing.Themainsourcesofnoisein
EEGsignalsincludephysiologicalartifacts,powerlineinterference,andinterferencegeneratedbyactivitiessuch
asblinkingandfacialmusclemovements.ConsideringthefrequencyrangeofEEGsignalsandthefrequency
rangeofnoiseinterference,thisstudyemployedwaveletthresholddenoisingmethodstodenoiseeachchannelof
therawEEGsignals.
4.2 FeatureExtractionandNormalization
Based on previous research analyzing the EEG signals of individual flow experiences, this study extracted
sometime-andfrequency-domainfeaturesthatmayberelevanttoindividualflowexperiences.Moreover,we
preliminarilyextractedEEGfeaturesrelatedtointer-brainsynchronyaspotentialsimultaneousflowfeatures.
Detailedexplanationsareasfollows.
4.2.1 FeaturesforIndividualFlow. Thisstudyextractstimeandfrequency-domainfeaturesthatmayberelated
to individual flow experiences, as listed in Appendix Table 2. The time-domain features include the Mean,
StandardDeviation(SD),Variance,andAverageAbsoluteFirst-OrderDifference(AAFOD),NormalizedFirst-Order
Difference(NFOD),Energy,Power,HjorthParameters(Activity,Mobility),Higher-OrderZero-Crossing(HOZC),
Peak-to-PeakMean(PPM),andKurtosis.
Frequency-domainfeaturesmainlyinvolvewaveletdecomposition[46]ofthecollectedEEGsignalsintofour
non-overlappingsub-bands:ğ›¿ (0â€“4Hz),ğœƒ (4â€“8Hz),ğ›¼ (8â€“16Hz),andğ›½ (16â€“32Hz).
Thefrequency-domainfeaturesarethenextractedfromthesesub-bandsintheeightEEGchannels.Power
SpectralDensity(PSD)andLogarithmicbandpower(LBP)arecalculatedforeachofthefourbands.Differential
Entropy(DE)featuresareextractedfromfourfrequencybands(ğ›¿,ğœƒ,ğ›¼,andğ›½)andthefullfrequencyband(FB)of
EEGsignalsineachchannel.ThisstudyemploystheWelchmethod[77]toextractthepowerspectraldensity
offourfrequencybands:delta,theta,alpha,andbeta,andcomputestheaverageofthepowerspectrumacross
channeldimensions.
4.2.2 Features for Simultaneous Flow. In the context of simultaneous flow, this study extracted inter-brain
synchronyfeaturesfromtheEEGsignalsofeachpairofparticipants,includingcross-correlationcoefficientsand
DynamicTimeWarping(DTW)distance.
Letğ´= {ğ‘ 1,ğ‘ 2,ğ‘ 3,...,ğ‘ ğ‘š}representtheEEGsignalvaluessampledfromParticipant1atacertainelectrode,
andğµ = {ğ‘ 1,ğ‘ 2,ğ‘ 3,...,ğ‘ ğ‘›}representtheEEGsignalvaluessampledfromParticipant2atthesameelectrode.The
featuresareintroducedasfollows:
(1) Cross-correlationCoefficient:ThePearsoncorrelationcoefficientforthebrainwavesignalsbetweeneach
channelandthefrequencybandofthetwoparticipantsarecomputed[33].Thisservesasanindexfor
assessingthesignalcorrelationbetweenteammembers.
cov(ğ´,ğµ)
ğœŒ ğ´,ğµ = ğœ ğœ (1)
ğ´ ğµ
Wherecov(ğ´,ğµ)calculatesthecovariancebetweenthetwosignals,ğœ ğ´ isthestandarddeviationofEEG
signalA,andğœ ğµ isthestandarddeviationofEEGsignalB.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:12 â€¢ Zhangetal.
(2) DynamicTimeWarpingDistance[18][25][48]:Thisisatechniquebasedondynamicprogrammingforquan-
tifyingthesimilaritybetweentwosignals,andisrobusttotimedecorrelation.Thespecificcomputational
stepsareasfollows:
(a) CalculatetheEuclideandistanceğ·(ğ‘ ğ‘–,ğ‘ ğ‘—)betweeneverytwosamplepointsinthetwosignalsequences,
where1 â‰¤ğ‘– â‰¤ğ‘š,1 â‰¤ ğ‘— â‰¤ğ‘›.
(b) Findtheshortestpathdistancefrom(ğ‘ 1,ğ‘ 1)to(ğ‘ ğ‘š,ğ‘ ğ‘›).Thenextnodefromacurrentnode(ğ‘ ğ‘–,ğ‘ ğ‘—)must
beamong(ğ‘ ğ‘–+1,ğ‘ ğ‘—),(ğ‘ ğ‘–,ğ‘ ğ‘—+1),or(ğ‘ ğ‘–+1,ğ‘ ğ‘—+1),andthepathmustbetheshortest.
(c) Duringeachiterationtoreach(ğ‘ ğ‘–,ğ‘ ğ‘—),followthedynamicprogrammingparadigmtochoosetheshortest
distanceto(ğ‘ ğ‘–,ğ‘ ğ‘—)fromamong(ğ‘ ğ‘–âˆ’1,ğ‘ ğ‘—),(ğ‘ ğ‘–,ğ‘ ğ‘—âˆ’1),or(ğ‘ ğ‘–âˆ’1,ğ‘ ğ‘—âˆ’1).
ğ·(ğ‘ ğ‘–,ğ‘ ğ‘—) =Dist(ğ‘ ğ‘–,ğ‘ ğ‘—)+min(cid:8)ğ·(ğ‘ ğ‘–âˆ’1,ğ‘ ğ‘—),ğ·(ğ‘ ğ‘–,ğ‘ ğ‘—âˆ’1),ğ·(ğ‘ ğ‘–âˆ’1,ğ‘ ğ‘—âˆ’1)(cid:9) (2)
4.2.3 FeatureNormalization. DuetothetypicallysignificantvariabilityinthefeaturevaluesofEEGsignals
amongdifferentparticipants,thisstudyemploystheZ-scorenormalizationmethodtostandardizethefeatures
ofallparticipants.Thisstrategyaimstomitigatetheimpactofinter-individualdifferencesintherangeand
variabilityofphysiologicalsignals[79].Letğ‘“ representthefeaturevaluesextractedforallparticipantsonagiven
feature,ğ‘“Â¯bethemeanvalueofthesefeaturevalues,and ğ‘“ ğ‘ ğ‘¡ğ‘‘ bethestandarddeviationofthefeaturevalues.The
normalizedvalueofthefeatureisthengivenby:
ğ‘“ âˆ’ğ‘“Â¯
ğ‘§ =
ğ‘“
ğ‘ ğ‘¡ğ‘‘
4.2.4 OverviewofFeatures. Theindividualandinter-brainsynchronyflowfeaturesselectedinthisstudyare
listedinAppendixTable2.Byextractingfeaturesfromtheeight-channelEEGsignals,272features(208features
ofindividualflowand64featuresofinter-brainsynchrony)wereselected,constitutingthesetofindividualand
simultaneousflowfeaturesusedinthisstudy.
5 VALIDATIONOFFLOWFEATURES
5.1 Purpose
The primary purpose of this section is to validate the effectiveness of individual and inter-brain synchrony
featuresindetectingdifferenttypesofsimultaneousflowandtoanalyzetheimpactofthesefeaturesinthis
process.Webeganwithvalidatingtheeffectivenessofthesefeaturesinbinaryandternaryclassificationtasks.
5.2 DatasetOrganization
BasedontheflowratingscoresonEEGdataset,weestablishedtwosetsofclassificationlabels.Thefirstset
distinguishesbetween"LowSimultaneousFlow"and"HighSimultaneousFlow."Thesecondsetcategorizesthe
dataintothreegroups:"SimultaneousFlow","IndividualbutnotSimultaneousFlow,"and"NeitherIndividualnor
SimultaneousFlow".
5.2.1 Binary Classification Labels. The binary classification labels of "Low Simultaneous Flow" and "High
SimultaneousFlow"areco-determinedbytheflowexperiencesofthepairedusers.Accordingtothedefinition
of simultaneous flow [10], this study operationally defined it as "simultaneous flow occurs only when both
participants achieve a high flow experience in collaborative task". Scores of "0" and "1" represent low flow
experiences,whilescoresof"2"and"3"representhighflowexperiences.Takingtheflowexperienceofparticipant
1asanexample,thelabelingrulesoftheSimultaneousFlowLabelsarelistedinTable1,andthedistributionof
samplelabelsinthedatasetislistedinTable2.
5.2.2 TernaryClassificationLabels. Accordingtothedefinitionofsimultaneousflow,theternaryclassification
labelsareco-determinedbythepairedparticipantsâ€™flowexperience.TakingtheflowexperienceofParticipant1
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:13
Table1. SimultaneousFlowLabelsbasedonthetwousersâ€™flowscore(BinaryClassification)
Scoreofparticipant1 Scoreofparticipant2 BinaryClassificationlabels
0 0/1 LowSimultaneousFlow
1 0/1 LowSimultaneousFlow
2 2/3 HighSimultaneousFlow
3 2/3 HighSimultaneousFlow
Table2. SampleLabelDistribution(BinaryClassification)
SimultaneousFlowLabel SampleCount Proportion(%)
LowSimultaneousFlow 512 37.1
HighSimultaneousFlow 868 62.9
asanexample,thelabelingrulesarelistedinTable3.Thedistributionoftheternaryclassificationsamplelabels
inthedatasetispresentedinTable4,revealingtheproblemofsampleimbalance.
Table3. ComparisonofIndividualFlowRatingsandSimultaneousFlowLabels(TernaryClassification)
Individual Individual
Flowof Flowof SimultaneousFlow TernaryClassificationLabelsofParticipant1
Participant1 Participant2
Low Low LowSimultaneousFlow NeitherIndividualnorSimultaneousFlow
Low High LowSimultaneousFlow NeitherIndividualnorSimultaneousFlow
High Low LowSimultaneousFlow IndividualbutnotSimultaneousFlow
High High HighSimultaneousFlow SimultaneousFlow
Table4. SampleLabelDistribution(TernaryClassification)
SimultaneousFlowLabel SampleCount Proportion(%)
NeitherIndividualnorSimultaneousFlow 278 20.1
IndividualbutnotSimultaneousFlow 234 17.0
SimultaneousFlow 868 62.9
5.3 ValidationTools
Researchonflowcomputationshasdemonstratedthatmachinelearninganddeeplearningmodelsareeffective
in detecing the individual flow experiences [45][59][79]. Building on these findings, this study employed a
rangeofclassicclassifiersandneuralnetworkstovalidatetheeffectivenessofEEGfeaturesforindividualand
simultaneousflow.Toevaluatemodelperformance,thisstudyusedmetricssuchasaccuracy,precision,recall
andF1scoreforacomprehensiveevaluation.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:14 â€¢ Zhangetal.
5.3.1 DetailsofImplementation. TrainingandevaluationofallthemodelswereimplementedusingPython3.9.12,
withtheScikit-Learn[52]library(version1.2.0)andPyTorch[51](version1.12.0).Modeltrainingwasperformed
onNvidiaGeforceRTX3070Ti.Weusedclassificationmodelstopredictthesimultaneousflowexperienceusing
thefeaturesintroducedinSection4.2.ArangeoftraditionalmachinelearningalgorithmsfromScikit-Learnwere
usedincludingLogisticRegression(LR),SupportVectorMachines(SVM),DecisionTrees(DT),andRandom
Forests(RF).Inaddition,wedesignedseveralNeuralNetwork(NN)modelswithdifferentarchitecturesusing
PyTorch,distinctfromScikit-Learn.AppendixTable3providesadetaileddescriptionsandconfigurationsofthe
models.
5.3.2 Metrics. Forbinaryandternaryclassificationtasks,weusedmetricsincludingAccuracy,Precision,Recall,
andF1Score.DetaileddefinitionsoftheseformulascanbefoundinAppendixTable1.
5.4 ValidationProcedure
Thefollowingstrategieswereusedtoreducethebiasintroducedbydatadistributionandtheselectionoftraining
andtestingsets.
5.4.1 Balancing the Dataset. For the binary classification task, the ratio of low simultaneous flow to high
simultaneousflowsamplesintheunbalanceddatasetwas1:1.64.WeusedtheSMOTEalgorithm[19]togenerate
minority-classdata,adjustingtheratioofdifferentclassesto1:1.Afteradjustment,thesamplesizeforboth
classeswas838,foratotalsamplesizeof1676.
Forthemulti-classificationtask,theratioof"NeitherIndividualFlownorSimultaneousFlow,""Individualbut
notSimultaneousFlow"and"SimultaneousFlow"intheunbalanceddatasetwas1.2:1:3.7.WeusedtheSMOTE
algorithmtogenerateminorityclassdataandadjusttheratioofallthreeclassesto1:1:1,therebyincreasingthe
totalnumberofsamplesto2604.
5.4.2 Ten-foldCross-Validation. Toavoidthebiasintroducedbydatapartitioning,weusedk-foldcross-validation
todividethesampledatasetrandomlyintokfolds.Ineachrun,onefoldwasusedasthetestsetandtheremaining
k-1foldswereusedasthetrainingset.Classifieraccuracy,precision,recall,andF1scorewerecalculatedbased
onthetestsetforeachtrainingset.
Tomitigateproblemsarisingfromimproperdatasplittingandevaluatethegeneralizabilityofthemodelmore
accurately,weusedten-foldcross-validation(k=10).Thefinalperformancemetricwastheaverageofthescores
acrossthetenfolds.
5.5 ValidationResults
5.5.1 ValidationResultsoftheBinaryClassification. Thevalidationresultsareintroducedasfollows.Table5
presentstheperformanceofvariousmachinelearningmodelsinabinaryclassificationtask,withorwithout
consideringinter-brainsynchronyfeatures,asevaluatedthroughten-foldcross-validation.Themainresultsare
summarizedasfollows:
(1) Modelsincludinginter-brainsynchronyfeaturesoutperformedthoseexcludingthem,demon-
stratingtheirbenefitstotheclassificationtask.Thesefindingsimplythatincludinginter-brainsynchrony
featurescanenhancetheclassificationperformanceofsimultaneousflowmodels,therebypreliminary
validatingthecontributionoftheEEGinter-brainsynchronyfeaturestosimultaneousflowclassification.
TheAppendixâ€™sTable4displaystheT-testresults,showingğ‘-valuesunder0.05(ğ‘ğ‘ <0.05).Thissuggests
thestatisticalsignificanceofthemodelâ€™sperformanceenhancementuponincludingsynchronyfeatures.
(2) Tree-basedmodelsexhibitthemostsignificantimprovementaftertheadditionofsynchrony
features.Whenconsideringsynchronyfeatures,theRandomForestmodelsurpassedmostothersinbinary
classification,achievinganaccuracyof83.9%.DecisionTreesdemonstratedthemostnotableimprovement,
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:15
Model Accuracy Recall Precision F1Score
Mean Std Mean Std Mean Std Mean Std
LR 0.606 0.032 0.606 0.033 0.607 0.033 0.605 0.032
LR(S) 0.632 0.023 0.632 0.025 0.633 0.025 0.630 0.025
SVM 0.663 0.034 0.664 0.035 0.664 0.034 0.663 0.034
SVM(S) 0.696 0.030 0.697 0.032 0.698 0.033 0.696 0.030
DT 0.618 0.049 0.618 0.049 0.620 0.050 0.616 0.049
DT(S) 0.757 0.039 0.757 0.039 0.757 0.039 0.756 0.039
RF 0.758 0.032 0.759 0.033 0.760 0.033 0.758 0.032
RF(S) 0.839 0.024 0.840 0.025 0.841 0.024 0.838 0.024
NN 0.714 0.031 0.714 0.035 0.717 0.036 0.712 0.032
NN(S) 0.757 0.026 0.757 0.027 0.759 0.026 0.756 0.027
DNN1 0.718 0.037 0.717 0.038 0.730 0.036 0.712 0.038
DNN1(S) 0.753 0.023 0.753 0.023 0.763 0.022 0.750 0.024
DNN2 0.722 0.030 0.724 0.031 0.729 0.031 0.720 0.031
DNN2(S) 0.754 0.035 0.753 0.038 0.766 0.030 0.749 0.040
DNN3 0.732 0.033 0.733 0.033 0.738 0.033 0.730 0.033
DNN3(S) 0.762 0.019 0.751 0.020 0.768 0.028 0.747 0.019
Table5. Averageperformanceof10-foldcrossvalidationonbinaryclassificationfordifferentmachinelearningmodels.Std
denotesstandarddeviation.TheconfigurationsofdifferentmodelsareprovidedinTable3ofAppendix.
enhancingby13.9%.ThisimprovementmightbeattributedtothemechanismofDTandRF.Effective
featureextractionandselectionensurethedecisionnodessplitdataeffectively,therebyenhancingthe
modelsâ€™predictiveaccuracy.Thediscussionsectionprovidesamorein-depthexplanationofthepossible
reasonsforthisresult.
5.5.2 Validation Results of the Ternary Classification. Table 6 presents the performance of various machine
learningmodelsinaternaryclassificationtask,consideringbothwithandwithoutinter-brainsynchronyfeatures,
asevaluatedthroughten-foldcross-validation.Themainresultsaresummarizedasfollows:
(1) Consistentwiththetrendsobservedinthebinaryclassificationtask,modelsincludingsynchrony
featuresgenerallyoutperformedthosewithoutthesefeatures.Thisvalidatestheeffectivenessof
inter-brainsynchronyfeaturesinenhancingmodelperformance.Thissuggeststhatinter-brainsynchrony
features contain a certain amount of information valuable for the three-class classification. Table 5 in
theAppendixpresentstheresultsoftheT-test,withğ‘-valueslessthan0.05(ğ‘ğ‘ <0.05),indicatingthatthe
improvementinmodelperformanceisstatisticallysignificant.
(2) Themetricsoftheternaryclassificationtaskweregenerallyhigherthanthoseofthebinary
classificationtask.Thiscouldbeattributedtothemoredistinctivecategoriesintheternaryclassification
task. The categories in ternary classification task offer clearer distinctions compared to the high and
lowsimultaneousflowstatesinbinaryclassification.Thediscussionsectionprovidesamorein-depth
explanationofthepossiblereasonsforthisresult.
(3) Fortheperformanceofdifferentmodels,theNNandDNN3withsynchronyfeaturesoutper-
formedthanothermodelswhichachieveanaccuracyof87.2%.However,wefoundthatmorecomplex
DNNmodelsmayoverfitthetrainingdata,especiallywhenthedatasetsizeisinadequate.Overfittingcan
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:16 â€¢ Zhangetal.
Model Accuracy Recall Precision F1Score
Mean Std Mean Std Mean Std Mean Std
LR 0.607 0.029 0.607 0.029 0.604 0.029 0.604 0.03
LR(S) 0.634 0.028 0.634 0.027 0.632 0.028 0.629 0.028
SVM 0.729 0.020 0.730 0.022 0.729 0.022 0.725 0.020
SVM(S) 0.758 0.029 0.758 0.029 0.756 0.029 0.755 0.027
DT 0.629 0.022 0.629 0.023 0.628 0.022 0.625 0.022
DT(S) 0.666 0.026 0.666 0.024 0.665 0.024 0.660 0.024
RF 0.842 0.016 0.842 0.017 0.842 0.017 0.839 0.017
RF(S) 0.868 0.019 0.870 0.019 0.869 0.019 0.867 0.019
NN 0.809 0.015 0.809 0.012 0.808 0.013 0.804 0.015
NN(S) 0.872 0.017 0.872 0.014 0.874 0.017 0.868 0.017
DNN1 0.837 0.016 0.837 0.014 0.841 0.015 0.829 0.016
DNN1(S) 0.862 0.020 0.863 0.015 0.868 0.019 0.856 0.019
DNN2 0.843 0.020 0.842 0.023 0.846 0.021 0.836 0.023
DNN2(S) 0.862 0.019 0.862 0.019 0.871 0.018 0.856 0.020
DNN3 0.845 0.020 0.844 0.024 0.848 0.020 0.838 0.024
DNN3(S) 0.872 0.016 0.871 0.018 0.876 0.019 0.866 0.018
Table6. Averageperformanceof10-foldcrossvalidationonternaryclassificationfordifferentmachinelearningmodels.Std
denotesstandarddeviation.TheconfigurationsofdifferentmodelsareprovidedinTable3ofAppendix.
limittheperformanceimprovementontestdata.ComparedtotheDNN2withfivehiddenlayers,theDNN3
withninehiddenlayersshowedlimitedimprovementinperformanceandoffersnoadvantageinnumber
ofparameters.
5.5.3 FeatureImportance. Afterthecomparisonexperiments,wehavethefollowingquestions:
â€¢ Q1:Whichbrainregionâ€™sfeaturescontributemoresignificantlytoflowclassification?
â€¢ Q2:Howsignificantisthecontributionofsynchronyfeaturestoclassification?
Toanswerthequestionsandfurthervalidatetheeffectivenessofsynchronyfeatures,weemployedfeature
selectionmethodsusingLR,RF,NNandDNN3.Table7and8respectivelyshowthetop20featuresforbinaryand
ternaryclassificationtasksincludinginter-brainsynchronyfeaturesonfourrepresentativemodels.Tables6and
7,8and9intheAppendixpresentthetop20featuresandtheirimportancevaluesforbinaryalongwithternary
classificationtasks,includingandexcludinginter-brainsynchronyfeatures.Thefeaturesandtheircalculation
methodsareshowninTable2oftheAppendix.
IntheLRmodel,eachfeatureisassociatedwithacoefficient,indicatingtheextentanddirectionofthefeatureâ€™s
impactontheprediction.Thesignofthecoefficient(COEF)denotesthepositiveornegativecorrelationofthe
featurewiththeresult.Themagnitudeofthecoefficientâ€™sabsolutevaluereflectstheimportanceofthefeature.
WecalculatedtheaveragecoefficientforeachfeatureusingLogisticRegressionmodelstrainedineachfoldof
ten-foldcross-validation.FeatureswerethenrankedaccordingtotheabsolutevalueoftheirCOEF,selectingthe
top20withthelargestabsolutevalues.
InthefeatureselectionbasedonRF,weusedrandomforestmodelstrainedineachfoldoften-foldcross-
validationtocalculatetheaverageimportanceofeachfeature.Featureswererankedaccordingtotheirimportance
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:17
(IMP), and the top 20 features with the highest importance were selected. The importance of a feature was
computedbasedontheMeanDecreaseinImpurity(MDI)[13].
SHapleyAdditiveexPlanations(SHAP)[44]valuesareamethodforinterpretingthepredictionsofmachine
learningmodels.SHAPworksbyassigningthemodelâ€™soutputtoeachinputfeature,thusshowinghowmuch
eachfeaturecontributestotheprediction.WecalculatedtheaverageSHAPvaluesforeachfeatureusingNNand
DNN3modelstrainedineachfoldoften-foldcross-validation.Featuresweresubsequentlyrankedbasedonthe
absolutevaluesoftheirSHAPvalues,andthetop20featureswiththelargestabsolutevalueswereselected.
LR RF NN DNN3
P7NFOD P7DTWğ›¼ F7PSDğ›¼ F7CCCğ›¿
F7PSDğ›¼ AF4CCCğ›¼ F8CCCğ›¿ AF4HOZC
AF4DTWğ›¼ F7CCCğ›½ F7PSDğœƒ F3CCCğœƒ
F3NFOD F4MeanPSD AF3DTWğ›¿ T7DEğ›¿
P7Mobility F3CCCğ›¼ F8CCCğ›¼ AF4CCCğ›½
AF3Mobility F3DTWğœƒ F4LBPğ›¿ T7PSDğ›½
AF3DTWğœƒ F7DTWğ›¿ F4AAFOD F4LBPğ›½
F8DTWğœƒ F3DTWğ›¼ F3DTWğ›¿ F8CCCğ›¿
T7PSDğœƒ AF3CCCğ›¼ P7Variance F7CCCğ›¼
F8DTWğ›¼ P7CCCğ›¼ F4DTWğœƒ AF4Energy
F4DTWğ›¼ AF4CCCğ›½ F3CCCğœƒ F4HOZC
F3DTWğ›½ F8CCCğ›¼ AF3PPM T7AAFOD
AF3DTWğ›¼ F8CCCğ›½ P7DTWğ›½ AF3DEFB
T7SD F8CCCğ›¿ T7PSDğ›¼ F3DEğœƒ
T7Mobility F4CCCğ›¼ F4LBPğœƒ AF3CCCğœƒ
AF3DTWğ›¿ AF4CCCğœƒ P7PSDğ›½ AF3PSDğ›¿
F7DTWğ›¿ F7CCCğ›¿ F8DTWğœƒ F7DEğ›¼
F4DTWğ›½ F4average F8PPM T7Mobility
F3Mobility F4PSDğ›¿ F4Energy AF3CCCğ›½
T7PSDğ›¼ P7CCCğœƒ P7PPM F8Mobility
Table7. Theresultsoffeatureimportanceexperimentonbinaryclassificationmodels.Featuresinboldrepresentinter-brain
synchronyfeatures,graycellsrepresentfeaturesextractedfromthefrontallobe.Thefullnamesofthefeaturesaredetailed
insection4.2.1.
Themainresultsaresummarizedasfollows:
(1) Comparedtofeaturesofthelefttemporallobe(T7,P7),thefeatureslocatedinthefrontallobe
(AF3,AF4,F3,F4,F7,F8)weremorepreferredbythemodels.AsshowninTables7and8,thefrontal
lobefeatures(graycells)accountforasignificantproportionbothinbinaryandternaryclassification.This
suggeststhatthefrontalloberegionlikelycarriesmoreeffectiveinformationthanthelefttemporallobe.
ThisisconsistentwithBruyaetal.â€™sopinion[15]thattheflowexperienceiscloselyrelatedtotheactivation
levelofcerebralcortexandfrontalcortex.
(2) Theinclusionofsynchronyfeaturesaccountedforasignificantproportioninthetop20features.
Inthebinaryclassificationtaskincludingsynchronyfeatures,thesynchronyfeaturesoccupiedover35%of
thetop20features,whileinternaryclassificationtasks,theyrepresentedover20%.Thissubstantiatesthe
efficacyofinter-brainsynchronyfeaturesinrecognizingsimultaneousflow.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:18 â€¢ Zhangetal.
LR RF NN DNN3
P7SD F4PSDğœƒ P7PSDğ›½ AF4CCCğœƒ
AF4DTWğ›¼ P7DTWğ›¼ F7PSDğ›½ P7Kurtosis
AF4Mobility F4average F8DTWğ›¿ AF3Kurtosis
F4DEğ›¿ AF3Kurtosis P7Kurtosis T7LBPğ›½
P7Mobility AF3PSDğ›½ F8PSDğœƒ F4CCCğœƒ
AF3DTWğœƒ P7DEğœƒ AF4PSDğ›¿ F7DEğ›½
F7NFOD F8PPM AF3LBPğ›½ F7DEFB
F8SD F4LBPğ›¿ AF3PSDğ›½ F7Kurtosis
P7Variance P7LBPğœƒ AF4AAFOD F4LBPğ›½
P7Activity F4Energy F4DEğ›¿ AF3DEğœƒ
T7DEFB F8Kurtosis P7HOZC F3NOFD
AF4DEFB F4Power F4DTWğ›¿ T7CCCğœƒ
P7NFOD P7CCCğ›½ F3LBPğœƒ P7DEFB
F8DEFB AF4CCCğ›¼ T7PSDğœƒ F4LBPğœƒ
AF4PPM F8CCCğ›¼ T7DEFB T7CCCğ›¿
T7DTWğ›¿ F7PSDğ›½ F3DEğœƒ F3CCCğ›¼
F7Mobility F4LBPğ›¼ T7PSDğ›¼ F4DTWğ›¿
F8PSDğ›½ T7DTWğœƒ AF3DTWğ›¼ F7CCCğ›¿
F4DTWğ›¼ F4DEğ›¼ P7PSDğ›¼ T7LBPğœƒ
AF4DEğ›¿ T7DTWğ›½ F7CCCğ›¿ T7AAFOD
Table8. Theresultsoffeatureimportanceexperimentonternaryclassificationmodels.Featuresinboldrepresentinter-brain
synchronyfeatures,graycellsrepresentfeaturesextractedfromthefrontallobe.Thefullnamesofthefeaturesaredetailed
insection4.2.1.
(3) Synchronyfeaturesweremorefocusedinbinaryclassificationtasks.Thepreferenceforsynchrony
featuresinbinaryclassificationmayattributetotheircriticalroleinreflectingteamcollaborationlevels
todifferentiatebetweenhighandlowsimultaneousflow.Incontrast,theternaryclassificationrequiresa
balancebetweenindividualandsynchronyfeaturesduetoitsclearercategorydistinctions.Thediscussion
sectionprovidesamorein-depthexplanationofthepossiblereasonsforthisresult.
5.5.4 AblationStudy. Toevaluatetheimpactofdifferentfeaturesetsontheperformanceofdifferentmodels,we
conductedablationexperiment.Thisexperimentaimstofurthervalidatethesignificanceoffeaturesrelatedto
thefrontallobeandinter-brainsynchrony.ThemodelsconsideredincludeLR,RF,NNandDNN3.Eachmodel
wastrainedusinga10-foldcross-validationtoensurethereliabilityoftheresults.Table9showstheresultsof
ablationexperimentsforLR,RF,NN,DNN3onternaryclassification.
Thefeaturesetsusedareextractedfromthelefttemporallobe(L),thefrontallobe(F),andtheircorresponding
synchronyfeatures(LSforlefttemporallobe,FSforfrontallobe).Thebaselinefeaturesetconsistedoffeatures
fromthelefttemporallobe(L).WecalculatedÎ”torepresentthepercentageincreaseinperformancemetrics.
Themainresultsaresummarizedasfollows:
(1) Featureadditionimprovesperformanceandtheimprovementsacrossdifferentmetricsisconsis-
tent.AsshowninTable9,theinclusionofadditionalfeaturesgenerallyimprovedtheperformancemetrics
forallmodels.Moreover,theconsistenttrendofperformanceimprovementsacrossallmetricsforeach
modelindicatestherobustnessofperformancegains.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:19
Model FeatureSet Accuracy Recall Precision F1Score
Mean Î”(%)â†‘ Mean Î”(%)â†‘ Mean Î”(%)â†‘ Mean Î”(%)â†‘
L 0.479 - 0.480 - 0.478 - 0.476 -
F 0.580 10.1 0.580 10.0 0.576 9.8 0.574 9.8
L+F 0.607 12.8 0.607 12.7 0.604 12.6 0.604 12.8
LR
L+F+LS 0.608 12.9 0.608 12.8 0.604 12.6 0.603 12.7
L+F+FS 0.633 15.4 0.633 15.3 0.630 15.2 0.629 15.3
L+F+FS+LS 0.634 15.5 0.634 15.4 0.632 15.4 0.629 15.3
L 0.772 - 0.772 - 0.772 - 0.768 -
F 0.831 5.9 0.831 5.9 0.830 5.8 0.828 6.0
L+F 0.842 7.0 0.842 7.0 0.842 7.0 0.839 7.1
RF
L+F+LS 0.851 7.9 0.852 8.0 0.851 7.9 0.850 8.2
L+F+FS 0.856 8.4 0.857 8.5 0.857 8.5 0.855 8.7
L+F+FS+LS 0.868 9.6 0.870 9.8 0.869 9.7 0.867 9.9
L 0.630 - 0.631 - 0.631 - 0.624 -
F 0.796 16.6 0.796 16.5 0.795 16.4 0.791 16.7
L+F 0.809 17.9 0.809 17.8 0.808 17.7 0.804 18.0
NN
L+F+LS 0.845 21.5 0.846 21.5 0.847 21.6 0.840 21.6
L+F+FS 0.863 23.3 0.864 23.3 0.865 23.4 0.860 23.6
L+F+FS+LS 0.872 24.2 0.872 24.1 0.874 24.3 0.868 24.4
L 0.762 - 0.762 - 0.766 - 0.753 -
F 0.824 6.2 0.823 6.1 0.831 6.5 0.815 6.2
L+F 0.845 8.3 0.844 8.2 0.848 8.2 0.838 8.5
DNN3
L+F+LS 0.858 9.6 0.858 9.6 0.864 9.8 0.852 9.9
L+F+FS 0.868 10.6 0.868 10.6 0.871 10.5 0.863 11.0
L+F+FS+LS 0.872 11.0 0.871 10.9 0.876 11.0 0.866 11.3
Table9. AblationexperimentsforLR,RF,NN,DNN3onternaryclassification.Î”representsthepercentageincreasein
performancemetricscomparedtothebaselinefeatureset(usingonlylefttemporallobefeatures,denotedasâ€™Lâ€™).Lstands
forfeaturesfromthelefttemporallobe,Fforfeaturesfromthefrontallobe,FSforsynchronyfeaturesofthefrontallobe,
andLSforsynchronyfeaturesofthelefttemporallobe.Thesamemodelconfigurationsareusedacrossdifferentfeaturesets,
asdetailedinTable3ofAppendix.
(2) Modelsusingfrontallobefeatures(F)exhibitsignificantimprovementsonallmetricscompared
to which uses using left lobe features(L). The performance enhancements(Î”) across four metrics
rangefrom5.8%to16.7%whichsuggestfeaturesfromfrontallobecarrymoreinformationrelatedtoflow
experience.
(3) Synchrony features in the frontal lobe appear to be more significant than those in the left
temporallobe.ThemodelsexhibitedgreaterperformanceenhancementswiththeinclusionoftheFS
featuresetcomparedtotheLSfeatureset.Thissuggeststhatsynchronyfeaturesfromthefrontallobe
containmorerelevantinformation,contributingtogreaterperformancegainscomparedtothosefromthe
lefttemporallobe.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:20 â€¢ Zhangetal.
(4) Inter-brainsynchronyfeaturesplayedasignificantroleinclassifyingflowstatesduringcoop-
erativetasks.Performanceimprovementswereobservedinnearlyallmodelsfollowingtheadditionof
theLS,FS,andcombinedLS+FSfeaturesets.Thisresult,inconjunctionwiththeresultsofthefeature
importanceexperiments,collectivelyvalidatestheimportanceofinter-brainsynchronyfeatures.
6 DISCUSSION
Thisstudyfocusonthesimultaneousflow,preliminarilyexploringitsfeaturesandthemethodsfordetecting
simultaneousflowexperiencesincollaborationbasedonEEGsignals.Thefollowingdiscussionaddressesseveral
importantquestions.
6.1 ValidityofSimultaneousFlowTasksandDataset
6.1.1 Thetwo-playerWhack-A-Molegameiseffectiveininducingsimultaneousflowexperiences. Experimentson
neuralsynchronyinsimultaneousflowarerare.AndThereisnounifiedtaskparadigmisspecificallydesigned
toinducesimultaneousflow.Ourstudyusedatwo-playerWhack-A-Molegame,whichiscommonlyusedin
individualflowresearch.Thetaskisrule-basedandstraightforward,allowingforquickconcentrationandthe
inductionofflowexperiencesinashorttime.Inaddition,thetaskisrobusttoisolatedandrandomoperator
errorsandiswell-suitedtotraditionalflowevaluationtechniques,suchastheExperienceSamplingMethod(ESM).
Ourresultsindicatethattheexperimentaltaskiseffectiveininducingsimultaneousflowexperiencesinthe
participants.
6.1.2 TheEEGdatasettoclassifysimultaneousflowwasconstructedeffectively. Ratherthanaimingforthehighest
numberofchannels,weselectedthosewithmostpotentialrelevancetoindividualandsimultaneousflow.Based
ontheneuralmechanismsofflow,thefrontallobebrainregionsarehighlyassociatedwithflowexperiences.
Inaddition,thelefttemporallobe,whichisinvolvedinadvancedneuralsynchrony,waspreliminarilyfound
activatedduringflowincollaborativetasks(relevanttosimultaneousflow).Therefore,wefocusedontheEEG
signalsfromthefrontal(AF3,AF4,F3,F4,F7,F8)andlefttemporal(T7,P7)lobestoconstructtheEEGdatasetof
simultaneousflow.
Allpreviousdatasetofflowfocusedonindividualflow,labelingtypesofexperienceinducedbydifferentmatch
ofskillsandchallenges,suchasboredom,flowandanxiety[37][57][66][79].Incontrast,ourstudyclassifiedexpe-
riencesintohighandlowsimultaneousflow,aswellassituationsthatwere"NeitherIndividualnorSimultaneous
Flow","IndividualbutnotSimultaneousFlow",and"SimultaneousFlow".Notonlydidwedelveintotheintensity
ofsimultaneousflow,butalsoconsideredontherelationshipbetweenindividualandsimultaneousflow,andour
resultsshowthatthisEEGdatasetcontainseffectiveflowlabelsandfeaturessuitablefordetectingsimultaneous
flowexperiences.
6.2 EEGFeaturesofSimultaneousFlow
6.2.1 Itisnecessarytofocusthefeaturesthatreflectthesynchronyofneuralactivity. AccordingtoYunetal.[80],
thesynchronyofneuralactivitybetweentheparticipantsincreasedaftercooperativeinteraction.Sinhaetal.[67]
havedemonstratedthatinter-brainsynchronyismorelikelytooccurwhenparticipantsarecooperatingtowards
achievingacommongoal.Simultaneousflowisauniquebrainexperienceassociatedwithenhancedinformation
integrationandneuralsynchrony.Therefore,toassesssimultaneousflow,itisnecessarytofocusnotonlyon
relevantbrainregionsbutalsoonEEGfeaturesthatreflectthesynchronyofneuralactivityacrossthebrain.The
globalinter-brainintegratedinformationandneuralsynchronyareenhancedinthestateofteamflow(relevant
tosimultaneousflow),particularlywithspecificfluctuationsobservedinthelefttemporallobecortex[65].The
individualflowexperienceiscloselyrelatedtotheactivationlevelofthefrontallobe[15].Asscientificknowledge
oftheneuralmechanismsofsimultaneousflowisscarce,ourstudyinitiallyusedthecross-correlationcoefficient
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:21
to measure the signal correlation between participants and Dynamic Time Warping (DTW) to quantify the
similaritybetweentwosignals.Althoughthesefeaturesmaynotfullycaptureallaspectsofinter-brainsynchrony,
theyserveasafirstattempt.
6.2.2 Theinter-brainsynchronyfeaturesweextractediseffective. Tovalidatetheeffectivenessofthesefeatures,
weemployedeightcommonlyusedclassifiersandneuralnetworks.Ourcomparisonexperimentsacrossbinary
andternaryclassificationtasksdemonstratedthattheinclusionofthepreliminaryinter-brainsynchronyfeatures
improvedtheperformanceofalleightmodelssignificantly.Furthermore,synchronyfeatureswereprominently
representedamongthetop-20featuresinfeatureimportanceexperiments,suggestingtheirimportance.Notably,
performanceenhancementswereobservedinnearlyallmodelswiththeadditionoftheLS,FS,andcombined
LS+FSfeaturesets.Theseresultscollectivelyvalidatedtheeffectivenessofinter-brainsynchronyfeaturesin
detectingsimultaneousflow.
6.3 ClassificationofSimultaneousFlow
6.3.1 Thesimultaneousflowcomputationpipelineproposedinthispaperisfeasible. Weemployedaten-fold
cross-validationmethodtoevaluatetheperformanceofLR,SVM,DT,RF,NN,DNN1,DNN2,andDNN3inbinary
andternaryclassification.Theaccuracyofthesemachinelearningmodelssurpassedtheresultsoftherandom
selection(50%,33.3%).Thissuggeststhefeasibilityofthesimultaneousflowcomputationpipelineproposed
inthisstudy,includingdataacquisition,featureextraction,andtheselectionofmachinelearningmodels.We
validatedtheeffectivenessofinter-brainsynchronyfeaturesinsimultaneousflowcomputationforthefirsttime,
offeringanovelperspectiveforthisfield.
6.3.2 RandomForestexhibitedthebestperformanceinbinaryclassificationtask. Inbinaryclassificationtasks,the
RandomForestmodelincludinginter-brainsynchronyfeaturesexhibitedthebestperformancewithanaccuracy
of83.9%,surpassingDNN3by8.3%.Morecomplicatedmodelsmayencountertheproblemofoverfittingwith
limiteduserdata.Ifalargerdatasamplecouldbecollectedastrainingdata,theDNNmodelscouldpotentially
achievebetterperformance.
6.3.3 DTandRFshowedsignificantimprovementsinexperimentswhenincludingsynchronyfeaturesonbinary
classification(ğ‘ğ‘  <0.001). DecisionTreesimprovedby13.9%,whileRandomForestenhancedby8.1%.Neural
network-based models like NN and DNN already perform well without the addition of synchrony features,
but their improvement is not as significant as tree models when these features are included. This might be
becauseneuralnetworksusuallyneedtoencodecategoricalfeaturesintonumericalformstoaccommodate
themathematicaloperationsofthenetwork.Suchtransformationscanleadtoinformationloss,whereastree
modelsinherentlysupporttheprocessingofcategoricalfeatures.Treemodelscreatedecisionrulesbysplitting
data,formingdifferentpathsforeachfeatureâ€™sdistinctvalues.ThisisalsoevidencedinTable7and8,whereRF
focusedmoreonthesynchronyfeatures.
6.3.4 Ternaryclassificationmetricsgenerallysurpassbinaryones. Ternaryclassificationmetricstypicallysur-
passedthoseofbinaryclassification,Thiscouldbeattributedtothemoredistinctivecategoriesintheternary
classificationtask.Intheternaryclassificationtask,thecategoriesaredefinedasfollows:1)NeitherIndividual
norSimultaneousFlow;2)IndividualbutNoSimultaneousFlow;3)SimultaneousFlow.Thesecategoriesoffer
clearerdistinctionscomparedtothehighandlowsimultaneousflowstatesinbinaryclassification.Specifically,
thecategoryof"individualflowbutnosimultaneousflow"cannotbeeffectivelyidentifiedinbinaryclassification,
andmightbemistakenlyclassifiedasloworhighsimultaneousflow.AsshowninTable8,duetotheclearer
categorydistinctions,theternaryclassificationrequiresabalancebetweenindividualandsynchronyfeatures.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:22 â€¢ Zhangetal.
6.3.5 Thefeatureslocatedinthefrontallobearemorepreferredbythemodels. Comparedtothefeaturesofthe
lefttemporallobe(T7,P7),thefeatureslocatedinthefrontallobe(AF3,AF4,F3,F4,F7,F8)aremorepreferredby
themodels.However,thisobservationslightlycontrastswiththetheoryproposedbyShehataetal.[65],which
suggeststhattheactivationofthelefttemporalcortexduringteamflowexperiences.Thisdifferencecanbe
attributedtotaskdesigns:Shehataetal.conductedtheirtasksonthesameiPad,whereasourstudydesigned
differenttasksperformedontwoseparatecomputers.Inourexperiment,bothparticipantsexhibitedhighlevels
ofconcentration,thedegreeofindividualflowisstrongerthanthatofteamflow.Thisalignswiththeopinionof
Bruyaetal.[15]that"individualflowiscloselyrelatedtotheactivationlevelofthefrontallobe."
6.3.6 Fromsimultaneousflowtoteamflow. Simultaneousflowmeansteammembersexperiencingindividual
flowatthesametime.VandenHoutetal.defineteamflowasaconcatenativeexperienceofteammembersâ€™
individualflow.Inourexperiments,bothparticipantsaimedtoachievethecommonteamgoalandoptimize
teamperformance,theyexhibitedhighlevelsofconcentrationinthestateofsimultaneousflow.Wheneach
participantisinindividualflowanddoingtheirowntasksfortheteamperformance,itmightleadtotheshared
teamflowtogether.
6.4 LimitationsandFutureDirections
Thereremainsomelimitationstothisstudythatneedtobeaddressedinfuturestudies.
First,furtherexplorationofsimultaneousflowfeaturesreflectinginter-brainsynchrony.Thisstudyextracted
featuresrelatedtointer-brainsynchronyinteamflow.However,asafirstattempt,thesefeatureswererelatively
simple. Future work could explore a more diverse set of features such as coherence and phase synchrony.
Therefore,ongoingresearchontheneuralmechanismsofsimultaneousflowisrequiredtoprovideamorerobust
neuroscientificbasisforfeatureextraction.
Second,extendingsimultaneousflowinductiontasksanddatasetstolargerteams.Asanexploratorystudy,
thisresearchbeganwiththesmallestscaledyadicteamstoconstructflowtasksandEEGdatasets.Inthefuture,
higherprecisionEEGdeviceswillbeusedtoacquiresignalsanditwillbeimportanttoextendthesetasksand
datasetstolargerteamstoaccommodateawidervarietyofteam-interactionscenarios.
Third,real-timedetectionofsimultaneousflowexperience.ThisstudyfocusedonEEG-basedsimultaneous-flow
detectionmethods.Throughfeatureextractionandtrainingmachinelearningmodels,weachievedthedetection
ofsimultaneousflowexperience.However,thismethoddoesnotcurrentlyprovideareal-timeevaluationofflow
experience.Basedonthisstudy,futureworkcouldbeexpectedtoachieveareal-timedetectionofsimultaneous
flowexperiencesforevaluationandhuman-computerinteractionpurposes.
Finally,experiencingflowsimultaneouslyisfoundationaltoachievingsharedteamflow.Simultaneousflowis
animportantstepbeforefocusingonteamflow.Futureworkwillexplorehowtoinduceanddetectteamflowin
differenttasks,potentiallyenhancingteamperformanceinmulti-usercollaborativesystems.
7 CONCLUSION
ThisstudyexploredmethodsfordetectingsimultaneousflowbasedonEEGsignals.Themainconclusionsofthis
studyareasfollows.
First,wedesignedatwo-usercollaborativeexperimentaltasktoeffectivelyinducesimultaneousflow.Based
onthistask,werecordedtheuserâ€™seight-channelEEGsignals(F3,F4,F7,F8,AF3,AF4,T7,andP7)fromthe
frontallobebrainregions(relatedtoindividualflow)andlefttemporallobebrainregions(relatedtointer-brain
synchronyofteamflow)andthenconstructedthefirsteffectiveEEGdatasetwithmultiplelabelsofsimultaneous
flow.
Second,weextractedspecificfeaturesrelevanttoindividualflowandinter-brainsynchrony,andprovedthat
usingthemcaneffectivelydetectingdifferenttypesofsimultaneousflow.Particularly,Wepreliminarilyproposed
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:23
inter-brain synchrony features relevant to simultaneous flow from EEG signals for the first time, including
Cross-CorrelationCoefficientsandDynamicTimeWarpingDistance.Basedonvariousmachinelearningmodels,
wevalidatedtheireffectivenessinenhancingthemachinelearningmodelsâ€™performanceinbinaryandternary
classificationtasks.
Third,Wecomparedthemetricsondifferentmodelsandconductedathoroughanalysisoftheexperiments.We
rankedtheextractedfeaturesintermsofimportancebyusingLR,RF,NNandDNN3andconductedaablation
study.Basedontheresultsoftheseexperiments,wefoundtheinter-brainsynchronyfeatureshashighimportance
insimultaneousflowdetectionandthefeaturesfromfrontallobeareaaregivenpriorityattentionbythemodels.
REFERENCES
[1] RamiAlazrai,RashaHomoud,HishamAlwanni,andMohammadIDaoud.2018. EEG-basedemotionrecognitionusingquadratic
time-frequencydistribution.Sensors18,8(2018),2739.
[2] KhalilAlSharabi,YasserBinSalamah,AkramMAbdurraqeeb,MajidAljalal,andFahdAAlturki.2022. EEGsignalprocessingfor
Alzheimerâ€™sdisordersusingdiscretewavelettransformandmachinelearningapproaches.IEEEAccess10(2022),89781â€“89797.
[3] OmarAlZoubi,SidneyKDâ€™Mello,andRafaelACalvo.2012.Detectingnaturalisticexpressionsofnonbasicaffectusingphysiological
signals.IEEETransactionsonaffectivecomputing3,3(2012),298â€“310.
[4] ZhongliBai,ZeyuLi,ZhiweiLi,YuSong,QiangGao,andZeminMao.2023.Domain-AdaptiveEmotionRecognitionBasedonHorizontal
VerticalFlowRepresentationofEEGSignals.IEEEAccess11(2023),55023â€“55034. https://doi.org/10.1109/ACCESS.2023.3270977
[5] IleneRBerson,MichaelJBerson,AmyMCarnes,andClaudiaRWiedeman.2018.Excursionintoempathy:exploringprejudicewith
virtualreality.SocialEducation82,2(2018),96â€“100.
[6] RiccardoBerta,FrancescoBellotti,AlessandroDeGloria,DanuPranantha,andCarlottaSchatten.2013. Electroencephalogramand
physiologicalsignalanalysisforassessingflowingames.IEEETransactionsonComputationalIntelligenceandAIinGames5,2(2013),
164â€“175.
[7] YulongBian,ChengleiYang,FengqiangGao,HuiyuLi,ShishengZhou,HanchaoLi,XiaowenSun,andXiangxuMeng.2016.Aframework
forphysiologicalindicatorsofflowinVRgames:constructionandpreliminaryevaluation.PersonalandUbiquitousComputing20(2016),
821â€“832.
[8] YulongBian,ChengleiYang,ChaoZhou,JuanLiu,WeiGai,XiangxuMeng,FengTian,andChiaShen.2018. ExploringtheWeak
AssociationbetweenFlowExperienceandPerformanceinVirtualEnvironments.InProceedingsofthe2018CHIConferenceonHuman
FactorsinComputingSystems(MontrealQC,Canada)(CHIâ€™18).AssociationforComputingMachinery,NewYork,NY,USA,1â€“12.
https://doi.org/10.1145/3173574.3173975
[9] YulongBian,ChaoZhou,JuanLiu,WenxiuGeng,andYingShi.2022.Theeffectofreducingdistractionontheflow-performancelinkin
virtualexperientiallearningenvironment.Virtualreality26,4(2022),1277â€“1290.
[10] MarioBoot,LauraKahnt,DeesPostma,MehmetBaranUlak,KarstGeurs,andPaulHavinga.2024. AreWeinFlow?Measuring
andSupportingSimultaneousFlowinDuosofElderlyCyclists.In2024IEEEInternationalConferenceonPervasiveComputingand
CommunicationsWorkshopsandotherAffiliatedEvents(PerComWorkshops).IEEE,255â€“260.
[11] StÃ©phaneBouchardandAARizzo.2019.Virtualrealityforpsychologicalandneurocognitiveinterventions.Springer.
[12] AysunBozanta,BirgulKutlu,NuketNowlan,andShervinShirmohammadi.2016.Effectsofseriousgamesonperceivedteamcohesiveness
inamulti-uservirtualenvironment.Computersinhumanbehavior59(2016),380â€“388.
[13] LeoBreiman.2001.Randomforests.Machinelearning45(2001),5â€“32.
[14] LeoBreiman.2017.Classificationandregressiontrees.Routledge.
[15] BrianBruya.2010.Effortlessattention:Anewperspectiveinthecognitivescienceofattentionandaction.MITPress.
[16] HenryCandra,MitchellYuwono,RifaiChai,ArdiHandojoseno,IrraivanElamvazuthi,HungTNguyen,andStevenSu.2015.Investigation
ofwindowsizeinclassificationofEEG-emotionsignalwithwaveletentropyandsupportvectormachine.In201537thAnnualinternational
conferenceoftheIEEEEngineeringinMedicineandBiologySociety(EMBC).IEEE,7250â€“7253.
[17] DebatriChatterjee,AniruddhaSinha,MeghamalaSinha,andSanjoyKumarSaha.2016.AProbabilisticApproachforDetectionand
AnalysisofCognitiveFlow..InBMA@UAI.44â€“53.
[18] ImanChatterjee,MajaGorÅ¡iÄ,MohammadSHossain,JoshuaDClapp,andVesnaDNovak.2023.Automatedclassificationofdyadic
conversationscenariosusingautonomicnervoussystemresponses.IEEETransactionsonAffectiveComputing(2023).
[19] NiteshVChawla,KevinWBowyer,LawrenceOHall,andWPhilipKegelmeyer.2002. SMOTE:syntheticminorityover-sampling
technique.Journalofartificialintelligenceresearch16(2002),321â€“357.
[20] ManuelCherep,MikolajKegler,Jean-PhilippeThiran,andPabloMainar.2022.MentalFlowEstimationthroughWearableEEG.In2022
44thAnnualInternationalConferenceoftheIEEEEngineeringinMedicine&BiologySociety(EMBC).IEEE,4672â€“4678.
[21] MihalyCsikszentmihalyi.2000.Beyondboredomandanxiety.Jossey-bass.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:24 â€¢ Zhangetal.
[22] MihalyCsikszentmihalyi.2008.Flow:thepsychologyofoptimalexperience.Flow:thepsychologyofoptimalexperience.
[23] MihalyCsikszentmihalyiandMihalyCsikzentmihaly.1990.Flow:Thepsychologyofoptimalexperience.Vol.1990.Harper&RowNew
York.
[24] MihalyCsikszentmihalyi,ReedLarson,andMihalyCsikszentmihalyi.2014.Theexperiencesamplingmethod.Flowandthefoundations
ofpositivepsychology:ThecollectedworksofMihalyCsikszentmihalyi(2014),21â€“34.
[25] AliDarziandDomenNovak.2021.Automatedaffectclassificationandtaskdifficultyadaptationinacompetitivescenariobasedon
physiologicallinkage:Anexploratorystudy.InternationalJournalofHuman-ComputerStudies153(2021),102673.
[26] EgonDejonckheere,FebeDemeyer,BirteGeusens,MaartenPiot,FrancisTuerlinckx,StijnVerdonck,andMerijnMestdagh.2022.
Assessingthereliabilityofsingle-itemmomentaryaffectivemeasurementsinexperiencesampling.PsychologicalAssessment34,12
(2022),1138.
[27] YueDing,XinHu,ZhenyiXia,Yong-JinLiu,andDanZhang.2018. Inter-brainEEGfeatureextractionandanalysisforcontinuous
implicitemotiontaggingduringvideowatching.IEEETransactionsonAffectiveComputing12,1(2018),92â€“102.
[28] Ruo-NanDuan,Jia-YiZhu,andBao-LiangLu.2013. DifferentialentropyfeatureforEEG-basedemotionclassification.In20136th
InternationalIEEE/EMBSConferenceonNeuralEngineering(NER).IEEE,81â€“84.
[29] ShkurtaGashi,ElenaDiLascio,andSilviaSantini.2018.UsingStudentsâ€™PhysiologicalSynchronytoQuantifytheClassroomEmotional
Climate.InProceedingsofthe2018ACMInternationalJointConferenceand2018InternationalSymposiumonPervasiveandUbiquitous
ComputingandWearableComputers. https://doi.org/10.1145/3267305.3267693
[30] LÃ¡szlÃ³Harmat,Ã–rjandeManzano,TÃ¶resTheorell,LennartHÃ¶gman,HÃ¥kanFischer,andFredrikUllÃ©n.2015.Physiologicalcorrelatesof
theflowexperienceduringcomputergameplaying.InternationalJournalofPsychophysiology97,1(2015),1â€“7.
[31] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Deepresiduallearningforimagerecognition.InProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition.770â€“778.
[32] JavierHernandez,IvanRiobo,AgataRozga,GregoryD.Abowd,andRosalindW.Picard.2014.Usingelectrodermalactivitytorecognize
easeofengagementinchildrenduringsocialinteractions.InProceedingsofthe2014ACMInternationalJointConferenceonPervasiveand
UbiquitousComputing. https://doi.org/10.1145/2632048.2636065
[33] JavierHernandez,IvanRiobo,AgataRozga,GregoryDAbowd,andRosalindWPicard.2014.Usingelectrodermalactivitytorecognize
easeofengagementinchildrenduringsocialinteractions.InProceedingsofthe2014ACMinternationaljointconferenceonpervasiveand
ubiquitouscomputing.307â€“317.
[34] GeoffreyE.Hinton,SimonOsindero,andYee-WhyeTeh.2006.AFastLearningAlgorithmforDeepBeliefNets.NeuralComputation(Jul
2006),1527â€“1554. https://doi.org/10.1162/neco.2006.18.7.1527
[35] SergeyIoffeandChristianSzegedy.2015.Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift.
InInternationalconferenceonmachinelearning.pmlr,448â€“456.
[36] SusanA.JacksonandHerbertW.Marsh.1996.Developmentandvalidationofascaletomeasureoptimalexperience:theFlowState
Scale.JournalofSport&ExercisePsychology18,1(1996),17â€“35.
[37] KenjiKatahira,YoichiYamazaki,ChiakiYamaoka,HiroakiOzaki,SayakaNakagawa,andNorikoNagata.2018.EEGcorrelatesofthe
flowstate:Acombinationofincreasedfrontalthetaandmoderatefrontocentralalpharhythminthementalarithmetictask.Frontiersin
psychology9(2018),300.
[38] StamosKatsigiannisandNaeemRamzan.2018.DREAMER:ADatabaseforEmotionRecognitionThroughEEGandECGSignalsFrom
WirelessLow-costOff-the-ShelfDevices.IEEEJournalofBiomedicalandHealthInformatics(Jan2018),98â€“107. https://doi.org/10.1109/
jbhi.2017.2688239
[39] MarkJ.Keith,GregAnderson,JamesGaskin,andDouglasL.Dean.2018. TeamVideoGamingforTeamBuilding:EffectsonTeam
Performance.AssociationforInformationSystemstransactionsonhuman-computerinteraction10,4(2018),205.
[40] KristianKiili,AnteroLindstedt,AnttiKoskinen,HilmaHalme,ManuelNinaus,andJakeMcMullen.2021.Flowexperienceandsituational
interestingame-basedlearning:Cousinsoridenticaltwins.InternationalJournalofSeriousGames8,3(2021),93â€“114.
[41] DiederikP.KingmaandJimmyBa.2014.Adam:AMethodforStochasticOptimization.arXiv:Learning,arXiv:Learning(Dec2014).
[42] XiangLi,DaweiSong,PengZhang,YazhouZhang,YuexianHou,andBinHu.2018.ExploringEEGfeaturesincross-subjectemotion
recognition.Frontiersinneuroscience12(2018),162.
[43] Yong-JinLiu,MinjingYu,GuozhenZhao,JinjingSong,YanGe,andYuanchunShi.2018.Real-TimeMovie-InducedDiscreteEmotion
RecognitionfromEEGSignals.IEEETransactionsonAffectiveComputing9,4(2018),550â€“562. https://doi.org/10.1109/TAFFC.2017.2660485
[44] ScottMLundbergandSu-InLee.2017.Aunifiedapproachtointerpretingmodelpredictions.Advancesinneuralinformationprocessing
systems30(2017).
[45] MarcoMaier,DanielElsner,ChadlyMarouane,MeikeZehnle,andChristophFuchs.2019.DeepFlow:DetectingOptimalUserExperience
FromPhysiologicalDataUsingDeepNeuralNetworks..InAAMAS.2108â€“2110.
[46] StephaneGMallat.1989.Atheoryformultiresolutionsignaldecomposition:thewaveletrepresentation.IEEEtransactionsonpattern
analysisandmachineintelligence11,7(1989),674â€“693.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:25
[47] TimothyMcMahan,IanParberry,andThomasD.Parsons.2015. EvaluatingPlayerTaskEngagementandArousalUsingElectroen-
cephalography.Procediamanufacturing3(2015),2303â€“2310.
[48] MichalMuszynski,TheodorosKostoulas,PatriziaLombardo,ThierryPun,andGuillaumeChanel.2018.Aesthetichighlightdetectionin
moviesbasedonsynchronizationofspectatorsâ€™reactions.
[49] LennartENackeandCraigALindley.2010.Affectiveludology,flowandimmersioninafirst-personshooter:Measurementofplayer
experience.arXivpreprintarXiv:1004.0248(2010).
[50] VinodNairandGeoffreyEHinton.2010. Rectifiedlinearunitsimproverestrictedboltzmannmachines.InProceedingsofthe27th
internationalconferenceonmachinelearning(ICML-10).807â€“814.
[51] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,Natalia
Gimelshein,LucaAntiga,AlbanDesmaison,AndreasKopf,EdwardYang,ZacharyDeVito,MartinRaison,AlykhanTejani,Sasank
Chilamkurthy,BenoitSteiner,LuFang,JunjieBai,andSoumithChintala.2019.PyTorch:AnImperativeStyle,High-PerformanceDeep
LearningLibrary.InAdvancesinNeuralInformationProcessingSystems32,H.Wallach,H.Larochelle,A.Beygelzimer,F.dâ€™AlchÃ©Buc,
E.Fox,andR.Garnett(Eds.).CurranAssociates,Inc.,8024â€“8035. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-
high-performance-deep-learning-library.pdf
[52] FabianPedregosa,GaÃ«lVaroquaux,AlexandreGramfort,VincentMichel,BertrandThirion,OlivierGrisel,MathieuBlondel,Peter
Prettenhofer,RonWeiss,VincentDubourg,etal.2011. Scikit-learn:MachinelearninginPython. theJournalofmachineLearning
research12(2011),2825â€“2830.
[53] CorinnaPeiferandStefanEngeser.2021.Advancesinflowresearch.Springer.
[54] ArttuPerttula,KristianKiili,AnteroLindstedt,andPauliinaTuomi.2017.Flowexperienceingamebasedlearningâ€“asystematicliterature
review.(2017).
[55] PanagiotisCPetrantonakisandLeontiosJHadjileontiadis.2009.EmotionrecognitionfromEEGusinghigherordercrossings.IEEE
TransactionsoninformationTechnologyinBiomedicine14,2(2009),186â€“197.
[56] TrungDuyPhamandDatTran.2012.EmotionRecognitionUsingtheEmotivEPOCDevice.394â€“399. https://doi.org/10.1007/978-3-642-
34500-5_47
[57] APlotnikov,NStakheika,AlessandroDeGloria,CarlottaSchatten,FrancescoBellotti,RiccardoBerta,CFiorini,andFlavioAnsovini.
2012.Exploitingreal-timeEEGanalysisforassessingflowingames.In2012IEEE12thInternationalConferenceonAdvancedLearning
Technologies.IEEE,688â€“689.
[58] RyanW.Quinn.2003.Nuclearweaponsanddailydeadlines:Theenergyandtensionofflowinknowledgework.(2003).
[59] RaphaelRissler,MarioNadj,MaximilianXilingLi,NicoLoewe,MichaelTKnierim,andAlexanderMaedche.2020.Tobeornottobein
flowatwork:physiologicalclassificationofflowusingmachinelearning.IEEEtransactionsonaffectivecomputing(2020).
[60] GabrielaRodrÃ­guez-Aflecht,TomiJaakkola,NonmanutPongsakdi,MinnaHannula-Sormunen,BoglÃ¡rkaBrezovszky,andErnoLehtinen.
2018.Thedevelopmentofsituationalinterestduringadigitalmathematicsgame.JournalofComputerAssistedLearning34,3(2018),
259â€“268.
[61] MiiaRonimus,JanneKujala,AskoTolvanen,andHeikkiLyytinen.2014.Childrenâ€™sengagementduringdigitalgame-basedlearningof
reading:Theeffectsoftime,rewards,andchallenge.Computers&Education71(2014),237â€“246.
[62] JeromeIRotgansandHenkGSchmidt.2018. Howindividualinterestinfluencessituationalinterestandhowbotharerelatedto
knowledgeacquisition:Amicroanalyticalinvestigation.TheJournalofEducationalResearch111,5(2018),530â€“540.
[63] DavidERumelhart,GeoffreyEHinton,andRonaldJWilliams.1986.Learningrepresentationsbyback-propagatingerrors.nature323,
6088(1986),533â€“536.
[64] R.KeithSawyer.2016.Groupcreativity:musicalperformanceandcollaboration.PsychologyofMusic34,2(2016),148â€“165.
[65] MohammadShehata,MiaoCheng,AngusLeung,NaotsuguTsuchiya,Daw-AnWu,Chia-hueiTseng,ShigekiNakauchi,andShinsuke
Shimojo.2021.Teamflowisauniquebrainstateassociatedwithenhancedinformationintegrationandinterbrainsynchrony.eneuro8,
5(2021).
[66] AniruddhaSinha,RahulGavas,DebatriChatterjee,RajatDas,andArijitSinharay.2015.Dynamicassessmentoflearnersâ€™mentalstate
foranimprovedlearningexperience.In2015IEEEfrontiersineducationconference(FIE).IEEE,1â€“9.
[67] NishantSinha,TomaszMaszczyk,ZhangWanxuan,JonathanTan,andJustinDauwels.2016.EEGhyperscanningstudyofinter-brain
synchronyduringcooperativeandcompetitiveinteraction.In2016IEEEinternationalconferenceonsystems,man,andcybernetics(SMC).
IEEE,004813â€“004818.
[68] NitishSrivastava,GeoffreyHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdinov.2014.Dropout:asimplewaytoprevent
neuralnetworksfromoverfitting.Thejournalofmachinelearningresearch15,1(2014),1929â€“1958.
[69] PenelopeSweetserandPetaWyeth.2005.GameFlow:amodelforevaluatingplayerenjoymentingames.ComputersinEntertainment
(CIE)3,3(2005),3â€“3.
[70] GrantS.TaylorandChristinaSchmidt.2012.EmpiricalEvaluationoftheEmotivEPOCBCIHeadsetfortheDetectionofMentalActions.
ProceedingsoftheHumanFactorsandErgonomicsSocietyAnnualMeeting(Sep2012),193â€“197. https://doi.org/10.1177/1071181312561017
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:26 â€¢ Zhangetal.
[71] J.J.JVan,denHout,O.CDavis,andBWalrave.2016. Theapplicationofteamflowtheory. FlowExperienceEmpiricalResearch&
Applications(2016).
[72] JefJJvandenHout,OrinCDavis,andMathieuCDPWeggeman.2018.Theconceptualizationofteamflow.TheJournalofpsychology
152,6(2018),388â€“423.
[73] JefJ.JvandenHoutandOrinC.Davis.2019.TeamFlow:ThePsychologyofOptimalCollaboration.SpringerInternationalPublishing
AG,Cham.
[74] KevinJVerdiÃ¨re,FrÃ©dÃ©ricDehais,andRaphaÃ«lleNRoy.2019. SpectralEEG-basedclassificationforoperatordyadsâ€™workloadand
cooperationlevelestimation.In2019IEEEInternationalConferenceonSystems,ManandCybernetics(SMC).IEEE,3919â€“3924.
[75] KalyaniPWaghandKVasanth.2022.Performanceevaluationofmulti-channelelectroencephalogramsignal(EEG)basedtimefrequency
analysisforhumanemotionrecognition.BiomedicalSignalProcessingandControl78(2022),103966.
[76] Chih-ChienWangandMing-ChangHsu.2014.Anexploratorystudyusinginexpensiveelectroencephalography(EEG)tounderstand
flowexperienceincomputer-basedinstruction.Information&Management51,7(2014),912â€“923.
[77] PeterWelch.1967.TheuseoffastFouriertransformfortheestimationofpowerspectra:amethodbasedontimeaveragingovershort,
modifiedperiodograms.IEEETransactionsonaudioandelectroacoustics15,2(1967),70â€“73.
[78] PaulT.P.Wong.2012.Flourishing:AVisionaryNewUnderstandingofHappinessandWell-Being,ByMartinE.P.Seligman.1(2012).
[79] XiaozhenYe,HuanshengNing,PerBacklund,andJianguoDing.2020. Flowexperiencedetectionandanalysisforgameusersby
wearable-devices-basedphysiologicalresponsescapture.IEEEInternetofThingsJournal8,3(2020),1373â€“1387.
[80] KyongsikYun,KatsumiWatanabe,andShinsukeShimojo.2012.Interpersonalbodyandneuralsynchronizationasamarkerofimplicit
socialinteraction.ScientificReports(Dec2012). https://doi.org/10.1038/srep00959
[81] GuanhuaZhang,MJYu,GuoChen,YihengHan,DanZhang,GZZhao,andYong-JinLiu.2019.AreviewofEEGfeaturesforemotion
recognition.Scientiasinicainformationis49,9(2019),1097â€“1118.
[82] GuanhuaZhang,MinjingYu,Yong-JinLiu,GuozhenZhao,DanZhang,andWenmingZheng.2023.SparseDGCNN:RecognizingEmotion
FromMultichannelEEGSignals.IEEETransactionsonAffectiveComputing14,1(2023),537â€“548. https://doi.org/10.1109/TAFFC.2021.
3051332
A APPENDIX
A.1 Metrics
Metrics Description Definition
BinaryClassificationMetrics
Accuracy The proportion of true results among Accuracy= ğ‘‡ğ‘ƒ+ğ‘‡ğ‘
ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘
thetotalnumberofcasesexamined.
Precision The proportion of positive identifica- Precision= ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ
tionsthatwereactuallycorrect.
Recall Theproportionofactualpositivesthat Recall= ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘
werecorrectlyidentified.
F1Score Theharmonicmeanofprecisionandre- F1Score=2Ã— ğ‘ƒğ‘Ÿğ‘’Ã—ğ‘…ğ‘’ğ‘
ğ‘ƒğ‘Ÿğ‘’+ğ‘…ğ‘’ğ‘
call,providingabalancebetweenthem.
TernaryClassificationMetrics
Accuracy The proportion of true results for a Accuracy=
(cid:205) ğ‘–3 =1ğ‘‡ğ‘ƒğ‘–+(cid:205) ğ‘–3 =1ğ‘‡ğ‘ğ‘–
ğ‘
three-classclassificationproblem.
(cid:16) (cid:17)
Precision Theaverageproportionofcorrectposi- Precision= 1 ğ‘‡ğ‘ƒ 1 + ğ‘‡ğ‘ƒ 2 + ğ‘‡ğ‘ƒ 3
3 ğ‘‡ğ‘ƒ 1+ğ¹ğ‘ƒ 1 ğ‘‡ğ‘ƒ 2+ğ¹ğ‘ƒ 2 ğ‘‡ğ‘ƒ 3+ğ¹ğ‘ƒ 3
tivepredictionsforeachclass.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:27
Metrics Description Definition
(cid:16) (cid:17)
Recall The average proportion of correctly Recall= 1 ğ‘‡ğ‘ƒ 1 + ğ‘‡ğ‘ƒ 2 + ğ‘‡ğ‘ƒ 3
3 ğ‘‡ğ‘ƒ 1+ğ¹ğ‘ 1 ğ‘‡ğ‘ƒ 2+ğ¹ğ‘ 2 ğ‘‡ğ‘ƒ 3+ğ¹ğ‘ 3
identifiedpositivesforeachclass.
F1Score The harmonic mean of precision and F1Score=2Ã— ğ‘ƒğ‘Ÿğ‘’Ã—ğ‘…ğ‘’ğ‘
ğ‘ƒğ‘Ÿğ‘’+ğ‘…ğ‘’ğ‘
recallforternaryclassification.
Table1. EvaluationMetricsandCalculationMethods
ğ‘‡ğ‘ƒ isthenumberoftruepositives.ğ¹ğ‘ƒ isthenumberoffalsepositives.ğ‘‡ğ‘ isthenumberoftruenegatives.ğ¹ğ‘
isthenumberoffalsenegatives.
A.2 Features
FeatureCategory Description Definition
Mean ThemeanasafeatureoftheEEG ğœ‡ ğ‘  = ğ‘‡1 (cid:205)ğ‘‡ ğ‘›=1ğ‘ (ğ‘›)
signal reflects the average level
of electrical potential over a pe-
riodoftime,revealingthebaseline
stateofbrainactivity.
âˆšï¸ƒ
StandardDeviation The standard deviation as a fea- ğœ ğ‘  = ğ‘‡1 (cid:205)ğ‘‡ ğ‘›=1(ğ‘ (ğ‘›)âˆ’ğœ‡ ğ‘ )2
ture of the EEG signal indicates
the dispersion of signal values,
quantifyingthevariabilityorsta-
bilityofbrainactivity.
Variance[81] Thevariancerepresentstherange Varğ‘  = ğ‘‡1 (cid:205)ğ‘‡ ğ‘›=1(ğ‘ (ğ‘›)âˆ’ğœ‡ ğ‘ )2
of fluctuation of the EEG signal
andalsothepowerofthesignal
todeviatefromthemeanvalue.
AverageAbsolute Theaverageabsolutevalueofthe ğ›¿ ğ‘  = ğ‘‡1 âˆ’1(cid:205)ğ‘‡ ğ‘›=âˆ’ 11|ğ‘ (ğ‘›+1)âˆ’ğ‘ (ğ‘›)|
First-OrderDifference[81] first-orderdifferencemeasuresthe
rate of change of the EEG sig-
nalovertime,helpingtocapture
short-termfluctuationswithinthe
signal.
NormalizedFirst-OrderDifference[81] Thenormalizedfirst-orderdiffer- ğ›¿ ğ‘  = ğœğ›¿ğ‘ 
ğ‘ 
ence eliminates the influence of
signal strength, making the fea-
turesmorecomparableunderdif-
ferentconditions.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:28 â€¢ Zhangetal.
FeatureCategory Description Definition
Energy[33] Fluctuationsinenergycanreflect ğ¸ ğ‘  =(cid:205)ğ‘‡ ğ‘›=1ğ‘ (ğ‘›)2
thelevelofactivityinthecerebral
cortex, often represented by the
squareofthetime-domainsignal.
Power[81] The power of the EEG signal is ğ‘ƒ ğ‘  = ğ‘‡1 (cid:205)ğ‘‡ ğ‘›=1ğ‘ (ğ‘›)2
obtained by dividing the energy
bythenumberofsamples.
HjorthParameters[66] Activitymeasuresthedeviationof Activity=ğœ ğ‘ 2
theamplitude,Mobilitymeasures Mobility=
ğœğ‘“
ğœğ‘ 
thechangeinslope.
Higher-OrderZero-Crossing[55,81] Higher-order zero-crossing re- By using the mean ğœ‡ ğ‘ , transform the
flects the oscillation level of the EEGsignalintoazero-meansequence
EEGsignal,indicatingthefluctua- ğ‘  new(ğ‘¡) =ğ‘ (ğ‘¡)âˆ’ğœ‡ ğ‘ .
tionfeatures. ğ» ğ‘  =(cid:205)ğ‘‡ ğ‘›=âˆ’ 11(ğ‘¥ ğ‘ (ğ‘¡ +1)âˆ’ğ‘¥ ğ‘ (ğ‘¡))2
Peak-to-PeakMean[42] Peak-to-peak mean is the arith- ppm =
maxğ‘¡ğ‘–âˆˆğ‘¡ğ‘ (ğ‘¡ğ‘–)âˆ’minğ‘¡ğ‘–âˆˆğ‘¡ğ‘ (ğ‘¡ğ‘–)
ğ‘  ğ‘‡
metic mean of the distance be-
tween the maximum and mini-
mum values in a time-series sig-
nal.
Kurtosis[2,75] Kurtosisreflectsthefrequencyof Kurğ‘  =
ğ¸(ğ‘ (ğ‘¡ ğœ) ğ‘ 4âˆ’ğœ‡ğ‘ )4
peakvaluesappearingaswellas
thewaveformfeaturesofthesig-
nal.
(cid:16) (cid:17)
LogarithmicBandPower[4] Thelogarithmicbandpowercon- LBPğ‘  =log ğ‘‡1 (cid:205)ğ‘‡ ğ‘›=1ğ‘ (ğ‘›)2
vertsthefrequencyinformationof
EEGsignalsintoanenergydistri-
butiononalogarithmicscale.
DifferentialEntropy[4,28] Differentialentropyisoftenused DEğ‘  = 1 2log(cid:0) 2ğœ‹ğ‘’ğœ ğ‘ 2(cid:1)
to represent the complexity of a
timeseries.
PowerSpectralDensity[1,4] The power spectral density de- PSDğ‘  = 1 2log(cid:0) 2ğœ‹ğ‘’ğœ ğ‘ 2(cid:1)
scribeshowthepowerofasignal
varieswithfrequency.
Cross-CorrelationCoefficient TheCross-CorrelationCoefficient ğœŒ ğ´,ğµ = co ğœv ğ´(ğ´ ğœğµ,ğµ)
isusedtomeasurethesimilarity
betweentwotimeseriesatdiffer-
enttimelags
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:29
FeatureCategory Description Definition
DynamicTimeWarpingDistance TheDynamicTimeWarpingDis- ğ·(ğ‘ ğ‘–,ğ‘ ğ‘—) = Dist(ğ‘ ğ‘–,ğ‘ ğ‘—) +
tanceisusedtomeasurethesimi- min{ğ·(ğ‘ ğ‘–âˆ’1,ğ‘ ğ‘—),ğ·(ğ‘ ğ‘–,ğ‘ ğ‘—âˆ’1),ğ·(ğ‘ ğ‘–âˆ’1,ğ‘ ğ‘—âˆ’1)}
laritybetweentwotimeseries.
Table2. FeaturesExtractedforIndividualandTeamFlow
A.3 DetailsofImplementation
Model Description Configuration
LogisticRegression(LR) LogisticRegressionusesalinearfunc- Feature standardization was
tion of input featuresğ‘¥ to predict the performed using the Standard-
labelğ‘¦.Thelabelğ‘¦canbeeitherbinary Scaler parameter, followed by
ormulti-class.Inbinaryclassification, the sklearnâ€™s LogisticRegression
the model predicts the probability of model. The maximum number
the label belonging to one of two cat- of iterations, max_iter, was
egories.Thisisusuallyrepresentedas set to 1000. For the ternary
ğ‘¦Ë† = 1 ,whereğ‘’ isthebaseofthe classification task, we speci-
1+ğ‘’âˆ’ğ‘¤ğ‘‡ğ‘¥
naturallogarithm,andğ‘¤ representsthe fied multi_class=â€™multinomialâ€™,
modelparameters.Inmulti-classscenar- enabling the model to handle
ios,logisticregressiontypicallyusesthe multi-classproblems.
softmaxfunctiontohandlemultiplecat-
egories and predict the probability of
each category. The goal of the model
is to estimate parameters ğ‘¤ by maxi-
mizingthelikelihoodfunction,usually
achievedbyminimizingtheloglossof
thetrainingdata.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:30 â€¢ Zhangetal.
Model Description Configuration
SupportVectorMachine(SVM) SVM classification models predict the The SVM model is configured
labelğ‘¦Ë†=ğ‘¤ğ‘‡ğ‘¥+ğ‘bysolvingthefollow-
with C=1.0, kernel=â€™rbfâ€™, and
ingoptimizationproblem:minğ‘¤ 1âˆ¥ğ‘¤âˆ¥2 gamma=â€™scaleâ€™. The C parame-
2
subject to the classification ter controls the penalty for mis-
constraint:ğ‘¦ (ğ‘¤ğ‘‡ğ‘¥ +ğ‘) â‰¥ 1âˆ’ğœ‰ classification, and kernel speci-
train train
whereğ‘¥ representstheinputfeatures, fies the kernel type. The value
ğ‘¤ is the weight vector, ğ‘ is the bias â€™scaleâ€™ means that gamma is set
term,andğœ‰ areslackvariablesallowing to 1 ,whereXisthe
ğ‘›ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ âˆ—ğ‘‹.ğ‘£ğ‘ğ‘Ÿ()
for some data points to violate the feature data. This setting takes
margin.Forbinaryclassificationtasks, into account the variance of fea-
the model output ğ‘¦Ë† is based on the tures and automatically adjusts
sign function determining to which thegammavalue.
category a data point belongs. In
ternaryormulti-classscenarios,SVM
can be extended to multi-category
classification by using one-vs-rest or
one-vs-onestrategies.
DecisionTree(DT) The Decision Tree model we used is The Decision Tree model is con-
basedonClassficationandRegression figuredwithmin_samples_split=4.
Tree(CART)[14].Abinarytreeiscon- Themin_samples_splitparameter
structed by iteratively splitting data specifiestheminimumnumberof
basedonfeaturethresholdsthatmaxi- samplesrequiredtosplitaninter-
mizeclassseparation. nalnode.Thesplittingstopswhen
thenumberofsamplesislessthan
min_samples_split.
RandomForest(RF) Random Forest[13] is an ensemble of TheRandomForestmodeliscon-
Decision Trees. It trains each tree on figured with max_features=0.3,
a random subset of the training data bootstrap=ture,max_samples=0.8
and uses a random subset of features and min_samples_leaf=3. The
foreachsplit.Thefinalpredictionisthe max_featuresparameterspecifies
averagepredictionofalltrees. thenumberoffeaturestoconsider
when looking for the best split.
Thebootstrapparameterspecifies
whether bootstrap samples
are used when building trees.
The max_samples parameter
specifies the number of samples
todrawfromXtotraineachbase
estimator.Themin_samples_leaf
parameterspecifiestheminimum
numberofsamplesrequiredtobe
ataleafnode.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:31
Model Description Configuration
NeuralNetwork(NN)(1-hidden) NeuralNetwork[63]isamodelinspired The NN includes a single hid-
by the biological neural networks. It den layer with 100 neurons, us-
consistsofmultiplelayersofneurons. ing ReLU function[50]. Batch
Each neuron is a linear function of normalization[35] is applied to
the input features followed by a non- the inputs of each layer, and
linearactivationfunction.Theoutput dropout[68]issetatarateof0.1.
ofthenetworkistheoutputofthelast The network is optimized using
layer.The output layer consists of an the Adam optimizer[41] with a
affinetransformtopredicttheflowex- learningrateof0.001,anditem-
perience. ploys cross-entropy loss as the
lossfunction.Wesetthebatchsize
to64for10-foldscrossvalidation.
Deep Neural Network1 (DNN1) (5- TheDNNreferstoNeuralNetworkwith In DNN1, we use 5 hidden lay-
hidden) multiplehiddenlayers[34]. ers where each layer consists of
abatchnormalizationlayer,ReLU
activationlayer,anddropoutlayer.
The number of neurons in each
layeris100.Otherconfigurations
arethesameasNN.
DNN2(5-hidden+residual) In Deep Neural Networks, a residual In DNN2, we employ a residual
connection[31]createsashortcutbydi- connection to connect the out-
rectlylinkingtwolayers,therebypro- putofthefirstandfourthlayers.
vidingashorterpathbetweenthem. Otherconfigurationsarethesame
asDNN1.
DNN3(9-hidden+residual) TheDNN3isconfiguredwith9hidden The DNN3 model is configured
layers with9hiddenlayerswithresidual
connectionsthatlinktheoutputof
thefirsthiddenlayertothefourth
andtheoutputofthefifthhidden
layertotheeighth.Otherconfigu-
rationsarethesameasDNN2.
Table3. Machinelearningmodelsandconfigurations
A.4 T-test
A.5 FeatureImportance
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:32 â€¢ Zhangetal.
Model PValueforAccuracy PValueforRecall PValueforPrecision PValueforF1Score
LR 0.022 0.024 0.021 0.032
SVM 0.034 0.041 0.036 0.034
DT 1.50Ã—10âˆ’6 1.50Ã—10âˆ’6 2.15Ã—10âˆ’6 1.36Ã—10âˆ’6
RF 4.98Ã—10âˆ’6 7.70Ã—10âˆ’6 6.42Ã—10âˆ’6 5.84Ã—10âˆ’6
NN 0.003 0.007 0.008 0.004
DNN1 0.021 0.020 0.024 0.015
DNN2 0.042 0.078 0.014 0.087
DNN3 0.023 0.157 0.042 0.175
Table4. BinaryClassificationComparisonofPValuesforDifferentModels
Model PValueforAccuracy PValueforRecall PValueforPrecision PValueforF1Score
LR 0.048 0.045 0.041 0.070
SVM 0.018 0.026 0.031 0.011
DT 0.003 0.002 0.002 0.003
RF 0.004 0.003 0.004 0.003
NN 6.28ğ‘’Ã—10âˆ’8 2.68ğ‘’Ã—10âˆ’9 1.31ğ‘’Ã—10âˆ’8 4.97ğ‘’Ã—10âˆ’8
DNN1 0.006 8.27ğ‘’Ã—1048 0.002 0.003
DNN2 0.043 0.048 0.010 0.053
DNN3 0.004 0.011 0.005 0.009
Table5. TernaryClassificationComparisonofPValuesforDifferentModels
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:33
2Class 2ClassSyn 3Class 3ClassSyn
Top-20features COEF Top-20features COEF Top-20features COEF Top-20features COEF
P7NFOD -1.78 P7NFOD -1.73 P7SD 1.82 P7SD 1.76
P7Mobility 1.27 F7PSDğ›¼ 1.25 AF4Mobility -1.28 AF4DTWğ›¼ -1.48
F7PSDğ›¼ 1.25 AF4DTWğ›¼ 1.13 P7Mobility -1.21 AF4Mobility -1.14
F3NFOD -1.22 F3NFOD -1.09 P7NFOD 1.09 F4DEğ›¿ 1.10
AF3Mobility 1.06 P7Mobility 1.03 F4DEğ›¿ 1.08 P7Mobility -1.05
F3Mobility 1.02 AF3Mobility 1.03 F8SD 1.01 AF3DTWğœƒ 0.98
T7PSDğœƒ 0.89 AF3DTWğœƒ -0.99 F7NFOD 0.95 F7NFOD 0.98
T7Mobility 0.79 F8DTWğœƒ -0.94 P7Variance -0.94 F8SD 0.93
P7Variance 0.78 T7PSDğœƒ 0.93 P7Activity -0.94 P7Variance -0.91
P7Activity 0.78 F8DTWğ›¼ -0.91 F4Mobility 0.87 P7Activity -0.91
AF3SD -0.77 F4DTWğ›¼ -0.88 F8DEFB -0.87 T7DEFB -0.90
AF4Mobility 0.76 F3DTWğ›½ -0.88 T7PPM 0.85 AF4DEFB -0.90
T7SD -0.75 AF3DTWğ›¼ 0.88 F7Mobility -0.85 P7NFOD 0.87
F7NFOD -0.74 T7SD -0.83 F7SD 0.84 F8DEFB -0.86
P7DEFB -0.70 T7Mobility 0.82 AF4DEFB -0.75 AF4PPM 0.85
T7PSDğ›¼ -0.66 AF3DTWğ›¿ 0.82 F8PSDğ›½ 0.75 T7DTWğ›¿ -0.83
F8PSDğ›½ -0.60 F7DTWğ›¿ -0.81 F4SD -0.74 F7Mobility -0.81
F4AAFOD 0.60 F4DTWğ›½ -0.80 P7DEğ›¿ -0.74 F8PSDğ›½ 0.79
AF3NFOD -0.56 F3Mobility 0.79 T7DEFB -0.72 F4DTWğ›¼ 0.79
F8average 0.56 T7PSDğ›¼ -0.77 AF4PPM 0.72 AF4DEğ›¿ -0.76
Table6. Thetop-20importantfeaturesselectedbysortingtheabsolutesignificantcoefficient(COEF)valuesofLR.Features
inboldrepresentinter-brainsynchronyfeatures,andSynrepresentsthemodeltrainedondatasetincludinginter-brain
synchronyfeatures.Thefullnamesofthefeaturesaredetailedinsection4.2.1.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:34 â€¢ Zhangetal.
2Class 2ClassSyn 3Class 3ClassSyn
Top-20features IMP(%) Top-20features IMP(%) Top-20features IMP(%) Top-20features IMP(%)
F4LBPğ›¿ 1.30 P7DTWğ›¼ 3.13 F4PSDğœƒ 1.10 F4PSDğœƒ 1.38
F4MeanPSD 1.20 AF4CCCğ›¼ 2.01 P7DEğœƒ 1.10 P7DTWğ›¼ 1.17
AF4Kurtosis 1.15 F7CCCğ›½ 1.90 F4Energy 1.03 F4average 1.12
P7DEğ›¼ 1.13 F4MeanPSD 1.17 F4Power 0.95 AF3Kurtosis 1.01
F8Kurtosis 1.09 F3CCCğ›¼ 1.07 F4average 0.94 AF3PSDğ›½ 1.01
P7LBPğœƒ 1.09 F3DTWğœƒ 1.06 F4LBPğ›¿ 0.90 P7DEğœƒ 0.94
P7DEğœƒ 1.07 F7DTWğ›¿ 1.02 F4MeanPSD 0.90 F8PPM 0.92
F7PSDğ›¼ 1.03 F3DTWğ›¼ 1.01 AF3Kurtosis 0.90 F4LBPğ›¿ 0.88
P7LBPğ›¼ 1.01 AF3CCCğ›¼ 1.01 F8PPM 0.87 P7LBPğœƒ 0.79
F4average 0.99 P7CCCğ›¼ 0.99 AF3PSDğ›½ 0.83 F4Energy 0.78
T7PSDğœƒ 0.95 AF4CCCğ›½ 0.98 P7LBPğœƒ 0.80 F8Kurtosis 0.76
T7Kurtosis 0.93 F8CCCğ›¼ 0.97 F4LBPğ›¼ 0.80 F4Power 0.75
F4Energy 0.92 F8CCCğ›½ 0.90 P7Kurtosis 0.76 P7CCCğ›½ 0.70
F8PSDğ›½ 0.90 F8CCCğ›¿ 0.87 AF3LBPğ›½ 0.75 AF4CCCğ›¼ 0.66
F4Power 0.90 F4CCCğ›¼ 0.87 AF3AAFOD 0.75 F8CCCğ›¼ 0.65
P7Kurtosis 0.90 AF4CCCğœƒ 0.84 F4DEğ›¼ 0.75 F7PSDğ›½ 0.65
F4PSDğœƒ 0.88 F7CCCğ›¿ 0.81 F7LBPğ›¼ 0.74 F4LBPğ›¼ 0.64
F4PSDğ›¿ 0.85 F4average 0.79 AF3DEğ›½ 0.72 T7DTWğœƒ 0.62
T7HOZC 0.80 F4PSDğ›¿ 0.78 AF4DEğ›½ 0.69 F4DEğ›¼ 0.62
AF4PSDğ›¼ 0.80 P7CCCğœƒ 0.77 F8Kurtosis 0.69 T7DTWğ›½ 0.61
Table7. Thetop-20importantfeaturesselectedbysortingtheabsolutesignificantimportance(IMP)valuesofRF.Features
inboldrepresentinter-brainsynchronyfeatures,andSynrepresentsthemodeltrainedondatasetincludinginter-brain
synchronyfeatures.Thefullnamesofthefeaturesaredetailedinsection4.2.1.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.AreWeinTheZone?ExploringTheFeaturesandMethodofDetectingSimultaneousFlowExperiencesBasedonEEGSignals â€¢ 0:35
2Class 2ClassSyn 3Class 3ClassSyn
Top-20features SHAP Top-20features SHAP Top-20features SHAP Top-20features SHAP
F7PSDğ›¼ 2.89 F7PSDğ›¼ 5.34 AF3AAFOD 4.88 P7PSDğ›½ -7.72
AF3Mobility -2.55 F8CCCğ›¿ -4.72 F7PSDğ›¼ -4.26 F7PSDğ›½ -4.68
T7Activity -2.46 F7PSDğœƒ 4.63 AF3PSDğ›½ 3.94 F8DTWğ›¿ 4.63
F7meanPSD -2.23 AF3DTWğ›¿ -4.14 AF3DEğ›½ -3.64 P7Kurtosis -4.53
F8Activity 2.20 F8CCCğ›¼ -3.50 F4AAFOD -3.11 F8PSDğœƒ 4.09
P7DEFB -2.00 F4LBPğ›¿ 3.44 F8AAFOD -3.11 AF4PSDğ›¿ -4.08
F4DEğ›¼ -1.67 F4AAFOD -3.38 F3PSDğ›¼ 3.06 AF3LBPğ›½ -3.97
F7PSDğœƒ 1.62 F3DTWğ›¿ 3.25 P7AAFOD 2.92 AF3PSDğ›½ -3.87
T7Kurtosis -1.62 P7Variance -3.15 AF3Kurtosis 2.91 AF4AAFOD 3.86
T7Mobility -1.52 F4DTWğœƒ -3.08 F7meanPSD -2.86 F4DEğ›¿ -3.81
P7AAFOD 1.47 F3CCCğœƒ -2.84 F7PSDğ›½ -2.73 P7HOZC -3.77
F3HOZC -1.33 AF3PPM 2.74 T7DEğ›¿ 2.61 F4DTWğ›¿ -3.76
F8PSDğ›¼ -1.19 P7DTWğ›½ 2.70 AF4LBPğ›¼ -2.47 F3LBPğœƒ 3.69
T7PSDğ›½ 1.17 T7PSDğ›¼ -2.65 AF3PSDğ›¼ -2.41 T7PSDğœƒ 3.63
F8Variance 1.11 F4LBPğœƒ -2.64 F4LBPğ›¼ -2.15 T7DEFB -3.50
AF4Variance 1.10 P7PSDğ›½ -2.61 F8DEğ›½ 2.03 F3DEğœƒ 3.39
F3NFOD -1.07 F8DTWğœƒ 2.57 F4Energy 1.97 T7PSDğ›¼ 3.26
F7PSDğ›½ 1.01 F8PPM 2.51 F3DEğ›½ 1.95 AF3DTWğ›¼ 3.20
F8PSDğ›½ 1.01 F4Energy -2.48 F7PSDğœƒ 1.93 P7PSDğ›¼ 3.12
AF4Mobility 1.00 P7PPM -2.40 AF4PPM -1.89 F7CCCğ›¿ -3.06
Table8. Thetop-20importantfeaturesselectedbysortingtheabsoluteSHapleyAdditiveexPlanations(SHAP)valuesof
NN.Featuresinboldrepresentinter-brainsynchronyfeatures,andSynrepresentsthemodeltrainedondatasetincluding
inter-brainsynchronyfeatures.Thefullnamesofthefeaturesaredetailedinsection4.2.1.Inbinaryclassificationtasks,the
unitofSHAPvalueis10âˆ’9,andinternaryclassificationtasks,theunitofSHAPvalueis10âˆ’8.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:36 â€¢ Zhangetal.
2Class 2ClassSyn 3Class 3ClassSyn
Top-20features SHAP Top-20features SHAP Top-20features SHAP Top-20features SHAP
P7DEFB 2.21 F7CCCğ›¿ 1.14 T7Kurtosis -2.13 AF4CCCğœƒ -2.27
F4PSDğ›¿ 1.92 AF4HOZC -1.12 AF4LBPğ›¼ -1.90 P7Kurtosis -2.22
P7DEğ›½ -1.82 F3CCCğœƒ 1.08 T7HOZC 1.81 AF3Kurtosis -2.17
F3LBPğœƒ -1.81 T7DEğ›¿ 1.06 AF4DEğ›¼ -1.74 T7LBPğ›½ -1.63
F4LBPğ›½ 1.50 AF4CCCğ›½ 0.98 F7AAFOD -1.57 F4CCCğœƒ -1.43
F3Activity 1.48 T7PSDğ›½ -0.98 T7Mobility -1.51 F7DEğ›½ 1.41
F7Kurtosis 1.43 F4LBPğ›½ 0.95 F3LBPğ›½ 1.50 F7DEFB -1.39
F3Variance 1.41 F8CCCğ›¿ 0.93 F7DEğ›¼ -1.40 F7Kurtosis -1.38
F4meanPSD 1.37 F7CCCğ›¼ -0.91 F7LBPğ›¼ -1.37 F4LBPğ›½ -1.30
P7PPM -1.35 AF4Energy -0.90 F7HOZC -1.35 AF3DEğœƒ 1.26
F4LBPğ›¼ -1.29 F4HOZC -0.88 T7DEğœƒ 1.27 F3NOFD -1.25
F3Energy 1.23 T7AAFOD 0.87 T7DEğ›½ 1.23 T7CCCğœƒ -1.24
F8DEğ›¼ -1.21 AF3DEFB -0.85 F3PSDğ›¿ -1.18 P7DEFB 1.19
AF4Energy -1.20 F3DEğœƒ -0.74 F8HOZC 1.18 F4LBPğœƒ -1.16
F3PSDğ›¿ 1.15 AF3CCCğœƒ -0.74 F4meanPSD -1.16 T7CCCğ›¿ 1.12
AF4DEğ›¿ 1.15 AF3PSDğ›¿ -0.74 P7Kurtosis 1.14 F3CCCğ›¼ -1.11
F3DEFB -1.14 F7DEğ›¼ -0.72 AF3HOZC -1.12 F4DTWğ›¿ -1.10
F3Mobility 1.11 T7Mobility -0.69 P7LBPğœƒ -1.08 F7CCCğ›¿ -1.08
AF3SD -1.10 AF3CCCğ›½ 0.67 F3DEğ›¼ -1.07 T7LBPğœƒ 1.07
F7PPM -1.09 F8Mobility -0.65 F3PSDğ›½ -1.06 T7AAFOD -1.06
Table9. Thetop-20importantfeaturesselectedbysortingtheabsoluteSHapleyAdditiveexPlanations(SHAP)valuesof
DNN3.Featuresinboldrepresentinter-brainsynchronyfeatures,andSynrepresentsthemodeltrainedondatasetincluding
inter-brainsynchronyfeatures.Thefullnamesofthefeaturesaredetailedinsection4.2.1.TheunitofSHAPvalueis10âˆ’8
bothinbinaryandternaryclassificationtasks.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.