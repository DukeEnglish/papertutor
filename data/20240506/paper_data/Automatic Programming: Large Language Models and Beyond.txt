AutomaticProgramming:LargeLanguageModelsandBeyond
MICHAELR.LYU, ChineseUniversityofHongKong,HongKong,China
BAISHAKHIRAY, ColumbiaUniversity,USA
ABHIKROYCHOUDHURY, NationalUniversityofSingapore,[CorrespondingAuthor],Singapore
SHINHWEITAN, ConcordiaUniversity,Canada
PATANAMONTHONGTANUNAM, UniversityofMelbourne,Australia
AutomaticprogramminghasseenincreasingpopularityduetotheemergenceoftoolslikeGitHubCopilotwhichrelyonLarge
LanguageModels(LLMs).Atthesametime,automaticallygeneratedcodefaceschallengesduringdeploymentduetoconcerns
aroundqualityandtrust.Inthisarticle,westudyautomatedcodinginageneralsenseandstudytheconcernsaroundcodequality,
securityandrelatedissuesofprogrammerresponsibility.Thesearekeyissuesfororganizationswhiledecidingontheusageof
automaticallygeneratedcode.Wediscusshowadvancesinsoftwareengineeringsuchasprogramrepairandanalysiscanenable
automaticprogramming.Weconcludewithaforwardlookingview,focusingontheprogrammingenvironmentofthenearfuture,
whereprogrammersmayneedtoswitchtodifferentrolestofullyutilizethepowerofautomaticprogramming.Automatedrepairof
automaticallygeneratedprogramsfromLLMs,canhelpproducehigherassurancecodefromLLMs,alongwithevidenceofassurance.
1 CHALLENGESINAUTOMATICPROGRAMMING
Thetaskofprogrammingbothintermsofintentcapture(capturingthedesireoftheuser)aswellasingenerationof
correctcode‚ÄîhasoccupiedmuchoftheComputingprofessionforthelast50‚Äì60years.Therehasbeensignificant
progressinmodelingandsystemdesigntosupportaccurateintentcaptureleadingtothegrowthofformalspecifications.
However,despitealltheprogress‚Äìsoftwareengineersarereluctanttowriteformalspecifications,andforlargesoftware
systemsaformaldescriptionofintentisnotavailable‚Äîleadingtotremendoushardshipindebuggingandfixing
errors.Thefieldofautomatedprogramrepairhasshownpromiseintermsofcodegenerationatamicro-scale.Thekey
questionthereishowtotrusttheautomaticallygeneratedcode.
RecentdevelopmentsonautomatedcodegenerationfromLargeLanguageModels(LLMs)bringthetrustissuesin
auto-codingevenmoreintotheforefront.Thisraisesnotonlytheoverallquestionofcorrectnessofautomatically
generatedcode,butatwhatpointwecanstarttrustingautomaticallygeneratedcodeenoughtointegrateitinto
ourcode-base.Inpastdecades,nicheindustrieshavegeneratedcodefrommodels,howeverthereisnoprecedentof
automaticallygeneratedcodefromnaturallanguagespecificationsbeingusedwidely.Wediscussthetrustissuesfor
suchautomaticallygeneratedcodethoroughlyinthisarticle.Whiletheimmediatemotivationofthearticleistostudy
thetrustissuesincodefromLargeLanguageModels(LLMs),westudythetopicofautomatedprogrammingmore
broadlyinthisarticle.
Wenoticethatincreasinglymanyorganizationsaremovingtowardsautomaticallygeneratedcode,evenapartfrom
thepopularityoflargelanguagemodels.ArecentkeynoteatOracleCloudWorld2023[27]mentionshowOracleis
consideringmovingawayfromwritingsoftwareinJava,andinsteadautomaticallygeneratingcodefornewsoftware
projectsinalanguagecalledApex.Apexisawell-knownlowcodeapplicationplatformtoassembleanapplicationout
ofapplicationpages.Thismovementtowardslowcodeenablesotherbenefitssuchaseasilyachievingsecurityauditof
Authors‚Äôaddresses:MichaelR.Lyu,lyu@cse.cuhk.edu.hkChineseUniversityofHongKong,HongKong,HongKong,China;BaishakhiRay,rayb@cs.
columbia.eduColumbiaUniversity,NewYork,USA;AbhikRoychoudhury,abhik@comp.nus.edu.sgNationalUniversityofSingapore,[Corresponding
Author],Singapore,Singapore;ShinHweiTan,shinhwei.tan@concordia.caConcordiaUniversity,Montreal,Canada;PatanamonThongtanunam,
patanamon.t@unimelb.edu.auUniversityofMelbourne,Melbourne,Australia.
1
4202
yaM
3
]ES.sc[
1v31220.5042:viXra2 Authors
asoftwareproject.Overall,wenotethatautomaticprogramminggoesbeyondtheuseoflargelanguagemodelsand
implicitlyincludesrecenttrendsinthegrowthoflow-codeno-codeapplicationdevelopment.
Asaresultoftherecentinterestinautomaticprogramming,thesetofproblemsassociatedwithautomatically
generatedcodehavereceivedwideattention.Apartfromcorrectness,thereareconcernsaboutsecurity,privacy,
explainabilityofthecode-particularlywhengeneratedfromalargelanguagemodel.Pragmaticallyspeaking,there
couldremainconcernsabout‚Äúpassingtheblame‚Äùwhenasoftwareprojectwhichincludesautomaticallygeneratedcode
fails.Tounderstandtheunderlyingissue,wecandrawananalogybetweeninteractionbetweenapplicationsoftware
andsystemssoftwarewhichleadstothewell-knownapplicationcompatibility(oftencalledappcompatbydevelopers)
problem(e.g.see[1]).Typicallyduetoversionchangeofsystemssoftwaresuchasoperatingsystem(OS),aspecific
applicationrunningontopoftheOS(suchasPDFreader)mayfail.However,thisneednotbeowingtotheoperating
systemitself.Itmaybeduetoamistakenunderstandingoftheexpectationsbetweentheapplicationsoftwareandthe
OS.Inasimilarfashion,whenautomaticallygeneratedcodeandmanuallywrittencodeco-existinasoftwareproject,
errorsmaycreepinduetomistakenunderstandingoftheexpectationsbetweendifferentsoftwarecomponents.
Inthisarticle,wethusexaminehowtrustboundariescanshiftwhenweintegrateautomaticallygeneratedcode
intoasoftwareproject.Oneofthetechnicalquestionsthatcouldbeofinterestfortheresearchcommunityarethe
acceptabilitycriterionforintegratingLLMgeneratedcodeintoasoftwareproject.ThecapabilityofLLMsaugmented
byprogramanalysistoolsinautomatingkeyprogrammingtaskssuchasbug-fixesandfeatureadditions,asarticulated
intherecentlyproposedSWEbench[69]isalsoworthyofstudy.Thelastmileimprovementofautomaticallygenerated
codefromLLMs,throughthesystematicuseofprogramrepair[28]remainsapossibilitytoexplore[90].Sucha
penultimateauto-repairstrategycanhelpprovidespecificevidenceofcorrectness(suchaspassingcuratedtests)that
buildsconfidencetowardsacceptingLLM-generatedcodeintoacoderepository.
WealsostudytheimpactofLLMsinautomatingnon-codeartifactsandprocessessuchastestgeneration,code
reviewandcodesummarisation.Moreimportantlyfromthehuman-LLMinteractionperspective,weseektoprovidean
emergingnewoutlookinfuturedayprogramming.Traditionally,whenformalspecificationsareunavailable,software
engineershaveresortedtoprogramcomprehensionorspecificationinferencetounderstandthefunctionalityofa
complexsoftwaresystem.Thispracticeisparticularlyrelevantwhenthesoftwaresystemisnotbuiltasamonolithic
artifact,butratherassembledviatheco-operationofdifferentteamsorviaopen-sourcecontributions.
Wenotethatthetraditionalprogramcomprehensionproblemisconductedbythehuman,andmayinvolvethe
usageofanalysis/debuggingtoolstounderstandtheworkingofacomplexsoftwaresystem.IntheageofLLMdriven
programming,wecouldpostulateanewcomprehensionproblem-wherebasedonthenaturallanguagerequirements
LLMsaugmentedbyprogramanalysistoolscanautomatebulkofthecomprehensiontasks.Therecouldbestructured
provisionsforconsultingthehumantodisambiguaterequirements,atdifferentstagesofthecomprehensionprocess.
Studyingthemechanismsforsuchhuman-LLMcollaborationandprovidingadequateprimitivesforconsultingthe
humanprogrammerbytheLLM/analyzerscouldpointustotheprogrammingenvironmentsofthenearfuture.We
alsounderlinethepossibilityofautomatedprogramrepairofautomaticallygeneratedcodeasaflexiblemechanismfor
trustedautomaticprogramming.Thesecouldfeatureinfuturedayprogrammingenvironmentsof2030-35andbeyond.
2 HISTORICALMILESTONESANDLITERATUREREVIEW
Inthissection,wewilldelveintothedevelopmentofautomaticprogramming,pinpointinghistoricalmilestonesand
conductingathoroughliteraturereview.Specifically,wewillfirstintroduceseveralkeytasksthatpropelledthefieldAutomaticProgramming:LargeLanguageModelsandBeyond 3
forwardincludingcodegeneration,programrepair,softwaretesting,andlogging.Additionally,wewillhighlightrecent
advancesofLLMsinautomaticprogramming.
2.1 CodeGeneration
Codegeneration,alsoknownasprogramsynthesis,referstotheautomatedgenerationofsoftwarecodebasedon
userintent.Thistechniqueboostsdeveloperproductivitybyreducingmanualcodingandacceleratingthesoftware
developmentlifecycle.Earlyresearchincodegenerationmainlycentersondeductiveandinductiveprogramsynthesis
whichcraftscodebasedonspecificationand/orinput-outputpairs.Withtheadventofdeeplearningtechniques,natural
language-basedcodegenerationthatdescribesusers‚Äôintentinplainlanguagegainsprominence.Besides,therearealso
otherworksthatfocusongeneratingcodebasedonimagesandstructureddata.Inthispaper,wewillmainlyintroduce
deductive,inductive,andnaturallanguage-basedcodegenerationmethodologies.
DeductiveandInductiveProgramSynthesis. Deductiveprogramsynthesiscraftsprogramsfromhigh-leveldescriptions,
whichinvolvesmechanicaltheoremprovingandformalmethods[45,49].Therequirementofdetailedspecifications
helpsreducelogicalerrors.Ithasdiverseapplicationsinfieldssuchasrobotics[31]andsoftwareengineering[57].For
example,STRIPS[31]isanautomaticplannerthataddressesrobotproblems,andPROW[141]generatesLISPcodefrom
specificationsinpredicatecalculusbyincorporatingatwo-stepprocessinvolvingtheoremprovingandcodeproduction.
Anotherwork[76]usesgeneticprogrammingapproachestoautomaticallyevolveprogramsthatareconsistentwitha
specification.Conversely,inductiveprogramsynthesis,orprogrammingbyexample(PBE),generatesprogramsdirectly
fromspecificinput-outputpairs.Thisapproachislesscomplexandmoreuser-friendlythandeductivesynthesisandhas
beenextensivelyinvestigated.Itenablesusersunfamiliarwithprogrammingtoinstructcomputersthroughexamples.
Forinstance,FlashFill[48],oneofthemostpopularreal-worldprogramsynthesisapplication,generatesprograms
forspreadsheetslikeExcelfromveryfewinput‚Äìoutputexamples.Similarmethods[142]arealsousedtogenerate
programsforrelationaldatabasesforschemarefactoring.
NaturalLanguage-basedCodeGeneration. Existingnaturallanguage-basedcodegenerationmainlyemploysdeep-
learningtechniquesandcantypicallybedividedintothreecategories:sequence-based,tree-based,andpre-trained
modelapproaches.Intherealmofsequence-basedmodels,thegenerationprocessemploysthesequence-to-sequence
paradigmandtreatsthisprocessasamachinetranslationprocesstotranslatethenaturallanguagedescriptioninto
sourcecode.Forinstance,Lingetal.[89]employaneuralnetworkwithastructuredattentionmechanismtohandle
semi-structuredinputsforcodegeneration.Fortree-basedmodels,thesemethodstakeintoaccounttheinherent
structurednatureofprogramsandparsethemintoatreesuchasanAbstractSyntaxTree(AST).Forexample,Yinet
al.[161]trainanLSTMtogenerateasequenceoftree-constructionactionsandsubsequentlybuildtheASTfromthese
actions.Rabinovichetal.[118]proposetheAbstractSyntaxNetworksanddirectlygeneratethetreestructureofsource
code.Anotherwork[130]designTransformerblockstoencodeboththenaturallanguageandthepreviouslygenerated
grammarrulesandthenpredictsubsequentgrammarrulesinthesequence.Inrecentyears,theadventofpre-trained
models[30,143]hasachievedsignificantimprovementinthefield.Thesemodelsarepre-trainedonextensivedatasets
andthenfine-tunedondatasetsrelatedtocodegeneration.Furthermore,somestudiesdrawinspirationfromcode
reusepracticestoenhancecodegenerationmodelsusingretrieval-augmentgeneration.Hayatietal.[55]improvecode
generationbyretrievingcodesimilartotheinputandcopyingn-gramactionsfromtheretrievedcode.Xuetal.[156]
introducestwoexternalknowledgebasesfromStackOverflowandAPIdocumentationforretrievalandimprovemodel4 Authors
performance.Parvezetal.[108]improvethegenerationprocessbyintroducingsimilarcodesnippetsalongsidethe
inputdescriptionintothegeneratorandtrainingthemodeltoselectivelyincorporatereusablecodes.
2.2 ProgramRepair
AutomatedProgramRepair(APR)methodologies[78]wereinitiallyintroducedtoautomaticallyfixprogrambugsand
reducetheneedforintensivemanualdebugging.Inthisarticlewewillalsoexaminethepossibilityofautomatedrepair
ofautomaticallygeneratedcode.APRleveragesautomatedtechniquestoanalyzebuggycodeandgeneratescorrect
patchestoaddresstheidentifiedissues.TheresearchofAPRtechniquescanbemainlydividedintothreecategories:
search-based,constraint-based,andlearning-based[78].
Search-basedProgramRepair. Search-basedAPRmethodsemployheuristicalgorithmstosearchfortherightfixin
apredefinedpatchspace[71].Thesemethodsuseheuristicstoidentifypotentialbugpositionsandgeneraterepair
candidates.Forinstance,GenProg[44]usesanextendedformofgeneticprogrammingtogenerateprogramvariants
thatcouldfixthebugsandretaintherequiredfunctionalities.RSRepair[116]employsthemutationtechniquesused
inGenProgandusesrandomsearchtogenerateafixpatch.ARJA[163]formulatesautomatedprogramrepairasa
multi-objectivesearchproblemandusesNSGA-II[21]tolookforsimplerrepairs.Onechallengeofsearch-basedAPRis
thecostlyvalidationofpatchesbytesting[33].Toenhanceefficiency,variousstrategiesthattrytominimizecandidate
patchesandtestcasesforvalidationhavebeenproposed.Forexample,AE[147]introducesRepairStratandTestStrat
whichleverageequivalentpatchestoprunesemantically-equivalentpatchesandsamplevalidatespatchestocutdown
costs.relifix[131]targetsregressionerrorfixesusingpreviousprogramversionsandcontextualrepairoperators.The
workof[35]focusesonthesoftwareregressionerrorsandproposestoleveragepreviousversionsofabuggyprogram.
Search-basedrepairtechniquesmaysufferfromhavingtonavigatealargesearchspaceandtoalleviatethisissue,
fixtemplateguidedrepair(suchastheworkofPAR[75])hasbeensuggested.Searchbasedrepairsuffersfromthe
moreseriousissueoftest-dataoverfittingwherethegeneratedrepaircanpassthegiventests,butnotothertests.
Theissueofoverfittinginprogramrepair,andspecificallysearch-basedprogramrepairhasbeenmentionedin[117]
Constraint-basedprogramrepairapproachesmitigatetheseconcerns,byconstructingageneralizationofgiventests
viasymbolicanalysis.
Constraint-basedProgramRepair. Constraint-basedAPRmethodsutilizeconstraintspecificationstoguidetherepair
byconvertingtherepairproblemsintoaconstraintsolverproblem.Forexample,SemFix[102]fixessingle-linebugs
usingsymbolicexecutionbycraftingrepairconstraints.DirectFix[99]improvespatchgenerationwithconstraint
solvingandprogramsynthesis,extendingtheabilitytofixmulti-linebugsbutsufferingfromthescalabilityproblem
duetomaxSMTsolvingoverheads.Toovercomethis,Angelix[100]proposestoemploythelightweightvaluebased
specifications(angelicforest)forbetterscalability.Nopol[158]wasproposedtofixif-conditionalbugsusingSMT.It
usesvaluereplacementinsteadofsymbolicexecution.AnotherworkcalledSPRperformsenumerativesearchtofind
suitablevaluestobereturnedbybooleanexpressionsindifferentiterationsofaloop[94].TherepairtoolProphet[95]
isanimprovementofSPR,whereamachinelearningmodelisemployedasthelaststeptorankpatchcandidates.
Learning-basedProgramRepair. Withtheadventofmachinelearning,numerousmethodshavebeenproposed
thatutilizelearning-basedmodelstocaptureprogramsemanticsforrepairingbugs.Earlydeeplearning-basedAPR
approaches[53,148]utilizedneuralmodelstolearncodesemanticsforaidingrepairtasks,insteadofdirectlygenerating
patches.DeepRepair[148]identifiessimilaritiesbetweenbuggycodeandpotentialfixestoguidethepatchgeneration.AutomaticProgramming:LargeLanguageModelsandBeyond 5
Morerecentmethods[19,86,98]employneuralmachinetranslation(NMT)techniquesusingencoder-decodermodels
tounderstandthesemanticsofbuggycodeandtranslatebuggycodeintofixedcode.Forinstance,CoCoNuT[98]
tokenizescodeintosequencesliketexttotranslatethebuggycodeintocorrectcode.DLFix[86]leveragesabstract
syntaxtreeswithtree-basedmodelstocapturecodestructureinformation.CURE[67]integratespre-trainedmodelsin
NMT-basedAPRandproposesacode-awaresearchstrategytofindcompilablepatches.Comparedwithgenerating
patches,Recoder[168]proposestogeneratetheedittoensurethesyntacticcorrectnessofthepatchedprogram.The
recentworkRewardRepair[160]improvesrepairperformanceandthesuccessfulcompilationrateofpatchesbytraining
modelswithprogramexecutioninformation.
SecurityVulnerabilityrepair. Programrepairtechniqueshaveshownpromiseinautomaticallyfixingsecurityvul-
nerabilities.ThishassignificantpromiseandrelevanceforautomaticallygeneratedcodefromLLMs,sincesecurity
vulnerabilitiesinLLMproducedcoderemainsabigconcern.TheworkofExtractFix[42]usesaddresssanitizersto
extractspecificationsofcrash-freedomandthenusessymbolicreasoningtoproducepatchesviaarepair-augmented
weakestpre-conditioncomputation.Thisleadstoacompletelyautomatedvulnerabilityrepairmethodformemory
errors.TheworkofSenX[59]requiressafetypropertieswhicharethenusedtoautomaticallygeneratevulnerability
patches. Last but not the least, the work of Crashrepair [40] suggests a promising workflow where vulnerability
detectionviagrey-boxfuzztestingandvulnerabilityrepairarefusedintoasinglestep-prioritizingtestswhichcan
betterdistinguishamongpatchcandidates.Suchworkflowsmayholdpromiseasautomaticallygeneratedcodefrom
LLMs(potentiallyrepletewithsecurityvulnerabilities)becomecommon-placeinfuture.
2.3 LLM-basedIntelligentProgramming
Inthissection,wewillfirstdetailintroducerecentrepresentativeLargeLanguageCodeModelsandthenintroduce
someworksthatutilizeLLMstoboosttheaboveprogrammingtasks.
2.3.1 Large Language Code Models. Recently the advent of pre-training techniques techniques has significantly
advancedprogressinautomaticprogramming.Pre-trainedcodemodelsarefirstpre-trainedonlarge-scaleunlabeled
datasetsusingself-supervisedlearningtasksandthenfine-tunedorpromptedfordownstreamtasks.Sincethisprocess
doesnotrequirehumanannotation,itcanbeappliedtolarge-scaleunlabeleddatasets,enablingthemodelstoacquire
avastamountofgeneralprogrammingknowledge.Recentstudies[73,145]showthatincreasingthesizeofthese
modelssignificantlybooststheirabilities,resultinginsubstantialenhancementsinperformanceoncethemodelsgrow
beyondacertainparameterthreshold.Theterm‚ÄúLargeLanguageModel‚Äù(LLM)hasbeenproposedtodistinguishthese
modelsbasedontheextentoftheirparameters.Inthissection,wewillprovideadetailedaccountofwell-knownLarge
LanguageCodeModels,ranginginsizefromBert-likemodelstothoseaslargeasChatGPT.
Onepioneerworkofpre-trainedcodemodelisCodeBERT[30],whichisanencoder-onlypre-trainedmodelonsix
programminglanguageswithtwoself-supervisedtasks,i.e.,maskedlanguagemodelingandreplacedtokendetection,
whichsignificantlyoutperformspreviousnon-pre-trainedmodels.Anothermodel,CodeT5[143]isanencoder-decoder
pre-trainedmodelfollowingthesamearchitectureasT5.Itformulatesallthetasksinasequence-to-sequenceparadigm
withdifferenttask-specificprefixesandachievespromisingresultsonavarietyofcodeintelligencetasks.CodeGPT[96]
isadecoder-onlymodelthatpre-trainsonprogramminglanguagesdatasetandhasthesamearchitectureasGPT-
2.PLBART[2]usesdenoisingsequence-to-sequencepretrainingforbothprogramunderstandingandgeneration
purposes.UniXCoder[51]involvesmulti-modalcontrastivelearningandcross-modalgenerationobjectivetolearnthe6 Authors
representationofcodefragments.Morerecently,therearealsosomepre-trainedcodemodelsthataredesignedfor
specificprogrammingtaskssuchasCodeReviewer[87]andCoditT5[166].
Apartfromthesesmallerpre-trainedmodelsinacademics,manypre-trainedcodemodelswithmuchlargersizes
havebeenproposedintheindustryinrecentyears.INCODER[34]isamodelthatadoptsacausalmaskingtraining
objective for both code infilling and synthesis and has two versions with 1.3B and 6.7B parameters, respectively.
CodeGen[105]isalargepre-trainedmodelwithmorethan16Bparameters,whichachievespromisingresultsfor
multi-turnprogramsynthesis.Codex[17]isalargecodepre-trainedmodelproposedbyOpenAIthatsupportsthe
serviceofCopilot.Itisadeptatunderstandingandgeneratingcode,facilitatingtheautomationofprogrammingtasks,
andsupportingdevelopersinwritingcodemoreefficiently.InadditiontoCodex,themodelsrecentlyreleasedby
OpenAI,suchasChatGPT[14]andGPT-4[107],arealsopre-trainedonsourcecodedataanddemonstrateimpressive
programmingabilities.AlphaCode[82]istrainedforgeneratingcodeforprogrammingcompetitionswith715Gdata
and41Bparameters.Itcangeneratenovelsolutionstounseenprogrammingproblemsandoutperformabouthalfof
developersincompetitiveprogrammingwithmorethan5,000participants.StarCoder[81]isanadvancedLLMfor
assistedprogramming.ItsbaseversionistrainedontheStackdatasetwith15.5Bparametersandincreasestheinputsize
into8000tokenstoenabledealingwithlongercode.CodeLlama[120]isafamilyoflarge-scalecodelanguagemodels
developedbyMetaandhasvariationsincludingbase,Python-specialized,andinstruction-followingmodels,ranging
from7Bto34Bparameters.Thesemodelsareadeptathandlingsequencesupto100ktokensandareavailablefor
bothresearchandcommercialuseunderlicense.Phi-1[50],fromMicrosoftResearch,isa1.3Bparameterdecoder-only
transformermodel,trainedonacurateddatasetof7Bsamples,designedforcode-relatedtasks.WizardCoder[97]
isanopen-sourceLLMbasedonStarCoder,fine-tunedwithinstruction-baseddatasetstoenhancecodegeneration
capabilitiesacrossvariouscomplexitylevels.DeepSeekCoder[22]istrainedonamixedcorpusofcodeandnatural
language.Itfocusesonproject-levelcodecompletionandinfillingandachievesstate-of-the-artperformanceinmultiple
programminglanguagesonvariousbenchmarks.Magicoder[146]isarecentworkthatistrainedonsyntheticinstruction
dataenhancedwithopen-sourcecodesnippets.Itsprimaryaimistoproducediversified,realistic,andcontrollabledata,
addressingthebiastypicallyfoundinsyntheticdatageneratedbyLLMs.
2.3.2 UtilizationofLLMsforIntelligentProgramming. Recently,apartfromtrainingabaseLLM,therearealsoalotof
worksthatfocusonhowtoutilizethesepowerfulLLMsbytuningorpromptingthemforautomaticprogramming[37,
38,79,112,152].Incodegeneration,thereisagrowinginterestinmethodsthatutilizethechain-of-thoughtprompt
togeneratebettercodeandsolvemorecomplicatedprogrammingproblems.Forexample,TIP[79]utilizesLLMsto
formulateahigh-levelcodesketchbeforeworkingondetailedcodingtasks,whichimprovestheprecisionandreliability
ofgeneratedcode.Dongetal.[24]proposesaself-collaborationmethodtoadvanceLLMsincomplexcodingtasksby
employingmultipleLLMsasdistinctexpertsandmakingtheminteractwitheachother.Besides,apartfromgenerating
codesatfunction-level,manyrecentworkalsoexploresextendingthescopeofcodegenerationintointoclass-level[25]
andrepository-level[125].Asforprogramrepair,therearealsoalotofstudiesutilizingLLMstorepairsoftwarebugs.
Xiaetal.[151]firstleverageLLMsforprogramrepairbyusingthecloze-stypeprediction.Huangetal.[58]studiesthe
impactofdifferentLLMsanddifferentprogramrepairscenarios.Pengetal.[112]proposestominedomain-awarefix
templatesandincorporatethemintocodepromptstorepairPythontypeerror.Apartfromtheaboveworksthatonly
generatetherepairpatchinaone-stopway.ChatRepair[152]leveragestheconversationalnatureofadvancedLLMs
likeChatGPTandlearnsfrombothprevioustestfailureinformationtoprovidethemodelwithimmediatefeedback.
Withthefeedbackinformationfromtestcases,itcouldproducemorepreciseandcontext-sensitivefixes.ForloggingAutomaticProgramming:LargeLanguageModelsandBeyond 7
activities,Lietal.[84]presentthefirstextensiveevaluationofLLMsforloggingstatementgeneration.Furthermore,
Sridharaetal.[128]exploretheproficiencyofChatGPTinsummarizinglogs,achievingpromisingresultssurpassingthe
existingmethod.TofacilitatetheeffectivenessofLLMforlogparsing,[157]leveragesLLMandin-contextlearning(ICL)
forlogtemplateextractionandanotherwork[68]improveslogparsingbyICLandparsingcache.Inloggeneration,a
recentwork[85]proposestoincorporatestaticcontextintocodepromptandemploysaself-refinementmannerto
furtherrectifypreviouserrors.LLMsarealsobeneficialingeneratingtestcasesfromnaturallanguagedescriptions,
whichenhancescooperationbetweensoftwaredevelopersandtesters.Theseincludetheautomatedtestcasegeneration
ofvariousscenariossuchasenhancingthecoverageoftesting[126,154]andthedetectionofpossibledefects[154].
Ryanetal.[121]proposestoprovideLLMswithpathconstrainsandcodecontexttoimprovethecoverageofgenerated
testcases.ChatUniTest[154]extractsessentialinformationandcreatesanadaptivefocalcontextforLLMstogenerate
tests.
3 PROGRAMREPAIRANDAUTO-CODING
Programsynthesisconvertsaformalorsemi-formalspecificationintoexpressionsorcodesnippets.Theareahasbeen
studiedasearlyas[114]andarecentsurveyappearsin[5].Thespecificationsdrivingprogramsynthesismayoftenbe
givenasacollectionof(input,output)examples-providingtheoracleforagiveninput.Programrepair[78]involvesa
correctionorrectificationofacode-basesothatitcanmeetcertaincorrectnesscriteria.Thecorrectnesscriteriacanbe
givenintermsofsystemleveltestcasesthattheoverallsoftwaresystemneedstopass.Bothprogramsynthesisand
repairsufferfromtheoverfittingproblemduetotheincompletenessofthespecificationsdrivingtheseprocesses.If
thespecificationisgivenasatest-suitetheoverfittingcanappearintheformofthegeneratedcodeoverfittingthe
test-data.Asasimpleexampleletussupposewehave(input,output)specificationsgivenintermsofcollectionsof
input-outputpairsasfollows.
(input = 2, output = 4)
(input = 3, outout = 9)
andwehaveabuggyprogram
output = input + input;
Aninadequateprogramrepairsystemmayfixtheaboveprogramto
if (input == 2) output = 4;
else if (input == 3) outout = 9;
whileourdesirewillbetoproducethefollowing(minimal)fixviaprogramrepair
output = input * input;
Thissimpleexamplealsomakesitapparentthecoreissueof"generalization"underlyingprogramsynthesisap-
proaches-particularlythosethataredrivenbyinput-outputexamples.Itisalwayspossibletosynthesizecodethat
worksexactlyforthegiveninput-outputexamplesbyproducingcodewiththefollowingschematic
if (input == input1) return output1
else if (input == input2) return output2
else ...
Imposingcertainqualityindicatorssuchascodesizemayinducetheprogramsynthesizertoproducemorecompact
codewhichgeneralizesthegiveninput-outputexamples.Whilethereexistalargenumberofsynthesisapproaches,8 Authors
manyofthemtypicallyperformanenumerativesearchoverthesearchspaceofexpressions.Theenumerativesearch
maybeguidedbyachoiceofoperatorsappearingintheexpression(component-basedsynthesis[65])orcertain
restrictionsoverthesyntaxofexpressionstypicallycapturedviaagrammar(syntax-guidedsynthesis[6]).Irrespective
ofthetechnicalmachineryusedtoconductthesynthesis-theissueofoverfittingofthesynthesizedcoderemains.The
concernisthatthesynthesizedcodemayreturntheexpectedoutputforthegiveninput-outputexamplesbutnotfor
otherinputs.Fortheprogramsynthesisproblem,thisproblemsometimesremainsimplicit-sincetheexpectedoutput
forinputsotherthanthoseappearinginthegiven(input,output)examplesmaynotevenbedocumentedfully.Inthe
problemofprogramrepair,whereabuggyprogramisgiven-theproblemofoverfittingismoreexplicit.Herethefixed
programmaypassthegiventestsinatest-suitewhichisusedtoguidetherepair;atthesametime,thefixedprogram
maynotpasstestsoutsidethegiventest-suite.
Wenowdiscussatreatmentofprogramrepairasafieldwithsometechnicalglimpsesontheunderlyingchallenges
suchasover-fitting.Thetreatmentisfromtheopen-sourceunpublishedarticlebythethirdauthor[41].
3.1 Programrepair
Theissueofoverfittinghasbeenwellstudiedandarticulatedintheareaofprogramrepair[117].Whileraisingawareness
abouttheissue,theseworkshavearticulatedconcernswhichgobeyondtheincompletenessoftests.Itisgenerally
knownthatanytest-suiteascollectionof(input,expectedoutput)pairsisanincompletespecificationofintended
programbehavior.Thereforerepairsgeneratedbyusingatest-suiteùëá asguidancemaynotpasstestsoutsideùëá.However
theconcernsaboutgeneratingoverfittingrepairsgobeyondtheincompletenessofùëá.Forexampleiftheoracleof
certaintestssaythatanexceptionshouldnotraised,arepairmaysimplydeletethecodewhichraisestheseexceptions
andmeettherequirement.
Forthisreason,itisimportantforautomatedprogramrepairtechniquesto
‚Ä¢ performanadequategeneralizationofthegiventest-suiteùëá,sothattherepairsdonotonlyworkontestsinùëá
‚Ä¢ satisfycertaincodequalityindicatorsapartfrompassingthegiventests,toavoidobviouslyunacceptablerepairs
suchasdeletingthecodecheckingtheoracle.
‚Ä¢ toensurequalitypatchescertainrepairtechniquesemphasizethesuccinctnessofthepatches-meaningsmaller
disruptiontothecode-baseissomehow"better".
Wenowdescribeindetailsoneconcreteapproachforprogramrepair,whichseekstoachievethesegoalsbysymbolic
analysisofthegiventestsinùëá.Heresymbolicanalysisofthetestexecutionsfortestsinùëá,amountstocomputinga
generalizationwhichwewanttoworkfortestsoutsideùëá aswell.Bysymbolicallyanalyzingthetestsinùëá,therepair
methodextractsspecificationsaboutthepatchcodeintheformofrepairconstraints.Theserepairconstraintscanbe
usedasguidanceingeneratingpatchesviasearchorprogramsynthesis.Weemphasizehereforthereaderthatthisis
onlyoneapproachforprogramrepair,andthereexistseveralotherapproachesbasedonsearchandlearning[78].One
motivationforpresentingthisconstraintbasedprogramrepairapproachistoillustrateideasabouthowautomatically
generatedcodeinprogramrepairtechniquescanavoidthetestover-fittingproblem.Conceptuallyspeaking,wecould
alwaysdefineadomainofprogramedits,andthenconductarandomsearchinthisdomaintofindeditswhichpass
giventests.However,theoutputofsuchasearchwouldbegreatlydependentonthesearchheuristicsanditwould
behardtogiveanyassuranceaboutthequalityofthepatches.Thus,insteadofsearchingatrandominthespaceof
patches-weshowhowtherepairtechniquecanbe"guided"toproducehigherqualitypatches.AutomaticProgramming:LargeLanguageModelsandBeyond 9
1 int tri_detect(int a, int b, int c){
2 if (a <= 0 || b <= 0 || c <= 0) a b c Output Outcome
3 return INVALID; -1 1 1 INVALID Pass
4 else if (a == b && b == c) 2 2 2 EQUILATERAL Pass
5 return EQUILATERAL; 2 2 3 ISOSCELES Pass
6 else if (a == b || b == c) 2 3 2 SCALENE Fail
7 return ISOSCELES; 3 2 2 ISOSCELES Pass
8 else return SCALENE; 2 3 4 SCALENE Pass
9 }
Fig.1. Triangleprogramfrom[78]andthetestdataaccompanyingtheprogram
Intheapproachthatweelaborateinpriorwork[103],therepairtechniqueis"guided"byarepairconstraintwhich
generatedbysymbolicallyexecutingthetestsinthegiventest-suiteùëá inanovelfashion.So,themainconceptualstep
isinusingconstraintstoreducethesearchspaceofpossiblepatches,asopposedtosearchinginthedomainofpatches.
Wedonotdiscussthecomputationoftheconstraintindetails,butratherconceptualizeatahighlevelhowthepresence
ofsuchaconstraintcanhelpgeneratehighqualityrepairsandavoidpatchoverfitting.
Letusconsideraprogramthattakesinthreesidesofatriangleanddeterminesthekindoftriangleconstructedout
ofthesethreesides.TheprogrammaylookliketheprograminFigure1.Thisprogramhasseveralbugs.Forthree
sideswhichviolatethetriangleinequality-itshouldreturnINVALID,butitisnotdoingso.Similarly,thedefinition
oftheisoscelestriangleissupposedtocheckifanytwoofthethreesidesareequal.Now,asshowninthetestsuite
fromFigure1,letusshowarealistictest-suiteconsistingonetestforinvalidtriangle,oneforequilateraltriangle,three
testsforisoscelestriangle(dependingonwhichtwosidesareequal),andonetestforascalenetriangle.Areasonably
constructedtest-suitebasedontherequirementswillindeedbeofthisnature.Letusassumenowthatbyacontrolflow
analysisofthepassingandfailingtests,line6isinferredasthefixlocation.Thefixlocalizationprocessisthesameas
thelocalizationofsearch-basedAPRtechniques.Theexactprocessoffixlocalizationisnotshownhere.Itmayinvolve
findingoutlocationswhichappearwithsignificantlygreaterfrequencyinfailingtests,thaninpassingtests.Oncethe
fixlocationisidentified,theexpressioninthatlocationissubstitutedasanunknownorasymbolicvariableX.
...
6 else if (X)
7 return ISOSCELES;
...
Now,itisrequiredtofindoutpropertiesaboutXwhichwouldmaketheprogrampassthetestcasesthataregiven.
‚Ä¢ thefirsttwotestsdonotevenreachline6.
‚Ä¢ amongtheremainingfourteststhatreachline6,Xshouldbetrueinthethird,fourth,andfifthtests.Moreover,
Xshouldbefalseinthesixthtest.
Gettingtheabove-mentionedrequirements,thoughputintuitivelyhere,isnotstraightforward.Itinvolvesananalysis
ofthetestexecutionsforthegiventests.EssentiallyitamountstofindingthedesiredvalueofX(inthiscaseaboolean
asitrepresentsabooleanexpression)sothatitcanmakethetestpass.Thisiscapturedbytherepairconstraint.
HowtoformallycapturetheserequirementsorconstraintsonX,whichessentiallyisaplaceholderforthecode
insertedinline6?AformalwayofunderstandingthisrepairconstraintisthattheunknownXisessentiallyanunknown10 Authors
functiononthevariableswhichareliveinline6.Thusessentially
ùëã =ùëì(ùëé,ùëè,ùëê) (1)
whereùëì isanunknownfunctionthatistobesynthesized.Theinformationaboutthefunctionùëì isgivenbythefollowing
repairconstraint.
ùëì(2,2,3)‚àßùëì(3,2,2)‚àßùëì(2,3,2)‚àß¬¨ùëì(2,3,4) (2)
Thisrepairconstraintcanbefedtoaprogramsynthesisengine.Thesynthesisenginecanbefedwiththeingredients
thatcanappearintheexpression:thevariables,theconstants,andtheoperators.Inthiscase,thevariablesarea,b,c,
theconstantsaretheintegerconstantsandtheoperatorsaretherelationaloperatorsandlogicaloperators.Withthese
ingredientsandtheprovidedrepairconstraint,acomponent-basedsynthesisengine[64]willyieldthecorrectfix
ùëì(ùëé,ùëè,ùëê)=(ùëé==ùëè||ùëè ==ùëê||ùëé==ùëê) (3)
Letusnowpresenttheformaltreatmentofrepairconstraintcomputation.Statisticalfaultlocalization[149]orother
offlineanalysistechniquesareappliedtoidentifypotentialfixlocations.Suchofflineanalysismayinvolveprogram
dependencyanalysis,orsimplycontrolflowanalysisofthepassing/failingtests.Letusexaminehowthecontrolflow
analysisofpassing/failingtestswillproceedundertheauspicesofstatisticalfaultlocalization.Insuchanapproach,
eachstatementùë†intheprogramisgivenasuspiciousnessscorebasedontheoccurrencesofùë†inthepassing/failing
tests.Constraint-basedAPRtechniquesalsorelyonfaultlocalizationtodeterminethelinetobefixed.Onceafixlineis
decided,arepairconstraintisthenconstructed.Thisisaconstraintontheexpressiontobeputinthecorresponding
lineasafix.Forthepurposesofexplanation,letusassumethatthefixiseitherabooleanexpressionoranarithmetic
expressionwhichistherighthandsideofanassignment.Howtoconstructtherepairconstraint?Foraboolean
expression,theexpressioncanbesimplyreplacedwithanewsymbolicvariableXasfollows.
if(e)‚Üíif(X) (4)
Foranarithmeticexpression,anewsymbolicvariableXisintroducedasfollows.
y=e‚Üíy=X (5)
Noteherethatyisaprogramvariableandeisanexpressionmadeoutofprogramvariables,whileXisasymbolicghost
variablewhichisintroducedbyus,forthepurposesofautomatedprogramrepair.NotethatthesymbolicvariableXis
introducedatthedeemedfixlocation,andfornowletusassumewearegeneratingaonelinefix.
GivensuchaghostsymbolicvariableX,therepairconstraintisdefinedintermsofXasfollows.Foragiventestùë°,
thepathuptothefixlocationùêøisconcrete.Fromthefixlocationùêø,thereareseveralpossiblepaths,dependingon
thevalueofX.Therefore,thepathconditionofapathùúã fromùêøandthesymbolicoutputalongthepathintermsofùëã
canbedefined.Letthesebeùëùùëê andùëúùë¢ùë° respectively,asillustratedatFigure2.Thenaconstraintforpathùúã canbe
ùúã ùúã
representedas
ùëùùëê ùúã ‚àßùëúùë¢ùë° ùúã =ùëúùëüùëéùëêùëôùëí(ùë°) (6)AutomaticProgramming:LargeLanguageModelsandBeyond 11
Fig.2. InferringSpecificationsforProgramRepair(ack.unpublishedarticle[41])
whereùëúùëüùëéùëêùëôùëí(ùë°)istheexpectedoutputfortestcaseùë°.Consideringthevariouspathsfromùêøfortheexecutionoftestùë°,
repairconstraintfortestùë° topassis
(cid:220)
ùê∂ ùë° ‚â° ùëùùëê ùúã ‚àßùëúùë¢ùë° ùúã =ùëúùëüùëéùëêùëôùëí(ùë°) (7)
ùúã
Theoverallrepairconstraintistheconjunctionoftherepairconstraintcollectedfromallthegiventests,sincethe
repairedprogramisexpectedtopassallthegiventests.Inotherwords,therepairconstraintCisgivenasfollows.
(cid:219)
ùê∂ ‚â° ùê∂ ùë° (8)
ùë°
3.2 LanguageModelbasedCodeGeneration
DesigningAI-basedsystemstoautomaticallysolveprogrammingtaskshasgainedconsiderableattentioninrecent
years.Themostnotableofthesecomesintheformoftransformer-basedlarge-scalelanguagemodels,whichusedto
transformnaturallanguagetext.Largelanguagemodels,suchasCodex[16]andAlphaCode[83],havealsosuccessfully
generatedcodeformanyprogrammingtasksinPython,Java,C,etc.
ProgramRepairforfixingCodeGeneratedbyLanguageModel. CodexandAlphaCodehaveshowncapabilityin
generatingcorrectsolutionsformanyprogrammingtasks.However,thesuccessrateofexistinglanguagemodels
remainslow,especiallyforcomplexprogrammingtasks.Oneofthereasonsisthatlanguagemodelslackawareness
ofprogramsemantics(e.g.,typeinformation,run-timeprogramstates,etc.),resultinginincorrectprograms.Alarge
partofbugsmadebyCodexaresyntaxerrorsormisalignedalgorithms,i.e.,uncompiledprogramsorprogramswith
incorrectalgorithms.Meanwhile,somebugsrequiresmallchanges,e.g.,changingoperators,modifyingexpressions,or
changingstatements.Forinstance,Figure3showsanexampleprogramproducedbyCodexforanprogrammingtaskin
LeetCode1.ThecommentsinFigure3aretheprogramdescriptions,whichareprovidedtoCodexasprompt,andthe
codeisautomaticallygeneratedbyCodex.Unfortunately,theproducedprogramhasabug,causingtheprogramto
failonsometestcases.Thecorrectfixistochangestatementùëñ‚àí=2atline9toùëñ‚àí=1.Comparedtolanguagemodels,
typicalrepairtoolsgeneratepatchesbyreasoningabouttheprogramsemanticsagainstthegivenspecification.Hence,
1https://leetcode.com12 Authors
1 //A fancy string is a string where no three consecutive characters are equal.
2 Given a string s, delete the minimum possible number of characters from s to make it fancy.
3 Return the final string after the deletion. It can be shown that the answer will always be unique.
4 public String makeFancyString(String s) {
5 StringBuilder sb = new StringBuilder(s);
6 for (int i = 2; i < sb.length(); i++) {
7 if (sb.charAt(i) == sb.charAt(i-1) && sb.charAt(i) == sb.charAt(i-2)) {
8 sb.deleteCharAt(i);
9 - i -= 2;
10 + i -= 1;
11 }
12 }
13 return sb.toString();
14 }
Fig.3. TheprogramforaLeetCodeprogrammingtaskgeneratedbyCodex.
therepairtechniquehasthepotentialtoincreasethesuccessrateoflanguagemodels.Intheaboveexample,several
existingrepairtoolscanautomaticallyfixthebugandmakeitpassallthetestcases.
LanguageModelforProgramRepair. Languagemodelscouldalsobeusedforfixingsoftwarebugs.InMarch2022,
anewversionofCodexeditmodewasreleased.Insteadofjusttranslatingprogramdescriptionstoprograms2,the
Codexeditmodelcanchangeexistingcodeinacompleteprogram.ThisnewfeaturemakesitpracticaltouseCodexfor
programrepair.Codexeditmoderequiresuserstoprovideinstructionstoguidethecodechange,suchas‚Äúfixthebugat
line2‚Äù,or‚Äúfixtheindex-out-of-boundexception‚Äù.Tofixabug,usersneedtoprovidepreciseandclearinstructions.The
repairbasedonlargelanguagemodelscouldevenproducebetterperformanceinfixingsoftwarebugsthanlearning
basedrepairtechniques.Comparedtoexistinglearning-basedrepair,e.g.,SequenceRandRecoder,Codexistrainedon
amuchlargerdatasetthanRecoder,whichhelpsCodextolearnmorefixpatterns(see[28]forcomparisonresults).In
fact,largelanguagemodelslearncodeeditpatternsfromhugeexistingprogrammingartifacts(includingcode,commits,
commentsandetc.).Neverthelessthetaskofrepairingautomaticallygeneratedcoderemainsachallenge.Arecent
work[28]hasshownthatautomaticallygeneratedcodeforevensimpleLeetcodeproblemscontainlargenumberof
occurrencesof‚Äúmisalignedalgorithms".Suchmisalignedalgorithmsconstitutecaseswheretheprogrammingofthetask
(oroneofitssub-tasks)isincorrectevenatanalgorithmiclevel-sothatthecodeneedstobecompletelyre-written.
UsingthelatestLLMs. Promptingsomeofthelargestcommercialmodels,suchasGemini1.5,GPT3.5,andGPT4,
showsthattheexamplegeneratedbyCodexcanbesuccessfullyrepaired.Providingnospecificinformationaboutthe
bug("fixtheissueplease")oronlythebuggyline("thereisanerroronline9")issufficienttogetapatch.Furthermore,
thesemodelsarecommonlyusedforchattingandgenerateahuman-readableexplanationofthefault.Figure4isan
exampleofsuchaninteractionwithOpenAI‚ÄôsGPT4model.Theflexibilityandpowerofsuchmodelscomeata(small)
cost.Withanimpreciseprompt,themodelmayrespondinanunexpectedformat,whichcannotbeusedtorepair
theprogramautomatically.Asolutionistodescribehowthemodelshouldreturnthefix.Anexamplemodification
isappendingthestring‚ÄúReturnthepatchedcodeintriplebackticksandnothingelse.‚ÄùWiththeadvancementofthe
models‚Äôsizeandarchitecture,wepostulatethatsyntaxerrorsforcommonlanguageswilloccurlessintheoutputof
2https://openai.com/blog/gpt-3-edit-insertAutomaticProgramming:LargeLanguageModelsandBeyond 13
The given code snippet aims to remove characters from the string `s`
to ensure no three consecutive characters are the same, thereby making it a "fancy string."
However, there's an issue with the logic that might not always yield the correct result
because of how the index `i` is managed after deletion. When you delete a character at index `i`,
you do not need to decrement `i` by 2. Instead, you should decrement `i` by 1 to ensure the loop
checks the next set of characters properly after a deletion.
Let's correct the code:
...
Here's the corrected version:
```java
public String makeFancyString(String s) {
StringBuilder sb = new StringBuilder(s);
for (int i = 2; i < sb.length(); i++) {
if (sb.charAt(i) == sb.charAt(i-1) && sb.charAt(i) == sb.charAt(i-2)) {
sb.deleteCharAt(i);
i--; // Corrected to decrement i by 1 instead of 2
}
}
return sb.toString();
}
```
...
Fig.4. ExamplerepairgeneratedbyGPT-4(abbreviated)
largecommercialmodelsinthecomingyears.Ontheotherhand,asnaturallanguageisinherentlyambiguous,the
issueofmisalignedalgorithmswillremainproblematic.Misalignedalgorithmsoccurwhenwhiletryingtowritecode
foratask,alanguagemodelgeneratesanincorrectsolutionevenatalgorithmiclevelforsomeofthesub-tasks[28].
Validationtechniquesmustbedefinedtoalleviatethis,andguaranteesofthemodel‚Äôsresponsemustbeprovidedto
ensuresafecodeintegration.
Commentaryonsemanticapproaches. Wepostulatethatlanguagemodelbasedcodegeneration,aswellaslanguage
modelbasedrepairapproachescouldplayaroleinfuture.Atthesametime,therelationshipofthelanguagemodel
basedrepairapproachwithrespecttoprogramsynthesisisnotwell-understoodtoday.Sincesemanticrepairapproaches
orconstraintbasedrepairapproaches,relyonsymbolicreasoning,thereexistopportunitiesincombiningsemantic
repairapproacheswithlanguagemodelbasedrepairinthefuture.Hereweneedtobecarefulaboutwhatkindof
back-endweusefortheconstraint-basedrepairapproach.Wenotethattheprogramsynthesisback-endcanbereplaced
byagenerativeAImodel,whiletheconstraint-basedrepaircanprovideasystematicselectionmechanismforselecting
amongpatchcandidates.Theseworkflowscanbeexaminedinthefuture.
4 CODELLMS,SOFTWAREQUALITYANDTRUSTWORTHINESS
TheuseofLLMsinautomaticprogrammingshowsimmensepotential.However,thisprogressbringswithitconcerns
aboutthetrustworthinessofthecodethesemodelsproduce.Forinstance,Jesseetal.[63]foundcodeLLMstendto14 Authors
introducesimplebugsinthecodebase.EstimatingcorrectnessofthecodegeneratedbyLLMsbecomesproblematic,
especiallyintheabsenceofclearspecifications.Infact,therehavebeeninstanceswherecodegeneratedbyLLMswas
foundtocontainsecurityvulnerabilities[111,113].Identifyingsecurityflawswithinthegeneratedcodeischallenging;
developersmightnotrevieweverypieceofcodeindetail,leadingtooverlookederrors.ThereisalsoariskofLLMs
beingexploitedbybadactorswhomaytamperwiththetrainingdataormanipulatethepromptsusedduringthe
queryphase[46].TheopaquenatureofLLMsaddsanotherlayerofcomplexitytothetaskofanalyzinganddebugging
automaticallygeneratedcode.Theintricatealgorithmsthatdrivethecodegenerationprocessarenotfullytransparentto
developers,makingithardforthemtograsphowthecodecomestoexist.Thisissuebecomesevenmorepronouncedin
programmingenvironmentswherecodeiscontinuouslyedited.TheneedforanIntegratedDevelopmentEnvironment
(IDE)thatcannotonlygeneratecodeefficientlybutalsoprovideclear,non-intrusiveexplanationsiscritical.
4.1 Quality
TosystematicallyaccessthesoftwarequalityofautomaticallygeneratedcodebyLLMs,werefertoISO/IEC25010
guidelines.Specifically,ISO/IEC25010includeseightqualitycharacteristics:(1)functionalsuitability(i.e.,functional
completeness,functionalcorrectness,andfunctionalappropriateness),(2)performanceefficiency,(3)compatibility,
(4)usability,(5)reliability,(6)security,(7)maintainability,and(8)portability.Overall,wenoticethatmostofthe
recentresearchfocusesonstudyingfunctionalsuitability[28],usability[138],reliability[115,167],security[113],and
maintainability[92].Fromtheseexistingstudies,wederiveafewobservations.Firstly,despitetherecentadvancement
inLLMslikeChatGPT,LLMsstillgenerallyproducelow-qualitycodebasedonrecentevaluationsthatcoverdifferent
qualitycharacteristics.Secondly,thereexistsarecenttrendofstudiestocovermorequalitycharacteristicsbyproposing
new benchmarks. For example, LMDefects [28] contains LLM-generated programs that are functionally incorrect,
whereasNoFunEvalbenchmark[127]hasbeenrecentlyproposedtoevaluatenon-functionalrequirementsincluding
performanceefficiencyandsecurityandthestudyonthebenchmarkhasnotedthelowperformancesofcodegeneration
modelsinnon-functionalrequirements.Thirdly,existingstudiesrelyontraditionalmetricsforhuman-writtencode
toaccessthequalityofautomaticallygeneratedcodeandthesetraditionalmetricsarestillgenerallyapplicablefor
automaticallygeneratedcode.Forexample,priorstudyreliesonstaticanalysistoolsformeasuringmaintainability[92]
andfoundthatChatGPT-generatedcodesufferfrommaintainabilityissues.
AnexperimentalevaluationofLargeLanguageModelsforcodeappearsin[15,155]andwereferthereadertothese
articles.TheevaluationincludesthePass@1rateforthecodegeneratedfromthesemodels,whichisthepercentage
wherearandomlygeneratedcodesamplepassesallgivenunittests.
DespitemanyrecentstudiesonLLM-generatedcode,wenoticethatqualitycharacteristicssuchascompatibility,and
portabilityarestillunder-explored.Notably,existingstudiesmostlyfocusoncompatibilityissuesrelatedtotestscripts
generation[162]andlibrary-relatedissues[88].Forexample,arecentapproach[88]proposedtoaddawarenessof
third-partylibraryinformationtoimprovetheaccuracyandthereusabilityofthegeneratedcode.Asaddingawareness
ofaqualitycharacteristichaveshownpromisingresultsinimprovingthequalityofgeneratedcode[88],oneviable
solutionwouldbetofusealltheeightqualitycharacteristicsintoLLMtoimprovetheoverallqualityofthegenerated
code.However,assomeofthecharacteristicsmayhaveconflictingrequirements,oneviablesolutionistoguideLLMto
prioritizecertainqualitycharacteristicsfordifferenttasksorapplications.Forexample,assecurecodemaybeless
efficientduetotheadditionalsecuritycheck,weneedtoencodethepriorityforsecurityoverperformanceefficiency
whenusingLLMtogeneratecodeforcertainsafety-criticalsystems.AnotherviablesolutionistodefineasetofAutomaticProgramming:LargeLanguageModelsandBeyond 15
Dimension Explanation
Security ThecodegeneratedbyLLMsshouldnothaveanysecurityvulnerabilities.
Code-specific Reliability LLMgeneratedcodeshouldbefreeofbugs.
Privacy CodeLLMswillnotleakunauthorizedinformation.
Explainability Themodelshouldbeabletoexplainitsrationalofproducingcertaincodeordecision.
Robustness CodeLLMsshouldmaintaintheirperformancesunderdiversenoisyinputs.
Model-specific Consistency Themodels‚Äôoutputsshouldbeconsistentandreproducible.
Fairness Themodelshouldnotproduceanycodeordecisionexhibitingunethicalorunfairbehavior.
Ethics Themodelshouldnotproduceanycodethatintentionallycausesharmtohumanity.
Table1. AttributesofTrustworthinessforCodeLLMs
anti-patterns[132]forvariousqualitycharacteristicsandencodethese‚Äúbadpatches‚ÄùintoLLMsasrulestoimprovethe
qualityofthegeneratedpatches.
WenowdiscussthemorespecificissueoftrustworthinessofLLMgeneratedcode,andwhatitwouldtaketotrustthe
integrationofLLMgeneratedcodeasmethodsintooursoftwareproject.
4.2 TrustworthinessinintegratingLLMgeneratedCode
Inthenearfuture,ensuringtheseamlessintegrationofLLM-generatedcodeintoreal-worldcodebaseswithgreater
reliabilitywillbecrucial.Itisimperativetodelveintotheconceptoftrustworthinessspecificallyconcerningcode
generatedbyLLMsandtodevelopsystematicmethodsforevaluatingthistrustworthiness.Thisexplorationwillnot
onlyinformthefuturedevelopmentofmodelsbutalsoshapetheentireSoftwareEngineeringecosystemsurrounding
them.Byunderstandingandaddressingtheseaspects,wecanpavethewayformorerobustanddependableutilization
ofLLMsincodingapplications.
Tothisend,wereviewedexistingliteratureonthetrustworthinessofsoftware[11,122]andtrustworthinessof
genericLLMs[129].Noneofthemindividuallyissufficientforourpurpose.Drawinguponthisresearch,weidentified
eightprimary attributesessential forevaluating thetrustworthiness of code LLMs,as outlinedin Table1. These
attributescanbebroadlycategorizedintotwomaingroups:(i)thosepertainingtothepropertiesofthegeneratedcode
and(ii)thosebroadlyrelevanttoLLMsbuttheycanbeadoptedforCodeLLMs.Bydelineatingtheseattributes,inthe
future,itwillbeessentialtoestablishacomprehensiveframeworkforassessingthetrustworthinessofcodegenerated
byLLMs,therebyfacilitatinginformeddecisionsregardingtheirutilizationinreal-worldapplications.Inthefollowing
paragraph,wewillelaborateonthis,especiallyforcoderelatedattributes.
Security. GiventhatLLMsaretrainedonavastamountofopen-sourcecodecorpus,itislikelythatthepre-trained
codecorpuscontainsunverifiedcodethatcontainsecurityvulnerabilities.TheLLMslearnfromsuchvulnerableand
exploitableexamples.Thisraisesconcernsaboutthesecurityofthecodeitgenerates.Tocheckthisissue,Pearceet
al.[111]methodicallyexaminetheprevalenceandcircumstancesunderwhichGitHubCopilotmightgenerateinsecure
code.TheiranalysisinvolvespromptingCopilottogeneratecodeinsituationsrelevanttohigh-riskcybersecurity
vulnerabilities,suchasthoseidentifiedinMITRE‚Äôs"Top25"CommonWeaknessEnumeration(CWE)list.Theycameup
with89uniquescenariosforCopilottotackle,resultinginthecreationof1,689programs.Amongthese,approximately
40%werefoundtobevulnerabletoexploitation.Similarobservationswerefoundinotherindependentstudies[8,113].16 Authors
Reliability. CodeLLMstendtoproducesubtletrivialbugsaswell.Infact,Jesseetal.[63]reportedthatCodexand
otherLLMsproduceverbatimsinglestatementbugsuptotwiceasoftenasknown,forverbatimcorrectcode.
Privacy. A substantial amount of code data, which is required to train these models, becomes a hindrance, as
companiesareunderstandablyhesitanttosharesuchsensitivedata.Thisreluctancestemsfromthefearofpotential
leaksofproprietaryinformation,includingsensitiveinformationlikenames,emails,passwords,etc[106].Evenwhen
consideringthird-partyfoundationLLMs,companiesremaincautiousaboutexposingtheircodetoexternalentities.
OneofthecentralchallengesinthiscontextrevolvesaroundharnessingthecapabilitiesofLLMswhileensuringthe
protectionofproprietaryinformation.Strikingabalancebetweenleveragingthepowerofthesemodelsforsoftware
engineeringtasksandsafeguardingsensitivedataposesasignificanthurdlethattheresearchcommunitymustnavigate.
AstheuseofLLMsbecomesmoreprevalent,addressingthesesecurityandprivacyconcernswillbecrucialtorealizing
theirfullpotentialinthefieldofsoftwareengineering.
PotentialRemedy. WhenincorporatingcodegeneratedbyLLMsintoprojects,itiscrucialtoensurethatthegenerated
codeisfreefromobviousvulnerabilities,errors,orleaksofsensitiveinformation.Integratingthechecksaspartofthe
automateddevelopmentprocesswillbeevenmoreimportantformaintainingthesecurityandintegrityofthesoftware.
Toachievethis,weoutlinefewstrategies:
‚Ä¢ Firstly,weshouldprioritizeusinghigh-qualitytrainingdatafortheLLMs.Thisentailstrainingthemodelson
datasetsthatarethoroughlyvettedandfreefromknownvulnerabilitiesorbugs.Bystartingwithcleanand
reliabledata,thelikelihoodoftheLLMgeneratingflawedcodecanbesignificantlyreduced.
‚Ä¢ Additionally,developingandemployinglight-weightstaticanalysistoolscanbeinstrumentalinevaluating
the quality of the code generated by LLMs. These tools can automatically analyze the code for potential
vulnerabilities,syntaxerrors,orotherissueswithouttheneedtoexecutethecode.Byrunningstaticanalysis
ontheLLM-generatedcode,developerscanidentifyandaddressanyissuesbeforeintegratingitintotheir
projects.Notethat,suchstaticanalysistoolsshouldbelight-weightandfastastheyneedtobeintegratedwith
IDEandshouldnothinderdevelopers‚Äôproductivitysignificantly.Thestaticanalysisshouldbeabletoanalyze
evenpartialprograms,asthecodegeneratedintheIDEsmaynotbecomplete.Further,lastmileimprovement
ofcodegeneratedfromLLMs,canbeenabledbyautomatedprogramrepair.
‚Ä¢ ToboostLLMintelligenceandreliability,integratingstep-by-steplogicalreasoningandself-debuggingabilities
iskey.Thisequipsmodelstobettergraspcodecontext,resultinginmoreaccurateoutputs.Self-debuggingem-
powersLLMstoautomaticallyidentifyandfixerrors,potentiallyreducingvulnerabilities.Theseenhancements
enhanceoverallmodelreliability,leadingtohigher-quality,securecode.
‚Ä¢ Lastbutnottheleast,thereexistenticingpossibilitiesofgeneratingverifiedcodewiththehelpofLarge
LanguageModels.Thiscantakemanyforms,including(a)generatingcodefromLLMsandsystematically
improvingittoproduceverifiedcode,or(b)generatingcodeinaverifiedprogramminglanguage.Wenotethat
someeffortsalongtheselineshavealreadybeenstarted,suchas[101]reportingtheLLM-assistedsynthesisof
verifiedDafnymethods.
5 PROGRAMMER-LLMINTERACTION
GiventheincreasingcapabilitiesofAI,particularlyLargeLanguageModels(LLMs),inautomaticprogramming,thereis
asurgeinthedevelopmentandintegrationoftoolsbasedoncode-fluentandLLMstoserveasprogrammingassistants.AutomaticProgramming:LargeLanguageModelsandBeyond 17
ThissectionprovidesanoverviewofhowhumanscanengagewithAImodelsandLargeLanguageModels(LLMs)
forautomaticprogramming.Specifically,wehighlighttwomaincommoninteractionpatterns:Autocompletionand
Prompting.WethendiscussthechallengesthatprogrammersfacewhenleveragingLLMs.
5.1 InteractionPatterns
Autocompletion. AutocompletionforcodereferstotheseamlessintegrationofAImodelsintoanIntegratedDe-
velopmentEnvironment(IDE)withoutrequiringexplicituserinvocation.Generallyspeaking,thistoolcontinuously
queriestheAImodelforcodesuggestionsandpromptlydisplaysthemtotheuser.UsersengagewiththeAImodelby
selectingandvalidatingthegeneratedsuggestions.AnotableexampleisGitHubCopilot[12],whichactsasanAIpair
programmer.Copilottakesprecedingcodecommentsorsourcecode(e.g.,afunctionheaderorpartialimplementation)
asinput.Itthenofferssuggestionstocompletetheremainingimplementationwhenevertheuserpauses.Oneoutstand-
ingadvantageofAI-basedautocompletionisitsabilitytocompletemultiplelinesofcodeinonesuggestion.Thisability
significantlyimprovesusabilitycomparedtotraditionalcompletiontools,whichtypicallysuggestonesubsequent
tokenatatime.Withthisadvantage,programmerscanalsoutilizethetoolasasubstituteforinternetsearches[139].
ThiswouldreducecognitiveloadasprogrammerscanfocusontaskswithintheIDE.
Prompting: InsteadofrelyingonAImodelstoinfertasksfromcodecommentsorprecedingsourcecode,programmers
explicitlyprovidespecializedinputcalledpromptsthatprovideinstructionsonhowtheLLMsshouldgeneratecode.
Withtheinteractionpatternofexplicitinvocationbyprogrammers,therearevariouswaysthatprogrammerscan
interactwiththeAImodels.Forexample,GenLine[66]providesacommand-likeinteractionstylewhereprogrammers
specifyacommand(e.g.,‚Äú[[html: make an OK button]]‚Äù)withinthecodetoinvoketheAImodels.Alternatively,
AImodelscanserveasvirtualcodingassistantswithintheIDE,allowingprogrammerstoprovideinstructionsthrough
adedicateduserinterfacelikeatextbox[74].Withthiskindofinteraction,programmerscanprovidestructured
instructions(e.g.,chain-of-thoughts)asaprompt.ToenablemoreengagementwiththeAImodels,programmerscan
engagewiththeAImodelsviaaconversationalinteractionwhereittakesthepreviousinvocationastheadditional
contextoftheinputprompts[119].Twocommonintentionsofdeveloperstousethesetoolsare1)accelerationand2)
exploration[10].Intheaccelerationmode,programmersintendtousethemodelswhentheyhavespecificprogramming
tasksinmindinmindandleveragetheAImodeltopromptlycompletetheminsteadoftyping.Thetasksaretypically
smallandlogicalsubtasksthatdemandlessanalysisandmorestraightforwardcoding,oftenperceivedasjusttedious
work.Thus,toharnesstheaccelerationpotentialoftheAImodelsincoding,itisessentialtofirstanalyzeanddecompose
thecomplextaskintosmallerlogicalsubtasks.Theexplorationmodeemergeswhenprogrammersencounteranew
problemandareuncertainabouthowtodecomposethetask.Asthetoolcantakecodecommentswhichisanatural
textdescribingprogrammingintenttogeneratesuggestions,programmerstocraftvariouscodecommentsasinputs
andthenexploremultipleimplementationsuggestions.Evenifthesuggestionsarenotentirelycorrect,theymaystill
provideacodeskeletonorstartingpoint[139].Alternatively,intheexplorationmode,programmersuseAI-based
autocompletioninsteadofsearchingforsolutionsontheinternetorStackOverflow[10].
5.2 UsabilityChallenges
WhileLLMshaveshownpromisingresultsincodingassistance,offeringprogrammerspromptcompletionofimple-
mentationsoropeningnewavenuesforexploringalternativeprogrammingsolutions,newchallengesemergewhen
programmersinteractwiththem.18 Authors
Thefirstchallengeliesincraftingtheinput.Themodelstakeanaturallanguagetext(e.g.,codecomments,prompt)
whichdescribesprogrammingintentionsasinputs.SeveralstudiesfoundthattheAImodelsaresensitivetothese
inputs.Aslightdeviationcanresultinsignificantlydifferentcodegeneration[23,139].Hence,programmersmayneed
tospendtimeexploring,crafting,andrevisinginputstogeneratecorrectsolutions[66].Occasionally,programmershad
towriteextensiveanddetaileddescriptionsfortheinputstomakethemodelsgeneratecorrectsolutions.However,this
processmayconsumemoretimecomparedtowritingthecodedirectly[74].WiththecurrentinteractionswithAI
models,theabilitytocreateeffectivepromptsmaypotentiallybecomeanimportantskillinprogramming.
Thesecondchallengecentersaroundunderstandingandvalidatingthegeneratedcode.Sincethecodeisgeneratedby
theAImodels,programmers‚Äômainfocushasshiftedfromprogrammingtoassessingthesuggestions.Unliketraditional
codecompletiontools,whichusuallysuggestonesubsequenttokenatatime,LLMscangeneratealengthysequenceof
tokenstocompletetheentireimplementation.Consequently,understandingthegeneratedcodeandvalidatingwhether
italignswithprogrammingintentionscoulddemandconsiderabletimeandcognitiveload[66,133].Barkeetal.[10]
foundthatprogrammerstendtolookforthepresenceofcertainkeywordsorcontrolstructurestoquicklyvalidate
suggestions.Theymayalsoexecutethecodeorrunastaticanalyzertohelpthemvalidatethesuggestions.Hereinalso
liesourhypothesisthatwiththearrivalofLLM-basedcoding,thenatureofprogramcomprehensionactivityislikelyto
shiftfrommanualcodecomprehensiontoaniterativedialoguewithLLMs.Thefirststepofsuchaniterativedialogue
isofcourseavalidationordisambiguationofartifactsproducedbyLLMs.
Thethirdchallengeinvolvesdebuggingandfixingthegeneratedcode.Eventhoughprogrammerscanunderstandthe
generatedcode,itmightstillrequirefixingorimprovement.However,theAImodelmaygeneratecomplexcodethatis
difficulttodebug[10].Programmersalsoneedtoconsiderthetimerequiredindebuggingandfixingthegenerated
code;otherwise,theymightgetstuckinatime-consumingdebuggingprocess[139].Additionally,constantcontext
switchingbetweenprogramminganddebuggingmodescanimposesignificantmentaldemandsonprogrammers.
5.3 LLMsforMaintenance&Evolution
LLMandAI-basedcodemodelshavedemonstratedsignificantadvancementsinexpeditingcodingwithinautomatic
programming.Itiscrucial,however,thatautomaticprogrammingnotonlyfocusesonacceleratingthecodingprocess
butalsocontributestosoftwaremaintenanceandpromotesfutureevolution.Considerableefforthasbeendevoted
todevelopingapproachesforLLMstoachievethisgoalinvariousways.Inthissection,wewilldiscussAI-based
approacheswheresourcecodeistakenasinputtogeneratenon-sourcecodeartifactsthatfacilitatemaintenance
andevolution.Specifically,wefocusonthreemaintasksthatarecloselyrelatedtotheprogrammingtask,i.e.,code
summarization,codechangesummarization,andcodereview.
CodeSummarization. Codesummarizationreferstothesummarizingofthebehaviourorpurposeoftheprovided
codesnippets[3,150].Thisisparticularlyusefulfordeveloperswhentheyneedtounderstandthesourcecode,especially
thecodetheyhavenotwrittenthemselves.RecentLLMslikeGPT,Codex,CodeT5,CodeBERT,UniXCoderhavebeen
investigatedforcodesummarizationpurposesasthesemodelsweretrainedwithmultimodaldata[4,7,39,47,143].
Thus,themodelscangeneratenaturallanguagedescriptionsfromsourcecode.Somestudiesalsofoundthatthe
performanceofLLMscanbeimprovedwhenthemodelslearnfew-shotexemplars(a.k.ain-contextlearning)[4,39].
CodeChangeSummarization. Codechangesummarizationreferstotheprocessofsummarizingacollectionofcode
changes(e.g.,commitsorpullrequests)madetothecodebase.Itinvolvesdescribinganoverviewandpurposeofthe
changes.ThistaskiscrucialfordevelopersasithelpsdevelopersandotherstakeholdersunderstandandkeeptrackofAutomaticProgramming:LargeLanguageModelsandBeyond 19
theevolutionofthecode,improvingtheunderstandabilityofthecodeandfacilitatingthedebuggingprocess.TheAI
modelshaveshownpromisingresultstogeneratebothdescription[72,93,104]anditstitle[60].Recentstudiesalso
havedemonstratedthecapabilityofLLMs,e.g.,ChatGPTandGitHubCopiliot[153]toperformthesesummarization
tasks.
AutomatedCode Review. AutomatedCode review refersto the processof automatically analyzing source code
andprovidingfeedbacktoadheretocodingstandardsandbestpractices.Thefocuscancovervariousaspectsof
codequalitysuchascodestyle,formatting,performance,security,andmaintainability.Automatedcodereviewcan
helpdeveloperscatchissuesearlyinthedevelopmentprocess,improvecodeconsistencyacrossprojects,andensure
thatcodemeetsqualitystandards.Severalrecentworkshaveshownthatvarioussub-tasksofcodereviewcanbe
automated.Thisincludesestimatingthequalityofcode[87],suggestingcoderefinement[134,136],generatingreview
comment[80,87,136],andsuggestingreviewcommentresolution[36,87,136,137].WhilemostoftheAImodels
forcodereviewsweretrainedwithcodereviewdatasets,ChatGPTalsohasrecentlyshownpromisingresultsfor
performingcodereviewtasks[52,135].
Summary. Asdiscussed,researchhasdemonstratedthatLLMsandAImodelscanassistdevelopersinenhancingthe
maintenanceandevolutionoftheirhuman-writtencode.Thispavesanewdirectionforfurtherimprovingautomatic
programmingtechniquestogeneratecodethatmeetsthenon-functionalqualityformaintenanceandevolution.For
instance,employingsummarizationtechniquestoautomaticallydescribethebehaviororthepurposesofthegenerated
codetoaiddevelopercomprehension.Furthermore,automatingcodereviewtechniquescanbebeneficialinassessing
thequalityofthegeneratedcode.
6 ENHANCEMENTSOFAUTO-GENERATEDCODING
LLMsarenotjustcodingassistants;theyhaveevolvedtobecomeversatilepartnersinthesoftwaredevelopment
process.However,despitethesignificantstridesmadebycurrentLLMs,thejourneytowardtheirfullintegrationinto
real-worldsoftwaredevelopmentisstilllinedwithchallenges.The‚Äúlastmile‚Äùofenhancementiscrucialfortheseamless
applicationofLLMsinpracticalprogrammingendeavors.AsweexplorethefutureprogressionofLLMsinautomatic
programming,ourroadmapencompassesseveralpivotalareasofdevelopment,aimedatunleashingtheseintelligent
systemstotheirutmostpotential.
Multi-modalcoding. Thefirstareaismulti-modalcoding.Currently,codeLLMsarelimitedtohandlingtextual
data.However,itiscrucialtorecognizethatdevelopersoftenworkwithmulti-modaldataduringthedevelopment
process.Forexample,ThegenerationofsoftwareUIfromimagesandvideosrequiresLLMstoanalyzevisualelements,
understandtheircontext,andtransformthemintocode.ThiscapabilitywouldempowerdeveloperstostreamlinetheUI
designprocessbysimplyprovidingvisualexamplesorprototypesandqueryLLMstogeneratethecorrespondingcode
automatically.InadditiontoUIgeneration,multi-modalcodinghasbroaderimplicationsforsoftwaredevelopment.
Considertheuseoffigures,tables,andflowchartsintherequirementanddesignphases.LLMsequippedwithmulti-
modal capabilities could analyze these visual representations and convert them into code snippets automatically.
Moreover,multi-modalcodingwouldenhancetheinteractionbetweendevelopersandAImodels.Developerscould
foster better collaboration with AI models by communicating their ideas and requirements using multi-modality
information.Thiswouldenableamorenaturalandintuitiveinteraction.Theintegrationofmulti-modalcodingin20 Authors
LLMshasthepotentialtorevolutionizethesoftwaredevelopmentprocess.Bybridgingthegapbetweenvisualdesign
andcodeimplementation,LLMscansignificantlyimproveproductivity,codequality,andtheoveralluserexperience.
Domains. Secondly,wefocusontheempowermentoflarge-scaledomain-specificsoftware.Inpracticaldevelopment
andmaintenanceprocesses,developersoftenencounterlarge-scaleprojectsthatrequirediversedomainknowledge.
ThisnecessitatescustomizingLLMstoeffectivelymanageandnavigatethecomplexitiesoftheseprojects.Software
developersfrequentlygrapplewithintricatesoftwaredevelopmentchallengeswithinspecificbusinessandtechnology
domains,suchase-commerceandautomotive.GeneratingcodeforsuchsoftwaredemandsthatAImodelscomprehend
variousdomain-specificconcepts.ByeffectivelyincorporatingspecializeddomainknowledgeintoLLMs,thesemodels
canprovidedeveloperswithevenmoreaccurateandrelevantsupport.However,handlinglarge-scaleprojectspresents
anadditionalchallenge.ThecurrentlimitationsincontextlengthmakeitdifficultforLLMstoprocesscodewithin
large-scalesoftwareprojects.Evenwithlongercontext,comprehendingandlocatingessentialinformationwithinsuch
extensivecoderemainsachallenge[91].OvercomingtheselimitationsiscrucialtomaximizethepotentialofLLMsand
ensuretheycaneffectivelymeetthedemandsofcomplexprojectsinpracticaluse.
KnowledgeUpdate. ThethirdstrategicareainvolvestheknowledgerepairandupdatingcapabilitiesofLLMs.LLMs
arerenownedfortheirlargemodelsize.Forinstance,GPT-3has175millionparameters,requiringaninvestmentof
approximately4.6milliondollarsintrainingandemitting552tonsofcarbondioxide,equivalenttotheemissionsof123
gasoline-poweredpassengervehiclesdrivenforoneyear[110].Nonetheless,theevolutionofAPIsandprogramming
introducesacontinuousstreamofnewknowledge,essentialforprovidingup-to-dateservicestodevelopers.Moreover,
duringthemaintenanceprocessofcoderepositories,pre-trainedmodelscanunintentionallyencounterincorrect
informationthatwaspreviouslyundiscovered.Thiscanoccurwhenthetrainingcodecontainsundetectedbuggy
codes,leadingthemodeltolearnandpotentiallyincorporateinaccurateknowledge.Consequently,thequalityofthe
generatedcodemaybealsodegraded.Therefore,effectivelyeditingtheknowledgeoflargegenerativeAImodels,rather
thanresortingtoperiodicretrainingofthesemodelsfromscratch,representsasignificantandrelativelyunexplored
researcharea.
ReliabilityandProgramRepair. Thefourthfocusareaisthequalityandreliabilityassuranceforthecontentgenerated
byLLMs.Despitelanguagemodels‚Äôproficiencyincodegeneration,theirinherentblack-boxnatureraisesconcerns
aboutthecorrectnessofthegeneratedcode.Theincreasingrelianceonautomatedprogrammingunderscorestheneed
foroutputthatmeetsthehigheststandardsofquality.Thispursuitisnotlimitedtotheaccuracyofthecode;itextends
toensuringthecode‚Äôsmaintainability,performance,andscalability.Therefore,enhancingthereliabilityofthecodeand
creatingautomatedmethodstoassessandverifythequalityoftheLLM-generatedcodeisofvitalimportance.
Overall,wewouldliketomakethefollowingtwoprojections:
‚Ä¢ Thereisaplaceforlastmilerepairofauto-generatedcodeusingautomatedprogramrepairtechniques[29].
‚Ä¢ Thereremainstheenticingpossibilityofthelastmilerepairofauto-generatedcodeprovidingevidenceof
correctnessofthe‚Äúimproved‚Äùcode.Thisevidenceofcorrectnessmaybeintheformofatestsuitewhichis
generatedasaby-productoftheautomatedprogramrepairprocess.Wenotethatsuchtestsuitesgeneratedin
theliteratureasaby-productofautomatedprogramrepairhavebeenstudied[124].
Security. Thefifthareaofimprovementinvolvessecurityalignment,whichisacrucialcomponentofthe‚Äúlastmile‚Äù
ofenhancement.ItisimperativetoaddresstrustissuesthatariseduetothegenerationofsensitiveorinsecurecontentAutomaticProgramming:LargeLanguageModelsandBeyond 21
andpotentialprivacyrisks.Oneoftheprimaryconcernsisthegenerationofinsecurecodethatcontainsvulnerabilities,
whichcouldleadtothecrashofsoftware.Additionally,privacyprotectionisofutmostimportancewhenutilizingcode
generatedbyLLMs.Thevastamountofdataandinformationprocessedbythesemodelsraisesconcernsaboutthe
handlingandstorageofsensitiveuserdata.Users‚ÄôprivacymustbesafeguardedtoensurethatLLMsdonotinadvertently
leakormisusepersonalinformation.Thepreventionofharmfulcontentisnotonlyatechnicalchallenge,butalso
anethicalissue,ensuringthatthesepowerfultoolscontributepositivelytothesoftwaredevelopmentcommunity.
Therefore,theobjectiveofsecurityalignmentistodesignLLMsinawaythatavoidsgeneratingpotentiallyharmful
orinsecurecontent,thusimprovingthetrustworthinessofLLMsandfacilitatingtheirwidespreadadoption.Wenote
thatboththereliabilityandsecurityoftheLLMgeneratedcodearefundamentaltothetrustworthinessofLLMoutput
examinedinSection4.2.
Datasets. Finally,asdifferentLLMsaretrainedusingdifferentbenchmarks,thepreparationofhigh-qualityand
multidimensionaldatasetsisthekeytoafairevaluationofthecodegeneratedautomatically.Ingeneral,current
benchmarksfocusmainlyonhighlightingthelimitationsofthecodegeneratedbyLLMs.Weforeseethatthenext
milestoneofLLMsistogeneratemorecomplexcodeandtoresolvemorecomplexGitHubissues.Withtheevolution
andincreasingcapabilityofLLMs,weforeseenewerbenchmarksthatfocusonnewercapabilities(e.g.,generatingcode
frommulti-modalinputs)ornewerdomains(e.g.,autonomousdevices).
Inconclusion,theroadmapforenhancingLargeLanguageModelsinautomatedprogrammingisbothambitiousand
essential.Byaddressingthesesixkeyperspectives,wecananticipateafuturewhereLLMsarenotonlymorecapable
butalsomorealignedwiththenuancedandevolvingneedsofsoftwaredevelopment.
7 DATASETS
Weperformaliteraturereviewoftheavailabledatasetsforevaluatingandstudyingcodegenerationmodels.Specifically,
westartedbydoingapreliminarysearchwithkeyword‚Äúcodegenerationdataset‚ÄùonGoogleScholar,weselected
allrelevantpapersandthentracedotherrelatedworkusingabackwardsnowballingapproach.Wefurtherfilter
benchmarksthatarecollectionofmultipledatasets(e.g.,CodeXGLUE[96])astheircharacteristicswillbecapturedby
theoriginaldatasetsinwhichthecollectionhasbeenderivedfrom.
Table2showstheexistingdatasetsforcodegeneration.Amongexistingdatasets,HumanEval[18]andMBPP[9]
areoneoftheearliestbenchmarksinwhichnewerbenchmarkshavebeenderivedfrom(e.g.,MultiPL-E[13]extends
HumanEvalandMBPPbysupportingmoreprogramminglanguages).TheHumanEvalbenchmarkcontainsmanually
writtenproblemsinwhichtheOpenAICodexmodelhasbeenevaluated.Recently,SWE-Bench[70]wasproposed
toevaluatewhetherLLMscanbeusedtoautomaticallyresolvereal-worldGitHubissues.Basedonthereported
findingsofstudiesconductedinthesedatasets,wenoticethatmostofthesedatasetsshowsthelimitationsofexisting
LLMsinsolvingcode-relatedtasks,highlightingtheneedsforrevolutionarytechniquesthatcanfurtherimprovethese
LLMs.Forexample,theevaluationonSWE-Bench[70]showsthattheirfine-tunedmodelSWE-Llamacanresolve
onlythesimplestGitHubissues.WenotethattheSWE-benchisgainingattentionfrompractitionerswhoattemptto
automatesoftwareengineeringbeyondasingleprompt.AveryrecentstartupeffortcalledDevin[77]reportsreasonable
efficacyonSWEbenchinautonomouslyfixingGitHubissues(bugfixesandfeatureadditions).Theopen-sourceagent
AutoCodeRover[164]reportshigherefficacythanDevin,byconsideringcodestructureinlocalizationandfixing.
Ingeneral,weobservethatmostexistingdatasetsrequireseveralsoftwareartifacts:(1)naturallanguagedescriptions
(mostlyEnglish-centric),(2)codewritteninacommonly-usedprogramminglanguages(mostlyfocusonPythonand22 Authors
Table2. DatasetsforCodeGeneration
Benchmark Natural Lan- Programming Supported Size Testcase UniqueFeatures
guages Languages Tasks
APPS[56] English Python Text-code 10,000 prob- 130,000 total One of the earlier
lems testcases dataset with crowd-
sourced questions for
programsynthesis
HumanEval[18] English Python Text-code 164problems Average 7.7 Handwrittenproblems
tests per prob- to evaluate functional
lem correctness and mea-
sure problem-solving
capabilities
MBPP[9] English Python Text-code 974 python 3testcasesfor Measuretheabilityof
functions eachproblem thesemodelstosynthe-
sizeshortPythonpro-
grams
CONCODE[61] English Java Text-code 100,000(classes, Notest Classesfromdiversedo-
NL,code)tuples mains
PandasEval,NumpyEval[165] English Python Text-code 101 program- 20testsforeach Library-oriented code
mingproblems problem generation
MCoNaLa[144] Spanish,Japan- Python Text-code 896 NL-Code Notest Supportseveralnatural
ese, and Rus- pairs languagesbeyondEng-
sian lish
LLMDefects[28] English Java Text&code- 113 program- 1-3publictests Contains mistakes
code ming tasks for each prob- in code generated by
from recent lem LLMs.
contests, 335
incorrect solu-
tions
ClassEval[26] English Python Text-code 100tasks Contain Class-levelcodegener-
method-level ation
and class-level
tests
AixBench[54] English, Chi- Java Text-code 175samplesfor Contain hand- Contain hand-crafted
nese automatedTest, craftedtests automatedtestcases
161NLTaskDe-
scription
MultiPL-E[13] English 19 languages Text-code 161 problems Use tests from Extend Hu-
(e.g., Julia, from Hu- prior bench- manEval [18] and
Swift) manEval [18], marks[9,18] MBPP [9] to 18 lan-
974 from guages by translating
MBPP[9] programsandunittests
SWE-Bench[70] English Python Text&code- 2294 prob- Average 120.8 Evaluate the ability
code lems from 12 total tests for to resolve real-world
projects eachproblem GitHub
CodeScope[159] English 43languages 8tasks 200‚Äì5,382sam- Contain tests Evaluate generated
ples for each forsometasks code on difficulty,
task efficiency,andlength
NoFunEval[127] English Python, Java, Text&code- 47‚Äì397samples Notest Evaluate non-
C, JavaScript, code, classify foreachtask functional require-
Kotlin correctness ments(latency,security,
efficiency)
LiveCodeBench[62] English Python Text&code- Collect new Use tests from Mitigatecontamination
code problems over programming issuesbycrawlingnew
time problems or problems
LLM-generated
tests
Java),(3)testcasestoverifythecorrectnessofthegeneratedprograms.Asshowninthe‚ÄúSupportedTasks‚Äùcolumn,
mostexistingdatasetssupporttext-to-codetasks(codegenerationfromnaturallanguagedescription).AutomaticProgramming:LargeLanguageModelsandBeyond 23
(a) (b)
Fig.5. EvolutionofprogrammerrolescapturedbyDALL-E;programmerrole(a)ascodecomposeranddesignerinsteadofcodewriter,
(b)asqualityassurancespecialistinsteadofcodewriter.
Althoughsomebenchmarks[61,144]usetextualsimilaritybetweenthegroundtruthprogramandthegenerated
programforvalidatingthecorrectness,the‚ÄúTestcase‚ÄùcolumnofTable2showsthatmostbenchmarksrelyontest
casesforvalidatingthecorrectnessofgeneratedprograms.Thesetestcasesareeither(1)hand-craftedor(2)translated
fromotherprogramminglanguages.Overall,weobservethatatest-drivenapproachhasbeenwidelyusedforvalidating
thecorrectnessofthegeneratedprograms.Thisindicatestheimportanceofimprovingthequalityofthetestsuitesused
forguidingthecodegeneration.Basedonthecolumn‚ÄúUniqueFeatures‚Äù,weobservethatrecentdatasetstypicallyadd
anewdimensiontostudytheeffectivenessofLLMsunderaspecificcondition(e.g.supportingdiversesetsofnatural
languages[144],studyingdefectsinautomaticallygeneratedcode[28]).Investigatingthediverseperspectivesofcode
generationmodelshelpstopointoutthelimitationsandthepotentialbiasoftheLLMs.
AsmostLLMsaretrainedusingprogramsfromopen-sourcerepositories,oneofthekeychallengesofadataset
forevaluatingtheeffectivenessofcodegenerationforLLMsisthedataleakageproblem(e.g.,overfittingthetraining
data).Existingdatasetsusuallysolvethisbyusing(1)handwritten[18]orcrowd-sourcedproblems[56],or(2)recently
publishedproblems[28].Recently,LiveCodeBench[62]hasbeenproposedtomitigatedataleakage(knownasthe
contaminationprobleminthearticle)bycontinuouslycrawlingnewproblemsfromprogrammingcontestplatforms
(LeetCode,AtCoder,andCodeForces).
8 FUTURE:PROGRAMMINGENVIRONMENTOF2030-35ANDBEYOND
Intheprogrammingenvironmentof2030-35whereLLM-basedauto-programmingtechniqueshavereachedcertain
levelofmaturity,programmersmayneedtoswitchtodifferentrolestofullyutilizethepowerofauto-programming.
Programmer as code composer and designer instead of code writer. With the advancement of LLM-based auto-
programming,manysoftwaremaintenancetasksthatrequirecodewritingcanbeautomaticallysolvedbyinvokingthe
appropriateLLMs.Figure5(a)showsanAI-generatedpicturebyImageCreatorthatusesDALL¬∑Ewhereprogrammers
actsascodecomposeranddesignerinsteadofcodewriter.Insteadofplayingthetraditionalroleofaprogrammerwho
meticulouslywritescodeforsolvingdifferenttasks,theycanfocusontasksthatrequirehigh-levelunderstandingof
therequirements(e.g.,designingtheoverallstructureoftheprogramandtentativealgorithms),allowingautomated24 Authors
Specification in
Natural Language Test-suite
Github Copilot Auto-generated Automated
or similar tools Code Program
Repair
Evidence Trustworthy Code
Evidence generated from Program Repair
Steering Search Specification Inference
Fig.6. Automatedrepairofauto-generatedcode
toolstoselectthemosteffectivemodelfortherelevantmaintenancetasksinwhichrelevantcodewillbeautomatically
generated.AscurrenttechniquesmainlyfocusonspecializingLLMsforaspecificdownstreamtaskofauto-coding(e.g.,
programrepair,logstatement,testgeneration)toimprovetheeffectivenessforthegiventask,afutureprogramming
environmentwillintelligentlypredictandselecttheappropriatemodeltoinvokebasedthecontextofthedownstream
task.TherearetwoscenariosinwhichLLM-basedauto-programmingcanchangethefutureprogrammingenvironment:
(1)inanIntegratedDevelopmentEnvironment(IDE)setting,(2)inacontinuousintegration(CI)workflow.Forexample,
intheIDEsettingthatrequiresinstantfeedbackfromtheauto-codingtoolforefficientinteraction,futuretechniques
candesignalightweighttoolthatcanautomaticallycompleteandsuggestrelevantcodesnippetsbasedonthe(1)
currentsurroundingcodeand(2)thelistofavailabletasks(e.g.,suggestaddingaJUnittestforthenewlywrittenJava
methodoraddingalogstatementbeforeagracefulexitofaprogram).Meanwhile,inaCIworkflow,certainevent
thatrepresentsabnormalbehaviorofasoftwaresystem(e.g.,testfailures,buildfailures)canautomaticallytriggerthe
needforasoftwaremaintenancetask(e.g.,theneedforarepaircanbetriggeredafteratestfailure).Inthisscenario,
moresophisticatedtechniquescanbeusedtofurtherconfirmthevalidityofthetrigger(e.g.,todistinguishbetween
testfailureorflakytest).Thesetechniquesincludeprogramanalysistechniques(suchassymbolicexecution),test
generationtechniques(basedoncodechangeswithinacommit),andloganalysis(programmonitoring).
Programmer as quality assurance specialist. Although many tasks can be automated, we foresee that concerns
regardingthequalityoftheautogeneratedcodestillremain.Assomeoftheautogeneratedcodecanbemisaligned
withtheintentionoftheprogrammer,theprogrammerwillneedtoplaytheroleofaqualityassurancespecialistand
spendmoretimeincheckingthevalidityofthegeneratedcode.Apartfromusingtraditionaltestingandstaticanalysis
tools,morespecializedautomatedprogramrepairtechniquescanbedesigned(e.g.,byreferringtopriorstudy[28]
thatinvestigatedthemistakesofauto-generatedcode)toreducethetimeandeffortinvolvingincheckingthequality
ofauto-generatedcode.Figure5(b)showsanAI-generatedpictureforprogrammermainroleasqualityassurance
specialist.Figure6showsaschematicthatconcretizesthislast-mileimprovementofautogeneratedcodeùëÉ producedAutomaticProgramming:LargeLanguageModelsandBeyond 25
fromnaturallanguagedescriptionusingatoollikeCopilot.Theautogeneratedcodemaybesubjecttoprogramrepair
guidedbyagiventestsuiteùëá.However,theprocessofrepairinspectsorexamines(eitherexplicitlyorimplicitly)a
domainofprogramedits‚ÄîtryingtoshrinkthespaceofcandidateeditswhicharesuitableforimprovingprogramùëÉ.In
thisway,arepairedprogramùëÉ‚Ä≤isgeneratedfromùëÉ.Intheprocessofexaminingthedomainofprogramedits(and
presumablyrulingoutalotofcandidateedits),theprogramrepairprocessgeneratesmanyadditionaltestsùëá‚Ä≤overand
abovethetestsuiteùëá whichwasusedtoguidetheprogramrepairprocess.Theoracle(orexpectedbehavior)ofthese
additionaltestsùëá‚Ä≤canbeobtainedviasomeprocessingofthenaturallanguagedescriptionfromwhichùëÉisderived.The
additionaltestinputsùëá‚Ä≤(alongwiththeiroracles)canthenserveasevidenceof"correctness"oftherepairedprogram
ùëÉ‚Ä≤.Weenvisionthatcode-generatorsofthefuturewillnotbeonlyLLMs,butLLMagentsaugmentedwithprogram
analysis/repaircapabilities.Theseaugmentedcodegeneratorsmaythentrytocommitcodelikehumanprogrammers,
whilesubmittingevidenceintheformofgeneratedtestsùëá‚Ä≤asevidenceofcorrectnessof(LLM-induced)codecommits.
Programmer-assisted safe auto-coding. We envision that Large Language Model (LLM) generated code may be
integratedintolegacycode-basesofexistingsoftwareprojects.Thiscouldbeintheformoflibrariesperformingspecific
tasks,wherethelibrarycodeisgeneratedwithLLM.ForsafeintegrationofsuchLLMgeneratedcodeinhuman-written
softwareprojects,onemayneedsanitizercode(e.g.[123])sothattheLLMgeneratedcodecanbeusedbythebigger
softwareprojectsafely.Untilwereachthestageofcompletelyautomatingthegenerationofentiresoftwareprojects,
theremaybeaneedtostudy(a)automatedrepairorimprovementofLLMgeneratedcode,or(b)executingLLM
generatedcodeinacontainedmannersothatitcancauselimitedharmtotherestofthesoftwaresystem,or(c)generate
verifiedLLMcodewheneverappropriateformalspecificationsareavailable.OnefirststeptowardsverifiedLLMcode
canbetogeneratecodeinaprogramminglanguagesupportingverification(e.g.,see[101]).Wecouldalsogenerateboth
programsandproofs(abouttheprogramsatisfyingsomeformalproperties)fromLLMs.Suchformalpropertiesmaybe
obtainedfromnaturallanguage,inwhichthereissomework[20].AutomatedgenerationofproofsfromLLMshasalso
beenrecentlystudied[32].AlloftheseworksprovideimpetusinmovingtowardshigherassurancecodefromLLMs.
AutonomousProgramImprovement. WeviewtheapproachofprogramrepaironunsafecodegeneratedbyLLMsto
beamoreflexibleapproachforautomaticallygeneratingsafecode(asitisbasedoncodetransformations),ascompared
totuningorrestrictionofLLMstogeneratesafeoutputs.Movingforward,researcherscanexaminethislineofwork,in
additiontosignificantshort-termeffortsinpromptengineeringandLLMtuning.Repairoftheautomaticallygenerated
codebasedontestsgivesusmoreflexibility,partlybecausewecanalsochoosethetestsweusetoguidetheprogram
repair.TherepairaswellasothertaskslikefeatureadditioncanbeachievedautonomouslybyLLMagentswhichare
awareofthestructureofthecode.TherecentworkonAutoCodeRover[164]isanexampleworkinthisdirection.In
thenearfuture,thefocuswillbetoimprovetheefficacyoftheseagents.Thecombinationofauto-codingfromnatural
languageandautonomoussoftwareimprovementusingLLMs,isanenticingpossibilitywhichcanbeachievedby2030.
Thiswouldshifttheroleofafuturesoftwareengineertowardsachievingassuredautonomybyfocusingontrustofthe
autonomousartifacts,insteadofengineeringsoftwaresystemsatscale.Thescaleofsoftwaresystemsislikelytobe
achievedautomaticallyinfuture,thusshiftingtheattentiontotrust.
Lookingevenfurther. Autonomousimprovementofautomaticallygeneratedcodeneednotberestrictedtoapplication
levelprogramming.Wecouldexaminethefeasibilityofautomaticallyrepairingprobabilisticprograms,whichare
generatedautomatically.Probabilisticprogrammingsuccinctlyexpressesstatisticalinferencetasksonprobabilistic
models[43],andaresupportedintheback-endbymachinelearningframeworkslikePytorch[109].Recentlysymbolic
executionofprobabilisticprogramswasproposed[140]whichraisesthepossibilityofsemantics-awareprobabilistic26 Authors
programrepair,afteraninitialLLMguidedautomatedgenerationofprobabilisticprogramsnippets.Thislineofwork
couldhelpusprogresstowardsautomatedselfimprovementoflearningtasks,aspeculativedirectionoffutureresearch.
ACKNOWLEDGMENTS
ThisworkispartiallysupportedbyaSingaporeMinistryofEducation(MoE)Tier3grantMOE-MOET32021-0001.
The authors thank Prem Devanbu for his valuable comments about the article. The corresponding author Abhik
RoychoudhurywouldliketothankXiangGaoandMartinMirchevforcontributingsomeexampleprogramstoillustrate
theissueswithAIbasedcoding.
REFERENCES
[1] 2021. ApplicationCompatibilityToolkit(ACT). https://learn.microsoft.com/en-us/windows/win32/win7appqual/application-
compatibility-toolkit--act-.
[2] WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2021.UnifiedPre-trainingforProgramUnderstandingandGeneration.
InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,
NAACL-HLT2021,Online,June6-11,2021.AssociationforComputationalLinguistics,2655‚Äì2668.
[3] WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2020.ATransformer-basedApproachforSourceCodeSummarization.
(52020). http://arxiv.org/abs/2005.00653
[4] ToufiqueAhmed,KunalSureshPai,PremkumarDevanbu,andEarlT.Barr.2023.AutomaticSemanticAugmentationofLanguageModelPrompts
(forCodeSummarization).(42023). http://arxiv.org/abs/2304.06815
[5] RajeevAluretal.2018.Search-basedProgramSynthesis.Commun.ACM61,12(2018).
[6] RajeevAlur,RastislavBodik,GarvitJuniwal,MiloMKMartin,MukundRaghothaman,SanjitASeshia,RishabhSingh,ArmandoSolar-Lezama,
EminaTorlak,andAbhishekUdupa.2013.Syntax-guidedsynthesis.IEEE.
[7] ShushanArakelyan,RocktimJyotiDas,YiMao,andXiangRen.2023.ExploringDistributionalShiftsinLargeLanguageModelsforCodeAnalysis.
(32023). http://arxiv.org/abs/2303.09128
[8] OwuraAsare,MeiyappanNagappan,andNAsokan.2023.Isgithub‚Äôscopilotasbadashumansatintroducingvulnerabilitiesincode?Empirical
SoftwareEngineering28,6(2023),129.
[9] JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,DavidDohan,EllenJiang,CarrieCai,MichaelTerry,QuocLe,
etal.2021.Programsynthesiswithlargelanguagemodels.arXivpreprintarXiv:2108.07732(2021).
[10] ShraddhaBarke,MichaelB.James,andNadiaPolikarpova.2023.GroundedCopilot:HowProgrammersInteractwithCode-GeneratingModels.
ProceedingsoftheACMonProgrammingLanguages7(42023).IssueOOPSLA1. https://doi.org/10.1145/3586030
[11] SteffenBecker,WilhelmHasselbring,AlexandraPaul,MarkoBoskovic,HeikoKoziolek,JanPloski,AbhishekDhama,HenrikLipskoch,Matthias
Rohr,DanielWinteler,etal.2006. Trustworthysoftwaresystems:adiscussionofbasicconceptsandterminology. ACMSIGSOFTSoftware
EngineeringNotes31,6(2006),1‚Äì18.
[12] ChristianBird,DenaeFord,ThomasZimmermann,NicoleForsgren,EiriniKalliamvakou,TravisLowdermilk,andIdanGazit.2023.TakingFlight
withCopilot:EarlyinsightsandopportunitiesofAI-poweredpair-programmingtools.Queue20(2023),35‚Äì57.Issue6.
[13] F.Cassano,J.Gouwar,D.Nguyen,S.Nguyen,L.Phipps-Costin,D.Pinckney,M.Yee,Y.Zi,C.Anderson,M.Q.Feldman,A.Guha,M.Greenberg,
andA.Jangda.2023.MultiPL-E:AScalableandPolyglotApproachtoBenchmarkingNeuralCodeGeneration.IEEETransactionsonSoftware
Engineering49,07(jul2023),3675‚Äì3691. https://doi.org/10.1109/TSE.2023.3267446
[14] ChatGPT.2022.ChatGPT.https://chat.openai.com/.
[15] MChenetal.2021.EvaluatingLargeLanguageModelsTrainedonCode.arxiv(2021).
[16] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,JaredKaplan,HarrisonEdwards,YuriBurda,Nicholas
Joseph,GregBrockman,AlexRay,RaulPuri,GretchenKrueger,MichaelPetrov,HeidyKhlaaf,GirishSastry,PamelaMishkin,BrookeChan,Scott
Gray,NickRyder,MikhailPavlov,AletheaPower,LukaszKaiser,MohammadBavarian,ClemensWinter,PhilippeTillet,FelipePetroskiSuch,Dave
Cummings,MatthiasPlappert,FotiosChantzis,ElizabethBarnes,ArielHerbert-Voss,WilliamHebgenGuss,AlexNichol,AlexPaino,Nikolas
Tezak,JieTang,IgorBabuschkin,SuchirBalaji,ShantanuJain,WilliamSaunders,ChristopherHesse,AndrewN.Carr,JanLeike,JoshuaAchiam,
VedantMisra,EvanMorikawa,AlecRadford,MatthewKnight,MilesBrundage,MiraMurati,KatieMayer,PeterWelinder,BobMcGrew,Dario
Amodei,SamMcCandlish,IlyaSutskever,andWojciechZaremba.2021.EvaluatingLargeLanguageModelsTrainedonCode.CoRRabs/2107.03374
(2021).arXiv:2107.03374 https://arxiv.org/abs/2107.03374
[17] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,JaredKaplan,HarriEdwards,YuriBurda,Nicholas
Joseph,GregBrockman,etal.2021.EvaluatingLargeLanguageModelsTrainedonCode.CoRRabs/2107.03374(2021).
[18] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,JaredKaplan,HarriEdwards,YuriBurda,Nicholas
Joseph,GregBrockman,etal.2021.Evaluatinglargelanguagemodelstrainedoncode.arXivpreprintarXiv:2107.03374(2021).AutomaticProgramming:LargeLanguageModelsandBeyond 27
[19] ZiminChen,SteveKommrusch,MicheleTufano,Louis-No√´lPouchet,DenysPoshyvanyk,andMartinMonperrus.2021.SequenceR:Sequence-to-
SequenceLearningforEnd-to-EndProgramRepair.IEEETrans.SoftwareEng.47,9(2021),1943‚Äì1959.
[20] Translatingcodecommentstoprocedurespecifications.2018.AriannaBlasiandAlbertoGoffiandKonstantinKuznetsovandAlessandraGorla
andMichaelD.ErnstandMauroPezz√®andSergioDelgadoCastellanos.InInternationalSymposiumonSoftwareTestingandAnalysis(ISSTA).
[21] KalyanmoyDeb,SamirAgrawal,AmritPratap,andT.Meyarivan.2002.Afastandelitistmultiobjectivegeneticalgorithm:NSGA-II.IEEETrans.
Evol.Comput.6,2(2002),182‚Äì197.
[22] DeepSeek.2023.Deepseekcoder:Letthecodewriteitself.https://github.com/deepseek-ai/DeepSeek-Coder.
[23] PaulDenny,VirajKumar,andNasserGiacaman.2022.ConversingwithCopilot:ExploringPromptEngineeringforSolvingCS1ProblemsUsing
NaturalLanguage.(102022). http://arxiv.org/abs/2210.15157
[24] YihongDong,XueJiang,ZhiJin,andGeLi.2023.Self-collaborationCodeGenerationviaChatGPT.CoRRabs/2304.07590(2023).
[25] XueyingDu,MingweiLiu,KaixinWang,HanlinWang,JunweiLiu,YixuanChen,JiayiFeng,ChaofengSha,XinPeng,andYilingLou.2023.
ClassEval:AManually-CraftedBenchmarkforEvaluatingLLMsonClass-levelCodeGeneration.CoRRabs/2308.01861(2023).
[26] XueyingDu,MingweiLiu,KaixinWang,HanlinWang,JunweiLiu,YixuanChen,JiayiFeng,ChaofengSha,XinPeng,andYilingLou.2023.
Classeval:Amanually-craftedbenchmarkforevaluatingllmsonclass-levelcodegeneration.arXivpreprintarXiv:2308.01861(2023).
[27] LarryEllison.2023.Oracle‚Äôsvisionforthefuture.KeynoteatOracleCloudWorld. https://www.youtube.com/watch?v=63DmgBN1rSI.
[28] ZhiyuFan,XiangGao,MartinMirchev,AbhikRoychoudhury,andShinHweiTan.2023. Automatedrepairofprogramsfromlargelanguage
models.In2023IEEE/ACM45thInternationalConferenceonSoftwareEngineering(ICSE).IEEE,1469‚Äì1481.
[29] ZhiyuFan,XiangGao,MartinMirchev,AbhikRoychoudhury,andShinHweiTan.2023.AutomatedRepairofProgramsfromLargeLanguage
Models.InIEEE/ACMInternationalConferenceonSoftwareEngineering(ICSE).
[30] ZhangyinFeng,DayaGuo,DuyuTang,NanDuan,XiaochengFeng,MingGong,LinjunShou,BingQin,TingLiu,DaxinJiang,andMingZhou.
2020.CodeBERT:APre-TrainedModelforProgrammingandNaturalLanguages.InFindingsoftheAssociationforComputationalLinguistics:
EMNLP2020(FindingsofACL,Vol.EMNLP2020).AssociationforComputationalLinguistics,1536‚Äì1547.
[31] RichardEFikesandNilsJNilsson.1971.STRIPS:Anewapproachtotheapplicationoftheoremprovingtoproblemsolving.Artificialintelligence
2,3-4(1971),189‚Äì208.
[32] EmilyFirst,MarkusRabe,TaliaRinger,andYuriyBrun.2023.Baldur:Whole-ProofGenerationandRepairwithLargeLanguageModels.InACM
JointEuropeanSoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE).
[33] StephanieForrest,ThanhVuNguyen,WestleyWeimer,andClaireLeGoues.2009.Ageneticprogrammingapproachtoautomatedsoftwarerepair.
InGeneticandEvolutionaryComputationConference,GECCO2009,Proceedings,Montreal,Qu√©bec,Canada,July8-12,2009.ACM,947‚Äì954.
[34] DanielFried,ArmenAghajanyan,JessyLin,SidaWang,EricWallace,FredaShi,RuiqiZhong,Wen-tauYih,LukeZettlemoyer,andMikeLewis.
2022.InCoder:AGenerativeModelforCodeInfillingandSynthesis.CoRRabs/2204.05999(2022).
[35] ZacharyP.Fry,BryanLandau,andWestleyWeimer.2012.Ahumanstudyofpatchmaintainability.InInternationalSymposiumonSoftwareTesting
andAnalysis,ISSTA2012,Minneapolis,MN,USA,July15-20,2012.ACM,177‚Äì187.
[36] AlexanderFr√∂mmgen,JacobAustin,PeterChoy,NimeshGhelani,LeraKharatyan,GabrielaSurita,ElenaKhrapko,PascalLamblin,Pierre-Antoine
Manzagol,MarcusRevaj,MaximTabachnyk,DanielTarlow,KevinVillela,DanielZheng,SatishChandra,andManiatisGoogle.2024.Resolving
CodeReviewCommentswithMachineLearning.InternationalConferenceonSoftwareEngineering:SoftwareEngineeringinPractice(ICSE-SEIP).
https://doi.org/10.1145/3639477.3639746
[37] ShuzhengGao,WenxinMao,CuiyunGao,LiLi,XingHu,XinXia,andMichaelR.Lyu.2024.LearningintheWild:TowardsLeveragingUnlabeled
DataforEffectivelyTuningPre-trainedCodeModels.InProceedingsofthe46thIEEE/ACMInternationalConferenceonSoftwareEngineering,ICSE
2024,Lisbon,Portugal,April14-20,2024.ACM.
[38] ShuzhengGao,Xin-ChengWen,CuiyunGao,WenxuanWang,HongyuZhang,andMichaelR.Lyu.2023. WhatMakesGoodIn-Context
DemonstrationsforCodeIntelligenceTaskswithLLMs?.In38thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering,ASE2023,
Luxembourg,September11-15,2023.IEEE,761‚Äì773.
[39] ShuzhengGao,Xin-ChengWen,CuiyunGao,WenxuanWang,HongyuZhang,andMichaelR.Lyu.2023. WhatMakesGoodIn-context
DemonstrationsforCodeIntelligenceTaskswithLLMs?(42023). https://doi.org/10.1109/ASE56229.2023.00109
[40] XiangGao,SergeyMechtaev,andAbhikRoychoudhury.2019.Crash-avoidingProgramRepair.InACMInternationalSymposiumonSoftware
TestingandAnalysis(ISSTA).
[41] XiangGao,YannicNoller,andAbhikRoychoudhury.2023.ProgramRepair.arXivpreprintarXiv:2211.12787(2023).
[42] XiangGao,BoWang,GregoryJDuck,RuyiJi,YingfeiXiong,andAbhikRoychoudhury.2021.Beyondtests:Programvulnerabilityrepairviacrash
constraintextraction.ACMTransactionsonSoftwareEngineeringandMethodology(TOSEM)30,2(2021),1‚Äì27.
[43] A.D.Gordon,T.A.Henzinger,A.V.Nori,andS.K.Rajamani.2014.ProbabilisticProgramming.InFutureofSoftwareEngineering(FOSE),co-locatesd
withInternationalConferenceonSoftwareEngineering(ICSE).
[44] ClaireLeGoues,ThanhVuNguyen,StephanieForrest,andWestleyWeimer.2012.GenProg:AGenericMethodforAutomaticSoftwareRepair.
IEEETrans.SoftwareEng.38,1(2012),54‚Äì72. https://doi.org/10.1109/TSE.2011.104
[45] CordellGreen.1969.Theoremprovingbyresolutionasabasisforquestion-answeringsystems.Machineintelligence4(1969),183‚Äì205.
[46] KaiGreshake,SaharAbdelnabi,ShaileshMishra,ChristophEndres,ThorstenHolz,andMarioFritz.2023. Morethanyou‚Äôveaskedfor:A
ComprehensiveAnalysisofNovelPromptInjectionThreatstoApplication-IntegratedLargeLanguageModels.arXive-prints(2023),arXiv‚Äì2302.28 Authors
[47] JianGu,PasqualeSalza,andHaraldC.Gall.2022.AssembleFoundationModelsforAutomaticCodeSummarization.Proceedings-2022IEEE
InternationalConferenceonSoftwareAnalysis,EvolutionandReengineering,SANER2022,935‚Äì946. https://doi.org/10.1109/SANER53432.
2022.00112
[48] SumitGulwani.2011.Automatingstringprocessinginspreadsheetsusinginput-outputexamples.InProceedingsofthe38thACMSIGPLAN-SIGACT
SymposiumonPrinciplesofProgrammingLanguages,POPL2011,Austin,TX,USA,January26-28,2011.ACM,317‚Äì330.
[49] SumitGulwani,OleksandrPolozov,RishabhSingh,etal.2017.Programsynthesis.FoundationsandTrends¬ÆinProgrammingLanguages4,1-2
(2017),1‚Äì119.
[50] SuriyaGunasekar,YiZhang,JyotiAneja,CaioC√©sarTeodoroMendes,AllieDelGiorno,SivakanthGopi,MojanJavaheripi,PieroKauffmann,
GustavodeRosa,OlliSaarikivi,AdilSalim,ShitalShah,HarkiratSinghBehl,XinWang,S√©bastienBubeck,RonenEldan,AdamTaumanKalai,
YinTatLee,andYuanzhiLi.2023.TextbooksAreAllYouNeed.CoRRabs/2306.11644(2023).
[51] DayaGuo,ShuaiLu,NanDuan,YanlinWang,MingZhou,andJianYin.2022.UniXcoder:UnifiedCross-ModalPre-trainingforCodeRepresentation.
InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),ACL2022,Dublin,Ireland,May
22-27,2022.AssociationforComputationalLinguistics,7212‚Äì7225.
[52] QiGuo,JunmingCao,XiaofeiXie,ShangqingLiu,XiaohongLi,BihuanChen,andXinPeng.2024.ExploringthePotentialofChatGPTinAutomated
CodeRefinement:AnEmpiricalStudy.AssociationforComputingMachinery(ACM),1‚Äì13. https://doi.org/10.1145/3597503.3623306
[53] RahulGupta,SohamPal,AdityaKanade,andShirishK.Shevade.2017. DeepFix:FixingCommonCLanguageErrorsbyDeepLearning.In
ProceedingsoftheThirty-FirstAAAIConferenceonArtificialIntelligence,February4-9,2017,SanFrancisco,California,USA.AAAIPress,1345‚Äì1351.
[54] YiyangHao,GeLi,YongqiangLiu,XiaoweiMiao,HeZong,SiyuanJiang,YangLiu,andHeWei.2022.Aixbench:Acodegenerationbenchmark
dataset.arXivpreprintarXiv:2206.13179(2022).
[55] ShirleyAnugrahHayati,Rapha√´lOlivier,PravalikaAvvaru,PengchengYin,AnthonyTomasic,andGrahamNeubig.2018.Retrieval-BasedNeural
CodeGeneration.InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Brussels,Belgium,October31-
November4,2018.AssociationforComputationalLinguistics,925‚Äì930.
[56] DanHendrycks,StevenBasart,SauravKadavath,MantasMazeika,AkulArora,EthanGuo,CollinBurns,SamirPuranik,HoraceHe,DawnSong,
etal.2021.Measuringcodingchallengecompetencewithapps.arXivpreprintarXiv:2105.09938(2021).
[57] RobertM.Hierons,KirillBogdanov,JonathanP.Bowen,RanceCleaveland,JohnDerrick,JeremyDick,MarianGheorghe,MarkHarman,Kalpesh
Kapoor,PaulJ.Krause,GeraldL√ºttgen,AnthonyJ.H.Simons,SergiyA.Vilkomir,MartinR.Woodward,andHusseinZedan.2009.Usingformal
specificationstosupporttesting.ACMComput.Surv.41,2(2009),9:1‚Äì9:76.
[58] KaiHuang,XiangxinMeng,JianZhang,YangLiu,WenjieWang,ShuhaoLi,andYuqingZhang.2023.AnEmpiricalStudyonFine-TuningLarge
LanguageModelsofCodeforAutomatedProgramRepair.In38thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering,ASE
2023,Luxembourg,September11-15,2023.IEEE,1162‚Äì1174.
[59] Z.Huang,D.Lie,G.Tan,andTJaeger.2019.Usingsafetypropertiestogeneratevulnerabilitypatches.InIEEESymposiumonSecurityandPrivacy
(S&P).
[60] IvanaClairineIrsan,TingZhang,FerdianThung,DavidLo,andLingxiaoJiang.2022. AutoPRTitle:AToolforAutomaticPullRequestTitle
Generation.Proceedings-2022IEEEInternationalConferenceonSoftwareMaintenanceandEvolution,ICSME2022,454‚Äì458. https://doi.org/10.
1109/ICSME55016.2022.00058
[61] SrinivasanIyer,IoannisKonstas,AlvinCheung,andLukeZettlemoyer.2018.MappingLanguagetoCodeinProgrammaticContext.InProceedings
ofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.1643‚Äì1652.
[62] NamanJain,KingHan,AlexGu,Wen-DingLi,FanjiaYan,TianjunZhang,SidaWang,ArmandoSolar-Lezama,KoushikSen,andIonStoica.2024.
LiveCodeBench:HolisticandContaminationFreeEvaluationofLargeLanguageModelsforCode. arXiv:2403.07974[cs.SE]
[63] Kevin Jesse, Toufique Ahmed, Premkumar T. Devanbu, and Emily Morgan. 2023. Large Language Models and Simple, Stupid Bugs.
arXiv:2303.11455[cs.SE]
[64] SusmitJha,SumitGulwani,SanjitSeshia,andAshishTiwari.2010.Oracle-guidedComponent-basedProgramSynthesis.InInternationalConference
onSoftwareEngineering(ICSE).
[65] SusmitJha,SumitGulwani,SanjitASeshia,andAshishTiwari.2010.Oracle-guidedcomponent-basedprogramsynthesis.InProceedingsofthe
32ndACM/IEEEInternationalConferenceonSoftwareEngineering-Volume1.215‚Äì224.
[66] EllenJiang,EdwinToh,AlejandraMolina,KristenOlson,ClaireKayacik,AaronDonsbach,CarrieJ.Cai,andMichaelTerry.2022.Discoveringthe
SyntaxandStrategiesofNaturalLanguageProgrammingwithGenerativeLanguageModels.ConferenceonHumanFactorsinComputingSystems-
Proceedings. https://doi.org/10.1145/3491102.3501870
[67] NanJiang,ThibaudLutellier,andLinTan.2021.CURE:Code-AwareNeuralMachineTranslationforAutomaticProgramRepair.In43rdIEEE/ACM
InternationalConferenceonSoftwareEngineering,ICSE2021,Madrid,Spain,22-30May2021.IEEE,1161‚Äì1173.
[68] ZhihanJiang,JinyangLiu,ZhuangbinChen,YichenLi,JunjieHuang,YintongHuo,PinjiaHe,JiazhenGu,andMichaelR.Lyu.2023.LLMParser:A
LLM-basedLogParsingFramework.CoRRabs/2310.01796(2023).
[69] CarlosE.Jimenez,JohnYang,AlexanderWettig,ShunyuYao,KexinPei,OfirPress,andKarthikNarasimhan.2023.SWE-bench:CanLanguage
ModelsResolveReal-WorldGitHubIssues?arXiv:2310.06770(Oct2023).
[70] CarlosEJimenez,JohnYang,AlexanderWettig,ShunyuYao,KexinPei,OfirPress,andKarthikNarasimhan.2023.SWE-bench:CanLanguage
ModelsResolveReal-WorldGitHubIssues?arXivpreprintarXiv:2310.06770(2023).AutomaticProgramming:LargeLanguageModelsandBeyond 29
[71] BarbaraJobstmann,AndreasGriesmayer,andRoderickBloem.2005.ProgramRepairasaGame.InComputerAidedVerification,17thInternational
Conference,CAV2005,Edinburgh,Scotland,UK,July6-10,2005,Proceedings(LectureNotesinComputerScience,Vol.3576).Springer,226‚Äì238.
[72] Tae-HwanJung.2021.Commitbert:Commitmessagegenerationusingpre-trainedprogramminglanguagemodel.arXivpreprintarXiv:2105.14242
(2021).
[73] JaredKaplan,SamMcCandlish,TomHenighan,TomB.Brown,BenjaminChess,RewonChild,ScottGray,AlecRadford,JeffreyWu,andDario
Amodei.2020.ScalingLawsforNeuralLanguageModels.CoRRabs/2001.08361(2020).
[74] MajeedKazemitabaar,JustinChow,CarlKaToMa,BarbaraJ.Ericson,DavidWeintrop,andToviGrossman.2023.StudyingtheeffectofAICode
GeneratorsonSupportingNoviceLearnersinIntroductoryProgramming. ConferenceonHumanFactorsinComputingSystems-Proceedings.
https://doi.org/10.1145/3544548.3580919
[75] DongsunKim,JaechangNam,JaewooSong,andSunghunKim.2013.Automaticpatchgenerationlearnedfromhuman-writtenpatches.In35th
InternationalConferenceonSoftwareEngineering,ICSE‚Äô13,SanFrancisco,CA,USA,May18-26,2013.IEEEComputerSociety,802‚Äì811.
[76] JohnRKoza.1994.Geneticprogrammingasameansforprogrammingcomputersbynaturalselection.Statisticsandcomputing4(1994),87‚Äì112.
[77] CognitionLabs.2024.Devin,AIsoftwareengineer. https://www.cognition-labs.com/introducing-devin.
[78] ClaireLeGoues,MichaelPradel,andAbhikRoychoudhury.2019.AutomatedProgramRepair.Commun.ACM62,12(2019).
[79] JiaLi,GeLi,YongminLi,andZhiJin.2023. EnablingProgrammingThinkinginLargeLanguageModelsTowardCodeGeneration. CoRR
abs/2305.06599(2023).
[80] LingweiLi,LiYang,HuaxiJiang,JunYan,TiejianLuo,ZihanHua,GengLiang,andChunZuo.2022.AUGER:automaticallygeneratingreview
commentswithpre-trainingmodels.ESEC/FSE2022-Proceedingsofthe30thACMJointMeetingEuropeanSoftwareEngineeringConferenceand
SymposiumontheFoundationsofSoftwareEngineering,1009‚Äì1021. https://doi.org/10.1145/3540250.3549099
[81] RaymondLi,LoubnaBenAllal,YangtianZi,NiklasMuennighoff,DenisKocetkov,ChenghaoMou,MarcMarone,ChristopherAkiki,JiaLi,Jenny
Chim,QianLiu,EvgeniiZheltonozhskii,TerryYueZhuo,ThomasWang,OlivierDehaene,MishigDavaadorj,JoelLamy-Poirier,Jo√£oMonteiro,
OlehShliazhko,NicolasGontier,NicholasMeade,ArmelZebaze,Ming-HoYee,LogeshKumarUmapathi,JianZhu,BenjaminLipkin,Muhtasham
Oblokulov,ZhiruoWang,RudraMurthyV,JasonStillerman,SivaSankalpPatel,DmitryAbulkhanov,MarcoZocca,MananDey,ZhihanZhang,
NourMoustafa-Fahmy,UrvashiBhattacharyya,WenhaoYu,SwayamSingh,SashaLuccioni,PauloVillegas,MaximKunakov,FedorZhdanov,
ManuelRomero,TonyLee,NadavTimor,JenniferDing,ClaireSchlesinger,HaileySchoelkopf,JanEbert,TriDao,MayankMishra,AlexGu,
JenniferRobinson,CarolynJaneAnderson,BrendanDolan-Gavitt,DanishContractor,SivaReddy,DanielFried,DzmitryBahdanau,YacineJernite,
CarlosMu√±ozFerrandis,SeanHughes,ThomasWolf,ArjunGuha,LeandrovonWerra,andHarmdeVries.2023.StarCoder:maythesourcebe
withyou!CoRRabs/2305.06161(2023).
[82] YujiaLi,DavidChoi,JunyoungChung,NateKushman,JulianSchrittwieser,R√©miLeblond,TomEccles,JamesKeeling,FelixGimeno,Agustin
DalLago,etal.2022.Science378,6624(2022),1092‚Äì1097.
[83] YujiaLi,DavidChoi,JunyoungChung,NateKushman,JulianSchrittwieser,R√©miLeblond,TomEccles,JamesKeeling,FelixGimeno,Agustin
DalLago,etal.2022.Competition-levelcodegenerationwithalphacode.Science378,6624(2022),1092‚Äì1097.
[84] YichenLi,YintongHuo,ZhihanJiang,RenyiZhong,PinjiaHe,YuxinSu,andMichaelR.Lyu.2023. ExploringtheEffectivenessofLLMsin
AutomatedLoggingGeneration:AnEmpiricalStudy.CoRRabs/2307.05950(2023).
[85] YichenLi,YintongHuo,RenyiZhong,ZhihanJiang,JinyangLiu,JunjieHuang,JiazhenGu,PinjiaHe,andMichaelRLyu.2024. GoStatic:
ContextualizedLoggingStatementGeneration.arXivpreprintarXiv:2402.12958(2024).
[86] YiLi,ShaohuaWang,andTienN.Nguyen.2020.DLFix:context-basedcodetransformationlearningforautomatedprogramrepair.InICSE‚Äô20:
42ndInternationalConferenceonSoftwareEngineering,Seoul,SouthKorea,27June-19July,2020.ACM,602‚Äì614.
[87] ZhiyuLi,ShuaiLu,DayaGuo,NanDuan,ShaileshJannu,GrantJenks,DeepMajumder,JaredGreen,AlexeySvyatkovskiy,ShengyuFu,and
NeelSundaresan.2022.Automatingcodereviewactivitiesbylarge-scalepre-training.InProceedingsofthe30thACMJointEuropeanSoftware
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering,ESEC/FSE2022,Singapore,Singapore,November14-18,2022.
ACM,1035‚Äì1047.
[88] DianshuLiao,ShidongPan,QingHuang,XiaoxueRen,ZhenchangXing,HuanJin,andQinyingLi.2023.Context-awarecodegenerationframework
forcoderepositories:Local,global,andthird-partylibraryawareness.arXivpreprintarXiv:2312.05772(2023).
[89] WangLing,PhilBlunsom,EdwardGrefenstette,KarlMoritzHermann,Tom√°sKocisk√Ω,FuminWang,andAndrewW.Senior.2016.LatentPredictor
NetworksforCodeGeneration.InProceedingsofthe54thAnnualMeetingoftheAssociationforComputationalLinguistics,ACL2016,August7-12,
2016,Berlin,Germany,Volume1:LongPapers.TheAssociationforComputerLinguistics.
[90] ChangshuLiu,PelinCetin,YogeshPatodia,SaikatChakraborty,YangruiboDing,andBaishakhiRay.2023. AutomatedCodeEditingwith
Search-Generate-Modify.TransactionofSoftwareEngineering(2023).
[91] NelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.2024. Lostinthemiddle:How
languagemodelsuselongcontexts.TransactionsoftheAssociationforComputationalLinguistics12(2024),157‚Äì173.
[92] YueLiu,ThanhLe-Cong,RatnadiraWidyasari,ChakkritTantithamthavorn,LiLi,Xuan-BachDLe,andDavidLo.2023.RefiningChatGPT-generated
code:Characterizingandmitigatingcodequalityissues.ACMTransactionsonSoftwareEngineeringandMethodology(2023).
[93] ZhongxinLiu,XinXia,ChristophTreude,DavidLo,andShanpingLi.2019.AutomaticGenerationofPullRequestDescriptions.
[94] FanLongandMartinRinard.2015. StagedProgramRepairwithConditionSynthesis.InJointMeetingoftheEuropeanSoftwareEngineering
ConferenceandtheACMSIGSOFTSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE).30 Authors
[95] FanLongandMartinRinard.2016.AutomaticPatchGenerationbyLearningCorrectCode.In43rdAnnualACMSIGPLAN-SIGACTSymposiumon
PrinciplesofProgrammingLanguages(St.Petersburg,FL,USA)(POPL‚Äô16).ACM,NewYork,NY,USA,298‚Äì312.
[96] ShuaiLu,DayaGuo,ShuoRen,JunjieHuang,AlexeySvyatkovskiy,AmbrosioBlanco,ColinClement,DawnDrain,DaxinJiang,DuyuTang,etal.
2021.CodeXGLUE:AMachineLearningBenchmarkDatasetforCodeUnderstandingandGeneration.InProceedingsoftheNeuralInformation
ProcessingSystemsTrackonDatasetsandBenchmarks1,NeurIPSDatasetsandBenchmarks2021,December2021,virtual,JoaquinVanschorenand
Sai-KitYeung(Eds.).
[97] ZiyangLuo,CanXu,PuZhao,QingfengSun,XiuboGeng,WenxiangHu,ChongyangTao,JingMa,QingweiLin,andDaxinJiang.2023.WizardCoder:
EmpoweringCodeLargeLanguageModelswithEvol-Instruct.CoRRabs/2306.08568(2023).
[98] ThibaudLutellier,HungVietPham,LawrencePang,YitongLi,MoshiWei,andLinTan.2020. CoCoNuT:combiningcontext-awareneural
translationmodelsusingensembleforprogramrepair.InISSTA‚Äô20:29thACMSIGSOFTInternationalSymposiumonSoftwareTestingandAnalysis,
VirtualEvent,USA,July18-22,2020.ACM,101‚Äì114.
[99] SergeyMechtaev,JooyongYi,andAbhikRoychoudhury.2015.DirectFix:LookingforSimpleProgramRepairs.In37thIEEE/ACMInternational
ConferenceonSoftwareEngineering,ICSE2015,Florence,Italy,May16-24,2015,Volume1.IEEEComputerSociety,448‚Äì458.
[100] SergeyMechtaev,JooyongYi,andAbhikRoychoudhury.2016. Angelix:scalablemultilineprogrampatchsynthesisviasymbolicanalysis.In
Proceedingsofthe38thInternationalConferenceonSoftwareEngineering,ICSE2016,Austin,TX,USA,May14-22,2016.ACM,691‚Äì701.
[101] MdRakibHossainMisu,CristinaVLopes,IrisMa,andJamesNoble.2024.TowardsAIAssistedSynthesisofVerifiedDafnyMethods.PACM-SE,
ProceedingsofInternationalConferenceonFoundationsofSoftwareEngineering(FSE)(2024).
[102] HoangDuongThienNguyen,DaweiQi,AbhikRoychoudhury,andSatishChandra.2013.SemFix:programrepairviasemanticanalysis.In35th
InternationalConferenceonSoftwareEngineering,ICSE‚Äô13,SanFrancisco,CA,USA,May18-26,2013.IEEEComputerSociety,772‚Äì781.
[103] HoangDuongThienNguyen,DaweiQi,AbhikRoychoudhury,andSatishChandra.2013.SemFix:ProgramRepairviaSemanticAnalysis.In
Proceedingsofthe2013InternationalConferenceonSoftwareEngineering(SanFrancisco,CA,USA)(ICSE‚Äô13).IEEEPress,Piscataway,NJ,USA,
772‚Äì781. https://doi.org/10.1109/ICSE.2013.6606623
[104] LunYiuNie,CuiyunGao,ZhicongZhong,WaiLam,YangLiu,andZenglinXu.2021.Coregen:Contextualizedcoderepresentationlearningfor
commitmessagegeneration.Neurocomputing459(2021),97‚Äì107.
[105] ErikNijkamp,BoPang,HiroakiHayashi,LifuTu,HuanWang,YingboZhou,SilvioSavarese,andCaimingXiong.2022.Codegen:Anopenlarge
languagemodelforcodewithmulti-turnprogramsynthesis.arXivpreprintarXiv:2203.13474(2022).
[106] LiangNiu,ShujaatMirza,ZaydMaradni,andChristinaP√∂pper.2023.{CodexLeaks}:Privacyleaksfromcodegenerationlanguagemodelsin
{GitHub}copilot.In32ndUSENIXSecuritySymposium(USENIXSecurity23).2133‚Äì2150.
[107] OpenAI.2023.GPT-4TechnicalReport. arXiv:2303.08774[cs.CL]
[108] Md.RizwanParvez,WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,andKai-WeiChang.2021.RetrievalAugmentedCodeGenerationand
Summarization.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2021,VirtualEvent/PuntaCana,DominicanRepublic,16-20
November,2021.AssociationforComputationalLinguistics,2719‚Äì2734.
[109] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,
LucaAntiga,AlbanDesmaison,AndreasK√∂pf,EdwardYang,ZachDeVito,MartinRaison,AlykhanTejani,SasankChilamkurthy,BenoitSteiner,
LuFang,JunjieBai,andSoumithChintala.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.InAdvancesinNeural
InformationProcessingSystems(NeurIPS).
[110] DavidA.Patterson,JosephGonzalez,QuocV.Le,ChenLiang,Lluis-MiquelMunguia,DanielRothchild,DavidR.So,MaudTexier,andJeffDean.
2021.CarbonEmissionsandLargeNeuralNetworkTraining.CoRRabs/2104.10350(2021).
[111] HammondPearce,BaleeghAhmad,BenjaminTan,BrendanDolan-Gavitt,andRameshKarri.2022.Asleepatthekeyboard?assessingthesecurity
ofgithubcopilot‚Äôscodecontributions.In2022IEEESymposiumonSecurityandPrivacy(SP).IEEE,754‚Äì768.
[112] YunPeng,ShuzhengGao,CuiyunGao,YintongHuo,andMichaelR.Lyu.2024.DomainKnowledgeMatters:ImprovingPromptswithFixTemplates
forRepairingPythonTypeErrors.InProceedingsofthe46thIEEE/ACMInternationalConferenceonSoftwareEngineering,ICSE2024,Lisbon,Portugal,
April14-20,2024.ACM,4:1‚Äì4:13.
[113] NeilPerry,MeghaSrivastava,DeepakKumar,andDanBoneh.2023.DouserswritemoreinsecurecodewithAIassistants?.InProceedingsofthe
2023ACMSIGSACConferenceonComputerandCommunicationsSecurity.2785‚Äì2799.
[114] AmirPnueliandRoniRosner.1989.Onthesynthesisofareactivemodule.InPOPL.
[115] GabrielPoesia,OleksandrPolozov,VuLe,AshishTiwari,GustavoSoares,ChristopherMeek,andSumitGulwani.2022.Synchromesh:Reliable
codegenerationfrompre-trainedlanguagemodels.arXivpreprintarXiv:2201.11227(2022).
[116] YuhuaQi,XiaoguangMao,YanLei,ZiyingDai,andChengsongWang.2014.Thestrengthofrandomsearchonautomatedprogramrepair.In36th
InternationalConferenceonSoftwareEngineering,ICSE‚Äô14,Hyderabad,India-May31-June07,2014.ACM,254‚Äì265.
[117] ZichaoQi,FanLong,SaraAchour,andMartinRinard.2015.Ananalysisofpatchplausibilityandcorrectnessforgenerate-and-validatepatch
generationsystems.InProceedingsofthe2015InternationalSymposiumonSoftwareTestingandAnalysis.24‚Äì36.
[118] MaximRabinovich,MitchellStern,andDanKlein.2017.AbstractSyntaxNetworksforCodeGenerationandSemanticParsing.InProceedingsof
the55thAnnualMeetingoftheAssociationforComputationalLinguistics,ACL2017,Vancouver,Canada,July30-August4,Volume1:LongPapers.
AssociationforComputationalLinguistics,1139‚Äì1149.AutomaticProgramming:LargeLanguageModelsandBeyond 31
[119] StevenI.Ross,FernandoMartinez,StephanieHoude,MichaelMuller,andJustinD.Weisz.2023.TheProgrammer‚ÄôsAssistant:Conversational
InteractionwithaLargeLanguageModelforSoftwareDevelopment.InternationalConferenceonIntelligentUserInterfaces,ProceedingsIUI,491‚Äì514.
https://doi.org/10.1145/3581641.3584037
[120] BaptisteRozi√®re,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,YossiAdi,JingyuLiu,TalRemez,J√©r√©myRapin,Artyom
Kozhevnikov,IvanEvtimov,JoannaBitton,ManishBhatt,CristianCanton-Ferrer,AaronGrattafiori,WenhanXiong,AlexandreD√©fossez,Jade
Copet,FaisalAzhar,HugoTouvron,LouisMartin,NicolasUsunier,ThomasScialom,andGabrielSynnaeve.2023.CodeLlama:OpenFoundation
ModelsforCode.CoRRabs/2308.12950(2023).
[121] GabrielRyan,SiddharthaJain,MingyueShang,ShiqiWang,XiaofeiMa,MuraliKrishnaRamanathan,andBaishakhiRay.2024. Code-Aware
Prompting:AstudyofCoverageGuidedTestGenerationinRegressionSettingusingLLM.CoRRabs/2402.00097(2024).
[122] FredBSchneider,NationalResearchCouncil,etal.1999.Trustincyberspace.NationalAcademyPressWashington,DC.
[123] KonstantinSerebryany,DerekBruening,AlexanderPotapenko,andDmitryVyukov.2012.-AddressSanitizer:afastaddresssanitychecker.In
USENIXconferenceonAnnualTechnicalConference.
[124] RidwanShariffdeen,YannicNoller,LarsGrunske,andAbhikRoychoudhury.2021.ConcolicProgramRepair.In42ndACMSIGPLANConferenceon
ProgrammingLanguageDesignandImplementation(PLDI).
[125] DishaShrivastava,HugoLarochelle,andDanielTarlow.2023. Repository-LevelPromptGenerationforLargeLanguageModelsofCode.In
InternationalConferenceonMachineLearning,ICML2023,23-29July2023,Honolulu,Hawaii,USA(ProceedingsofMachineLearningResearch,
Vol.202).PMLR,31693‚Äì31715.
[126] MohammedLatifSiddiq,JoannaC.S.Santos,RidwanulHasanTanvir,NoshinUlfat,FahmidAlRifat,andViniciusCarvalhoLopes.2023.Exploring
theEffectivenessofLargeLanguageModelsinGeneratingUnitTests.CoRRabs/2305.00418(2023).
[127] ManavSinghal,TusharAggarwal,AbhijeetAwasthi,NagarajanNatarajan,andAdityaKanade.2024.NoFunEval:FunnyHowCodeLMsFalteron
RequirementsBeyondFunctionalCorrectness.arXivpreprintarXiv:2401.15963(2024).
[128] GiriprasadSridhara,RanjaniH.G.,andSouravMazumdar.2023.ChatGPT:AStudyonitsUtilityforUbiquitousSoftwareEngineeringTasks.
CoRRabs/2305.16837(2023).
[129] LichaoSun,YueHuang,HaoranWang,SiyuanWu,QihuiZhang,ChujieGao,YixinHuang,WenhanLyu,YixuanZhang,XinerLi,etal.2024.
Trustllm:Trustworthinessinlargelanguagemodels.arXivpreprintarXiv:2401.05561(2024).
[130] ZeyuSun,QihaoZhu,YingfeiXiong,YicanSun,LiliMou,andLuZhang.2020. TreeGen:ATree-BasedTransformerArchitectureforCode
Generation.InTheThirty-FourthAAAIConferenceonArtificialIntelligence,AAAI2020,TheThirty-SecondInnovativeApplicationsofArtificial
IntelligenceConference,IAAI2020,TheTenthAAAISymposiumonEducationalAdvancesinArtificialIntelligence,EAAI2020,NewYork,NY,USA,
February7-12,2020.AAAIPress,8984‚Äì8991.
[131] ShinHweiTanandAbhikRoychoudhury.2015.relifix:AutomatedRepairofSoftwareRegressions.In37thIEEE/ACMInternationalConferenceon
SoftwareEngineering,ICSE2015,Florence,Italy,May16-24,2015,Volume1.IEEEComputerSociety,471‚Äì482.
[132] ShinHweiTan,HiroakiYoshida,MukulRPrasad,andAbhikRoychoudhury.2016.Anti-patternsinsearch-basedprogramrepair.InProceedingsof
the201624thACMSIGSOFTInternationalSymposiumonFoundationsofSoftwareEngineering.ACM,727‚Äì738.
[133] NingzhiTang,MengChen,ZhengNing,AakashBansal,YuHuang,CollinMcMillan,andTobyJia-JunLi.2023.AnEmpiricalStudyofDeveloper
BehaviorsforValidatingandRepairingAI-GeneratedCode.InPLATEAUWorkshop.
[134] PatanamonThongtanunam,ChanathipPornprasit,andChakkritTantithamthavorn.2022.AutoTransform:AutomatedCodeTransformationto
SupportModernCodeReviewProcess.Proceedings-InternationalConferenceonSoftwareEngineering2022-May,237‚Äì248. https://doi.org/10.
1145/3510003.3510067
[135] RosaliaTufano,OzrenDabic,AntonioMastropaolo,MatteoCiniselli,andGabrieleBavota.2023. CodeReviewAutomation:Strengthsand
WeaknessesoftheStateoftheArt.IEEETransactionsonSoftwareEngineering(22023).
[136] RosaliaTufano,SimoneMasiero,AntonioMastropaolo,LucaPascarella,DenysPoshyvanyk,andGabrieleBavota.2022. UsingPre-Trained
ModelstoBoostCodeReviewAutomation. Proceedings-InternationalConferenceonSoftwareEngineering2022-May,2291‚Äì2302. https:
//doi.org/10.1145/3510003.3510621
[137] RosaliaTufano,LucaPascarella,MicheleTufano,DenysPoshyvanyk,andGabrieleBavota.2021.Towardsautomatingcodereviewactivities.
Proceedings-InternationalConferenceonSoftwareEngineering,163‚Äì174. https://doi.org/10.1109/ICSE43902.2021.00027
[138] PriyanVaithilingam,TianyiZhang,andElenaLGlassman.2022.Expectationvs.experience:Evaluatingtheusabilityofcodegenerationtools
poweredbylargelanguagemodels.InChiconferenceonhumanfactorsincomputingsystemsextendedabstracts.1‚Äì7.
[139] PriyanVaithilingam,TianyiZhang,andElenaL.Glassman.2022.Expectationvs.Experience:EvaluatingtheUsabilityofCodeGenerationTools
PoweredbyLargeLanguageModels.ConferenceonHumanFactorsinComputingSystems-Proceedings. https://doi.org/10.1145/3491101.
3519665
[140] EVoogd,EBJohnsen,ASilva,ZJSusag,andAWƒÖsowski.2023.Symbolicsemanticsforprobabilisticprograms.InInternationalConferenceon
QuantitativeEvaluationofSystems(QEST).
[141] RichardJ.WaldingerandRichardC.T.Lee.1969.PROW:AStepTowardAutomaticProgramWriting.InProceedingsofthe1stInternationalJoint
ConferenceonArtificialIntelligence,Washington,DC,USA,May7-9,1969.WilliamKaufmann,241‚Äì252.
[142] YuepengWang,JamesDong,RushiShah,andIsilDillig.2019. Synthesizingdatabaseprogramsforschemarefactoring.InProceedingsofthe
40thACMSIGPLANConferenceonProgrammingLanguageDesignandImplementation,PLDI2019,Phoenix,AZ,USA,June22-26,2019,KathrynS.32 Authors
McKinleyandKathleenFisher(Eds.).ACM,286‚Äì300.
[143] YueWang,WeishiWang,ShafiqR.Joty,andStevenC.H.Hoi.2021.CodeT5:Identifier-awareUnifiedPre-trainedEncoder-DecoderModelsfor
CodeUnderstandingandGeneration.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,EMNLP2021.
AssociationforComputationalLinguistics,8696‚Äì8708.
[144] ZhiruoWang,GraceCuenca,ShuyanZhou,FrankFXu,andGrahamNeubig.2022.Mconala:abenchmarkforcodegenerationfrommultiple
naturallanguages.arXivpreprintarXiv:2203.08388(2022).
[145] JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,MaartenBosma,DennyZhou,DonaldMetzler,
etal.2022.EmergentAbilitiesofLargeLanguageModels.Trans.Mach.Learn.Res.2022(2022).
[146] YuxiangWei,ZheWang,JiaweiLiu,YifengDing,andLingmingZhang.2023.Magicoder:SourceCodeIsAllYouNeed.CoRRabs/2312.02120
(2023).
[147] WestleyWeimer,ZacharyP.Fry,andStephanieForrest.2013.Leveragingprogramequivalenceforadaptiveprogramrepair:Modelsandfirst
results.In201328thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering,ASE2013,SiliconValley,CA,USA,November11-15,
2013.IEEE,356‚Äì366.
[148] MartinWhite,MicheleTufano,MatiasMartinez,MartinMonperrus,andDenysPoshyvanyk.2019.SortingandTransformingProgramRepair
IngredientsviaDeepLearningCodeSimilarities.In26thIEEEInternationalConferenceonSoftwareAnalysis,EvolutionandReengineering,SANER
2019,Hangzhou,China,February24-27,2019.IEEE,479‚Äì490.
[149] WEWong,RGao,YLi,RAbreu,andFWotawa.2016.Asurveyonsoftwarefaultlocalization.IEEETransactionsonSoftwareEngineering42,8
(2016).
[150] HongqiuWu,HaiZhao,andMinZhang.2020.CodeSummarizationwithStructure-inducedTransformer.(122020). http://arxiv.org/abs/
2012.14710
[151] ChunqiuStevenXia,YuxiangWei,andLingmingZhang.2023.AutomatedProgramRepairintheEraofLargePre-trainedLanguageModels.In
45thIEEE/ACMInternationalConferenceonSoftwareEngineering,ICSE2023,Melbourne,Australia,May14-20,2023.IEEE,1482‚Äì1494.
[152] ChunqiuStevenXiaandLingmingZhang.2023.KeeptheConversationGoing:Fixing162outof337bugsfor$0.42eachusingChatGPT.CoRR
abs/2304.00385(2023).
[153] TaoXiao,HideakiHata,ChristophTreude,andKenichiMatsumoto.2024.GenerativeAIforPullRequestDescriptions:Adoption,Impact,and
DeveloperInterventions.(22024). https://doi.org/10.1145/3643773
[154] ZhuokuiXie,YinghaoChen,ChenZhi,ShuiguangDeng,andJianweiYin.2023.ChatUniTest:aChatGPT-basedautomatedunittestgeneration
tool.CoRRabs/2305.04764(2023).
[155] F.Xu,U.Alon,G.Neubig,andV.Hellendoorn.2022.ASystematicEvaluationofLargeLanguageModelsofCode.In6thACMSIGPLANInternational
SymposiumonMachineProgramming.
[156] FrankF.Xu,ZhengbaoJiang,PengchengYin,BogdanVasilescu,andGrahamNeubig.2020.IncorporatingExternalKnowledgethroughPre-training
forNaturalLanguagetoCodeGeneration.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,ACL2020,
Online,July5-10,2020.AssociationforComputationalLinguistics,6045‚Äì6052.
[157] JunjielongXu,RuichunYang,YintongHuo,ChengyuZhang,andPinjiaHe.2023. PromptingforAutomaticLogTemplateExtraction. CoRR
abs/2307.09950(2023).
[158] JifengXuan,MatiasMartinez,FavioDemarco,MaximeClement,SebastianR.LamelasMarcote,ThomasDurieux,DanielLeBerre,andMartin
Monperrus.2017.Nopol:AutomaticRepairofConditionalStatementBugsinJavaPrograms.IEEETrans.SoftwareEng.43,1(2017),34‚Äì55.
[159] WeixiangYan,HaitianLiu,YunkunWang,YunzheLi,QianChen,WenWang,TingyuLin,WeishanZhao,LiZhu,ShuiguangDeng,etal.2023.
CodeScope:AnExecution-basedMultilingualMultitaskMultidimensionalBenchmarkforEvaluatingLLMsonCodeUnderstandingandGeneration.
arXivpreprintarXiv:2311.08588(2023).
[160] HeYe,MatiasMartinez,andMartinMonperrus.2022.NeuralProgramRepairwithExecution-basedBackpropagation.In44thIEEE/ACM44th
InternationalConferenceonSoftwareEngineering,ICSE2022,Pittsburgh,PA,USA,May25-27,2022.ACM,1506‚Äì1518.
[161] PengchengYinandGrahamNeubig.2018.TRANX:ATransition-basedNeuralAbstractSyntaxParserforSemanticParsingandCodeGeneration.
InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,EMNLP2018:SystemDemonstrations,Brussels,Belgium,
October31-November4,2018.AssociationforComputationalLinguistics,7‚Äì12.
[162] ShengchengYu,ChunrongFang,YuchenLing,ChentianWu,andZhenyuChen.2023.Llmfortestscriptgenerationandmigration:Challenges,
capabilities,andopportunities.In2023IEEE23rdInternationalConferenceonSoftwareQuality,Reliability,andSecurity(QRS).IEEE,206‚Äì217.
[163] YuanYuanandWolfgangBanzhaf.2020. ARJA:AutomatedRepairofJavaProgramsviaMulti-ObjectiveGeneticProgramming. IEEETrans.
SoftwareEng.46,10(2020),1040‚Äì1067.
[164] ZhiyuFanAbhikRoychoudhuryYuntongZhang,HaifengRuan.2024.AutoCodeRover:AutonomousProgramImprovement.arXiv:2404.05427
(April2024).
[165] DaoguangZan,BeiChen,DejianYang,ZeqiLin,MinsuKim,BeiGuan,YongjiWang,WeizhuChen,andJian-GuangLou.2022.CERT:Continual
Pre-trainingonSketchesforLibrary-orientedCodeGeneration.arXivpreprintarXiv:2206.06888(2022).
[166] JiyangZhang,SheenaPanthaplackel,PengyuNie,JunyiJessyLi,andMilosGligoric.2022.CoditT5:PretrainingforSourceCodeandNatural
LanguageEditing.In37thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering,ASE2022,Rochester,MI,USA,October10-14,2022.
ACM,22:1‚Äì22:12.AutomaticProgramming:LargeLanguageModelsandBeyond 33
[167] LiZhongandZilongWang.2023.Astudyonrobustnessandreliabilityoflargelanguagemodelcodegeneration.arXivpreprintarXiv:2308.10335
(2023).
[168] QihaoZhu,ZeyuSun,Yuan-anXiao,WenjieZhang,KangYuan,YingfeiXiong,andLuZhang.2021.Asyntax-guidededitdecoderforneural
programrepair.InESEC/FSE‚Äô21:29thACMJointEuropeanSoftwareEngineeringConferenceandSymposiumontheFoundationsofSoftware
Engineering,Athens,Greece,August23-28,2021.ACM,341‚Äì353.