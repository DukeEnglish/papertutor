Preprint.
MITIGATING SUBOPTIMALITY OF DETERMINISTIC
POLICY GRADIENTS IN COMPLEX Q-FUNCTIONS
AyushJain1âˆ—, NorioKosaka2, XinhuLi1, Kyung-MinKim3, ErdemBÄ±yÄ±k1, JosephJLim4
1UniversityofSouthernCalifornia 2LineYahooCorp 3NAVERCloud 4KAIST
ABSTRACT
Inreinforcementlearning,off-policyactor-criticapproacheslikeDDPGandTD3
arebasedonthedeterministicpolicygradient. Herein,theQ-functionistrained
fromoff-policyenvironmentdataandtheactor(policy)istrainedtomaximizethe
Q-functionviagradientascent. Weobservethatincomplextaskslikedexterous
manipulation and restricted locomotion, the Q-value is a complex function of
action,havingseverallocaloptimaordiscontinuities. Thisposesachallengefor
gradientascenttotraverseandmakestheactorpronetogetstuckatlocaloptima.
Toaddressthis,weintroduceanewactorarchitecturethatcombinestwosimple
insights:(i)usemultipleactorsandevaluatetheQ-valuemaximizingaction,and(ii)
learnsurrogatestotheQ-functionthataresimplertooptimizewithgradient-based
methods. Weevaluatetaskssuchasrestrictedlocomotion,dexterousmanipulation,
andlargediscrete-actionspacerecommendersystemsandshowthatouractorfinds
optimalactionsmorefrequentlyandoutperformsalternateactorarchitectures.
1 INTRODUCTION
In sequential decision-making, the goal is to
build an optimal agent that maximizes the
expected cumulative returns (Sondik, 1971;
Littman, 1996). Value-based reinforcement
learning(RL)approacheslearneachactionâ€™sex-
pectedreturnswithaQ-functionandmaximize
it(Sutton&Barto,1998). However,incontin-
Figure1: AnactorÂµtrainedwithgradientascent
uous action spaces, evaluating the Q-value of
onachallengingQ-landscapegetsstuckinlocalop-
every possible action is impractical. This ne-
tima.Ourapproachlearnsasequenceofsurrogates
cessitatesanactortogloballymaximizetheQ-
Î¨ oftheQ-functionthatsuccessivelypruneout
functionandefficientlynavigatethevastaction i
theQ-landscapebelowthecurrentbestQ-values,
space(Grondmanetal.,2012). Yet,thisispar-
resultinginfewerlocaloptima. Thus,theactors
ticularlychallengingintaskssuchasrestricted
Î½ trainedtoascendonthesesurrogatesproduce
locomotion,withanon-convexQ-functionland- i
actionswithamoreoptimalQ-value.
scapewithmanylocaloptima(Figure2).
CanwebuildanactorarchitecturetofindclosertooptimalactionsinsuchcomplexQ-landscapes?
PriormethodsperformasearchovertheactionspacewithevolutionaryalgorithmslikeCEM(DeBoer
et al., 2005; Kalashnikov et al., 2018; Shao et al., 2022), but this requires numerous costly re-
evaluationsoftheQ-function. Toavoidthis,deterministicpolicygradient(DPG)algorithms(Silver
etal.,2014),suchasDDPG(Lillicrapetal.,2015)andTD3(Fujimotoetal.,2018)trainaparameter-
izedactortooutputactionswiththeobjectiveofmaximizingtheQ-functionlocally.
AsignificantchallengearisesinenvironmentswheretheQ-functionhasmanylocaloptima,asshown
in Figure 2. An actor trained via gradient ascent may converge to a local optimum with a much
lowerQ-valuethantheglobalmaximum. Thisleadstosuboptimaldecisionsduringdeploymentand
sample-inefficienttraining,astheagentfailstoexplorehigh-rewardtrajectories(Kakade,2003).
Toimproveactorsâ€™abilitytoidentifyoptimalactionsincomplex,non-convexQ-functionlandscapes,
weproposetheSuccessiveActorsforValueOptimization(SAVO)algorithm. SAVOleveragestwo
âˆ—Correspondenceto:ayushj@usc.edu.
1
4202
tcO
51
]GL.sc[
1v33811.0142:viXraPreprint.
Hopper-Restricted Recsim
3.4
315 3.3
Q -v
a lu
e3
3
31
20
00
95 0
5
Q -value3
3
3. .2
.1
0
290 2.9
285 2.8
A0.1 c0 t.2 io0. n3 0 . S4 p0.5 a0 c.6 e0.7 0.8 0.9 0.9 0.8 0.7Ac0.6tion0.5 Sp0.4ace0.3 0.2 0.1 0.1 A0.2 ct0 i. o3 n0 .4 Sp0.5 ac0. e6 0.7 0.8 0.9 0.8 0.7 A0.6ctio0n.5 Sp0.a4ce0.3 0.2 0.1
Figure2: ComplexQ-landscapes. WeplotQ-valueversusactionaforsomestate. Incontrolof
Inverted-Double-Pendulum-Restricted(left)andHopper-Restricted(middle),certainactionrangesare
unsafe,resultinginvariouslocallyoptimalactionpeaks. Inalargediscrete-actionrecommendation
system(right),therearelocalpeaksatactionsrepresentingrealitems(blackdots).
key insights: (1) combining multiple policies using an argmax on their Q-values to construct a
superiorpolicy(Â§4.1),and(2)simplifyingtheQ-landscapebyexcludinglowerQ-valueregionsbased
onhigh-performingactions,inspiredbytabusearch(Glover,1990),therebyreducinglocaloptima
andfacilitatinggradient-ascent(Â§4.2). Byiterativelyapplyingthesestrategiesthroughasequenceof
simplifiedQ-landscapesandcorrespondingactors,SAVOprogressivelyfindsmoreoptimalactions.
WeevaluateSAVOincomplexQ-landscapessuchas(i)continuouscontrolindexterousmanipu-
lation(Rajeswaranetal.,2017)andrestrictedlocomotion(Todorovetal.,2012),and(ii)discrete
decision-makinginthelargeactionspacesofsimulated(Ieetal.,2019)andreal-datarecommender
systems(Harper&Konstan,2015),andgridworldminingexpedition(Chevalier-Boisvertetal.,2018).
WeusethereframingoflargediscreteactionRLtocontinuousactionRLfollowingVanHasselt&
Wiering(2009)andDulac-Arnoldetal.(2015),whereapolicyactsincontinuousactions,suchasthe
featurespaceofrecommenderitems(Figure2),andthenearestdiscreteactionisexecuted.
Our key contribution is SAVO, an actor architecture to find better optimal actions in complex
non-convex Q-landscapes (Â§4). In experiments, we visualize how SAVOâ€™s successively learned
Q-landscapeshavefewerlocaloptima(Â§6.2),makingitmorelikelytofindbetteractionoptimawith
gradientascent. ThisenablesSAVOtooutperformalternativeactorarchitectures,suchassampling
moreactioncandidates(Dulac-Arnoldetal.,2015)andlearninganensembleofactors(Osbandetal.,
2016)(Â§6.1)acrosscontinuousanddiscreteactionRL.
2 RELATED WORK
Q-learning(Watkins&Dayan,1992;Tesauroetal.,1995)isafundamentalvalue-basedRLalgorithm
thatiterativelyupdatesQ-valuestomakeoptimaldecisions. DeepQ-learning(Mnihetal.,2015)
hasbeenappliedtotaskswithmanageablediscreteactionspaces,suchasAtari(Mnihetal.,2013;
Espeholt et al., 2018; Hessel et al., 2018), traffic control (Abdoos et al., 2011), and small-scale
recommender systems (Chen et al., 2019). However, scaling Q-learning to continuous or large
discreteactionspacesrequiresspecializedtechniquestoefficientlymaximizetheQ-function.
Analytical Q-optimization. Analytical optimization of certain Q-functions, such as wire fitting
algorithm(Baird&Klopf,1993)andnormalizedadvantagefunctions(Guetal.,2016;Wangetal.,
2019), allows closed-form action maximization without an actor. Likewise, Amos et al. (2017)
assume that the Q-function is convex in actions and use a convex solver for action selection. In
contrast,theQ-functionsconsideredinthispaperareinherentlynon-convexinactionspace,making
such an assumption invalid. Generally, analytical Q-functions lack the expressiveness of deep
Q-networks(Horniketal.,1989),makingthemunsuitabletomodelcomplextaskslikeinFigure2.
Evolutionary Algorithms for Q-optimization. Evolutionary algorithms like simulated anneal-
ing(Kirkpatricketal.,1983),geneticalgorithms(Srinivas&Patnaik,1994),tabusearch(Glover,
1990),andthecross-entropymethod(CEM)(DeBoeretal.,2005)areemployedinRLforglobal
optimization(Huetal.,2007). ApproachessuchasQT-Opt(Kalashnikovetal.,2018;Leeetal.,
2023;Kalashnikovetal.,2021)utilizeCEMforactionsearch,whilehybridactor-criticmethods
likeCEM-RL(Pourchot&Sigaud,2018),GRAC(Shaoetal.,2022),andCross-EntropyGuided
Policies(Simmons-Edleretal.,2019)combineevolutionarytechniqueswithgradientdescent.Despite
theireffectiveness,CEM-basedmethodsrequirenumerousQ-functionevaluationsandstrugglewith
2Preprint.
high-dimensionalactions(Yanetal.,2019). Incontrast,SAVOachievessuperiorperformancewith
onlyafew(e.g.,three)Q-evaluations,asdemonstratedinexperiments(Â§6).
Actor-CriticMethodswithGradientAscent. Actor-criticmethodscanbeon-policy(Williams,
1992;Schulmanetal.,2015;2017)primarilyguidedbythepolicygradientofexpectedreturns,or
off-policy(Silveretal.,2014;Lillicrapetal.,2015;Fujimotoetal.,2018;Chenetal.,2020)primarily
guidedbytheBellmanerroronthecritic. DeterministicPolicyGradient(DPG)(Silveretal.,2014)
anditsextensionslikeDDPGLillicrapetal.(2015),TD3(Fujimotoetal.,2018)andREDQ(Chen
etal.,2020)optimizeactorsbyfollowingthecriticâ€™sgradient. SoftActor-Critic(SAC)(Haarnoja
et al., 2018) extends DPG to stochastic actors. However, these methods can get trapped in local
optimawithintheQ-functionlandscape. SAVOaddressesthislimitationbyenhancinggradient-based
actor training. This issue also affects stochastic actors, where a local optimum means an action
distribution(insteadofasingleaction)thatfailstominimizetheKLdivergencefromtheQ-function
densityfully,andisapotentialareaforfutureresearch.
Sampling-AugmentedActor-Critic. SamplingmultipleactionsandevaluatingtheirQ-valuesis
a common strategy to find optimal actions. Greedy actor-critic (Neumann et al., 2018) samples
high-entropyactionsandtrainstheactortowardsthebestQ-valuedaction,yetremainssusceptibleto
localoptima. Inlargediscreteactionspaces,methodslikeWolpertinger(Dulac-Arnoldetal.,2015)
usek-nearestneighborstoproposeactions,requiringextensiveQ-evaluationsonupto10%oftotal
actions. Incontrast,SAVOefficientlygenerateshigh-qualityactionproposalsthroughsuccessive
actorimprovementswithoutbeingconfinedtolocalneighborhoods.
Ensemble-Augmented Actor-Critic. Ensembles of policies enhance exploration by providing
diverseactionproposalsthroughvariedinitializations(Osbandetal.,2016;Chen&Peng,2019;Song
etal.,2023;Zheng12etal.,2018;Huangetal.,2017). ThebestactionisselectedbasedonQ-value
evaluations. Unlikeensemblemethods, SAVOsystematicallyeliminateslocaloptima, offeringa
morereliableoptimizationprocessforcomplextasks(Â§6).
3 PROBLEM FORMULATION
OurworktacklestheeffectiveoptimizationoftheQ-valuelandscapeinoff-policyactor-criticmethods
forcontinuousandlarge-discreteactionRL.WemodelataskasaMarkovDecisionProcess(MDP),
definedbyatuple{S,A,T,R,Î³}ofstates,actions,transitionprobabilities,rewardfunction,anda
discountfactor. TheactionspaceAâŠ†RD isaD-dimensionalcontinuousvectorspace. Ateverystep
tintheepisode,theagentreceivesastateobservations âˆˆS fromtheenvironmentandactswith
t
a âˆˆA.Then,itreceivesthenewstateaftertransitions andarewardr .Theobjectiveoftheagent
t t+1 t
istolearnapolicyÏ€(a|s)thatmaximizestheexpecteddiscountedreward,max E [(cid:80) Î³tr ].
Ï€ Ï€ t t
3.1 DETERMINISTICPOLICYGRADIENTS(DPG)
DPG(Silveretal.,2014)isanoff-policyactor-criticalgorithmthattrainsadeterministicactorÂµ to
Ï•
maximizetheQ-function. Thishappensviatwostepsofgeneralizedpolicyiteration,GPI(Sutton&
Barto,1998): policyevaluationestimatestheQ-function(Bellman,1966)andpolicyimprovement
greedilymaximizestheQ-function. ToapproximatetheargmaxovercontinuousactionsinEq.(2),
DPGproposesthepolicygradienttoupdatetheactorlocallyinthedirectionofincreasingQ-value,
QÂµ(s,a)=r(s,a)+Î³E [QÂµ(sâ€²,Âµ(sâ€²))], (1)
sâ€²
Âµ(s)=argmaxQÂµ(s,a), (2)
a
âˆ‡ Ï•J(Ï•)=E sâˆ¼ÏÂµ(cid:104) âˆ‡ aQÂµ(s,a)(cid:12) (cid:12) a=Âµ(s)âˆ‡ Ï•Âµ Ï•(s)(cid:105) . (3)
DDPG(Lillicrapetal.,2015)andTD3(Fujimotoetal.,2018)madeDPGcompatiblewithdeep
networksviatechniqueslikeexperiencereplayandtargetnetworkstoaddressnon-stationarityof
onlineRL,twincriticstomitigateoverestimationbias,targetpolicysmoothingtopreventexploitation
oferrorsintheQ-function,anddelayedpolicyupdatessocriticisreliabletoprovideactorgradients.
3.2 THECHALLENGEOFANACTORMAXIMIZINGACOMPLEXQ-LANDSCAPE
DPG-basedalgorithmstraintheactorfollowingthechainruleinEq.(3). Specifically,itsfirstterm,
âˆ‡ QÂµ(s,a) involves gradient ascent in Q-versus-a landscape. This Q-landscape is often highly
a
non-convex(Figures2,3)andnon-stationarybecauseofitsowntraining. Thismakestheactorâ€™s
3Preprint.
outputÂµ(s)getstuckatsuboptimalQ-values, thusleadingtoinsufficientpolicyimprovementin
Eq.(2). WecandefinethesuboptimalityoftheÂµw.r.t. QÂµatstatesas
âˆ†(QÂµ,Âµ,s)=argmaxQÂµ(s,a)âˆ’QÂµ(s,Âµ(s))â‰¥0. (4)
a
Suboptimalityinactorsisacrucialproblembecauseitleadsto(i)poorsampleefficiencybyslowing
down GPI, and (ii) poor inference performance even with an optimal Q-function, Qâˆ— as seen
inFigure3whereaTD3actorgetsstuckatalocallyoptimumactiona inthefinalQ-function.
0
Thischallengefundamentallydiffersfromthewell-studied
field of non-convex optimization, where non-convexity
arisesinthelossfunctionw.r.t. themodelparameters(Good-
fellow, 2016). In those cases, stochastic gradient-based
optimizationmethodslikeSGDandAdam(Kingma&Ba,
2014)areeffectiveatfindingacceptablelocalminimadue
to the smoothness and high dimensionality of the param-
eterspace,whichoftenallowsforescapefrompoorlocal
optima(Choromanskaetal.,2015). Moreover,overparame-
terizationindeepnetworkscanleadtolosslandscapeswith
numerousgoodminima(Neyshaburetal.,2017).
Incontrast,ourchallengeinvolvesnon-convexityintheQ-
functionw.r.t.theactionspace.Theactorâ€™staskistofind,for
everystates,theactionathatmaximizesQÂµ(s,a). Since
theQ-functioncanbehighlynon-convexandmultimodalin Figure3: Non-convexQ-landscapein
a,thegradientascentstepâˆ‡ QÂµ(s,a)usedinEq.(3)may Inverted-Pendulum-Restrictedleadsto
a
lead the actor to converge to suboptimal local maxima in asuboptimallyconvergedactor.
actionspace. Unlikeparameterspaceoptimization,theactor
cannotrelyonhighdimensionalityoroverparameterizationtosmoothouttheoptimizationlandscape
inactionspacebecausetheQ-landscapeisdeterminedbythetaskâ€™sreward. Furthermore,thenon-
stationarityoftheQ-functionduringtrainingcompoundsthischallenge. Thesepropertiesmakeour
non-convexchallengeunique,requiringaspecializedactortonavigatethecomplexQ-landscape.
Tasks with several local optima in the Q-function include restricted inverted pendulum shown
in Figure 3, where certain regions of the action space are invalid or unsafe, leading to a rugged
Q-landscape(Florenceetal.,2022). Dexterousmanipulationtasksexhibitdiscontinuousbehaviors
likeinsertingaprecisepeginplacewithasmallregionofhigh-valuedactions(Rajeswaranetal.,
2017)andsurgicalroboticshaveahighvarianceinQ-valuesofnearbymotions(Barnoyetal.,2021).
3.2.1 LARGEDISCRETEACTIONRLREFRAMEDASCONTINUOUSACTIONRL
Wediscussanotherpracticaldomainwherenon-convexQ-functionsarepresent. Inlargediscrete
action tasks like recommender systems (Zhao et al., 2018; Zou et al., 2019; Wu et al., 2017), a
commonapproach(VanHasselt&Wiering,2009;Dulac-Arnoldetal.,2015)istousecontinuousrep-
resentationsofactionsasamediumofdecision-making.Givenasetofactions,I ={I ,...,I },a
1 N
predefinedmoduleR:I â†’AassignseachI âˆˆI toitsrepresentationR(I),e.g.,textembedding
ofagivenmovie(Zhouetal.,2010). AcontinuousactionpolicyÏ€(a | s)islearnedintheaction
representationspace,witheachaâˆˆAconvertedtoadiscreteactionI âˆˆI vianearestneighbor,
f (a)=arg min âˆ¥R(I )âˆ’aâˆ¥ .
NN i 2
I iâˆˆI
Importantly,thenearestneighboroperationcreatesachallengingpiece-wisecontinuousQ-function
withsuboptimaatvariousdiscretepointsasshowninFigure2(Jainetal.,2021;2020).
4 APPROACH: SUCCESSIVE ACTORS FOR VALUE OPTIMIZATION (SAVO)
Ourobjectiveistodesignanactorarchitecturethatefficientlydiscoversbetteractionsincomplex,
non-convexQ-functionlandscapes. Wefocusongradient-basedactorsandintroducetwokeyideas:
1. Maximizing Over Multiple Policies: By combining policies using an argmax over their Q-
values,wecanconstructapolicythatperformsatleastaswellasanyindividualpolicy(Â§4.1).
2. SimplifyingtheQ-Landscape: Drawinginspirationfromtabusearch(Glover,1990),wepropose
using actions with good Q-values to eliminate or â€œtabuâ€ the Q-function regions with lower
Q-values,therebyreducinglocaloptimaandfacilitatinggradient-basedoptimization(Â§4.2).
4Preprint.
ğ€ğœğ­ğ¨ğ« Actors
ğœˆ
0
ğœˆ
1
ğœˆ
2
sn
o
itc FiLM
A
ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ’‚âˆ— Deep Set
0 0 1 0 1 2
ğ‚ğ«ğ¢ğ­ğ¢ğœ e ta Action
tS
ğ‘„ Î¨ 0 Î¨ 1 Î¨ 2 ğœˆ
ğ‘–
Surrogates
Figure4: SAVOArchitecture. (left)Q-networkisunchanged. (center)Insteadofasingleactor,we
learnasequenceofactorsandsurrogatenetworksconnectedviaactionpredictions. (right)Condition-
ingonpreviousactionsisdonewiththehelpofadeep-setsummarizerandFiLMmodulation.
4.1 MAXIMIZERACTOROVERACTIONPROPOSALS
WefirstshowhowadditionalactorscanimproveDPGâ€™spolicyimprovementstep. GivenapolicyÂµ
beingtrainedwithDPGoverQ,considerkadditionalarbitrarypoliciesÎ½ ,...,Î½ ,whereÎ½ :S â†’A
1 k i
andletÎ½ =Âµ. WedefineamaximizeractorÂµ fora =Î½ (s)fori=0,1,...,k,
0 M i i
Âµ (s):= argmax Q(s,a), (5)
M
aâˆˆ{a0,a1,...,ak}
Here,Âµ isshowntobeabettermaximizerofQ(s,a)inEq.(2)thanÂµâˆ€s:
M
Q(s,Âµ (s))=maxQ(s,a )â‰¥Q(s,a )=Q(s,Âµ(s)).
M i 0
ai
Therefore,bypolicyimprovementtheorem(Sutton&Barto,1998),VÂµM(s)â‰¥VÂµ(s),provingthat
Âµ isbetterthanasingleÂµforagivenQ. AppendixAprovesthefollowingtheorembyshowing
M
thatpolicyevaluationandimprovementwithÂµ converge.
M
Theorem4.1(ConvergenceofPolicyIterationwithMaximizerActor). Amodifiedpolicyiteration
algorithmwhereÎ½ =ÂµisthecurrentpolicylearnedwithDPGandmaximizeractorÂµ definedin
0 M
Eq.(5),convergesinthetabularsettingtotheoptimalpolicy.
ThisalgorithmisvalidforarbitraryÎ½ ,...Î½ . WeexperimentwithÎ½â€™sobtainedbysamplingfroma
1 k
GaussiancenteredatÂµorensemblingonÂµtogetdiverseactions. However,inhigh-dimensionality,
randomnessaroundÂµisnotsufficienttogetactionproposalstosignificantlyimproveÂµ.
4.2 SUCCESSIVESURROGATESTOREDUCELOCALOPTIMA
TotrainadditionalpoliciesÎ½ thatcanimproveuponÂµ ,weintroducesurrogateQ-functionswith
i M
fewerlocaloptima,inspiredbytheprinciplesoftabusearch(Glover,1990),whichisanoptimization
technique that uses memory structures to avoid revisiting previously explored inferior solutions,
therebyenhancingthesearchforoptimalsolutions. Similarly,oursurrogatefunctionsactasmemory
mechanismsthatâ€œtabuâ€certainregionsoftheQ-functionlandscapedeemedsuboptimalbasedon
previouslyidentifiedgoodactions. Givenaknownactionaâ€ ,wedefineasurrogatefunctionthat
elevatestheQ-valuesofallinferioractionstoQ(s,aâ€ ),whichservesasaconstantthreshold:
Î¨(s,a;aâ€ )=max{Q(s,a),Q(s,aâ€ )}. (6)
Extendingthisidea,wedefineasequenceofsurrogatefunctionsusingtheactionsfromprevious
policies. Leta ={a ,a ,...,a }. Thei-thsurrogatefunctionis:
<i 0 1 iâˆ’1
(cid:26) (cid:27)
Î¨ (s,a;a )=max Q(s,a),maxQ(s,a ) . (7)
i <i j
j<i
Theorem4.2. Forastates âˆˆ S andsurrogatesÎ¨ definedasabove,thenumberoflocaloptima
i
decreaseswitheachsuccessivesurrogate:
N (Q(s,Â·))â‰¥N (Î¨ (s,Â·;a ))â‰¥Â·Â·Â·â‰¥N (Î¨ (s,Â·;a )),
opt opt 1 0 opt k <k
whereN (f)denotesthenumberoflocaloptimaoffunctionf overA.
opt
ProofSketch. AsÎ¨ â†’Î¨ ,theanchorQ-valueinEq.(7)weaklyincreases,max Q(s,a )â‰¤
i i+1 j<i j
max Q(s,a ),thus,eliminatingmorelocalminimabelowit(proofinAppendixB.1).
j<(i+1) j
5Preprint.
4.3 SUCCESSIVEACTORSFORSURROGATEOPTIMIZATION
To effectively reduce local optima using the surrogates Î¨ ,...,Î¨ , we design the policies Î½ to
1 k i
optimize their respective surrogates Î¨ (s,a;a ). Each Î½ focuses on regions where Q(s,a) â‰¥
i <i i
max Q(s,a ),allowingittofindbetteroptimathanpreviouspolicies. TheactorÎ½ isconditioned
j<i j i
onpreviousactions{a ,...,a },summarizedusingdeepsets(Zaheeretal.,2017)(Figure4). The
0 iâˆ’1
maximizeractorÂµ (Eq.(5))selectsthebestactionamongallproposals.
M
WetraineachactorÎ½ usinggradientascentonitssurrogateÎ¨ ,similartoDPG:
i i
âˆ‡ Ï•iJ(Ï• i)=E sâˆ¼ÏÂµM (cid:2) âˆ‡ aÎ¨ i(s,a;a <i)(cid:12) (cid:12) aâˆ‡ Ï•iÎ½ i(s;a <i)(cid:3) . (8)
4.4 APPROXIMATESURROGATEFUNCTIONS
ThesurrogatesÎ¨ havezerogradientswhenQ(s,a)<Ï„,whereÏ„ =max Q(s,a ),
i j<i j
âˆ‡ Î¨ (s,a;a
)=(cid:26) âˆ‡ aQÂµM(s,a) ifQ(s,a)â‰¥Ï„,
a i <i 0 ifQ(s,a)<Ï„.
ThismeansthepolicygradientonlyupdatesÎ½ whenQ(s,a)â‰¥Ï„,whichmayslowdownlearning.
i
Toaddressthisissue,weeasethegradientflowbylearningasmoothlossyapproximationÎ¨Ë† ofÎ¨ .
i i
We approximate each surrogate Î¨
i
with a neural network Î¨Ë† . This ap-
i
proachleveragestheuniversalapprox-
imationtheorem(Horniketal.,1989;
Cybenko,1989)andbenefitsfromem-
pirical evidence that deep networks
caneffectivelylearnnon-smoothfunc-
tions(Imaizumi&Fukumizu,2019).
ThesmoothsurrogateÎ¨Ë† enablescon-
i
tinuous gradient propagation, which
isessentialforoptimizingtheactors Figure5: Inrestrictedinvertedpendulum,givenananchor
Î½ . We train Î¨Ë† to approach Î¨ by Q(a )value,Î¨(left)hassomezero-gradientsurfaceswhich
i i i 0
minimizingthemeansquarederrorat Î¨Ë† (right) approximately follows while allowing non-zero
twocriticalpoints: gradientstowardshighQ-valuestoflowintoitsactorÎ½.
1. ÂµËœ (s)istheactionselectedbythecurrentmaximizeractorÂµ ,havingahighQ-value,
M M
2. Î½ (s;a )istheactionproposedbythei-thactorconditionedonpreviousactionsa ,
i <i <i
ï£® ï£¹
L approx =E sâˆ¼ÏÂµM ï£° (cid:88) (cid:13) (cid:13) (cid:13)Î¨Ë† i(s,a;a <i)âˆ’Î¨ i(s,a;a <i)(cid:13) (cid:13) (cid:13)2 ï£». (9)
2
aâˆˆ{ÂµËœM(s),Î½i(s;a<i)}
ThisdesignensuresÎ¨Ë† isupdatedonhighQ-valueactionsandthusthelandscapeisbiasedtowards
i
thosevalues. ThismakesthegradientflowtrendinthedirectionofhighQ-values. So,evenwhena
i
fromÎ½ fallsinaregionofzerogradientsforÎ¨ ,inÎ¨Ë† wouldprovidepolicygradientinahigher
i i i
Q-valuedirection,ifitexists. Figure5showsÎ¨ andÎ¨Ë† inrestrictedinvertedpendulumtask.
1 1
4.5 SAVO-TD3ALGORITHMANDDESIGNCHOICES
WhiletheSAVOarchitecture(Figure4)canbeintegratedwithanyoff-policyactor-criticalgorithm,
wechoosetoimplementitwithTD3(Fujimotoetal.,2018)duetoitscompatibilitywithcontinuous
andlarge-discreteactionRL(Dulac-Arnoldetal.,2015). UsingtheSAVOactorinTD3enhances
its ability to find better actions in complex Q-function landscapes. Algorithm 1 depicts SAVO
(highlighted)appliedtoTD3. WediscussdesignchoicesinSAVOandvalidatetheminÂ§6.
1. Removing policy smoothing: We eliminate TD3â€™s policy smoothing, which adds noise
to the target action aËœ during critic updates. In non-convex landscapes, nearby actions may
have significantly different Q-values and noise addition might obscure important variations.
6Preprint.
2. ExplorationinAdditionalActors:
Algorithm1SAVO-TD3
AddedactorsÎ½ explorethesurrogateland-
i
scapesforhigh-rewardregionsbyadding InitializeQ,Q ,Âµ, Î¨Ë† ,...,Î¨Ë† , Î½ ,...,Î½
2 1 k 1 k
OU(Lillicrapetal.,2015)orGaussian(Fu- InitializetargetnetworksQâ€² â†Q,Qâ€² â†Q
2 twin
jimotoetal.,2018)noisetotheiractions. InitializereplacebufferB.
fortimestept=1toT do
3. TwinCriticsforSurrogates:
SelectAction:
To prevent overestimation bias in surro-
Evaluatea =Âµ(s),a =Î½ (s;a )
gatesÎ¨Ë† ,weusetwincriticstocomputethe 0 i i <i
i AddperturbationswithOUNoiseaË† =a +Ïµ
targetofeachsurrogate,mirroringTD3. i i i
EvaluateÂµ (s)=argmax QÂµ(s,a)
M aâˆˆ{aË†0,...,aË†k}
4. ConditioningonPreviousActions: Explorationactiona=ÂµËœ (s)=Âµ (s)+Ïµ
M M
Actors Î½ and surrogates Î¨Ë† are condi- Observerewardrandnewstatesâ€²
i i
tionedonprecedingactionsviaFiLMlay- Store(s,a,{aË† i}K i=0,r,sâ€²)inB
ers(Perezetal.,2018)asinFigure4. Update:
SampleNtransitions(s,a,{aË† }K ,r,sâ€²)fromB
5. DiscreteActionSpaceTasks: ComputetargetactionaËœ=Âµi (i s= â€²0
)
M
Weapply1-nearest-neighborf NNbeforeQ- UpdateQ,Q â†r+Î³min{Qâ€²(sâ€²,aËœ),Qâ€²(sâ€²,aËœ)}
valueevaluationtoensuretheQ-function 2 2
isonlyqueriedatin-distributionactions. UpdateÎ¨Ë† withEq.9âˆ€i=1,...k
i
SAVO-TD3 employs SAVO actor to sys- UpdateactorÂµwithEq.3
tematically reduce the local optima in its UpdateactorÎ½ iwithEq.8âˆ€i=1,...k
basealgorithmTD3. Weempiricallyvali- endfor
datetheproposeddesignimprovements.
5 ENVIRONMENTS
WeevaluateSAVOondiscreteandcontinuousactionspaceenvironmentswithchallengingQ-value
landscapes. MoreenvironmentdetailsarepresentedinAppendixCandFigure13.
LocomotioninMujoco. WeevaluateonMuJoCo(Todorovetal.,2012)environmentsofHopper-v4,
Walker2D-v4,InvertedPendulum-v4,andInvertedDoublePendulum-v4.
Locomotion in Restricted Mujoco. We create a restricted loco- ValidActionSpace
OriginalActionSpace
motion suite of the same environments as in MuJoCo. A hard
Q-landscapeisrealizedviahigh-dimensionaldiscontinuitiesthatre-
stricttheactionspace. Concretely,asetofpredefinedhyper-spheres
(asshowninFigure6)intheactionspacearesampledandsetto
bevalidactions,whiletheotherinvalidactionshaveanulleffectif
selected. ThecompletedetailscanbefoundinAppendixC.3.1.
AdroitDexterousManipulation. Rajeswaranetal.(2017)propose
manipulationtaskswithadexterousmulti-fingeredhand. Door: In
thistask,arobotichandisrequiredtoopenadoorwithalatch. The
Figure6: Hopperâ€™s3Dvisual-
challenge lies in the precise manipulation needed to unlatch and
izationofActionSpace.
swingopenthedoorusingthefingers. Hammer: therobotichand
mustuseahammertodriveanailintoaboard. Thistaskteststhehandâ€™sabilitytograspthehammer
correctlyandapplyforceaccuratelytoachievethegoal. Pen: Thistaskinvolvestherobotichand
manipulatingapentoreachaspecificgoalpositionandrotation. Theobjectiveistocontrolthepenâ€™s
orientationandpositionusingfingers,whichdemandsfinemotorskillsandcoordination.
MiningExpeditioninGridWorld. Wedevelopa2DMininggridworldenvironment(Chevalier-
Boisvertetal.,2018)wheretheagent(Figure13)navigatesa2Dmazetoreachthegoal,removing
mineswithcorrectpick-axetoolstoreachthegoalintheshortestpath. Theactionspaceincludes
navigationandtool-choiceactions, withaprocedurally-definedactionrepresentationspace. The
Q-landscapeisnon-convexbecauseofthediverseeffectsofnearbyactionrepresentations.
SimulatedandReal-DataRecommenderSystems. RecSim(Ieetal.,2019)simulatessequential
user interactions in a recommender system with a large discrete action space. The agent must
recommendthemostrelevantitemfromasetof10,000itemsbasedonuserpreferenceinformation.
Theactionrepresentationsaresimulateditemcharacteristicvectorsinsimulatedandmoviereview
embeddingsinthereal-datataskbasedonMovieLens(Harper&Konstan,2015)foritems.
7Preprint.
Performance Profiles Performance Profiles
1.00 1.00
0.75 0.75
0.50 0.50
SAVO SAVO (Ours)
1-Actor (TD3) SAVO - Approximation
0.25 1-Actor, k-Samples (Wolpertinger) 0.25 SAVO - Previous Actions
Evolutionary Actor (CEM) SAVO + Action Smoothing
k-Actors (Ensemble) SAVO + Joint Action
0.00 0.00
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Normalized Score ( ) Normalized Score ( )
(a)SAVOversusbaselineactorarchitectures. (b)SAVOversusablationsofSAVO
Figure7: Aggregateperformanceprofilesusingnormalizedscoresover7tasksand10seedseach.
MineWorld RecSim RecSim-Data
0.98 6.3 55
5.5
0.78 46
4.7
0.58 3.9 37
0.38 3.1 28
0.18 2.3 19
0.02 1.5 10
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Env Steps 1e7 Env Steps 1e7 Env Steps 1e7
Hopper (Restricted) Inverted Pendulum (Restricted) Inv. Dbl Pend. (Restricted) Walker2D (Restricted)
3000 1000 10000 4000
2400 800 8000 3200
1800 600 6000 2400
1200 400 4000 1600
600 200 2000 800
0 0 0 0
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
SAVO (Ours) 1-Actor (TD3) 1-Actor, k-Samples (Wolpertinger) k-Actors (Ensemble)
Evolutionary Actor (CEM) Greedy-AC Greedy-TD3
Figure8: SAVOagainstbaselinesondiscreteandcontinuoustasks. Resultsaveragedover10seeds.
6 EXPERIMENTS
6.1 EFFECTIVENESSOFSAVOINCHALLENGINGQ-LANDSCAPES
WecompareSAVOagainstthefollowingbaselineactorarchitectures,wherek =3:
â€¢ 1-Actor(TD3): Conventionalsingleactorarchitecturewhichissusceptibletolocaloptima.
â€¢ 1-Actor,ksamples(Wolpertinger): Gaussiansamplingcenteredonactorâ€™soutput. Fordiscrete
actions,weselectk-NNdiscreteactionsaroundthecontinuousaction(Dulac-Arnoldetal.,2015).
â€¢ k-Actors(Ensemble): BestQ-valueamongdiverseactionsfromensemble(Osbandetal.,2016).
â€¢ Evolutionaryactor(CEM):IterativeCEMsearchovertheactionspace(Kalashnikovetal.,2018).
â€¢ Greedy-AC:GreedyActorCritic(Neumannetal.,2018)trainsahigh-entropyproposalpolicyand
primaryactortrainedfrombestproposalswithgradientupdates.
â€¢ GreedyTD3: OurversionofGreedy-ACwithTD3explorationandupdateimprovements.
â€¢ SAVO:OurmethodwithksuccessiveactorsandsurrogateQ-landscapes.
WeablatethecrucialcomponentsanddesigndecisionsinSAVO:
â€¢ SAVO-Approximation: removestheapproximatesurrogates(Â§4.4),usingÎ¨ insteadofÎ¨Ë† .
i i
â€¢ SAVO-PreviousActions: removesconditioningona inSAVOâ€™sactorsandsurrogates.
<i
â€¢ SAVO+ActionSmoothing: TD3â€™spolicysmoothing(Fujimotoetal.,2018)computeQ-targets.
â€¢ SAVO+JointAction: trainsanactorwithajointactionspaceof3Ã—D. Thekactionsamplesare
obtainedbysplittingthejointactionintoDdimensions. ValidatessuccessivenatureofSAVO.
Aggregateperformance. Weutilizeperformanceprofiles(Agarwaletal.,2021)toaggregateresults
acrossdifferentenvironmentsinFigure7a(evaluationmechanismdetailedinAppendixG.1). SAVO
8
nruteR
lavE
>
erocs
htiw
snur
fo
noitcarF
etaR
sseccuS
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
>
erocs
htiw
snur
fo
noitcarF
nruteR
lavE
nruteR
lavEPreprint.
consistentlyoutperformsbaselineactorarchitectureslikesingle-actor(TD3)andsampling-augmented
actor(Wolpertinger),showingwiderobustnessacrosschallengingQ-landscapes. InFigure7b,SAVO
outperformsitsablations,validatingeachproposedcomponentanddesigndecision.
Per-environmentresults. Indiscreteactiontasks,theQ-valuelandscapeisonlywell-definedat
exactactionrepresentationsandnearbydiscreteactionsmighthaveverydifferentvalues(Â§3.2.1).
ThismakestheQ-valuelandscapeuneven,withmultiplepeaksandvalleys(Figure2). Forexample,
actionsinMiningExpeditioninvolvebothnavigationandtool-selectionwhicharequitedifferent,
whileRecSimandRecSim-Datahavemanydiverseitemstochoosefrom. MethodslikeWolpertinger
thatsamplemanyactionsalocalneighborhoodperformbetterthanTD3whichconsidersasingle
action(Figure8). However,SAVOachievesthebestperformancebydirectlysimplifyingthenon-
convexQ-landscape. Inrestrictedlocomotion,thereareseveralgoodactionsthatarefarapart. SAVO
actorscansearchandexplorewidelytooptimizetheQ-landscapebetterthanonlynearbysampled
actions. Figure16ablatesSAVOinall7environmentsandshowsthatthemostcriticalfeaturesare
itssuccessivenature,removingpolicysmoothing,andapproximatesurrogates.
6.2 Q-LANDSCAPEANALYSIS: DOSUCCESSIVESURROGATESREDUCELOCALOPTIMA?
Figure9visualizessurrogatelandscapesinInverted-Pendulum-Restrictedforagivenstates. Succes-
sivepruningandapproximationsmooththeQ-landscapes,reducinglocaloptima. Asingleactorgets
stuckatalocaloptimuma (left),butsurrogateÎ¨Ë† usesa asananchortofindabetteroptimum
0 1 0
a . ThemaximizerpolicyfinallyselectsthehighestQ-valuedactionamonga ,a ,a . Figure24
1 0 1 2
extendsthisvisualizationtoInverted-Double-Pendulum-Restricted. Figure23showshowoneactoris
sufficientintheconvexQ-landscapeofunrestrictedInverted-Pendulum-v4. Figures25,26showhow
Hopper-v4Q-landscapeprovidesapathtoglobaloptimum,whileHopper-Restrictedisnon-convex.
(a)Q(s,a 0) (b)Î¨Ë† 1(s,a 1;a 0) (c)Î¨Ë† 2(s,a 2;{a 0,a 1})
Figure9:EachsuccessivesurrogatelearnsaQ-landscapewithfewerlocaloptimaandthusiseasierto
optimizebyitsactor. SAVOhelpsasingleactorescapethelocaloptimuma inInvertedPendulum.
0
6.3 CHALLENGINGDEXTEROUSMANIPULATION(ADROIT)
In Adroit dexterous manipulation tasks (Door, Pen, Hammer) (Rajeswaran et al., 2017), SAVO
improvesthesampleefficiencyofTD3(Figure10). Thenon-convexityinQ-landscapelikelyarises
fromnearbyactionshavinghighvarianceoutcomeslikegrasping,missing,dropping,ornoimpact.
Adriot Door Adriot Pen Adriot Hammer
1.0 0.6 1.0
TD3 + SAVO TD3 + SAVO TD3 + SAVO
TD3 TD3 TD3
0.8 0.8
0.4 0.6 0.6
0.4 0.4
0.2
0.2 0.2
0.0 0.0 0.0
0.00 0.75 1.50 2.25 3.00 0.00 0.75 1.50 2.25 3.00 0.00 0.75 1.50 2.25 3.00
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
Figure10: SAVOimprovesthesample-efficiencyofTD3onAdroitdexterousmanipulationtasks.
6.4 QUANTITATIVEANALYSIS: THEEFFECTOFSUCCESSIVEACTORSANDSURROGATES
Weinvestigatetheeffectofincreasingthenumberofsuccessiveactor-surrogatesinSAVOinPendulum
(Figure11a)andMineWorld(Figure11b). Additionalactor-surrogatessignificantlyhelptoreduce
severelocaloptimainitially. However,theimprovementsaturatesasthesuboptimalitygapreduces.
9
etaR
sseccuS
lavE
etaR
sseccuS
lavE
etaR
sseccuS
lavEPreprint.
Inv. Dbl Pend. (Restricted) MineWorld Inv. Dbl Pend. (Restricted)
10000 10000
1.0 1st-Actor
Subsequent-Actors
8000 0.8 8000
6000 0.6 6000
4000 0.4 4000
Length=10 Length=5
2000 L Le en ng gt th h= =1 50 0.2 L Le en ng gt th h= =4 3 2000
Length=3 Length=2
Length=1 Length=1
0 0.0 0
0 125000 250000 375000 500000 0.00 1.25 2.50 3.75 5.00 0.0 0.5 1.0 1.5 2.0
Env Steps Env Steps 1e6 Env Steps 1e6
(a)LengthAnalysisPendulum (b)LengthanalysisMineWorld (c)ReturnImprovement
Figure11: (L)Moresuccessiveactor-surrogatesarebetter,(R)SAVOv/ssingle-actoroninference.
Next,weshowthatsuccessiveactorsareneededbecauseasingleactorcangetstuckinlocaloptima
evenwithanoptimalQ-function. InFigure11c,weconsideraSAVOagenttrainedtooptimalitywith
3actors. Whenweremovetheadditionalactors,theremainingsingle-actoragentresemblesTD3
trainedtomaximizeanâ€œoptimalâ€Q-function. However,thesignificantperformancegapindicates
thatthesingleactorcouldnotfindoptimalactionsforthegivenQ-function.
6.5 DOESRLWITHRESETSADDRESSTHEISSUEOFQ-FUNCTIONOPTIMIZATION?
Primacybias(Nikishinetal.,2022;Kimetal.,2024)occurs
whenanagentistrappedinsuboptimalbehaviorsfromearly
training. To mitigate this, methods like resetting parameters
andre-learningfromthereplaybufferaimtoreducerelianceon
initialsamples. WerunTD3inMineEnvwitheitherafull-reset
or last-layer reset every 200k, 500k, or 1 million iterations.
None of these versions outperformed the original TD3 algo-
rithmwithoutresets. Thisisbecauseresettingdoesnothelp
anactortonavigatetheQ-landscapebetterandcanevencause
anotherwiseoptimalactortogetstuckinasuboptimalsolu-
tionduringretraining. Incontrast,theSAVOactorarchitecture
specificallyaddresses thenon-convexQ-landscapes, beinga
morerobustmethodtofindingclosertooptimalactions. Figure 12: Reset (primacy bias)
doesnotimproveQ-optimization.
6.6 FURTHEREXPERIMENTSTOVALIDATESAVO
â€¢ Unrestrictedlocomotion. Figure15showsthatbothSAVOandbaselinesachieveoptimalper-
formanceinsimpleQ-landscapes,confirmingeffectivehyperparametertuning(Â§G.4,Â§G.3)and
indicatingthatthebaselinesunderperformduetothecomplexityintroducedinQ-landscapes.
â€¢ SAVOorthogonaltoSAC.Figure17showsthatSAVO+TD3>SAC>TD3,indicatingthatSACâ€™s
stochasticpolicydoesnotaddressnon-convexity,butcanitselfsufferfromlocaloptima(Figure18)
â€¢ DesignChoices.Figure20showsthatLSTM,DeepSet,andTransformersareallvalidchoicesas
summarizersofprecedingactionsa inSAVO.Figure21showsthatFiLMconditioningona
<i <i
especiallyhelpsfordiscreteactionspacetasksbuthasasmallereffectincontinuousactionspace.
InFigure22a,wefindOrnstein-Uhlenbeck(OU)noiseandGaussiannoisetobelargelyequivalent.
â€¢ MassiveDiscreteActions. SAVOoutperformsinRecSimwith100kand500kactions(Figure19).
7 LIMITATIONS AND CONCLUSION Method GPUMem. Return Time
TD3 619MB 1107.795 0.062s
IntroducingmoreactorsinSAVOhasnegligible
SAVOk=3 640MB 2927.149 0.088s
influenceonGPUmemory,butleadstolonger
SAVOk=5 681MB 3517.319 0.122s
inference time (Table 1). However, even for
3 actor-surrogates, SAVO achieves significant
Table1: Computev/sPerformanceGain(Mujoco)
improvementsinallourexperiments. Further,
fortaskswithasimpleconvexQ-landscape,asingleactordoesnotgetstuckinlocaloptima,making
thegainfromSAVOnegligible. Inconclusion,weimproveQ-landscapeoptimizationindeterministic
policygradientRLwithSuccessiveActorsforValueOptimization(SAVO)inbothcontinuousand
large discrete action spaces. We demonstrate with quantitative and qualitative analyses how the
improvedoptimizationofQ-landscapewithSAVOleadstobettersampleefficiencyandperformance.
10
nruteR
lavE
etaR
sseccuS
lavE
nruteR
lavEPreprint.
ACKNOWLEDGEMENTS
ThisworkwassupportedbyInstituteofInformation&communicationsTechnologyPlanning&
Evaluation(IITP)grant(No.RS-2019-II190075,ArtificialIntelligenceGraduateSchoolProgram,
KAIST)andNationalResearchFoundationofKorea(NRF)grant(NRF-2021H1D3A2A03103683,
BrainPoolResearchProgram),fundedbytheKoreagovernment(MSIT).AyushJainwassupported
partly as intern at Naver AI Lab during the initiation of the project. We appreciate the fruitful
discussionswithmembersofCLVRLabatKAISTandLiraLabatUSC.
BIBLIOGRAPHY
MonirehAbdoos, NasserMozayani, andAnaLCBazzan. Trafficlightcontrolinnon-stationary
environmentsbasedonmultiagentq-learning. In201114thInternationalIEEEconferenceon
intelligenttransportationsystems(ITSC),pp.1580â€“1585.IEEE,2011.
RishabhAgarwal,MaxSchwarzer,PabloSamuelCastro,AaronCCourville,andMarcBellemare.
Deepreinforcementlearningattheedgeofthestatisticalprecipice.Advancesinneuralinformation
processingsystems,34:29304â€“29320,2021.
BrandonAmos,LeiXu,andJZicoKolter. Inputconvexneuralnetworks. InInternationalconference
onmachinelearning,pp.146â€“155.PMLR,2017.
LeemonCBairdandAHarryKlopf. Reinforcementlearningwithhigh-dimensionalcontinuous
actions. WrightLaboratory,Wright-PattersonAirForceBase,Tech.Rep.WL-TR-93-1147,15,
1993.
YotamBarnoy,MollyOâ€™Brien,WillWang,andGregoryHager. Roboticsurgerywithleanreinforce-
mentlearning. arXivpreprintarXiv:2105.01006,2021.
RichardBellman. Dynamicprogramming. Science,153(3731):34â€“37,1966.
LukasBiewald. Experimenttrackingwithweightsandbiases. Softwareavailablefromwandb.com,
2:233,2020.
GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,and
WojciechZaremba. Openaigym. arXivpreprintarXiv:1606.01540,2016.
GangChenandYimingPeng. Off-policyactor-criticinanensemble: Achievingmaximumgeneral
entropy and effective environment exploration in deep reinforcement learning. arXiv preprint
arXiv:1902.05551,2019.
XinshiChen,ShuangLi,HuiLi,ShaohuaJiang,YuanQi,andLeSong. Generativeadversarialuser
modelforreinforcementlearningbasedrecommendationsystem. InInternationalConferenceon
MachineLearning,pp.1052â€“1061.PMLR,2019.
XinyueChen,CheWang,ZijianZhou,andKeithWRoss. Randomizedensembleddoubleq-learning:
Learningfastwithoutamodel. InInternationalConferenceonLearningRepresentations,2020.
MaximeChevalier-Boisvert,LucasWillems,andSumanPal. Minimalisticgridworldenvironment
foropenaigym. https://github.com/maximecb/gym-minigrid,2018.
AnnaChoromanska,MIkaelHenaff,MichaelMathieu,GerardBenArous,andYannLeCun.TheLoss
SurfacesofMultilayerNetworks. InGuyLebanonandS.V.N.Vishwanathan(eds.),Proceedings
oftheEighteenthInternationalConferenceonArtificialIntelligenceandStatistics,volume38of
ProceedingsofMachineLearningResearch,pp.192â€“204,SanDiego,California,USA,09â€“12
May2015.PMLR. URLhttps://proceedings.mlr.press/v38/choromanska15.
html.
GeorgeCybenko. Approximationbysuperpositionsofasigmoidalfunction. Mathematicsofcontrol,
signalsandsystems,2(4):303â€“314,1989.
Pieter-TjerkDeBoer, DirkPKroese, ShieMannor, andReuvenYRubinstein. Atutorialonthe
cross-entropymethod. Annalsofoperationsresearch,134(1):19â€“67,2005.
11Preprint.
GabrielDulac-Arnold,RichardEvans,HadovanHasselt,PeterSunehag,TimothyLillicrap,Jonathan
Hunt,TimothyMann,TheophaneWeber,ThomasDegris,andBenCoppin. Deepreinforcement
learninginlargediscreteactionspaces. arXivpreprintarXiv:1512.07679,2015.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam
Doron,VladFiroiu,TimHarley,IainDunning,etal. Impala: Scalabledistributeddeep-rlwith
importanceweightedactor-learnerarchitectures. InInternationalconferenceonmachinelearning,
pp.1407â€“1416.PMLR,2018.
PeteFlorence,CoreyLynch,AndyZeng,OscarARamirez,AyzaanWahid,LauraDowns,Adrian
Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In
ConferenceonRobotLearning,pp.158â€“168.PMLR,2022.
ScottFujimoto,HerkeHoof,andDavidMeger. Addressingfunctionapproximationerrorinactor-
criticmethods. InInternationalconferenceonmachinelearning,pp.1587â€“1596.PMLR,2018.
FredGlover. Tabusearch: Atutorial. Interfaces,20(4):74â€“94,1990.
IanGoodfellow. Deeplearning,2016.
IvoGrondman,LucianBusoniu,GabrielADLopes,andRobertBabuska. Asurveyofactor-critic
reinforcementlearning: Standardandnaturalpolicygradients. IEEETransactionsonSystems,
Man,andCybernetics,partC(applicationsandreviews),42(6):1291â€“1307,2012.
ShixiangGu, TimothyLillicrap, IlyaSutskever, andSergeyLevine. Continuousdeepq-learning
withmodel-basedacceleration. InInternationalconferenceonmachinelearning,pp.2829â€“2838.
PMLR,2016.
TuomasHaarnoja, AurickZhou, Pieter Abbeel, andSergey Levine. Softactor-critic: Off-policy
maximumentropydeepreinforcementlearningwithastochasticactor. InInternationalconference
onmachinelearning,pp.1861â€“1870.PMLR,2018.
F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm
transactionsoninteractiveintelligentsystems(tiis),5(4):1â€“19,2015.
MatteoHessel,JosephModayil,HadoVanHasselt,TomSchaul,GeorgOstrovski,WillDabney,Dan
Horgan,BilalPiot,MohammadAzar,andDavidSilver. Rainbow: Combiningimprovementsin
deepreinforcementlearning. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume32,2018.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universalapproximators. Neuralnetworks,2(5):359â€“366,1989.
Jiaqiao Hu, Michael C Fu, and Steven I Marcus. A model reference adaptive search method for
globaloptimization. Operationsresearch,55(3):549â€“568,2007.
ZheweiHuang,ShuchangZhou,BoErZhuang,andXinyuZhou. Learningtorunwithactor-critic
ensemble. arXivpreprintarXiv:1712.08987,2017.
ZhihengHuang,WeiXu,andKaiYu. Bidirectionallstm-crfmodelsforsequencetagging,2015.
EugeneIe,Chih-weiHsu,MartinMladenov,VihanJain,SanmitNarvekar,JingWang,RuiWu,and
CraigBoutilier. Recsim: Aconfigurablesimulationplatformforrecommendersystems. arXiv
preprintarXiv:1909.04847,2019.
MasaakiImaizumiandKenjiFukumizu.Deepneuralnetworkslearnnon-smoothfunctionseffectively.
InThe22ndinternationalconferenceonartificialintelligenceandstatistics,pp.869â€“878.PMLR,
2019.
AyushJain,AndrewSzot,andJosephLim.Generalizationtonewactionsinreinforcementlearning.In
HalDaumÃ©IIIandAartiSingh(eds.),Proceedingsofthe37thInternationalConferenceonMachine
Learning,volume119ofProceedingsofMachineLearningResearch,pp.4661â€“4672.PMLR,
13â€“18Jul2020. URLhttp://proceedings.mlr.press/v119/jain20b.html.
12Preprint.
AyushJain,NorioKosaka,Kyung-MinKim,andJosephJLim.Knowyouractionset:Learningaction
relationsforreinforcementlearning. InInternationalConferenceonLearningRepresentations,
2021.
ShamMachandranathKakade. Onthesamplecomplexityofreinforcementlearning. Universityof
London,UniversityCollegeLondon(UnitedKingdom),2003.
DmitryKalashnikov,AlexIrpan,PeterPastor,JulianIbarz,AlexanderHerzog,EricJang,Deirdre
Quillen,EthanHolly,MrinalKalakrishnan,VincentVanhoucke,etal. Scalabledeepreinforcement
learningforvision-basedroboticmanipulation. InConferenceonRobotLearning,pp.651â€“673.
PMLR,2018.
Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski,
ChelseaFinn,SergeyLevine,andKarolHausman. Mt-opt: Continuousmulti-taskroboticrein-
forcementlearningatscale. arXivpreprintarXiv:2104.08212,2021.
WoojunKim,YongjaeShin,JongeuiPark,andYoungchulSung. Sample-efficientandsafedeep
reinforcementlearningviaresetdeepensembleagents.AdvancesinNeuralInformationProcessing
Systems,36,2024.
DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. arXivpreprint
arXiv:1412.6980,2014.
ScottKirkpatrick,CDanielGelattJr,andMarioPVecchi. Optimizationbysimulatedannealing.
science,220(4598):671â€“680,1983.
Kuang-HueiLee,TedXiao,AdrianLi,PaulWohlhart,IanFischer,andYaoLu. Pi-qt-opt: Predictive
informationimprovesmulti-taskroboticreinforcementlearningatscale. InConferenceonRobot
Learning,pp.1696â€“1707.PMLR,2023.
TimothyPLillicrap, JonathanJHunt, AlexanderPritzel, NicolasHeess, TomErez, YuvalTassa,
DavidSilver,andDaanWierstra. Continuouscontrolwithdeepreinforcementlearning. arXiv
preprintarXiv:1509.02971,2015.
MichaelLedermanLittman. Algorithmsforsequentialdecision-making. BrownUniversity,1996.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra,andMartinRiedmiller. Playingatariwithdeepreinforcementlearning. arXivpreprint
arXiv:1312.5602,2013.
VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,MarcGBellemare,
AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal. Human-levelcontrol
throughdeepreinforcementlearning. nature,518(7540):529â€“533,2015.
SamuelNeumann,SungsuLim,AjinJoseph,YangchenPan,AdamWhite,andMarthaWhite.Greedy
actor-critic: A new conditional cross-entropy method for policy improvement. arXiv preprint
arXiv:1810.09103,2018.
BehnamNeyshabur,SrinadhBhojanapalli,DavidMcAllester,andNatiSrebro. Exploringgeneraliza-
tionindeeplearning. Advancesinneuralinformationprocessingsystems,30,2017.
EvgeniiNikishin,MaxSchwarzer,PierlucaDâ€™Oro,Pierre-LucBacon,andAaronCourville. The
primacybiasindeepreinforcementlearning. InInternationalconferenceonmachinelearning,pp.
16828â€“16847.PMLR,2022.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrappeddqn. Advancesinneuralinformationprocessingsystems,29,2016.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performancedeeplearninglibrary. Advancesinneuralinformationprocessingsystems,32,
2019.
13Preprint.
EthanPerez,FlorianStrub,HarmDeVries,VincentDumoulin,andAaronCourville. Film: Visual
reasoningwithageneralconditioninglayer. InProceedingsoftheAAAIconferenceonartificial
intelligence,volume32,2018.
AloÃ¯sPourchotandOlivierSigaud. Cem-rl: Combiningevolutionaryandgradient-basedmethodsfor
policysearch. arXivpreprintarXiv:1810.01222,2018.
AravindRajeswaran, VikashKumar, AbhishekGupta, GiuliaVezzani, JohnSchulman, Emanuel
Todorov,andSergeyLevine. Learningcomplexdexterousmanipulationwithdeepreinforcement
learninganddemonstrations. arXivpreprintarXiv:1709.10087,2017.
JohnSchulman,SergeyLevine,PieterAbbeel,MichaelJordan,andPhilippMoritz. Trustregion
policyoptimization. InInternationalconferenceonmachinelearning,pp.1889â€“1897.PMLR,
2015.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
LinShao,YifanYou,MengyuanYan,ShenliYuan,QingyunSun,andJeannetteBohg. Grac: Self-
guidedandself-regularizedactor-critic. InConferenceonRobotLearning,pp.267â€“276.PMLR,
2022.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministicpolicygradientalgorithms. InInternationalconferenceonmachinelearning,pp.
387â€“395.Pmlr,2014.
RileySimmons-Edler,BenEisner,EricMitchell,SebastianSeung,andDanielLee. Q-learningfor
continuousactionswithcross-entropyguidedpolicies. arXivpreprintarXiv:1903.10605,2019.
Edward Jay Sondik. The optimal control of partially observable Markov processes. Stanford
University,1971.
Yanjie Song, Ponnuthurai Nagaratnam Suganthan, Witold Pedrycz, Junwei Ou, Yongming He,
Yingwu Chen, and Yutong Wu. Ensemble reinforcement learning: A survey. Applied Soft
Computing,pp.110975,2023.
MandavilliSrinivasandLalitMPatnaik. Geneticalgorithms: Asurvey. computer,27(6):17â€“26,
1994.
RichardSSuttonandAndrewGBarto. ReinforcementLearning: AnIntroduction. TheMITPress,
1998.
GeraldTesauroetal. Temporaldifferencelearningandtd-gammon. CommunicationsoftheACM,38
(3):58â€“68,1995.
EmanuelTodorov,TomErez,andYuvalTassa. Mujoco: Aphysicsengineformodel-basedcontrol.
In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026â€“5033.
IEEE,2012.
HadoVanHasseltandMarcoAWiering. Usingcontinuousactionspacestosolvediscreteproblems.
In2009InternationalJointConferenceonNeuralNetworks,pp.1149â€“1156.IEEE,2009.
PinWang,HanhanLi,andChing-YaoChan. Quadraticq-networkforlearningcontinuouscontrolfor
autonomousvehicles. arXivpreprintarXiv:1912.00074,2019.
ChristopherJCHWatkinsandPeterDayan. Q-learning. Machinelearning,8(3):279â€“292,1992.
RonaldJWilliams. Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcement
learning. Machinelearning,8:229â€“256,1992.
QingyunWu,HongningWang,LiangjieHong,andYueShi. Returningisbelieving: Optimizinglong-
termuserengagementinrecommendersystems. InProceedingsofthe2017ACMonConference
onInformationandKnowledgeManagement,pp.1927â€“1936,2017.
14Preprint.
MengyuanYan,AdrianLi,MrinalKalakrishnan,andPeterPastor.Learningprobabilisticmulti-modal
actormodelsforvision-basedroboticgrasping. In2019InternationalConferenceonRoboticsand
Automation(ICRA),pp.4804â€“4810.IEEE,2019.
ManzilZaheer,SatwikKottur,SiamakRavanbakhsh,BarnabasPoczos,RussRSalakhutdinov,and
AlexanderJSmola. Deepsets. Advancesinneuralinformationprocessingsystems,30,2017.
Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. Deep re-
inforcement learning for page-wise recommendations. Proceedings of the 12th ACM Con-
ference on Recommender Systems, Sep 2018. doi: 10.1145/3240323.3240374. URL http:
//dx.doi.org/10.1145/3240323.3240374.
ZhuobinZheng12,ChunYuan,ZhihuiLin12,andYangyangCheng12. Self-adaptivedoubleboot-
strappedddpg. InInternationalJointConferenceonArtificialIntelligence,2018.
TaoZhou,ZoltÃ¡nKuscsik,Jian-GuoLiu,MatÃºÅ¡Medo,JosephRushtonWakeling,andYi-Cheng
Zhang. Solvingtheapparentdiversity-accuracydilemmaofrecommendersystems. Proceedingsof
theNationalAcademyofSciences,107(10):4511â€“4515,2010.
LixinZou,LongXia,ZhuoyeDing,JiaxingSong,WeidongLiu,andDaweiYin. Reinforcement
learning to optimize long-term user engagement in recommender systems. In Proceedings of
the25thACMSIGKDDInternationalConferenceonKnowledgeDiscovery&DataMining,pp.
2810â€“2818,2019.
15Preprint.
APPENDIX
Table of Contents
A ProofofConvergenceofMaximizerActorinTabularSettings 17
B ProofofReducingNumberofLocalOptimainSuccessiveSurrogates 18
C EnvironmentDetails 19
C.1 MiningEnv . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C.2 RecSim . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.3 ContinuousControl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D AdditionalResults 22
D.1 ExperimentsonContinuousControl(UnrestrictedMujoco). . . . . . . . . . . . 22
D.2 Per-EnvironmentAblationResults . . . . . . . . . . . . . . . . . . . . . . . . 22
D.3 SACisOrthogonaltoSAVO. . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.4 IncreasingSizeofDiscreteActionSpaceinRecSim . . . . . . . . . . . . . . . 24
E ValidatingSAVODesignChoices 25
E.1 DesignChoices:Actionsummarizers . . . . . . . . . . . . . . . . . . . . . . . 25
E.2 ConditioningonPreviousActions:FiLMvs.MLP . . . . . . . . . . . . . . . . 25
E.3 ExplorationNoisecomparison:OUNoisevsGaussian . . . . . . . . . . . . . . 25
F NetworkArchitectures 26
F.1 SuccessiveActors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.2 SuccessiveSurrogates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.3 ListSummarizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.4 Feature-wiseLinearModulation(FiLM) . . . . . . . . . . . . . . . . . . . . . 27
G ExperimentandEvaluationSetup 27
G.1 AggregatedResults:PerformanceProfiles. . . . . . . . . . . . . . . . . . . . . 27
G.2 ImplementationDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
G.3 CommonHyperparameterTuning . . . . . . . . . . . . . . . . . . . . . . . . . 27
G.4 Hyperparameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
H Q-ValueLandscapeVisualizations 28
H.1 1-DimensionalActionSpaceEnvironments . . . . . . . . . . . . . . . . . . . . 28
H.2 High-DimensionalActionSpaceEnvironments . . . . . . . . . . . . . . . . . . 29
16Preprint.
A PROOF OF CONVERGENCE OF MAXIMIZER ACTOR IN TABULAR SETTINGS
TheoremA.1(ConvergenceofPolicyIterationwithMaximizerActor). InafiniteMarkovDecision
Process(MDP)withfinitestatespaceS,consideramodifiedpolicyiterationalgorithmwhere,at
eachiterationn,wehaveasetofk+1policies{Î½ ,Î½ ,...,Î½ },withÎ½ =Âµ beingthepolicyat
0 1 k 0 n
thecurrentiterationlearnedwithDPG.WedefinethemaximizeractorÂµ as:
M
Âµ (s)=arg max QÂµn(s,a), (10)
M
aâˆˆ{Î½0(s),Î½1(s),...,Î½k(s)}
where QÂµn(s,a) is the action-value function for policy Âµ n. Then, the modified policy iteration
algorithmusingthemaximizeractorisguaranteedtoconvergetoafinalpolicyÂµ .
N
Proof. Toproveconvergence,wewillshowthatthesequenceofpoliciesÂµ yieldsmonotonically
n
non-decreasingvaluefunctionsthatconvergetoastablevaluefunctionVN.
POLICYEVALUATIONCONVERGES
Thus,iterativelyapplyingTÏ€ startingfromanyinitialQ convergestotheuniquefixedpointQÏ€.
0
GiventhecurrentpolicyÂµ n,thepolicyevaluationcomputestheaction-valuefunctionQÂµn,satisfying:
(cid:88)
QÂµn(s,a)=R(s,a)+Î³ P(s,a,sâ€²)VÂµn(sâ€²),
sâ€²
whereVÂµn(sâ€²)=QÂµn(sâ€²,Âµ n(sâ€²)).
Inthetabularsetting,theBellmanoperatorTÂµn definedby
(cid:88)
[TÂµnQ](s,a)=R(s,a)+Î³ P(s,a,sâ€²)Q(sâ€²,Âµ (sâ€²))
n
sâ€²
isacontractionmappingwithrespecttothemaxnormâˆ¥Â·âˆ¥ withcontractionfactorÎ³,
âˆ
âˆ¥TÂµnQâˆ’TÂµnQâ€²âˆ¥ â‰¤Î³âˆ¥Qâˆ’Qâ€²âˆ¥ .
âˆ âˆ
Therefore,iterativelyapplyingTÂµn convergestotheuniquefixedpointQÂµn.
POLICYIMPROVEMENTWITHDPGANDMAXIMIZERACTOR
Step1: DPGUpdate
WedefineÂµËœ astheDPGpolicythatlocallyupdatesÂµ towardsmaximizingtheexpectedreturn
n n
basedonQÂµn. Foreachstates,weperformagradientascentstepusingtheDeepPolicyGradient
(DPG)methodtoobtainanimprovedpolicyÂµËœ :
n
ÂµËœ n(s)â†Âµ n(s)+Î±âˆ‡
aQÂµn(s,a)(cid:12)
(cid:12) a=Âµn(s),
whereÎ±>0isasuitablestepsize.
ThisDPGgradientstepleadstolocalpolicyimprovementfollowingoverÂµ (Silveretal.,2014):
n
VÂµËœn(s)â‰¥VÂµn(s), âˆ€sâˆˆS.
(b)MaximizerActor
GivenadditionalpoliciesÎ½ ,...,Î½ ,definethemaximizeractorÂµ as:
1 k n+1
Âµ (s)=arg max QÂµn(s,a).
n+1
aâˆˆ{ÂµËœn(s),Î½1(s),...,Î½k(s)}
SinceÂµ n+1(s)selectstheactionmaximizingQÂµn(s,a)amongcandidates,wehave:
QÂµn(s,Âµ (s))= max QÂµn(s,a)â‰¥QÂµn(s,ÂµËœ (s))â‰¥VÂµn(s).
n+1 n
aâˆˆ{ÂµËœn(s),Î½1(s),...,Î½k(s)}
17Preprint.
BythePolicyImprovementTheorem,sinceQÂµn(s,Âµ n+1(s))â‰¥VÂµn(s)foralls,itfollowsthat:
VÂµn+1(s)â‰¥VÂµn(s), âˆ€sâˆˆS.
Thus,thesequence{VÂµn}ismonotonicallynon-decreasing.
CONVERGENCEOFPOLICYITERATION
Sincethesequence{VÂµn}ismonotonicallynon-decreasingandboundedabovebyVâˆ—,itconverges
tosomeVâˆ â‰¤Vâˆ—. Giventhefinitenumberofpossiblepolicies,thesequence{Âµ }musteventually
n
repeatapolicy. SupposethatatsomeiterationN,thepolicyrepeats,i.e.,Âµ =Âµ .
N+1 N
Atthispoint,sincethepolicyhasnâ€™tchanged,wehave:
Âµ (s)=arg max QÂµN(s,a), âˆ€sâˆˆS.
N
aâˆˆ{ÂµËœN(s),Î½1(s),...,Î½k(s)}
SinceÂµËœ (s)isobtainedbyperformingaDPGupdateonÂµ (s),andwehavethatÂµ (s)maximizes
N N N
QÂµN(s,a)among{ÂµËœ N(s),Î½ 1(s),...,Î½ k(s)},itmustbethat:
QÂµN(s,Âµ (s))â‰¥QÂµN(s,a), âˆ€aâˆˆ{ÂµËœ (s),Î½ (s),...,Î½ (s)}.
N N 1 k
Moreover,sinceÂµËœ (s)isobtainedviagradientascentfromÂµ (s),andyetdoesnotyieldahigher
N N
Q-value,itimpliesthat:
âˆ‡
aQÂµN(s,a)(cid:12)
(cid:12)
a=ÂµN(s)
=0.
ThissuggeststhatÂµ N(s)isalocalmaximumofQÂµN(s,a). Thisshowsthatthismodificationtothe
policyiterationalgorithmofDPGisguaranteedtoconverge.
Sincetheset{ÂµËœ (s),Î½ (s),...,Î½ (s)}includesmoreactionsfromA,Âµ (s)istheactionthatbetter
N 1 k N
maximizesQÂµN(s,a)thanÂµËœ N. Therefore,Âµ
N
isagreedierpolicywithrespecttoQÂµN thanÂµËœ N.
B PROOF OF REDUCING NUMBER OF LOCAL OPTIMA IN SUCCESSIVE
SURROGATES
TheoremB.1. ConsiderastatesâˆˆS,thefunctionQasdefinedinEq.1,andthesurrogatefunctions
Î¨ asdefinedinEq.7. LetN (f)denotethenumberoflocaloptima(assumedcountable)ofa
i opt
functionf :Aâ†’R,whereAistheactionspace. Then,
N (Q(s,a))â‰¥N (Î¨ (s,a;{a }))â‰¥N (Î¨ (s,a;{a ,a }))â‰¥Â·Â·Â·â‰¥N (Î¨ (s,a;{a ,...,a })).
opt opt 0 0 opt 1 0 1 opt k 0 k
Proof. Foreachiâ‰¥0,definethesurrogatefunctionÎ¨ recursively:
i
Î¨ (s,a;{a ,...,a })=max{Q(s,a),Ï„ }, (11)
i 0 i i
where
Ï„ = max Q(s,a ).
i j
0â‰¤jâ‰¤i
NotethatÏ„ isnon-decreasingwithrespecttoi,i.e.,Ï„ â‰¥Ï„ .
i i+1 i
Ourgoalistoshowthatforeachiâ‰¥0,
N (Î¨ (s,a;{a ,...,a }))â‰¥N (Î¨ (s,a;{a ,...,a })).
opt i 0 i opt i+1 0 i+1
WeproceedbyconsideringhowthesetoflocaloptimachangesfromÎ¨ toÎ¨ .
i i+1
Consideranylocaloptimumaâ€²ofÎ¨ . Therearetwocases:
i
Case1: Q(s,aâ€²)>Ï„
i
Inthiscase,Î¨ (s,aâ€²) = Q(s,aâ€²)andÎ¨ coincideswithQinaneighborhoodofaâ€². Sinceaâ€² isa
i i
localoptimumofÎ¨ ,itisalsoalocaloptimumofQ. BecauseÏ„ â‰¥Ï„ ,therearetwosubcases:
i i+1 i
18Preprint.
Subcase1a: Q(s,aâ€²)>Ï„
i+1
Here,Î¨ (s,aâ€²)=Q(s,aâ€²)and,inaneighborhoodofaâ€²,Î¨ coincideswithQ. Thus,aâ€²remains
i+1 i+1
alocaloptimumofÎ¨ .
i+1
Subcase1b: Q(s,aâ€²)â‰¤Ï„
i+1
SinceQ(s,aâ€²)>Ï„ andÏ„ â‰¥Ï„ ,thisimpliesÏ„ >Ï„ andQ(s,aâ€²)=Ï„ . Then,
i i+1 i i+1 i i+1
Î¨ (s,aâ€²)=Ï„ ,
i+1 i+1
and in a neighborhood around aâ€², Î¨ (s,a) â‰¥ Ï„ . Thus, aâ€² is not a local optimum of Î¨
i+1 i+1 i+1
becausethereisnoneighborhoodwhereÎ¨ (s,a)<Î¨ (s,aâ€²).
i+1 i+1
Case2: Q(s,aâ€²)â‰¤Ï„
i
In this case, Î¨ (s,aâ€²) = Ï„ , and Î¨ is constant at Ï„ in a neighborhood of aâ€². Thus, aâ€² may be
i i i i
consideredalocaloptimuminÎ¨ ifthefunctiondoesnotexceedÏ„ nearby. WhenmovingtoÎ¨ ,
i i i+1
sinceÏ„ â‰¥Ï„ ,wehave:
i+1 i
Î¨ (s,aâ€²)=Ï„ â‰¥Ï„ .
i+1 i+1 i
Intheneighborhoodofaâ€²,Î¨ remainsatleastÏ„ ,soaâ€² isnotalocaloptimuminÎ¨ unless
i+1 i+1 i+1
Q(s,a)<Ï„ inaneighborhoodaroundaâ€².
i+1
However,sinceÎ¨ (s,a)â‰¥Ï„ foralla,thefunctiondoesnotdecreasebelowÎ¨ (s,aâ€²)inany
i+1 i+1 i+1
neighborhoodofaâ€². Therefore,aâ€²isnotalocaloptimumofÎ¨ .
i+1
Conclusion: Fromtheabovecases,weobservethat:
â€¢ Anylocaloptimumaâ€²ofÎ¨ whereQ(s,aâ€²)>Ï„ remainsalocaloptimuminÎ¨ .
i i+1 i+1
â€¢ Anylocaloptimumaâ€² ofÎ¨ whereQ(s,aâ€²) â‰¤ Ï„ doesnotremainalocaloptimumin
i i+1
Î¨ .
i+1
Since Î¨ does not introduce new local optima (because Î¨ (s,a) â‰¥ Î¨ (s,a) for all a and
i+1 i+1 i
coincideswithQonlywhereQ(s,a)>Ï„ ),thenumberoflocaloptimadoesnotincreasefromÎ¨
i+1 i
toÎ¨ .
i+1
BaseCase: Fori=0,wehave:
Î¨ (s,a;{a })=max{Q(s,a),Q(s,a )}.
0 0 0
IfweconsiderQ(s,a )tobelessthantheminimumvalueofQ(s,a)(whichcanbearrangedby
0
choosinga appropriatelyorbydefiningÏ„ tobelessthaninf Q(s,a)),thenÎ¨ (s,a)=Q(s,a),
0 0 a 0
andthebasecaseholdstrivially.
InductiveStep: Assumingthat
N (Î¨ (s,a;{a ,...,a }))â‰¤N (Î¨ (s,a;{a ,...,a })),
opt i 0 i opt i 0 i
wehaveshownthat
N (Î¨ (s,a;{a ,...,a }))â‰¤N (Î¨ (s,a;{a ,...,a })).
opt i+1 0 i+1 opt i 0 i
Byinduction,itfollowsthat:
N (Q(s,a))â‰¥N (Î¨ (s,a;{a }))â‰¥N (Î¨ (s,a;{a ,a }))â‰¥Â·Â·Â·â‰¥N (Î¨ (s,a;{a ,...,a })).
opt opt 0 0 opt 1 0 1 opt k 0 k
C ENVIRONMENT DETAILS
C.1 MININGENV
Thegridworldenvironment,introducedin Â§5,requiresanagenttoreachagoalbynavigatinga2D
mazeassoonaspossiblewhilebreakingtheminesblockingtheway.
State: Thestatespaceisan8+Kdimensionalvector,whereKequalstomine-category-size. This
vectorconsistsof4independentpiecesofinformation: AgentPosition,AgentDirection,Surrounding
Path,andFrontCellType.
19Preprint.
Movie
Click or Not
â†“ â†“
Simulated
Movielens
x100 Data
(a) Mine World (b) Recommender Systems (c) Continuous Control
Figure 13: Benchmark Environments involve discrete action space tasks like Mine World and
recommendersystems(simulatedandMovieLens-Data)andrestrictedlocomotiontasks.
Agent Empty Basic Complex Empty A Tc yt pio en N Nav ui mga bt eio rn M Bein fe o- rI eD M Ain fte e- rID
Mines Mines (No mine)
Basic Tools 0 0 [7, 1 8 4, ] â€¦, 15
Mine x8
Select All
Complex Tools 0 0 [0, 1, â€¦, 6] 1[ 47 ], 8 o, r â€¦ 15,
x63
Select 42
Goal
Navigation actions 1 [0, 1, 2, 3] 0 0
x4
Mining World Select All
Overview Action Overview
Figure14: MiningExpedition. Theredagentmustreachthegreengoalbynavigatingthegridand
usingoneormorepick-axestocleareachmineblockingthepath.
1. AgentPosition: A2Dvectorrepresentingtheagentâ€™sxandycoordinates.
2. AgentDirection: Onedimensionrepresentingdirections(0: right,1: down,2: left,3: up).
3. SurroundingPath: A4-dimensionalvectorindicatingiftheadjacentcellsareemptyoragoal(1:
empty/goal,0: otherwise).
4. FrontCellType: A(K+1)-dimensionalone-hotvectorwithfirstK dimensionsrepresentingthe
typeofmineandthelastdimensionrepresentingifthecellisempty(zero)orgoal(one).
Finally,wewillnormalizeeachdimensionto[0,1]witheachdimensionâ€™sminimum/maximumvalue.
Termination: Anepisodeterminateswhentheagentreachesthegoalorafter100timesteps. Upon
reset,thegridlayoutchangeswhilekeepingtheagentâ€™sstartandgoalpositionsfixed.
Actions: Actions include navigation (up, down, left, right) and pick-axe categories. Navigation
changestheagentâ€™sdirectionandattemptstomoveforward. Theagentcannotstepintoaminebut
willchangedirectionwhentryingtostepontoamineortheborderofthegrid. Thepick-axetool
actions(50types)haveapredefinedone-to-onemappingofhowtheyinteractwiththemines,which
meanstheycanbesuccessfullyappliedtoonlyonekindofmine,andeithertransformthatkindof
mineintoanothertypeofmineordirectlybreakit.
Reward: Theagentâ€™srewardcomprisesagoal-reachingreward,adistance-basedstepreward,and
rewardsforsuccessfultooluseormine-breaking. Thegoalrewardisdiscountedbystepstakenover
theepisode,encouragingshorterpathstoreachthegoal.
(cid:18) (cid:19)
N
R(s,a) =1 Â·R 1âˆ’Î» currentsteps +
Goal Goal Goal N
maxsteps
R (D âˆ’D ) + (12)
Step distance before distance after
1 Â·R +
correcttoolapplied Tool
1 Â·R
successfullybreakmine Bonus
20
â†“ â†“Preprint.
where R =10, R =0.1, R =0.1, R =0.1, Î» =0.9, N =100
Goal Step Tool Bonus Goal maxsteps
ActionRepresentationsActionsarerepresentedasa4Dvectorwithnormalizedvalues[0,1]as
describedinFigure14. Dimensionsrepresentskillcategory(navigationorpick-axe), movement
direction (right, down, left, up), mine type where this action can be applied, and the outcome of
applyingthetooltothemine,respectively.
C.2 RECSIM
Inthesimulatedrecommendationsystem(RecSys)environment,theagentselectsanitemfromalarge
setthatalignswiththeuserâ€™sinterests.Usersaremodeledwithdynamicallychangingpreferencesthat
evolvebasedontheirinteractions(clicks). Theagentâ€™sobjectiveistoinfertheseevolvingpreferences
fromuserclicksandrecommendthemostrelevantitemstomaximizethetotalnumberofclicks.
State: Theuserâ€™sinterestisrepresentedbyanembeddingvectore âˆˆRn,wherenisthenumberof
u
itemcategories. Thisembeddingevolvesovertimeastheuserinteractswithdifferentitems. Whena
userclicksonanitemwithembeddinge âˆˆRn,theuserinterestembeddinge isupdatedasfollows:
i u
eâŠ¤e +1
e â†e +âˆ†e , withprobability u i
u u u 2
1âˆ’eâŠ¤e
e â†e âˆ’âˆ†e , withprobability u i,
u u u 2
whereâˆ†e representsanadjustmentthatdependsonthealignmentbetweene ande . Thisupdate
u u i
mechanismadjuststheuserâ€™spreferencetowardstheclickeditem,reinforcingtheconnectionbetween
thecurrentactiononfuturerecommendations.
Action: Theactionsetconsistsofallitemsthatcanberecommended,andtheagentmustselectthe
itemmostrelevanttotheuserâ€™slong-termpreferencesovertheepisode.
Reward: Therewardisbasedonuserfeedback: eitheraclick(reward=1)orskip(reward=0). The
usermodelcomputesascoreforeachitemusingthedotproductoftheuseranditemembeddings:
score =âŸ¨e ,e âŸ©
item u i
Theclickprobabilityiscomputedwithasoftmaxovertheitemscoreandapredefinedskipscore:
escoreitem
p = , p =1âˆ’p
item escoreitem +escoreskip skip item
Theuserthenstochasticallychoosestoclickorskipbasedonthisdistribution.
ActionRepresentations: FollowingJainetal.(2021),itemsarerepresentedascontinuousvectors
sampledfromaGaussianMixtureModel(GMM),withcentersrepresentingitemcategories.
C.3 CONTINUOUSCONTROL
MuJoCo(Todorovetal.,2012)isaphysicsenginethatprovidesasuiteofstandardreinforcement
learningtaskswithcontinuousactionspaces,commonlyusedforbenchmarkingcontinuouscontrol
algorithms. Webrieflydescribesomeofthesetasksbelow:
Hopper: Theagentcontrolsaone-leggedrobotthatmustlearntohopforwardwhilemaintaining
balance. Theobjectiveistomaximizeforwardvelocitywithoutfalling.
Walker2d: Theagentcontrolsatwo-leggedbipedalrobotthatmustlearntowalkforwardefficiently
whilemaintainingbalance. Thegoalistoachievestablelocomotionathighspeeds.
HalfCheetah:Theagentcontrolsaplanar,cheetah-likerobotwithmultiplejointsina2Denvironment.
Thetaskrequireslearningacoordinatedgaittopropeltherobotforwardasquicklyaspossible.
Ant:Theagentcontrolsafour-legged,ant-likerobotwithmultipledegreesoffreedom.Thechallenge
istolearntowalkandnavigateefficientlywhilemaximizingforwardprogress.
C.3.1 RESTRICTEDLOCOMOTIONINMUJOCO
TherestrictedlocomotionMujocotasksareintroducedtodemonstratehowcommonDPG-based
approachesgetstuckinlocaloptimawhentheQ-landscapeiscomplexandnon-convex. Thissetting
21Preprint.
limitstherangeofactionstheagentcanperformineachdimension,simulatingrealisticscenarios
such as wear and tear of hardware. For example, action space may be affected as visualized in
Figure6. Amixture-of-hypersphereactionspaceisusedtosimulatesuchasymmetricrestrictions,
whichaffecttherangeoftorquesfortheHopperandWalkerjoints,aswellastheforcesappliedtothe
invertedpendulumanddoublependulum. Thehyperspheresaresampledrandomly,andtheirsizeand
radiusarecarefullytunedtoensurethattheactionspacehasenoughvalidactionstosolvethetask.
Definitionofrestriction.
â€¢ RestrictedHopper&Walker
Invalidactionvectorsarereplacedwith0bychangingtheenvironmentâ€™sstepfunctioncode:
1 def step(action):
2 ...
3 if check_valid(action):
4 self.do_simulation(action)
5 else:
6 self.do_simulation(np.zeros_like(action))
7 ...
The Hopper action space is 3-dimensional, with torque applied to [thigh,leg,foot],
while the Walker action space is 6-dimensional, with torque applied to
[rightthigh,rightleg,rightfoot,leftthigh,leftleg,leftfoot]. The physical implication of re-
strictedlocomotionisthatzerotorquesareexertedfortheâˆ†tdurationbetweentwoactions,i.e.,
notorquesareappliedfor0.008seconds. Thiseffectivelyslowsdowntheagentâ€™scurrentvelocities
andangularvelocitiesduetofrictionwhenevertheagentselectsaninvalidaction.
â€¢ InvertedPendulum&InvertedDoublePendulum
Invalidactionvectorsarereplacedwith-1bychangingtheenvironmentâ€™sstepfunctioncode:
1 def step(action):
2 ...
3 if not check_valid(action):
4 action[:] = -1.
5 self.do_simulation(action)
6 ...
Theactionspaceis1-dimensional,withforceappliedonthecart. Theimplicationisthatthecartis
pushedintheleftdirectionfor0.02(default)seconds. Notethattheactionvectorsarenotzeroed
because a 0-action is often the optimal action, particularly when the agent starts upright. This
wouldmaketheoptimalpolicytriviallybelearningtoselectinvalidactions.
D ADDITIONAL RESULTS
D.1 EXPERIMENTSONCONTINUOUSCONTROL(UNRESTRICTEDMUJOCO)
In standard MuJoCo tasks, the Q-landscape is likely easier to optimize compared to MuJoCo-
Restrictedtasks. InFigure15,wefindthatbaselinemodelsconsistentlyperformwellinallstandard
tasks,unlikeinMuJoCo-Restrictedtasks. Thus,wecaninferthefollowing:
1. Baselineshavesufficientcapacity,arewell-tuned,andcannavigatesimpleQ-landscapesoptimally.
2. SAVOperformsonparwithothermethodsinMuJoCotaskswheretheQ-landscapeiseasierto
optimize,showingthatSAVOisarobust,widelyapplicableactorarchitecture.
3. Baselinesperformingwellinunrestrictedlocomotionbutsuboptimallyinrestrictedlocomotion
delineatesthecauseofsuboptimalitytobethecomplexityoftheunderlyingQ-landscapes,such
asthoseshowninFigure2. SAVOisclosertooptimalinbothsettingsbecauseitcannavigate
bothsimpleandcomplexQ-functionsbetterthanalternateactorarchitectures.
D.2 PER-ENVIRONMENTABLATIONRESULTS
Figure 16 shows the per-environment performance of SAVO ablations, compiled into aggregate
performanceprofilesinFigure7b. TheSAVO-Approximationvariantunderperformssignificantly
22Preprint.
Ant-v4 HalfCheetah-v4 Hopper-v4 Walker2D-v4
12000 4000
6000
4800 9600 3200 4000
3600 7200 2400 3200
2400 2400 4800 1600
1600
1200 2400 800 800
0 0 0 0
0.000 0.375 0.750 1.125 1.500 0.000 0.375 0.750 1.125 1.500 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
SAVO (Ours) 1-Actor (TD3) 1-Actor, k-Samples (Wolpertinger) k-Actors (Ensemble)
Evolutionary Actor (CEM) Greedy-AC Greedy-TD3
Figure 15: Unrestricted Locomotion (Â§D.1). SAVO and most baselines perform optimally in
standardMuJoCocontinuouscontroltasks,wheretheQ-landscapeiseasytonavigate.
MineWorld RecSim RecSim-Data Hopper (Restricted)
0.98 6.3 3000
0.78 5.5 55 2400 0.58 4.7 46 1800
3.9 37
0.38 3.1 28 1200
0.18 2.3 19 600
0.02 1.5 10 0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.0 0.5 1.0 1.5 2.0
Env Steps 1e7 Env Steps 1e7 Env Steps 1e7 Env Steps 1e6
Walker2D (Restricted) Inverted Pendulum (Restricted) Inv. Dbl Pend. (Restricted)
4000 1000 10000
3200 800 8000
2400 600 6000
1600 400 4000
800 200 2000
0 0 0
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
SAVO (Ours) SAVO -Approximation SAVO -Action Smoothing SAVO + Previous Actions SAVO + Joint Action
Figure16: AblationsofSAVOvariations(Â§D.2)showstheimportanceof(i)theapproximation
ofsurrogates,(ii)removingTD3â€™sactionsmoothing,(iii)conditioningonprecedingactionsinthe
successiveactorandsurrogatenetworks,and(iv)individualactorsthatseparatetheactioncandidate
predictioninsteadofajointhigh-dimensionallearningtask.
indiscreteactionspacetasks,wheretraversingbetweenlocaloptimaiscomplexduetonearbyactions
having diverse Q-values (see the right panel of Figure 2). Similarly, adding TD3â€™s target action
smoothingtoSAVOresultsininaccuratelearnedQ-valueswhenseveraldifferentlyvaluedactions
existnearthetargetaction,asinthecomplexlandscapesofalltasksconsidered.
RemovinginformationaboutprecedingactionsdoesnotsignificantlydegradeSAVOâ€™sperformance
sinceprecedingactionsâ€™Q-valuesareindirectlyincorporatedintothesurrogatesâ€™trainingobjective
(seeEq.(9)),exceptforMineWorldwherethisinformationhelpsimproveefficiency.
TheSAVO+Jointablationlearnsasingleactorthatoutputsajointactioncomposedofkconstituents,
aimingtocovertheactionspacesothatmultiplecoordinatedactionscanbettermaximizetheQ-
functioncomparedtoasingleaction. However,thisincreasesthecomplexityofthearchitectureand
onlyworksinlow-dimensionaltaskslikeInverted-PendulumandInverted-Double-Pendulum. SAVO
simplifiesactioncandidategenerationbyusingseveralsuccessiveactorswithspecializedobjectives,
enablingeasiertrainingwithoutexplodingtheactionspace.
D.3 SACISORTHOGONALTOSAVO
WecompareSoftActor-Critic(SAC),TD3,andTD3+SAVOacrossvariousMujoco-Restrictedtasks.
Figure17showsthatSACsometimesoutperformsandsometimesunderperformsTD3. Therefore,
SACâ€™sstochasticpolicydoesnotaddressthechallengeofnon-convexityintheQ-function.Incontrast,
SAVO+TD3consistentlyoutperformsTD3andSAC,demonstratingtheeffectivenessofSAVOin
complexQ-landscapes. WhileSACcanbebetterthanTD3incertainenvironments,itsalgorithmic
modificationsareorthogonaltothearchitecturalimprovementsduetotheSAVOactor.
23
nruteR
lavE
etaR sseccuS
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavEPreprint.
Hopper (Restricted) Walker2D (Restricted) Inverted Pendulum (Restricted) Inv. Dbl Pend. (Restricted)
3000 4000 1000 10000
2400 3200 800 8000
1800 2400 600 6000
1200 1600 400 4000
600 SAVO 800 SAVO 200 SAVO 2000 SAVO
TD3 TD3 TD3 TD3
0 SAC 0 SAC 0 SAC 0 SAC
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
Figure17: SACisorthogonaltotheeffectofSAVO(Â§D.3). SACisadifferentalgorithmthanTD3,
whereasSAVOisaplug-inactorarchitectureforTD3. Thus,taskswhereSACoutperformsTD3
differfromtaskswhereSAVOoutperformsTD3. Also,TD3outperformsSACinRestrictedHopper
andInverted-Double-Pendulum. However,SAVO+TD3guaranteesimprovementoverTD3.
Inv. Dbl Pend. (Restricted)
10000
IntheRestrictedInvertedDoublePendulumtask(Fig-
ure18),SACunderperformsevenTD3.Analogousto
8000
TD3,thesuboptimalityisduetothenon-convexityin
thesoftQ-functionlandscape,wheresmallchanges
6000
innearbyactionscanleadtosignificantlydifferent
environmentreturns. WecombineSACwithSAVOâ€™s 4000
successiveactorsandsurrogatestobettermaximize
thesoftQ-function,naivelyconsideringactioncan- 2000 TD3 + SAVO
TD3
didatesforÂµ asthemeanactionofthestochastic SAC + SAVO
M SAC
actors. Weobservethatthispreliminaryversionof 0
0.0 0.5 1.0 1.5 2.0
SAC+SAVOshowssignificantimprovementsover Env Steps 1e6
SACincomplexQ-landscapes. Infuturework,we
Figure 18: SAC is suboptimal in complex
aim to formalize a SAVO-like objective that effec-
Q-landscape (Â§D.3) of Restricted Inverted
tivelyenablesSACâ€™sstochasticactorstonavigatethe
DoublePendulum,butSAVOhelps.
non-convexityofitssoftQ-function.
D.4 INCREASINGSIZEOFDISCRETEACTIONSPACEINRECSIM
WetesttherobustnessofourmethodtomorechallengingQ-valuelandscapesinFigure19,especially
indiscreteactionspacetaskswithmassiveactionspaces. InRecSim,weincreasethenumberof
actual discrete actions from 10,000 to 100,000 and 500,000. The experiments show that SAVO
outperformsthebest-performingbaselineofTD3+Sampling(Wolpertinger)andthebest-performing
ablationofSAVO+JointAction. ThisshowsthatSAVOmaintainsrobustperformanceevenasthe
actionspacesizeincreasesandtheQ-functionlandscapebecomesmoreintricate. Incontrast,the
baselinesexperiencedperformancedeteriorationasactionsizesgrewlarger.
RecSim RecSim
6.3 6.3
SAVO SAVO
TD3+Sampling TD3+Sampling
5.5 Joint 5.5 Joint
4.7 4.7
3.9 3.9
3.1 3.1
2.3 2.3
1.5 1.5
0.00 0.25 0.50 0.75 1.00 0 2 4 6 8
Env Steps 1e7 Env Steps 1e6
Figure 19: Increasing RecSim action set size (Â§D.4). (Left) 100,000 items, (Right) 500,000
items(6seeds)maintainstheperformancetrendsofSAVOandthebest-performingbaseline(TD3+
Sampling)andthebest-performingablation(SAVOwithJoint-Action).
24
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavEPreprint.
MineWorld RecSim RecSim-Data
6.3
0.98
0.78 5.5 55
4.7 46
0.58
3.9 37
0.38
3.1 28
0.18 SAVO-DEEPSET 2.3 SAVO-DEEPSET 19 SAVO-DEEPSET
SAVO-LSTM SAVO-LSTM SAVO-LSTM
SAVO-TRANSFORMER SAVO-TRANSFORMER SAVO-TRANSFORMER
0.02 1.5 10
0.000 1.875 3.750 5.625 7.500 0.00 0.25 0.50 0.75 1.00 0 2 4 6 8
Env Steps 1e6 Env Steps 1e7 Env Steps 1e6
Figure20: Actionsummarizercomparison(Â§E.1). TheeffectisnotsignificantTheresultsare
averagedover5randomseeds,andtheseedvarianceisshownwithshading.
RecSim RecSim-Data MineWorld
6.3 0.98
5.5 55
0.78 4.7 46
3.9 37 0.58
3.1 28 0.38
2.3 SAVO-FILM (Ours) 19 SAVO-FILM (Ours) 0.18 SAVO-FILM (Ours)
1.5 SAVO-No-FILM 10 SAVO-No-FILM 0.02 SAVO-No-FILM
0.00 0.25 0.50 0.75 1.00 0 2 4 6 8 0.000 1.875 3.750 5.625 7.500
Env Steps 1e7 Env Steps 1e6 Env Steps 1e6
Hopper (Restricted) Walker2D (Restricted) Inverted Pendulum (Restricted) Inverted Double Pendulum (Restricted)
3000 4000 1000 10000
2400 3200 800 8000
1800 2400 600 6000
1200 1600 400 4000
600 800 200 2000
SAVO-FILM (Ours) SAVO-FILM (Ours) SAVO-FILM (Ours) SAVO-FILM (Ours)
0 SAVO-No-FILM 0 SAVO-No-FILM 0 SAVO-No-FILM 0 SAVO-No-FILM
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Env Steps 1e6 Env Steps 1e6 Env Steps 1e6 Env Steps 1e6
Figure21: FiLMtoconditiononprecedingactions(Â§E.2). FiLMensureslayerwisedependenceon
theprecedingactionsforactinginactorsÎ½ andforpredictingvalueinsurrogatesÎ¨Ë† ,whichgenerally
i i
resultsinbetterperformanceacrosstasks.
E VALIDATING SAVO DESIGN CHOICES
E.1 DESIGNCHOICES: ACTIONSUMMARIZERS
Threekeyarchitectureswereconsideredforthedesignoftheactionsummarizer: DeepSets,LSTM,
andTransformermodels,representedbySAVO,SAVO-LSTM,andSAVO-TransformerinFigure20,
respectively. Ingeneral,theeffectoftheactionsummarizerisnotsignificant,andwechooseDeepSet
foritssimplicityformostexperiments.
E.2 CONDITIONINGONPREVIOUSACTIONS: FILMVS. MLP
Weexaminedtwoapproachesforconditioningonthepreviousactionlistsummary: Feature-wiseLin-
earModulation(FiLM)andconcatenationwithinput,representedbytheFiLMandnon-filmvariants
inFigure21. Acrosstasks,FiLMoutperformedthenon-FiLMversion,showingtheeffectiveness
oflayerwiseconditioninginleveragingprioractioninformationforactionselectionandsurrogate
valueprediction. Thisshowsthatthesuccessiveactorsareappropriatelyutilizingtheactionsfrom
theprecedingactorstotailortheirsearchforoptimalactions,andthesuccessivesurrogatescanbetter
evaluateQ-values,knowingwheretheycouldbethresholdedbythelossfunction.
E.3 EXPLORATIONNOISECOMPARISON: OUNOISEVSGAUSSIAN
WecompareOrnstein-Uhlenbeck(OU)noisewithGaussiannoiseacrossourenvironmentsandfind
that OU noise was generally better, with the difference being minimal. We chose to use OU for
ourexperimentsandacomparisononHopper-RestrictedisshowninFigure22a. WenotethatTD3
(Fujimotoetal.,2018)alsosuggestsnosignificantdifferencebetweenOUandGaussiannoiseand
favoredGaussianforsimplicity. Allourbaselinesusethesameexplorationbackbone,andweconfirm
25
etaR
sseccuS
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
nruteR
lavE
etaR
sseccuS
lavE
nruteR
lavE
nruteR
lavEPreprint.
Hopper (Restricted)
3000
OU-TD3
Gaussian-TD3
2400
1800
1200
600
0
0.0 0.5 1.0 1.5 2.0
Env Steps 1e6
(a)Noisetypes
Figure22: OUversusGaussianNoise(Â§E.3). Wedonotseeasignificantdifferenceduetothis
choice,andselectOUnoiseduetobetteroverallperformanceinexperiments.
thatusingOUnoiseisconsistentwiththeavailablestate-of-the-artresultswithTD3+Gaussiannoise
oncommonenvironmentslikeAnt,HalfCheetah,Hopper,andWalker2D.
F NETWORK ARCHITECTURES
F.1 SUCCESSIVEACTORS
The entire actor is built as a successive architecture (see Figure 4), where each successive actor
receives two pieces of information: the current state and the action list generated by preceding
actors. Eachactionisconcatenatedwiththestatetocontextualizeitandthensummarizedusinga
list-summarizer,describedinÂ§F.3. Thislistsummaryisconcatenatedwiththestateagainandpassed
intoanMLPwithReLU(3layersforMuJoCotasksand4layersforMineWorld andRecSim)as
describedinTable2. ThisMLPgeneratesoneactionforeachsuccessiveactor,whichissubsequently
usedasaninputactiontothesucceedingactionlists. Fordiscreteactionspacetasks,thisgenerated
actionisprocessedwitha1-NNtofindthenearestexactdiscreteaction. Finally,theactionsgenerated
byeachindividualsuccessiveactorareaccumulated,andthemaximizeractorÂµ stepfromEq.(5)
M
selectsthehighest-valuedactionaccordingtotheCriticQ-network,describedinÂ§F.2.
F.2 SUCCESSIVESURROGATES
AsFigure4illustrates,thereisasurrogatenetworkforeachactorinthesuccessiveactor-architecture.
Eachsuccessivecriticreceivesthreepiecesofinformation: thecurrentstate,theactionlistgenerated
byprecedingactors,andtheactiongeneratedbytheactorcorrespondingtothecurrentsurrogate.Each
actionisconcatenatedwiththestatetocontextualizeitandthensummarizedusingalist-summarizer,
describedinÂ§F.3. Thislistsummaryisconcatenatedwiththestateandthecurrentaction,andpassed
intoa2-layerMLPwithReLU(SeeTable2). ThisMLPgeneratesthesurrogatevalueÎ¨Ë† (s,a;a )
i <i
andisusedasanobjectivetoascendoverbyitscorrespondingactorÎ½ .
i
F.3 LISTSUMMARIZERS
To extract meaningful information from the list of candidate actions, we employed several list
summarizationmethodsfollowingJainetal.(2021). Thesemethodsaredescribedbelow:
Bi-LSTM: The action representations of the preceding actorsâ€™ actions are first passed through a
two-layermultilayerperceptron(MLP)withReLUactivationfunctions. TheoutputofthisMLPis
thenprocessedbyatwo-layerbidirectionalLSTMnetwork(Huangetal.,2015). Theresultingoutput
isfedintoanothertwo-layerMLPtocreateanactionsetsummary,whichservesasaninputforthe
actor-network(Â§F.1)andthesurrogatenetwork(Â§F.2).
26
nruteR
lavEPreprint.
DeepSet: Theactionrepresentationsoftheprecedingactorsâ€™actionsareinitiallyprocessedbya
two-layerMLPwithReLUactivations. Theoutputsarethenaggregatedusingmeanpoolingoverall
candidateactionstocompresstheinformationintoafixed-sizesummary. Thissummaryispassed
throughanothertwo-layerMLPwithReLUactivationtoproducetheactionsetsummary, which
servesasaninputfortheactor-network(Â§F.1)andthesurrogatenetwork(Â§F.2).
Transformer:SimilartoBi-LSTM,theactionrepresentationsoftheprecedingactorsâ€™actionsarefirst
processedbyatwo-layerMLPwithReLUactivations. TheoutputsaretheninputintoaTransformer
networkwithself-attentionandfeed-forwardlayerstosummarizetheinformation. Theresulting
summaryisusedaspartoftheinputtotheactor-network(Â§F.1)andthesurrogatenetwork(Â§F.2).
F.4 FEATURE-WISELINEARMODULATION(FILM)
Feature-wise Linear Modulation (Perez et al., 2018) is a technique used in neural networks to
conditionintermediatefeaturerepresentationsbasedonexternalinformation,enhancingthenetworkâ€™s
adaptabilityandperformanceacrossvarioustasks.FiLMmodulatesthefeaturesofalayerbyapplying
learned,feature-wiseaffinetransformations. Specifically,givenasetoffeaturesF,FiLMappliesa
scalingandshiftingoperation,
FiLM(F)=Î³âŠ™F+Î²,
whereÎ³ andÎ² aremodulationparameterslearnedfromanothersource(e.g.,aseparatenetworkor
input),andâŠ™denoteselement-wisemultiplication. Thisapproachallowsthenetworktoselectively
emphasizeorde-emphasizeaspectsoftheinputdata, effectivelycapturingcomplexandcontext-
specificrelationships. FiLMhasbeensuccessfullyappliedintaskssuchasvisualquestionanswering
andimagecaptioning, whereconditioningvisualfeaturesontextualinputisessential. Weapply
FiLMwhileconditioningtheactorandsurrogatenetworksonthesummaryofprecedingactions.
G EXPERIMENT AND EVALUATION SETUP
G.1 AGGREGATEDRESULTS: PERFORMANCEPROFILES
Torigorouslyvalidatetheaggregateefficacyofourapproach,weadopttherobustevaluationmethod-
ologyproposedbyAgarwaletal.(2021). Byincorporatingtheirsuggestedperformanceprofiles,
weconductacomprehensivecomparisonbetweenourmethodandbaselineapproaches,providing
a thorough understanding of the statistical uncertainties inherent in our results. Figure 7a shows
theperformanceprofilesacrossalltasks. Thex-axisrepresentsnormalizedscores,calculatedusing
min-max scaling based on the initial performance of untrained agents aggregated across random
seeds (i.e., Min) and the final performance from Figure 8 (i.e., Max). The results show that our
methodconsistentlyoutperformsthebaselinesacrossvariousrandomseedsandenvironments. Our
performancecurveremainsat thetopas thex-axis progresses, while thebaseline curvesdecline
earlier. ThishighlightsthereliabilityofSAVOoverdifferentenvironmentsand10seeds.
G.2 IMPLEMENTATIONDETAILS
WeusedPyTorch(Paszkeetal.,2019)forourimplementation,andtheexperimentswereprimarily
conductedonworkstationswitheitherNVIDIAGeForceRTX2080Ti,P40,orV32GPUson. Each
experimentseedtakesabout4-6hoursforMineWorld,12-72hoursforMujoco,and6-72hoursfor
RecSim,toconverge. WeusetheWeights&Biasestool(Biewald,2020)forplottingandlogging
experiments. AlltheenvironmentswereinterfacedusingOpenAIGymwrappers(Brockmanetal.,
2016). WeusetheAdamoptimizer(Kingma&Ba,2014)throughoutfortraining.
G.3 COMMONHYPERPARAMETERTUNING
Toensurefairnessacrossallbaselinesandourmethods,weperformedhyperparametertuningover
parametersthatarecommonacrossmethods:
â€¢ Learning Rates of Actor and Critic: (Actor) We searched over learning rates
{0.01,0.001,0.0001,0.0003} and found that 0.0003 was the most stable for the actorâ€™s learn-
ingacrossalltasks. (Critic)Similartotheactor,wesearchedoverthesamesetoflearningrates
andfoundthesamevalueof0.0003wasthemoststableforthecriticâ€™slearningacrossalltasks.
27Preprint.
â€¢ NetworkSizesofActorandCritic: Foreachtask,wesearchedoversimple3or4MLPlayersto
determinethenetworksizethatperformedbestbutdidnotobservemajordifferences. (Critic)To
ensureafaircomparison,weusedthesamenetworksizeforthecritic(Q-network)andsurrogates
acrossallmethodswithineachtask. (Actor)Similartothecritic,weusedthesamenetworksize
forthevariousactorsinallthebaselinesandsuccessiveactorsinSAVOwithinaparticulartask.
G.4 HYPERPARAMETERS
TheenvironmentandRLalgorithmhyperparametersaredescribedinTable2.
Hyperparameter MineWorld MuJoCo&Adroit RecSim
Environment
TotalTimesteps 107 3Ã—106 107
Numberofepochs 5,000 8,000 10,000
#EnvsinParallel 20 10 16
EpisodeHorizon 100 1000 20
NumberofActions 104 N/A 10000
TrueActionDim 4 5 30
ExtraActionDim 5 N/A 15
RLTraining
Batchsize 256 256 256
Buffersize 5Ã—105 5Ã—105 106
Actor: LR 3Ã—10âˆ’4 3Ã—10âˆ’4 3Ã—10âˆ’4
Actor: Ïµ 1 1 1
start
Actor: Ïµ 0.01 0.01 0.01
end
Actor: Ïµdecaysteps 5Ã—106 5Ã—105 107
Actor: ÏµinEval 0 0 0
Actor: MLPLayers 128_64_64_32 256_256 64_32_32_16
Critic: LR 3Ã—10âˆ’4 3Ã—10âˆ’4 3Ã—10âˆ’4
Critic: Î³ 0.99 0.99 0.99
Critic: MLPLayers 128_128 256_256 64_32
#updatesperepoch 20 50 20
ListLength 3 3 3
TypeofListEncoder DeepSet DeepSet DeepSet
ListEncoderLR 3Ã—10âˆ’4 3Ã—10âˆ’4 3Ã—10âˆ’4
Table2: Environment/Policy-specificHyperparameters
H Q-VALUE LANDSCAPE VISUALIZATIONS
H.1 1-DIMENSIONALACTIONSPACEENVIRONMENTS
WeanalyzedtheQ-valuelandscapesinMujocoenvironmentstoshowhowsuccessivecriticshelp
actors find better actions by reducing local optima. Figure 23 illustrates a typically smooth and
easy-to-optimizeQ-valuelandscapeinunrestrictedInverted-Pendulum. Figure24illustratesthat
in restricted locomotion, the Q-value landscape (leftmost and rightmost figures) is uneven with
manylocaloptima. However,theQ-valuelandscapeslearnedbysuccessivesurrogatesÎ¨Ë† become
i
successivelysmootherbyremovinglocalpeaksbelowtheQ-valuesofpreviouslyselectedactions.
Thishelpsactorsfindclosertooptimalactionsthanwithasinglecritic.
Finally,whenweplottheactionsa ,a ,a selectedbythelearnedsuccessiveactorsontheoriginal
0 1 2
Q-landscape(rightmostfigure),weseetheyoftenachievehigherQ-valuesthana ,theactionasingle
0
actorhaslearned. Thus,themaximizeractorÂµ oftenfindsclosertooptimalactionsthanasingle
M
actor,resultinginbetterperformanceasshowninthereturncomparisonbetweenÂµ andsingleactor
M
(Figure11c)andtheperformanceagainstbaselines(Figure8).
28Preprint.
(a)Q (s,a ) (b)Q (s,a |a ) (c)Q (s,a |{a ,a }) (d)Q(s,a )âˆ€i=0,1,2
0 0 1 1 0 2 2 0 1 i
Figure23: SuccessivesurrogatelandscapesandtheQ-landscapeofInvertedPendulum-v4.
Figure24: SuccessivesurrogatelandscapesandQlandscapeforRestrictedInverted-Pendulumand
RestrictedInverted-Double-Pendulumenvironments.
H.2 HIGH-DIMENSIONALACTIONSPACEENVIRONMENTS
Figure25visualizeQ-valuelandscapesforaTD3agentinHopper-v4.Weprojectactionsfromthe3D
actionspaceofHopper-v4ontoa2DplaneusingUniformManifoldApproximationandProjection
(UMAP)andsample10,000actionsevenlytoensurethoroughcoverage. TheQ-valuesareplotted
using trisurf, which may introduce some artificial roughness but offers more reliable visuals
thangrid-surfaceplotting. Despitelimitationsofdimensionalityreductionâ€”suchasdistortionof
distancesâ€”theQ-landscapeforHopper-v4revealsalargegloballyoptimalregion(showninyellow),
offeringacleargradientpaththatpreventsthegradient-basedactorfromgettingstuckinlocaloptima.
Incontrast,Hopper-Restricted(Figure26)hasmorecomplexQ-landscapesduetovalidactionsbeing
restrictedinoneofthehyperspheresshowninFigure6. Consequently,theseQ-landscapesappearto
havemorelocallyoptimalregionsthanHopper-v4. Thiscreatesmanypeakswheregradient-based
actorsmightgettrapped,degradingtheresultantagentperformance.
The curse of dimensionality limits conclusive analyses on higher dimensional environments like
Walker2D-v4(6D)andAnt-v4(8D)becauseprojectingto2Dcausessignificantinformationloss,
makingitdifficulttoassessconvexityintheirQ-landscapes.
29Preprint.
Figure25: Hopper-v4: Qlandscapevisualizationatdifferentstatesshowapathtooptimum.
Figure26: Hopper-restricted: Qlandscapevisualizationatdifferentstatesshowseverallocaloptima.
30