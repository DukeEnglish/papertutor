Preprint
STACKFEED: STRUCTURED TEXTUAL ACTOR-
CRITIC KNOWLEDGE BASE EDITING WITH
FEEDBACK
NamanGupta‚àó1 ShashankKirtania‚àó2 PriyanshuGupta2 KrishnaKariya2
SumitGulwani3 ArunIyer1 SureshParthasarathy1 ArjunRadhakrishna3
SriramK.Rajamani1 GustavoSoares3
1MicrosoftResearchIndia,Bengaluru2Microsoft,Bengaluru3Microsoft,Redmond
{t-nagupta, t-skirtania, priyansgupta, t-kkariya}@microsoft.com
{sumitg, ariy, supartha, arradha, sriram, gsoares}@microsoft.com
ABSTRACT
Large Language Models (LLMs) often generate incorrect or outdated infor-
mation, especially in low-resource settings or when dealing with private data.
To address this, Retrieval-Augmented Generation (RAG) uses external knowl-
edge bases (KBs), but these can also suffer from inaccuracies. We introduce
STACKFEED,anovelStructuredTextualActor-CriticKnowledgebaseediting
withFEEDbackapproachthatiterativelyrefinestheKBbasedonexpertfeedback
using a multi-actor, centralized critic reinforcement learning framework. Each
document is assigned to an actor, modeled as a ReACT agent, which performs
structured edits based on document-specific targeted instructions from a central-
izedcritic.Experimentalresultsshowthat STACKFEED significantlyimproves
KBqualityandRAGsystemperformance,enhancingaccuracybyupto8%over
baselines.
1 INTRODUCTION
Large Language Models (LLMs) often produce incorrect or outdated information, particularly in
low-resource settings or when handling private data. Even if the information provided is accurate,
LLMscangeneratehallucinatedorimaginarycontentalongsideit(Maynezetal.,2020;Zhouetal.,
2021). A promising solution to address these issues is the integration of retrieval components that
extractrelevantinformationfromexternalknowledgesources,knownasRetrieval-AugmentedGen-
eration (RAG) (Chen et al., 2017; Khandelwal et al., 2020; Guu et al., 2020; Izacard et al., 2022;
Shietal.,2023).Forclarity,wewillrefertotheseexternalknowledgesourcesasKnowledgeBases
(KBs). However, KBs themselves can suffer from inaccuracies, incompleteness, or outdated con-
tent. To address these challenges, there is growing interest in Knowledge Editing (KE) techniques
toenhanceLLMswithup-to-dateandaccurateknowledge.
AdvancementsinKEhavefocusedonupdatingthemodel‚Äôsparameters(DeCaoetal.,2021a;Meng
etal.,2022;2023),addingnewparameterstomodel(Huangetal.,2023;Yuetal.,2024),andholding
additionalmemory(Madaanetal.,2022;Wangetal.,2024a;b).Contrarytoapproachesthateither
updatemodelparametersoraddnewparametersthatrequirewhite-boxaccesstoLLMs,memory-
based approaches can work with black-box access to LLMs. In similar line of thought, recently,
KE approaches have also focused on refining the KBs themselves (Li et al., 2024). For example,
the method proposed in Li et al. (2024) continuously updates KBs with new information, such as
thecurrentidentityoftheBritishPrimeMinister.Thisapproachdemonstratesthatdirectlyediting
the KB is more effective than simply adding new documents, which may coexist with outdated or
inaccurate ones. Removing older documents is often not feasible, as only certain sections may be
incorrect,whileotherpartscouldstillprovidevaluableinformationfordifferentqueries.However,in
applicationslikechatbotsorcodegenerationusingAPIdocumentation,whereupdatedinformation
mightnotbereadilyavailableindocumentform,expertinterventioncanbecrucial(Ramjeeetal.,
‚àóEqualcontribution.
1
4202
tcO
41
]IA.sc[
1v48501.0142:viXraPreprint
2024;Afzaletal.,2024).Insuchcases,expertfeedbackcanbeusedtodirectlyupdatetheKBwith
accurateinformationwhentheLLMproduceserroneousresults.
Toleverageexpertororaclefeedback,weproposeSTACKFEED,aStructuredTextualActor-Critic
KnowledgebaseeditingwithFEEDbacktechnique.Ourcontributionsareasfollows:
1. Introduction of Feedback-Driven KB Editing: We present STACKFEED, a novel
framework that refines the KB using structured edits based on expert feedback. This ap-
proachallowsfordirect,document-levelupdateswithoutrequiringaccesstoLLMparam-
eters,makingitapplicabletobothwhite-boxandblack-boxLLMs.
2. Multi-Actor, Centralized Critic Architecture: We design a multi-agent reinforcement
learningframeworkwhereeachactorisresponsibleforaspecificdocument,andacentral-
ized critic coordinates updates based on a global reward signal. This architecture ensures
thatdocument-leveleditsareconsistentandcontributetotheoverallaccuracyoftheRAG
system.
3. ParameterizedActionSpaceforDocumentEditing:Weproposeaparameterizedaction
spaceforeachdocument-specificactor,enablingfine-grainedcontroloveredits,additions,
anddeletionswithineachdocument.Thisstructuredactionspaceallowstheactorstoper-
formprecisemodificationsbasedonexpertfeedback,resultinginarefinedKBthatbetter
supportstheRAGsystem.
4. Definition and Evaluation of KB Characteristics: We define desirable characteristics
forKBrefinement,includingcoherence,completeness,andgeneralizability,andintroduce
corresponding metrics to quantitatively assess these properties. These metrics provide a
systematicwaytomeasuretheeffectivenessofKBupdates.
5. EmpiricalEvaluationandPerformanceGains:WedemonstratethatSTACKFEEDsig-
nificantly improves the accuracy and reliability of the QA system in a variety of set-
tings. Through extensive experiments, we show that incorporating expert feedback into
document-leveleditsleadstoasubstantialreductioninerrorratesandenhancestheKB‚Äôs
abilitytosupportaccurateanswergeneration.
Thispaperisorganizedasfollows:Section2reviewsrelevantpriorwork,whileSection3presentsan
illustrativeexampletointroduceandexplainourapproach.Section4detailstheproposedmethod-
ology, and Section 5 outlines the desired characteristics for the edited KB along with metrics for
evaluation.Section6describestheexperimentalsetup,andfinally,Section7reportstheresults.
2 RELATED WORK
The STACKFEED framework addresses a key limitation of current RAG systems: the inability
to dynamically update Knowledge Bases (KBs) without retraining or altering model parameters.
Our work draws from research in Retrieval-Augmented Generation (RAG), Continual Learning,
ModelEditing,andfeedback-drivenpromptoptimization,incorporatinginsightsfromMulti-Agent
ReinforcementLearning(MARL)toproposeaneffectivesolutionforKBediting.
Retrieval Augmented Generation (RAG): RAG systems enhance LMs by retrieving relevant
knowledgefromaKBbasedontheinputqueryandappendingittothecontext,therebyaddressing
thelimitationsofstandaloneLMsthatlacksufficientcontextandproduceinaccurateanswers(Chen
etal.,2017;Khandelwaletal.,2020;Guuetal.,2020;Izacardetal.,2022;Shietal.,2023).These
systemsdynamicallyconstructcontextsfromunstructuredKBswithoutmodifyingtheLM‚Äôsinter-
nal parameters. STACKFEED further enhances RAG systems by refining the KB itself based on
feedback,ensuringmoreaccurateandup-to-dateinformation.
ContinualLearning:ContinualLearning(CL)methodsaddressthechallengeofupdatingLMsin
non-stationaryenvironmentsbyensuringthatnewinformationislearnedwithoutforgettingprevi-
ously acquired knowledge (Jin et al., 2022; Xu et al., 2023; Padmanabhan et al., 2023; Akyu¬®rek
etal.,2024).Thesemethodsareoftencomputationallyintensiveandrequirelarge-scaleretraining,
making them less suitable for scenarios requiring frequent updates or minimal computational re-
sources. STACKFEED, by contrast, leverages expert feedback to perform direct edits to the KB,
avoidingtheneedforextensiveretraining.
2Preprint
KnowledgeEditing:KnowledgeEditingapproachesfallintotwocategories:ModelEditing,which
modifiestheLMparametersdirectly,andInputEditing,whichupdatestheknowledgesuppliedto
themodel.WhileModelEditingefficientlyaltersspecificfactsusingspecializedsecondarymodels
or altering parameters (De Cao et al., 2021b; Meng et al., 2023), it struggles to ensure consistent
updatesacrosscontexts(Onoeetal.,2023;Huaetal.,2024).Incontrast,InputEditingmodifiesthe
KBitself,enablingupdatestobereflectedinoutputswithoutchangingmodelparameters(Madaan
etal.,2022;Wangetal.,2024a;b;Lietal.,2024).STACKFEEDbuildsoninputeditingtechniques
byleveragingexpertfeedbacktorefinetheKBsystematically,ensuringmoreaccurateandconsistent
responses.
PromptOptimization:WiththeadventofLMs,somerecentworksapproximategradientsintext-
basedenvironmentsusingLMs(Pryzantetal.,2023;Wangetal.,2023;Junejaetal.,2024;Gupta
etal.,2024)foroptimizingtaskprompts. STACKFEED isinspiredbytheseapproachesandgen-
eratestextualreflections,similartoMetaReflection(Guptaetal.,2024)andShinnetal.(2023),as
proxies for gradients. It provides actionable guidance for document updates without the need for
differentiablemodels.Additionally,STACKFEEDadoptsclusteringstrategiesforfeedbackaggre-
gation from works like UniPrompt (Juneja et al., 2024)- ensuring that actors receive coherent and
non-redundantinstructions.
Multi-AgentReinforcementLearning(MARL):Multi-agentreinforcementlearning(MARL)has
been applied to various domains, with early research focusing on tabular methods (Busoniu et al.,
2008; Canese et al., 2021; Gronauer & Diepold, 2022) and later expanding to deep learning tech-
niquesforhigh-dimensionalinputs(Tampuuetal.,2017;Leiboetal.,2017).Studieshaveexplored
independentQ-learning(Tan,1993),agentcommunication(Foersteretal.,2016;Dasetal.,2017),
andcentralizedtrainingwithdecentralizedexecution(Guptaetal.,2017).However,mostofthese
approachesdonotaddressthecriticalchallengeofmulti-agentcreditassignment.Actor-criticmeth-
odshavebeenintroducedtoovercomethislimitationbyemployingcentralizedcriticswithdecen-
tralized actors (Foerster et al., 2018; Iqbal & Sha, 2019; Wang et al., 2021; Chen et al., 2023).
STACKFEED extends such actor-critic framework to operate directly on textual content, using
the centralized critic to decompose feedback into actionable textual gradients for each document-
specificactor.
Inthenextsection,weprovideanexampletoillustratetheKBeditingproblem,whilealsoproviding
anoverviewofSTACKFEED.
3 EXAMPLE AND OVERVIEW
Figure 1 illustrates our technique applied to the ARKS Pony domain (Su et al., 2024a), where a
knowledgebase(KB)forthelow-resourceprogramminglanguagePonysupportsanaturallanguage-
to-code task. Due to Pony‚Äôs rarity, language models often generate code that fails to compile. To
address this, we use the Pony compiler as an expert to provide feedback in the form of compile
errors.
1 Evaluating the Knowledge Base State: We start with an initial KB, including documents like
builtin-array.md. The system retrieves relevant documents based on the given task (e.g.,
countingnon-inversionsinanarray)andgeneratesaprogram,whichisevaluatedbythecompiler,
resultinginfeedback(e.g.,compileerrors).
2 Centralized Feedback Analysis: We analyze compile errors to generate reflections that explain
whytheerrorsoccurred.Forinstance,iftheapplymethodintheArrayclassispartialandmay
raise an error, the reflection suggests adding a ? to handle potential failures. These reflections are
matchedtothedocumentstheypertainto,refiningtheunderstandingoferrors.
3 Distributing Gradients: Reflections are generalized into gradients, which summarize modifica-
tionsneededforeachdocument.Forexample,thethememightbethepartialnatureoffunctionslike
applyandupdate,whichneedbettererrorhandlinginthedocumentation.
4 GeneratingEditActions:Gradientsareconvertedintostructurededitactions,suchasaddingor
modifyingcontentinspecificsectionsofthedocuments.
5 Re-evaluation and MCTS Search: After edits are applied, the KB is re-evaluated, generating
newfeedbackandarewardscore.ThisscoreguidesaMonteCarloTreeSearch(MCTS)toexplore
3Preprint
Task:Givenanarrayofintegersnums,writeafunction KnowledgeBase
thatreturnsthenumberofgoodpairs.Apair(i,j)is builtin-array.md
calledgoodifnums[i]==nums[j]andi<j. collections-persistent-vec.md
math-fibonacci.md
RetrievedDocuments: ...
builtin-array.md
collections-persistent-vec.md Retrieval
math-fibonacci.md
random-dice.md Expert(Compiler)Feedback
Simulate main.pony:15:16:callisnotpartialbutthemethodis
Task -aquestionmarkisrequiredafterthiscall
OutputProgram: ifnums(i)==nums(j)then
funnumIdenticalPairs(ns:Array[I32]):I32=> ÀÜ
varcount:I32=0 main.pony:15:27:callisnotpartialbutthemethodis
foriinRange(0,ns.size()-1)do -aquestionmarkisrequiredafterthiscall
forjinRange(i+1,ns.size())do Feedback ifnums(i)==nums(j)then
ifns(i)==ns(j)then ÀÜ
count=count+1 main.pony:11:42:functionsignatureisnotmarkedas
... partialbutthefunctionbodycanraiseanerror
funnumIdenticalPairs(nums:Array[I32]):I32=>
ÀÜ
ReflectionandSelection
Reflection:Theoutputcodecontainsthefollowingerrors:
1.Thearrayaccesscallsnums(i)andnums(j)arenotmarkedaspartial,butthey
shouldbeastheapplyfunctionintheArrayclassispartialandcanthrowerrors.The
correctedcodeis:nums(i)?=nums(j)?Tofurtherillustratethefix,considerthe
followingsimplifiedexample: Edit:AddSection
Codewitherror: WhenworkingwitharraysinPony,itisimportantto
funget(arr:Array[I32],index:USize):I32?=>arr(index) understandthatsomemethodsarepartial,meaningthe
Fixedcode: maynotsucceedinallcasesandmightraiseanerror.To
funget(arr:Array[I32],index:USize):I32?=>arr(index)? safelyhandletheseerrors,Ponyprovidesthe?operator...
... Theapplymethodonarraysispartialandwillraisean
Selection:builtin-array.md errorintheindexisoutofbounds...
Forexample,the‚Äòapply‚Äòmethodonarraysispartialand
Reasoning:ThisfilecontainsinformationabouttheArrayclass,includingapply... requireserrorhandling:
‚Äú‚Äòpony
try
myarray.apply(42)==7
Collectedselectionreasoningforbuiltin-array.md end
‚Äú‚Äò
Inthissnippet,iftheindex42isoutofbounds
AggregateDocumentGradients GenerateEdits
the‚Äòapply‚Äòmethodwillraiseanerror,whichisthen
handledbythe‚Äòtry‚Äòblock,preventingtheprogramfrom
Document-wisePartialGradient:builtin-array.mdisaKBdocumentdetailingthe
useofthemethodsandfunctionsoftheArrayclass... crashing.
MethodAvailabilityandErrorHandling:Thecurrentfilelacksexplicitguidanceonthe
partialityofmethodsandtheuseof?operator.Theomissioncanleaddevelopersto...
Thefileshouldaccuratelyreflectthepartialityofmethodslikeapplyandupdate...
Figure1:ExampleoftheSTACKFEEDintheARKSPonyscenario
differentstatesoftheKB,iteratingthroughsteps 1 - 4 toprogressivelyrefinetheKBandimprove
thesystem‚Äôsoverallperformance.
4 METHODOLOGY
WewillstartbydescribingatypicalRetrieval-AugmentedGeneration(RAG)systemoverunstruc-
turedKnowledgeBases.
Errors in such systems can arise from multiple components: 1) the LLM B might fail to reason
correctlyovertheprovidedinformation,2)theretrieverRmightnotselecttherightsetofrelevant
documents from K, or 3) the knowledge base K itself might contain incorrect or incomplete in-
formation.Weassumeanexpertismonitoringthesystem,identifyingwhenanswersareincorrect,
determining which component is at fault, and providing feedback on why the answer is incorrect
andwhatthecorrectanswermustbe.
ThisworkfocusesonscenarioswhereincorrectanswersresultfromissuesintheKnowledgeBase
(K).OurgoalistoimproveKbyaddressingmistakesinKandfillinginmissinginformationbased
onexpertfeedback,thusenhancingtheRAGsystem‚Äôsperformanceonfuturequeries.
4.1 PROBLEMFORMULATION
We are provided with a training set T = {(q ,o ,c ,f )}l , where q is a user query, o is the
i i i i i=1 i i
RAG system‚Äôs answer, c is the correct answer, and f is an optional expert feedback on incorrect
i i
answers.Wealsoassumeaccesstoascoringfunctiong,whichcompareo andc tooutputascore.
i i
TheobjectiveistooptimizetheknowledgebaseKtomaximizethesumofthescoresforallqueries
inthetrainingset:
4Preprint
1 (cid:88)
K‚àó =argmax g(B(q ,Œì(q ,K)),c ) (1)
K |T| i i i
(qi,ai,ci,fi)‚ààT
Inthenextsection,weshowhowsuchanobjectivecanbeseenasastatesearchproblem.
4.2 KNOWLEDGEBASEEDITINGASSTATESEARCH
In our problem setting, the Knowledge Base (K) is defined as a collection of documents K =
{D }n .Weassumeeachdocumentconsistsofanumberofchunksoftextandcanberepresented
i i=1
asD = [c ].Thestates ‚àà S ofthesystemisrepresentedbythecurrentconfigurationoftheKB,
i ij
i.e.,thecontentofalldocumentsinK.
Given a query q and a set of retrieved documents Œì(q ,K), the LLM B generates an answer o .
i i i
Whenerrorsariseduetoincompleteorincorrectinformationintheretrieveddocuments,ourgoal
istoidentifytheoptimalconfigurationofK thatimprovestheaccuracyofthesystem‚Äôsresponses.
Thus,wedefineourstatesearchproblemasfindingthebeststates‚àóoftheKB.
StateSpace:ThestatespaceS encompassesallpossibleconfigurationsoftheKB.Eachstatescor-
respondstoaparticularsetofdocumentcontents,representedas:s = {D }n ,whereD denotes
i i=1 i
thecontentofdocumentiandnisthenumberofdocumentsinK.Thestatescapturestheoverall
structureandcontentoftheKBatanygivenpoint.Wesets =K.
0
State Transition Function: The state transition function T(s,u) defines how the KB changes in
response to the action u taken by the agent. Each action contains modifications to one or more
documentswithintheKB,resultinginanewKBconfiguration.Thestatetransitionisformalizedas:
s‚Ä≤ =T(s,u),wheres‚Ä≤isthenewstateoftheKBafterapplyingu.
ActionSpace:TheactionspaceAconsistsoflistofdiffsd correspondingtoeachdocumentD .
i i
Essentially,u=[d ]|K|.
i i=1
Environment:Wemodeltheenvironmentsimplyasa‚Äúpatch‚Äùfunction,thattakesthediffgenerated
bytheagentandpatchestheKBtoproducethenewstate.
OptimizationObjective:FollowingEquation1,ourobjectivethenistofindtheoptimalstates‚àóof
theKBthatmaximizestheoverallperformanceoftheRAGsystem,asmeasuredbyaglobalreward
functionR.Theoptimizationproblemisformulatedas:
1 (cid:88)
s‚àó =argmaxR(s)=argmax g(B(q ,Œì(q ,s)),c ) (2)
s‚ààS s‚ààS |T| i i i
(qi,ai,ci,fi)‚ààT
where R(s) represents the cumulative reward of the KB state s, reflecting its ability to support
accurateandcompleteresponsesforasetofqueries.
The reward function R(s) is derived from the expert feedback on the system‚Äôs generated answers
andcapturesimprovementsintermsofcorrectness,coherence,andcompletenessoftheinformation
in the KB. By optimizing for s‚àó, we ensure that the final state of the KB maximizes the overall
accuracyandeffectivenessoftheRAGsystem,ratherthanfocusingonanintermediatesequenceof
statetransitions.
Insummary,thestatesearchformulationdefinestheproblemoffindingtheoptimalstates‚àó ofthe
KBthatmaximizesthesystem‚Äôsperformance.Thisapproachenablesustomaketargeted,feedback-
driveneditstotheKBandachievearefined,high-qualityknowledgebasethatbettersupportsaccu-
rateanswergeneration.
Monte Carlo Tree Search: We employ Monte Carlo Tree Search (MCTS) similar to
PROMPTAGENT (Wang et al., 2023) to search for the optimal state s‚àó. However, this introduces
severalchallenges:(1)ThesearchspaceforallpossibleKBeditsisvastlylargerthanthatofstan-
dardprompteditstypicallyexploredintheliterature(Pryzantetal.,2023;Wangetal.,2023;Juneja
et al., 2024; Gupta et al., 2024), making exhaustive search infeasible. (2) Generating actions and
5Preprint
subsequent states, as done in methods like PROMPTAGENT , is difficult in the KB editing context
sincefittingtheentireKBintothepromptofalanguagemodelisimpractical.Despiteadvancements
inhandlinglongcontexts(Wangetal.,2020;Kitaevetal.,2020;Pressetal.,2022;Suetal.,2024b),
thesemodelsoftenstruggletoleverageextensivecontextseffectivelyLiuetal.(2024).(3)Finally,
theLMwouldneedtooutputtheentireeditedKB,whichischallengingduetotheinherentdifficulty
LMsfaceingeneratinglong,coherentoutputs(Baietal.,2024).
To address these challenges, we decouple the KB edits by isolating document-level modifications
based on the required updates. Since individual documents can be large, we further break down
theeditsintomanageablesections,enablingastructurededitingmechanismthatfocusesonspecific
portionsofadocumentatatime.Inthenextsection,weintroduceSTACKFEED,anagentdesigned
toefficientlyperformthesestructurededitsbasedonfeedback.
4.3 STACKFEED
STACKFEED Architecture
Reflection
‚àá
Doc Selection Critic ùê∂
Doc-wise Reflection
ùúï ,ùúï ,‚Ä¶ ùúï
0 1 n
Reward ùëÖ
ùúï 0 ùúï 1 ùúï ùëõ
‚Ä¶
ùê∑ 0 ùê∏ùëëùëñùë°ùëúùëü ùê∑ 1 ùê∏ùëëùëñùë°ùëúùëü ùê∑ ùëõ ùê∏ùëëùëñùë°ùëúùëü Actors {ùê¥ ùëñ}
ùëë 0 ùëë 1 ùëë ùëõ
Environment
Figure 2: STACKFEED Multi-actor, centralized critic architecture: On receiving a reward from
the environment, the critic generates a reflection over the failures to calculate the textual gradient
‚àá.Thecriticusesthisreflectiontoselectthedocumentsresponsiblefortheerrorandproceedsto
assigns credit to the actors in the form of document-wise reflections. The actors then proceed to
iterativelyeditthedocuments.Allthedocument-wiseeditsarethenpooledtodefinetheKBedit.
TheproposedapproachSTACKFEEDisdesignedtoenhanceaRAGsystembyrefiningtheunder-
lyingKnowledgeBase(K)usingexpertfeedback.Ourapproachemploysamulti-actor,centralized
criticarchitecture,whereeachactorisresponsibleformakingupdatestoaspecificdocumentwithin
K, and a centralized critic uses global feedback to coordinate these updates. The objective is to
iterativelyimproveKsuchthattheoverallaccuracyoftheRAGsystemismaximized.
4.3.1 REWARDSIGNAL
Foragivenqueryq andthegeneratedanswero ,theexpertprovidesfeedback(c ,f )thatincludesa
i i i i
groundtruthanswerc andqualitativeexpertfeedbackf onanyerrors.Theglobalrewardsignalis
i i
derivedfromc asperthescoringfunctions(ReferEquation2).
i
4.3.2 KBEDITINGAGENT
Toeffectivelyincorporateexpertfeedback,weemployamulti-actor,centralizedcriticarchitecture.
6Preprint
Centralized Critic: The centralized critic, denoted as C, is responsible for evaluating the overall
performanceoftheRAGsystembasedontheglobalrewardsignalrderivedfromexpertfeedback.
ThecriticanalyzesthefeedbackreceivedgiventhecurrentstatesofK.Thecritic‚Äôsanalysisisthen
usedtoprovidetailoredreflectionstoeachactor,guidingdocumentupdates.
The centralized critic aggregates the reward signal across multiple queries to generate a holistic
evaluationofK.
1 (cid:88)
R(s)= g(B(q ,Œì(q ,s)),c ) (3)
|T| i i i
(qi,ai,ci,fi)‚ààT
Togeneratefeedbackforthedocuments,thecriticneedstotakegradientofthisrewardwithrespect
tothedocuments.Thiswouldgiveus,
‚àÇR(s) 1 (cid:88) ‚àÇ
‚àÇ = = g(B(q ,Œì(q ,s)),c ) (4)
j ‚àÇD |T| ‚àÇD i i i
j j
(qi,ai,ci,fi)‚ààT
Figure2illustratestheenvironmentalinteractionoftheactor-criticmodel.Followingmethodologies
inpriorworks(Pryzantetal.,2023;Junejaetal.,2024;Guptaetal.,2024),weuseLLMstogener-
ateanoveralltextgradient‚àáovereachfailingexample.Thecriticfirstidentifiesandselectwhich
documentsinŒì(q ,s)areresponsibleforanyinaccuraciesino .Reflectionsarethengeneratedfor
i i
these documents based on the correct answer, expert feedback and the text gradient. However, as
showninEquation4,weneedtoaggregatethesereflectionsacrossallqueries.Insteadofasimple
concatenation,weadopttheclusteringapproachsimilartoJunejaetal.(2024),producinggeneral-
ized reflections that effectively capture the core insights from multiple queries. These aggregated
reflections can be effectively considered as the partial textual gradient ‚àÇ with respect to the doc-
ument. These partial gradients are provided as feedback to the document-specific actor A , which
j
thenperformtheactionstoeditthespecificdocuments.
Actors:EachdocumentD ‚àà Kismanagedbyadistinctactor,A ,whichismodeledasaReACT
i i
agentYaoetal.(2023)responsibleformakingstructurededitstoitsdocument.Eachactoroperates
independently, receiving reflections from the centralized critic on how to modify the content of
D = [c ].The actors need to only update these chunks as needed. The set of possible actions
i ij
includes:
‚Ä¢ EditChunk:TheactionisdefinedasEditChunk(j,t ),wherej indicateswhichchunkc
j ij
ofD tomodify,andt istheupdatedcontentforthechunk.
i j
‚Ä¢ AddChunk:TheactionisdefinedasAddChunk(n ,t ),wheren indicatesthenameofthe
j j j
newchunk,andt isthecontentforthechunk.
j
‚Ä¢ DeleteChunk:TheactionisdefinedasDeleteChunk(j),wherejspecifieswhichchunkc
ij
ofD toremove.
i
This parameterized action space allows the actors to perform precise edits within the document,
ensuring that the refinement process is both flexible and context-specific. Each actor leverages its
localstates andthedocument-specificfeedbackfromthecritictoproduceasequenceofstructured
i
edits, ensuring that modifications are consistent and contribute towards enhancing the document‚Äôs
relevanceandcompleteness.
TheReACTagentutilizesthesereflectionsanditerativelygeneratesatrajectoryt =a ,a ,a ¬∑a
0 0 1 2 n
ofeditactionstothedocumentuntiltheerrorsareresolvedortheknowledgegapsarefilled.This
controllededitingprocessimprovestheaccuracyoftheRAGsystembyensuringthattheKBcon-
tains up-to-date and relevant information. After the completion of the actor runs, we generate the
editdiffsforeachdocumentd andpoolthemtogeneratetheKBeditactionu=[d ]|K|
i i i=1
However,theremightbemanywaystoeditaKBandwemayneedtohavesomedesirablecharac-
teristicsfortheeditedKB.Inthenextsection,wediscusswhatthosedesirablecharacteristicscould
beandhowwemightmeasurethem.
7Preprint
5 EVALUATING KNOWLEDGE BASE EDITING QUALITY
AKnowledgeBaseshouldbecompletewithrespecttoatask-itshouldcontainalltheinformation
necessarytoassisttheRAGsystemtosolvethetaskathand.Giventheopen-endednatureoftasks
thattypicalRAGagentsaredesignedfor,itishardtoquantifyaclosed-formmetricofcompleteness.
That said, an ideal Knowledge Base editing system should at least be able to incorporate as much
externalfeedbackaspossible.
Further,ItwillbeextremelyundesirableforanyKnowledgeBasetoonlyhelptheRAGsystemfor
a small subset of tasks. Given the tendencies for data-driven techniques to over-fit on the train-set
distribution,itisimportantthatknowledgebaseeditsaregeneralizabletounseenexamples.
Lastly, given the semantic and textual nature of the Knowledge Base, it is important that the doc-
uments in the Knowledge base are coherent and consistent throughout. This not only makes the
document interpretable for human consumption, it also help reduce in-context noise during LLM
inteference,whichhasbeenshowntoaffectLLMperformance(Liuetal.,2024).
6 EXPERIMENTAL SETUP
6.1 BASELINE
Whiletherehasbeenarichbodyofworksintheareaofknowledgeeditingandpromptoptimization,
tothebestofourknowledge,STACKFEEDisthefirstworktargetingthefeedback-driventextual
Knowledge Base Editing problem. Therefore, to perform a holistic evaluation of STACKFEED
we implement - PROMPTAGENT-E, an extension of PROMPTAGENT Wang et al. (2023) for the
KBeditingtask. PROMPTAGENT formulatespromptoptimizationasastrategicplanningproblem
usingMonteCarloTreeSearch(MCTS).Atahigh-levelourbaselineapproach,PROMPTAGENT-E
createsseparatePROMPTAGENT-styleagentstooptimizespecificdocumentintheKB.Tominimize
spuriouseditsintheKnowledgeBase,werestrict PROMPTAGENT -Etoonlyoptimizedocuments
that were part of the retrievals for more than 2 training sample. After identifying the best nodes
foreachofthedocument-wiseruns,weputthembackintheknowledgebasetogeneratethenew
version of the KB. In contrast to STACKFEED, PROMPTAGENT -E can be seen as a collection
of document-wise Independent Actor-Critic models (Foerster et al., 2017). We present in-depth
comparisonsbetweenPROMPTAGENT-EandSTACKFEEDinSection7
6.2 DATASETS
KnowledgeBaseEditingcanbeusefulforscenarioswheretheKBis1.Incomplete,or2.Incorrect.
WeevaluateSTACKFEED on5datasetsspanningthesedifferentsettings.
6.2.1 INCOMPLETEKNOWLEDGEBASE
We adapt two code generation datasets from
ARKS (Su et al., 2024a), namely ARKS-Pony Dataset Train Eval Test Documents
andARKS-Ring.ThedatasetconsistsofLeetCode Pony 31 32 45 601
problems and their solutions in low-resource lan- Ring 26 27 39 577
ScipyM 22 22 98 3921
guages Pony and Ring respectively. Each datapoint
TensorflowM 9 9 26 5859
issupplementedwithacorrespondinglanguagedoc- CLARKSNews 30 30 60 138
umentation, with execution accuracy as the success
metricandexecutionfailuresasfeedbacktothesys- Table1:DatasetStatistics
tem.Giventhattheselanguagedon‚Äôtappearpromi-
nentlyinLLMpre-trainingdata,theperformanceofcodegenerationRAGagentsonthesedatasets
depends significantly on the quality of the Knowledge Base. However, given that these languages
havesmallercommunities,theirdocumentationisn‚Äôtaswellmaintainedandoftenlackcriticalinfor-
mation..Forthepurposeofevaluationonthesedatasets,wesplitthemintotrain,eval,testsplitsas
specifiedinTable 1.Toensurethatwehaveagoodrepresentationoffailurecasesduringtraining,
wefirstexecutetheRAGpipelineontheentiredatasetanddividethefailuresatrandomina1:1:2
ratiofortrain,evalandtestrespectively.Allthedatapointswithsuccessfulexecutionmatchareput
8Preprint
Dataset Ring Pony SciPy Tensorflow CLARK-news
Acc œÉ Acc œÉ Acc œÉ Acc œÉ Acc œÉ
BaseKB 30.77 2.09 29.99 1.57 52.04 0.00 28.88 2.18 26.27 1.20
PROMPTAGENT-E 33.33 2.81 32.22 1.57 53.40 3.12 47.77 3.57 28.80 2.39
STACKFEED 36.75 1.21 37.04 1.28 59.38 1.22 53.84 3.11 37.28 1.69
Table 2: Generalization performance comparison between STACKFEED and baseline models
acrossmultipledatasets,reportedasaccuracypercentages(higherisbetter).
in the test split. We use the compiler feedback from the executions as the expert feedback to the
STACKFEEDsystem.
6.2.2 INCORRECTKNOWLEDGEBASE
Forevaluatingunderthissetting,weleveragetheARKS-ScipyMandARKS-TensorflowMdatasets
fromARKSandtheCLARK-newsdatasetfromErase(Lietal.,2024).TheARKSdatasetsconsist
ofdatascienceproblemssourcedfromtheDS-1000dataset (Laietal.,2022),whicharetobesolved
byartificiallyperturbedversionsofscipyandtensorflowlibrariesrespectively,whilereferringtothe
original unperturbed documentation. Similar to Pony and Ring, we use the execution accuracy on
a test bench as a success metric and use compiler outcome as expert feedback. We also follow a
similarapproachfordatasplitting.
While fact retrieval is one of the most popular use cases of RAG systems, evolving nature of in-
formation requires us to keep the knowledge bases up to date. To simulate these dynamic factual
knowledgeupdatesweusetheCLARKS-newsdatasetfromErase(Lietal.,2024)whichcontains
questionsandtheirrespectiveanswersextractedfromWikidataatdifferenttimestamps.Eachtimes-
tampischaracterizedbyasetofarticlesthatwereaddedinthedataatthattime.Forourevaluation,
we pool all the questions whose answers changed for the first time at a given timestamp and split
themacrosstrain,evalandtestsplitsina1:1:2ratio(Table1).
6.3 EVALUATIONMETRICS
In section 5 we discussed the desirable properties of a Knowledge Base edit. We leverage these
propertiestodesign3metricsfortheKBEditingproblemasfollows:
Completeness:Weusethetrainsetaccuracytoestimatethedegreeofexpertfeedbackincorporated
inthelearntKnowledgeBase.
Generalization:ToestimatethedegreeofgeneralizationofourKnowledgeBaseedits,weusethe
heldouttestsetaccuracy.
Coherence: To quantify the degree of coherence of the KB, we first calculate a document-wise
coherence score using G-Eval (Liu et al., 2023) with GPT4-1106-PREVIEW as the judge model.
TheG-evalprompt assignsa1-5scoretothediff ofchangeswithrespecttotheoriginaldocument,
checking for thematic similarity of the diff. We pool all the edited documents for a KB edit and
averagethererespectivecoherencescoretodefinetheKBcoherencemetric.
6.4 SYSTEMCONFIGURATIONS
MCTS parameters: We use the Upper Confidence bounds applied to Trees (UCT) algorithm for
selectingexpansionnodes,enablingeffectiveexplorationandexploitationoftheKBstatespace.For
ourexperiments,wesetamaximumsearchdepthof3,anexpansionwidthof3,andamaximumof5
iterations.TheUCTexplorationconstantissetto2.5.Theseparameterswerechosentobalancethe
computationalcostandtheneedforadequateexploration.Adepthof3ensuresthatthesearchcan
explore sufficient variations in the KB states without unnecessary expansion, while an expansion
width of 3 allows a moderate number of candidate states to be evaluated at each step. Similarly,
5 iterations provide enough opportunity to refine the state search, and the UCT constant of 2.5
encourages sufficient exploration in early stages while converging towards high-reward states in
9Preprint
Completeness(in%) Coherence(1-5,higherisbetter)
Dataset Ring Pony SciPy Tensorflow CLARK-news Ring Pony SciPy Tensorflow CLARK-news
PROMPTAGENT-E 4.27 3.22 33.33 33.33 11.86 4.33 1.86 2.0 4.0 1
STACKFEED 8.98 9.68 31.38 44.44 13.79 4.67 4.6 4.30 4.0 1
Table 3: Completeness and coherence comparison between STACKFEED and baseline models
acrossmultipledatasets.Completenessisreportedasaccuracypercentages(higherisbetter),while
coherenceismeasuredonascaleof1-5(higherisbetter).
laterstages.Forunstructureddata,thedocumentsarechunkedafterevery50linesandtheneditthe
chunks.
RAG System: For the purpose of our evaluations, we setup a generic RAG system which uses an
embedding similarity for semantic retrieval. Additionally, in lines with prior works like (Zhang
etal.,2023)forcodingrelatedtasks,weuseaniterativeretrievalsetupwhereinwefirstgeneratea
codeusingnaive retrievalandthenquery thedatabaseagainwith boththequestionand generated
codetoimprovethequalityofretrievalbeforegeneratingthefinalresult.
LLM configs: We use OPENAI-TEXT-EMBEDDING-3-LARGE as the embedding model with di-
mensions size of 3072 and use cosine similarity as a metric of embedding match for ranking. To
accountforthe8191maxinputlimit,wecreatedocumentchunksofatmost7500tokens.Forthe
reasoningmodel,weuse GPT4-1106-PREVIEW,withatemperatureof0.SinceLLMsareknown
toperformpoorlywithlongercontextinput(Liuetal.,2024),werestrictthemaxtokenbudgetfor
retrievalsat18000tokensandremoveanylowerrankedretrievaltofitthistokenbudget.
7 RESULTS
7.1 COMPLETENESSANDGENERALIZATION
WeobserveconsistentimprovementsoverthePROMPTAGENT-Ebaselineincompletenessandgen-
eralizabilityscores,withSTACKFEEDachievingapproximately2xperformancegainsonRingand
Ponydatasets.However,feedbackincorporationremainslimited,likelyduetosuboptimalretrieval
orlimiteddocument-queryassociationshinderinglearnabilityofedit. STACKFEED alsodemon-
strateshighergeneralizabilityandlowervariance,attributedtoitsstructuredandfocuseddocument
editsthatenhancecoherence.
7.2 STACKFEEDMAKESHIGHQUALITYCOHERENTEDITS
As seen in Table 3, STACKFEED produces edits with a coherence score of 4 or higher for most
datasets. For KBs which need long term maintenance (like language and code documentation as
seen in the ARKS datasets), STACKFEED makes more coherent edits compared to the baseline.
ThisisespeciallytrueforlongdocumentsasseenintheARKSPonydataset.Fornews-articlelike
datasetlikeCLARK-newswithfactualedits.Incoherencyisnaturallyinducedwhenthefactsofthe
articlearechanged.Forinstance,anarticleonthecoronationofakingwilllosecoherencywhenthe
articleisupdatedtoaddinformationaboutthecoronationofanewking.
8 CONCLUSION
We introduced STACKFEED, a novel framework for refining Knowledge Bases (KBs) in
Retrieval-Augmented Generation (RAG) systems using a multi-actor, centralized critic architec-
ture. STACKFEED enables efficient KB updates without retraining or altering model parameters
byleveragingfeedback-drivenstructurededitsandtextualgradients.
Ourapproachachievedsuperiorperformanceinpreservingknowledgebase(KB)coherence,consis-
tency,andcompleteness,resultinginenhancedRAGsystemresponses.Nonetheless,thereremains
considerablepotentialforfurtheradvancements.Futureworkwillfocusonrefiningthesethreemet-
ricstoelevatesystemperformanceevenfurther.
10Preprint
REFERENCES
Anum Afzal, Alexander Kowsik, Rajna Fani, and Florian Matthes. Towards optimizing and
evaluating a retrieval augmented qa chatbot using llms with human in the loop, 2024. URL
https://arxiv.org/abs/2407.05925.
Afra Feyza Akyu¬®rek, Ekin Akyu¬®rek, Leshem Choshen, Derry Wijaya, and Jacob Andreas. De-
ductive closure training of language models for coherence, accuracy, and updatability. In Lun-
Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computa-
tional Linguistics ACL 2024, pp. 9802‚Äì9818, Bangkok, Thailand and virtual meeting, August
2024.AssociationforComputationalLinguistics. doi:10.18653/v1/2024.findings-acl.584. URL
https://aclanthology.org/2024.findings-acl.584.
Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and
JuanziLi. Longwriter:Unleashing10,000+wordgenerationfromlongcontextllms,2024. URL
https://arxiv.org/abs/2408.07055.
LucianBusoniu,RobertBabuska,andBartDeSchutter.Acomprehensivesurveyofmultiagentrein-
forcementlearning. IEEETransactionsonSystems,Man,andCybernetics,PartC(Applications
andReviews),38(2):156‚Äì172,2008. doi:10.1109/TSMCC.2007.913919.
LorenzoCanese,GianCarloCardarilli,LucaDiNunzio,RoccoFazzolari,DanieleGiardino,Marco
Re, and Sergio Spano`. Multi-agent reinforcement learning: A review of challenges and appli-
cations. Applied Sciences, 11(11), 2021. ISSN 2076-3417. doi: 10.3390/app11114948. URL
https://www.mdpi.com/2076-3417/11/11/4948.
DanqiChen,AdamFisch,JasonWeston,andAntoineBordes. ReadingWikipediatoansweropen-
domainquestions. InReginaBarzilayandMin-YenKan(eds.),Proceedingsofthe55thAnnual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1870‚Äì
1879,Vancouver,Canada,July2017.AssociationforComputationalLinguistics. doi:10.18653/
v1/P17-1171. URLhttps://aclanthology.org/P17-1171.
Wubing Chen, Wenbin Li, Xiao Liu, Shangdong Yang, and Yang Gao. Learning explicit credit
assignment for cooperative multi-agent reinforcement learning via polarization policy gradient.
Proceedings of the AAAI Conference on Artificial Intelligence, 37(10):11542‚Äì11550, Jun 2023.
doi:10.1609/aaai.v37i10.26364. URLhttps://ojs.aaai.org/index.php/AAAI/article/view/26364.
AbhishekDas,SatwikKottur,Jose¬¥M.F.Moura,StefanLee,andDhruvBatra.Learningcooperative
visual dialog agents with deep reinforcement learning. In 2017 IEEE International Conference
onComputerVision(ICCV),pp.2970‚Äì2979,2017. doi:10.1109/ICCV.2017.321.
Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In
Marie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.),Proceed-
ingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.6491‚Äì
6506,OnlineandPuntaCana,DominicanRepublic,November2021a.AssociationforComputa-
tionalLinguistics. doi:10.18653/v1/2021.emnlp-main.522. URLhttps://aclanthology.org/2021.
emnlp-main.522.
Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In
Marie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.),Proceed-
ingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.6491‚Äì
6506,OnlineandPuntaCana,DominicanRepublic,November2021b.AssociationforComputa-
tionalLinguistics. doi:10.18653/v1/2021.emnlp-main.522. URLhttps://aclanthology.org/2021.
emnlp-main.522.
Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to com-
municatewithdeepmulti-agentreinforcementlearning. InProceedingsofthe30thInternational
ConferenceonNeuralInformationProcessingSystems,NIPS‚Äô16,pp.2145‚Äì2153,RedHook,NY,
USA,2016.CurranAssociatesInc. ISBN9781510838819.
JakobN.Foerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShimonWhiteson.
Counterfactualmulti-agentpolicygradients. InAAAIConferenceonArtificialIntelligence,2017.
URLhttps://api.semanticscholar.org/CorpusID:19141434.
11Preprint
Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon White-
son. Counterfactual multi-agent policy gradients. In Proceedings of the Thirty-Second AAAI
Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelli-
gence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelli-
gence,AAAI‚Äô18/IAAI‚Äô18/EAAI‚Äô18.AAAIPress,2018. ISBN978-1-57735-800-8.
Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. Artificial
Intelligence Review, 55(2):895‚Äì943, February 2022. ISSN 1573-7462. doi: 10.1007/s10462-
021-09996-w. URLhttps://doi.org/10.1007/s10462-021-09996-w.
Jayesh K. Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control us-
ingdeepreinforcementlearning. InGitaSukthankarandJuanA.Rodriguez-Aguilar(eds.),Au-
tonomous Agents and Multiagent Systems, pp. 66‚Äì83, Cham, 2017. Springer International Pub-
lishing. ISBN978-3-319-71682-4.
PriyanshuGupta,ShashankKirtania,AnanyaSingha,SumitGulwani,ArjunRadhakrishna,Sherry
Shi, and Gustavo Soares. Metareflection: Learning instructions for language agents using past
reflections,2024. URLhttps://arxiv.org/abs/2405.13009.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: retrieval-
augmentedlanguagemodelpre-training. InProceedingsofthe37thInternationalConferenceon
MachineLearning,ICML‚Äô20.JMLR.org,2020.
WenyueHua,JiangGuo,MingwenDong,HenghuiZhu,PatrickNg,andZhiguoWang.Propagation
andpitfalls:Reasoning-basedassessmentofknowledgeeditingthroughcounterfactualtasks. In
Lun-WeiKu,AndreMartins,andVivekSrikumar(eds.),FindingsoftheAssociationforCompu-
tationalLinguisticsACL2024,pp.12503‚Äì12525,Bangkok,Thailandandvirtualmeeting,August
2024.AssociationforComputationalLinguistics. doi:10.18653/v1/2024.findings-acl.743. URL
https://aclanthology.org/2024.findings-acl.743.
Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong.
Transformer-patcher:Onemistakeworthoneneuron. InTheEleventhInternationalConference
onLearningRepresentations,2023. URLhttps://openreview.net/forum?id=4oYUGeGBPm.
ShariqIqbalandFeiSha. Actor-attention-criticformulti-agentreinforcementlearning,2019. URL
https://openreview.net/forum?id=HJx7l309Fm.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning
withretrievalaugmentedlanguagemodels,2022. URLhttps://arxiv.org/abs/2208.03299.
XisenJin,DejiaoZhang,HenghuiZhu,WeiXiao,Shang-WenLi,XiaokaiWei,AndrewArnold,and
XiangRen. Lifelongpretraining:Continuallyadaptinglanguagemodelstoemergingcorpora. In
Angela Fan, Suzana Ilic, Thomas Wolf, and Matthias Galle¬¥ (eds.), Proceedings of BigScience
Episode#5‚ÄìWorkshoponChallenges&PerspectivesinCreatingLargeLanguageModels,pp.
1‚Äì16,virtual+Dublin,May2022.AssociationforComputationalLinguistics. doi:10.18653/v1/
2022.bigscience-1.1. URLhttps://aclanthology.org/2022.bigscience-1.1.
GurushaJuneja,NagarajanNatarajan,HuaLi,JianJiao,andAmitSharma. Taskfacetlearning:A
structuredapproachtopromptoptimization,2024. URLhttps://arxiv.org/abs/2406.10504.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-
tionthroughmemorization:Nearestneighborlanguagemodels. InInternationalConferenceon
LearningRepresentations,2020. URLhttps://openreview.net/forum?id=HklBjCEKvH.
NikitaKitaev,LukaszKaiser,andAnselmLevskaya. Reformer:Theefficienttransformer. InInter-
nationalConferenceonLearningRepresentations,2020. URLhttps://openreview.net/forum?id=
rkgNKkHtvB.
YuhangLai,ChengxiLi,YimingWang,TianyiZhang,RuiqiZhong,LukeZettlemoyer,ScottWen
tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for
datasciencecodegeneration. ArXiv,abs/2211.11501,2022.
12Preprint
JoelZ.Leibo,ViniciusZambaldi,MarcLanctot,JanuszMarecki,andThoreGraepel. Multi-agent
reinforcementlearninginsequentialsocialdilemmas. InProceedingsofthe16thConferenceon
Autonomous Agents and MultiAgent Systems, AAMAS ‚Äô17, pp. 464‚Äì473, Richland, SC, 2017.
InternationalFoundationforAutonomousAgentsandMultiagentSystems.
BelindaZ.Li,EmmyLiu,AlexisRoss,AbbasZeitoun,GrahamNeubig,andJacobAndreas. Lan-
guagemodelingwitheditableexternalknowledge,2024. URLhttps://arxiv.org/abs/2406.11830.
NelsonF.Liu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,and
Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the
AssociationforComputationalLinguistics,12:157‚Äì173,2024. doi:10.1162/tacl a 00638. URL
https://aclanthology.org/2024.tacl-1.9.
Yang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg
evaluation using gpt-4 with better human alignment. In Conference on Empirical Methods in
NaturalLanguageProcessing,2023. URLhttps://api.semanticscholar.org/CorpusID:257804696.
AmanMadaan,NiketTandon,PeterClark,andYimingYang. Memory-assistedprompteditingto
improveGPT-3afterdeployment. InYoavGoldberg,ZornitsaKozareva,andYueZhang(eds.),
Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.
2833‚Äì2861,AbuDhabi,UnitedArabEmirates,December2022.AssociationforComputational
Linguistics. doi:10.18653/v1/2022.emnlp-main.183. URLhttps://aclanthology.org/2022.emnlp-
main.183.
JoshuaMaynez,ShashiNarayan,BerndBohnet,andRyanMcDonald.Onfaithfulnessandfactuality
in abstractive summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault
(eds.),Proceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,
pp.1906‚Äì1919,Online,July2020.AssociationforComputationalLinguistics. doi:10.18653/v1/
2020.acl-main.173. URLhttps://aclanthology.org/2020.acl-main.173.
KevinMeng,DavidBau,AlexAndonian,andYonatanBelinkov.Locatingandeditingfactualassoci-
ationsinGPT.AdvancesinNeuralInformationProcessingSystems,36,2022.arXiv:2202.05262.
KevinMeng,ArnabSenSharma,AlexJAndonian,YonatanBelinkov,andDavidBau.Mass-editing
memoryinatransformer.InTheEleventhInternationalConferenceonLearningRepresentations,
2023. URLhttps://openreview.net/forum?id=MkbcAHIYgyS.
Yasumasa Onoe, Michael Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol Choi. Can
LMs learn new entities from descriptions? challenges in propagating injected knowledge. In
Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st An-
nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.
5469‚Äì5485, Toronto, Canada, July 2023. Association for Computational Linguistics. doi:
10.18653/v1/2023.acl-long.300. URLhttps://aclanthology.org/2023.acl-long.300.
ShankarPadmanabhan,YasumasaOnoe,MichaelJQZhang,GregDurrett,andEunsolChoi. Prop-
agatingknowledgeupdatestoLMsthroughdistillation. InThirty-seventhConferenceonNeural
InformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=DFaGf3O7jf.
OfirPress,NoahSmith,andMikeLewis. Trainshort,testlong:Attentionwithlinearbiasesenables
inputlengthextrapolation. InInternationalConferenceonLearningRepresentations,2022. URL
https://openreview.net/forum?id=R8sQPpGCv0.
ReidPryzant,DanIter,JerryLi,YinLee,ChenguangZhu,andMichaelZeng. Automaticprompt
optimizationwith‚Äúgradientdescent‚Äùandbeamsearch.InHoudaBouamor,JuanPino,andKalika
Bali(eds.),Proceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguagePro-
cessing,pp.7957‚Äì7968,Singapore,December2023.AssociationforComputationalLinguistics.
doi:10.18653/v1/2023.emnlp-main.494. URLhttps://aclanthology.org/2023.emnlp-main.494.
PragnyaRamjee,BhuvanSachdeva,SatvikGolechha,ShreyasKulkarni,GeetaFulari,KaushikMu-
rali,andMohitJain.Cataractbot:Anllm-poweredexpert-in-the-loopchatbotforcataractpatients,
2024. URLhttps://arxiv.org/abs/2402.04620.
13Preprint
WeijiaShi,SewonMin,MichihiroYasunaga,MinjoonSeo,RichJames,MikeLewis,LukeZettle-
moyer,andWentauYih. Replug:Retrieval-augmentedblack-boxlanguagemodels,2023. URL
https://arxiv.org/abs/2301.12652.
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and
Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. URL
https://arxiv.org/abs/2303.11366.
HongjinSu,ShuyangJiang,YuhangLai,HaoyuanWu,BoaoShi,CheLiu,QianLiu,andTaoYu.
Arks: Active retrieval in knowledge soup for code generation. ArXiv, abs/2402.12317, 2024a.
URLhttps://api.semanticscholar.org/CorpusID:267750919.
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En-
hancedtransformerwithrotarypositionembedding. Neurocomput.,568(C),March2024b. ISSN
0925-2312. doi: 10.1016/j.neucom.2023.127063. URL https://doi.org/10.1016/j.neucom.2023.
127063.
Ardi Tampuu, Tambet Matiisen, Davide Kodelja, Igor Kuzovkin, Kristjan Korjus, Jaan Aru, Bo-
rysTeodorAru,andRaulVicente. Multiagentcooperationandcompetitionwithdeepreinforce-
ment learning. PLOS ONE, 12(4):e0172395, 2017. doi: 10.1371/journal.pone.0172395. URL
https://doi.org/10.1371/journal.pone.0172395.
MingTan. Multi-agentreinforcementlearning:independentversuscooperativeagents. InProceed-
ings of the Tenth International Conference on International Conference on Machine Learning,
ICML‚Äô93,pp.330‚Äì337,SanFrancisco,CA,USA,1993.MorganKaufmannPublishersInc.ISBN
1558603077.
Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao, Jiarong Xu, and Fandong Meng. Cross-
lingual knowledge editing in large language models. In Lun-Wei Ku, Andre Martins, and
Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pp. 11676‚Äì11686, Bangkok, Thailand, August
2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.627. URL
https://aclanthology.org/2024.acl-long.627.
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
withlinearcomplexity,2020. URLhttps://arxiv.org/abs/2006.04768.
WeixuanWang,BarryHaddow,andAlexandraBirch. Retrieval-augmentedmultilingualknowledge
editing. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd
AnnualMeeting oftheAssociationfor ComputationalLinguistics(Volume 1:LongPapers),pp.
335‚Äì354, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi:
10.18653/v1/2024.acl-long.21. URLhttps://aclanthology.org/2024.acl-long.21.
XinyuanWang,ChenxiLi,ZhenWang,FanBai,HaotianLuo,JiayouZhang,NebojsaJojic,EricP.
Xing,andZhitingHu.Promptagent:Strategicplanningwithlanguagemodelsenablesexpert-level
promptoptimization,2023. URLhttps://arxiv.org/abs/2310.16427.
YihanWang,BeiningHan,TonghanWang,HengDong,andChongjieZhang. {DOP}:Off-policy
multi-agentdecomposedpolicygradients. InInternationalConferenceonLearningRepresenta-
tions,2021. URLhttps://openreview.net/forum?id=6FqKiVAdI3Y.
Yan Xu, Mahdi Namazifar, Devamanyu Hazarika, Aishwarya Padmakumar, Yang Liu, and Dilek
Hakkani-Tur. KILM: Knowledge injection into encoder-decoder language models. In Anna
Rogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.),Proceedingsofthe61stAnnualMeet-
ing of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5013‚Äì5035,
Toronto,Canada,July2023.AssociationforComputationalLinguistics. doi:10.18653/v1/2023.
acl-long.275. URLhttps://aclanthology.org/2023.acl-long.275.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
React: Synergizing reasoning and acting in language models, 2023. URL https://arxiv.org/abs/
2210.03629.
14Preprint
LangYu,QinChen,JieZhou,andLiangHe. Melo:Enhancingmodeleditingwithneuron-indexed
dynamic lora. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):19449‚Äì
19457, Mar. 2024. doi: 10.1609/aaai.v38i17.29916. URL https://ojs.aaai.org/index.php/AAAI/
article/view/29916.
Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang
Lou,andWeizhuChen. Repocoder:Repository-levelcodecompletionthroughiterativeretrieval
andgeneration,2023. URLhttps://arxiv.org/abs/2303.12570.
Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzma¬¥n, Luke Zettlemoyer,
and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence gen-
eration. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the
AssociationforComputationalLinguistics:ACL-IJCNLP2021,pp.1393‚Äì1404,Online,August
2021.AssociationforComputationalLinguistics. doi:10.18653/v1/2021.findings-acl.120. URL
https://aclanthology.org/2021.findings-acl.120.
15Preprint
A APPENDIX
A.1 PROMPTSUSEDINSTACKFEED
""" There exists a Language Model based software named CodeRAG that automatically does the following
task for a developer: task - taskdesc
CodeRAG uses a knowledge base to perform this task: kbdesc
A developer used CodeRAG to perform the task on multiple files, and CodeRAG made some errors on them.
Here is one knowledge base file that was involved in these errors: """ for i, file in
enumerate(kbfiles): prompt += f""" File i+1: id: file[‚Äôid‚Äô] content:
n<file>
nfile[‚Äôcontent‚Äô]
n</file>
n""" if "specialnotes" in file and file["specialnotes"] != "": prompt += f"""
nspecialnotes: file[‚Äôspecialnotes‚Äô]"""
""" The following are the reflections on the errors made by CodeRAG: reflectionsstr
The reflections show the relationship of the file with the errors made by CodeRAG. If the file is named
"None," it means the information about the error on which the reflection is based does not fully fit any
knowledge base file.
Your task is to use the reflections on the errors made by CodeRAG and provide a generalization on the
issues with the file and how it can be improved to prevent the errors.
You should mention common issues found in the reflections and provide a plan for improving the knowledge
base files to prevent future errors. Use the reflections to suggest additions or changes in the file,
explaining what new content should be added to prevent errors. Before suggesting your plan, give context
on the errors using code snippets and other relevant information from the reflections.
You have a scratchpad to reason and plan your generalization. Your scratchpad is for your use only and
will not be shared with anyone else. The scratchpad is represented by the <scratchpad></scratchpad>
tags.
Your generalization should follow this format: <scratchpad> The contents of the scratchpad </scratchpad>
<generalization> Your generalization for this file </generalization>
You must provide the filled-out scratchpad and generalization in the above format.
General guidelines: 1. Carefully analyze the reflections to understand the errors CodeRAG is making. 2.
"None" is a special file, representing that to fix the error, the information should be in a new file.
"""
Figure3:GeneralizationStagePrompt
16Preprint
""" Your task is to reflect upon the errors made by CodeRAG based on the user feedback and provide a
reflection on the role of the knowledge base files in the making of those errors.
Your reflection should be very specific to the knowledge base files as these reflections will be used to
improve the knowledge base files to prevent such errors in the future.
There may be other causes for the error, but you should only focus on whether the knowledge base files
could have prevented the error.
You should also provide a way for improving the knowledge base files to prevent the error from happening
again.
You should try and see if there is any error in the information provided by the knowledge base or if the
knowledge base is missing some information that could have prevented the error.
You also have to figure out if the file should be edited or not. That you do through the needsediting
flag.
You have a scratchpad in which you can reason and plan your reflection. Your scratchpad is for your use
only and will not be shared with anyone else. This scratchpad is represented by the <scratchpad> tags.
Your output should be in the following format:
<scratchpad>
The contents of the scratchpad
</scratchpad>
<reflection>
<File 1>
File: Name of the first file
needs_editing: True/False
Reflection: The reflection for this file
</File 1>
<File 2>
File: Name of the second file
needs_editing: True/False
Reflection: The reflection for this file
</File 2>
...
</reflection>
You have to provide the filled-out scratchpad and the reflection in the above-described formats. You
have to reflect on all the files that were extracted for the code file.
Here are some general guidelines to follow:
1. You should first analyze the question, the test bench, the feedback, and the output to
understand the error made by CodeRAG.
2. Then you should carefully analyze the knowledge base files to see if the theme and the
contents of any knowledge base file are relevant to the error. Particularly, you should look
out for files that have a factual error related to the error or are missing some information
which should have been in the file according to the theme of the file.
(a) Read the content of the file and understand the theme of the file. The theme of this
file is of course based on the file ID and the content of the file but you should also
consider its positioning in the knowledge base. That means you should consider the other
files that were extracted for the code file and see how this file fits in with them.
For example, if the file is a very basic general guide to the task with other files
providing more detailed information, then it would make sense for this file to not have
detailed information about specific cases.
(b) See if the file has any information related to the error. Check for relevant keywords
and how the file might have biased the language model to make the error.
(c) If the file has information related to the error, see if the information is correct and
complete. If the information is incorrect or incomplete, the file is responsible for the
error.
(d) If it doesn¬¥t have information related to the error, check if it makes sense for the
file to have information related to the error. If it doesn‚Äôt make sense, the file is not
responsible for the error. When deciding this, check whether the information would be
better suited in any of the other knowledge base files. If the missing info fits better
in another file, then deem this file to not be responsible for the error as the missing
content can be better placed in the other file.
(e) If the file is responsible for the error, explain the error in your reflection and set
the needsediting flag to True. And if the file is not responsible for the error, set the
needsediting flag to False.
3. If none of the files have any error or if you think the content for the error should be in
a new file, put a file with the name ‚Äò‚ÄòNone‚Äô‚Äô in your reflection and for its reflection,
describe the error and mention why it is not due to the knowledge base files. For the ‚Äò‚ÄòNone‚Äô‚Äô
file, the needsediting flag should always be set to True. The ‚Äò‚ÄòNone‚Äô‚Äô file should be placed
as File n+1 where n is the number of files extracted for the code file.
4. Choose the least number of files for editing, we want to change as few files as we can for any
error. For example, if we have 5 knowledge base files, unless very extreme cases, we wouldn‚Äôt
want to set the needsediting flag as True on more than 2 files. Figure out what the most
relevant files for the error are and focus on them.
5. When you choose to edit multiple files, you should make sure that their involvements in the
error are distinct and not overlapping. If they are overlapping, think about whether changing
one file would be enough to fix the error.
"""
Figure4:SelectionStagePrompt
17Preprint
""" There exists a Language Model based software named CodeRAG that automatically does the following
task for a developer:
{test_bench_code}
The test bench code gives a code where a function must be inserted and then it is tested with some
test cases.
CodeRAG then outputted the following code to answer the question:
if task_desc != "":
prompt += f"""
{task} - {task_desc}
"""
else:
prompt += f"""
{task}
"""
prompt += f"""
The developer used CodeRAG for a question. The question is as follows:
{query}
In the question, the developer provided the following test bench code:
{test_bench_code}
The test bench code gives a code where a function must be inserted and then it is
tested with some test cases.
CodeRAG then outputted the following code to answer the question:
{output_code}
Based on the above output, the developer gave the following feedback to CodeRAG:
{feedback}
CodeRAG uses a knowledge base to do this task
{kb_desc}
The following files were extracted for this particular code file (the content of
each file is surrounded in <file></file> tags):
"""
for i, instruction in enumerate(instructions):
prompt += f"""
File {i+1}:
id: "{instruction[‚Äôid‚Äô]}"
content: \n<file>\n{instruction[‚Äôcontent‚Äô]}\n</file>\n
"""
if "special_notes" in instruction and instruction["special_notes"] != "":
prompt += f"""\nspecial_notes: {instruction[‚Äôspecial_notes‚Äô]}"""
prompt += """
Your task is to reflect upon the errors made by CodeRAG based on the user feedback.
You have to explain in detail the error made by CodeRAG. The reflection should be
very specific to the question, the output code and the feedback.
You should start by explaining the question that CodeRAG was asked to solve before talking about the error.
Your reflection should have relevant code snippets from the output
code which have errors and what should be done to fix them.
You should also add a small code example to demonstrate the error and potential methods to fix it.
You can talk about multiple different methods here to address the error.
You have a scratchpad in which you can reason and plan your reflection.
Your scratchpad is for your use only and will not be shared with anyone else.
Your reflection should be in the following format:
<scratchpad>
The contents of the scratchpad
</scratchpad>
<reflection>
Your reflection
</reflection>
"""
"""
Figure5:ReflectionStagePrompt
18Preprint
"""
There exists a Language Model based software named CodeRAG that automatically does the following task
for a developer:
{task} - {task_desc}
CodeRAG was used to perform the task on a specific repository.
The following files in the repository were edited by CodeRAG:
<files>
{files}
</files>
A developer has provided the following feedback on CodeRAG‚Äôs output:
<feedback>
{feedback}
</feedback>
Your task is to parse this feedback.
You must separate the larger feedback into smaller feedbacks, where each feedback corresponds
to a specific file in the repository.
**DO NOT** change the content of the feedback. Your job is only to split the feedback into file-level
feedbacks, without altering the feedback‚Äôs content in any way.
You should respond in the following output format:
‚Äò‚Äò‚Äòjson
{
"task": The task name, like {task},
"num_files": The number of files mentioned in the feedback,
"feedback_files": [
{
"target_file": The file name that the feedback is about, it should be the
whole path of the file, leave it empty if you are not sure which file the feedback is about. If
the file name is not empty,
it should be one of the files in the repository.
If the file name does not exist in the repository, put an empty string here,
"num_feedbacks": The number of feedbacks for this file,
"feedbacks": [
{
"feedback_tone": The tone of the feedback, can take the values ‚Äòpositive‚Äò or
‚Äònegative‚Äò,
"target_spans": The spans of the code that the feedback is about.
It is a list of json objects with two keys, ‚Äôstart‚Äô and ‚Äôend‚Äô.
Put an empty list if you are not sure which code span the feedback is about,
"feedback": Description of the feedback,
}
]
}
]
}
Make sure to include the backticks (‚Äò‚Äò‚Äò) surrounding the output. They are needed for further parsing of
your output. """
Figure6:FeedbackParsingPrompt
19