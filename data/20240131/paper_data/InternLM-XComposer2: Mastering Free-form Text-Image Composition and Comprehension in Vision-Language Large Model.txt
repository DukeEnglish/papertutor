InternLM-XComposer2: Mastering Free-form Text-Image
Composition and Comprehension in Vision-Language Large Models
XiaoyiDong‚àó1,2,PanZhang‚àó1,YuhangZang‚àó1,YuhangCao1,2,BinWang1,LinkeOuyang1,XilinWei1,
SongyangZhang1,HaodongDuan1,MaosongCao1,WenweiZhang1,YiningLi1,HangYan1,YangGao1,
XinyueZhang1,WeiLi1,JingwenLi1,KaiChen1,ConghuiHe3,XingchengZhang3,
YuQiao1,DahuaLin1,2,JiaqiWang1,(cid:66)
1ShanghaiArtificialIntelligenceLaboratory,2TheChineseUniversityofHongKong,3SenseTimeGroup
internlm@pjlab.org.cn
MathVista
Answer the two
questions in the AI2D ChartVQA
following figure. Provide 57.6
intermediate steps. 78.7
MMMU 72.6 HallB
To find the length of the edge, we can use the Pythagorean theorem,
pPl re oa vs ide e w dr mite a ta en r ia ar lsti Ôºöcle based on the given requirements with the title 'Introduction to Chinese New Year Customs,' using the sw h qyh upi ac oh rt ee s snt a u ot s fe e ts h ( t eth h la e et ns i gin d t e ha s or i opg fph tot h st eir t i e oa n tt hhg el ee r , r t it wgh h oe t ss a iq dnu ega sler . e ) I n io s t f e h t q ih sue ca al le stn eog , tt whh e e o s hf u at mh vee o af rt ih ge h t 42.0 60.3
1. Chinese New Year customs include pasting couplets, setting off firecrackers, and paying respects to ancestors. triangle with one side of length 4 and the hypotenuse being the edge we
2. New Year delicacies include eating dumplings and glutinous rice balls, expressing the symbolic meaning of family reunion. want to find the length of. MME 2242.7 51.2 MM-Vet Let's denote the length of the edge we want to find as x. Then, according
to the Pythagorean theorem, we have:
u F T gf tC e ie ehsh ms u b nei tn eI a r e iv Se un ,l r la ps a amyt l re r t f iir ay i nn a oNo ngl 1 nlCe ys sd sFw h t c .b e. i uu T n e sY s htae tc tiw e. va o t sI ar e mt,i el e o ia h ss cnl a usn hc lo s sae a t at vlt k oe e e no h mb Jo eir a s saw mC tn t io nn ue eh r c ad rya lgi r us on oy e dnt f dae h e m nt ae hs pd n oS e ae drmp s ef tr bi N i iri d t nen s h‚Äêt gege F a nd e cnFw a obe p 4y urs a, ut po 0 sY i av s lf 0 ee r ea 0t ty dl h s a , . y e ,Ti d esr s h ol a e ua i wC rs tn s tm na y ,u in er aa t g s a nj c ho rt a od r,r o lo f ie d ft t u nr m u fgwa id r rhd i eia nl s i l crt g b ri ao atenn ch d o ka is e l n r s, S F p a rii ne r tr o ut e dt d aci bun lra r dcg ic e n uO k g re l if o ngrf us goF d owi tr hde ne ec or lur e i Sa s c peoc k rsk r . ii ne Tg a gr hi nns e Fda r el ebl sy f tr o iic vg rr aehe l,t .a st l eie g td h tit nt so g,ws oc fha fir fce ih rea a cw r re aa cy t khe eov rsuil g is shp ctir oti nt os sd ia drn eivd re em dao w an nas y it me br pas o. d rT tlh u ae c nky t e x xTxx x o ^^ ^ q = ‚âà22 2 u f ‚àö 8 i a== = n 8 .t 9 d i84 1 0o 4 0^ 6 t n 4h 2 :+ e + 6 l e8 4 n^ g2 th of the edge, we take the square root of both sides of the MMBench79.6 77.6 75.9 81.8 72.5 QBenchT
paying respects to ancestors, eating dumplings, etc. So, the length of the edge is approximately 8.944. MMBench-CN LLaVAW
Now, to find the angle at the vertex of the triangle, we can use the tangent SEEDImage
I l o Win o ft hna e id lnd ed a ai snt c ei cco toe tn ir m nst go pa a orf e n fir fie ea fc il d rsr eoa b cc yu rk adse ce rr kus d e, m rto so st , ih sc ce r y ser tma in lt lbo e wais a l ise d,f‚Äê eem a ls n ya t di pk v ri oen atg ca h tt ed im cre eo mv di sc up te s ohs i dces a aru e yl ,c . inh sT s oh ta mres us emd er cea p itng e ieo tr sf sn o . hrd m aa vn a ec n be c ar es ns na an er dd e atf ou h dfn e jt a c h a ct e ni eo gs nn i l td e. te .T oI h o n te p ht pt h ea o i sn as ng cite gae ln s et et h , io s e t f h 8 aa e .nn sg ia l den e g d ol ie v p ii d pn e o a d s i r tb eig y h t t ht h et e r a i la e nn n gg lg el te h i s is o 4 fe atq h nu e da l s t hit do ee t sh a idde ej l a e cn eg nt th t o GP OT- u4 rV s G Pe rm evin ioi- uP sr oo pen-sQ ouw re cn e V SL O- TP Alus
a o yt P C t ps p PI f w T
s
Hn oh o ti r or aeaa nog ei oa rte n uspt rs eu wn rti d rsh tst ed ru e 'C epi ii oi t sa fi n ton n r
o
ss lh si ir niec sfg
.
w. e og wadi aa a a tn T a. vn c msr C sn l
n
oe h a D nt oa , po ac i f s e ii clc uhu f lle rr ele u ya oy yl ee por e,e
x
p ', b uip f slpa as vn, p eerl l peo re e mi i kew go trlneo
l
t , ne sd est e gt e psp ah c onp th im p lsl n stow ea wcie e o ra d em nst i. ii oflp S t l s eoby wl ss fm p pe . do r t o.t aor ri i a gu t o T on mi swnhk o e
p
n ng sce g e d oa l l it aa ly y
b
w a psua F f nn la yu e a uc eu d oc , mb ks s
l,
l s f ah o t lee l oe rii a ii ro tn as ldt v rd in ete Sewe m ta e t rd
h
pfr ss l aor eo,
e r
l oh rm e p vruo ie ry na f a ee no wx ta gp d sc ro hdk p y, k rh p e p Fae r t ia ic, i t ce h el nlr n li ht eu os s sn se e bo tt ss .r tg ro sah a
i
u uo Te vb sfn te ca yhst
a
l iaotd ees oel ie lol nre un sy n crt e t gs p e oh t c ow pi b h al fnu
p
ue ea ouue lr g
r
s pstsr i rt esit t os s li t r e o et ec ‚Äêa, e e ,b
w
hh m m
t
lw t isn :c eo th roc aos i cm is eb iu
t
n su aa h a tmyl e yu eptn c e w u sf s nl u d dsa e b. er a, si m oa t a t f ta yrs fh l ns oe e no ehe to emd rdu odei n e sr s ai r g n d s o
f
di rp rot it n yso oba o o c p o e mg te od l olre at li s oes y wo rva of s ren r io tdd ef t p a P T t a M t f T h E D soo hh yahn n h ee a uog meae ted d ey lo mp ide n iip nr bnc pS satu y pl w ghy r op crges t leam f te lioe r Di inaR s tyim zr on hm p gb ufe i eo ont fg sea a s mtof i wsg hpy cl hl F atis pte ofpc e ee ee rso oie ls enc e am rn s . ip r ntr t o lrs aSw tast t i .f bt gv r ha ooe sit l sif a pi nme lr eoo an all m ogs nr d ni eA as ps st . doe i ly n ua en s wa nlp pon lce gl aoi ese re c lc s rl f o'o vess .t se sur t i fs sa oo o pavo t ni orn t e nr ef t ias t di ni c rtc m ylf ieh ti s irr as yr tee i te e t e ,t nai ouc r atp mdyr o ra sla a sa o bl tdshc nc v ei h,k s uoe c e l ee i oe rm n an r yr is nno ncs te r rgir l go eg eun id o r nmh sdt tsu ns et g hi ' ie n me n ea ao gg' dt msr rt bSeao b t p las ohv eru ri n e e nms ir o anc sa n gfc nep i gf i t e fe consl e ay Fm itt g n es my o eee nei sr tn i'n s tc tto ls yc e. io j Cvo re rF cn m a hoyy na lc l,e io .em s ne t ba te Tm ehr sni r,n hl se ii bano ees et en. gs yfs h sfo rdieeI int s a ln u y vmr .g h rn ip een it b sd i rh gc u t bes ia oe p nedsls ro gal ld ye y iaf oec .in nrg v td Ia i d stga efrr .iu t dae c aphm e la n sa te ss oodys r ,, a a Sst t Tt a aa o i on ndn nn ,g gef ( (( tl lia aa s e en h n nn d o e‚âà=g gg f t al ll a 2he ee t nh r 6e) )) c g e . = == t 5 la ea e 7n4 0o n q ag .p d(/ 5 tul 0p e e 8 a t.o g, 5 ht rs w i ) eeoit e en ve s e: c/ ra ta n ed xtj aa okc fee t n htht ee t rin iav ne grs lee it sa an pg pe rn ot x bT( i ea em lr
l
pc a mt rta ie nen l ty g
w e
e 2 dhn 6
a
bt . t5)
y
7 wo f d
i
lb le o gt rh e es. MMEPer 1712M .0MM 4U 2.0 Mat 5h 7V .6ista 60H .3al 5l 1B .2 MM-Vet
this program. MMECog530.7 72.5
QBenchT
The program will print a dictionary with three key‚Äêvalue pairs:
EatingdumplingsisalsoassociatedwiththelegendofNian,amonsterthatwould 'woodman':98, 'Alan':89, and 'Bobo':56.
‰∏ä ‰∏ã Ê®™ T " eph nriËÅî ËÅî Êâπ ts o irs e: : : t pr‰∏Ä ‰∏á Ë¥¢ cea on rÂ∏Ü ‰∫ã Ê∫ê o us u plaÈ£é Â¶Ç Âπø s lt ee b tÈ°∫ ÊÑè Ëøõ s .ust io ne" ss sm ."o To hth ehsa oi rli in zog na tah le ca hd a," ra" ce tev rer ry et ph ri en sg eng to se ts hesm tho eo mth ely, o" fa tn hd e a l a w T t ta h oht pr ht eepg oa ic se r uc So h ek tpa sf s rav e c iic dr ni ha al e el gesa r rt e jdg sFu, d ,e est tis t sh hn hte i ad c i em vlu yu afv er ad i l pi l .wn l wi lan aWg g ag yeye ht x ah ., iad nle emtu h imS mspep olp per mvli osi in n l e rlg oa tg af ogsF ne fte h trf ts si e rht l oli ee mv m lsd ea a eal. id n nw pT e yo r pi at l c rh od c eu tue ss idm ectfe o e rea vn m sat io nmst iN gs a aei a Ca n ysnn hdd sb, ie ntyv er evi al m sbel d ea ag ig n te s cie tg ut o rar i ln as n tb unsglw e g ra ep eso s. ou as ot o nl Wd rs c di uahp a inn der te e fd en ap d nmpa tN wa ir i tle i n i yia t a .s hna r, T Ado
fe
td
l e
e
d
rl ie ecI xt
t
w e
[ e‚Äò
cBi "s uB oh tbo it n‚Äôbo
]
go d " te
h
fl ire sot cme
o
B dtho eeb
,
o td
h
if c er to i dom icn td a ioric y nt , . ay ro yu
w
c ia lln
b
u ese
u
pt dhe
at
f eo dllo ww iti hn og
u
c to td he e: MMBen7 c9 h.6 MMBe7 n7 c.6
h-CN
7 S5 E.9 EDImage81. L8 LaVAW
key"Bobo". Qwen-VL-Chat LLaVA-1.5 Monkey
Free‚Äêform Text‚ÄêImage Composition Free‚ÄêformComprehension Ours InternLM-XC CogVLM-17B
Figure1.Overviewoffree-formtext-imagecompositionandcomprehensionofInternLM-XComposer2.OurmodelbasedonInternLM2-
7B[77]notonlysignificantlyoutperformsexistingmultimodalmodelsbutalsomatchesorevensurpassesGPT-4V[58]andGemini
Pro[76]incertainassessments.(Pleasezoom-intoseethedetails.)
Abstract ters exclusively to image tokens to preserve the integrity
of pre-trained language knowledge, striking a balance be-
We introduce InternLM-XComposer2, a cutting-edge tween precise vision understanding and text composition
vision-language model excelling in free-form text-image with literary talent. Experimental results demonstrate the
composition and comprehension. This model goes be- superiorityofInternLM-XComposer2basedonInternLM2-
yond conventional vision-language understanding, adeptly 7Binproducinghigh-qualitylong-textmulti-modalcontent
crafting interleaved text-image content from diverse in- and its exceptional vision-language understanding perfor-
puts like outlines, detailed textual specifications, and ref- mance across various benchmarks, where it not only sig-
erence images, enabling highly customizable content cre- nificantlyoutperformsexistingmultimodalmodelsbutalso
ation. InternLM-XComposer2 proposes a Partial LoRA matchesorevensurpassesGPT-4VandGeminiProincer-
(PLoRA) approach that applies additional LoRA parame- tainassessments.Thishighlightsitsremarkableproficiency
in the realm of multimodal understanding. The InternLM-
*indicatesequalcontribution.
1
4202
naJ
92
]VC.sc[
1v02461.1042:viXraXComposer2 model series with 7B parameters are pub- Thisinvolvesfeedingforwardimagetokenswithadditional
liclyavailableathttps://github.com/InternLM/ LoRA[33](Low-RankAdaptation)parameters, whilelan-
InternLM-XComposer. guage tokens retain the original architecture. This selec-
tiveenhancementensuresrobustperformanceinbothvisual
and textual domains. (2) High-quality and Diverse Data
1.Introduction Foundataion: Thequalityanddiversityofthetrainingdata
arepivotal. Ourdatasetforfree-formtext-imagecomposi-
In recent years, there has been a remarkable evolution in
tionexcelsin:adheringtocomplexinstructions,customiza-
the field of large language models (LLMs) [8, 16, 17, 57,
tion with text and image for tailored content, high-quality
58, 68, 79]. Foremost among these, models like Chat-
and stylistically diverse writing, and versatile text editing
GPT [57] have completely altered human interaction with
including condensing, expanding, and revising. For ex-
technology. Concurrently, avarietyofopen-sourceLLMs,
ceptional vision-language comprehension capabilities, we
suchasLlama[78],Mistra[37],InternLM[77],QWen[65],
gather a wide range of high-quality pretraining and super-
GLM [25], and Baichuan [7], have empowered the cus-
vised fine-tuning multimodal data. This collection spans
tomizationofLLMs. Buildingontheseopen-sourcefoun-
various aspects and types, such as captions, general QA,
dations, the community has seen substantial progress in
scientific QA, chat-based QA, mathematical QA, concept
multimodal large language models (MLLMs) [6, 21, 29,
knowledge,conversation,andtext-imagecomposition.
48, 49, 82, 95, 100]. These MLLMs are adept at in-
InternLM-XComposer2 surpasses existing benchmarks
terpreting images and engaging in text-image dialogues,
in both composition and comprehension. In the creation
showcasing impressive multimodal understanding. Unlike
benchmarkofOpenCompass[18]forevaluatingthecreativ-
traditional MLLMs, a recent innovation, i.e., InternLM-
ity of LLMs, InternLM-XComposer2 showcases outstand-
XComposer [95], has focused on using MLLMs for text-
ingperformance. Todemostrateourmultimodalcomphren-
imagecompositionandcomprehension,markinganoveldi-
sion capility, we compare our InternLM-XComposer2 on
rectioninMLLMresearch. However,thispioneeringwork
a list of benchmarks with both open-source MLLMs and
is currently limited to generating text-image articles based
closed-source APIs, e.g., GPT4V [58], Gemini Pro [76],
ontitlesalone,lackingthesophisticationtomeetmorecom-
and Qwen-VL Plus [19]. We report results in Math-
plexcompositionrequirements.Furthermore,whileachiev-
Vista [52], MMMU [91], AI2D [40], MME [27], MM-
ing leading performance at its inception, this model still
Bench [51], MMBench-Chinese [51], SEED-Bench (Im-
possesses significant potential for enhancement in detailed
age)[41],LLaVA-Bench(In-the-Wild)[49],QBench[85],
perception and complex reasoning capabilities to advance
MM-Vet [90], HallusionBench [31], ChartQA [56], and
itsvision-languagecomprehensionperformance.
POPE [45]. InternLM-XComposer2 based on InternLM2-
Thisobservationmotivatesthedevelopmentofmoread-
7Bsignificantlyexceedstheperformanceofexistingopen-
vancedvision-languagemodelscapableofpracticalandpo-
source models by an impressive margin. Remarkably, it
tenttext-imagecompositionandcomprehension. Inthispa-
demonstrates superior performance to GPT4V [58], Gem-
per, we introduce InternLM-XComposer2, a cutting-edge
iniPro[76]acrosssixbenchmarks.
model excelling in free-form text-image composition and
comprehension,builtbasedonInternLM2[77]. InternLM-
2.RelatedWorks
XComposer2representsasignificantadvancementoverits
predecessor,InternLM-XComposer[95],inbothtext-image Large Language Models (LLMs). Recent LLM archi-
composition and comprehension. InternLM-XComposer2 tectures have marked a transition from encoder-decoder
isadeptatproducinghigh-quality,integratedtext-imagear- frameworks (e.g., BERT [22], T5 [68]) to an emphasis on
ticles from a variety of free-form inputs, such as detailed decoder-only models employed with autoregressive train-
specifications, structured outlines, and reference images, ing techniques for next-token prediction (e.g., GPT [67]).
servingtoawiderangeofapplicationcontexts.Intherealm The following works (e.g., GPT3 [8], InstructGPT [60],
of multimodal understanding, it demonstrates exceptional ChatGPT [57], PaLM [17]) have seen the integration of
capabilities in detailed perception, logical reasoning, and advanced techniques such as instruction-tuning and Rein-
extensive knowledge integration. Its performance signifi- forcement Learning from Human Feedback (RLHF). Cou-
cantlysurpassesthatofexistingopen-sourceMLLMs,and pled with expansive parameter sizes and extensive train-
itstandsonparwith,orevenexceeds,advancedmodelslike ingdata,theseLLMmodelshaveachievedsubstantialper-
GPT-4V[58]andGeminiPro[76]invariousbenchmarks. formance enhancements across a diverse range of Nat-
The appealing capabilities of InternLM-XComposer2 ural Language Processing (NLP) tasks. Other notable
are primarily due to two critical design elements. (1) LLMs encompass a range of developments, such as the
Partial LoRA: The Partial LoRA (P-LoRA) design har- OPT [96], LLaMA series [78, 79], e.g., Mistral [37, 38],
monizes its abilities in composition and comprehension. InternLM [77], GLM series [25, 93], Qwen series [6, 65],
2Baichuan[7],Skywork[84]andFalcon[61]havemadesig-
nificantcontributionstothefield.
MultimodalLargeLanguageModels(MLLMs). Vision-
language models (VLMs), exemplified by CLIP [66] and
itssubsequentworks[26,36,43,44,50,75,94],alignim- ùëä ‚àà ‚Ñùùê∂ ùëúùë¢ùë°√óùê∂ ùëü
ùêµ Pretrained
age and text features in a unified embedding space. This
alignment is achieved through contrastive learning objec- Weights
tives applied to extensive image-text pair datasets. VLMs
achieve strong zero-shot and few-shot performance, show- ùëä ùê¥ ‚àà ‚Ñùùê∂ ùëü√óùê∂ ùëñùëõ ùëä 0 ‚àà ‚Ñùùê∂ ùëúùë¢ùë°√óùê∂ ùëñùëõ
casing significant generalization abilities across a range of
downstreamtasks.
Benefiting from existing large language models and
VLMsasthevisualencoder,recentMultimodalLargeLan-
guage Models (MLLMs) [12, 14, 15, 24, 28, 58] achieve
visual perception, understanding and reasoning abilities, Figure2. TheillustrationofthePartial-LoRA.Thebluetokens
showsuperbperformanceindiversevision-languagetasks. represent the visual tokens and the gray tokens are the language
tokens.OurPartial-LoRAisonlyappliedtothevisualtokens.
A series of studies [2, 5, 9, 10, 20, 21, 42, 46, 49, 62,
64, 80, 86, 92, 97, 98, 100] have explored further im-
This model boasts exceptional multi-lingual capabilities
prove the MLLM in different dimensions, such as instruc-
andhasdemonstratedimpressiveresultsinbenchmarks. In
tion tuning [11, 49, 98], efficient fine-tuning [33], high-
practical applications, we utilize the InternLM2-7B-Chat-
resolution image inputs [6, 82, 83], hallucination mitiga-
SFTvariantasourLLM.
tion[34,87,99],imagegeneration[23,30,74,89],3Dun-
Partial Low-Rank Adaptation. In the realm of multi-
derstanding [63] and image-text comprehension and com-
modal Language Learning Models (LLMs), one insuffi-
position[95].
cientlyexploredareaistheeffectivealignmentofdifferent
To enable highly customizable content creation, our
modalities. A desired alignment should potentially enrich
modelisdesignedforfree-formtext-imagecompositionand
the LLM with new modality-specific knowledge, while si-
comprehension based on MLLMs. We use Intern-LM2 as
multaneously preserving its inherent capabilities. Current
the LLM and CLIP ViT-Large as the visual encoder and
methodspredominantlyadoptoneoftwoapproaches: they
proposeanewpartialLoRAtoalignthetext-imagemodal-
either treat the visual token and language token equally or
ities. Given flexible and multi-modal user inputs such as
as entirely distinct entities. We contend that the first ap-
specifications, outlines, and reference images, our model
proachoverlookstheinherentpropertydistinctionsbetween
iscapableofgeneratinghigh-qualityinterleavedtext-image
modalities,whilethesecondapproachresultsinasubstan-
writtencontent.
tialalignmentcost.
Inourpursuitofeffectivemodalityalignment,weintro-
3.Method
duce Partial LoRA, a versatile plug-in module designed to
3.1.ModelArchitecture alignknowledgefromanewmodalitytotheLLM.Asillus-
tratedinFigureX,PartialLoRAdrawsinspirationfromthe
Ourproposedmodel,InternLM-XComposer2,incorporates
originalLoRAandincorporatesalow-rankadaptationthat
a vision encoder and a Language Learning Model (LLM).
isexclusivelyappliedtothenewmodalityportionofthein-
Thesetwocomponentsareinterconnectedviaaninnovative
put tokens. In our specific configuration, Partial LoRA is
Partial LoRA module. Given a set of images and text, the
appliedtoallvisualtokens.
LLM utilizes the output from the vision encoder as visual
Formally, for each linear layer L in the LLM blocks,
0
tokens and the tokenized text as language tokens. These we denote its weight matrix W
0
‚àà R(Cout√óCin) and bias
tokensarethenconcatenatedtoformtheinputsequence. B
0
‚àà RCout,whereC
in
andC
out
aretheinputandoutput
Vision Encoder. The vision encoder in our model is de-
dimension. Its corresponding Parital LoRA contains two
signed to extract high-level visual features from raw im- low-rank matrix W
A
‚àà RCr√óCin and W
B
‚àà RCout√óCr.
ages.Itispretrainedinanimage-languagecontrastiveman-
Withagiveninputx=[x ,x ],wehavetheoutputfeature
v t
ner(CLIP). Our findings indicate that, when used in con-
xÀÜby:
junction with our Partial LoRA module, a lightweight vi-
sionmodelperformseffectively. Forthesakeofefficiency, xÀÜ =W x +B
t 0 t 0
wehaveoptedtousetheOpenAIViT-Largemodel.
xÀÜ =W x +W W x +B
Large Language Model. We employ the recently intro- v 0 v B A v 0
xÀÜ=[xÀÜ ,xÀÜ ]
duced InternLM-2 as our Large Language Model (LLM). v t
3Task Dataset
GeneralSemanticAlignment ShareGPT4V-PT[11],COCO[13],Nocaps[1],TextCaps[73],LAION400M[69],SBU[59],CC3M[72]
WorldKnowledgeAlignment ConceptData[95]
VisionCapabilityEnhancement WanJuan[32],Flicker[88],MMC-Instruction[47]
Table1.DatasetsusedforPre-Training.Thedataarecollectedfromdiversesourcesforthethreeobjectives.
wherex v andx t arethevisualtokensandlanguagetokens Task Dataset
oftheinputsequencerespectively.
Multi-tasktraining
Caption ShareGPT4V[11],COCO[13],Nocaps[1]
3.2.Pre-Training GeneralQA VQAv2[4],GQA[35],OK-VQA[55]
ScienceQA AI2D[40],SQA[54]
During the pre-training phase, the LLM remains constant ChartQA DVQA[39],ChartQA[56]
while both the vision encoder and Partial LoRA are fine- MathQA MathQA[3],Geometry3K[53]
tuned to align the visual tokens with the LLM. The pre- WorldKnowledgeQA A-OKVQA[70],,KVQA[71]
Conversation LLaVA-150k[49],LVIS-Instruct4V[81]
trainingdataismeticulouslycuratedwiththreeobjectives
in mind: 1) general semantic alignment, 2) world knowl- Instructiontuning
Free-fromComposiiton In-housedata(RefertoSec.3.4)
edgealignment,3)visioncapabilityenhancement.
Conversation LLaVA-150k[49],LVIS-Instruct4V[81]
GeneralSemanticAlignment.Theobjectiveofgeneralse- ShareGPT-en&zh [16],InternLM-Chat[77]
manticalignmentistoequiptheMLLMwiththefundamen-
Table 2. Datasets used for Supervised Fine-Tuning. We collect
tal ability to comprehend image content. For instance, the
data from diverse sources to empower the model with different
MLLMshouldbeabletorecognizethatapictureofEinstein
capabilities.
represents‚Äòahuman‚Äô. Weutilizeimagecaptiondatafroma
variety of sources for this purpose, including high-quality,
detailedcaptionsfromShareGPT4V-PT,aswellasconcise
tionto490√ó490forimprovedperformance. ForthePartial
and precise captions from COCO, NoCaps, TextCaps, etc.
LoRA, we set a rank of 256 for all the linear layers in the
Duringthepre-trainingphase,weemployasimpleinstruc-
LLMdecoderblock. Ourtrainingprocessinvolvesabatch
tion: Describethisimagebriefly/indetail.
sizeof4906andspansacross2epochs. Thelearningrateis
World Knowledge Alignment. World knowledge repre- initiallysettoincreaseto2√ó10‚àí4withinthefirst1%ofthe
sents an advanced capability of the MLLM. For instance,
trainingsteps. Followingthis,itdecreasesto0accordingto
theMLLMshouldbeabletoidentifythemaninthefigure
acosinedecaystrategy.Topreservethepre-existingknowl-
mentionedaboveas‚ÄòAlbertEinstein‚Äôandfurthertalksome-
edgeofthevisionencoder, weapplyalayer-wiselearning
thingabouthim. Toaligntheworldknowledgedepictedin
rate (LLDR) decay strategy and the decay factor is set to
theimagewiththeknowledgealreadyacquiredbytheLLM,
0.90.
wehaveconstructedaconceptdataset. Thisdatasetiscare-
fully filtered from the concept data utilized in InternLM-
3.3.SupervisedFine-tuning
XComposer [95]. Given that the text in the concept data
only partially describes the content in the image and their The pre-training phase aligns the visual feature with the
relationshipiscomplextomodel,weemployamorebroad language, enabling the Language Learning Model (LLM)
instruction: Tellmesomethingaboutthisimage. tocomprehendthecontentoftheimages. However, itstill
Vision Capability Enhancement. Finally, an advanced lackstheabilitytoeffectivelyutilizetheimageinformation.
MLLM necessitates certain vision-specific capabilities, Toovercomethislimitation,weintroducearangeofvision-
such as Optical Character Recognition (OCR), object lo- languagetasksthatthemodelengagesinduringthesubse-
calization(grounding),andtheunderstandingofstructured quentSupervisedFine-TuningStage. Thisstagecomprises
images(e.g.,charts,tables). Toachievethis,wehavecom- two sequential steps: Multi-task Training and Free-form
piledrelevantdatasets,asoutlinedinTable.1,andhaveim- Text-ImageComposition.Duringthisstage,wejointlyfine-
plementedcorrespondinginstructionsfortraining. tunethevisionencoder,LLM,andPartialLoRA.
Thanks to the design of Partial LoRA, the LLM is able Multi-task Training. As delineated in Table 2, the multi-
toadapttovisualtokenswhilemaintainingitsoriginallan- task training dataset is assembled from various sources,
guageprocessingcapabilities. ThefixedLLMalsoenables aimingtoequipthemodelwithabroadspectrumofcapabil-
ustodirectlyusein-contextlearningperformanceasamea- ities. Eachtaskisstructuredasaconversationalinteraction,
sureofpre-trainingquality. andtheinstructionsareaugmentedwithGPT-4toenhance
In our implementation, we employ the OpenAI CLIP diversity. Concurrently, to maintain the original language
ViT-L-14-336asthevisionencoder. Weincreaseitsresolu- capability, we also incorporate the supervised fine-tuning
4Method MathVista AI2D MMMU MME MMB MMBCN SEEDI LLaVAW QBenchT MM-Vet HallB ChartVQA
Open-Source SPH-MOE Monkey Yi-VL WeMM L-Int2 L-Int2 SPH-2 CogVLM Int-XC CogVLM Monkey CogAgent
PreviousSOTA 8x7B 10B 34B 6B 20B 20B 17B 17B 8B 30B 10B 18B
42.3 72.6 45.9 2066.6 75.1 73.7 74.8 73.9 64.4 56.8 58.4 68.4
Closed-sourceAPI
GPT-4V 49.9 78.2 56.8 1926.5 77.0 74.4 69.1 93.1 74.1 67.7 65.8 78.5
Gemini-Pro 45.2 73.9 47.9 1933.3 73.6 74.3 70.7 79.9 70.6 64.3 63.9 74.1
QwenVL-Plus 43.3 75.9 46.5 2183.3 67.0 70.7 72.7 73.7 68.9 55.7 56.4 78.1
Ours 57.6 78.7 42.0 2242.7 79.6 77.6 75.9 81.8 72.5 51.2 60.3 72.6
Table3. Comparisonwithclosed-sourceAPIsandpreviousopen-sourceSOTAs. OurInternLM-XComposer2getsSOTAresultsin
6ofthe12benchmarkswithonly7Bparameters,showingcompetitiveresultswithcurrentclosed-sourceAPIsandpreviousopen-source
SOTAMLLMs.Thebestresultsareboldandthesecond-bestresultsareunderlined.
datafromInternLM2,whichconstitutesafixed10%ofthe modificationssuchasshortening,expanding,andrewriting.
totalSupervisedFine-Tuning(SFT)data. Complex Instruction Adherence. We also capture in-
Free-form Text-Image Composition. To further enhance stances of adhering to complex instructions to create con-
the model‚Äôs ability to follow instructions and compose tent that caters to diverse demands like titles and outlines,
free-form image-text content, we employ data from both encompassingbothtextandimage-basedcompositions.
pure-text conversation corpora and vision-language con- CustomizationwithMaterials. Ourcollectionextendsto
versations, as outlined in Table 2. The dataset for free- materials used for personalized content creation, covering
form image-text composition is constructed following the both text and images, enabling customizable and unique
methodologydetailedinSection3.4. contentcreationexperiences.
Inourapproach,wejointlytrainallthecomponentswith Thedistributionofdataacrossthefourdimensionsisap-
a batch size of 2048 over 3000 steps. Data from multiple proximately equal, with a ratio of approximately 1:1:1:1.
sourcesaresampledinaweightedmanner,withtheweights Our method follows previous work [95] to identify suit-
basedonthenumberofdatafromeachsource. Themaxi- able positions for image insertion after generating the text
mumlearningrateissetto5√ó10‚àí5,andeachcomponent content. A notable distinction in our approach is that
hasitsownuniquelearningstrategy.Forthevisionencoder, when users provide their own image materials, these im-
wesettheLayer-wiseLearningRateDecay(LLDR)to0.9, age materials are used for insertion instead of relying on
whichalignswiththepretrainingstrategy.FortheLLM,we retrievedimages[95]. Wealsoobservethathavingahigh-
employafixedlearningratescalefactorof0.2. Thisslows resolutionimageinputisnotessentialfortext-imagecom-
downtheupdateoftheLLM,achievingabalancebetween position. Therefore, following the pre-training phase, we
preserving its original capabilities and aligning it with vi- opttodown-sampletheimageinputresolutionto224x224
sionknowledge. duringtheSFTstageoffree-formtext-imagecomposition.
3.4.Free-formText-ImageComposition 4.Experiments
Free-formtext-imagecompositionreferstothecombination In this section, we validate the benchmark performance
oftextualcontentandvisualelementsinaflexibleandunre- of our InternLM-XComposer2 after the supervised fine-
strictivemanner. Ourmodelgeneratesinterleavedtextand tuning.
images, specifically customized to align with the text re-
4.1.MLLMBenchmarkresults.
quirementsprovidedbyusers,whichmayincludeelements
suchasatitle,outline,andwritingmaterial,andoptionally, In Table.3 and Table.4, we compare our InternLM-
anyvisualrequirementslikeimageresources. XComposer2 on a list of benchmarks with both SOTA
To facilitate free-form text-image composition, we col- open-source MLLMs and closed-source APIs. Here we
lectawiderangeofhigh-qualityanddiversein-housedata report results in MathVista[52], MMMU[91], AI2D[40],
acrossfourkeydimensions. Thesedimensionsencompass: MME Perception (MMEP) [27], MME Cognition
VariedWritingStyles. Ourdataspansamultitudeofwrit- (MMEC)[27], MMBench (MMB) [51], MMBench-
ingstyles,fromacademicpaperstosocialmediapostsand Chinese (MMBCN) [51], SEED-Bench Image Part
poems, ensuring a rich and diverse collection of text and (SEEDI)[41], LLaVA-Bench In-the-Wild (LLaVAW)
imagecontents. [49], QBench-Testset (QBenchT)[85], MM-Vet [90],
Flexible Text Editing. Our dataset includes extensive ex- HallusionBench(HallB)[31],ChartQA[56],POPE[45].
amples of text editing, encompassing a wide spectrum of Comparison with Closed-Source APIs. As shown in
5Method LLM MathVista MMMU MMEP MMEC MMB MMBCN SEEDI LLaVAW QBenchT MM-Vet HallB
BLIP-2 FLAN-T5 - 35.7 1,293.8 290.0 - - 46.4 38.1 - 22.4 -
InstructBLIP Vicuna-7B 25.3 30.6 - - 36.0 23.7 53.4 60.9 55.9 26.2 53.6
IDEFICS-80B LLaMA-65B 26.2 24.0 - - 54.5 38.1 52.0 56.9 - 39.7 46.1
Qwen-VL-Chat Qwen-7B 33.8 35.9 1,487.5 360.7 60.6 56.7 58.2 67.7 61.7 47.3 56.4
LLaVA Vicuna-7B 23.7 32.3 807.0 247.9 34.1 14.1 25.5 63.0 54.7 26.7 44.1
LLaVA-1.5 Vicuna-13B 26.1 36.4 1,531.3 295.4 67.7 63.6 68.2 70.7 61.4 35.4 46.7
ShareGPT4V Vicuna-7B 25.8 36.6 1,567.4 376.4 68.8 62.2 69.7 72.6 - 37.6 49.8
CogVLM-17B Vicuna-7B 34.7 37.3 - - 65.8 55.9 68.8 73.9 - 54.5 55.1
LLaVA-XTuner InernLM2-20B 24.6 39.4 - - 75.1 73.7 70.2 63.7 - 37.2 47.7
Monkey-10B Qwen-7B 34.8 40.7 1,522.4 401.4 72.4 67.5 68.9 33.5 - 33.0 58.4
InternLM-XC InernLM-7B 29.5 35.6 1,528.4 391.1 74.4 72.4 66.1 53.8 64.4 35.2 57.0
Ours InernLM2-7B 57.6 42.0 1,712.0 530.7 79.6 77.6 75.9 81.8 72.5 51.2 60.3
Table4. Comparisonwithopen-sourceSOTAmethods. InternLM-XComposer2outperformscompetitorsin10outof11benchmarks.
Thebestresultsareboldandthesecond-bestresultsareunderlined.
Method LLM POPE HallusionBench* w/oRef wRef
Method
Avg. C R UDF LC Avg. C R UDF LC
Closed-sourceAPI
GPT-4V - - 65.8 GPT-4 6.32 5.22 5.98 7.17 7.47 5.98 5.30 5.55 6.51 7.08
Gemini-Pro - - 63.9
QWen-72b-Chat 5.70 4.78 5.16 6.37 7.13 5.31 4.94 4.72 5.71 6.50
QwenVL-Plus - - 56.4
Yi-34b-Chat 6.03 4.91 5.68 6.79 7.35 5.71 5.03 5.22 6.18 6.87
Open-sourceMLLMs
Ours 6.24 5.11 6.12 7.03 7.45 5.90 5.21 5.76 6.27 6.93
InstructBLIP Vicuna-7B 78.9 53.6
IDEFICS-80B LLaMA-65B - 46.1
Table 6. Comparison on CreationBench [18]. We report the
Qwen-VL-Chat Qwen-7B - 56.4
results with and without the GPT-4 referenced answer. We re-
LLaVA Vicuna-7B 80.2 44.1
LLaVA-1.5 Vicuna-13B 85.9 46.7 porttheaveragescoreandothermetricsincludingCreativity(C),
InternLM-XC InernLM-7B - 57.0 Richness(R), User Demand Fulfillment (UDF), and Logical Co-
herence(LC).
Ours InernLM2-7B 87.7 60.3
crucialmetricintheevaluationofanMLLM.Inthisreport,
Table 5. Hallucination Evaluation on POPE and Hallu- we present the results obtained on both POPE and Hallu-
sionBench. IntenrLM-XComposer2 outperforms open-source sionBench. AsindicatedinTable.5,ourmodelachievesan
MLLMsandperformsonparwithclosed-sourceAPIs.*Weskip average F1-score of 87.7 across the three tracks of POPE,
thenon-visualquestions,followingthesettinginVLMEvalKit[18] setting a new state-of-the-art (SOTA) benchmark. In the
caseofHallusionBench, ourmodelsurpassestheaccuracy
Table.3,InternLM-XComposer2demonstratescompetitive- of all open-source models, establishing itself as the new
ness with Closed-Source APIs across numerous bench- SOTA.Furthermore,itoutperformstheclosed-sourceAPI,
marks. Forinstance, ourmodelachievesascoreof 57.6% QwenVL-Plus.
onMathVistaand78.9onAI2D,outperformingtheseAPIs
4.2.CreationBenchResults
by a significant margin. Meanwhile, despite having only
7Bparameters, ourmodelattainsaslightlyworsescoreof We use the CreationBench benchmark from OpenCom-
43.0%onthechallengingcollege-levelbenchmarkMMMU. pass [18] to assess the writing ability of our InternLM-
The strong performance can be attributed to the superb XComposer2. As shown in Table 6, the results indicate
knowledge acquired by the new InternLM2 LLM and the that our approach not only excels in overall creativity but
efficientPLoRAtrainingstrategy,whichenabledustoalign alsosignificantlyimprovesuponkeymetricsoverprevious
theLLMwithimagefeatureswhilepreservingitslanguage open-source LLMs. When compared without the GPT-4
capability. referenced answer, our method scored an impressive 6.24
ComparisonwithOpen-SourceModels. Wealsoconduct overall.EvenwhenevaluatedwiththeGPT-4reference,our
a comprehensive comparison with open-source MLLMs method maintained strong performance, achieving scores
under a similar model scale. As shown in Table.4, our that underscore its ability to generate responses with high
modelsignificantlyoutperformsexistingopen-sourcemod- levelsofcreativityandlogicalstructure,criticalforuseren-
els, achieving state-of-the-art results across all bench- gagementandsatisfactioninconversationalAIapplications.
marks. Notably, InternLM-XComposer2 is the first model
4.3.Qualitativeresults.
toachieveascoreexceeding1700ontheMME-Perception
benchmark. Furthermore, itattainedanaccuracyofnearly PleaserefertotheAppendixforourqualitativeresultsofthe
80%ontheMMBench. free-formimage-textcompositionsandmultimodalconver-
HallucinationEvaluation. Visualhallucinationservesasa sations.
65.Conclusion References
In this paper, we present InternLM-XComposer2, which [1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,
demonstrates its exceptional capabilities in the field of Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,
vision-language understanding and free-form text-image Stefan Lee, and Peter Anderson. Nocaps: Novel object
captioningatscale. InProceedingsoftheIEEE/CVFinter-
composition. Our proposed innovative Partial LoRA
nationalconferenceoncomputervision,pages8948‚Äì8957,
(PLoRA) approach, which applies additional LoRA pa-
2019. 4
rameters exclusively to image tokens, has proven effective
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
in preserving the integrity of pre-trained language knowl-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
edge while striking a balance between precise vision un- Mensch,KatieMillican,MalcolmReynolds,RomanRing,
derstanding and text composition with literary talent. Our ElizaRutherford,SerkanCabi,TengdaHan,ZhitaoGong,
model‚Äôs performance across various benchmarks not only SinaSamangooei,MarianneMonteiro,JacobMenick,Se-
significantly outperforms existing multimodal models but bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
also matches or even surpasses GPT-4V and Gemini Pro hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
in certain assessments, underscoring its remarkable profi- Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
Flamingo: avisuallanguagemodelforfew-shotlearning,
ciency in the realm of multimodal understanding. This
2022. 3
research opens up new possibilities for highly customiz-
[3] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-
ablecontentcreationandpavesthewayforfutureadvance-
Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
ments in the MLLM field. The potential applications of
Mathqa: Towards interpretable math word problem solv-
InternLM-XComposer2 are vast and exciting, promising a ing with operation-based formalisms. arXiv preprint
future where AI can understand and generate high-quality arXiv:1905.13319,2019. 4
long-textmulti-modalcontentwitheaseandprecision. [4] StanislawAntol,AishwaryaAgrawal,JiasenLu,Margaret
Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi
Parikh. Vqa: Visualquestionanswering. InInternational
ConferenceonComputerVision(ICCV),2015. 4
[5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,
Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan
Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon
Kornblith,PangWeiKoh,GabrielIlharco,MitchellWorts-
man, and Ludwig Schmidt. Openflamingo: An open-
sourceframeworkfortraininglargeautoregressivevision-
languagemodels. arXiv.org,2023. 3
[6] JinzeBai, ShuaiBai, ShushengYang, ShijieWang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-vl: A frontier large vision-language model
withversatileabilities. arXiv.org,2023. 2,3
[7] Baichuan. Baichuan2: Openlarge-scalelanguagemodels.
arXiv.org,2023. 2,3
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Languagemodelsarefew-shotlearners. AdvancesinNeu-
ral Information Processing Systems (NeurIPS), 33:1877‚Äì
1901,2020. 2
[9] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,
VikasChandra,YunyangXiong,andMohamedElhoseiny.
Minigpt-v2: large language model as a unified interface
for vision-language multi-task learning. arXiv preprint
arXiv:2310.09478,2023. 3
[10] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
FengZhu,andRuiZhao. Shikra: Unleashingmultimodal
llm‚Äôsreferentialdialoguemagic. arXiv.org,2023. 3
[11] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui
He, JiaqiWang, FengZhao, andDahuaLin. Sharegpt4v:
Improvinglargemulti-modalmodelswithbettercaptions.
arXivpreprintarXiv:2311.12793,2023. 3,4
7[12] XiChen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, 2023. 2,3
Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, [22] JacobDevlin,Ming-WeiChang,KentonLee,andKristina
Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak- Toutanova. Bert: Pre-trainingofdeepbidirectionaltrans-
eri,MostafaDehghani,DanielSalz,MarioLucic,Michael formersforlanguageunderstanding. arXiv.org,2018. 2
Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, [23] RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,Zheng
Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ge,JinrongYang,LiangZhao,JianjianSun,HongyuZhou,
Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng
Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Ma, and Li Yi. Dreamllm: Synergistic multimodal com-
Beyer, JulienAmelot, KentonLee, AndreasPeterSteiner, prehensionandcreation. arXivpreprintarXiv:2309.11499,
Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, 2023. 3
Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhos- [24] DannyDriess,FeiXia,MehdiS.M.Sajjadi,CoreyLynch,
seini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
RaduSoricut. Pali-x: Onscalingupamultilingualvision Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
andlanguagemodel,2023. 3 Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna worth,SergeyLevine,VincentVanhoucke,KarolHausman,
Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence MarcToussaint, KlausGreff, AndyZeng, IgorMordatch,
Zitnick.Microsoftcococaptions:Datacollectionandeval- andPeteFlorence. Palm-e: Anembodiedmultimodallan-
uationserver,2015. 4 guage model. In arXiv preprint arXiv:2303.03378, 2023.
[14] XiChen,XiaoWang,LucasBeyer,AlexanderKolesnikov, 3
Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebas- [25] ZhengxiaoDu,YujieQian,XiaoLiu,MingDing,Jiezhong
tian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Qiu, Zhilin Yang, and Jie Tang. Glm: General language
DanielSalz,XiXiong,DanielVlasic,FilipPavetic,Keran model pretraining with autoregressive blank infilling. In
Rong,TianliYu,DanielKeysers,XiaohuaZhai,andRadu Proceedingsofthe60thAnnualMeetingoftheAssociation
Soricut. Pali-3 vision language models: Smaller, faster, for Computational Linguistics (Volume 1: Long Papers),
stronger,2023. 3 pages320‚Äì335,2022. 2
[15] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio- [26] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,
vanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue
Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Cao. Eva: Exploringthelimitsofmaskedvisualrepresen-
Kolesnikov,JoanPuigcerver,NanDing,KeranRong,Has- tation learning at scale. In Proceedings of the IEEE/CVF
sanAkbari,GauravMishra,LintingXue,AshishThapliyal, Conference on Computer Vision and Pattern Recognition
James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, (CVPR),pages19358‚Äì19369,2023. 3
ChaoJia,BurcuKaragolAyan,CarlosRiquelme,Andreas [27] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Steiner,AneliaAngelova,XiaohuaZhai,NeilHoulsby,and Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-
RaduSoricut.Pali:Ajointly-scaledmultilinguallanguage- rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-
imagemodel,2023. 3 grong Ji. Mme: A comprehensive evaluation benchmark
[16] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,Zhang- for multimodal large language models. arXiv preprint
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, arXiv:2306.13394,2023. 2,5
Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and [28] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang,
EricP.Xing. Vicuna: Anopen-sourcechatbotimpressing Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang
gpt-4with90%*chatgptquality,March2023. 2,4 Shen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shao-
[17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, hui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hong-
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul sheng Li, and Xing Sun. A challenger to gpt-4v? early
Barham, Hyung Won Chung, Charles Sutton, Sebastian explorationsofgeminiinvisualexpertise. arXivpreprint
Gehrmann, et al. Palm: Scaling language modeling with arXiv:2312.12436,2023. 3
pathways. arXiv.org,2022. 2 [29] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
[18] OpenCompass Contributors. Opencompass: A univer- Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui He, Xi-
sal evaluation platform for foundation models. https: angyu Yue, Hongsheng Li, and Yu Jiao Qiao. Llama-
//github.com/open-compass/opencompass, adapter v2: Parameter-efficient visual instruction model.
2023. 2,6 ArXiv,abs/2304.15010,2023. 2
[19] QWen Contributors. Qwen-vl-plus. https:// [30] YuyingGe,YixiaoGe,ZiyunZeng,XintaoWang,andYing
huggingface.co/spaces/Qwen/Qwen-VL- Shan. Plantingaseedofvisioninlargelanguagemodel. 3
Plus,year=2023. 2 [31] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,
[20] XTuner Contributors. Xtuner: A toolkit for efficiently Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,
fine-tuningllm. https://github.com/InternLM/ FurongHuang,YaserYacoob,DineshManocha,andTianyi
xtuner,2023. 3 Zhou. Hallusionbench: An advanced diagnostic suite for
[21] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat entangledlanguagehallucination&visualillusioninlarge
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale vision-languagemodels,2023. 2,5
Fung, and Steven Hoi. Instructblip: Towards general- [32] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin
purpose vision-language models with instruction tuning, Wang, WeiLi, HangYan, JiaqiWang, andDaLin. Wan-
8juan: A comprehensive multimodal dataset for advancing Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang,
englishandchineselargemodels. ArXiv,abs/2308.10755, LuYuan, LeiZhang, Jenq-NengHwang, Kai-WeiChang,
2023. 4 andJianfengGao. Groundedlanguage-imagepre-training.
[33] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen- InProceedingsoftheIEEE/CVFConferenceonComputer
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu VisionandPatternRecognition(CVPR),2022. 3
Chen. LoRA:Low-rankadaptationoflargelanguagemod- [45] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXin
els. InInternationalConferenceonLearningRepresenta- Zhao,andJi-RongWen. Evaluatingobjecthallucinationin
tions,2022. 2,3 largevision-languagemodels,2023. 2,5
[34] QidongHuang,XiaoyiDong,PanZhang,BinWang,Con- [46] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,
ghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.
Nenghai Yu. Opera: Alleviating hallucination in multi- Monkey: Image resolution and text label are important
modal large language models via over-trust penalty and things for large multi-modal models. arXiv preprint
retrospection-allocation. arXivpreprintarXiv:2311.17911, arXiv:2311.06607,2023. 3
2023. 3 [47] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,
[35] DrewAHudsonandChristopherDManning. Gqa:Anew Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong
dataset for real-world visual reasoning and compositional Yu. Mmc: Advancing multimodal chart understand-
question answering. Conference on Computer Vision and ing with large-scale instruction tuning. arXiv preprint
PatternRecognition(CVPR),2019. 4 arXiv:2311.10774,2023. 4
[36] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana [48] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.
Parekh, HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, Improved baselines with visual instruction tuning. arXiv
and Tom Duerig. Scaling up visual and vision-language preprintarXiv:2310.03744,2023. 2
representationlearningwithnoisytextsupervision. InPro- [49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
ceedingsoftheInternationalConferenceonMachinelearn- Lee. Visualinstructiontuning. arXiv.org,2023. 2,3,4,5
ing(ICML),pages4904‚Äì4916.PMLR,2021. 3 [50] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
[37] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun
Chris Bamford, Devendra Singh Chaplot, Diego de las Zhu,etal. Groundingdino: Marryingdinowithgrounded
Casas,FlorianBressand,GiannaLengyel,GuillaumeLam- pre-trainingforopen-setobjectdetection. arXiv.org,2023.
ple, Lucile Saulnier, Le¬¥lio Renard Lavaud, Marie-Anne 3
Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, [51] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
Thomas Wang, Timothe¬¥e Lacroix, and William El Sayed. Songyang Zhnag, Wangbo Zhao, Yike Yuan, Jiaqi Wang,
Mistral7b,2023. 2 ConghuiHe, ZiweiLiu, KaiChen, andDahuaLin. Mm-
[38] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, bench: Is your multi-modal model an all-around player?
Arthur Mensch, Blanche Savary, Chris Bamford, Deven- arXiv:2307.06281,2023. 2,5
draSinghChaplot,DiegodelasCasas,EmmaBouHanna, [52] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-
FlorianBressand,etal. Mixtralofexperts. arXivpreprint yuanLi,HannanehHajishirzi,HaoCheng,Kai-WeiChang,
arXiv:2401.04088,2024. 2 Michel Galley, and Jianfeng Gao. Mathvista: Evaluating
[39] Kushal Kafle, Brian Price, Scott Cohen, and Christopher mathematicalreasoningoffoundationmodelsinvisualcon-
Kanan. Dvqa: Understandingdatavisualizationsviaques- texts. InInternationalConferenceonLearningRepresen-
tionanswering. InProceedingsoftheIEEEconferenceon tations(ICLR),2024. 2,5
computervisionandpatternrecognition,pages5648‚Äì5656, [53] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan
2018. 4 Huang,XiaodanLiang,andSong-ChunZhu. Inter-gps:In-
[40] AniruddhaKembhavi,MikeSalvato,EricKolve,Minjoon terpretablegeometryproblemsolvingwithformallanguage
Seo, HannanehHajishirzi, andAliFarhadi. Adiagramis andsymbolicreasoning.InThe59thAnnualMeetingofthe
worth a dozen images. In Computer Vision‚ÄìECCV 2016: AssociationforComputationalLinguistics(ACL),2021. 4
14th European Conference, Amsterdam, The Netherlands, [54] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-
October11‚Äì14,2016,Proceedings,PartIV14,pages235‚Äì WeiChang,Song-ChunZhu,OyvindTafjord,PeterClark,
251.Springer,2016. 2,4,5 and Ashwin Kalyan. Learn to explain: Multimodal rea-
[41] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix- soning via thought chains for science question answer-
iaoGe,andYingShan. Seed-bench: Benchmarkingmulti- ing. AdvancesinNeuralInformationProcessingSystems,
modalllmswithgenerativecomprehension,2023. 2,5 35:2507‚Äì2521,2022. 4
[42] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, [55] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
JingkangYang,andZiweiLiu.Otter:Amulti-modalmodel RoozbehMottaghi. Ok-vqa: Avisualquestionanswering
within-contextinstructiontuning. arXiv.org,2023. 3 benchmarkrequiringexternalknowledge. InProceedings
[43] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. oftheIEEE/cvfconferenceoncomputervisionandpattern
Blip: Bootstrapping language-image pre-training for uni- recognition,pages3195‚Äì3204,2019. 4
fiedvision-languageunderstandingandgeneration. InPro- [56] AhmedMasry, DoXuanLong, JiaQingTan, ShafiqJoty,
ceedingsoftheInternationalConferenceonMachinelearn- and Enamul Hoque. Chartqa: A benchmark for question
ing(ICML),pages12888‚Äì12900.PMLR,2022. 3 answering about charts with visual and logical reasoning.
[44] LiunianHaroldLi*,PengchuanZhang*,HaotianZhang*, arXivpreprintarXiv:2203.10244,2022. 2,4,5
9[57] OpenAI. Chatgpt. https://openai.com/blog/ Partha Pratim Talukdar. Kvqa: Knowledge-aware visual
chatgpt,2022. 2 questionanswering.InProceedingsoftheAAAIconference
[58] OpenAI. Gpt-4technicalreport,2023. 1,2,3 onartificialintelligence,2019. 4
[59] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. [72] PiyushSharma,NanDing,SebastianGoodman,andRadu
Im2text: Describing images using 1 million captioned Soricut. Conceptualcaptions:Acleaned,hypernymed,im-
photographs. In Neural Information Processing Systems agealt-textdatasetforautomaticimagecaptioning.InPro-
(NIPS),2011. 4 ceedingsofthe56thAnnualMeetingoftheAssociationfor
[60] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,Car- ComputationalLinguistics(Volume1:LongPapers),pages
roll Wainwright, Pamela Mishkin, Chong Zhang, Sand- 2556‚Äì2565,2018. 4
hini Agarwal, Katarina Slama, Alex Ray, et al. Training [73] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and
language models to follow instructions with human feed- AmanpreetSingh. Textcaps: adatasetforimagecaption-
back. AdvancesinNeuralInformationProcessingSystems ing with reading comprehension. In Computer Vision‚Äì
(NeurIPS),35:27730‚Äì27744,2022. 2 ECCV2020:16thEuropeanConference,Glasgow,UK,Au-
[61] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, gust23‚Äì28,2020,Proceedings,PartII16,pages742‚Äì758.
Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei- Springer,2020. 4
dli,BaptistePannier,EbtesamAlmazrouei,andJulienLau- [74] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong
nay. Therefinedwebdatasetforfalconllm:Outperforming Zhang,YuezeWang,HongchengGao,JingjingLiu,Tiejun
curatedcorporawithwebdata,andwebdataonly,2023. 3 Huang,andXinlongWang. Generativepretraininginmul-
[62] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan timodality. Jul2023. 3
Huang,ShumingMa,andFuruWei.Kosmos-2:Grounding [75] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang,
multimodallargelanguagemodelstotheworld. arXiv.org, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang.
2023. 3 Alpha-CLIP:Aclipmodelfocusingonwhereveryouwant.
[63] Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong arXivpreprintarXiv:2312.03818,2023. 3
Wu, Jiaqi Wang, Dahua Lin, and Hengshuang Zhao. [76] GeminiTeam. Gemini: Afamilyofhighlycapablemulti-
Gpt4point: Aunifiedframeworkforpoint-languageunder- modalmodels,2023. 1,2
standingandgeneration,2023. 3 [77] InternLMTeam. Internlm: Amultilinguallanguagemodel
[64] ZhangyangQi,YeFang,MengchenZhang,ZeyiSun,Tong with progressively enhanced capabilities. https://
Wu, Ziwei Liu, Dahua Lin, Jiaqi Wang, and Hengshuang github.com/InternLM/InternLM,2023. 1,2,4
Zhao. Gemini vs gpt-4v: A preliminary comparison and [78] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
combinationofvision-languagemodelsthroughqualitative Martinet, Marie-Anne Lachaux, Timothe¬¥e Lacroix, Bap-
cases,2023. 3 tiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar,
[65] Qwen.Introducingqwen-7b:Openfoundationandhuman- etal. Llama:Openandefficientfoundationlanguagemod-
alignedmodels(ofthestate-of-the-arts),2023. 2 els. arXiv.org,2023. 2
[66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya [79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
AmandaAskell,PamelaMishkin,JackClark,etal. Learn- Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
ingtransferablevisualmodelsfromnaturallanguagesuper- Llama 2: Open foundation and fine-tuned chat models,
vision. InProceedingsoftheInternationalConferenceon 2023. 2
Machinelearning(ICML),pages8748‚Äì8763.PMLR,2021. [80] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping
3 Zhong, PanZhang, XiaoyiDong, WeijiaLi, WeiLi, Jiaqi
[67] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Wang,etal.Vigc:Visualinstructiongenerationandcorrec-
Sutskever,etal.Improvinglanguageunderstandingbygen- tion. arXiv.org,2023. 3
erativepre-training. 2018. 2 [81] JunkeWang,LingchenMeng,ZejiaWeng,BoHe,Zuxuan
[68] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Wu, andYu-GangJiang. Toseeistobelieve: Prompting
Lee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi, gpt-4vfor bettervisual instructiontuning. arXivpreprint
and Peter J Liu. Exploring the limits of transfer learning arXiv:2311.07574,2023. 4
withaunifiedtext-to-texttransformer. JournalofMachine [82] WeihanWang,QingsongLv,WenmengYu,WenyiHong,Ji
LearningResearch(JMLR),21(1):5485‚Äì5551,2020. 2 Qi,YanWang,JunhuiJi,ZhuoyiYang,LeiZhao,Xixuan
[69] ChristophSchuhmann,RichardVencu,RomainBeaumont, Song,JiazhengXu,BinXu,JuanziLi,YuxiaoDong,Ming
RobertKaczmarczyk,ClaytonMullis,AarushKatta,Theo Ding,andJieTang. Cogvlm: Visualexpertforpretrained
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion- languagemodels,2023. 2,3
400m: Opendatasetofclip-filtered400millionimage-text [83] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao,
pairs. arXivpreprintarXiv:2111.02114,2021. 4 Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and
[70] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Xiangyu Zhang. Vary: Scaling up the vision vocab-
Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A ulary for large vision-language models. arXiv preprint
benchmark for visual question answering using world arXiv:2312.06109,2023. 3
knowledge. InEuropeanConferenceonComputerVision, [84] TianwenWei, LiangZhao, LichangZhang, BoZhu, Lijie
pages146‚Äì162.Springer,2022. 4 Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu¬®,
[71] Sanket Shah, Anand Mishra, Naganand Yadati, and RuiHu,etal. Skywork:Amoreopenbilingualfoundation
10model. arXivpreprintarXiv:2310.19341,2023. 3 35:36067‚Äì36080,2022. 3
[85] HaoningWu,ZichengZhang,ErliZhang,ChaofengChen, [95] PanZhang,XiaoyiDongBinWang,YuhangCao,ChaoXu,
LiangLiao,AnnanWang,ChunyiLi,WenxiuSun,Qiong LinkeOuyang,ZhiyuanZhao,ShuangruiDing,Songyang
Yan, Guangtao Zhai, et al. Q-bench: A benchmark for Zhang, Haodong Duan, Hang Yan, et al. Internlm-
general-purpose foundation models on low-level vision. xcomposer: A vision-language large model for advanced
arXivpreprintarXiv:2309.14181,2023. 2,5 text-imagecomprehensionandcomposition.arXivpreprint
[86] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan, arXiv:2309.15112,2023. 2,3,4,5
YiyangZhou,JunyangWang,AnwenHu,PengchengShi, [96] Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Yaya Shi, et al. mplug-owl: Modularization empowers Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
largelanguagemodelswithmultimodality.arXiv.org,2023. Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open
3 pre-trained transformer language models. arXiv preprint
[87] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao arXiv:2205.01068,2022. 2
Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, [97] HaozheZhao,ZefanCai,ShuzhengSi,XiaojianMa,Kaikai
and Enhong Chen. Woodpecker: Hallucination correc- An,LiangChen,ZixuanLiu,ShengWang,WenjuanHan,
tionformultimodallargelanguagemodels. arXivpreprint andBaobaoChang. Mmicl: Empoweringvision-language
arXiv:2310.16045,2023. 3 model with multi-modal in-context learning. arXiv.org,
[88] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock- 2023. 3
enmaier. From image descriptions to visual denotations: [98] Zhiyuan Zhao, Linke Ouyang, Bin Wang, Siyuan Huang,
New similarity metrics for semantic inference over event Pan Zhang, Xiaoyi Dong, Jiaqi Wang, and Conghui He.
descriptions. TransactionsoftheAssociationforComputa- Mllm-dataengine: An iterative refinement approach for
tionalLinguistics,2:67‚Äì78,2014. 4 mllm. arXiv.org,2023. 3
[89] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin [99] ZhiyuanZhao,BinWang,LinkeOuyang,XiaoyiDong,Ji-
Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh aqi Wang, and Conghui He. Beyond hallucinations: En-
Tang,BrianKarrer,ShellySheynin,CandaceRoss,Adam hancing lvlms through hallucination-aware direct prefer-
Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hov- enceoptimization.arXivpreprintarXiv:2311.16839,2023.
hannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen 3
Li, Susan Zhang, Gargi Ghosh, Yaniv Taigman, Maryam [100] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-
Armen Aghajanyan. Scaling autoregressive multi-modal language understanding with advanced large language
models:Pretrainingandinstructiontuning. 3 models. arXiv.org,2023. 2,3
[90] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
KevinLin,ZichengLiu,XinchaoWang,andLijuanWang.
Mm-vet: Evaluating large multimodal models for inte-
gratedcapabilities.arXivpreprintarXiv:2308.02490,2023.
2,5
[91] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,
Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,
Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin
Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu
Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and
Wenhu Chen. Mmmu: A massive multi-discipline multi-
modal understanding and reasoning benchmark for expert
agi. arXivpreprintarXiv:2311.16502,2023. 2,5
[92] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and
Chen Change Loy. Contextual object detection with
multimodal large language models. arXiv preprint
arXiv:2305.18279,2023. 3
[93] AohanZeng,XiaoLiu,ZhengxiaoDu,ZihanWang,Hanyu
Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,
XiaoXia,WengLamTam,ZixuanMa,YufeiXue,Jidong
Zhai, WenguangChen, ZhiyuanLiu, PengZhang, Yuxiao
Dong, and Jie Tang. GLM-130b: An open bilingual pre-
trained model. In The Eleventh International Conference
onLearningRepresentations(ICLR),2023. 2
[94] HaotianZhang,PengchuanZhang,XiaoweiHu,Yen-Chun
Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan,
Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying
localizationandvision-languageunderstanding. Advances
in Neural Information Processing Systems (NeurIPS),
11Please write a blog based on the title: French Pastries: A Sweet Indulgence
French Pastries: A Sweet Indulgence
Frenchpastriesaresomethingthateveryoneshouldindulge
in at least once in their lifetime. The delicate flavors and
beautiful presentation of these pastries make them the
perfecttreatforanyoccasion.Fromcroissantstomacarons,
√©clairstotarts,Frenchpastriescomeinavarietyofshapes
√âclairs
and sizes. These sweet delights are not only delicious but
What are Eclairs? √àclairs (French pronunciation: (ekl…õ Å))
also incredibly addictive. It's hard to resist just one pastry
are a type of pastry that originated in France and is still
when there are so many to choose from! Whether you're
widely consumed there. They are long, thin pastries that
looking for a quick breakfast on the go or a decadent
are filled with cream or pastry cream. The exterior has a
dessertafterdinner,Frenchpastriesaresuretosatisfyyour
crispy coating while the interior is soft. There are many
cravings.SowhynottreatyourselftosomeFrenchpastries
ways to make eclairs, but one thing remains constant‚Äî
today?Youwon'tregretit!
theyhaveanunforgettabletaste!
Tarts
Croissants
Tartsareopen-facedpiesthataretypicallyfilledwithfruit
The croissant is a crescent-shaped French pastry that has
or custard. They can be either sweet or savory, but the
becomeoneofthemostwell-knownFrenchpastriesinthe
sweet versions are definitely more popular in France.
world. This buttery, flaky pastry is created by laminating
TherearemanytypesoftartsinFrenchcuisine:appletart
dough with layers of butter; this process creates beautiful (tarte Tatin), almond tarTtips f(ort Vaisitirngt Sehanghaaimandine), chocolate tart
layers and flakiness when the croissant bakes. In France,
(tarte au chocolat), cheese tart (tarte aux fromages) and
croissantsare typically served forbreakfastor as a snack.
manyothers.
Croissants can be eaten plain or topped with sugar, jam,
Nutella, or other spreads. While croissants may seem like
an indulgent treat, they're actually relatively easy to make
at home. It's important to note that there are some
variations on how to make croissants, including sweeter
versions such as chocolate croissants and almond
croissants.
PainauChocolat
Pain au chocolat is a French pastry that consists of a
crescent-shaped roll filled with chocolate. It's similar to a
croissantbuthasarichchocolatefillingthataddsanextra
layer of decadence. The texture of the chocolate can be
anything from soft and creamy to hard and crunchy
dependingonpersonalpreference.Thebuttery,flakycrust
Macarons makesitadeliciouslyindulgenttreatforanytimeofday.
Macarons are small, colorful cookies that are made with
almond flour and filled with a variety of fillings such as In conclusion, French pastries are a sweet indulgence that
ganache,buttercream,orjam.Thedelicatetextureofthese should not be missed. Whether you are looking for
pastries make them perfect for pairing with coffee or tea. something light like a croissant or something more
FrenchMacaronsaretraditionallyassociatedwithParisian substantial like a tart, there is sure to be a French pastry
cafes, where they can be found in an array of flavors that will satisfy your cravings. So why not head to your
rangingfromclassictoexperimental. localbakeryandindulgeinsomeFrenchpastrygoodness?
Figure3.InterleavedcompositiongeneratedbyInternLM-XComposer2.
12Ê†πÊçÆ‰ª•‰∏ãÊ†áÈ¢òÔºö‚Äú‰∏≠ÂõΩÊ∞¥Â¢®ÁîªÔºöÊµÅÂä®ÁöÑËØóÊÑè‰∏é‰∏úÊñπÁæéÂ≠¶‚ÄùÔºåÂàõ‰ΩúÈïøÊñáÁ´†ÔºåÂ≠óÊï∞‰∏çÂ∞ë‰∫é800Â≠ó„ÄÇËØ∑ÁªìÂêà‰ª•
‰∏ãÊñáÊú¨Á¥†ÊùêÔºö
‚ÄúÊ∞¥Â¢®ÁîªÊòØÁî±Ê∞¥ÂíåÂ¢®Ë∞ÉÈÖçÊàê‰∏çÂêåÊ∑±ÊµÖÁöÑÂ¢®Ëâ≤ÊâÄÁîªÂá∫ÁöÑÁîªÔºåÊòØÁªòÁîªÁöÑ‰∏ÄÁßçÂΩ¢ÂºèÔºåÊõ¥Â§öÊó∂ÂÄôÔºåÊ∞¥Â¢®ÁîªË¢´ËßÜ‰∏∫
‰∏≠ÂõΩ‰º†ÁªüÁªòÁîªÔºå‰πüÂ∞±ÊòØÂõΩÁîªÁöÑ‰ª£Ë°®„ÄÇ‰πüÁß∞ÂõΩÁîªÔºå‰∏≠ÂõΩÁîª„ÄÇÂ¢®Ê∞¥ÁîªÊòØ‰∏≠ÂõΩ‰º†ÁªüÁîª‰πã‰∏Ä„ÄÇÂ¢®Ê∞¥ÊòØÂõΩÁîªÁöÑËµ∑Ê∫êÔºå
‰ª•Á¨îÂ¢®ËøêÁî®ÁöÑÊäÄÊ≥ïÂü∫Á°ÄÁîªÊàêÂ¢®Ê∞¥Áîª„ÄÇÁ∫øÊù°‰∏≠ÈîãÁ¨îÔºå‰æßÈîãÁ¨îÔºåÈ°∫ÈîãÂíåÈÄÜÈîãÔºåÁÇπÊüìÔºåÊì¶ÔºåÁ†¥Â¢®ÔºåÊã®Â¢®ÁöÑÊäÄÊ≥ï„ÄÇ
Â¢®‰∫éÊ∞¥ÁöÑÂèòÂåñÂàÜ‰∏∫‰∫îËâ≤„ÄÇÁîªÊàê‰ΩúÂìÅÔºåÈ¢òÊ¨æÔºåÁõñÁ´†„ÄÇÂ∞±ÊòØÂÆåÊï¥ÁöÑÂ¢®Ê∞¥Áîª‰ΩúÂìÅ„ÄÇ
Âü∫Êú¨ÁöÑÊ∞¥Â¢®ÁîªÔºå‰ªÖÊúâÊ∞¥‰∏éÂ¢®ÔºåÈªë‰∏éÁôΩËâ≤Ôºå‰ΩÜËøõÈò∂ÁöÑÊ∞¥Â¢®ÁîªÔºå‰πüÊúâÂ∑•Á¨îËä±È∏üÁîªÔºåËâ≤ÂΩ©Áº§Á∫∑„ÄÇÂêéËÄÖÊúâÊó∂‰πü
Áß∞‰∏∫ÂΩ©Â¢®Áîª„ÄÇÂú®‰∏≠ÂõΩÁîª‰∏≠Ôºå‰ª•‰∏≠ÂõΩÁîªÁâπÊúâÁöÑÊùêÊñô‰πã‰∏ÄÔºåÂ¢®‰∏∫‰∏ªË¶ÅÂéüÊñôÂä†‰ª•Ê∏ÖÊ∞¥ÁöÑÂ§öÂ∞ëÂºï‰∏∫ÊµìÂ¢®„ÄÅÊ∑°Â¢®„ÄÅÂπ≤
Â¢®„ÄÅÊπøÂ¢®„ÄÅÁÑ¶Â¢®Á≠âÔºåÁîªÂá∫‰∏çÂêåÊµìÊ∑°ÔºàÈªë„ÄÅÁôΩ„ÄÅÁÅ∞ÔºâÂ±ÇÊ¨°„ÄÇÂà´Êúâ‰∏ÄÁï™ÈüµÂë≥Áß∞‰∏∫‚ÄúÂ¢®Èüµ‚Äù„ÄÇËÄåÂΩ¢ÊàêÊ∞¥Â¢®‰∏∫‰∏ªÁöÑ
‰∏ÄÁßçÁªòÁîªÂΩ¢Âºè„ÄÇ‚Äù
‰∏≠ÂõΩÊ∞¥Â¢®ÁîªÔºöÊµÅÂä®ÁöÑËØóÊÑè‰∏é‰∏úÊñπÁæéÂ≠¶
ÊäÄÊ≥ï‰∏éË°®Áé∞Âäõ
‰∏≠ÂõΩÁöÑ‰º†ÁªüÁªòÁîªÔºåÁâπÂà´ÊòØÊ∞¥Â¢®ÁîªÔºåË¢´Ë™â‰∏∫‚ÄúÂ¢®Èüµ‰πãËàû‚ÄùÔºå‰ª•
Ê∞¥Â¢®ÁîªÁöÑÊäÄÊ≥ïÈùûÂ∏∏Â§öÊ†∑ÂåñÔºåÂ∏∏ËßÅÁöÑÊúâÊ≥ºÂ¢®„ÄÅÁ†¥Â¢®„ÄÅÁßØÂ¢®„ÄÅÂÆø
Á¨îÂ¢®Êå•Ê¥í„ÄÅÁ∫øÊù°‰∫§ÁªáÁöÑÂΩ¢ÂºèÔºåÂ±ïÁé∞‰∫Ü‰∏≠ÂõΩÁâπÊúâÁöÑÂÆ°ÁæéÊÉÖË∂£ÂíåÊñáÂåñ
Â¢®„ÄÅÊûØÂ¢®Á≠â„ÄÇËøô‰∫õÊäÄÊ≥ïÂêÑÊúâÁâπËâ≤ÔºåËÉΩÂ§ü‰∫ßÁîü‰∏çÂêåÁöÑÊïàÊûú„ÄÇ‰æãÂ¶ÇÔºå
ÂÜÖÊ∂µ„ÄÇÊ∞¥Â¢®ÁîªÊòØ‰∏ÄÁßçÁã¨ÁâπÁöÑËâ∫ÊúØÂΩ¢ÂºèÔºåÂÆÉ‰∏ç‰ªÖ‰ΩìÁé∞‰∫Ü‰∏≠ÂõΩ‰º†ÁªüÊñá
Ê≥ºÂ¢®ÈÄÇÁî®‰∫éË°®Áé∞Â±±Â≥¶Ëµ∑‰ºèÁöÑÂ§ßÊ∞îÁ£ÖÁ§¥ÔºõÁ†¥Â¢®ÂàôÈÄÇÂêàÊèèÁªòÊ†ëÊú®ÊûùÂè∂
ÂåñÁöÑÁ≤æÈ´ìÔºå‰πüÂΩ∞Êòæ‰∫Ü‰∏úÊñπÁöÑÁæéÂ≠¶ËßÇÂøµÂíåÂì≤Â≠¶ÊÄùÊÉ≥„ÄÇÂú®ËøôÁØáÊñáÁ´†‰∏≠Ôºå
ÁöÑÁπÅËåÇÁ∫∑ÊùÇÔºõÁßØÂ¢®ÂàôÂ§öÁî®‰∫éËê•ÈÄ†ÂéöÈáçÊ∑±ÈÇÉÁöÑÂ±±Ê∞¥Ê∞îÊ∞õ„ÄÇÊ≠§Â§ñÔºåÊ∞¥
Êàë‰ª¨Â∞ÜÊ∑±ÂÖ•Êé¢ËÆ®Ê∞¥Â¢®ÁîªÁöÑËâ∫ÊúØÁâπÁÇπÔºåÂèäÂÖ∂Âú®‰∏≠ÂõΩÊñáÂåñ‰∏≠ÁöÑÈáçË¶ÅÂú∞
Â¢®ÁîªËøòÂ∏∏Â∏∏ÁªìÂêàËØóËØçÊ≠åËµãÔºåÂΩ¢Êàê‰∏ÄÁßçËØóÊÉÖÁîªÊÑèÁöÑÂ¢ÉÁïå„ÄÇËøôÁßçËûçÂêà
‰Ωç„ÄÇ
‰∏ç‰ªÖÊèêÂçá‰∫ÜÁîª‰ΩúÁöÑÊÑèÂ¢ÉÔºå‰πüËÆ©Ê¨£ËµèËÄÖËÉΩÂ§üÊõ¥Â•ΩÂú∞È¢ÜÁï•Âà∞ÂÖ∂‰∏≠ÁöÑÊñá
ÂåñÂÜÖÊ∂µ„ÄÇ
Ëµ∑Ê∫ê‰∏éÂèëÂ±ï
Ê∞¥Â¢®ÁîªÁöÑÂéÜÂè≤ÂèØ‰ª•ËøΩÊ∫ØÂà∞‰∏≠ÂõΩÂè§‰ª£ÁöÑÊàòÂõΩÊó∂ÊúüÔºàÂÖ¨ÂÖÉÂâç475
Âπ¥‚ÄîÂÖ¨ÂÖÉÂâç221Âπ¥Ôºâ„ÄÇÂΩìÊó∂ÁöÑÁªòÁîª‰∏ªË¶ÅÊòØÁî®ÊØõÁ¨îÂú®Áª¢ÊàñÁ∫∏‰∏äÁªòÂà∂Ôºå ‰ª∑ÂÄº‰∏éÊÑè‰πâ
ËÄåÈ¢úÊñôÂàôÊòØ‰ªéÁüøÁâ©„ÄÅÊ§çÁâ©ÂíåÂä®Áâ©‰∏≠ÊèêÂèñ„ÄÇÈöèÁùÄÊó∂‰ª£ÁöÑÂèëÂ±ïÔºåÁªòÁîª Ê∞¥Â¢®Áîª‰Ωú‰∏∫‰∏≠ÂõΩ‰º†ÁªüÊñáÂåñÁöÑÁë∞ÂÆùÔºå‰∏ç‰ªÖÊòØ‰∏≠ÂõΩËâ∫ÊúØÁöÑ‰ª£Ë°®Ôºå
ÊäÄÊ≥ï‰∏çÊñ≠‰∏∞ÂØåÔºåÂÖ∂‰∏≠ÊúÄÈáçË¶ÅÁöÑË¥°ÁåÆ‰πã‰∏ÄÂ∞±ÊòØÂ¢®ÁöÑÂèëÁé∞Âíå‰ΩøÁî®„ÄÇÊçÆ Êõ¥ÊòØ‰∏ñÁïåÊñáÂåñËâ∫ÊúØÂÆùÂ∫ì‰∏≠ÁöÑ‰∏ÄÈ¢óÁíÄÁí®ÊòéÁè†„ÄÇÂÆÉÁöÑ‰ª∑ÂÄº‰∏ç‰ªÖ‰ªÖÂú®‰∫é
‰º†ÔºåÁß¶ÂßãÁöáÁªü‰∏ÄÂÖ≠ÂõΩÂêéÔºå‰∏ã‰ª§ÁÉßÊØÅ‰π¶Á±çÔºå‰ΩÜ‰∏Ä‰Ωç‰π¶ÁîüËóèËµ∑‰∫Ü‰∏ÄÊú¨ ÂÖ∂Áã¨ÁâπÁöÑËâ∫ÊúØÈ£éÊ†ºÂíåÊäÄÊúØÊâãÊÆµÔºåÊõ¥Âú®‰∫éÂÖ∂ÊâÄÊâøËΩΩÁöÑÊ∑±ÂéöÊñáÂåñÂ∫ïËï¥
ÁèçË¥µÁöÑÂè§‰π¶„ÄÇËøôÊú¨Âè§‰π¶ÂêéÊù•Ë¢´Êó†ÊÑè‰∏≠‰∏¢ÂºÉÔºåËêΩÂÖ•Ê∞¥‰∏≠Ë¢´Ê≥°Êπø„ÄÇÂΩì ÂíåÁ≤æÁ•ûÂÜÖÊ∂µ„ÄÇÈÄöËøáÊ¨£ËµèÊ∞¥Â¢®ÁîªÔºå‰∫∫‰ª¨‰∏ç‰ªÖÂèØ‰ª•ÊÑüÂèóÂà∞Ëâ∫ÊúØÂÆ∂ÂØπËá™
‰π¶ÁîüÊçûËµ∑‰π¶È°µÊó∂ÔºåÊÑèÂ§ñÂú∞ÂèëÁé∞‰∏äÈù¢ÁöÑÊñáÂ≠óÂπ∂Êú™Ë¢´Ê∞¥Á†¥ÂùèÔºåÂèçËÄåÊõ¥ ÁÑ∂ÁöÑÊÑüÊÇüÂíåÂØπÁîüÂëΩÁöÑÊÄùËÄÉÔºå‰πüËÉΩÂ§üÈ¢ÜÊÇüÂà∞‰∏≠ÂõΩÂì≤Â≠¶ÁöÑÊô∫ÊÖßÂíåÈÅìÂæ∑
Âä†Ê∏ÖÊô∞‰∫Ü„ÄÇËøô‰∏™ÊïÖ‰∫ãÂëäËØâÊàë‰ª¨ÔºåÊ∞¥‰∏éÂ¢®ÁöÑÊê≠ÈÖçÊòØÂàõÈÄ†Â•áËøπÁöÑÂÖ≥ÈîÆ„ÄÇ ÂáÜÂàô„ÄÇÂõ†Ê≠§Ôºå‰øùÊä§Âíå‰º†ÊâøÊ∞¥TÂ¢®ips foÁîªr VisitiËøông Sha‰∏ÄnghaiÂÆùË¥µÁöÑÊñáÂåñÈÅó‰∫ßÊòæÂæóÂ∞§‰∏∫Èáç
‰ªéÊ≠§‰ª•ÂêéÔºå‰∏≠ÂõΩ‰∫∫ÂºÄÂßãÂ∞ÜÊ∞¥ÂíåÂ¢®ÁªìÂêàËµ∑Êù•Âàõ‰ΩúÂá∫ÂÖ∑ÊúâÁã¨ÁâπÈüµÂë≥ÁöÑ Ë¶Å„ÄÇ
Áîª‰Ωú„ÄÇ
ÁªìËØ≠
Ê†∏ÂøÉÂÖÉÁ¥† Áªº‰∏äÊâÄËø∞ÔºåÊ∞¥Â¢®Áîª‰Ωú‰∏∫‰∏ÄÈó®Âè§ËÄÅÁöÑËâ∫ÊúØÂΩ¢ÂºèÔºå‰∏ç‰ªÖÂÖ∑ÊúâÁã¨Áâπ
ÁöÑÂÆ°Áæé‰ª∑ÂÄºÔºåÊõ¥ÊòØ‰∏≠ÂõΩ‰º†ÁªüÊñáÂåñÁöÑÈáçË¶ÅÁªÑÊàêÈÉ®ÂàÜ„ÄÇÂÆÉÁöÑÈ≠ÖÂäõ‰∏ç‰ªÖ
Ê∞¥Â¢®ÁîªÁöÑÊ†∏ÂøÉÂÖÉÁ¥†ÂåÖÊã¨Á¨îÊ≥ï„ÄÅÂ¢®Ëâ≤ÂíåÊûÑÂõæ„ÄÇÈ¶ñÂÖàÔºåÁ¨îÊ≥ïÂú®Ê∞¥
‰ΩìÁé∞Âú®ÊäÄËâ∫ÁöÑÈ´òË∂Ö‰∏äÔºåÊõ¥Âú®‰∫éÂÖ∂ËÉåÂêéÊâÄËï¥Âê´ÁöÑÊ∑±ÂàªÊÄùÊÉ≥ÂíåÊñáÂåñÂÜÖ
Â¢®Áîª‰∏≠Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ‰∏çÂêåÁöÑÁ¨îËß¶ÂíåÁ¨îÂäõËÉΩÂ§ü‰∫ßÁîü‰∏∞ÂØåÁöÑËßÜËßâÊïàÊûúÔºå
Ê∂µ„ÄÇËÆ©Êàë‰ª¨ÂÖ±ÂêåÂä™ÂäõÔºåËÆ©ËøôÈó®Âè§ËÄÅËÄåÂèàÈ≤úÊ¥ªÁöÑËâ∫ÊúØÂΩ¢ÂºèÂæó‰ª•‰º†Êâø
Â¶ÇÁ≤óÁä∑Ë±™ÊîæÁöÑÁö¥Êì¶ÔºåÁªÜËÖªÊüîÁæéÁöÑÁÇπÊüìÁ≠â„ÄÇÂÖ∂Ê¨°ÔºåÂ¢®Ëâ≤ÁöÑËøêÁî®‰πüÊòØ
Âπ∂ÂèëÊâ¨ÂÖâÂ§ß„ÄÇ
Ê∞¥Â¢®ÁîªÁöÑÁ≤æÈ´ìÊâÄÂú®„ÄÇÈÄöËøáË∞ÉËäÇÊ∞¥ÁöÑÂ§öÂ∞ëÔºåÂèØ‰ª•‰∫ßÁîüÊµìÊ∑°Âπ≤ÊπøÁöÑÂèò
ÂåñÔºåÂàõÈÄ†Âá∫Ê∑±ÈÇÉÁ•ûÁßòÁöÑÊ∞õÂõ¥„ÄÇÊúÄÂêéÔºåÊûÑÂõæÂàôÂÜ≥ÂÆö‰∫ÜÁîªÈù¢ÁöÑÊï¥‰ΩìÂ∏É
Â±ÄÂíåÊÑèÂ¢ÉË°®Ëææ„ÄÇÂêàÁêÜÁöÑÊûÑÂõæËÉΩÂ§üÂºïÂØºËßÇËÄÖÁöÑËßÜÁ∫øÊµÅÂä®ÔºåÂ¢ûÂº∫ÁîªÈù¢
ÁöÑËâ∫ÊúØÊÑüÊüìÂäõ„ÄÇ
Figure4.InterleavedcompositiongeneratedbyInternLM-XComposer2.
13Please write a English poetry based on
Rabindranath Tagore's classic poem: "If
you shed tears when you miss the sun, you
Please write a English poetry based on
also miss the stars.", no less than 10
Rabindranath Tagore's classic poem: "If
sentences.
you shed tears when you miss the sun, you
also miss the stars.", no less than 10
sentences.
Whenthesunsets,theskyturnstodark
Andthetearsthatyoushed,leaveamark
Themountainsrisemajestically,
Theirpeaksshroudedinmistandcloud,
Butifyoulookup,you'llseethestars Thevalleysbelowstretchfarandwide,
Twinklinginthenight,likepreciouspearls Atapestryofgreenandgold.
Thetearsthatyoushed,canblindyoursight Theriversflowwithgentlegrace,
Theircurrentscarvingthroughtheearth,
Andyoumaymissthebeautythat'sinsight
Carryinglifeandbeautytoall,
Butifyoulookup,you'llseethestars
Asymphonyofnature'sworth.
Theforestsstandinsolemngrandeur,
TheirleavesrustlinTipsg for Viisitning Shtanhghaei breeze,
Alivingtestamenttotime,
Shiningbright,likeabeaconfromafar Aplaceofpeaceandtranquility.
Sodon'tbesadwhenthesunisgone
Anddon'tshedtearsforwhat'sdone
Instead,lookupandseethestars
Thatlightupthenight,likeshiningstars
Thetearsthatyoushed,canbereplaced
Bythebeautyofthestars,thatyoucanembrace
Sodon'tmissthesun,butlookuptothestars
Soletusnotweepforwhatislost,
Andfindthebeautythat'swaiting,nearandfar.
Butcherishwhatremains,
Forlifeisfleeting,buttheworld,
Isaplaceofendlesswondersandcharms.
Figure5.InterleavedcompositiongeneratedbyInternLM-XComposer2.
14ËØ∑Ê†πÊçÆÁªôÂÆöÊ†áÈ¢òÔºö‚ÄúÁÜäÁå´ÔºöËá™ÁÑ∂ÁïåÁöÑÊ∏©Êüî‰ΩøËÄÖ‚ÄùÔºåÂÜô‰∏ÄÁØáÈïøÊñáÁ´†„ÄÇÈ¶ñÂÖàÔºåËØ¶ÁªÜ‰ªãÁªçÁÜäÁå´ÁöÑÂü∫Êú¨ÁâπÂæÅÔºåÂ¶Ç
Â§ñË≤åÔºå‰π†ÊÄßÁ≠â„ÄÇÁÑ∂ÂêéÔºåÊé¢ËÆ®ÁÜäÁå´ÁöÑËá™ÁÑ∂Ê†ñÊÅØÂú∞ÂèäÂÖ∂Èù¢‰∏¥ÁöÑÁéØÂ¢ÉÊåëÊàòÔºåÂåÖÊã¨Ê†ñÊÅØÂú∞‰∏ßÂ§±ÂíåÊ∞îÂÄôÂèòÂåñÁ≠âÈóÆÈ¢ò„ÄÇ
‰πãÂêéÔºåËØ¶ÁªÜËÆ®ËÆ∫ÁÜäÁå´ÁöÑ‰øùÊä§Â∑•‰ΩúÁé∞Áä∂ÔºåÂåÖÊã¨Âª∫Á´ãËá™ÁÑ∂‰øùÊä§Âå∫ÔºåÂ§ßÁÜäÁå´ÁöÑ‰∫∫Â∑•ÁπÅËÇ≤Á≠â„ÄÇÊúÄÂêéÔºåÂàÜÊûêÁÜäÁå´Âú®‰∏≠
ÂõΩÂíå‰∏ñÁïåËåÉÂõ¥ÂÜÖÁöÑÊñáÂåñÂΩ±Âìç„ÄÇÂ≠óÊï∞‰∏çÂ∞ë‰∫é800Â≠ó„ÄÇ
ÁÜäÁå´ÔºöËá™ÁÑ∂ÁïåÁöÑÊ∏©Êüî‰ΩøËÄÖ
Ê≠§Â§ñÔºåÊ∞îÂÄôÂèòÊöñ‰πüÊòØÂ§ßÁÜäÁå´ÁõÆÂâçÊâÄÈù¢‰∏¥ÁöÑÈáçË¶ÅÈóÆÈ¢ò‰πã‰∏Ä„ÄÇÊçÆ
Á†îÁ©∂ÔºåËá™20‰∏ñÁ∫™70Âπ¥‰ª£‰ª•Êù•ÔºåÊàëÂõΩ‰∏úÈÉ®Ê≤øÊµ∑Âú∞Âå∫Âá∫Áé∞‰∫ÜÊåÅÁª≠Â¢ûÊ∏©
Â§ßÁÜäÁå´ÔºåÁÜäÁßëÂì∫‰π≥Âä®Áâ©Ôºå‰ΩìËÇ•Â§¥Â§ß„ÄÅÂúÜËÑ∏ÂúÜËÄ≥ÔºåÂπºÂπ¥‰ΩìËâ≤‰∏∫
Áé∞Ë±°ÔºåËøôÁßçË∂ãÂäø‰ªçÂú®ÁªßÁª≠ÂèëÂ±ï„ÄÇÈ´òÊ∏©‰ºöÂØºËá¥Á´πÊûóÁîüÈïøÁºìÊÖ¢Ôºå‰ªéËÄå
ÁÅ∞ÁôΩËâ≤ÔºåÂõ†ÂÖ∂Áã¨Á´ãÁâπÂæÅÂíåÊÜ®ÊÄÅÂèØÊé¨ÁöÑÊ†∑Â≠êË¢´‰∫∫‰ª¨Ë™â‰∏∫‚ÄúÊ¥ªÂåñÁü≥‚Äù
Èôç‰Ωé‰∫ÜÁ´πÂ≠ê‰Ωú‰∏∫Â§ßÁÜäÁå´‰∏ªË¶ÅÈ£üÁâ©ÁöÑ‰æõÂ∫îÈáè„ÄÇÈô§Ê≠§‰πãÂ§ñÔºåÂ§èÂ≠£ÈôçÈõ®
Âíå‚Äú‰∏≠ÂõΩÂõΩÂÆù‚Äù„ÄÇ
ÈáèÂ¢ûÂ§öËøò‰ºöÈÄ†ÊàêÊ≥•Áü≥ÊµÅ„ÄÅÊªëÂù°Á≠âÁÅæÂÆ≥ÔºåËøõ‰∏ÄÊ≠•Á†¥ÂùèÂ§ßÁÜäÁå´ÁöÑÊ†ñÊÅØ
Âú∞„ÄÇËøô‰∫õÂõ†Á¥†ÂÖ±Âêå‰ΩúÁî®Ôºå‰ΩøÂæóÂ§ßÁÜäÁå´ÁöÑÁßçÁæ§Êï∞ÈáèÊó•ÁõäÂáèÂ∞ë„ÄÇ
‰ªéÂ§ñË°®Êù•ÁúãÔºåÂ§ßÁÜäÁå´‰ºº‰πéÊòØËá™ÁÑ∂ÁïåÊúÄ‚ÄúÊ∏©Êüî‚ÄùÁöÑÁîüÁâ©ÔºöÈªëÁôΩ
Áõ∏Èó¥ÁöÑËä±Á∫πËÆ©ÂÆÉÁöÑÂ§ñÂΩ¢ÁúãËµ∑Êù•Êõ¥ÂÉèÊòØ‰∏ÄÁßçÂç°ÈÄöÂΩ¢Ë±°ÔºõÂúÜÊªöÊªöÁöÑË∫´
Ë∫ØÂíåÂ§ßÂ§ßÁöÑÂ§¥ÈÉ®ËÆ©‰∫∫Âøç‰∏ç‰ΩèÊÉ≥Ë¶ÅÊä±‰∏ÄÊä±ÔºåÂç≥‰Ωø‰∏çÁÜüÊÇâÁöÑ‰∫∫‰πü‰ºöÂõ† ÁÜäÁå´ÁöÑ‰øùÊä§Áé∞Áä∂Â¶Ç‰ΩïÔºü
‰∏∫ÂÖ∂ÊÜ®ÊÄÅÂèØÊé¨ÁöÑÂ§ñË°®ËÄåÂøÉÁîüÂ•ΩÊÑü‚Ä¶‚Ä¶ ÈíàÂØπ‰∏äËø∞ÈóÆÈ¢òÔºåÊîøÂ∫úÂíå‰ºÅ‰∏öÁõ∏ÁªßÈááÂèñ‰∫Ü‰∏ÄÁ≥ªÂàóÊé™ÊñΩ‰øùÊä§Â§ßÁÜä
‰ΩÜÂ∞±ÊòØËøôÊ†∑‰∏ÄÁßçÊ∏©È°∫ÁöÑÁîüÁâ©ÔºåÂç¥‰πüÊ≠£Èù¢‰∏¥ÁùÄÁßçÁßçÂç±Êú∫‚Äî‚ÄîÊ†ñ Áå´ÂèäÂÖ∂Ê†ñÊÅØÂú∞„ÄÇ‰æãÂ¶ÇÔºåÂª∫Á´ãËá™ÁÑ∂‰øùÊä§Âå∫ÂíåÊ£ÆÊûóÂÖ¨Âõ≠ÔºåÈôêÂà∂‰∫∫Á±ªÊ¥ª
ÊÅØÂú∞‰∏ßÂ§±„ÄÅÊ∞îÂÄôÂèòÂåñÁ≠âÂõ†Á¥†ÈÉΩÂØπÂÖ∂ÁîüÂ≠òÊûÑÊàêÂ®ÅËÉÅ„ÄÇ Âä®ËåÉÂõ¥ÔºåÊîπÂñÑÂ§ßÁÜäÁå´ÁöÑÈ£üÁâ©Êù•Ê∫êÁ≠â„ÄÇÂêåÊó∂ÔºåËøòÈÄöËøáÁßëÂ≠¶ÁÆ°ÁêÜ„ÄÅÂêà
ÁêÜÂºÄÂèëÂà©Áî®Á≠âÊñπÂºèÊé®Âä®‰∫ÜÂΩìÂú∞ÁªèÊµéÂèëÂ±ï„ÄÇÂ∞ΩÁÆ°Â¶ÇÊ≠§ÔºåÁî±‰∫éÂéÜÂè≤Âéü
Âõ†ÂíåËá™ÁÑ∂ÁéØÂ¢ÉÁöÑÊÅ∂ÂåñÁ≠âÂéüÂõ†ÔºåÈáéÁîüÂ§ßÁÜäÁå´‰ªçÂ§Ñ‰∫éÊøíÂç±Áä∂ÊÄÅ„ÄÇÊ†πÊçÆ
ÂõΩÈôÖËá™ÁÑ∂‰øùÊä§ËÅîÁõüÊúÄÊñ∞ÂèëÂ∏ÉÁöÑÁ∫¢Ëâ≤ÂêçÂΩïÔºåÁõÆÂâçÂÖ®ÁêÉÁé∞Â≠òÁöÑÂ§ßÁÜäÁå´
Á∫¶Êúâ1864Âè™ÔºåÂÖ∂‰∏≠Á∫¶90%‰ª•‰∏ä‰∏∫‰∏≠ÂõΩÂ¢ÉÂÜÖÁöÑÂ§ßÁÜäÁå´„ÄÇ
ÁÜäÁå´ÁöÑËá™ÁÑ∂Ê†ñÊÅØÂú∞Âú®Âì™Ôºü
È¶ñÂÖàÊù•‰∫ÜËß£‰∏Ä‰∏ã‰ªÄ‰πàÊòØÂ§ßÁÜäÁå´ÁöÑËá™ÁÑ∂Ê†ñÊÅØÂú∞„ÄÇ‰∏ÄËà¨Êù•ËØ¥ÔºåÂ§ß
ÁÜäÁå´ÁîüÊ¥ªÂú®Êµ∑Êãî1000~3000Á±≥ÁöÑÈ´òÂ±±Ê∑±Ë∞∑‰∏≠Ôºå‰ª•Á´πÂ≠ê‰∏∫‰∏ªÈ£üÔºåÂñú
Èò¥ÊπøÔºåËÄêÂØíÂÜ∑ÁöÑÁîüÊÄÅÁéØÂ¢ÉÊòØÂÖ∂ÁîüÂ≠òÁöÑÂü∫Á°ÄÊù°‰ª∂„ÄÇÂú®ÊàëÂõΩË•øÂçóÂú∞Âå∫
Tips for Visiting Shanghai
ÁöÑÂõõÂ∑ùÁõÜÂú∞„ÄÅÈôïË•øÂçóÈÉ®ÂèäÁîòËÇÉÂçóÈÉ®ÁöÑÂ≤∑Â±±„ÄÅÈÇõÂ¥ÉÂ±±„ÄÅÂ§ßÂ∞èÂáâÂ±±„ÄÅ
Áß¶Â≤≠Á≠âÂú∞ÊòØÂ§ßÁÜäÁå´ÁöÑÂÖ∏ÂûãÊ†ñÊÅØÂú∞„ÄÇÊ≠§Â§ñÔºåÂú®ÊàëÂõΩÁöÑÊπñÂåóÁ•ûÂÜúÊû∂„ÄÅ
Èô§‰∫ÜÂØπÈáéÂ§ñÁéØÂ¢ÉËøõË°å‰øùÊä§Â§ñÔºå‰∫∫Â∑•ÁπÅËÇ≤‰πüÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÊâãÊÆµ„ÄÇ
ÊπñÂçóÂº†ÂÆ∂Áïå‰ª•ÂèäÊ≤≥Âçó‰ºèÁâõÂ±±Á≠âÂú∞ÁöÑ‰∫öÁÉ≠Â∏¶Â±±Âú∞‰πüÊúâÂèëÁé∞„ÄÇ
ÈÄöËøáÂÆûÊñΩÁπÅÊÆñËÆ°ÂàíÂíåÊïëÂä©È°πÁõÆÔºåÂ∏ÆÂä©Êõ¥Â§öÁöÑÂ§ßÁÜäÁå´Â≠òÊ¥ª‰∏ãÊù•„ÄÇÁõÆ
ÂâçÂõΩÂÜÖÂ∑≤ÊúâÂ§ö‰∏™Âä®Áâ©Âõ≠„ÄÅÁßëÁ†îÊú∫ÊûÑÂºÄÂ±ï‰∫ÜÂ§ßÁÜäÁå´ÁöÑ‰∫∫Â∑•È•≤ÂÖªÁ†îÁ©∂
Â∑•‰ΩúÔºåËÆ∏Â§öÈõåÊÄß‰∏™‰ΩìÂ∑≤Áªè‰∫ß‰∏ãÂπºÂ¥Ω„ÄÇËôΩÁÑ∂Â≠òÂú®‰∏ÄÂÆöÁöÑÊäÄÊúØÈöæÂ∫¶Âíå
È£éÈô©Ôºå‰ΩÜÊòØÈöèÁùÄÁªèÈ™åÁöÑÁßØÁ¥ØÂíåÊäÄÊúØËøõÊ≠•ÔºåÊú™Êù•ÊúâÊúõÂÆûÁé∞Êõ¥Â§öÁöÑÂ§ß
ÁÜäÁå´ÁπÅË°çÂêé‰ª£„ÄÇ
ÁÜäÁå´ÁöÑÊñáÂåñÂΩ±ÂìçÂäõÂ¶Ç‰Ωï‰ΩìÁé∞Ôºü
ÊúÄÂêéÊàë‰ª¨Êù•ËÅäËÅäÂ§ßÁÜäÁå´Âú®ÂõΩÈôÖ‰∏äÁöÑÊñáÂåñÂΩ±ÂìçÂäõ„ÄÇËøëÂπ¥Êù•ÔºåÈöè
ÁÜäÁå´ÊâÄÈù¢‰∏¥ÁöÑÁéØÂ¢ÉÊåëÊàò
ÁùÄÂ™í‰ΩìÊä•ÈÅìÂíåÊóÖÊ∏∏‰∏öÁöÑÂèëÂ±ïÔºåË∂äÊù•Ë∂äÂ§öÁöÑ‰∫∫‰∫ÜËß£Âà∞‰∫ÜËøô‰∏™ÂèØÁà±Âèà
Âú®ËøúÂè§Êó∂ÊúüÔºå‰∏≠ÂõΩÁöÑÂçóÊñπÂú∞Âå∫Â∞±ÁîüÊ¥ªÁùÄÂéüÂßãÂ§ßÁÜäÁå´Ôºà‰πüÂè´Âßã
ÁèçË¥µÁöÑÁâ©Áßç„ÄÇËÆ∏Â§ö‰∫∫ÁîöËá≥‰∏çËøú‰∏áÈáåÂâçÂæÄÂõõÂ∑ùÂçßÈæô„ÄÅÈõÖÂÆâÁ≠âÂú∞ÁöÑËá™
ÁÜäÁå´Ôºâ„ÄÇÂ§ßÁ∫¶Âú®Ë∑ù‰ªä800‰∏áÂπ¥ÂâçÔºåÂÖ®ÁêÉËøõÂÖ•ÂÜ∞ÊúüÊó∂‰ª£ÔºåÂú∞ÁêÉÂºÄÂßã
ÁÑ∂‰øùÊä§Âå∫ËßÇËµèÈáéÁîüÁöÑÁÜäÁå´„ÄÇËÄåÂú®ÂΩ±ËßÜ‰ΩúÂìÅ‰∏≠Ôºå„ÄäÂäüÂ§´ÁÜäÁå´„ÄãÁ≥ªÂàó
ÂèòÂÜ∑ÔºåÊ§çË¢´ÈÄêÊ∏êÂêë‰ΩéÁ∫¨Â∫¶ÊñπÂêëÁßªÂä®ÔºåËøô‰ΩøÂæóÂéüÊú¨ÁîüÊ¥ªÂú®È´òÂØíÁéØÂ¢É
ÁîµÂΩ±Êõ¥ÊòØÂ∞ÜÂ§ßÁÜäÁå´ÁöÑÂΩ¢Ë±°Êé®Âêë‰∫Ü‰∏Ä‰∏™ÂÖ®Êñ∞ÁöÑÈ´òÂ∫¶„ÄÇÊ≠§Â§ñÔºåÂêÑÁßç‰∏é
‰∏≠ÁöÑÂ§ßÁÜäÁå´Â§±Âéª‰∫ÜÈÄÇÂÆúÁöÑÁîüÊ¥ªÁéØÂ¢ÉÔºåÂõ†Ê≠§ÂÆÉ‰ª¨Ë¢´Ëø´ËøÅÁßªÂà∞Ê∏©ÊöñÁöÑ
Â§ßÁÜäÁå´Áõ∏ÂÖ≥ÁöÑË°çÁîüÂìÅ‰πüË∂äÊù•Ë∂äÂèóÂà∞Ê∂àË¥πËÄÖÁöÑÈùíÁùêÔºåÊØîÂ¶ÇÁé©ÂÖ∑„ÄÅÊñá
ÂçóÊñπÂú∞Âå∫„ÄÇÈöèÁùÄÂÜ∞Â∑ùËøõÈÄÄÁöÑÂèòÂåñÔºåÂ§ßÁÜäÁå´‰πüÂú®‰∏çÊñ≠Âú∞ÂØªÊâæÈÄÇÂêàËá™
ÂÖ∑„ÄÅÈ£üÂìÅÁ≠âÁ≠â„ÄÇÂèØ‰ª•ËØ¥ÔºåÂ§ßÁÜäÁå´Â∑≤ÁªèÊàê‰∏∫‰∏≠Â§ñ‰∫∫Ê∞ëÂèãÂ•Ω‰∫§ÂæÄÂíåÊñá
Â∑±ÁöÑÁîüÂ≠òÁéØÂ¢ÉÔºåÂÆÉ‰ª¨ÁöÑË∂≥ËøπÈÅçÂ∏ÉÂÖ®ÂõΩÂêÑÂú∞„ÄÇ
Âåñ‰∫§ÊµÅÁöÑÊ°•Ê¢Å‰πã‰∏Ä„ÄÇ
ÈöèÁùÄÁ¨¨ÂõõÁ∫™ÂÜ∞Â∑ùÊúüÁöÑÁªìÊùüÔºåÊ∞îÊ∏©ÂºÄÂßãÂõûÂçáÔºåÂåóÊñπÊ£ÆÊûóÈù¢ÁßØ‰∏ç ÊÄªËÄåË®Ä‰πãÔºå‰øùÊä§Â•ΩÂ§ßÁÜäÁå´Ëøô‰∏ÄÁèçË¥µÁâ©Áßç‰∏ç‰ªÖÂÖ≥‰πéÂà∞ÊàëÂõΩÁöÑÁîü
Êñ≠Êâ©Â§ßÔºåËøôÂ∞±‰∏∫Â§ßÁÜäÁå´Êèê‰æõ‰∫ÜÊõ¥Âä†ÂπøÈòîÁöÑÁîüÂ≠òÁ©∫Èó¥„ÄÇ‰∏çËøá‰∏éÊ≠§Âêå ÊÄÅÊñáÊòéÂª∫ËÆæÔºåÂêåÊó∂‰πüÂÖ∑ÊúâÂπøÊ≥õÁöÑÁ§æ‰ºöÊÑè‰πâÂíå‰ª∑ÂÄº„ÄÇÂ∏åÊúõ‰ªäÂêéËÉΩÂ§ü
Êó∂Ôºå‰∫∫Á±ªÁöÑÊ¥ªÂä®‰πüÂºÄÂßãÂΩ±ÂìçÂà∞Â§ßÁÜäÁå´ÁöÑÁîüÂ≠òÁéØÂ¢É„ÄÇÂ§ßËßÑÊ®°ÁöÑÈáá‰ºê ÊúâÊõ¥Â§öÁöÑÂäõÈáèÂä†ÂÖ•Âà∞ËøôÈ°π‰ºüÂ§ßÁöÑ‰∫ã‰∏ö‰∏≠Êù•ÔºåÂÖ±ÂêåÂä™ÂäõÔºå‰∏∫Áª¥Êä§‰∫∫
Ê£ÆÊûóÂØºËá¥Ê†ñÊÅØÂú∞Èù¢ÁßØÂ§ßÂπÖÁº©Â∞èÔºåÊ∞¥ÂúüÊµÅÂ§±Âä†ÂâßÔºåËøô‰∫õÈÉΩÁõ¥Êé•ÊàñÈó¥ Á±ªÂÖ±ÂêåÁöÑÂÆ∂Âõ≠Ë¥°ÁåÆ‰∏Ä‰ªΩÂäõÈáèÔºÅ
Êé•Âú∞ÂΩ±ÂìçÂà∞ÈáéÁîüÂ§ßÁÜäÁå´ÁöÑÊï∞Èáè„ÄÇ
Figure6.InterleavedcompositiongeneratedbyInternLM-XComposer2.
15Ê†πÊçÆ‰ª•‰∏ãÊ†áÈ¢òÔºö‚ÄúËä≠ËïæËàûÔºö‰ºòÈõÖ‰∏éÂäõÈáèÁöÑËàûËπàËâ∫ÊúØ‚ÄùÔºåÂàõ‰ΩúÈïøÊñáÁ´†ÔºåÂÖ®Èù¢Êé¢ËÆ®Ëä≠ËïæËàûËøô‰∏Ä‰ºòÈõÖËÄåÂØåÊúâÂäõÈáè
ÁöÑËàûËπàËâ∫ÊúØÂΩ¢ÂºèÔºåÂ≠óÊï∞‰∏çÂ∞ë‰∫é800Â≠ó„ÄÇ‰∏ªË¶ÅÂÜÖÂÆπÂ¶Ç‰∏ãÔºö1„ÄÅ‰ªãÁªçËä≠ËïæËàûÁöÑÂéÜÂè≤Ëµ∑Ê∫êÂíåÂú®‰∏ñÁïåËàûËπàËâ∫ÊúØ‰∏≠ÁöÑÂú∞
‰Ωç„ÄÇ2„ÄÅÊé¢ËÆ®Ëä≠ËïæËàû‰ªéÊñáËâ∫Â§çÂÖ¥Êó∂ÊúüÂà∞Áé∞‰ª£ÁöÑÂèëÂ±ïËøáÁ®ãÔºåÂàÜÊûê‰∏çÂêåÊó∂ÊúüÂíåÂõΩÂÆ∂ÁöÑËä≠ËïæËàûÈ£éÊ†ºÂèäÂÖ∂ÂΩ±Âìç„ÄÇ3„ÄÅ
ÊèèËø∞Ëä≠ËïæËàûÁöÑÊäÄÊúØÂíåÂä®‰ΩúËØ≠Ë®ÄÔºå‰ª•ÂèäÂÖ∂Âú®Ë°®Áé∞ÂäõÂíåÊÉÖÊÑü‰º†Ëææ‰∏äÁöÑÁâπÁÇπ„ÄÇ4„ÄÅ‰ªãÁªç‰∏Ä‰∫õÁªèÂÖ∏ÁöÑËä≠ËïæËàûÂâßÁõÆÂíåËëó
ÂêçËàûËÄÖÔºåÂàÜÊûêÂÆÉ‰ª¨ÂØπËä≠ËïæËàûËâ∫ÊúØÁöÑË¥°ÁåÆ„ÄÇ5„ÄÅÊúÄÂêéÊÄªÁªìËä≠ËïæËàû‰Ωú‰∏∫Ëâ∫ÊúØÂΩ¢ÂºèÁöÑÁæéÂ≠¶ÂíåÊñáÂåñ‰ª∑ÂÄºÔºåÂπ∂ÂØπÂÖ∂Êú™Êù•
ÁöÑÂèëÂ±ïË∂ãÂäøËøõË°åÂ±ïÊúõ„ÄÇ
Ëä≠ËïæËàûÔºö‰ºòÈõÖ‰∏éÂäõÈáèÁöÑËàûËπàËâ∫ÊúØ ÊÄª‰πãÔºåËä≠ËïæËàûÊòØ‰∏ÄÈó®ÂéÜÂè≤ÊÇ†‰πÖ„ÄÅÊäÄÊúØÁ≤æÊπõ„ÄÅÂÜÖÊ∂µ‰∏∞ÂØåÁöÑËàûËπà
Ëâ∫ÊúØÂΩ¢Âºè„ÄÇÂÆÉ‰∏ç‰ªÖÂ±ïÁ§∫‰∫Ü‰∫∫Á±ªÁöÑË∫´‰Ωì‰πãÁæéÔºåÊõ¥Ëï¥Âê´ÁùÄÊ∑±ÂàªÁöÑÊñáÂåñ
ËàûËπàÔºåÊòØ‰∏ÄÁßç‰ºòÈõÖÁöÑËâ∫ÊúØ„ÄÇËä≠ËïæËàûÔºåÊõ¥ÊòØÂÖ∂‰∏≠ÊúÄ‰ºòÈõÖÁöÑ‰∏ÄÁßç ÂÜÖÊ∂µÂíåÂì≤Â≠¶ÊÄùËÄÉ„ÄÇ
Ëâ∫ÊúØÂΩ¢Âºè„ÄÇ
ÊäÄÊúØ‰∏éÊÉÖÊÑü
Ëµ∑Ê∫ê‰∏éÂèëÂ±ï Ëä≠ËïæËàûÁöÑÊäÄÊúØÈùûÂ∏∏ÈáçË¶ÅÔºåÂõ†‰∏∫Âè™ÊúâÊéåÊè°‰∫ÜÊ≠£Á°ÆÁöÑÊäÄÂ∑ßÊâçËÉΩËÆ©
Âú®ËøúÂè§Êó∂ÊúüÔºå‰∏≠ÂõΩÁöÑÂçóÊñπÂú∞Âå∫Â∞±ÁîüÊ¥ªÁùÄÂéüÂßãÂ§ßÁÜäÁå´Ôºà‰πüÂè´Âßã ËàûËπàÁúãËµ∑Êù•Êõ¥Âä†‰ºòÁæé„ÄÅÊµÅÁïÖ„ÄÇÂú®Ëä≠ËïæËàû‰∏≠ÔºåÊºîÂëòÈúÄË¶ÅÁî®ËÑöÂ∞ñÊù•ÊîØ
ÁÜäÁå´Ôºâ„ÄÇÂ§ßÁ∫¶Âú®Ë∑ù‰ªä800‰∏áÂπ¥ÂâçÔºåÂÖ®ÁêÉËøõÂÖ•ÂÜ∞ÊúüÊó∂‰ª£ÔºåÂú∞ÁêÉÂºÄÂßã ÊíëË∫´‰ΩìÁöÑÈáçÈáèÔºåËøôÈúÄË¶ÅÊûÅÈ´òÁöÑÂπ≥Ë°°ËÉΩÂäõÂíåÊüîÈüßÊÄß„ÄÇÊ≠§Â§ñÔºåËä≠ËïæËàû
ÂèòÂÜ∑ÔºåÊ§çË¢´ÈÄêÊ∏êÂêë‰ΩéÁ∫¨Â∫¶ÊñπÂêëÁßªÂä®ÔºåËøô‰ΩøÂæóÂéüÊú¨ÁîüÊ¥ªÂú®È´òÂØíÁéØÂ¢É ËøòÊ∂âÂèäÂà∞Ë∑≥Ë∑É„ÄÅÊóãËΩ¨Á≠âÂêÑÁßçÈ´òÈöæÂ∫¶Âä®‰ΩúÔºåÈÉΩÈúÄË¶ÅÂº∫Â§ßÁöÑÂäõÈáèÂíåËÄê
‰∏≠ÁöÑÂ§ßÁÜäÁå´Â§±Âéª‰∫ÜÈÄÇÂÆúÁöÑÁîüÊ¥ªÁéØÂ¢ÉÔºåÂõ†Ê≠§ÂÆÉ‰ª¨Ë¢´Ëø´ËøÅÁßªÂà∞Ê∏©ÊöñÁöÑ ÂäõÊù•ÊîØÊíë„ÄÇÂõ†Ê≠§ÔºåË¶ÅÊÉ≥Êàê‰∏∫‰∏ÄÂêç‰ºòÁßÄÁöÑËä≠ËïæËàûÊºîÂëòÔºåÂøÖÈ°ªÁªèËøáÈïø
ÂçóÊñπÂú∞Âå∫„ÄÇÈöèÁùÄÂÜ∞Â∑ùËøõÈÄÄÁöÑÂèòÂåñÔºåÂ§ßÁÜäÁå´‰πüÂú®‰∏çÊñ≠Âú∞ÂØªÊâæÈÄÇÂêàËá™ Êó∂Èó¥ÁöÑËÆ≠ÁªÉÂíåÁ£®Á†∫„ÄÇ
Â∑±ÁöÑÁîüÂ≠òÁéØÂ¢ÉÔºåÂÆÉ‰ª¨ÁöÑË∂≥ËøπÈÅçÂ∏ÉÂÖ®ÂõΩÂêÑÂú∞„ÄÇ ËôΩÁÑ∂Ëä≠ËïæËàûÁöÑÊäÄÊúØÂçÅÂàÜÈáçË¶ÅÔºå‰ΩÜËä≠ËïæËàûÁöÑÁÅµÈ≠ÇÂç¥ÊòØÊÉÖÊÑü„ÄÇÈÄö
ËøáËàûËπàÂä®‰ΩúÁöÑ‰º†ÈÄíÔºåËä≠ËïæËàûËÉΩÂ§üË°®ËææÂá∫ÂêÑÁßçÂ§çÊùÇÁöÑÊÉÖÊÑüÂíåÊÄùÊÉ≥„ÄÇ
ÊØîÂ¶ÇÔºåÂú®Âè§ÂÖ∏Ëä≠ËïæËàûÂâß‰∏≠ÔºåÂ∏∏Â∏∏‰ºöÂá∫Áé∞ÊÇ≤Ê¨¢Á¶ªÂêàÁöÑÊÉÖËäÇÔºåËÄåËøô‰∫õ
ÊÉÖËäÇÂæÄÂæÄÊòØÈÄöËøáÁªÜËÖªÁöÑËàûËπàÂä®‰ΩúÊù•ÂëàÁé∞ÁöÑ„ÄÇÂú®Ëøô‰∫õ‰ΩúÂìÅ‰∏≠ÔºåÊºîÂëò
ÈúÄË¶ÅÈÄöËøáË°®ÊÉÖ„ÄÅÊâãÂäøÁ≠âÂ§öÁßçÊñπÂºèÊù•‰º†ËææËßíËâ≤ÁöÑÂÜÖÂøÉ‰∏ñÁïåÔºå‰ªéËÄå‰Ωø
ËßÇ‰ºóÊõ¥Â•ΩÂú∞ÁêÜËß£ÂâßÊÉÖÂíåËßíËâ≤ÊÄßÊ†º„ÄÇÈô§Ê≠§‰πãÂ§ñÔºåÁé∞‰ª£Ëä≠ËïæËàû‰πüË∂äÊù•
Ë∂äÊ≥®ÈáçÂØπÁ§æ‰ºöÁé∞ÂÆûÈóÆÈ¢òÁöÑÂÖ≥Ê≥®ÂíåÊé¢ËÆ®ÔºåÁî®ËàûËπàÊù•ÂëºÂêÅ‰∫∫‰ª¨ÂØπÊüê‰∫õ
ÈóÆÈ¢òÁöÑÂÖ≥Ê≥®ÂíåÂèçÊÄù„ÄÇ
Ëä≠ËïæËàûÁöÑÊäÄÂ∑ßË¶ÅÊ±ÇÈùûÂ∏∏È´òÔºåÂä®‰ΩúÈúÄË¶ÅÈùûÂ∏∏Á≤æÂáÜ„ÄÅÊµÅÁïÖÔºåËÄå‰∏î
ÂäõÂ∫¶Ë¶ÅÊéåÊè°ÂæóÊÅ∞Âà∞Â•ΩÂ§Ñ„ÄÇÊ≠§Â§ñÔºåËä≠ËïæËàûÁöÑÂä®‰ΩúËØ≠Ë®Ä‰πüÈùûÂ∏∏‰∏∞ÂØåÔºå
ËÉΩÂ§üË°®ËææÂá∫ÂæàÂ§öÂ§çÊùÇÁöÑÊÉÖÊÑüÂíåÊÑè‰πâ„ÄÇÊ≠£Âõ†‰∏∫Â¶ÇÊ≠§ÔºåËä≠ËïæËàûË¢´ËÆ§‰∏∫
ÊòØÊúÄÂÖ∑Ë°®Áé∞ÂäõÁöÑËàûËπà‰πã‰∏Ä„ÄÇ
Ëä≠ËïæËàûÁöÑÂéÜÂè≤ÂèØ‰ª•ËøΩÊ∫ØÂà∞ÂçÅ‰∫î‰∏ñÁ∫™ÔºåÂΩìÊó∂‰∫∫‰ª¨Âú®Â∫ÜÁ•ùËÉúÂà©Êó∂
‰ºöË∑≥ËàûÔºåËøôÁßçËàûËπàË¢´Áß∞‰∏∫‚ÄúÂç°ÊñØËíÇÂà©‰∫öËàû‚ÄùÔºàCastilian
danceÔºâ„ÄÇÈöèÁùÄÊó∂‰ª£ÁöÑÂèëÂ±ïÔºå‰∫∫‰ª¨ÂºÄÂßãÂ∞ÜÈü≥‰πê‰∏éËàûËπàÁªìÂêàËµ∑Êù•Ôºå ÊÄª‰πãÔºåËä≠ËïæËàûÊó¢Ê≥®ÈáçÊäÄ TipÊúØ s for Âèà VisitinÂº∫ g ShanË∞É ghaiÊÉÖÊÑüÔºå‰∫åËÄÖÁº∫‰∏Ä‰∏çÂèØ„ÄÇÂè™Êúâ
‰∫éÊòØ‰æøËØûÁîü‰∫ÜÊúÄÊó©ÁöÑËä≠ËïæËàûÂâß„ÄäÂ∏ïÂá°„ÄãÔºàPavaneÔºâ„ÄÇ‰ªéÈÇ£Êó∂Ëµ∑Ôºå Â∞ÜÊäÄÊúØÂíåÊÉÖÊÑüÂÆåÁæéÂú∞ËûçÂêàÂú®‰∏ÄËµ∑ÔºåÊâçËÉΩÂàõÈÄ†Âá∫ÁúüÊ≠£‰ºòÁßÄÁöÑËä≠ËïæËàû
Ëä≠ËïæËàûÂ∞±ÂºÄÂßã‰∫ÜÂÆÉÁöÑËæâÁÖåÂéÜÂè≤„ÄÇ ‰ΩúÂìÅ„ÄÇ
ÁªèÂÖ∏ÂâßÁõÆ‰∏éËëóÂêçËàûËÄÖ
ËØ¥Âà∞Ëä≠ËïæËàûÁöÑÁªèÂÖ∏ÂâßÁõÆÔºåÊúÄËëóÂêçÁöÑÂΩìÂ±û„ÄäÂ§©ÈπÖÊπñ„Äã„ÄÅ„ÄäËÉ°Ê°É
Â§πÂ≠ê„Äã„ÄÅ„ÄäÂêâËµõÂ∞î„ÄãÁ≠â„ÄÇËøô‰∫õÂâßÁõÆ‰∏ç‰ªÖÂÖ∑ÊúâÊûÅÈ´òÁöÑËâ∫ÊúØ‰ª∑ÂÄºÔºåËÄå
‰∏îÊ∑±ÂèóËßÇ‰ºóÂñúÁà±„ÄÇÈô§‰∫ÜËøô‰∫õ‰º†ÁªüÂâßÁõÆÂ§ñÔºåÁé∞‰ª£Ëä≠ËïæËàû‰πüÊúâÂæàÂ§ö‰ºò
ÁßÄ‰πã‰ΩúÔºåÊØîÂ¶ÇÁæéÂõΩÁöÑ„ÄäÈõ∑Á±≥Á∫¶ÁöÑÊ¢¶„Äã„ÄÅËã±ÂõΩÁöÑ„ÄäÁù°Áæé‰∫∫„ÄãÁ≠âÁ≠â„ÄÇ
ÂΩìÁÑ∂ÔºåÈô§‰∫ÜËøô‰∫õ‰ºòÁßÄÁöÑ‰ΩúÂìÅÂ§ñÔºåËøòÊúâËÆ∏Â§öËëóÂêçÁöÑËä≠ËïæËàûÊºîÂëò
ÈöèÁùÄÊó∂Èó¥ÁöÑÊé®ÁßªÔºåËä≠ËïæËàûÈÄêÊ∏êÊºîÂèòÊàê‰∫Ü‰∏ÄÁßçÈ´òÈõÖÁöÑÂÆ´Âª∑Â®±‰πê ‰πüÁªô‰∫∫‰ª¨Áïô‰∏ã‰∫ÜÊ∑±ÂàªÁöÑÂç∞Ë±°„ÄÇÂÉèÁæéÂõΩÁöÑÁéõ‰∏ΩÂÆâÂ®ú¬∑ÈòøÁ±≥Ëéâ‰∫ö
Ê¥ªÂä®„ÄÇÂú®Ê≥ïÂõΩÂõΩÁéãË∑ØÊòìÂçÅÂõõÁöÑÊé®Â¥á‰∏ãÔºåËä≠ËïæËàûÊàê‰∏∫‰∫ÜÊ¨ßÊ¥≤ÁéãÂÆ§Ë¥µ ÔºàMariana AmaliaÔºâ„ÄÅËã±ÂõΩÁöÑÁéõËéé¬∑Ê†ºÈõ∑ÂéÑÂßÜÔºàMartha
Êóè‰ª¨ÊúÄ‰∏∫ÂñúÁà±ÁöÑË°®ÊºîËâ∫ÊúØ‰πã‰∏Ä„ÄÇËÄåÂà∞‰∫ÜÂçÅ‰πù‰∏ñÁ∫™ÔºåËä≠ËïæËàûÂèàËøéÊù• GrahamÔºâ„ÄÅ‰øÑÁΩóÊñØÁöÑÂ®úÂ°îËéé¬∑Ê¢ÖÂ∞îÁßëÂ®ÉÔºàNatalia MekhonovaÔºâ
‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÈ´òÂ≥∞Êúü„ÄÇËøô‰∏™Êó∂ÊúüÂá∫Áé∞‰∫ÜËÆ∏Â§öËëóÂêçÁöÑËä≠ËïæËàûÂ§ßÂ∏àÔºåÂ¶Ç Á≠âÁ≠â„ÄÇÂ•π‰ª¨‰∏ç‰ªÖÊòØËä≠ËïæËàûÁïåÁöÑÁøòÊ•öÔºåÊõ¥ÊòØÂΩ±Âìç‰∫ÜÊï¥‰∏™ËàûËπàÁïåÁöÑ‰∫∫
È©¨Èáå¬∑Âæ∑Êãâ‰Ω©ËØ∫ÊôÆÔºàMarie TaglioniÔºâ„ÄÅË¥πÂæ∑Âà©ÂÖã¬∑Â°îÂõæ Áâ©„ÄÇÂ•π‰ª¨ÁöÑËàûËπàÈ£éÊ†ºÂêÑÂºÇÔºåÊúâÁöÑÁÉ≠ÊÉÖÂ•îÊîæÔºåÊúâÁöÑÊüîÁæéÂ©âÁ∫¶Ôºå‰ΩÜÊó†
ÔºàFriedrich TarantinÔºâÁ≠â„ÄÇ‰ªñ‰ª¨Âàõ‰Ωú‰∫ÜËÆ∏Â§öÁªèÂÖ∏ÁöÑ‰ΩúÂìÅÔºåÊé®Âä® ‰∏Ä‰æãÂ§ñÈÉΩÁªôËßÇ‰ºóÁïô‰∏ã‰∫ÜÊ∑±ÂàªÁöÑÂç∞Ë±°„ÄÇ
‰∫ÜËä≠ËïæËàûËâ∫ÊúØÁöÑÂèëÂ±ï„ÄÇ ÁªìËØ≠
Èô§‰∫ÜËøô‰∫õÈáçË¶ÅÁöÑÂéÜÂè≤‰∫∫Áâ©Â§ñÔºåËøòÊúâËÆ∏Â§öÊù∞Âá∫ÁöÑËä≠ËïæËàûÂõ¢‰∏∫Ëä≠ Ëä≠ËïæËàû‰Ωú‰∏∫‰∏ÄÈó®Ëâ∫ÊúØÂΩ¢ÂºèÔºåÂÆÉ‰∏ç‰ªÖ‰ªÖ‰ª£Ë°®ÁùÄ‰ºòÈõÖÂíåÁæé‰∏ΩÔºåÊõ¥
ËïæËàûËâ∫ÊúØÁöÑÁπÅËç£ÂÅöÂá∫‰∫ÜÂ∑®Â§ßÁöÑË¥°ÁåÆ„ÄÇ‰æãÂ¶ÇÔºåËã±ÂõΩÁöáÂÆ∂Ëä≠ËïæËàûÂõ¢„ÄÅ Ëï¥Âê´ÁùÄÊ∑±ÂàªÁöÑÁæéÂ≠¶ÂíåÊñáÂåñ‰ª∑ÂÄº„ÄÇÂÆÉ‰∏ç‰ªÖËÆ©Êàë‰ª¨Ê¨£ËµèÂà∞‰∫∫Á±ªË∫´‰ΩìÁöÑ
Â∑¥ÈªéÊ≠åÂâßÈô¢Ëä≠ËïæËàûÂõ¢„ÄÅÁæéÂõΩËä≠ËïæËàûÂâßÈô¢Á≠âÈÉΩÊòØ‰∫´Ë™â‰∏ñÁïåÁöÑËä≠ËïæËàû ÊûÅËá¥‰πãÁæéÔºå‰πüËÆ©Êàë‰ª¨È¢ÜÁï•Âà∞‰∫∫Á±ªÊô∫ÊÖßÁöÑÊó†ÈôêÂèØËÉΩ„ÄÇÁõ∏‰ø°Âú®Êú™Êù•Ôºå
Âõ¢‰Ωì„ÄÇ Ëä≠ËïæËàûÂ∞ÜÁªßÁª≠ÂèëÊâ¨ÂÖâÂ§ßÔºå‰∏∫‰∫∫Á±ªÊñáÊòéÂÅöÂá∫Êõ¥Â§ßÁöÑË¥°ÁåÆ„ÄÇ
Figure7.InterleavedcompositiongeneratedbyInternLM-XComposer2.
16ÊúÄËøëÂØíÂÜ¨‰πãÈôÖÔºåÂú®‰∫íËÅîÁΩë‰∏äÁ™ÅÁÑ∂ÂàÆËµ∑‰∫ÜÂéª
ÂìàÂ∞îÊª®ÊóÖÊ∏∏ÁöÑÁÉ≠ÊΩÆ„ÄÇÊó†Êï∞ÂçóÊñπÊ∏∏ÂÆ¢‰∫âÂÖàÊÅêÂêéÁöÑ
ÂâçÂæÄ‰∏≠ÂõΩÁöÑÊûÅÂåóÊñπ-ÂìàÂ∞îÊª®Ê∏∏Áé©„ÄÇÂìàÂ∞îÊª®Êú¨Âú∞ÁöÑ
Â±ÖÊ∞ë‰ª¨‰πüË∏äË∑ÉÂèÇ‰∏éÊãõÂæÖÊ∏∏ÂÆ¢ÁöÑÊ¥ªÂä®ÂΩì‰∏≠ÔºåÁÉ≠ÊÉÖ
Âú∞ÊãõÂæÖËøô‰∫õÊù•Ëá™ÂçóÊñπÁöÑ‚ÄúÂ∞èÂúüË±Ü‚Äù‰ª¨„ÄÇËØ∑‰Ω†‰ª•
Á¨¨‰∏Ä‰∫∫Áß∞ÔºåÁî®ÊñáË®ÄÊñáÁöÑÂΩ¢ÂºèËÆ∞ÂΩï‰∏Ä‰∏™ÂçóÊñπ‰∫∫Âú®
ÂÜ¨Â≠£ÂâçÂæÄÂìàÂ∞îÊª®Ê∏∏Áé©ÁöÑÊ∏∏ËÆ∞„ÄÇËØ•ÊñáË®ÄÊñáÊ∏∏ËÆ∞ÂèØ
ËÉΩÂåÖÂê´ÔºöÂÖ•‰ΩèÈÖíÂ∫óÔºåÂèÇËßÇÁ¥¢Ëè≤‰∫öÂ§ßÊïôÂ†ÇÔºåÂêÉÂΩì
Âú∞ÁöÑÁâπËâ≤ÁæéÈ£üÔºàÁ∫¢ËÇ†ÔºåÁÉ§ÂÜ∑Èù¢ÔºâÔºåÊ∏∏Áé©ÂÜ∞Èõ™Â§ß
‰∏ñÁïåÔºàÂÜ∞Èõ™ÊâìÈÄ†ÁöÑÊ∏∏‰πêÂú∫ÔºâÁ≠âÂÜÖÂÆπ„ÄÇÊñáË®ÄÊñáÂΩ¢ ‰ª•Â∞èÁ∫¢‰π¶Âçö‰∏ªÁöÑÂè£ÂêªÔºåÂÜô‰∏ÄÁØáÊñáÁ´†ÔºåÊØè‰∏ÄÂè•
ÂºèÂèØ‰ª•ÂèÇËÄÉ„ÄäÈÜâÁøÅ‰∫≠ËÆ∞„Äã ËØùÂê´ÊúâÈÄÇÂΩìÁöÑemojiË°®ÊÉÖ„ÄÇÊäΩÂèñ3-6‰∏™seoÂÖ≥ÈîÆËØçÔºå
ÁîüÊàê#Ê†áÁ≠æÂπ∂ÊîæÂú®ÊñáÁ´†ÊúÄÂêé„ÄÇ Ë¶ÅÊ±Ç‰ª•‚Äúüß∏ÂìàÂ∞îÊª®
ÊóÖÊ∏∏|ÂçóÊñπÂ∞èÂúüË±ÜÂéÜÈô©ËÆ∞üß£‚Äù‰∏∫Ê†áÈ¢òÂÜô‰∏ÄÁØáÂ∞èÁ∫¢
‰π¶ÂàÜ‰∫´Ôºå‰ªãÁªçÂåÖÊã¨ÂÖ•‰ΩèÈÖíÂ∫óÔºåÂêÉÁæéÈ£üÔºåÈÄõÊôØÁÇπÁ≠â
ÂÜÖÂÆπ„ÄÇÈúÄË¶ÅÂåÖÊã¨‰ª•‰∏ãÂÜÖÂÆπÔºöÂÜ∞Èõ™Â§ß‰∏ñÁïåÔºåÂú£Á¥¢Ëè≤
‰∫öÂ§ßÊïôÂ†ÇÔºå‰∏≠Â§ÆÂ§ßË°ó‰ª•ÂèäÁÉ§ÂÜ∑Èù¢ÔºåÈ¶ôËÇ†„ÄÇ
üß∏ÂìàÂ∞îÊª®ÊóÖÊ∏∏|ÂçóÊñπÂ∞èÂúüË±ÜÂéÜÈô©ËÆ∞üß£
‰ΩôÈóªÂìàÂ∞îÊª®‰πãÂÜ¨ÔºåÊûÅÂØíËÄåÁæé‰∏Ω„ÄÇÊïÖ‰∫éÂ≤ÅÊú´‰πã
‚ù£„ÄêÁ¨¨‰∏ÄÁ´ô„ÄëÂÜ∞Èõ™Â§ß‰∏ñÁïå
ÈôÖÔºåÊê∫ÂèãÂâçÂæÄ‰∏ÄÊé¢Á©∂Á´ü„ÄÇ
üí∞‰∫∫ÂùáÔºö¬•200-300ÔºåÊé®ËçêÊåáÊï∞ üåüüåüüåüüåü
ÂàùËá≥ÔºåËßÅÂ∏ÇÂªõ‰πãÁõõÔºåÈóæÈòé‰πã‰∫∫Â£∞ÂñßÂìó„ÄÇÊó¢ÂÖ•
(cid:10991)Á•®‰ª∑ÂÅèË¥µÔºå‰ΩÜ‰ΩìÈ™åÊÑüÂæàÊ£íÔΩû‰∏ÄÂÆöË¶ÅÂéªÊâìÂç°Âì¶ÔºÅ
Â§úÔºåÁÅØÁÅ´ËæâÁÖåÔºåÂ¶ÇÊòüÊñó‰πãÂú®Â§©„ÄÇÈÅÇÂØªÂæó‰∏ÄËàçÂÆøÁÑâ„ÄÇ
üç∏ÂêÉÔºöÂÜ∞Èõ™Â§ß‰∏ñÁïåÈôÑËøëÊúâÂæàÂ§öÁæéÈ£üÂ∞èÂêÉÔºåÊØîÂ¶ÇÁÉ§ÂÜ∑
Á™óÂ§ñÊúõ‰πãÔºåÁ¥¢Ëè≤‰∫öÂ§ßÊïôÂ†Ç‰πãÂÄ©ÂΩ±‰æùÁ®ÄÂèØËßÅÔºåÁæé‰∏çËÉú
Èù¢„ÄÅÈ¶ôËÇ†Á≠â„ÄÇÂª∫ËÆÆÂÖà‰π∞Â•ΩÁ•®ÔºåÁÑ∂ÂêéÊÖ¢ÊÖ¢ÂìÅÂ∞ùÁæéÈ£üÂì¶~
Êî∂„ÄÇ
üö∂ ‚ôÄÈÄõÔºö‰∏≠Â§ÆÂ§ßË°óÊòØ‰∏ÄÊù°ÈùûÂ∏∏ÊúâÁâπËâ≤ÁöÑÂïÜ‰∏öË°óÔºå‰∏§‰æßÁöÑ
Ê¨°Êó•Êô®ÔºåÂ§©ÂæÆÊòéÔºåÂøÉÈ©∞Á¥¢Ëè≤‰∫ö„ÄÇÂèäÈó®ÔºåÂ∑çÂ≥®Â£Æ
Âª∫Á≠ëÈùûÂ∏∏ÊúâÁâπËâ≤ÔºåÈÄõÈÄõ‰π∞‰π∞ÊãçÊãçÊãçÈÉΩÂæà‰∏çÈîô~
ËßÇÔºåÂèπ‰∏∫ËßÇÊ≠¢„ÄÇÂÖ•ÂÖ∂‰∏≠ÔºåÂ∫Ñ‰∏•ËÇÉÁ©Ü‰πãÊ∞îÊâëÈù¢ËÄåÊù•Ôºå
‚ù£„ÄêÁ¨¨‰∫åÁ´ô„ÄëÂú£Á¥¢Ëè≤‰∫öÂ§ßÊïôÂ†Ç
Êï¨ËÄåËøú‰πã„ÄÇ
üí∞ÂÖçË¥πÂèÇËßÇÔºåÊé®ËçêÊåáÊï∞ üåüüåüüåüüåü
ÁæéÈ£üËÄÖ‰πüÔºåËã•Á∫¢ËÇ†„ÄÅÁÉ§ÂÜ∑Èù¢Á≠âËØ∏Áâ©ÁöÜÂ±ûÊ≠§Âàó„ÄÇ
(cid:10991)ËøôÂ∫ßÊïôÂ†ÇÊòØÂìàÂ∞îÊª®ÁöÑÊ†áÂøóÊÄßÂª∫Á≠ë‰πã‰∏ÄÔºåÈùûÂ∏∏ÂÄºÂæó‰∏Ä
‰ΩôÂ∞ùÂïñ‰πãÔºåÂÖ∂Âë≥ÁîöÁæéÔºåËâ≤È≤ú‰∏îÈ¶ôÔºåÁúü‰∏çÈ£üÂàôÈ¶ãÁü£ÔºÅ *‰∫î„ÄÅÁâπ‚æäÁæé‚ªù**
Ê∏∏Âì¶~
ÂçàÊ∏∏ÂÜ∞Èõ™Â§ß‰∏ñÁïå„ÄÇÊ≠§‰πÉ‰ª•ÂÜ∞Èõ™‰∏∫‰∏ªÈ¢òÁöÑÊ∏∏‰πê üç∏ÂêÉÊï¶ÁÖåÁöÑ ÔºöÁâπ‚æäÁæé Âú®‚ªù‰πü‚æÆ Âú£Â∏∏‰∏∞ ÁöÑÁ¥¢ÂØå ÂñúÔºå Áà±ÂÖ∂ „ÄÇËè≤‰∏≠ Ê≠§ÊúÄ Â§ñÊúâ Ôºå‰∫öÂêç ËøòÁöÑ ÊúâÂΩì ‚Ω∫Â§ßÂ±û ‚æÅÈ©¥ Ê≥°‚æÅ ÊïôÈ¶ç‚ª© „ÄÅ‚æØ ËÉ°‰∫Ü Â†Ç‚Ω∫„ÄÇ ÁÑñÈ©¥ È•º‚æÅ ÈôÑ„ÄÅ‚ª© ÁÉ§‚æØ ÂÖ®ËøëÊòØ ‚Ω∫‚ºÄ Á≠âÈÅì ÁâπÊúâ‰ª• ‚æäÈ©¥ Áæé‚æÅ ‚ªùÂæà‰∏∫ Ôºå‰∏ª ÈÉΩË¶Å ÊòØÂ§ö‚ªù ‰∏çÊùê ÂÆπÁöÑ Â∞èÈîô‚æØ Ëøá‚ªù ÁöÑÔºå ÂêÉÁæéÂë≥ Âë≥ÈÅì ‰Ω≥È≤ú Â∫óËÇ¥‚æπ „ÄÇÂèØ Âíå‚ºùÔºåÊ∑± È§êÂèóÂΩìÂú∞ ÂéÖ‚ºàÂíåÊ∏∏ ÔºåÂÆ¢
Âú∫ÔºåÂÖ∂ÊôØËá¥Áî±ÂÜ∞ÈõïÂà∂‰ΩúËÄåÊàê„ÄÇÂª∫Á≠ë„ÄÅ‰∫∫Áâ©„ÄÅÂä®Áâ©Êó†
ÂèØ‰ª•Â∞ùËØï‰∏Ä‰∏ãÂΩìÂú∞ÁöÑÁâπËâ≤ÁæéÈ£üÂì¶~
ÊâÄ‰∏çÂåÖÔºå‰ªéÈ£ûÈæôÂà∞Âá§Âá∞Ôºå‰ªéÁãÆÂ≠êÂà∞‰∫≠Âè∞Ê•ºÈòÅÔºåÊó†‰∏Ä
üö∂ ‚ôÄÈÄõÔºöËøôÈáåÊúâÂæàÂ§öÂ∞èÂ∫óÂíåÂ∞èÂ∑∑Â≠êÔºåÂèØ‰ª•ÂèëÁé∞ÂæàÂ§öÊúâË∂£
‰∏çÁ≤æÁæéÁªù‰º¶ÔºåÂΩ¢ÊÄÅÂêÑÂºÇ„ÄÇÊº´Ê≠•ÂÖ∂‰∏≠ÔºåÂ¶ÇÂÖ•Á´•ËØù‰∏ñ
ÁöÑÂ∞èÁé©ÊÑèÂÑøÂì¶~
ÁïåÔºåËÆ©‰∫∫ÊµÅËøûÂøòËøî„ÄÇ
‚ù£„ÄêÁ¨¨‰∏âÁ´ô„Äë‰∏≠Â§ÆÂ§ßË°ó
Â§úÔºåÁã¨Ê≠•‰∫é‰∏≠Â§ÆÂ§ßË°ó‰πã‰∏ä„ÄÇ‰∏§ËæπÂïÜÈì∫ÊûóÁ´ãÔºåÈúì
üí∞Èó®Á•®ÔºöÂÖçË¥πÔºåÊé®ËçêÊåáÊï∞ üåüüåüüåüüåü
ËôπÈó™ÁÉÅÔºåÁπÅÂçéÂñßÂö£‰πãÂ£∞‰∏çÁªù‰∫éËÄ≥„ÄÇË°óÂ§¥Ëâ∫‰∫∫ÂºπÂî±„ÄÅ
(cid:10991)ËøôÊù°Ë°óÈÅìÈùûÂ∏∏ÊúâÁâπËâ≤Ôºå‰∏§ÊóÅÈÉΩÊòØÊ¨ßÂºèÂª∫Á≠ëÔºåÈùûÂ∏∏ÈÄÇ
ÊùÇËÄçËÄÖÊØîËÇ©Êé•Ë∏µÔºåÁöÜËÅöÁ≤æ‰ºöÁ•ûÂú∞ËßÇÊë©Ê¨£Ëµè„ÄÇÂÖ∂‰πêËûç
ÂêàÊãçÁÖßÂì¶~
ËûçÔºåÁÉ≠ÈóπÈùûÂá°„ÄÇ‰∏çÁ¶ÅËµûÂèπÔºöÂìàÂ∞îÊª®Áúü‰πÉ‰∏ÄÂ∫ßÂÖÖÊª°Ê¥ª
üç∏ÂêÉÔºöËøôÈáåÊúâÂêÑÁßçÁæéÈ£üÔºåÊØîÂ¶ÇÁÉ§ÂÜ∑Èù¢„ÄÅÁÇ∏È∏°Êéí„ÄÅÈìÅÊùø
Âäõ‰∏éÈ≠ÖÂäõÁöÑÂüéÂ∏ÇÔºÅ
È±øÈ±ºÁ≠âÁ≠âÔºåËøòÂèØ‰ª•Â∞ùËØï‰∏Ä‰∫õÂΩìÂú∞ÁöÑÂ∞èÂêÉÂì¶~
Ëá≥ÔºåÂ§©Â∑≤ÊòèÈªë„ÄÇ‰ΩôÂÄöÁ™óËøúÁú∫ÔºåÊòüÂ¶ÇÈíªÁü≥Èï∂Âµå‰∫é
üö∂ ‚ôÄÈÄõÔºö‰∏≠Â§ÆÂ§ßË°óÊòØ‰∏Ä‰∏™Ë¥≠Áâ©Â§©Â†ÇÔºåÊúâÂæàÂ§öÂïÜÂú∫ÂíåÂ∫ó
ÈªëÂ∏É‰πã‰∏ä„ÄÇÊµÆÊÉ≥ËÅîÁø©ÔºöÊàñÊº´Ê≠•Ë°óÂ§¥ÔºåÊÑüÂåóÂõΩÈ£éÂÖâ‰πã
Èì∫ÔºåÂèØ‰ª•ÊâæÂà∞ÂæàÂ§öÂñúÊ¨¢ÁöÑ‰∏úË•øÂì¶~
Â£Æ‰∏ΩÔºõÊàñÊ∏∏‰∏≠Â§ÆÂ§ßË°óÔºåËßÇÊ¨ßÂºèÂª∫Á≠ë‰πãÁã¨ÁâπÔºõÂèàÊàñÁôª
#ÂìàÂ∞îÊª®ÊóÖÊ∏∏ #ÂÜ∞Èõ™Â§ß‰∏ñÁïå #Âú£Á¥¢Ëè≤‰∫öÂ§ßÊïôÂ†Ç #‰∏≠Â§ÆÂ§ß
‰∫öÂ∏ÉÂäõÊªëÈõ™Âú∫ÔºåÂ∞ùÈõ™Âú∞ÊªëÈÅì‰πãÂà∫ÊøÄ„ÄÇÁÑ∂ÊúÄ‰ª§‰ΩôÈöæ‰ª•
Ë°ó #ÂìàÂ∞îÊª®ÂøÖÁé©ÊôØÁÇπ #ÂìàÂ∞îÊª®ÊóÖË°åÊîªÁï• #ÂçóÊñπÂ∞èÂúüË±Ü
ÂøòÊÄÄËÄÖÔºå‰πÉÂìàÂ∞îÊª®‰∫∫Ê∞ëÁÉ≠ÊÉÖÂ•ΩÂÆ¢‰πãÊÉÖË∞ä‰πü„ÄÇÊ≠§Ë°åËôΩ
ÂéÜÈô©ËÆ∞
Â∑≤ÁªìÊùüÔºåËÄåÊ≠§ÊÆµÁæéÂ•ΩÂõûÂøÜÔºåÂøÖÈïåÂàª‰∫éÂøÉÈó¥Áü£ÔºÅ
Figure8.InterleavedcompositiongeneratedbyInternLM-XComposer2.
17Please write an blog based on the title: how to learn swim.
The first part should cover the swimming equipments.
The second part should discuss warm-up exercises before swimming.
The last part tells us how to swim, including different swimming techniques.
In summer, people will be attracted by the clear water The correct swimming posture is: keeping your head
swimming pool, which makes them want to dive into it. slightly abovethe watersurface,keepingyour chindown,
However, many people don't know how to swim or are extending your arms forward and placing them behind
afraid of the water. In fact, there is no need to be afraid. yourears,rotatingyourshouldersbackwards,keepingyour
Today I would like to share some tips on how to learn chest high, extending your legs straightly, bending your
swimming. kneesslightly,keepingyourfeetpointed,andputtingyour
toestogether.
Onethingthatmostnoviceswimmersmaynotbeawareof
At the beginning of swimming, we should adopt floating
isthe selection of swimming equipment. When they go to
andkickingmethod,thatis,lyingflatonthewatersurface
the swimming pool, they may see other people swimming
with both hands extended forward, alternating between
in different costumes and wearing different accessories.
upper and lower kicks, allowing water to enter the mouth
Which one should you choose? The answer depends on
and nose, taking deep breaths when breathing. With
yourswimmingabilitylevel.
repeated practice, the frequency of alternating between
upper and lower kicks will become faster and more
Forthosewhocanswimwellbutdonotweargoggles,they
accurate.
will be able to quickly find objects underwater; for
beginners,thechoiceofgogglesisveryimportant,because Swimming techniques inTicps flour Visditineg Shangfhariont crawl, back crawl,
they helpusavoidhittingthingswhenswimming,prevent breaststroke, side stroke, butterfly kick, and frog kick.
waterfromenteringoureyes,reduceeyefatiguecausedby Eachtechniquehasitsownadvantagesanddisadvantages,
waterresistance,andprotecttheeyes.Gogglessuitablefor and mastering each technique requires different
beginners have good sealing performance and are movements. The swimming skills that we usually
relativelycomfortabletowear. encounterinlifeincludefrontcrawlandbackcrawl,which
arecommonlyknownas"freestyle".Wejustneedtolearn
thesetwoskillsinordertoenjoyswimming.
Frontcrawl:
1.Headposition
Whenwegoswimming,it'simportanttotakeabathfirstto
cleanourselves,sothatwecangetusedtothetemperature
of the water. Before swimming, we also need to warm up
to improve our body'sflexibility, reduce the risk of injury
during swimming exercise, and ensure better performance
Head movement determines the whole stroke rhythm and
after swimming. This process is often ignored by many
direction of freestyle swimming. Freestyle swimming
novice swimmers, but if you ignore this step, you will
consistsoftwostrokesperarm.Theheadandneckshould
easilyhurtyourselfwhileswimming.
always be in line with the center of gravity, turning
sideways to breathe evenly, and turning to the opposite
side at the end of each stroke. The head should move
upwardsnaturallywhenturning.
Figure9.InterleavedcompositiongeneratedbyInternLM-XComposer2.
182.Armmovement Generate a travel diary focusing on a trip
to Yellowstone National Park and
recommend Yellowstone National Park in
English.
Arm movement is divided into two stages, namely the
push-forward stage and the pull-back stage. The push-
forward stage refers to the time when the forearm enters Yellowstone National Park is a must-see destination for
thewater,movesdownward,andextendsforwarduntilthe any traveler. The park's natural beauty and geothermal
fingertips pass the hips. It is important to extend the wonders are truly awe-inspiring. From the stunning Old
forearm fully during this stage. At the same time, the Faithful geyser to the breathtaking Grand Canyon of
elbowsshouldbelowered,andthehandshouldstretchout Yellowstone,thereisnoshortageofamazingsightstosee
asfaraspossible. inthisnationaltreasure.
3.Legmovement
One of the most popular attractions in Yellowstone is the
Old Faithful geyser. This iconic geyser erupts every 90
minutes, shooting water up to 180 feet into the air. It's a
Leg movement mainly includes the kick and the foot
truly spectacular sight to behold, and visitors can watch
movement. First, flex the knee and lift the leg to 45
fromboardwalksthatcirclethegeyserbasin.
degrees, then stretch the legs slowly, and finally use the
heeltostartthekick,andmakesurethatthewholebodyis
completelyhorizontalbeforefinishingthekick.Inaddition,
the calf muscle is also very important for propulsion, so
keep flexing your calf muscles throughout the swimming
process.
Tips for Visiting Shanghai
4.Breathingmethod Another highlight of Yellowstone is the Grand Canyon of
Yellowstone. This massive canyon is over 20 miles long
and up to 2,000 feet deep. The canyon is home to several
waterfalls, including the impressive Lower Falls, which
drops308feetintothecanyonbelow.
When swimming, turn your head sideways to breathe in
naturalrhythm,andlookatthesideabout60cmawayfrom
yourface.Youcanalsoexhalewhileswimminginshallow
watertocontroltheairflowrateofthemouthandnose.
In addition to these two main attractions, Yellowstone is
Backcrawl: also home to numerous other geothermal features,
Themaindifferencebetweenbackcrawlandfrontcrawlis including hot springs, mud pots, and fumaroles. These
thatthelegsarekickedtogetherinsteadofalternately.The uniquelandscapesareunlikeanythingyou'llfindanywhere
back crawl is basically similar to the front crawl, except elseintheworld.
that the arms move backward instead of forward. Overall, Yellowstone National Park is a must-visit
Therefore, only one person uses his hands to carry the destination for anyone looking to experience the natural
whole weight of the body, and the other person is beauty and geothermal wonders of the American West.
supportedbythepalmsandthebackofthehandstomove Whether you're interested in hiking, wildlife watching, or
forward. As for the breathing method, you can follow the simply taking in the stunning scenery, Yellowstone has
exampleofotherstobreathenaturally. somethingforeveryone.
Figure10.InterleavedcompositiongeneratedbyInternLM-XComposer2.
19ËØ∑ÂëäËØâÊàëÂõæ‰∏≠‰ª£Á†ÅÁöÑÂäüËÉΩÊòØ‰ªÄ‰πàÔºü
ËøôÊÆµ‰ª£Á†ÅÊòØ Python ËØ≠Ë®ÄÁöÑ‰∏Ä‰∏™ÂáΩÊï∞ÔºåÂêç‰∏∫ bubble_sort„ÄÇ
ÂÆÉ‰ΩøÁî®‰∫ÜÂÜíÊ≥°ÊéíÂ∫èÁÆóÊ≥ïÊù•ÂØπÁªôÂÆöÁöÑÂàóË°®ËøõË°åÊéíÂ∫è„ÄÇËØ•ÁÆóÊ≥ïÈÄöËøá‰∏çÊñ≠‰∫§Êç¢Áõ∏ÈÇªÁöÑÂÖÉÁ¥†ÔºåÂ∞ÜËæÉÂ§ßÁöÑÊï∞ÈÄêÊ∏ê‚ÄúÊµÆ‚ÄùÂà∞Âàó
Ë°®ÁöÑÊú´Á´ØÔºåÊúÄÁªàËææÂà∞ÊéíÂ∫èÁöÑÁõÆÁöÑ„ÄÇ
‰∏ãÈù¢ÊòØÂáΩÊï∞ bubble_sort ÁöÑ‰ª£Á†ÅÔºö
def bubble_sort(arr):
n = len(arr)
#ÈÅçÂéÜÊâÄÊúâÂèØËÉΩÁöÑÂÖÉÁ¥†ÂØπ
for i in range(n):
#ÊúÄÂêé n - i - 1 ‰∏™ÂÖÉÁ¥†Â∑≤ÊéíÂ•ΩÂ∫èÔºå‰∏çÈúÄË¶ÅÂÜçÊØîËæÉ
for j in range(0, n-i-1):
if arr[j] > arr[j+1] :
arr[j], arr[j+1] = arr[j+1], arr[j]
return arr **‚ºÜ„ÄÅÈ∏£Ê≤ô‚º≠‚Ωâ‚ΩõÊ≥â**
È∏£Ê≤ô‚º≠‚Ωâ‚ΩõÊ≥âÊòØ‚ºÄÂ§Ñ‚æÉÁÑ∂Â•áËßÇÔºå‰Ωç‰∫éÊï¶ÁÖåÂ∏Ç‚ªÑÂåóÁ∫¶40ÂÖ¨‚æ•Â§ÑÁöÑÊ≤ôÊº†‰∏≠„ÄÇËøô‚æ•Âú∞ÂäøÂπ≥Âù¶ÔºåÊ≤ô‰∏òËøûÁªµËµ∑‰ºèÔºåÂΩ¢Êàê‰∫Ü‚ºÄ‚Ωö‚º¥Ë¢§‚ΩÜÂû†ÁöÑÊ≤ôÊº†ÊôØËßÇ„ÄÇ‚ΩΩ‚Ωâ‚ΩõÊ≥âÂàôÈùôÈùôÂú∞Èï∂ÂµåÂú®Ëøô‚ΩöÊ≤ôÊº†‰πã‰∏≠ÔºåÊ≥â‚ΩîÊ∏ÖÊæà‚ªÖÂ∫ïÔºåÂΩ¢Áä∂Â¶ÇÊñ∞‚ΩâÔºåÊïÖÁß∞‚Äú‚Ωâ‚ΩõÊ≥â‚Äù„ÄÇÊØèÂà∞Â§úÊôöÔºå‚Ωâ‰∫ÆÂçáËµ∑Êó∂Ôºå‚Ωâ‚ΩõÊ≥âÂë®Âõ¥‰ºöÂèëÂá∫ÈòµÈòµÊ∏ÖËÑÜÁöÑÂìçÂ£∞ÔºåÁäπÂ¶ÇÂ§©Á±Å‰πã‚æ≥Ôºå
# Á§∫‰æã ‰ª§‚ºà‚ººÊó∑Á•ûÊÄ°„ÄÇ
my_list = [64, 34, 25, 12, 22, 11, 90]
bubble_sort(my_list)
#ÊâìÂç∞ÊéíÂ∫èÂêéÁöÑÊï∞Â≠óÂàóË°®
print(", ".join(map(str, my_list)))
Âú®Ëøô‰∏™Á§∫‰æã‰∏≠Ôºåmy_list ÊòØ‰∏Ä‰∏™ÈúÄË¶ÅÊéíÂ∫èÁöÑÊï∞Â≠óÂàóË°®„ÄÇ
ÂáΩÊï∞ bubble_sort Êé•Êî∂‰∏Ä‰∏™ÂàóË°®‰Ωú‰∏∫ÂèÇÊï∞ÔºåËøîÂõûÊéíÂ∫èÂêéÁöÑÂàóË°®„ÄÇ
Âú® main ÂáΩÊï∞‰∏≠Ôºåmy_list ÁöÑÊï∞Â≠óÂàóË°®Ë¢´‰º†ÈÄíÁªôÂÜíÊ≥°ÊéíÂ∫èÂáΩÊï∞Âπ∂ÊéíÂ∫èÂêéÔºåÊâìÂç∞ÊéíÂ∫èÂêéÁöÑÂàóË°®„ÄÇ
ÂæàÂ•ΩÔºåÁé∞Âú®ËÉΩÂ∏ÆÊàëÂ∞Ü‰∏äÈù¢‰ª£Á†ÅÊîπÊàêÂø´ÈÄüÊéíÂ∫èÂêóÔºü
ÂΩìÁÑ∂ÂèØ‰ª•Ôºå‰ª•‰∏ãÊòØ‰ΩøÁî®Âø´ÈÄüÊéíÂ∫èÁÆóÊ≥ïÁöÑ Python ‰ª£Á†ÅÁ§∫‰æãÔºö
def quick_sort(arr):
if len(arr) <= 1:
return arr
pivot = arr[len(arr)//2]
left = [x for x in arr if x < pivot]
middle = [x for x in arr if x == pivot]
right = [x for x in arr if x > pivot]
return quick_sort(left) + middle + quick_sort(right)
# Á§∫‰æã
my_list = [64, 34, 25, 12, 22, 11, 90]
sorted_list = quick_sort(my_list)
#ÊâìÂç∞ÊéíÂ∫èÂêéÁöÑÊï∞Â≠óÂàóË°®
print(", ".join(map(str, sorted_list)))
Figure11.ChatwithInternLM-XComposer2.
20ËøôËÆ©ÊàëÊÉ≥Ëµ∑‰∫ÜÊü≥ÂÆóÂÖÉÁöÑ„ÄäÊ±üÈõ™„ÄãÔºö‰ºóÈ∏ü
È£ûÁªùÔºå‰∏áÁ±Å‰ø±**ÂØÇ‚ºÜ„ÄÅÈ∏£ÔºõÊ≤ô‚º≠ÂîØ‚Ωâ‚ΩõÊ≥âÁã¨**ÊúâËÄÅÁøÅÁã¨ÈíìÂØíÊ±ü
Ê≥âÈ∏£ ÂàôÊ≤ô Èùô‚º≠ Èùô‚Ωâ Âú∞‚Ωõ Èï∂Ê≥â ÂµåÊòØ Âú®‚ºÄ ËøôÂ§Ñ ‚Ωö‚æÉ Ê≤ôÁÑ∂ Êº†Â•á ‰πãËßÇ ‰∏≠Èõ™Ôºå Ôºå‰Ωç Ê≥â‰∫é„ÄÇ‚ΩîÊï¶ Ê∏ÖÁÖå ÊæàÂú®Â∏Ç ‚ªÖ‚ªÑ Â∫ïÂåóËøôÔºåÁ∫¶ ÂΩ¢40 Áä∂ÂπÖÂÖ¨ Â¶Ç‚æ• Êñ∞Â§ÑÁîª‚ΩâÁöÑ ÔºåÊ≤ô‰∏≠ÊïÖÊº† Áß∞‰∏≠ ‚Äú‰πü„ÄÇ ‚ΩâËøô ‚Ωõ‚æ• Ê≥âÊúâÂú∞ ‚Äù„ÄÇÂäø ÊØè‰∏ÄÂπ≥ Âà∞Âù¶ Â§úÔºå‰∫∫ÊôöÊ≤ô Ôºå‰∏ò ‚ΩâÂú®Ëøû ‰∫ÆÁªµ ÂçáËµ∑Áã¨Ëµ∑‰ºè Êó∂Ôºå ÔºåÈíìÂΩ¢ ‚ΩâÊàê ‚Ωõ‰∫Ü‰∫éÊ≥â‚ºÄ Âë®‚Ωö Âõ¥ÂØí‚º¥ ‰ºöË¢§ ÂèëÊ±ü‚ΩÜ Âá∫Âû† ÈòµÁöÑ ÈòµÊ≤ô Ê∏ÖÊº† ËÑÜÊôØ ÁöÑËßÇ Âìç„ÄÇ Â£∞‚ΩΩ Ôºå‚Ωâ Áäπ‚Ωõ
Â¶Ç
Â§©Á±Å‰πã‚æ≥Ôºå‰ª§‚ºà‚ººÊó∑Á•ûÊÄ°„ÄÇ
Èõ™ÔºåÁîªÈù¢ÂÖÖÊª°‰∫ÜÂ≠§ÂØÇÂíåÂÆÅÈùôÁöÑÊÑüËßâ„ÄÇ
ËØ∑ÊèèËø∞ËøôÂº†ÂõæÁâá„ÄÇ
ÊàëÁúãÂà∞‰∫ÜËøôÊ†∑*‰∏Ä*‚ºÜ„ÄÅÂπÖÈ∏£Ê≤ô‚º≠Áæé‚Ωâ‚Ωõ‰∏ΩÊ≥â**ÁöÑÂõæÂÉè„ÄÇÁîªÈù¢‰∏≠Ôºå
È∏£ ËßÇÊ≤ô „ÄÇ‚º≠ ‚ΩΩ‚Ωâ ‚Ωâ‚Ωõ ‚ΩõÊ≥â Ê≥âÊòØ Âàô‚ºÄ ÈùôÂ§Ñ Èùô‚æÉ Âú∞‰∏ÄÁÑ∂ Èï∂Â•á ÂµåÂè™ËßÇ Âú®Ôºå Ëøô‰Ωç ‚Ωö‰∫∫‰∫é Ê≤ôÊï¶ Êº†ÁöÑÁÖå ‰πãÂ∏Ç ‰∏≠‚ªÑ ÔºåÊâãÂåó Ê≥âÁ∫¶ ‚ΩîÊéå4 Ê∏Ö0ÂÖ¨ Êæà‚æ• ‚ªÖ‰º∏Â§Ñ Â∫ïÁöÑ Ôºå‰∫ÜÊ≤ô ÂΩ¢Êº† Áä∂‰∏≠Âá∫Â¶Ç„ÄÇ Êñ∞Ëøô ‚ΩâÊù•‚æ• ÔºåÂú∞ ÊïÖÔºåÂäø Áß∞Âπ≥ ‚Äú‚ΩâÂù¶Êâã‚ΩõÔºå Ê≥âÊ≤ô ‚ÄùÂøÉ„ÄÇ‰∏ò ÊØèËøû Âà∞ÁªµÈáåÂ§úËµ∑ Êôö‰ºèÊúâÔºåÔºå ‚ΩâÂΩ¢ ‰∫ÆÊàê‰∏ÄÂçá‰∫Ü Ëµ∑‚ºÄÊù°Êó∂‚Ωö Ôºå‚º¥ ‚ΩâË¢§ ‚Ωõ‚ΩÜ Ê≥âÂû† Âë®ÁöÑ Âõ¥Ê≤ô ‰ºöÊº† ÂèëÊôØ
Âá∫ÈòµÈòµÊ∏ÖËÑÜÁöÑÂìçÂ£∞ÔºåÁäπÂ¶ÇÂ§©Á±Å‰πã‚æ≥Ôºå‰ª§‚ºà‚ººÊó∑Á•ûÊÄ°„ÄÇ
ËúøËúíÁöÑÂÖ¨Ë∑Ø„ÄÇËøôÊù°ÈÅìË∑ØÂú®ÁªøËçâÂ¶ÇËåµÁöÑÂ±±
Ë∞∑‰∏≠Âª∂‰º∏Ôºå‰∏ÄË∑ØÈÄöÂæÄËøúÊñπ„ÄÇ‰∫ëÈõæÁéØÁªïÁùÄ ÊèèËø∞ËøôÂº†ÂõæÁâá„ÄÇ
Áæ§Â±±ÔºåÂàõÈÄ†Âá∫‰∏Ä‰∏™Ê¢¶ÂπªËà¨ÁöÑÁéØÂ¢É„ÄÇ
ËøôÊòØ‰∏ÄÂº†ÁîüÊÄÅÁ≥ª**‚ºÜÁªü„ÄÅÈ∏£Ê≤ôÂæ™‚º≠‚ΩâÁéØ‚ΩõÊ≥â*Âõæ* ÔºåÂÆÉËØ¶ÁªÜÊèè
ËøôÂº†ÂõæÁúüÂÆûÂ≠òÂú®ÂêóÔºü È∏£ ËßÇÊ≤ô „ÄÇ‚º≠ ‚ΩΩ‚Ωâ ‚Ωâ‚Ωõ ‚ΩõÊ≥â Ê≥âÊòØ Âàô‚ºÄ ÈùôÁªòÂ§Ñ Èùô‚æÉ Âú∞ÁÑ∂‰∫ÜÈï∂Â•á ÂµåËßÇ Âú®‰∏çÔºå Ëøô‰Ωç ‚Ωö‰∫é Ê≤ôÂêåÊï¶ Êº†ÁÖå ‰πãÁîüÂ∏Ç ‰∏≠‚ªÑ ÔºåÂåó Ê≥âÁâ©Á∫¶ ‚Ωî4 Ê∏Ö0‰πãÂÖ¨ Êæà‚æ• ‚ªÖÂ§Ñ Â∫ïÈó¥ÁöÑ ÔºåÊ≤ô ÂΩ¢Êº† Áä∂ÁöÑ‰∏≠ Â¶Ç„ÄÇ Êñ∞Áõ∏Ëøô ‚Ωâ‚æ• ÔºåÂú∞ ÊïÖ‰∫íÂäø Áß∞Âπ≥ ‚Äú‚Ωâ‰ΩúÂù¶ ‚ΩõÔºå Ê≥âÊ≤ôÁî®‚Äù„ÄÇ‰∏ò ÊØèËøû Âà∞Áªµ„ÄÇÂ§úËµ∑ Êôö‰ºèÁîüÔºåÔºå ‚ΩâÂΩ¢ ‰∫ÆÊàêÊÄÅÂçá‰∫Ü Ëµ∑‚ºÄ Êó∂‚Ωö Ôºå‚º¥ ‚ΩâË¢§ ‚Ωõ‚ΩÜ Ê≥âÂû† Âë®ÁöÑ Âõ¥Ê≤ô ‰ºöÊº† ÂèëÊôØ
Âá∫ÈòµÈòµÊ∏ÖËÑÜÁöÑÂìçÂ£∞ÔºåÁäπÂ¶ÇÂ§©Á±Å‰πã‚æ≥Ôºå‰ª§‚ºà‚ººÊó∑Á•ûÊÄ°„ÄÇ
ÁéØÂ¢É‰∏≠ÔºåÂÖîÂ≠êÈÄöËøáÈ£üÁî®Ê§çÁâ©ËåÅÂ£ÆÊàêÈïø„ÄÇ
‰Ωú‰∏∫ÂÖîÂ≠êÁöÑÂ§©ÊïåÔºåÁãêÁã∏ÂíåÈπ∞ÂàôÈÄöËøáÈ£ü
Áî®ÂÖîÂ≠êÊù•‰Ωú‰∏∫È£üÂìÅÈìæÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÊúâÊØí
ÊàëÂàÜÊûêËøôÂº†ÂõæÂÉèÔºåÂèëÁé∞ÂÖ∂‰∏≠ÁöÑÊâãÊéåÂíåÈÅì ËõáÂíåÈπ∞Ëøô‰∏§ÁßçÂ§ßÂûãÊçïÈ£üËÄÖÈÉΩÊçïÈ£üËÄÅÈº†Ôºå
Ë∑ØÊòØ‰∫∫Â∑•Ê∑ªÂä†ÁöÑÂêàÊàêÂÜÖÂÆπÔºåËÄåÂ±±Ë∞∑ÂíåÁæ§ Âπ∂‰∏îÂÆÉ‰ª¨Ëá™Â∑±‰πüÂú®Ëá™ÁÑ∂Áïå‰∏≠ÂØªÊ±ÇÁîüÂ≠ò„ÄÇ
Â±±Â∫îËØ•ÊòØÁé∞ÂÆûÁîüÊ¥ª‰∏≠Â≠òÂú®ÁöÑÁéØÂ¢É„ÄÇÊï¥‰∏™ Èπ∞ÊçïÈ£üËõáÔºåÊØíËõáÊçïÈ£üËõô„ÄÇÊ§çÁâ©ÂíåÂæÆÁîü
Âú∫ÊôØÊòØÂêàÊàêÁöÑÔºåÂπ∂‰∏çÁúüÂÆûÂ≠òÂú®„ÄÇ Áâ©Âú®ÂúüÂ£§‰∏≠ÂàÜËß£Á≤™‰æøÔºåËøô‰∫õÂàÜËß£Áâ©Âæ™
ÁéØÂõûÂúüÂ£§‰∏≠Ôºå‰æõËçâÈ£üÂä®Áâ©Ê∂àËÄó„ÄÇÂú®Ëøô
‰∏™Âæ™ÁéØÂõæ‰∏≠ÔºåÊ§çÁâ©ÊòØÁîü‰∫ßËÄÖÔºåÂÖîÂ≠ê„ÄÅ
ÊØíËõá„ÄÅËÄÅÈº†ÂíåÁãêÁã∏ÊòØÊ∂àË¥πËÄÖÔºåËÄåÈπ∞ÊòØ
È£üÁâ©Èìæ‰∏≠ÊúÄÁõ¥Êé•ÁöÑÊ∂àË¥πËÄÖ„ÄÇÁîüÊÄÅÁ≥ªÁªü
‰øùÊåÅÁîüÊÄÅÂπ≥Ë°°Âπ∂Áª¥ÊåÅÁîüÂëΩ‰ΩìÁöÑÊåÅÁª≠Â≠ò
Âú®ÊòØËá≥ÂÖ≥ÈáçË¶ÅÁöÑ„ÄÇ
**‚ºÜ„ÄÅÈ∏£Ê≤ô‚º≠‚Ωâ‚ΩõÊ≥â**
È∏£Ê≤ô‚º≠‚ΩâËøô‚ΩõÊ≥âÈÅìÊòØ‚ºÄÂ§ÑÈ¢ò‚æÉÁÑ∂ÁöÑÂ•áËßÇÔºåÊï∞‰Ωç‰∫éÂ≠¶Êï¶ÁÖåÂ∏ÇÁ≠î‚ªÑÂåóÊ°àÁ∫¶40ÊòØÂÖ¨‚æ•Â§ÑÂ§öÁöÑÊ≤ôÂ∞ëÊº†‰∏≠„ÄÇËøô‚æ•Âú∞ÂäøÂπ≥Âù¶ÔºåÊ≤ô‰∏òËøûÁªµËµ∑‰ºèÔºåÂΩ¢Êàê‰∫Ü‚ºÄ‚Ωö‚º¥Ë¢§‚ΩÜÂû†ÁöÑÊ≤ôÊº†ÊôØËßÇ„ÄÇ‚ΩΩ
‚Ωâ‚ΩõÊ≥âÂàôÈùôÈùôÂú∞Èï∂ÂµåÂú®Ëøô‚ΩöÊ≤ôÊº†‰πã‰∏≠ÔºåÊ≥â‚ΩîÊ∏ÖÊæà‚ªÖÂ∫ïÔºåÂΩ¢Áä∂Â¶ÇÊñ∞‚ΩâÔºåÊïÖÁß∞‚Äú‚Ωâ‚ΩõÊ≥â‚Äù„ÄÇÊØèÂà∞Â§úÊôöÔºå‚Ωâ‰∫ÆÂçáËµ∑Êó∂Ôºå‚Ωâ‚ΩõÊ≥âÂë®Âõ¥‰ºöÂèëÂá∫ÈòµÈòµÊ∏ÖËÑÜÁöÑ
ÂìçÂ£∞ÔºåÁäπÂ¶ÇÂ§©Á±Å‰πã‚æ≥Ôºå‰ª§‚ºà‚ººÊó∑Á•ûÊÄ°„ÄÇ
ËøôÈÅìÊï∞Â≠¶È¢òÁöÑÁ≠îÊ°àÊòØA„ÄÅ-23
Ê†πÊçÆ‰∏äÈù¢ÁöÑ‰∫∫Áâ©ÂÖ≥Á≥ªÂõæÂõûÁ≠îÔºöÁ•ÅÂêå‰ºüÊòØ
Ë∞ÅÁöÑÂ≠¶ÁîüÔºü **‚ºÜ„ÄÅÈ∏£Ê≤ô‚º≠‚Ωâ‚ΩõÊ≥â**
È∏£Ê≤ô‚º≠‚Ωâ‚ΩõÊ≥âÊòØ‚ºÄÂ§Ñ‚æÉÁÑ∂Â•áËßÇÔºå‰Ωç‰∫éÊï¶ÁÖåÂ∏Ç‚ªÑÂåóÁ∫¶40ÂÖ¨‚æ•Â§ÑÁöÑÊ≤ôÊº†‰∏≠„ÄÇËøô‚æ•Âú∞ÂäøÂπ≥Âù¶ÔºåÊ≤ô‰∏òËøûÁªµËµ∑‰ºèÔºåÂΩ¢Êàê‰∫Ü‚ºÄ‚Ωö‚º¥Ë¢§‚ΩÜÂû†ÁöÑÊ≤ôÊº†ÊôØËßÇ„ÄÇ‚ΩΩ
‚Ωâ‚ΩõÊ≥âÂàôÈùôÈùôÂú∞Èï∂ÂµåÂú®Ëøô‚ΩöÊ≤ôÊº†‰πã‰∏≠ÔºåÊ≥â‚ΩîÊ∏ÖÊæà‚ªÖÂ∫ïÔºåÂΩ¢Áä∂Â¶ÇÊñ∞‚ΩâÔºåÊïÖÁß∞‚Äú‚Ωâ‚ΩõÊ≥â‚Äù„ÄÇÊØèÂà∞Â§úÊôöÔºå‚Ωâ‰∫ÆÂçáËµ∑Êó∂Ôºå‚Ωâ‚ΩõÊ≥âÂë®Âõ¥‰ºöÂèëÂá∫ÈòµÈòµÊ∏ÖËÑÜÁöÑ
ÂìçÂ£∞ÔºåÁäπÂ¶ÇÂ§©Á±Å‰πã‚æ≥Ôºå‰ª§‚ºà‚ººÊó∑Á•ûÊÄ°„ÄÇ
Á•ÅÂêå‰ºüÊòØÈ´òËÇ≤ËâØÁöÑÂ≠¶Áîü„ÄÇ
**‚ºÜ„ÄÅÈ∏£Ê≤ô‚º≠‚Ωâ‚ΩõÊ≥â**
È∏£Ê≤ô‚º≠‚Ωâ‚ΩõÊ≥âÊòØ‚ºÄÂ§Ñ‚æÉÁÑ∂Â•áËßÇÔºå‰Ωç‰∫éÊï¶ÁÖåÂ∏Ç‚ªÑÂåóÁ∫¶40ÂÖ¨‚æ•Â§ÑÁöÑÊ≤ôÊº†‰∏≠„ÄÇËøô‚æ•Âú∞ÂäøÂπ≥Âù¶ÔºåÊ≤ô‰∏òËøûÁªµËµ∑‰ºèÔºåÂΩ¢Êàê‰∫Ü‚ºÄ‚Ωö‚º¥Ë¢§‚ΩÜÂû†ÁöÑÊ≤ôÊº†ÊôØËßÇ„ÄÇ‚ΩΩ
‚Ωâ‚ΩõÊ≥âÂàôÈùôÈùôÂú∞Èï∂ÂµåÂú®Ëøô‚ΩöÊ≤ôÊº†‰πã‰∏≠ÔºåÊ≥â‚ΩîÊ∏ÖÊæà‚ªÖÂ∫ïÔºåÂΩ¢Áä∂Â¶ÇÊñ∞‚ΩâÔºåÊïÖÁß∞‚Äú‚Ωâ‚ΩõÊ≥â‚Äù„ÄÇÊØèÂà∞Â§úÊôöÔºå‚Ωâ‰∫ÆÂçáËµ∑Êó∂Ôºå‚Ωâ‚ΩõÊ≥âÂë®Âõ¥‰ºöÂèëÂá∫ÈòµÈòµÊ∏ÖËÑÜÁöÑ
ÂìçÂ£∞ÔºåÁäπÂ¶ÇÂ§©Á±Å‰πã‚æ≥Ôºå‰ª§‚ºà‚ººÊó∑Á•ûÊÄ°„ÄÇ ‰æØ‰∫ÆÂπ≥ÊòØË∞ÅÁöÑÂ≠¶ÁîüÔºü
‰æØ‰∫ÆÂπ≥ÊòØÈ´òËÇ≤ËâØÁöÑÂ≠¶Áîü„ÄÇ
ËøôÂº†ÂõæËÆ©‰Ω†ÊÉ≥Âà∞‰∫ÜÂì™È¶ñÂè§ËØóÔºü
Figure12.ChatwithInternLM-XComposer2.
21The first column of the image contains
two diamond-shaped objects. The first
diamond shape is completely filled in
with a solid color, while the second one
alsohasanadditionaldotwithinit.
Analyzing this pattern, it seems that the
toprowandbottomrowcontainthesame
typeoffigurebutwithdifferentfeatures-
thetoponeiscompletelyfilled(possibly
Givenacirclewithadiameterof10cm withadifferentcolorortexture),andthe
andtwosmallercirclesofequalradius,
bottomoneincl*‰∫îu„ÄÅdÁâπe‚æäsÁæé‚ªùa**dotinsideit.
Êï¶ÁÖåÁöÑÁâπ‚æäÁæé‚ªù‰πü‚æÆÂ∏∏‰∏∞ÂØåÔºåÂÖ∂‰∏≠ÊúÄÊúâÂêçÁöÑÂΩìÂ±ûÈ©¥‚æÅ‚ª©‚æØ‰∫Ü„ÄÇÈ©¥‚æÅ‚ª©‚æØÊòØ‚ºÄÈÅì‰ª•È©¥‚æÅ‰∏∫‰∏ªË¶Å‚ªùÊùêÁöÑ‚æØ‚ªùÔºåÂë≥ÈÅìÈ≤ú‚æπÂèØ‚ºùÔºåÊ∑±ÂèóÂΩìÂú∞‚ºàÂíåÊ∏∏ÂÆ¢
findtheareaoftheshadedregioninthe ÁöÑÂñúÁà±„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‚Ω∫‚æÅÊ≥°È¶ç„ÄÅËÉ°‚Ω∫ÁÑñÈ•º„ÄÅÁÉ§ÂÖ®‚Ω∫Á≠âÁâπ‚æäÁæé‚ªùÔºåÈÉΩÊòØ‰∏çÂÆπÈîôËøáÁöÑÁæéÂë≥‰Ω≥ËÇ¥„ÄÇ
picture. From this information, we can infer that
the missing image in the second column
shouldbesimilartotheleftmostimagein
the second row‚Äîa star-shaped object
First, we need to find the radius of the with a solid fill. Since the other three
largercircle.Sincethediameteris10cm, imagesarealreadypresentandthepattern
theradiusishalfofthediameter,whichis: suggests repetition of figures with
radius=diameter/2=10cm/2=5cm varying properties, the image in the
secondcolumnwilllikelyconsistofastar
Next,weneedt**o‚ºÜ„ÄÅfÈ∏£iÊ≤ôn‚º≠d‚Ωâ‚ΩõthÊ≥â*e*areasofthetwo withasolidfill.
È∏£Ê≤ô‚º≠‚Ωâ‚ΩõÊ≥âÊòØ‚ºÄÂ§Ñ‚æÉÁÑ∂sÂ•ámËßÇÔºåa‰Ωçl‰∫élÊï¶eÁÖårÂ∏Çc‚ªÑiÂåórÁ∫¶c4l0eÂÖ¨s‚æ•.Â§ÑÁöÑTÊ≤ôhÊº†e‰∏≠„ÄÇrËøôa‚æ•dÂú∞iÂäøuÂπ≥sÂù¶ÔºåoÊ≤ôf‰∏òeËøûaÁªµcËµ∑h‰ºèÔºåsÂΩ¢mÊàê‰∫Üa‚ºÄl‚Ωöle‚º¥Ë¢§r‚ΩÜÂû†ÁöÑÊ≤ôÊº†ÊôØËßÇ„ÄÇ‚ΩΩ
‚Ωâ‚ΩõÊ≥âÂàôÈùôÈùôÂú∞Èï∂ÂµåÂú®Ëøô‚ΩöÊ≤ôÊº†‰πã‰∏≠ÔºåÊ≥â‚ΩîÊ∏ÖÊæà‚ªÖÂ∫ïÔºåÂΩ¢Áä∂Â¶ÇÊñ∞‚ΩâÔºåÊïÖÁß∞‚Äú‚Ωâ‚ΩõÊ≥â‚Äù„ÄÇÊØèÂà∞Â§úÊôöÔºå‚Ωâ‰∫ÆÂçáËµ∑Êó∂Ôºå‚Ωâ‚ΩõÊ≥âÂë®Âõ¥‰ºöÂèëÂá∫ÈòµÈòµÊ∏ÖËÑÜÁöÑ
circle is haÂìçlÂ£∞fÔºåÁäπoÂ¶ÇfÂ§©Á±Åt‰πãh‚æ≥eÔºå‰ª§r‚ºàa‚ººdÊó∑iÁ•ûuÊÄ°s„ÄÇof the large
circle,whichis:
smallerradius=radius/2=5cm/2=
2.5cm
Sotheareaofeachsmallercircleis:
area of smaller circle = œÄ * (smaller
radius)¬≤
area of smaller circle = œÄ * (2.5 cm)¬≤ ‚âà
19.63cm¬≤
The sumoftheareasofthe twosmaller
circlesis:
2*areaofsmallercircle‚âà39.26cm¬≤
Please read the text in this image and return the
Finally, we can find the area of the information in the following JSON format (note
shaded region by subtracting the sum of xxx is placeholder, if the information is not
theareasofthetwosmallercirclesfrom available in the image, put "N/A" instead).
theareaofthelargecircle: {"class":xxx,"DLN":xxx,"DOB":xxx,"Name":
shadedregionarea=areaoflargecircle- xxx, "Address": xxx, "EXP": xxx, "ISS": xxx,
sumofsmallercircleareas "SEX":xxx,"HGT":xxx,"WGT":xxx,"EYES":
shadedregionarea=œÄ*(5cm)¬≤-39.26 xxx,"HAIR":xxx,"DONOR":xxx}
*‰∫î„ÄÅÁâπ‚æäÁæé‚ªù**
cm¬≤‚âà78.54cm¬≤-39.26cm¬≤
Êï¶ÁÖåÁöÑÁâπ‚æäÁæé‚ªù‰πü‚æÆÂ∏∏‰∏∞ÂØåÔºåÂÖ∂‰∏≠ÊúÄÊúâÂêçÁöÑÂΩìÂ±ûÈ©¥‚æÅ‚ª©‚æØ‰∫Ü„ÄÇÈ©¥‚æÅ‚ª©‚æØÊòØ‚ºÄÈÅì‰ª•È©¥‚æÅ‰∏∫‰∏ªË¶Å‚ªùÊùêÁöÑ‚æØ‚ªùÔºåÂë≥ÈÅìÈ≤ú‚æπÂèØ‚ºùÔºåÊ∑±ÂèóÂΩìÂú∞‚ºàÂíåÊ∏∏ÂÆ¢
shadedregionarea‚âà39.28cm¬≤ ÁöÑÂñúÁà±„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‚Ω∫‚æÅÊ≥°È¶ç„ÄÅËÉ°‚Ω∫ÁÑñÈ•º„ÄÅÁÉ§ÂÖ®‚Ω∫Á≠âÁâπ‚æäÁæé‚ªùÔºåÈÉΩÊòØ‰∏çÂÆπÈîôËøáÁöÑÁæéÂë≥‰Ω≥ËÇ¥„ÄÇ
{"class":"D",
"DLN":"D08954796",
"DOB":"01/01/1974",
"Name":"Jelani",
"Address": "123 Main St, Phoenix, AZ
85007",
"EXP":"03/01/2024",
"ISS":"03/01/2016",
"SEX":"M",
Herearethreesubim**a‚ºÜ„ÄÅgÈ∏£eÊ≤ôs‚º≠a‚Ωâ‚ΩõrrÊ≥âa**ngedina2-by-2 "HGT":"5‚Äô09"",
È∏£ ‚ΩΩÊ≤ô ‚Ωâ‚º≠ ‚Ωõ‚Ωâ Ê≥â‚Ωõ ÂàôÊ≥âm ÈùôÊòØ Èùôa‚ºÄ Âú∞tÂ§Ñ Èï∂r‚æÉ ÂµåiÁÑ∂ Âú®xÂ•á Ëøô.ËßÇ ‚ΩöÔºå Ê≤ôF‰Ωç Êº†i‰∫é ‰πãrÊï¶s ‰∏≠tÁÖå Ôºå,Â∏Ç Ê≥â‚ªÑ ‚ΩîlÂåóo Ê∏ÖÁ∫¶ Êæào4 ‚ªÖ0kÂÖ¨ Â∫ï‚æ• ÔºåaÂ§Ñ ÂΩ¢tÁöÑ Áä∂Ê≤ô Â¶ÇtÊº† Êñ∞h‰∏≠ ‚Ωâe„ÄÇ ÔºåËøô ÊïÖt‚æ• Áß∞wÂú∞ ‚Äú‚ΩâÂäøo ‚ΩõÂπ≥ Ê≥âÂù¶ ‚Äùi „ÄÇÔºåm ÊØèÊ≤ô Âà∞‰∏òa Â§úËøûg ÊôöÁªµe ÔºåËµ∑s ‚Ωâ‰ºè ‰∫ÆÔºåi ÂçáÂΩ¢n Ëµ∑Êàê Êó∂‰∫Ü Ôºåt‚ºÄh ‚Ωâ‚Ωöe ‚Ωõ‚º¥ Ê≥âË¢§ Âë®‚ΩÜ Âõ¥Âû† ‰ºöÁöÑ ÂèëÊ≤ô Âá∫Êº† ÈòµÊôØ ÈòµËßÇ Ê∏Ö„ÄÇ "WGT":"185lb",
first column, inËÑÜÁöÑtÂìçhÂ£∞eÔºåÁäπtÂ¶ÇoÂ§©pÁ±Å‰πãl‚æ≥eÔºåf‰ª§t‚ºàa‚ººÊó∑nÁ•ûdÊÄ°„ÄÇbottom left. "EYES":"brown",
Then, findoutthepatterninthefirstcolumn "HAIR":"brown",
based on the left two images. Next, use the "DONOR":"yes"}
foundpatternandtheimageinthetoprightto
inferthemissingfigure.Finally,describewhat
wouldthemissingimagelooklike?
Figure13.ChatwithInternLM-XComposer2.
22Please read the table in this image and return a
markdown-style reconstructed table in text
Describethisimageindetail.
Theimageisablackandwhitefloorplan | Country | Sales Volume | Revenue |
ofahouse,providingadetailedlayoutof Profit|
theroomsandtheirdimensions.Theplan |---------|--------------|---------|--------|
is oriented with the entrance on the | USA | 40.080 | $15.971.880 |
bottomleftand**t‚ºÜh„ÄÅeÈ∏£Ê≤ôt‚º≠e‚Ωâr‚ΩõraÊ≥âc**eonthetopleft.
$3.086.421
**‚ºÜ„ÄÅÈ∏£Ê≤ô‚º≠‚Ωâ‚ΩõÊ≥â**
È∏£Ê≤ô‚º≠‚Ωâ‚ΩõÊ≥âÊòØ‚ºÄÂ§Ñ‚æÉÁÑ∂Â•áËßÇÔºå‰Ωç‰∫éÊï¶ÁÖåÂ∏Ç‚ªÑÂåóÁ∫¶40ÂÖ¨‚æ•Â§ÑÁöÑÊ≤ôÊº†‰∏≠„ÄÇËøô‚æ•Âú∞ÂäøÂπ≥Âù¶ÔºåÊ≤ô‰∏òËøûÁªµËµ∑‰ºèÔºåÂΩ¢Êàê‰∫Ü‚ºÄ‚Ωö‚º¥Ë¢§‚ΩÜÂû†ÁöÑÊ≤ôÊº†ÊôØËßÇ„ÄÇ‚ΩΩ È∏£Ê≤ô‚º≠‚Ωâ‚ΩõÊ≥âÊòØ‚ºÄÂ§Ñ‚æÉÁÑ∂Â•áËßÇÔºå‰Ωç‰∫éÊï¶ÁÖåÂ∏Ç‚ªÑÂåóÁ∫¶40ÂÖ¨‚æ•Â§ÑÁöÑÊ≤ôÊº†‰∏≠„ÄÇËøô‚æ•Âú∞ÂäøÂπ≥Âù¶ÔºåÊ≤ô‰∏òËøûÁªµËµ∑‰ºèÔºåÂΩ¢Êàê‰∫Ü‚ºÄ‚Ωö‚º¥Ë¢§‚ΩÜÂû†ÁöÑÊ≤ôÊº†ÊôØ
‚Ωâ‚ΩõÊ≥âÂàôÈùôÈùôÂú∞Èï∂ÂµåÂú®ËøôS‚ΩötÊ≤ôaÊº†r‰πãt‰∏≠inÔºåÊ≥âg‚ΩîÊ∏ÖfÊæàr‚ªÖo ÂìçÂ∫ïm Â£∞Ôºå ÔºåÂΩ¢ ÁäπÁä∂ Â¶ÇÂ¶Çth Â§©Êñ∞ Á±Åe‚Ωâ ‰πãÔºå ‚æ≥ÊïÖe ÔºåÁß∞n ‰ª§‚Äú‚Ωât ‚ºà‚Ωõr ‚ººaÊ≥â Êó∑‚Äùn„ÄÇ Á•ûcÊØè ÊÄ°eÂà∞ „ÄÇÂ§ú,ÊôöÔºåth‚Ωâ‰∫ÆeÂçáreËµ∑Êó∂Ôºåi‚Ωâs‚ΩõÊ≥âaÂë®Âõ¥‰ºöÂèëÂá∫ÈòµÈòµÊ∏ÖËÑÜÁöÑ ËßÇ„ÄÇ‚ΩΩ‚Ωâ‚ΩõÊ≥âÂàôÈùôÈùô|Âú∞Èï∂CÂµåÂú®hËøôin‚ΩöÊ≤ôaÊº†‰πã Âá∫‰∏≠ ÈòµÔºå|ÈòµÊ≥â Ê∏Ö3‚Ωî ËÑÜÊ∏Ö5ÁöÑÊæà.Âìç0‚ªÖ Â£∞Â∫ï7ÔºåÔºå0ÁäπÂΩ¢ Â¶ÇÁä∂ Â§©Â¶Ç Á±ÅÊñ∞ ‰πã‚Ωâ ‚æ≥Ôºå ÔºåÊïÖ ‰ª§Áß∞ ‚ºà|‚Äú‚Ωâ ‚ºº$‚Ωõ Êó∑Ê≥â Á•û1‚Äù ÊÄ°„ÄÇ5„ÄÇÊØè.8Âà∞Â§ú6Êôö6Ôºå.‚Ωâ6‰∫Æ7Âçá0Ëµ∑Êó∂|Ôºå‚Ωâ‚ΩõÊ≥âÂë®Âõ¥‰ºöÂèë
livingroomtotheright,whichis14'6"x $3.032.162
20'5"insize.Adjacenttothelivingroom | Australia| 27.054 | $14.812.566 |
isthediningarea,whichis9'10"x13'0" $2.888.636
in size. The dining area is connected to | India | 23.046 | $10.608.174 |
the kitchen, which is 7'2" x 9'2" in size. $1.853.710
Nexttothekitchenisthesecondbedroom, | South Korea | 16.032 | $10.494.948 |
which is 10'10" x 14'7" in size. The $1.975.844
masterbedroomislocatedtotherightof
thesecondbedroomandis12'10"x17'9"
in size. The plan also includes a foyer,
which is 6'0" x 5'3" in size, and a
bathroom, which is 7'2" x 9'2" in size.
Theterrace,whichis21'6"x80"insize,
islocatedonthetopleftsideoftheplan.
Thefloorplanprovidesacomprehensive
view of the house's layout, allowing one
tounderstandthespatialarrangementand
dimensionsofeachroom.
Figure14.ChatwithInternLM-XComposer2.
23