underreview
MVDRAG3D: DRAG-BASED CREATIVE 3D EDITING
VIA MULTI-VIEW GENERATION-RECONSTRUCTION
PRIORS
HonghuaChen,YushiLan,YongweiChen,YifanZhou,XingangPan
S-Lab,NanyangTechnologicalUniversity
Drag Mesh APAP PhysGaussian DiffEditor Ours
Drag 3D Gaussians PhysGaussian DiffEditor Ours
Figure 1: Comparison of our MVDrag3D with state-of-the-art approaches. The first two rows
present results of dragging on meshes, while the last two focus on 3D Gaussians. Notably,
APAP (Yoo et al., 2024) is specifically designed for mesh structures, and thus, it was not tested
on3DGaussians. Overall,ourmethoddemonstratestheabilitytoproducemoreplausibleandgen-
erativeeditingresults,showingbetterperformanceacrossboth3DGaussiansandmeshes.
ABSTRACT
Drag-based editing has become popular in 2D content creation, driven by the
capabilities of image generative models. However, extending this technique to
3D remains a challenge. Existing 3D drag-based editing methods, whether em-
ployingexplicitspatialtransformationsorrelyingonimplicitlatentoptimization
within limited-capacity 3D generative models, fall short in handling significant
topology changes or generating new textures across diverse object categories.
Toovercometheselimitations,weintroduceMVDrag3D,anovelframeworkfor
more flexible and creative drag-based 3D editing that leverages multi-view gen-
eration and reconstruction priors. At the core of our approach is the usage of
a multi-view diffusion model as a strong generative prior to perform consistent
drag editing over multiple rendered views, which is followed by a reconstruc-
tion model that reconstructs 3D Gaussians of the edited object. While the ini-
tial 3D Gaussians may suffer from misalignment between different views, we
address this via view-specific deformation networks that adjust the position of
Gaussians to be well aligned. In addition, we propose a multi-view score func-
tion that distills generative priors from multiple views to further enhance the
viewconsistencyandvisualquality. ExtensiveexperimentsdemonstratethatMV-
Drag3D provides a precise, generative, and flexible solution for 3D drag-based
1
4202
tcO
12
]VC.sc[
1v27261.0142:viXraunderreview
editing,supportingmoreversatileeditingeffectsacrossvariousobjectcategories
and 3D representations. Video demos can be found on our project webpage:
https://chenhonghua.github.io/MyProjects/MvDrag3D/.
1 INTRODUCTION
Deforming3Dshapesbydraggingpointhandleshasbeenanessentialinteractivetoolincomputer
graphics, enabling intuitive manipulation of complex shapes and structures. Traditionally, such
drag-based 3D editing is often defined on mesh structures, utilizing optimization functions to pre-
servespecificpropertiesundertheconstraintofcontrolhandles. Thesepropertiesincludethemesh
Laplacian (Lipman et al., 2004; 2005; Sorkine et al., 2004), local rigidity (Igarashi et al., 2005;
Sorkine&Alexa,2007),andsurfaceJacobians(Aigermanetal.,2022;Gaoetal.,2023),aswellas
morerecentconsiderationsofperceptualplausibility(Yooetal.,2024). However,thesemethodsare
constrainedbythefixedtopologyofmeshstructures,limitingtheirflexibility,especiallyincomplex
editsthatrequiresubstantialchangestothetopologyorthegenerationofnewtextures,e.g.,editing
abirdtoopenitswings.
In light of the recently introduced 3D Gaussian splatting (Kerbl et al., 2023) that is more expres-
siveandeasytoedit,Interactive3D(Dongetal.,2024)introducesaseriesofdeformableandrigid
3D operations to directly manipulate local 3D Gaussians. This is followed by Gaussian-to-NeRF
reformattingandrefinementthroughScoreDistillationSampling(SDS)(Pooleetal.,2022). How-
ever, this method suffers from prolonged NeRF optimization and the typical limitations of vanilla
SDS,suchasover-saturation. PhysGaussian(Xieetal.,2024)alsosimulatesdrag-inducedmotion
byintegratingphysicallygroundeddynamicsinto3DGaussians. However, itrequiresanaccurate
predefinition of the physical properties involved, which can be difficult to obtain. Besides, both
methodsstillfacechallengesinmakinglargestructuralchangesandgeneratingnewcontent.
Notably,recentdrag-basededitinghasseenconsiderablesuccessinthe2Ddomain(Panetal.,2023;
Mouetal.,2023;2024;Zhangetal.,2024;Shinetal.,2024),largelyduetothecapabilitiesofpow-
erfulimagegenerativemodels,suchasGANs(Karrasetal.,2020)anddiffusionmodels(Rombach
etal.,2022).Thesemodelsencompassalatentspacethatenablesvariousharmoniousmanipulations,
including object deformation, layout adjustments, and coherent new content generation. Building
on this success, some 3D editing methods have begun to explore generative 3D dragging within a
3Dlatentspace.Forinstance,Drag3D(Tang,2023),adaptsDragGAN(Panetal.,2023)byincorpo-
ratinga3DGAN(Shenetal.,2021)intoamotion-basedlatentoptimizationframework. Similarly,
CNS-Edit (Hu et al., 2024) employs a latent-based method but combines it with a 3D neural vol-
ume diffusion model (Hui et al., 2022). This approach requires training separate models for each
shape category, making it less flexible and more resource-intensive. Obviously, both of the above
approachesarelimitedbythecapacityandgeneralizationofcurrent3Dgenerativemodels.
Inpursuitofastrongergenerativepriorformorepowerfuldrag-based3Dediting,wehaveobserved
thefollowingfromexisting3Dgenerationandreconstructionwork: 1)most3Drepresentationscan
berenderedintomultipleviews; 2)3Dobjectscanbefaithfullyreconstructedfromfourandmore
views(Tangetal.,2024a;Xuetal.,2024b);and3)existingmulti-viewdiffusionmodelsprovidea
strongpriorforgeneratingconsistentimagesacrossfourorthogonalviews(Shietal.,2023b;Kant
et al., 2024). These observations inspire us to explore the potential of leveraging both large-scale
multi-view generation and reconstruction models as 3D priors, agnostic to 3D representations, to
facilitate precise, generative, and general 3D dragging. Ideally, we expect that the 3D dragging
operationshouldexhibitthefollowingproperties1)Accuracy:theabilitytopreciselydraganypoint
ona3Dobject‚Äôssurfacetoatargetspatialposition;2)Generativecapability: theabilitytogenerate
visually plausible new content to match the drag intention; and 3) Versatility: compatibility with
variousinputobjectcategoriesandmost3Drepresentations,suchas3DGaussiansormeshes.
Tothisend,weintroduceMVDrag3D,anovelframeworkfordrag-based3Deditingthatleverages
multi-viewgenerationandreconstructionpriors. Ourmethodbeginsbyrenderingfourorthogonal
views of a 3D object and projecting the dragging points onto the corresponding views. To ensure
consistent 3D edits, we extend the score-based gradient guidance mechanism within a multi-view
diffusionmodelandproposeamulti-viewguidanceenergyfunction,enablingconsistenteditsacross
allfourviews. Thankstothegenerativecapabilitiesofthemulti-viewdiffusionmodel,editsacross
four views can faithfully reflect significant structural changes or newly synthesized textures. The
2underreview
editedviewsarethenfusedintoa3DGaussianrepresentationusingamulti-viewGaussianrecon-
structionmodel. Althoughtheinitial3DGaussianappearscomplete, weobservealossofappear-
ancedetail,andthe3DGaussiansintheoverlappingregionsbetweenviewsdonotalignaccurately,
leadingtonoticeablediscrepanciesinthe2Drendering. Toaddresstheseissues, weemployade-
formation network that predicts the displacement of each Gaussian to correct the 3D alignment.
Additionally,weformulateanimage-conditionedmulti-viewscorefunctiontodistillgenerativepri-
ors from the multiple views simultaneously, ensuring high-fidelity results while preserving details
acrossallviews. Wesummarizeourcontributionsasfollows:
1. We propose MVDrag3D, a drag-based 3D editing framework that leverages multi-view
generation-reconstructionpriors. Itisaccurate, generative, andadaptabletodiverseinput
categoriesandmost3Drepresentations,suchas3DGaussiansandmeshes.
2. Weextendthegradientguidancemechanismintoamulti-viewdiffusionmodelandintro-
duce multi-view guidance energy, which ensures consistent drag-based edits across four
views.
3. Wedesignalightweightdeformationnetworkthatcorrectseach3DGaussian‚Äôspositionand
enhancesgeometricconsistency. Furthermore, weintroduceanimage-conditionedmulti-
viewscorefunctiontoiterativelyrefinethe3DGaussian,ensuringhigh-fidelityappearance
andpreservingfinedetailsacrossallviews.
2 RELATED WORK
Wewillreviewpriorresearch,startingfromdrag-based2Dimageeditingtechniques,andprogress-
ingtomorerecentdevelopmentsindrag-based3Deditingand3Dgeneration-reconstructionpriors.
Drag-based image editing. Drag-based image manipulation allows users to exert precise control
over specific areas of the image via manual interactions like dragging and clicking. Most existing
techniquesemployiterativelatentoptimizationinthelatentspace,andtheycanberoughlydivided
intotwocategories: methodsthatrelyonmotiontracking(Panetal.,2023;Shietal.,2024;Zhang
etal.,2024;Cuietal.,2024;Liuetal.,2024a;Lingetal.,2024)andthosebasedonguidancegradi-
ents(Mouetal.,2023;2024). DragGAN(Panetal.,2023),forinstance,optimizesthelatentspace
of GANs using iterative motion supervision and point tracking. Later, diffusion-based methods,
includingDragDiffusion(Shietal.,2024), GoodDrag(Zhangetal.,2024), StableDrag(Cuietal.,
2024), DragNoise (Liu et al., 2024a), and FreeDrag (Ling et al., 2024), have further refined these
motion-driventechniquesformorerefinedresults. Meanwhile,DragonDiffusion(Mouetal.,2023)
andDiffEditor(Mouetal.,2024)utilizeagradient-basedapproachbyoptimizinganenergyfunc-
tion(Epsteinetal.,2023)toachievedesirededits. Sincebothmotion-andgradient-basedmethods
requiretime-consumingiterations,SDEDrag(Nieetal.,2024)andFastDrag(Zhaoetal.,2024)have
beenproposedtoacceleratetheeditingprocess. Morerecently,InstantDrag(Shinetal.,2024)de-
composesthedraggingtaskintotwocomponents:learningmotiondynamicsandgeneratingimages
conditionedonmotion,achievingabetterbalanceamonginteractivity,speed,andquality.
Drag-based3Dediting. Toachievedrag-based3Dediting,classicalmeshdeformationtechniques
are commonly employed. These methods often design optimization functions to preserve specific
geometricproperties,suchasthemeshLaplacian(Lipmanetal.,2004;2005;Sorkineetal.,2004),
localrigidity(Igarashietal.,2005;Sorkine&Alexa,2007),andsurfaceJacobians(Aigermanetal.,
2022; Gao et al., 2023), under the constraints of user-interactive handles like key points or cages.
Despite their widespread use, these techniques frequently result in unnatural shape distortion, pri-
marilyduetotheirinabilitytoensureperceptualplausibility. Toaddressthislimitation,APAP(Yoo
etal.,2024)introducedaninnovativeapproachbyincorporatingSDSlosstooptimizetheJacobian
deformationfield. However,likepreviousmeshdeformationmethods,APAPisconstrainedbythe
fixedtopologyofmeshstructures,limitingitsflexibility,particularlyforcomplexeditsthatrequire
generating entirely new content. On the other hand, Interactive3D (Dong et al., 2024) introduces
a series of deformable and rigid 3D point operations on 3D Gaussians and also employs SDS to
optimizethedeformedortransformedGaussians/NeRFs. Besides,PhysGaussian(Xieetal.,2024)
alsoinvolvescertaintypesofdrag-relatedmotionbyintegratingphysicallygroundeddynamicsinto
3DGaussians,however,itrequiresasuitablepredefinitionofthephysicsinvolved. Althoughthese
latter two methods employ more expressive 3D representations, they often require labor-intensive
post-processingandfacechallengesinrefiningfinedetailsorgeneratingcoherentnewcontent.
3underreview
As drag-based image editing techniques evolve, some 3D editing methods have begun to explore
generative 3D dragging within a 3D latent space. For instance, Drag3D (Tang, 2023), built upon
DragGAN (Pan et al., 2023), integrates a 3D GAN model into a motion-based latent optimization
framework. However, the approach is inherently limited by the capacity and generalization con-
straintsofcurrent3DGANmodels. Later,CNS-Edit(Huetal.,2024)introducesacoupledneural
shape representation to facilitate 3D shape editing. This method utilizes a latent code to capture
high-level global semantics, while a 3D neural feature volume provides spatial context for local
shape modifications. However, CNS-Edit‚Äôs category-specific design requires separate models for
different 3D shape categories. Different from them, in this work, we achieve 3D generative drag-
gingwithinamorepowerfulmulti-viewlatentspace.
Multi-viewImageGeneration. 2Ddiffusionmodels(Rombachetal.,2022;Sahariaetal.,2022)
initiallyfocusongeneratingasingle-viewimage. Recently,severalmodels(Shietal.,2023b;Wang
& Shi, 2023; Shi et al., 2023a; Li et al., 2023b; Long et al., 2024; Kant et al., 2024; Tang et al.,
2024b;Liuetal.,2024b)turnedtoemploya3D-awaremulti-viewdiffusionapproach,incorporating
camera poses as additional inputs and fine-tuning the diffusion model on multi-view data (Deitke
etal.,2023). Thisstrategyenablestheconsistentgenerationofmulti-viewimagesrepresentingthe
sameobject.Essentially,thesemulti-viewdiffusionmodelscapturearich,generalizabledistribution
of 3D data, agnostic to a specific 3D representation. Also, given the limitations of current ‚Äúpure‚Äù
3D generative models‚Äîthose trained directly on 3D data‚Äîwe believe that leveraging multi-view
diffusionmodelsasa3Dpriorproxycouldofferapromisingsolutionforflexible3Dediting.
Feed-forward Multi-view 3D Reconstruction. By generating 3D-consistent multi-view images,
variousoptimizationtechniquescanbeemployedtoreconstruct3Dobjects(Shietal.,2023b;Wang
& Shi, 2023; Liu et al., 2023). To improve generation speed and quality, more recent work has
explored large-scale reconstruction models using multi-view images (e.g., 4 or 6) (Wang et al.,
2023; Xu et al., 2023; Li et al., 2023a; Wang et al., 2024; Xu et al., 2024a). These approaches
leveragetransformerstodirectlyregresstriplane-basedNeRFrepresentations. Newermethodslike
LGM (Tang et al., 2024a) and GRM (Xu et al., 2024b) replaced triplane NeRF with 3D Gaus-
sians (Kerbl et al., 2023), achieving high-fidelity rendering at faster speeds. In summary, these
recentfeed-forwardmulti-viewreconstructionmodelsprovidearobust3Dreconstructionprior,en-
ablingthefastandfaithfulrecreationofcomplete3Dobjectsfromsparse-viewimages.Inthiswork,
we utilized a 4-view reconstruction model (Tang et al., 2024a) and a 4-view diffusion model (Shi
etal.,2023b)asourgeneration-reconstructionpriors.
3 METHOD
Inthissection, webrieflyintroducescore-basedguidanceenergyforimageediting, followedbya
detailedexplanationofourmethod.
3.1 PRELIMINARY
Score-basedgradientguidanceforimageediting. Recently,DragonDiffusion(Mouetal.,2023)
andDiffEditor(Mouetal.,2024)haveappliedscore-basedgradientguidance(Dhariwal&Nichol,
2021)toefficientandflexibleimage-editingtasks.Thescorefunctionenablessamplingfromamore
enricheddistribution,generallydefinedas:
œµÀút(x )=œµt(x )+Œ∑¬∑‚àá E(x ,y), (1)
Œ∏ t Œ∏ t xt t
where the first term is the unconditional denoiser, and the second term is the conditional gradient
producedbyanenergyfunction. Here,Œ∑ isthelearningrate,andyrepresentstheedittarget,such
as text embedding. During the diffusion sampling process, the gradient guidance from the energy
functionalignswiththeeditingtarget,graduallymodifyingtheinputimagetomeetthedesirededit.
In recent 2D dragging task (Mou et al., 2024; 2023), the guidance energy function is constructed
basedonimagefeaturecorrespondencewithinapre-traineddiffusionmodelasfollows:
‚àá logq(y|z )=Œ±¬∑m ¬∑‚àá E +Œ≤¬∑(1‚àím )¬∑‚àá E , (2)
zt t edit xt edit edit xt content
where m is the editing region mask. The energy function E measures the diffusion feature
edit edit
similaritybetweenareasnearthedraggingstartanddestinationpoints,whileE ensuresthat
content
4underreview
Inversion
PR
noitcejor gniredne
MultiT -ve ix et w-c o gn end eit ri ao tn ioe nd
prior
Camera poses, text prompt:
Input 3D object ùëÄ, dragging points, Multi-view images ‚Äúcrocodile, open mouth‚Äù Multi-view drag results
and text prompt Multi-view gradient guidance for dragging Camera
poses
Final 3D-consistent editing results ‚ÑíLPIPS De nf eo tr wm oa rt kion
R
Image-c go en nd erit ai to in oe nd
p
m riou rlti-view gniredne (ùë•ùëñ,ùë¶ùëñ,ùëßùëñ) recoM nsu trl uti c- tv ioie nw
p rior
‚ÑíSDS Initial 3D Gaussian Initial 3D Gaussian
View 1 View 2 ‚Ä¶ Multi-view reconstruction and Gaussian refinement
Figure2: Methodoverview. Givena3Dmodelandmultiplepairsof3Ddraggingpoints, wefirst
render the model into four orthogonal views, each with corresponding projected dragging points.
Then, to ensure consistent dragging across these views, we define a multi-view guidance energy
withinamulti-viewdiffusionmodel. Theresultingdraggedimagesareusedtoregressaninitialset
of3DGaussians. Ourmethodfurtheremploysatwo-stageoptimizationprocess: first, adeforma-
tionnetworkadjuststhepositionsoftheGaussiansforimprovedgeometricalignment,followedby
image-conditionedmulti-viewscoredistillationtoenhancethevisualqualityofthefinaloutput.
uneditedcontentstaysconsistentwiththeoriginalimage. Œ±andŒ≤arebalanceweights. Inourwork,
we extend both the editing energy and content energy to a multi-view version. This ensures that
modificationsmadeinoneviewarecoherentlyreflectedacrossallviews.
3.2 OVERVIEW
The entire process is visualized in Fig. 2. Given a 3D model M to be edited, and k pairs of 3D
dragging points {(p3D,q3D)}k , we first render M into four orthogonal images I = {I }4 ,
j j j=1 i i=1
along with the corresponding dragging points (Sec. 3.3). We then propose a multi-view guidance
energy function (Sec. 3.4), which ensures consistent and coherent dragging across all views. The
edited images I = {I }4 are used to regress 3D Gaussians using (Tang et al., 2024a). While
e e,i i=1
theinitialreconstructionappearscomplete,wefurtheruseadeformationnetworkandintroducean
image-conditioned multi-view score distillation to correct the misalignment between Gaussians in
theoverlappingregionsofeachviewandenhancethevisualappearanceacrossallviews,resulting
inthefinaleditedresults(representedin3DGaussians)(Sec.3.5).
3.3 3D-2DRENDERINGANDPROJECTION
Wedecomposethe3Ddraggingoperationinamulti-viewmanner. First, werenderthe3Dmodel
M into four orthogonal images {I }4 using any suitable renderer. Since MVDream typically
i i=1
generatesimageswithgraybackgrounds,weadoptasimilargraybackgroundforrendering.Interms
ofcamerasetup,weadoptthesameconfigurationasMVDream(Shietal.,2023b)andLGM(Tang
etal.,2024a),whichserveasourgeneration-reconstructionpriors. Specifically,thefourviewsare
chosenatorthogonalazimuths(0‚ó¶,90‚ó¶,180‚ó¶,270‚ó¶)andafixedelevation(0‚ó¶). Then,thekpairsof
3Ddraggingpointscanbeprojectedontothecorrespondingviews,representedas{(p2D,q2D)}k .
i,j i,j j=1
However,duetopotentialocclusionsincertainviews,wediscardthepointpairsifthez-axisvalue
ofp2D orq2D exceedstherendereddepthatthecorresponding2Dposition.
i,j i,j
3.4 MULTI-VIEWGRADIENTGUIDANCEFORDRAGGING
Sincea3Dobjectcanberenderedintomultipleimagesandnumerousdrag-based2Deditingmeth-
ods already exist, a straightforward approach to achieve drag-based 3D editing would be to inde-
pendentlyediteachviewandthenreconstructthe3Dmodel. However,thisleadstosignificant3D
inconsistencies(seetheresultsofDiffEditor(Mouetal.,2024)inFig.1), astheeditingresultsof
5underreview
(b) Inversion w/o random noise
(a) Inputs (c) Inversion w/ random noise (d) Results w/o (e) Results w/
random noise random noise
Figure3:EffectofDDIMinversionwithrandomnoise.Fortherenderedfourimages,wheninverted
intoMVDream‚Äôsdatadistribution,theresultingnoisedeviatesfromaGaussiandistribution(b). By
adding random noise (N(0,0.01)) to the background‚Äôs pixel domain, we help the latent variables
conformmorecloselytoaGaussiandistribution(c). Theresultingmulti-vieweditsareshownin(d)
and(e). Yellowarrowsindicatetheviewswithevidentidentitychanges.
eachimagebecomemisalignedacrossvariousfactorssuchaspose,layout,texture,andmore.Based
ontheobservationthatmulti-viewdiffusionmodelscansimultaneouslygenerateaconsistentsetof
multi-view images, and recognizing the effectiveness of score-based gradient guidance in image
editing,weextendgradientguidancetoamulti-viewversion.
Specifically, we first apply DDIM inversion (Song et al., 2020) to transform each of {I }4 into
i i=1
a Gaussian distribution. These distributions are combined and represented as z ‚àà R4√óH√óW√óC
T
withinthelatentspaceofMVDream. Usingz ,wecanextractanintermediatefeatureFfromthe
T
UNet decoder. Note that MVDream reshapes z into a 4HW √óC format, thus extending self-
T
attention to the cross-view version. This ensures that guidance from one view can influence the
others. Withthis,wefollow(Mouetal.,2023)anddefineamulti-viewguidanceenergy:
4
(cid:88) 1
E = ,
edit 0.5¬∑cos(cid:0) Fedi[medi], sg(Fori[mori])(cid:1) +0.5
i=1 i,t i i,t i
(3)
4
(cid:88) 1
E = ,
content 0.5¬∑cos(cid:0) Fedi[munedited], sg(Fori[munedited])(cid:1) +0.5
i=1 i,t i i,t i
where Fedi and Fori are intermediate features of zedi and zori. zori corresponds to the latent
i,t i,t i,t i,t i,t
variablesoforiginalimageattimestept,whilezedirepresentstheeditedlatentvariable.sg(¬∑)isthe
i,t
gradientclippingoperation. Inthedraggingoperation,mori (ormedi)isa3√ó3rectangularpatch
centeredaroundthe2Ddraggingpointsp2D (orq2D). munediteddenotestheareaswithoutediting.
Toenhancereadability,theindexlabelsoneachimageareomitted. Notealsothatalllayersofthe
UNetdecoderfeaturesareusedtocomputetheguidanceenergy,ensuringmorecomprehensiveand
robustresults. ThegradientofE isthenusedtogenerateconsistentlyeditedimages{I }4 ,
edit e,i i=1
whileE employedtopreservetheappearanceoftheuneditedregions,keepingthemasclose
content
totheoriginalimagesaspossible.
DDIM inversion with random noise. During DDIM inversion, we observed that for the given
four images, their latent noise does not follow a Gaussian distribution, as depicted in Fig. 3 (b).
Thisdiscrepancyoftencausesinstabilityduringtheeditingprocess, makingitdifficulttopreserve
the object‚Äôs identity (see Fig. 3 (d)). We believe this issue arises because MVDream was never
trained on images with smooth, noise-free regions like the background, leading to a domain gap
during inversion (Ouyang et al., 2024). To address this issue, we found that introducing small,
nearly imperceptible perturbations to the pixel domain‚Äîespecially in smooth areas like the back-
ground‚Äîsignificantly improves the inversion process. These subtle disturbances help the latent
variablesconformmorecloselytoaGaussiandistribution(seeFig.3(c)). Thefinalresultsexhibit
smoothertransitionsandbetteroverallfidelityintheeditedimages,asshowninFig.3(e).
3.5 3DGAUSSIANRECONSTRUCTIONANDREFINEMENT
Onceweobtainthefoureditedimages,weemployLGM(Tangetal.,2024a)toregressapartial3D
Gaussians for each view and then fuse them into a unified 3D Gaussian representation. However,
6underreview
we encountered two significant challenges: (1) because we only use four orthogonal views, the
predicted Gaussians in the overlapping regions between views are usually not aligned correctly,
resulting in noticeable discrepancies in the 2D rendering (see Fig. 4 (c)), and (2) the appearance
detailsarefrequentlylostduringLGM‚Äôsregressionprocess,reducingthevisualfidelityofthefinal
3Dreconstruction(seeFig.5(c)).
In our early tests, to address these issues, we
applied vanilla SDS on the initial reconstruc-
tion,incorporatingamulti-viewreconstruction
loss across the four views. However, these
adjustments did not resolve the underlying is-
sues. We attribute these challenges to the in-
herent ambiguity in the SDS and reconstruc-
(a) Inputs (c) Reconstruction results w/o deformation
tion losses. Specifically, it is difficult to di-
rectly optimize independent Gaussians consis-
tentlywithoutregularization,andthelossesdo
not effectively indicate when to adjust the po-
sition or when to densify or prune the Gaus-
sians,resultinginsuboptimaloutcomes. Toad-
dress these challenges, we propose a two-step
(b) Multi-view drag results (d) Reconstruction results w/ deformation
approach: first, we adjust the Gaussian‚Äôs posi-
tionviadeformationfieldstoachievebetterge- Figure 4: Effect of Gaussian position optimiza-
ometricalignmentandthenfocusonenhancing tion. (c) shows 3D reconstruction result may ex-
visualquality. hibitstructuralmisalignment.Byemployingade-
formation network to optimize the Gaussian po-
Gaussian position optimization. Consider-
sition,weachievebettercompactnessandconsis-
ing that the geometric misalignment problem
tencyamongtheGaussiansacrossdifferentviews,
across views mainly involves low-frequency
asshownin(d).
overall structural changes and the Gaussians
belonging to the same view should be moved
more consistently, for each view‚Äô Gaussian set, we propose to use an individual deformation net-
workf topredicteachGaussian‚Äôsmovement(Œ¥x ,Œ¥y ,Œ¥z ). Thismeansweemployatotaloffour
i i i
lightweightindividualMLPs,oneforeachview. Besides,sincestandardMLPsaregenerallyinef-
fectiveforlow-dimensionalcoordinate-basedregressiontasks(Tanciketal.,2020),weenhancethe
modelbyapplyingFourierpositionalembeddings(pe(¬∑))toeachGaussian‚Äôs(x,y,z)coordinates.
ThenewpositionforeachGaussianisthencalculatedas: (x‚Ä≤,y‚Ä≤,z‚Ä≤)=(x,y,z)+f(pe((x,y,z))).
ThetraininglossistheVGG-basedLPIPSloss,appliedtothefourimages. Thishelpsmaintainper-
ceptual similarity and ensures better alignment across views: L = (cid:80)4 LPIPS(I ,Irender),
LPIPS i=1 e,i e,i
whereIrender istherenderedimagebytheoptimizedGaussiansaftertheirpositionshavebeencor-
e,i
rected. Note that Gaussian densification and pruning are not performed at this stage. Fig. 4 (d)
showstheeffectivenessoftheGaussianpositionoptimizationstage.
Gaussian appearance optimization. The de-
formation network described above is lim-
ited to optimizing the positions of the Gaus-
sians. WhenextendingMLPstooptimizeother
Gaussianproperties,suchassphericalharmon-
ics, we observe no significant improvement
in appearance details. Inspired by ReconFu-
sion(Wuetal.,2024a),weproposetoframethe
Gaussian appearance enhancement task as an
image-conditioned multi-view SDS optimiza-
(a) Inputs (b) Drag results (c) w/o optimization (d) w/ optimization
tionproblem. Ourobjectivesaretwo-fold: (1)
ensuring multi-view consistency across novel Figure 5: Effect of image-conditioned multi-
cameraanglesbeyondtheinitialfourviewsand view SDS. (c) presents the reconstruction results
(2) preserving the identity of the edited four without appearance optimization, while (d) dis-
views. To achieve this, we define the edited- playsthecorrespondingresultsafteroptimization,
imageconditionedmulti-viewscorefunction: whicharenoticeablysharperandclearer.
7underreview
‚àÇIÀÜ
‚àá L =E [(œµ (IÀÜ;t,I ,o)‚àíœµ) ],andi=1,2,3,or4, (4)
œï SDS t,œµ,o Œ∏ e,i ‚àÇœï
where IÀÜrepresents the rendered batch images from any four orthogonal views, and o denotes the
correspondingcameraposes.DuringeachSDSiteration,werandomlyrenderfourorthogonalviews
andrandomlyselectoneeditedimageI asaconditiontocomputetheSDSloss. Themulti-view
e,i
diffusion model employed is ImageDream (Wang & Shi, 2023), which can be seen as an image-
conditionedversionofMVDream. Thisallowsittobeseamlesslyintegratedintoourframework. In
eachiteration,wealsocomputeL . NotethatallGaussianpropertiesareoptimizedduringthis
LPIPS
process,withdensificationandpruningoperationsenabled.
4 EXPERIMENTS
4.1 EXPERIMENTALSETUP
ImplementationDetails. Weconductedallexperimentsonasingle48GBA6000GPU.Formulti-
view image dragging, we employed DDIM sampling with 150 steps, applying random Gaussian
noise N(0,0.01) to the background. In the Gaussian deformation stage, we used 4 MLPs, each
trained for 2,000 iterations with a learning rate of 0.00001. Each MLP consists of a linear layer,
a ReLU activation, and another linear layer arranged in a residual structure. For multi-view SDS
optimization,weperformed1,000iterations,graduallydecayingT from0.49to0.02.
max
Datasets. We perform dragging on two of the most popular 3D representations: meshes and 3D
Gaussians. For the mesh experiments, we collected 8 meshes from (Yoo et al., 2024) and Ge-
nie (Luma AI). For the 3D Gaussian experiments, we collected 8 3D Gaussians from Tang et al.
(2024a). Wecollectdatathatarerepresentativetodemonstratedrageditingbutdonotcherry-pick
based on any results. The 3D drag points are manually specified using MeshLab, following (Yoo
etal.,2024).
Metrics. In this work, we employ two assessment metrics for quantitative evaluation: Dragging
AccuracyIndex(DAI)(Zhangetal.,2024)andGPTEval3D(Wuetal.,2024b). DAImeasuresthe
effectiveness of a method in transferring source content to a target point. While DAI effectively
measures drag accuracy, it is insufficient because the editing process sometimes introduce overall
distortionsorartifacts, resultinginunrealisticorunnaturalresults. Toaddressthis, weuseGPTE-
val3D,whichleveragesGPT-4Vandcustomizable3D-awarepromptstoofferflexiblecomparisons
between two 3D assets based on a set of specific evaluation criteria. For more details about these
metrics,pleaserefertoSec.A.2.
4.2 RESULTS
Baselines. One baseline comparison involves leveraging a 2D drag method to edit each view in-
dependently. In this setup, we use DiffEditor (Mou et al., 2024) to drag the four rendered views,
followedbythesamereconstructionandoptimizationstepsasourstoproducethefinal3Dresults.
Duringourinitialexperiments,weobservedthatwheneditingmuchmorethanfourviews,suchas
120,DiffEditorintroducedsignificant2Dinconsistencies. Thus,forafaircomparison,welimitthe
processto fourimagesas inourapproach. We also compareourmethod withAPAP,the state-of-
the-artdrag-basedmeshdeformationtechnique. Additionally,weincludePhysGaussian(Xieetal.,
2024), which enables user control over Gaussian-based dynamics. For this comparison, we start
witha3Dmodel,renderfourimages,reconstructa3DGaussian,andfeeditintothePhysGaussian
simulator. MoredetaileddragsetupforPhysGaussianpleaserefertoSec.A.3. Notethatasthere-
leasedcodeofInteractive3D(Dongetal.,2024)cannotberunsuccessfully,weareunabletoinclude
itinourcomparisons.Butconceptually,ourapproachprovidesastrongermulti-viewdiffusionprior
comparedtotheSDSlossinInteractive3D,aswecanalsoobserveinourcomparisonwithAPAP.
Visual Comparisons. We first conduct a visual comparison of the proposed MVDrag3D against
baselines, as demonstrated in Fig. 6. The first three rows present results of dragging on meshes,
while the last three rows show results on 3D Gaussians. For each method, we render two views
to highlight the respective editing results. Take the wolf mode in the first row as an example, we
8underreview
Drag Mesh APAP PhysGaussian DiffEditor Ours
Drag 3D Gaussians PhysGaussian DiffEditor Ours
Figure 6: 3D dragging results on meshes and 3D Gaussians. The first three rows show the results
for the mesh, and the last three rows show the results for the 3D Gaussians. Black dashed circles
indicatesomedetaileddifferences.
Table1: Quantitativecomparisonwithstate-of-the-artmethodsonbothmeshesand3DGaussians.
Leftsideof‚Äú/‚Äù: Mesh. Rightside: 3DGaussians. Œ≥ representsthepatchradius,whichdefinesthe
neighborhood around the 2D dragging points. APAP was not tested on 3D Gaussians. In the last
column,wereportaroughaveragerunningtime.
Method Œ≥=1(‚Üì) Œ≥=3(‚Üì) Œ≥=5(‚Üì) Œ≥=7(‚Üì) Œ≥=10(‚Üì) Time
APAP 0.2154/‚Äì 0.2467/‚Äì 0.2150/‚Äì 0.1859/‚Äì 0.1672/‚Äì 6minutes
PhysGaussian 0.1763/0.2468 0.1887/0.2331 0.1671/0.2153 0.1448/0.1979 0.1296/0.1814 1minutes
DiffEditor 0.1564/0.1722 0.1452/0.1735 0.1348/0.1619 0.1299/0.1486 0.1300/0.1358 6minutes
Ours(LGM) 0.1153/0.1702 0.1080/0.1588 0.0989/0.1397 0.0890/0.1260 0.0865/0.1130 3minutes
Ours+deformation 0.1121/0.1269 0.1044/0.1150 0.0975/0.1081 0.0908/0.1017 0.0881/0.0937 5minutes
Ours+deformation+SDS 0.1461/0.1159 0.1292/0.1074 0.1175/0.1020 0.1064/0.0960 0.0994/0.0900 8minutes
aim to lift its left leg. While APAP deforms the leg, it bends rather than lifts it, resulting in a
less realistic motion. In contrast, our method produces an articulation-like motion that is more
natural. DiffEditorgeneratesasuccessfuleditinsomeviews,butothersfail,leadingtoinconsistent
3D results. As for PhysGaussian, it relies on predefined physical properties. Since the optimal
parameters are unknown, its results exhibit some distortion. Additionally, it is unable to generate
newcontent. Formorevisualresults,pleaserefertothesupplementalvideodemo.
Quantitative Comparisons. In addition to the visual comparisons, we conducted a quantitative
evaluationtoassesstheeffectivenessofallcomparedmethodsintermsofdraggingaccuracy(DAI)
and overall editing quality (GPTEval3D). Table 1 reports different methods‚Äô DAI across varying
patchradiusvaluesŒ≥. AsŒ≥ increasesfrom1to10,ourmethod,bothwithandwithoutSDS,shows
consistentlylowererroragainstotherapproacheslikeAPAP,PhysGaussian,andDiffEditor. InTa-
ble 2, the GPTEval3D evaluation reveals that the ‚ÄúOurs + deformation + SDS‚Äù method performs
almost the best across all criteria on both meshes and 3D Gaussians. Notably, we observed that
9underreview
Table2: EvaluationresultsofGPTEval3D.‚ÄúOurs+deformation+SDS‚Äùperformsalmostthebest
acrossallcriteriaonbothmeshesand3DGaussians.
Text-Asset Text-Geometry
Method 3DPlausibility(‚Üë) TextureDetails(‚Üë) GeometryDetails(‚Üë) Overall(‚Üë)
Alignment(‚Üë) Alignment(‚Üë)
Mesh 3DGS Mesh 3DGS Mesh 3DGS Mesh 3DGS Mesh 3DGS Mesh 3DGS
APAP 895.53 ‚Äì 906.63 ‚Äì 961.97 ‚Äì 945.32 ‚Äì 905.80 ‚Äì 917.80 ‚Äì
PhysGaussian 828.46 973.08 870.32 881.52 911.28 950.91 920.78 977.59 898.65 968.70 891.62 979.76
DiffEditor 982.32 883.25 1054.11 924.96 1045.48 868.99 1042.24 894.55 975.34 885.61 992.50 897.78
Ours(LGM) 1074.58 1047.74 1001.04 975.45 1090.78 1011.64 1075.72 959.59 1084.85 1026.61 1041.38 1048.89
Ours+deformation 1023.55 954.67 1060.81 947.32 1012.23 961.58 945.32 1066.18 1051.28 962.77 1066.18 982.10
Ours+deformation+SDS1172.77 1113.36 1139.37 1103.98 1059.67 1122.44 1076.25 1098.33 1109.46 1108.64 1136.80 1100.33
Figure 7: Results of dragging on image-conditioned multi-view diffusion model. We extend the
dragging stage to ImageDream (Wang & Shi, 2023). The results are less flexible as indicated by
blackarrows.
whiletheSDSversionofourmethodmaynotalwaysachievethehighestDAIscore,thisisunder-
standable.TheSDStendstosharpenvisualdetails,whichcanleadtominornumericaldecreases,but
itultimatelyresultsinmorevisuallypleasingoutputs. ThisisfurthersupportedbytheGPTEval3D
results,wheretheSDSversionachievesthehighestscoreintexturedetails.
4.3 ABALATIONANDDISCUSSION
Abalation. We start with the initial reconstruction from (Tang et al., 2024a) as a baseline (Ours
(LGM))andprogressivelyintegrateourtwo-stepoptimizations: (i)Gaussianpositionoptimization
(Ours + deformation), and (ii) image-conditioned multi-view SDS (Ours + deformation + SDS).
Table1presentsaclearcomparisonoftheimpactofeachstageonbothmeshdataand3DGaussians.
Fig.4andFig.5alsovisuallydemonstratetheeffectivenessofourproposedoptimizationstrategy.
Drag on image-conditioned diffusion model. Considering the existence of several image-
conditioned multi-view diffusion models, such as Imagedream (Wang & Shi, 2023) and
Zero123++(Shietal.,2023a),anintuitiveideaistoextendthemulti-viewdraggingstagetothese
models. Here,wespecificallyextendittoImagedream. Fig.7showstwocases. Theconditioning
imageisthefrontviewofeachinput. Underthissetting, weobservethattheresultsarelessvisu-
ally pleasing. We suspect the reason is that the image condition is too strong, thereby restricting
the editing effects. In Mou et al. (2024), the authors introduce the use of both image and text for
fine-grained image editing by tuning a new encoder, enabling a more detailed description of the
desiredchanges. Weseethisasapotentialdirectionforourwork,aimingtoenhanceprecisionand
flexibilityinmulti-viewediting.
5 CONCLUSION
In this work, we introduce MVDrag3D, a novel paradigm that harnesses the power of multi-view
generation-reconstructionpriorsforcreative3Dediting.MVDrag3Dfirstappliesamulti-viewdrag-
gingtechniquetoensureconsistenteditsacrossfourorthogonalviews. Followingthis,areconstruc-
tion model generates 3D Gaussians of the edited object. To refine these initial 3D Gaussians, we
introduce a deformation network that aligns the Gaussians across different views, complemented
byamulti-viewscorefunctiontoenhancevisualsharpnessandconsistency. Extensiveexperiments
showcasetheprecision,generativecapabilities,andflexibilityofourmethod,makingitaversatile
solutionfor3Deditingacrossvariousobjectcategoriesandrepresentations.
10underreview
REFERENCES
Noam Aigerman, Kunal Gupta, Vladimir G Kim, Siddhartha Chaudhuri, Jun Saito, and Thibault
Groueix. Neuraljacobianfields: Learningintrinsicmappingsofarbitrarymeshes. arXivpreprint
arXiv:2205.02904,2022.
YutaoCui,XiaotongZhao,GuozhenZhang,ShengmingCao,KaiMa,andLiminWang.StableDrag:
Stabledraggingforpoint-basedimageediting. arXivpreprintarXiv:2403.04437,2024.
MattDeitke, DustinSchwenk, JordiSalvador, LucaWeihs, OscarMichel, EliVanderBilt, Ludwig
Schmidt,KianaEhsani,AniruddhaKembhavi,andAliFarhadi. Objaverse: Auniverseofanno-
tated3dobjects. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.13142‚Äì13153,2023.
PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advances
inneuralinformationprocessingsystems,34:8780‚Äì8794,2021.
Shaocong Dong, Lihe Ding, Zhanpeng Huang, Zibin Wang, Tianfan Xue, and Dan Xu. Interac-
tive3d: Create what you want by interactive 3d generation. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.4999‚Äì5008,2024.
Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-
guidanceforcontrollableimagegeneration. AdvancesinNeuralInformationProcessingSystems,
36:16222‚Äì16239,2023.
William Gao, Noam Aigerman, Thibault Groueix, Vova Kim, and Rana Hanocka. Textdeformer:
Geometrymanipulationusingtextguidance. InACMSIGGRAPH2023ConferenceProceedings,
pp.1‚Äì11,2023.
JingyuHu, Ka-HeiHui, ZhengzheLiu, HaoZhang, andChi-WingFu. Cns-edit: 3dshapeediting
viacoupledneuralshapeoptimization. arXivpreprintarXiv:2402.02313,2024.
Ka-HeiHui,RuihuiLi,JingyuHu,andChi-WingFu.Neuralwavelet-domaindiffusionfor3dshape
generation. InSIGGRAPHAsia2022ConferencePapers,pp.1‚Äì9,2022.
Takeo Igarashi, Tomer Moscovich, and John F Hughes. As-rigid-as-possible shape manipulation.
ACMtransactionsonGraphics(TOG),24(3):1134‚Äì1141,2005.
YashKant,AliaksandrSiarohin,ZiyiWu,MichaelVasilkovsky,GuochengQian,JianRen,RizaAlp
Guler,BernardGhanem,SergeyTulyakov,andIgorGilitschenski. Spad: Spatiallyawaremulti-
view diffusers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.10026‚Äì10038,2024.
TeroKarras,SamuliLaine,MiikaAittala,JanneHellsten,JaakkoLehtinen,andTimoAila. Analyz-
ingandimprovingtheimagequalityofstylegan. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pp.8110‚Äì8119,2020.
BernhardKerbl,GeorgiosKopanas,ThomasLeimku¬®hler,andGeorgeDrettakis. 3dgaussiansplat-
tingforreal-timeradiancefieldrendering. ACMTrans.Graph.,42(4):139‚Äì1,2023.
Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan
Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view gen-
erationandlargereconstructionmodel. arXivpreprintarXiv:2311.06214,2023a.
WeiyuLi, RuiChen, XuelinChen, andPingTan. Sweetdreamer: Aligninggeometricpriorsin2d
diffusionforconsistenttext-to-3d. arXivpreprintarXiv:2310.02596,2023b.
PengyangLing, LinChen, PanZhang, HuaianChen, YiJin, andJinjinZheng. FreeDrag: Feature
draggingforreliablepoint-basedimageediting. InProc.IEEE/CVFConf.Comput.Vis.Pattern
Recognit.(CVPR),2024.
YaronLipman,OlgaSorkine,DanielCohen-Or,DavidLevin,ChristianRossi,andHans-PeterSei-
del. Differentialcoordinatesforinteractivemeshediting. InProceedingsShapeModelingAppli-
cations,2004.,pp.181‚Äì190.IEEE,2004.
11underreview
YaronLipman,OlgaSorkine,DavidLevin,andDanielCohen-Or. Linearrotation-invariantcoordi-
natesformeshes. ACMTransactionsonGraphics(ToG),24(3):479‚Äì487,2005.
Haofeng Liu, Chenshu Xu, Yifei Yang, Lihua Zeng, and Shengfeng He. Drag your noise: Inter-
activepoint-basededitingviadiffusionsemanticpropagation. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.6743‚Äì6752,2024a.
MinghuaLiu,RuoxiShi,LinghaoChen,ZhuoyangZhang,ChaoXu,XinyueWei,HanshengChen,
Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with
consistentmulti-viewgenerationand3ddiffusion. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.10072‚Äì10083,2024b.
YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,LingjieLiu,TakuKomura,andWenpingWang.
Syncdreamer: Generatingmultiview-consistentimagesfromasingle-viewimage. arXivpreprint
arXiv:2309.03453,2023.
Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma,
Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d
usingcross-domaindiffusion. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.9970‚Äì9980,2024.
GenieLumaAI. Lumaai,genie.
ChongMou,XintaoWang,JiechongSong,YingShan,andJianZhang. Dragondiffusion: Enabling
drag-stylemanipulationondiffusionmodels. arXivpreprintarXiv:2307.02421,2023.
ChongMou,XintaoWang,JiechongSong,YingShan,andJianZhang. Diffeditor: Boostingaccu-
racyandflexibilityondiffusion-basedimageediting.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.8488‚Äì8497,2024.
ShenNie,HanzhongAllanGuo,ChengLu,YuhaoZhou,ChenyuZheng,andChongxuanLi. The
blessingofrandomness: SDEbeatsODEingeneraldiffusion-basedimageediting. InProc.Int.
Conf.Learn.Represent.(ICLR),2024.
WenqiOuyang,YiDong,LeiYang,JianlouSi,andXingangPan. I2vedit: First-frame-guidedvideo
editingviaimage-to-videodiffusionmodels. arXivpreprintarXiv:2405.16537,2024.
Xingang Pan, Ayush Tewari, Thomas Leimku¬®hler, Lingjie Liu, Abhimitra Meka, and Christian
Theobalt. Drag your gan: Interactive point-based manipulation on the generative image mani-
fold. InACMSIGGRAPH2023ConferenceProceedings,pp.1‚Äì11,2023.
BenPoole,AjayJain,JonathanTBarron,andBenMildenhall. Dreamfusion: Text-to-3dusing2d
diffusion. arXivpreprintarXiv:2209.14988,2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¬®rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.10684‚Äì10695,2022.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistic
text-to-imagediffusionmodelswithdeeplanguageunderstanding. Advancesinneuralinforma-
tionprocessingsystems,35:36479‚Äì36494,2022.
TianchangShen,JunGao,KangxueYin,Ming-YuLiu,andSanjaFidler. Deepmarchingtetrahedra:
ahybridrepresentationforhigh-resolution3dshapesynthesis. AdvancesinNeuralInformation
ProcessingSystems,34:6087‚Äì6101,2021.
RuoxiShi,HanshengChen,ZhuoyangZhang,MinghuaLiu,ChaoXu,XinyueWei,LinghaoChen,
Chong Zeng, and Hao Su. Zero123++: a single image to consistent multi-view diffusion base
model. arXivpreprintarXiv:2310.15110,2023a.
YichunShi,PengWang,JianglongYe,MaiLong,KejieLi,andXiaoYang. Mvdream: Multi-view
diffusionfor3dgeneration. arXivpreprintarXiv:2308.16512,2023b.
12underreview
YujunShi,ChuhuiXue,JiachunPan,WenqingZhang,VincentYFTan,andSongBai. DragDiffu-
sion: Harnessingdiffusionmodelsforinteractivepoint-basedimageediting. InProc.IEEE/CVF
Conf.Comput.Vis.PatternRecognit.(CVPR),2024.
Joonghyuk Shin, Daehyeon Choi, and Jaesik Park. Instantdrag: Improving interactivity in drag-
basedimageediting. arXivpreprintarXiv:2409.08857,2024.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprintarXiv:2010.02502,2020.
OlgaSorkineandMarcAlexa. As-rigid-as-possiblesurfacemodeling. InSymposiumonGeometry
processing,volume4,pp.109‚Äì116.Citeseer,2007.
Olga Sorkine, Daniel Cohen-Or, Yaron Lipman, Marc Alexa, Christian Ro¬®ssl, and H-P Seidel.
Laplacian surface editing. In Proceedings of the 2004 Eurographics/ACM SIGGRAPH sympo-
siumonGeometryprocessing,pp.175‚Äì184,2004.
MatthewTancik,PratulSrinivasan,BenMildenhall,SaraFridovich-Keil,NithinRaghavan,Utkarsh
Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn
highfrequencyfunctionsinlowdimensionaldomains.Advancesinneuralinformationprocessing
systems,33:7537‚Äì7547,2020.
JiaxiangTang. Drag3d,2023. URLhttps://github.com/ashawkey/Drag3D.
Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm:
Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint
arXiv:2402.05054,2024a.
Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas
Chandra, Yasutaka Furukawa, and Rakesh Ranjan. Mvdiffusion++: A dense high-resolution
multi-view diffusion model for single or sparse-view 3d object reconstruction. arXiv preprint
arXiv:2402.12712,2024b.
Peng Wang and Yichun Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation.
arXivpreprintarXiv:2312.02201,2023.
PengWang, HaoTan, SaiBi, YinghaoXu, FujunLuan, KalyanSunkavalli, WenpingWang, Zexi-
ang Xu, and Kai Zhang. Pf-lrm: Pose-free large reconstruction model for joint pose and shape
prediction. arXivpreprintarXiv:2311.12024,2023.
ZhengyiWang,YikaiWang,YifeiChen,ChendongXiang,ShuoChen,DajiangYu,ChongxuanLi,
HangSu,andJunZhu. Crm: Singleimageto3dtexturedmeshwithconvolutionalreconstruction
model. arXivpreprintarXiv:2403.05034,2024.
RundiWu, BenMildenhall, PhilippHenzler, KeunhongPark, RuiqiGao, DanielWatson, PratulP
Srinivasan,DorVerbin,JonathanTBarron,BenPoole,etal.Reconfusion:3dreconstructionwith
diffusion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.21551‚Äì21561,2024a.
Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and
GordonWetzstein. Gpt-4v(ision)isahuman-alignedevaluatorfortext-to-3dgeneration. InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.22227‚Äì
22238,2024b.
Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang.
Physgaussian: Physics-integrated 3d gaussians for generative dynamics. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.4389‚Äì4398,2024.
JialeXu,WeihaoCheng,YimingGao,XintaoWang,ShenghuaGao,andYingShan. Instantmesh:
Efficient3dmeshgenerationfromasingleimagewithsparse-viewlargereconstructionmodels.
arXivpreprintarXiv:2404.07191,2024a.
13underreview
Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli,
Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large
reconstructionmodel. arXivpreprintarXiv:2311.09217,2023.
Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and
GordonWetzstein. Grm: Largegaussianreconstructionmodelforefficient3dreconstructionand
generation. arXivpreprintarXiv:2403.14621,2024b.
Seungwoo Yoo, Kunho Kim, Vladimir G Kim, and Minhyuk Sung. As-plausible-as-possible:
Plausibility-awaremeshdeformationusing2ddiffusionpriors. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.4315‚Äì4324,2024.
ZeweiZhang,HuanLiu,JunChen,andXiangyuXu. GoodDrag: Towardsgoodpracticesfordrag
editingwithdiffusionmodels. arXivpreprintarXiv:2404.07206,2024.
XuanjiaZhao, JianGuan, CongyiFan, DongliXu, YoutianLin, HaiweiPan, andPengmingFeng.
Fastdrag: Manipulateanythinginonestep. arXivpreprintarXiv:2405.15769,2024.
A APPENDIX
A.1 ADDITIONALPARAMETERSFORMULTI-VIEWDRAGGING
Formulti-viewimagedragging,parameterssuchastheeditingandcontentenergybalanceweights
Œ±andŒ≤(seeEq.2)andtheclassifier-freeguidance(CFG)needtobeconfigured. Weleavetheseas
openparametersforusers,astheoptimalsettingsmayvarydependingonthespecificedittarget.
A.2 METRICEXPLANATION
DAI.DAImeasurestheeffectivenessofamethodintransferringsemanticcontenttoatargetpoint.
Specifically, it evaluates whether the content at the source position denoted as p , has been suc-
j
cessfully moved to the target location q in the edited 3D object. For each 3D object, the DAI is
j
computedoverfourviewsandconsidersallnon-occludeddraggingpointsasfollows:
DAI=
1(cid:88)4 (cid:88)k (cid:13) (cid:13)I i¬∑‚Ñ¶(p2 i,D j,Œ≥)‚àíI e,i¬∑‚Ñ¶(q i2 ,D
j
,Œ≥)(cid:13) (cid:13)2
2, (5)
4 (1+2Œ≥)2
i=1j=1
where‚Ñ¶(p2D,Œ≥)representsapatchcenteredatp2DwithradiusŒ≥.Eq.5calculatesthemeansquared
i,j i,j
errorbetweenthepatchatp2D ofIandthepatchatq2D ofI . ByadjustingtheradiusŒ≥,themetric
j j e
canfocusondifferentlevelsofcontext. AsmallerŒ≥ providesapreciseevaluationofdifferencesat
the exact control points, while a larger Œ≥ includes a broader region, allowing for an assessment of
thesurroundingcontext. ThisadaptabilitymakesDAIaflexibletoolforexaminingvariousaspects
ofeditingquality. Giventhattheimageresolutionis256√ó256,wesetŒ≥ =1,3,5,7,10.
GPTEval3D.WhileDAIeffectivelymeasuresdragaccuracy,itisnotsufficientonitsownbecause
theeditingprocesscanintroducedistortionsorartifacts,leadingtounrealisticorunnaturalresults.
Therefore,evaluatingthenaturalnessandfidelityoftheeditedimagesiscrucialforacomprehensive
qualityassessment. Thistaskisparticularlychallengingduetotheabsenceofground-truthedited
3D objects for reference. To address this, we utilize GPTEval3D, which leverages GPT-4V with
customizable3D-awareprompts. GPTEval3Dalignswellwithhumanjudgmentacrossseveraldi-
mensions, including text-to-asset alignment, 3D plausibility, texture-‚Äìgeometry coherence, texture
details,andgeometrydetails. Specifically,GPTEval3DpromptsGPT-4Vtocomparetwo3Dassets
generated by different methods using four rendered images and normal maps. The pairwise com-
parisonsarethenusedtocalculateEloratings,whichreflecteachmethod‚Äôsperformance. Formore
details,pleasereferto(Wuetal.,2024b).
Fig.8presentsapairwisecomparisonexampleofGPTEval3Dontwoversionsofourmethod: Ours
(LGM) and the full version, Ours + deformation + SDS. The visual results on the left show that
Ours (LGM) produces somewhat blurry output with noticeable noise in the geometry, particularly
aroundthetailregion.Thiscanbeattributedtothelackofoptimizationprovidedbythedeformation
14underreview
Ours (LGM) Ours (full) GPTEval3D
Analysis:
1. Text prompt & Asset Alignment:
The left model resembles a crocodile, but it appears to have a bulkier, less defined body, and the posture seems less dynamic. The right model has a clearer and
better defined crocodilian shape with accurate limbs and head posture as indicated in the text prompt.
The right one is better.
2. 3D Plausibility:
The left model appears quite bulky and less natural in its proportions. The right model has a more natural stance, body shape, limbs, and head positioning that
are typical of real crocodiles.
The right one is better.
3. Geometry-Texture Alignment:
The left model's texture does not align well with its geometry, particularly in the head and tail regions. The right model‚Äôs texture adheres well to the geometry,
showing a coherent and realistic alignment.
The right one is better.
4. Low-Level Texture Details:
The left model has blurriness in textures, lacking detailed skin patterns. The right model exhibits fine details such as scales and skin texture, and appears more
detailed and aligned with the typical aesthetic of a crocodile.
The right one is better.
5. Low-Level Geometry Details:
The left model's geometry appears noisier and less organized, particularly in the head and forelimb areas. The right model presents more accurate and smooth
geometry without meaningless noise.
The right one is better.
6. Overall:
Considering all the evaluations, the right model is superior in terms of text prompt alignment, 3D plausibility, geometry-texture alignment, low-level texture
details, and low-level geometry details.
The right one is better.
Final answer:
2 2 2 2 2 2
Figure 8: An analysis example of GPTEval3D on two versions of our method: Ours (LGM) and
the full version, Ours + deformation + SDS. The left side of the figure shows selected four-view
resultsfrombothmethods,includingboththeappearanceimageandthenormalmap. Ontheright,
GPT-4V‚Äôsevaluationispresented,whichalignswithhumanobservations. Thefinallineontheright
confirms that the second method, Ours + deformation + SDS, outperforms the first, Ours (LGM),
acrossallfiveevaluationcriteria.
network and SDS in this version. On the right side of the figure, GPT-4V‚Äôs judgment aligns with
ourobservations,concludingthatthesecondmethod,Ours+deformation+SDS,outperformsOurs
(LGM)acrossallfiveevaluationcriteria.
A.3 DRAGSETUPFORPHYSGAUSSIAN
InPhysGaussian(Xieetal.,2024),weusethetranslationfunctionasaproxyforthedragoperation.
Wesetthedragstartingpointsasthecenterpointsandusethedirectionfromthestartingpointstothe
destinationpointstodefinetheinitialvelocity. Foreachdraggingpointpair,weassignatranslation
movement,andthesimulationcontinuesuntileitherthestartingpointreachesthedestinationorthe
iterationcountreachesthesetmaximum(75bydefault).
A.4 RUNNINGTIMESTATISTICS
ThelastcolumnofTable1alsosummarizesthe
rough average running time for each method.
APAP, DiffEditor, and the full version of our
method are slower than PhysGaussian, Ours
(LGM),and‚ÄúOurs+deformation‚Äù,mainlydue
to the absence of SDS optimization in their
pipelines. PhysGaussian runs the fastest since ‚Äúa dachshund with
Input ‚Äúa dachshund‚Äù
mouth open‚Äù
itdoesnotinvolveanyoptimizationprocess.
A.5 TEXTPROMPT
Interestingly, during our early tests, we ob-
served that text input plays a crucial cue for
‚Äúa lotus bud,
generative editing. As shown in Fig. 9, when Input ‚Äúa lotus bud‚Äù
in bloom‚Äù
draggingthedog‚Äôsmouthtoopen,usingamore
specifictextpromptlike‚Äúadachshundwithan Figure 9: Effect of different text prompts. When
openmouth‚Äùcaneffectivelyguidetheprocess. editing images, a text prompt that better aligns
Thisprovesthesignificanceofpromptdesignin withthedragintentioncanhelpquerymoremean-
aligningthediffusionmodel‚Äôsfeatureswiththe ingful features from the diffusion model, ulti-
intendededits. Inallourexperiments,wepro- mately leading to more visually pleasing results.
videamoredetailedtextpromptwhenthedrag Blackdashedcircleshighlighteditdifferences.
15underreview
ÔºàÔºè
„ÄÅ
„ÄÅ Ôºè„ÄÅ
I
ÔºàÔºè „ÄÅÔºø ÔºàÔºè
„ÄÅ
ÔºàÔºè „ÄÅÔºø „ÄÅ.
I
Drag Mesh APAP PhysGaussian DiffEditor Ours
Figure 10: An example of local identity change. In this example, our goal is to drag the owl suit.
Althoughourmethodsuccessfullyclosesthesuit,thetiepartofthesuitchangesduringthemulti-
viewdraggingprocess,asshowninthedashedcircleregion.
intentionisclear. However,forcaseswherethe
intentionislessdefined,weuseamoregeneral
descriptioninstead.
A.6 LIMITATIONS
Despiteachievingconsistentresults,thefour-viewimageeditingprocesssometimesrequiressignif-
icant parameter tuning, highlighting the need for a simpler, more user-friendly multi-view editing
tool, akin to InstantDrag (Shin et al., 2024). Additionally, the editing quality can occasionally al-
ter the object‚Äôs identity (the tie part of the owl suit in Fig. 10), how to achieve more precise local
controlisnon-trivial. Finally,whileweusemulti-viewimagesasa3Dproxy,draggingpointscan
sometimesbecomeoccludedinallviews. Thislimitationmotivatesfutureworkontraininga‚Äúpure‚Äù
3Dgenerativemodeltoenablemoreflexibleandaccurate3Dediting.
16