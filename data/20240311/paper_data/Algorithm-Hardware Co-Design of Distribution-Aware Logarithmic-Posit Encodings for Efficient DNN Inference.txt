Algorithm-Hardware Co-Design of Distribution-Aware
Logarithmic-Posit Encodings for Efficient DNN Inferenceâˆ—
AkshatRamachandran1,ZishenWan1,GeonhwaJeong1,JohnGustafson2,TusharKrishna1
1GeorgiaInstituteofTechnology,Atlanta,GA,2ArizonaStateUniversity,Tempe,AZ
ABSTRACT
TraditionalDeepNeuralNetwork(DNN)quantizationmethods
usinginteger,fixed-point,orfloating-pointdatatypesstruggleto
capturediverseDNNparameterdistributionsatlowprecision,and
oftenrequirelargesiliconoverheadandintensivequantization-
aware training. In this study, we introduce Logarithmic Posits
(LP),anadaptive,hardware-friendlydatatypeinspiredbyposits
thatdynamicallyadaptstoDNNweight/activationdistributions (a) (b)
byparameterizingLPbitfields.Wealsodevelopanovelgenetic- Figure1:(a)WeightdistributionsofResNet50andViT(De:
algorithmbasedframework,LPQuantization(LPQ),tofindoptimal Decoder,En:Encoder)layers,(b)LPâ€™srelative-accuracyplot,
layer-wiseLPparameterswhilereducingrepresentationaldiver- showingdistribution-awarepropertiescomparedtoAF[20].
gencebetweenquantizedandfull-precisionmodelsthroughanovel notefficientenoughforadoptioninresource-constraineddevices.
global-localcontrastiveobjective.Additionally,wedesignaunified Inspiredbytheefficiencyofintegerscombinedwiththebenefits
mixed-precisionLPaccelerator(LPA)architecturecomprisingof offloatsin[7],weproposeLogarithmicPosits(LP),acomposite
processingelements(PEs)incorporatingLPinthecomputational datatypethatblendstheadaptabilityofpositswiththehardwareef-
datapath.Ouralgorithm-hardwareco-designdemonstratesonaver- ficiencyofLNS.LPexploitsthetaperedaccuracyofposits(regime),
age<1%dropintop-1accuracyacrossvariousCNNandViTmodels. exponentsizeandscalefactor(exponentbias)totailortherepresen-
Italsoachievesâˆ¼2Ã—improvementsinperformanceperunitarea tationrange,shapeandpositiontotheDNNparameterdistribution
and2.2Ã—gainsinenergyefficiencycomparedtostate-of-the-art whilecapitalizingonthecomputationalefficiencyofLNS(Fig.1(b)).
quantizationacceleratorsusingdifferentdatatypes. ToutilizeLPforDNNquantization,weintroduceanautomated
1 INTRODUCTION
LPQuantization(LPQ)Frameworkbasedongeneticalgorithms.
LPQoperatesinaPostTrainingQuantization(PTQ)settingwith
Inresponsetotheescalatingcomputationalandstoragedemandsof
accesstoasmallunlabelledcalibrationdataset(128images).Build-
DNNs,compressingmodelsbeforedeployingthemonedgedevices
ingonpreviousworks[2,6,14],weincorporateanovelglobal-local
andcloudservershasbecomeimperative[14,20].Quantization
contrastiveobjectivetocombatover-fittingtothecalibrationdata
hasemergedasoneofthemostpromisingsolutionstoaddressthe
and prevent premature convergence by minimizing divergence
challengesofdeployingDNNsonresource-constraineddevices.
betweenintermediaterepresentations(intermediatelayeroutput
Alongtheselines,numeroustechniquesfocusonuniformquanti-
activations)ofthequantizedandfullprecision(FP)model.
zation,representingvaluesasintegers[19]orfixed-pointnumbers.
ToefficientlyexecutecomputationswithLP,wefurtherpropose
However,asFig.1(a)illustrates,thereissubstantialdistributional
amixed-precisionLPAccelerator(LPA)thatintegratesmixed-
varianceandordersofmagnitudedifferencesinDNNparameters
precisionLPprocessingelements(PEs)intoasystolicarrayarchitec-
betweenlayersandacrossmodels,leadingtosignificantquantiza-
ture.Ourco-designtargetsmixed-precisionquantizationofDNNs.
tionerrorswhenappliedtomodernDNNs[7].
ExtensiveexperimentsonCNNsandViTsdemonstrate<1%dropin
Seeking wider dynamic range and distribution-adaptive data
top-1accuracyacrossmodelfamiliesandsurpassstate-of-the-art
formats,interesthasgrowninnon-uniformquantizationmethods
mixed-precisionaccelerators[7,19]with 2Ã—performanceperunit
involvingfloating-point[14,20],posits[10,16],andlogarithmic
areaimprovementand2.2Ã—energyreduction.
numbersystems(LNS)[1].Adaptivefloating-pointtechniquessuch
as[20]adjusttheexponentrangeempiricallybasedonthedynamic
2 BACKGROUNDANDRELATEDWORK
rangeofparameters.However,theyfailtoadapttothetapered
distributionofDNNparametersanduseflataccuracy.Floating-
StandardPositRepresentation.Thestandardpositformat[8]
pointencodingsalsocomewithincreasedhardwarecomplexity,
withsizeð‘›-bitsincludesthesign,regime(ð‘Ÿ)ofsizers,exponent(ð‘’)
wastedbitpatterns,andconvolutedexceptionhandling,hindering
ofsizees,andfraction(ð‘“)fields.Unlikefloats,positsencodethe
regimefieldusingarun-lengthð‘šof0sor1sterminatedbya1or0,
adoptioninedgedevices[15].
respectively,orbythefinalbit.Theregimevalueð‘˜isdetermined
Recently, posit-based representations have demonstrated ad-
vantagesoverstandardfloatsforDNNinference,offeringtapered
asð‘˜ =âˆ’ð‘šifthefirstbitoftheregimeis0,orð‘˜ =ð‘šâˆ’1otherwise.
Theregimecreatestaperedaccuracyinthepositrepresentation
accuracy(duetorun-lengthencodedregime),providingalarger
(Fig.1(b))unlikefloats,whichhasaflataccuracy.Thisproperty,
dynamicrange,higheraccuracy,andsimplerexceptionhandling
whichisbeneficiallyusedinpriorworkssuchas[10,15],ispartic-
[8,15].Posithardware,thoughcheaperthanfloathardware,isstill
ularlyuseful.InPositNN[11],rsismanuallyconfiguredforDNNs,
âˆ—202461stIEEE/ACMDesignAutomationConference(DAC) butthismaynotguaranteeperformanceacrossdifferentmodels.
4202
raM
8
]RA.sc[
1v56450.3042:viXraI hn anc co en atr da as pt, taw bie lip tyro wp io thse oup ta hra am nde ct re ar fi tz ein dg tua nll inp go ,s wit hb ili et- mfie al id ns tat io nie nn g-
L0
Ste Lp
1
1: Candidate In Lit Niz -a 1lization DPNrNeFc u Misll oiodnel
Mbeatxwimeeizne u innlnikeer cplraosdsuecst
( sf sf sf, )
a simha ilr ad rw aa dr ae p-f tr ai be in lid tyly inre sp tr ae ns de an rt dat Lio Nn S( wSe oc ut li don res q3 ua irn ed a4 c). oA mc bh inie av ti in ong ( L0 sf L1 sf LN-1 sf, ) Population BInaptcuht Intermediate RepresC Ceo onn nc ta ca att ie to en n na aste t e( I I R IR R) bMeitnwimeeizne s iinmnielarr pcrloadsusects
ofa Qr ubi atr na tr iy zab ta is oe ns[ O1] b, jl ee ca td ii vn eg .t Po rc eo vm iop ul sic sa tt ue dd iea srit lh evm ee rt ai gc eci crc ou ni vtr ey n.
- poc pa uS n le adl tie d ioc a nt t et wo s ip tf h r2 o bm est
Ste Cp
h
r2
o
e:
o g s
eR
e
ne eBg
rl
ae
o
tn
c
ioke
n
r foa rtion
CP roe srf s co o hr vm ile d rM tou t pa rti oo dn u & ce
Qu MDa onNdtNi ez led
Step 4: EvaluatioS ne ale nct ded PBl ooc pk ulation Update
tionallossfunctionslikeKL-Divergence,mean-squared-error(MSE),
andcosinesimilarityasaglobalquantizationobjective(finaloutput) Step 3: Diversity Prompting Selection Eva cclu aha li ilt bde rr aea tnl il o g o ne n dn t ae h tr e aated Obtai cn a ndid f ao tr e each poA o pd f u d ( la tion , a sa (nd . .b ..)e ts ot )
todeterminethebestparameters[2,7,14].InourPTQframework,
traditionallossfunctionstendtooverfittothecalibrationdataand G rae nn de ora mte p m aru elt nip tsle Mutate with chilG dre en ne (rate d ,i verse ....)
lackgeneralizationtothetestset.Furthermore,relyingsolelyonthe
finaloutputforthequantizationsearchprocesscanleadtoprema- Figure2:OverviewofLPQFrameworkillustratingthefour
tureconvergenceorsub-optimalsolutionsasthesearchprogresses, majorstepsandevaluationoffitnessfunction.
ignoringtherepresentationalcollapseofintermediatelayeroutputs
RegimeSize(rs).Thisparameterenablesustocontrolthede-
comparedtotheFPmodel.Contrastivelossfunctions,common
greeoftaperingofthenumbersystem(shapeofdistribution)as
inself-supervisedsettings,havebeenprovenbypriorwork[5]to
highlightedinFig.1(b).Standardpositshaveafixedtaperingforall
combatoverfittingbyregularizingagainstnegativesamplesinthe
precisions.ð‘Ÿð‘ isconstrainedtoatmostð‘›âˆ’1bitstoallow1bitfor
testset.Weleverageaglobal-localcontrastiveloss,estimatingthe
thesign.Thefractionfield(ð‘“)occupiestheremainingbits,ifany.
representationaldivergenceofintermediaterepresentations(inter-
Becausethereisanimpliedvaluebeforetheradixpoint(similar
mediatelayeroutputactivations)inadditiontothefinaloutputto
tothehiddenbitinfloats),theabsenceoffractionbitsdoesnot
identifythebestprecision,preventingtherepresentationalcollapse
representzero.
ofthequantizedmodel.(Section.4.1)
ScaleFactorsf.Thescalefactorisacontinuous-valuedparame-
QuantizationAccelerators.ThereisarecentsurgeinDNN
terthatbiasesthescalingoftherepresentationupwardsordown-
inferenceacceleratorsembracingmixed-precisiontechniquesand
wards. By adding a scale factor bias we can shift the region of
noveldatatypes.BitFusionfeaturesfusiblelow-precisioninteger
maximumaccuracyofposits(taperedregion)tothedesiredregion.
PEswithinasystolic-arrayarchitecture[19].AdaptivFloatintro-
Instandardpositsthereisnoscalefactorbiasandthetaperedregion
ducesadaptivefloating-pointquantizationandhybridfloatPEsto
isalwayscenteredatmagnitude0.
mitigateintegerquantizationerrors,albeitwithsubstantialarea
Finally,inspiredbythearithmeticefficiencyandlowhardware
overheads[20].Improvingonpriorworks[19,20],theANT[7]
cost of LNS [1], we augment the parameterized posit represen-
designemploysa4-bitINTPEwithdecoderstosupportfloatcom-
tationabovetoincludeitsadvantages.Weexpressthestandard
putationsonthesameINTPE.Recenteffortsalsoexplorebenefits
positfraction(1.ð‘“)andexponentfieldsinthelogarithmicdomain
ofpositsoverfloatforDNNinference,likethefixed-resolutionposit
as a unified fixed-point exponent of the power of two as 2ð‘’+ð‘“â€²,
MACunitandthemixed-precisionpositPEin[10].Despitepositsâ€™
whereð‘“â€² =ð‘™ð‘œð‘”2(1.ð‘“).ð‘“â€²correspondstothefractionalpartandð‘’
superiorityoverfloats,thehighresourceutilizationinpriorworks
totheintegerpartofthefixed-pointformatwhichwetermUnified
hindersadoptioninresource-constraineddevices.Inspiredby[7]
LogarithmicFractionandExponent(ulfx).
andpositâ€™sadvantages,inthisworkweproposeaLogarithmicPosit
Mathematically,LPcanberepresentedas,
PEdesign,thatexploitsboththehigheraccuracyandadaptability
ð‘¥âŸ¨ð‘›,es,rs,sfâŸ©=sign(ð‘)Ã—22esÃ—ð‘˜+ð‘’âˆ’sfÃ—2ulfx (1)
ofposits(Fig.1(b))andthecomputationalefficiencyofLNS.
4 LPQ:LPQUANTIZATIONFRAMEWORK
WepresentanoverviewofourLPQframeworkinFig.2,whichis
3 LP:LOGARITHMICPOSITS composedoffourstages.
TheproposedLogarithmicPositdatatype(LP)closelyfollowsthe Step1:CandidateInitialization.Aquantizationsolutioncom-
generalschemeofthestandardpositformat[8]whileleveraging prisesanencodedvectorÎ”oflength4ð‘ andeachsetof4values
thecomputational/hardwareefficiencyofLNS[1].Weparameter- representthe4LPparametersofalayerð‘™:Î”[ð‘™] =âŸ¨ð‘› ð‘™,ð‘’ð‘  ð‘™,ð‘Ÿð‘  ð‘™,ð‘ ð‘“ ð‘™âŸ©.
izeseveraladditionalbitfieldsofastandardposit,whichprovides Weconstrainthesearchspaceasfollows,ð‘›within [2,8],esand
fine-grainedcontroloverthedynamicrange,position,andshape rs within [0,ð‘›âˆ’3] and [2,ð‘›âˆ’1].Followingpreviouswork[6],
ofthedistributionofnumberencodingstobetteremulatethehet- whichhighlightsquantizationsensitivitytosmallscaleperturba-
erogeneousweight/activationdistributionsofDNNencodings.The tions,weextendthistosf.Thesf searchspaceforeachlayerð‘™ is
parametersweincorporateare: auniformballofradius10âˆ’3 centeredaroundthemeanweight
Numberofbits (ð‘›). Wedynamicallyadjustthenumberof distributionofthatlayer.Prospectivescalefactorsaresampledas
bits to allow mixed-precision quantization of a DNN to enable sfð‘™ = mean(ð‘™)+ðœ‚(âˆ’10âˆ’3,+10+3),whereðœ‚ isarandomsampling
ustochoosetheoptimalprecisionforalayerandachievehigher function.LPQinitiatesthepopulationbyrandomlysamplingKcan-
compressionrate. didateÎ”vectorsconsistingofdifferentquantizationstrategiesper
ExponentSize(es).ModifyingesallowsLPtoadapttodiverse layer.ThefitnessfunctionLð¹ isevaluatedforeachcandidate(used
dynamicranges.Eachincrementinesdoublesthedynamicrange. toidentifybestcandidatesinlaterstages,explainedinSec.4.1).
esislimitedtoð‘›âˆ’3bitstoallow1-bitsignandatleast2-bitsregime. WecreateKtuples(Î” ð‘˜,Lð‘˜ ð¹)toformtheinitialpopulation.Fitnessvaluesofinitialcandidatesarepre-computedandstoredtoavoid tensorofintermediaterepresentationsfromeachlayeroftheFPand
repeatedevaluations. quantizedmodelbedenotedasHFP ={HF 0P,HF 1P,...,HF ð‘P âˆ’1}and
didS at te ep in2: tR hee- pg oe pn uer laa tt ioio nn is(C raro ns ks eo dve br asa en dd oM nu tht eat fii to nn e) s. sE fa uc nh ctc ia on n- H ouð‘ž tp= ut{ tH enð‘ž 0 s, oH rð‘ž 1 a, f. te. r., pH asð‘ž ð‘ siâˆ’ n1 g} t, hw roh ue gre hH layð‘™ er re ð‘™p .r Hes oe wn ets vet rh ,e usin inte gr Hm Fe Pd ,i Hat ð‘že
andthetoptwocandidatesserveasparentsforgeneratingthenext
directlyisimpracticalduetohighdimensionality.Weaddressthis
candidategeneration(child).Whenevolvingcandidates,perturbing
byapplyingrow-wisepoolingusingKurtosis-3[3]insteadofmean
toomanylayerparametersbasedonparentscanleadtoahigh-
pooling.Kurtosis-3bettercharacterizesdistributiontailednessof
dimensionalsearchspace;tomitigatethis,weemployablock-wise
DNNparameters.Thecontrastiveobjectiveforrepresentational
regenerationapproach,evolvingonlyasubset/blockofsizeBof
divergenceestimationisthusformulatedinEquation.6,closely
childparametersbasedonchosenparents,settingallotherstothe
followingthecontrastivelossdefinitionin[5].
bestparentâ€™sparameters.Thechildâ€™sparameterregenerationforthe
specificblockinvolvesadjustmentsaccordingtoparentcandidates LCO=log(1+ð‘’âˆ’âŸ¨Hð‘ð‘ž,Hð‘FP +âŸ©/ðœâˆ‘ï¸ ð‘’âŸ¨Hð‘ð‘ž,Hð‘FP âˆ’âŸ©/ðœ ) (6)
(ð‘1,ð‘2)andisformulatedas:
ð‘âˆ’
ð‘› =rand(min(ð‘1.ð‘›,ð‘2.ð‘›)âˆ’1,max(ð‘1.ð‘›,ð‘2.ð‘›)+1) (2)
child whereðœcontrolsconcentrationlevel[5]followingthetypicaldefini-
ð‘’ð‘  child=rand(min(p1.es,ð‘2.ð‘’ð‘ )âˆ’1,max(ð‘1.ð‘’ð‘ ,ð‘2.ð‘’ð‘ )+1) (3)
tionusedincontrastivelossliterature;ð‘,ð‘+,andð‘âˆ’arequantized
ð‘Ÿð‘  =rand(0,ceil(mean(ð‘1.ð‘Ÿð‘ ,ð‘2.ð‘Ÿð‘ ))+1) (4) modelpredictiononaparticularimage,thecorrespondingFPmodel
child
ð‘ ð‘“ =mean(ð‘1.ð‘ ð‘“,ð‘2.ð‘ ð‘“)+ðœ‚(âˆ’10âˆ’3,103 ) (5) prediction(positiveexample),andFPmodelpredictionsforallother
child calibrationdataimages(negativeexamples).
Weprefermean()forparametersthatinfluencetheshapeofthe
Wefurtherpenalizehigherbit-widthcandidatesusingaloss
distributionofthenumberencodingsandmin()/max()forparam-
etersthataffectthedynamicrange. thatquantizesthecompressionfactorinspiredby[14]as,LCR =
Step3:DiversityPromotingSelection.Insteadofdirectlyadding (cid:205) ð‘™âˆˆð‘ #PARAM(H ð‘™FP) Ã—ð‘› ð‘™. The complete fitness function LF is
definedasacombinationofthetwocomponentsdefinedabove,
theregeneratedchildbackintothepopulationforuseinthenext
iterationofthesearchprocess,weproposetointroducediversity
balancedbyacoefficientðœ†,LF=LCOÂ·L Cðœ† R.Inourexperiments,
intothepopulationandpreventprematureconvergence.Tothis
weempiricallysettheparameterðœ†=0.4toachievethebestcompro-
misebetweenrepresentationalaccuracyandcompressionfactor.
end,wecreateadditionalrandomparents(empiricallychosento
befiveinthiswork)andusetheregeneratedchildintheprevious
stageastheotherparenttogeneratefivediversechildren. 5 LPA:LP-BASEDDNNACCELERATOR
Step4:EvaluationandPopulationUpdate. Weevaluateall Inthissection,weintroducethemodificationstoasystolicarray
generatedchildreninStep2and3andacquirethefitnessfunction.
architecturetosupportLPandthedesignofanLPPEtonatively
ThechildgeneratedinStep2andthecorrespondingfitnessfunc-
handlemultipleprecisionsandparametersets.
tionvalueisaddedtothepopulation.Wethenrankthediversity
promotingchildreninStep3andselectthebestchildtobeadded
tothepopulationforthenextiteration. 5.1 ArchitectureOverview
Inourblock-wisegeneticalgorithmsearchstrategy,weemploy Fig.3,showsLPAwitharchitecturaloptimizationsforLPsupport
P passesoverthewholeDNN,i.e.overalltheblocksofsizeB, onasystolicarray.Ourdesignoptimizescomputationthroughput
andeachblockisiteratedoverC cyclesineachpass.Therefore, inaweight-stationarydataflow,enablingthemappingofmultiple
thepopulationisupdatedPÃ—Ctimes,i.e.,Steps2,3,and4are low-precisionweightssharinganeastboundinputactivationtoa
executedPÃ—Ctimes. singlePE.
QuantizationforActivation.Afterdeterminingthequanti- WeightandActivationOrganization. Forhardwareefficiency
zationparametersforallDNNweights,weidentifytheLPquanti- andbitpackingduringDNNinferenceonLPA,weconstrainthe
zationvaluesforeachinputactivationinthecorrespondinglayer. LPQsearchspaceofð‘›tointegerpowersof2.Forweights,ð‘›ranges
Activationquantizationsensitivitycloselyalignswiththatofthe from 2 to 8, and corresponding activations are 4- or 8-bit. The
weightparametersproducingthem.TheLPparametersofoutputac- weightsandinputactivationsarestoredas8-bitLPintheircorre-
tivationoflayerð‘™areð‘›ð‘™
ð‘Žð‘ð‘¡
=min(8,ð‘›ð‘™ ð‘¤Ã—2),ð‘’ð‘  ð‘Žð‘™
ð‘ð‘¡
=min(5,ð‘’ð‘ ð‘™ ð‘¤Ã—2), spondingbuffers.Intheinputbuffer,the4-bitactivationsarealso
and sfð‘Žð‘™ ð‘ð‘¡ = sfð‘Žð‘™ ð‘âˆ’ ð‘¡1 +sfð‘¤ð‘™. We find that retaining the regime, i.e. storedasan8-bitvaluebyzero-extendingtheLSB.Theinterpreta-
ð‘Ÿð‘ ð‘™ act=rsð‘¤ð‘™,achievesbestperformance. t pi ro en cio sf ioth ne ow fte hig eh mtb au pff pe er dâ€™s wb eit igp ha tt ste ar nn dd ie sp ie dn ed ns tio fin edth be yq tu ha ent Miz Oat Dio En
4.1 FitnessFunction (providedbythecontroller).EachPEsupportsthreemodesbasedon
Weintroduceafitnessfunction,Lð¹,toevaluatequantizationstrate- thequantizationprecision:MODE-A(four2-bitweights),MODE-B
gies.Itassessestwokeymetrics:intermediaterepresentationdiver- (two4-bitweights),andMODE-C(one8-bitweight).
genceandcompressionratiorelativetotheFPmodel.Therepresen- LPDecoder. WeinsertLPdecodersbetweenon-chipbuffersand
tationdivergencemetricaimstoalignthedistributionofquantized thePEarray,strategicallyplacingthemonlyalongtheboundary
modelâ€™sintermediaterepresentationscloselywiththeFPmodel, tominimizeareaoverhead.Quantizedtensorsarestoredinlow
whilethecompressionratiometricincentivizeslowerbit-widths. precisionbothon-chipandoff-chip,andLPdecodersareemployed
Weformulateacombinedglobal-localcontrastiveobjectivetoad- toconvertthemtoaunifiedformat.Foran8-bitLPweightinthe
dresslimitationsintraditionallossfunctions.Lettheconcatenated WeightBuffer(WB),wedecomposeitintosign(4bits),regimeop7:6op7 op5:4op7op7op5 op3:2op7op3op3 op1:0op7op3op1
Weight Buffer(WB)
rw +: 1 W : Meig Ah Ct, fa r: o mAc ct uiv ra reti no tn P, Er: , MMA OC D eEf sr :o mm p =r e mv. 1 P mE 0,
XOR XOR
1010 m m0
1 XOR
1010 m m0
1 XOR
1010 m m0
1
External
Memory
Input Buffer
(IB)
Unified LP
Decoder
P
PPU
E
EEnifie PP Pd EE
E
LP PP PD EE Eecod PP Pe EE ErM es,O sfD ControllerE
8-bit LP
Input
2's
Complement [MSB-R m 1e ]gim
Zero
<< LZCe
Count
Out
ulfx
constructor
+/m
- <<
u (
R
(1 1l ef 6 6x g-
-
ib b( mi iu t tew s s) ))
(rw) M M MO O OD D DE E E-
-( -ba
i C B
At)
7
op_+ Ln Pg 7 2:6 -m bi1
t
(Am 30 )o Lp P7
4-bi5 t
(B1o )p_ L+ n Pg 5 2:4 -bm it1 (Am 20 )Loo Ppp 57
8-bi3 t
(C0o )p L_+ Png 23: -2 bm it1 (Am 1)0 Lo Pp 3
4-bi1 t
(B0o )p L_+ Png 21 -:0 bm
it
1 (Am 0)0ooo ppp 137
0
Post P Uro nc itessing U On uif ti pe ud t L BP u fE fen rc (o Ode Br )MODE m E Sx it gra nct m `1` sfSign (sw) c_AL 3ZD-4 v_A3 +c_A2LZ vD _- A3 3 v_A2 c_ALZ 1D-2 v_A1 +c_A0LZD v- _1 A1 v_A0
s r ua a a441 rw[15:12] +4 m rw[11:8] 0+4 16-bs m iw
t
4 rr mw[7:4] 0+4 m r 0w[3:0] +4 mrw 16 uw[15:12]ua[3:0] +400ua[3:2] 000ua[3:3] m uw[11:8] 0ua[3:0] +4 B100ua[1:0]
6
i
et-000u ma[2:2]
Ubi
nmu
t
p
w 1
u
au cw[7:4] m06
lk
nua[3:0] + f4 m00ua[3:2] m000ua[1:1] muw[3:0] 0
s
X
sw
mOua[3:0] +
s
R4 a00ua[1:0] 4000u( a[0:0]4 m-bits) M U L S tage (F oa ni )g
P
tau honr ese d
t
c(b 4 oL) P: ne rc v A t: a:
o
vc rao dr ol cu id cn i
e
lt nhv l_
s
eA gi s3 rt ie tZ
n
oc e gt qr0 u o
U
urc ae_ n1 DB n1 io e
tt
if t ze
(
em Pc tt PO i hoxR v
U
e_ reB1
)
pd ( .L a- rp Z
T
tr D
ih
ae e) lc -i ( sPvv s b__ uBA Pi )1 1 mo . Un o0 ic 2 us_0 C â€™ t0 s c+ 1 po1 ucc n_ o tB s0 fimv
g
f_ rB p uv1 oO _ l rv CR mO _ e0 eBR 0 m
d
thbe en
as
Pt ee Edr
-
m Log - Linear Converter arraytoeither4-or8-bitLP,calculateactivationscalefactors,and
sr4 er sr sm Two's Comlf pm l= em (1 e+ nf t) A D D performnon-linearoperations(ReLU/Softmax)similarto[8,9].
rr16 rr < lfm[7:6]lfr[7:6]lfm[5:4]lfA r[l 5i :g 4]n
l
fF mr [a 3c :2t ]io lfrn [s 3:2]lfmlf [1r
:0]lfr[1:0]
S tage
5.2 PEArchitecture
+2 m +2 m +2 m +2 Theweight-stationaryPEinthearrayisdouble-bufferedwiththe
lfr16 0
8-bit
lfr+0
1
0
abilitytostoredecodedweightsforthenextcomputation,allowing
Two's Complement amortizationofdecodingandfillingthesystolic-arrayPEsforeach
rr+1er+1sr+1 lfr+1 computation.EachPEreceivesdecodedactivationsfromtheleft
Figure3:LPAArchitecturedepictingdetailedLPPEandUni- andpartialsumsfromthetopwhicharepropagatedtotheright
fiedLPDecoderunits. andbottomrespectively,everycycle.
Multi-precision.Incontrasttoearliermixed-precisionsystolic
arrayarchitectureslike[7,19],whichemploylow-precisionPro-
(16bits),andulfx(16bits),asillustratedinFig.3.Thedecoding cessingElements(PEs)andcombinemultiplePEstosupporthigher
processbeginswithaunified2â€™scomplementer,highlightedinFig. precisions.Wesuggestastrategyofmappingmultipleweights(de-
4(a)thathandlesmultipleprecisionssimultaneouslycontrolledby pendsonMODE-A/-B/-C)thatsharethesameinputactivationtoa
multiplexers.Theregimeisthendecodedbycountingthenumberof singlePE.Byusinglowerprecisionweightsinalayer,itbecomes
leadingonesorzerosafterthesignbit.Toavoidtheimplementation possibletoassignmoreweightstothesamePE.Thisfacilitates
of both a leading zero and a leading one counter, the binary is theparallelevaluationofmultiplepartialsumsalongthesamePE
invertedaccordingtotheregimeâ€™sfirstbit.TheLZC(Fig.4(b)), column,therebyamortizingtheoverheadofmemoryandcontrol,
similartotheunified2â€™scomplementer,providesthezerocountof andincreasingperformanceperunitarea.
multipleprecisioninputs.Basedonthezerocount,theregimevalue MultiplicationStage. InLP,themultiplicationofweightsand
isshiftedoutfromeachLPusingfoursmallleftshifters.Depending activationsisreplacedbyadditionofulfxandregimes.Asshown
ontheMODE,theshiftedvaluefromoneshiftermaybesenttothe inFig.3,thisisdoneinparallelusing2-setsoffour4-bitadders.
nextshifter. Theactivationâ€™sregimeandulfxtobeaddedarechosenbasedon
Aftershiftingouttheregime,theremainingbitsformtheulfx, theMODE.InMODE-A,eachweighthastobeaddedwitheach
consistingoftheexponentandlog-domainfraction.Theulfxis activation,thereforethecompleteulfxandregimearepassedtothe
interpretedasafixed-pointnumberwithequalbitallocationsto correspondingadderswithnocarrypropagationbetweenadders.
theexponent(integerpart)andlog-domainfraction(fractionpart). Similarly,inMODE-B/-C,theactivationâ€™sulfxandregimebitsare
For example, in MODE-B, the ulfx is interpreted as two fixed- splitintomultiplelowerbit-widthcomponents,zero-extendedand
pointnumbers,eachwith4-bitintegerpartand4-bitfractionpart. passedtoeachoftheweightsindividually.TheMUL-stageresults
Thezerocount(in2â€™scomplement)isalsousedtocalculatethe ina16-bitregime,16-bitulfxandisguaranteedtonotoverflow.
regimevalue,adjustedforscalefactor,andstoredasa16-bitregime The16-bitulfxissplitinto8-bitexponentand8-bitfraction(lnf)
value.Thehigherprecisiontorepresentulfxandregimeinthe forthenextstage.
unifiedformatischosentopreventoverflowduringcalculations. AccumulationStage. Thisstagereceivesthesplitexponent,lnf
Theactivationdecoderfollowsasimilarprocess,excepttheoutputs andthe16-bitregimefromtheMUL-stagealongwiththeexponent,
area1-bitsign,4-bitregime,and4-bitulfx.Thesesizesfacilitate lf(linear-domainfraction),regimeandsignofthepartialsumfrom
easierroutingtotheaddersintheProcessingElement(PE). thepreviousPEinthesamecolumn.Whilemultiplicationinthelog-
LPEncoder. TheunifiedLPencoder,mirroringdecoderprimi- domainischeap,additionisinefficient.Therefore,weconvertlnfto
tivecomponents,performstheinverseoperation,packingLPcom- lf(fractioninthelineardomaini.e.,1.ð‘“).Insteadofimplementing
ponentsintoan8-bitzero-extendedformat.Theencoderisalso anexpensiveLUTbasedconverter,inspiredby[1],weusean8-bit
responsibleforconvertingthelineardomain(lf)fractionsofthe Log-Linearconverterusingasetofgates.Thelogicfunctionfor
partialsumsintothelog-domain(lnf)(explainedlater). theconverterisidentifiedbyusingaKarnaugh-mapsolveronthe
10 10 10 10 10 10 10 10Table1:QuantizationaccuracycomparisonagainstcompetingmethodsonResNet18,ResNet50andMobileNetV2.
ResNet18 ResNet50 MobileNetV2
Method W/A ModelSize(MB) Top-1Accuracy W/A ModelSize(MB) Top-1Accuracy W/A ModelSize(MB) Top-1Accuracy
Baseline 32/32 44.60 71.08 32/32 97.80 77.72 32/32 13.40 72.49
EMQ[4] MP/4 5.50 70.12 MP/5 17.86 76.70 MP/8 1.50 70.75
HAWQ-V3[21] 4/4 5.81 68.45 MP/MP 18.70 75.39 MP/MP 1.68 70.84
AFP[14] - - - MP4.8/MP 13.20 76.09 MP4.8/MP 1.94 70.91
ANT[7] MP/MP 5.87 70.30 MP/MP 14.54 76.70 MP/MP 1.84 70.74
BREC-Q[12] MP/8 5.10 68.88 MP/8 13.15 76.45 MP/8 1.30 68.99
LPQ(Ours) MP4.2/MP5.5 4.10 70.30 MP5.3/MP5.9 14.0 76.98 MP4.1/MP4.98 1.30 71.20
Table 2: Quantization accuracy comparison against SoTA Table3:ComparisonofLPAwithbaselinesunder28nmpro-
methodsonVisionTransformers(ViT-B,DeiT-SandSwin-T).
cesswiththesameon-chipbuffer(512kB(4.2ð‘šð‘š2
)).
ViT-B DeiT-S Swin-T ComputeAreaThroughputComputeDensityTotalArea
Method W/A Top-1Accuracy W/A Top-1Accuracy W/A Top-1Accuracy Architecture Component(Area) (ðœ‡ð‘š2) (GOPS) (TOPS/ð‘šð‘š2) (ð‘šð‘š2)
Baseline 32/32 84.53 32/32 79.80 32/32 81.20 Decoder(5.2ðœ‡ð‘š2)
Evol-Q[6] 4/8 79.50 4/8 77.06 4/8 80.43 LPA Encoder(9.4ðœ‡ð‘š2) 12078.72 203.4 16.84 4.212
FQ-ViT[13] 4/8 78.73 4/8 76.93 4/8 80.73 2/4/8-bitPE(187.43ðœ‡ð‘š2)
LPQ(Ours)MP4.7/MP6.3 80.14 MP3.9/MP5.5 78.01 MP4.5/MP6.2 80.98 ANT 4/8-D bie tc Io nd te Pr E(4 (.9 79ðœ‡ .5ð‘š 72 ðœ‡) ð‘š2) 5102.28 44.95 8.81 4.205
truthtableconstructedforallpossiblelog-linearconversionsand BitFusion 2/4/8-bitPE 5093.75 44.01 8.64 4.205
allpossiblebit-patterninterpretations.Thelinear-logconverterin AdaptivFloat 8-bitPE 23357.14 63.99 2.74 4.223
theencoderisalsoimplementedinasimilarfashionbutwithan
primitivesandotherconventionalrepresentations.LPQisutilized
inversetruthtable.Afterlfisobtained,itistwoâ€™scomplemented
forquantizationofalldatatypes,withmodifiedsearchparameters
throughaunifiedtwoâ€™scomplementer,andasimplefloating-point
suitedtoeachdatatypeforafaircomparison.Figure5(b)illustrates
fractionalignmentandscale-factorshifterlogicisemployed.The
per-layerquantizationerror,measuredwithRootMeanSquared
alignedfractionsareaddedthroughfour2-bitadderstoobtainthe
Error (RMSE), for various data types on ViT-B. LP consistently
accumulatedlfin2â€™scomplementformalongwiththejointregime,
exhibitsthelowestaverageRMSE,outperformingallothernumber
8-bitexponent,and4-bitsign.Thefractionisretainedinthelinear
formats.AdaptivFloatfarespoorlycomparedtoLP,primarilydue
domainandnotjuxtaposedwithexponenttopreventredundant
toitslimitedabilitytoadaptonlythedynamicrange,lackingthe
conversions to linear domain since the partial sum output of a
distributionaladjustmentofferedbyLP.
PEisalwaysprogressivelyaccumulated.Thisiswhytheencoder
ComparisonwithState-of-The-Art(SoTA).Ourmixedpre-
employsalinear-logconverter.
cisionquantizationframework,LPQ,iscomparedagainstvarious
competingworks,bothmixed-precisionanduniform.Resultsare
6 EVALUATION tabulatedinTable1forCNNsandTable2forViTs.LPQconsistently
Inthissection,weevaluatethethreecontributionsofthepaper(LP, outperformsothertechniques,demonstrating<1%averagedropin
LPQandLPA)ontheaspectsofquantizationaccuracy,performance, accuracy.Notably,LPQachievesloweraveragebit-widthsforboth
area,throughput,andenergyefficiency. weightsandactivations,resultinginanaveragecompressionof
BenchmarksandDatasets.Ourexperimentsareconducted 7.5Ã—.Theseoutcomescanbeattributedtotwokeyfactors:1)LPâ€™s
on the ImageNet (ILSVRC2012) dataset, evaluating top-1 accu- dynamicadaptationtotheDNNparameterdistribution,allowing
racyacrossvariousCNNs(ResNet18,ResNet50,MobileNetV2)and forlowerbit-widthtolerance,and2)theproposedfitnessfunction
Transformer-basedmodels(ViT-B,DeiT-S,andSwin-T)forcom- components,whichpreventrepresentationcollapse(contrastive
putervisiontasks.TheFPpre-trainedmodelsfrompytorchcvserve objective)whileencouraginglowerbit-width(costfunction).
asthebaselineforourexperiments. ConvergenceBehavior.Tovalidatetheeffectivenessofthepro-
WeimplementLPQinPyTorchandemployacalibrationdataset posedglobal-localcontrastiveloss,wecompareitagainstcommon
comprising128randomlysampledimagesfromtheImageNettrain- globallossfunctionsâ€”meansquarederror(MSE),KL-divergence,
ingset.Thealgorithmâ€™ssearchparametersareempiricallydeter- andglobalcontrastiveloss[6].InFig.5(a)weobservethatwith
mined:PopulationSize(K)=20,NumberofPasses(P)=10,Number increasingLPQiterations,MSEandKL-Divergencecurvesplateau,
ofCycles(C)=4,andBlockSize(B)issetto4forCNNsandone indicatingoverfittingtothecalibrationdataset.Conversely,the
attentionblockforTransformer-basedmodels. globalcontrastivelossinitiallymatchestheperformanceofthe
LPA, consisting of LP PEs, decoders, and encoders, is imple- global-localcontrastiveobjective.However,asthenumberofitera-
mentedinVerilogRTLandsynthesizedusingSynopsysDesign tionsincreases,theaccuracy-gapwidensbecause,theglobalcon-
CompilerwithaTSMC28nmprocess.LPAiscomparedagainst trastiveobjectivefailstoaccountfortherepresentationalcollapseof
threestate-of-the-artbaselines(referSection.2),ANT[7],BitFusion intermediaterepresentationsasmorelayersundergoquantization.
[19],andAdaptivFloat[20].Forend-to-endperformanceevaluation
ofLPAandallbaselines,wedevelopacycle-accuratesimulatortool 6.2 EffectivenessofLPA
basedonDnnWeaver[18].DeepScale[17]isemployedtoscaleall Area.WecomparetheacceleratorareabreakdownofLPAwiththe
designstothe28nmprocessforafaircomparison. baselinesinTable3.Allacceleratorshaveidenticalconfigurations,
featuringan8Ã—8weightstationarysystolicarraywithsameon-
6.1 EffectivenessofLPQ chipbufferconfiguration.Thereporteddecoderandencoderarea
NumberFormatComparison.LPQemploysanoveldatatype, representsasingleblockforeachrow/columnofthesystolicarray.
LP,consistingoftwoprimitivedatatypesâ€”LNSandposits.We TheAdaptivFloatarchitecture,notsupportingmixed-precisionand
assesstheimpactofLPonquantizationaccuracycomparedtoits limitedto8-bit[20],exhibitssignificantlylargerareautilization(a) (b)
Figure5:(a)LPQperformancewithvariouslossfunctions,(b) Figure6:NormalizedLatencyandEnergycomparisonofLPA.
RMSEdistributionofquantizationerrorofdifferentformats. 7 CONCLUSION
Thispaperpresentsanalgorithm-hardwareco-designfeaturinga
Table4:Impactonperformance,accuracyandenergyeffi-
novelcompositedatatype,LP,whichcombinespositsandLNS.
ciencywithdifferentPEtypesinLPA.
LP dynamically adapts to diverse DNN parameter distributions
PE-type Density(TOPS/ð‘šð‘š2) Top-1Accuracy Efficiency(GOPS/W)
LPA-2/4/8 16.84 76.98 212.17 anddynamicrangesbyconfiguringbitfields.LPQ,anautomated
LPA-8 6.98 77.70 124.26 quantizationframeworkemployinggeneticalgorithmsoptimizes
LPA-2 23.79 0.0 438.96
Posit-2/4/8 3.15 73.65 70.36 LPparametersthroughaglobal-localcontrastiveobjective.Wealso
AdaptivFloat-8 2.74 76.13 71.12 propose LPA that integrates a unified LP PE in a systolic array
architecture.Ourco-designachievesonaverage<1%accuracydrop
duetoitsfloating-pointformat.LPAPEsnativelysupport2/4/8- andsignificantlyimprovesPPAandenergyefficiencycomparedto
bitmixedprecision.Whereas,ANTandBitFusionsupport4-bit SoTAquantizationacceleratorsandframeworks.
and2-bitPEsrespectively,achievingmixed-precisionsupportby
groupingneighboringPEs.DespiteANTandBitFusionexhibiting 8 ACKNOWLEDGEMENTS
lowerareawhencomparedwithLPAforthesamenumberofPEs, ThisworkwassupportedinpartbyCoCoSys,oneofsevencenters
LPAresultsinproportionatelyhigherperformanceperunitarea inJUMP2.0,aSemiconductorResearchCorporation(SRC)program
(TOPS/ð‘šð‘š2)formixed-precisionDNNinference.
sponsoredbyDARPA.
PerformancePerUnitArea(TOPS/ð‘šð‘š2 ).UsingResNet50
astheworkload,wedetermineper-layerquantizationparameters REFERENCES
forLPAandBitFusionusingLPQ.ForANTandAdaptivFloat,we [1] S.A.Alametal.2021.Low-precisionlogarithmicnumbersystems:beyondbase-2.
adheretotheframeworksintheiroriginalpapers.Weensureall
ACMTACO18,4(2021),1â€“25.
[2] Y.Caietal.2020.ZeroqAnovelzeroshotquantizationframework.InCVPR.
baselines use quantization parameters that showcase their best [3] LawrenceTDeCarlo.1997.Onthemeaninganduseofkurtosis.Psychological
possibleaccuracyforafaircomparison.InTable3(column5),we methods2,3(1997),292.
[4] P.Dongetal.2023.EMQ:EvolvingTraining-freeProxiesforAutomatedMixed
presenttheperformanceperunitareaofeachdesignduringquan-
PrecisionQuantization.InCVPR.17076â€“17086.
tizedResNet50inference.LPAachievesnearlya2Ã—improvementin [5] P.Fradkinetal.2022. RobustnesstoAdversarialGradients:AGlimpseInto
performanceperunitareacomparedtoANTandBitFusionforthe theLossLandscapeofContrastivePre-training.InWorkshoponPre-training:
Perspectives,Pitfalls,andPathsForwardatICML2022.
samearchitectureconfiguration.Becausethesearchitecturestend [6] N.Frumkinetal.2023.JumpingthroughLocalMinima:QuantizationintheLoss
tobehaveas8-by-4or8-by-2systolicarraysathigherprecisions LandscapeofVisionTransformers.InCVPR.16978â€“16988.
[7] C.Guoetal.2022.Ant:Exploitingadaptivenumericaldatatypeforlow-bitdeep
(becauseofPEfusion),LPAâ€™sadvantagebecomespronouncedby
neuralnetworkquantization.InMICRO.IEEE,1414â€“1433.
stillmaintainingan8-by-8behavior.TomatchLPAâ€™sperformance, [8] J.GustafsonandI.Yonemoto.2017.Beatingfloatingpointatitsowngame:Posit
ANT/BitFusionwouldneedwidersystolicarrays,8Ã—16or8Ã—24 arithmetic.Supercomputingfrontiersandinnovations4,2(2017),71â€“86.
[9] B.Kelleretal.2023.A95.6-TOPS/WDeepLearningInferenceAcceleratorWith
respectively,offsettingtheirareaadvantage.
Per-VectorScaled4-bitQuantizationin5nm.IJSSC58,4(2023),1129â€“1141.
PerformanceandEnergyComparisonwithBaselines.We [10] H.Langroudietal.2019.Cheetah:Mixedlow-precisionhardware&software
compareLPAwiththebaselinesonViT-BandResNet50,andreport co-designframeworkforDNNsontheedge.arXiv:1908.02386(2019).
[11] H.F.Langroudietal.2019.Positnnframework:Taperedprecisiondeeplearning
thenormalizedexecutiontimeandenergyinFig.6.LPAexhibits inferencefortheedge.In2019(SCC).IEEE,53â€“59.
thelowestlatencyacrossmodels,withamodestincreaseinenergy [12] Y.Lietal.2021.Brecq:Pushingthelimitofpost-trainingquantizationbyblock
consumptionoverANTattributedtooverheadsduetonativemixed-
reconstruction.arXiv:2102.05426(2021).
[13] Y.Linetal.2021.Fq-vit:Post-trainingquantizationforfullyquantizedvision
precisionsupportandconversionlogic. transformer.arXiv:2111.13824(2021).
NumberFormatandMixed-PrecisionComparison.Exam- [14] F.Liuetal.2021.Improvingneuralnetworkefficiencyviapost-trainingquanti-
iningtheimpactonperformance,accuracy,andenergyefficiency
zationwithadaptivefloating-point.InCVPR.5281â€“5290.
[15] R.Murilloetal.2020.DeepPeNSieve:Adeeplearningframeworkbasedonthe
withdifferentPEssupportingsingle-/mixed-precisionforResNet50 positnumbersystem.DSP102(2020),102762.
inTable4,weobservethattheidealscenarioforthebestperfor- [16] A.Ramachandranetal.2022.PositIV:AConfigurablePositProcessorArchitec-
tureforImageandVideoProcessing.In202225thEuromicroDSD.
manceperunitareaandenergyefficiencyoccurswhenalllayersare [17] S.Sarangietal.2021. DeepScaleTool:Atoolfortheaccurateestimationof
quantizedto2-bit(LPA-2),albeitwithpooraccuracy.Conversely, technologyscalinginthedeep-submicronera.InISCAS.IEEE,1â€“5.
thebestquantizationperformanceisachievedwhenalllayersare
[18] H.Sharmaetal.2016.Fromhigh-leveldeepneuralmodelstoFPGAs.InMICRO.
[19] H.Sharmaetal.2018.Bitfusion:Bit-leveldynamicallycomposablearchitecture
quantizedto8-bit(LPA-8),butwithlowerperformanceperunit foracceleratingdeepneuralnetwork.InISCA.IEEE,764â€“775.
areaandenergyefficiency.Despiteincorporatingmixed-precision [20] T.Tambeetal.2020.Algorithm-hardwareco-designofadaptivefloating-point
support,LPA-2/4/8achievesaccuracytendingtotheidealscenario
encodingsforresilientdeeplearninginference.InDAC.IEEE,1â€“6.
[21] Z.Yaoetal.2021.Hawq-v3:Dyadicneuralnetworkquantization.InICML,PMLR.
forbothmetrics,demonstratingabalancedtrade-off.