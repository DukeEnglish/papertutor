DeepSeek-VL: Towards Real-World Vision-Language
Understanding
HaoyuLu*1â€ ,WenLiu*1,BoZhang*1â€¡,BingxuanWang1â€ ,KaiDong1,BoLiu1â€ ,JingxiangSun1â€ ,
TongzhengRen1â€ ,ZhuoshuLi1,YaofengSun1,ChengqiDeng1,HanweiXu1,ZhendaXie1,ChongRuan1
1DeepSeek-AI
{neal, liuwen, bo}@deepseek.com
https://github.com/deepseek-ai/DeepSeek-VL
Abstract
WepresentDeepSeek-VL,anopen-sourceVision-Language(VL)Modeldesignedforreal-world
visionandlanguageunderstandingapplications. Ourapproachisstructuredaroundthreekey
dimensions:
â€¢ DataConstruction: Westrivetoensureourdataisdiverse,scalableandextensivelycovers
real-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based
content(expertknowledge,textbooks),aimingforacomprehensiverepresentationofpractical
contexts. Further, we create a use case taxonomy from real user scenarios and construct an
instructiontuningdatasetaccordingly. Thefine-tuningwiththisdatasetsubstantiallyimproves
themodelâ€™suserexperienceinpracticalapplications.
â€¢ ModelArchitecture: Consideringefficiencyandthedemandsofmostreal-worldscenarios,
DeepSeek-VLincorporatesahybridvisionencoderthatefficientlyprocesseshigh-resolution
images(1024x1024)withinafixedtokenbudget,whilemaintainingarelativelylowcomputa-
tionaloverhead. Thisdesignchoiceensuresthemodelâ€™sabilitytocapturecriticalsemanticand
detailedinformationacrossvariousvisualtasks.
â€¢ Training Strategy: Wepositthat aproficientVision-Language Modelshould, foremost,
possess strong language abilities. To ensure the preservation of LLM capabilities during
pretraining,weinvestigateaneffectiveVLpretrainingstrategybyintegratingLLMtraining
fromthebeginningandcarefullymanagingthecompetitivedynamicsobservedbetweenvision
andlanguagemodalities. Startingwithafocusontext,wegraduallyadjusttheratiotofacilitate
abalancedintegrationofbothmodalities.
TheDeepSeek-VLfamily(both1.3Band7Bmodels)showcasessuperioruserexperiencesasa
vision-languagechatbotinreal-worldapplications,achievingstate-of-the-artorcompetitive
performanceacrossawiderangeofvisual-languagebenchmarksatthesamemodelsizewhile
maintainingrobustperformanceonlanguage-centricbenchmarks. Wehavemadeboth1.3B
and7Bmodelspubliclyaccessibletofosterinnovationsbasedonthisfoundationmodel.
âˆ—Equalcontribution.
â€ WorkdoneduringtheinternshipatDeepSeek-AI.
â€¡Projectlead.
4202
raM
8
]IA.sc[
1v52550.3042:viXraContents
1 Introduction 3
2 DataConstruction 6
2.1 Vision-LanguagepretrainingData . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 SupervisedFine-tuningData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3 Approach 10
3.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2 TrainingPipelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.2.1 Stage1: TrainingVision-LanguageAdaptor . . . . . . . . . . . . . . . . . . 12
3.2.2 Stage2: JointVision-Languagepretraining . . . . . . . . . . . . . . . . . . 13
3.2.3 Stage3: SupervisedFine-tuning . . . . . . . . . . . . . . . . . . . . . . . . 14
3.3 HyperparametersandInfrastructures . . . . . . . . . . . . . . . . . . . . . . . . . 15
4 Evaluation 16
4.1 PublicMultimodalBenchmarksEvaluation . . . . . . . . . . . . . . . . . . . . . . 16
4.2 PublicLanguageBenchmarksEvaluation . . . . . . . . . . . . . . . . . . . . . . . 17
4.3 HumanEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.4 AblationStudy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
5 Conclusion,Limitation,andFutureWork 22
A Appendix 29
21. Introduction
The remarkable success of large language models (LLMs) (Anthropic, 2023; Google, 2023;
OpenAI,2022,2023a)hasfueledthedemandforaversatileinterfacethatcanhandlemultiple
modalitiesbeyondlanguage. Inresponsetothisgrowingdemand,wehaveseenanemergence
ofLargeMultimodalModels(LMMs)likeGPT-4V(OpenAI,2023b)andGemini(Teametal.,
2023),whichserveasversatileassistantscapableofcomprehendingandactinguponinstructions
thatspanvisionandlanguage. Thesemodelsexhibitconsiderablepromiseinexecutingcomplex,
diversereal-worldtasks,enablingmorenaturalandhuman-likeinteractions.
Recently,therehasbeenasurgeofopen-sourcelargemultimodalmodelsaimedatnarrowing
the gap with proprietary counterparts. Substantial strides have been made, especially in
benchmarkperformance,yetasignificantdividepersistsbetweenthemajorityofopen-source
modelsandstate-of-the-artclosed-sourcemodels(Baietal.,2023;Bavishietal.,2023;OpenAI,
2023b; Team et al., 2023) when it comes to real-world performance and user experience. It
remains challenging for the open-source community to develop models with robust general
multimodalcapabilitiesforreal-worldapplications.
Theperformancegapbetweenthemostopen-sourcemodelsandtheproprietarymodelsis
largelypronouncedinreal-worldscenarios,primarilyduetothefollowingreasons:
â€¢ Manyopen-sourcesolutionsallocateasignificantproportionofcomputationalresources
totheinstructiontuningphase. However,theexperienceoftrainingpowerfullanguage
modelsunderscorestheimportanceofextensivepretraininginthedevelopmentofgeneral
intelligence. Toimbuemultimodalmodelswithrichworldknowledge,thereshouldbe
an emphasis on comprehensive pretraining that leverages a broad spectrum of vision-
languagedata.
â€¢ Acommonpracticeistoamalgamatevariousacademicdatasetsduringinstructiontuning.
Whilesuchanapproachmayyieldgoodbenchmarkresults,itoftenfallsshortinproviding
anauthenticreal-worldusageexperience.
â€¢ Intermsofmodelarchitecture,priorworksmostlyadaptavisiontransformer,typically
text-aligned, toa pre-trainedlanguagemodel. However, mostofthesemodels operate
onarelativelylowresolution,e.g.,336Ã—336or448Ã—448. Theintricaciesofcomplexreal-
worldscenarios,suchasopticalcharacterrecognitionortinyobjectdiscernment,demand
high-resolutionprocessingcapability.
â€¢ Whilesomemodels(01-ai,2024;Linetal.,2023;Sunetal.,2023;Wangetal.,2023b)have
beguntoexploitpretraining,theyoftenoverlookthepreservationoflanguageskills. Often,
thereisadegradationoflanguagecapabilityafterprolongedmultimodaltraining. Since
weaimforageneralistthatpossessesstrongcapabilitiesinbothmodalities,thereshould
beatrainingstrategythatwellpreservesthelanguagecapabilitywhendevelopingthe
newmodalityability.
Inlightofthese,wepresentDeepSeek-VL,anopen-sourcelargemultimodalmodel,which
is built upon the DeepSeek language model series. We develop the model in the pursuit of
adeptperformanceinreal-worldscenarios,whichinvolvesextensivepretraining,carefuldata
curationbasedonausecasetaxonomy,modelarchitecturedesignforhigh-resolutionprocessing,
and a training strategy that balances the multi-modalities. On top of these, we develop a
training methodology that steers the model scaling, from 1B to 7B. These comprehensive
explorationsbringasignificantperformanceadvantageinpracticalsettings,comparedtoother
largemultimodalmodels(LMMs)ofsimilarsize.
3Prompt Response Prompt Response
å›¾ç‰‡æè¿°äº†ä»€ä¹ˆç®—æ³•ï¼Ÿ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªåä¸ºCLIPçš„ç®—æ³•çš„æµç¨‹å›¾ã€‚è¯¥ç®—æ³•è¢«æè¿° You are a home robot. Now you are at the kitchen, this To fetch something from the fridge, I would follow these steps:
ä¸ºä¸€ç§æ–°çš„é›¶æ ·æœ¬å›¾åƒåˆ°å›¾åƒè½¬æ¢æ–¹æ³•ï¼Œå®ƒä½¿ç”¨æ–‡æœ¬æè¿° is what you can see from there. To fetch something Approach the refrigerator: The refrigerator is located on the
æ¥é¢„æµ‹ç›®æ ‡æ•°æ®é›†çš„ç±»ã€‚ from the fridge, what should you do? right side of the kitchen island. It's a stainless steelmodel with
åœ¨æµç¨‹å›¾ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼š a freezer compartment at the bottom and an ice maker
1.Contrastive pre-trainingï¼ˆå¯¹æ¯”å¼é¢„è®­ç»ƒï¼‰ï¼šåœ¨è¿™ dispenser above it.
ä¸ªé˜¶æ®µï¼Œæ¨¡åž‹è¢«è®­ç»ƒä»¥æœ€å°åŒ–ä¸¤ä¸ªç¼–ç å™¨ä¹‹é—´çš„å·®å¼‚ï¼Œä¸€ Open the refrigerator door: To open the refrigerator door, I
ä¸ªç”¨äºŽå›¾åƒï¼Œå¦ä¸€ä¸ªç”¨äºŽæ–‡æœ¬ã€‚è¿™é€šè¿‡å°†å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ would use my hands to grasp the handle or pull bar located on
æŠ•å½±åˆ°ç›¸åŒçš„æ½œåœ¨ç©ºé—´ä¸­æ¥å®žçŽ°ã€‚ the front edge of the refrigerator door. This action should be
2.Create dataset classifier from label textï¼ˆåˆ›å»º done carefully to avoid any accidental bumps into other items
æ ‡ç­¾æ–‡æœ¬åˆ†ç±»å™¨ï¼‰ï¼šè¿™ä¸ªæ­¥éª¤æ¶‰åŠä»Žç»™å®šçš„æ ‡ç­¾æ–‡æœ¬ä¸­æ in the kitchen.
å–ç‰¹å¾å‘é‡ï¼Œå¹¶å°†å…¶ä¸Žå›¾åƒåµŒå…¥ä¸€èµ·è¾“å…¥åˆ°ä¸€ä¸ªåˆ†ç±»å™¨ä¸­ã€‚ Locate the desired item: Once inside the refrigerator, I would
3.Use for zero-shot predictionï¼ˆç”¨äºŽé›¶æ ·æœ¬æ–‡æœ¬é¢„ scan through the shelves and compartments for the specific
æµ‹ï¼‰ï¼šæœ€åŽï¼Œä½¿ç”¨å‰é¢æ­¥éª¤ä¸­å­¦ä¹ çš„æ¨¡åž‹è¿›è¡Œé›¶æ ·æœ¬æ–‡æœ¬ item I am looking for. The refrigerator has multiple shelves and
é¢„æµ‹ã€‚è¿™æ¶‰åŠåˆ°å°†å›¾åƒåµŒå…¥å’Œæå–çš„ç‰¹å¾å‘é‡ä¼ é€’ç»™åˆ†ç±» drawers that are likely organized by type of food or drink.
å™¨ï¼Œä»¥é¢„æµ‹å›¾åƒå±žäºŽå“ªä¸ªç±»åˆ«ã€‚ Retrieve the item: After locating the desired item, I would
æ•´ä¸ªè¿‡ç¨‹éƒ½å›´ç»•ç€å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿç†è§£å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´å…³ç³» reach out and grab it using one hand while ensuring not to
çš„æ¨¡åž‹è¿™ä¸€æ ¸å¿ƒæ¦‚å¿µã€‚è¿™ç§æ–¹æ³•çš„å…³é”®åœ¨äºŽåˆ©ç”¨å¤§é‡çš„æ—  disturb the organization of the contents. If the item is placed
æ ‡æ³¨æ•°æ®ï¼Œé€šè¿‡å¯¹æ¯”å¼çš„é¢„è®­ç»ƒæ¥æé«˜æ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ on a shelf or drawer, I would lift it up gently to remove it
åœ¨æµç¨‹å›¾çš„åº•éƒ¨ï¼Œæœ‰ä¸€ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨CLIPè¿›è¡Œé›¶ without knocking anything over.
æ ·æœ¬é¢„æµ‹ã€‚è¿™é‡Œï¼Œä¸€å¼ ç‹—çš„ç…§ç‰‡å’Œä¸€ä¸ªå…³äºŽç‹—çš„çŸ­è¯­è¢«ç”¨ Close the refrigerator door: After retrieving the item, I would
æ¥é¢„æµ‹è¿™å¼ ç…§ç‰‡ä¸­çš„ç‹—å±žäºŽå“ªä¸ªå“ç§ã€‚ close the refrigerator door securely using the handle or pull bar.
Prompt æ€» ä½œçš„ åŽŸæ¥ ç†è¯´ ï¼Œï¼Œ å¼ºæµ è°ƒç¨‹ äº†å›¾ å®ƒæ¸… åœ¨æ™° å¤„åœ° ç†æ å¤§ç»˜ é‡äº† æœªæ ‡CL è®°IP æ•°ç®— æ®æ³• æ–¹çš„ é¢ç»“ çš„æž„ æ½œå’Œ åŠ›å®ƒ ä»¥çš„ åŠå·¥ T aph pis
l
ie an ns cu er e ans dth mat
a
t inh te
a
ic no sl d
it
a
s
i tr
e
r mem pea ri an ts
u
t rr ea p ep ffe icd
ie
w ni ct yh .in the
å…¶å¯¹é›¶æ ·æœ¬ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚ Prompt Return to the task at hand: With the item retrieved from the
Which bathroom is the bedroom A closer to? fridge, I can now proceed with whatever activity I was doing
Tell me the main contribution of this paper. before, such as cooking or preparing a meal.
Response
Response
Bedroom A iscloser to Bathroom A.
This can be determined by observing the floor plan
provided in the image. The layout of the rooms and The main contribution of this paper, as stated in the abstract, is
bathrooms are clearly marked with labels, allowing us to to develop a cognitive model that integrates individual
identify their relative positions. Bedroom A is located on differences into human-automated vehicle interaction. The
the left side of the floor plan, while Bathroom A is situated authors aim to provide insights into how humans interact with
directly below it, towards the bottom-left corner of the new technologies and propose a framework for understanding
image. This positioning indicates that there is no other these interactions. They also discuss potential pitfalls
bathroom between them, making Bathroom A the closest associated with using a one-size-fits-all model and explain how
one for Bedroom A. modeling individual differences can help design effective user
interfaces and interventions. Additionally, they conduct a
simulation study to demonstrate possible benefits from
incorporating individual differences into the design process.
This conclusion is drawn directly from the text under the
Response "ABSTRACT" section, where it outlines the purpose and
contributions of the research.
The flowchart provided is a simple decision-making process
that compares two variables, x and y, to determine which one
has the greater value. The steps are as follows:
1.ï»¿ï»¿ï»¿Start with an input for variable 'x' and get its next input. Prompt Response
Prompt 2.ï»¿ï»¿ï»¿Repeat step 1 for variable 'y.
3.ï»¿ï»¿ï»¿Compare 'x' and 'y' using the "x > y" condition. If true, it è¿™å¹…å›¾ç‰‡æ˜¯Minecraftå®˜æ–¹ç½‘ç«™çš„ç”ŸåŠ¨æ¬¢è¿Žç•Œé¢ã€‚èƒŒæ™¯æ˜¯
Can you translate the flowchart to a python code? m twe oa n vas l' uX e' sis i ng tr oe a tt he er oth ua tpn u ' ty .' , I fs o fa p lsu et , t ith me m eaa nx sim 'xu ' m is no of tt h ge res ae t er æè¿°å›¾ç‰‡ ä¸€ å……ç‰‡ æ»¡è¾½ æ´»é˜” åŠ›çš„ çš„è“ åƒå¤© ç´ ï¼Œ è‰ºç‚¹ æœ¯ç¼€ ä¸–ç€ ç•Œæœµ ï¼Œæœµ ç‰¹è“¬ è‰²æ¾ æ˜¯çš„ ä¸€ç™½ ä¸ªäº‘ æ‘ã€‚ åº„ä¸‹ ã€æ–¹ ä¸€æ˜¯ ç‰‡ä¸€ èŒ‚ä¸ª å¯†
than 'y', so put the maximum of 'x' and 'y' into the output. çš„æ£®æž—å’Œä¸€æ¡èœ¿èœ’çš„å°è·¯ã€‚
4.ï»¿ï»¿ï»¿Finally, end the program by putting the max value into the åœ¨å›¾ç‰‡çš„ä¸­å¿ƒï¼Œä¸€ä¸ªç»¿è‰²çš„æŒ‰é’®å¸å¼•ç€æ³¨æ„åŠ›ï¼Œä¸Šé¢ç”¨
output. ç²—ä½“ç™½è‰²å­—æ¯å†™ç€â€œèŽ·å–Minecraftâ€ã€‚è¿™ä¸ªæŒ‰é’®ä¼¼ä¹Žé‚€
Here is how you can translate this flowchart into Python code: è¯·è§‚çœ‹è€…åŠ å…¥æ¸¸æˆå¹¶å¼€å§‹ä»–ä»¬çš„Minecraftå†’é™©ã€‚
åœ¨å›¾ç‰‡çš„é¡¶éƒ¨ï¼Œä¸€æ®µæ–‡å­—çƒ­æƒ…åœ°æ¬¢è¿Žè§‚ä¼—æ¥åˆ°å®˜æ–¹
Minecraftç½‘ç«™ã€‚å®ƒå†™é“ï¼šâ€œæ¬¢è¿Žæ¥åˆ°å®˜æ–¹Minecraftç½‘
ç«™ï¼åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒºï¼Œäº†è§£å¦‚ä½•çŽ©è¿™æ¬¾æ¸¸æˆï¼ŒåŠ å…¥æˆ‘ä»¬
æœ€å¤§çš„ç¤¾åŒºä¹‹ä¸€ï¼Œå¹¶åœ¨ä»Šå¤©å¼€å§‹åˆ›é€ å§ï¼â€è¿™æ®µæ–‡å­—ä¼ 
è¾¾äº†æ¸¸æˆçš„åŒ…å®¹æ€§å’Œç¤¾åŒºç²¾ç¥žï¼Œé¼“åŠ±æ–°çŽ©å®¶æŽ¢ç´¢å’Œåˆ›é€ ã€‚
æ€»çš„æ¥è¯´ï¼Œè¿™å¹…å›¾ç‰‡æ•æ‰åˆ°äº†Minecraftä¸–ç•Œçš„ç²¾é«“ä¸€å®ƒ
çš„æ— é™å¯èƒ½æ€§ä»¥åŠä¸Žä»–äººçš„äº’åŠ¨ä¹è¶£ã€‚
Response
In this Python code, we define a function called max_of_two Prompt The image shows a graph with various lines representing
that takes two arguments, x and y, and returns the maximum different models' validation log perplexity over the length of
of these two values. We then call this function with the Which direction of paper is the most likely source of the validation sequence, which is measured in x10^3. The title
appropriate inputs and print the result. the table in the picture. "C4 language modeling (large model)" suggests that this graph
pertains to a study or analysis related to large-scale natural
language processing tasks, specifically focusing on language
Prompt modeling using a C4 dataset.
To determine the most likely direction of paper from which the
table originated, we should look for visual clues in the image:
What art style is evident in this image? Response 1.Graph Style: The graph has a clear and professional
appearance, suggesting it could be part of an academic paper
The art style evident in this image isImpressionism. This can or technical report.
be determined by several visual clues present: 2.Data Representation: The use of multiple lines with markers
such as squares, diamonds, and circles indicates comparisons
Brushwork: The painting exhibits a loose, visible brushstroke between different models or methods. This kind of data
technique which is characteristic of Impressionist art. The representation is common in research papers where different
strokes are not smooth or blended but rather they create a experimental results are presented side by side for comparison.
sense of movement and texture. 3.Title and Content: The title provides context about the
nature of the content â€“it mentions "language modeling" and
Color Palette: The use of bright, vibrant colors that seem to "large model," indicating that the paper deals with
capture the light and atmosphere of the scene without precise computational linguistics or machine learning topics.
detail is typical of Impressionist painters. The sky's blue with 4.Legend: There is a legend at the bottom left that matches the
white clouds, the warm tones on the cliffs, and the varied hues colors of the lines to their respective models. This is typical in
used for the boats all contribute to this impressionistic scientific graphs to help readers understand what each line
approach. Prompt represents without having to refer back to the text.
5.Axis Labels: The axes have labels that are consistent with
S wu itb hje fc igt uM rea st t ae nr d: T bh oe a s tsu â€”bj ie sc ct o m ma mtt oe nr i it ns e Imlfâ€” pra e sc so ioas nt ia stl l pa an id ns tc ina gp se , Please generate the latex code of the formula in the image. s "c Vi ae ln idt aif tic io n no lt oa gt i po en r ( p" lV ea xl ii td ya ")t .i o Tn h ese seq u lae bn ec le s l se un gg gt eh s" t a thn ad t the
which often depicted scenes from daily life and nature. paper is intended for an audience familiar with these terms,
which would include researchers or students in fields like
Lack of Detail: There is a lack of fine detail in the depiction of computer science or engineering.
objects such as the boats and figures, which suggests an 6.Font and Formatting: The font used for the title and axis
emphasis on capturing the overall mood and impression of the labels appears to be sans-serif, which is often used in modern
scene rather than meticulous realism. publications due to its readability and clean design.
Based on these observations, the most likely source of the
Light and Shadow: The play of light and shadow is another table in the picture is a publication from a field related to
h tha ell m ara tr isk t o hf a I sm cp ar pe ts us ri eo dn i ts ht ew io nr tk es r. p I ln a yt h oi fs sp ua nin ligti hn tg o, nw e th s ee we ah to ew r Response a linrt gif uic isia til c i sn . t Ie t l cli og ue ln dc e b, e m a a joch ui rn ne a ll e aa rr tn ici ln eg , , c o or n fc eo rm enp cu et a pt ri oo cn ea el dings,
and rocks, creating a dynamic effect. a thesis, or even a presentation slide from a seminar or
These elements together strongly suggest that the artwork A_n= a_0 \left[ 1 + \frac{3}{4} \sum_{k=1}^{n} \left( \frac{4}{9} workshop. The style and content align well with scholarly work
belongs to the Impressionist genre. \right)^k \right] in those domains.
Figure1 | DeepSeek-VLpossessesgeneralmultimodalunderstandingcapabilities,capableof
processinglogicaldiagrams,webpages,formularecognition,scientificliterature,naturalimages,
andembodiedintelligenceincomplexscenarios.
4DeepSeek-VLâ€™spretrainingdatasetiscompiledfromavarietyofsources,includingbutnot
limitedtoCommonCrawl,WebCode,E-books,EducationalMaterials,andarXivArticles. This
collectionthoroughlyencompassesreal-worldscenariossuchaswebscreenshots,PDFs,OCR,
charts,andknowledge-basedcontent(expertise,textbooks),aimingforabroadandpractical
representationwhileremainingscalable.
Whileourpretrainingdataencompassesawidearrayofworldknowledge,wemeticulously
curateourinstruction-tuningdatasettoreflectreal-worldusagescenarios. Toachievethis,we
manuallygatherauthentictestcasesforGPT-4VandGeminifromtheInternet. Thesecaseshave
beensystematicallyorganizedintoacomprehensivetaxonomy. Weusethisstructuredtaxonomy
to choose prompts for each test image, ensuring a practical and relevant instruction tuning
dataset. This taxonomy is also used to create an evaluation dataset that effectively assesses
real-worldperformance.
Thevisualmoduleisdesignedtooptimizetheutilizationofhigh-resolutionvisualinputs
whileremainingwithinafixedtokenbudgettomanageinferencecostseffectively. Assuch,we
employ a hybrid vision encoder, which combines a text-aligned encoder for coarse semantic
extractionat384Ã—384resolutionwithahigh-resolutionencoderthatcapturesdetailedvisual
information at 1024Ã—1024 resolution. By fusing these two encoders, our hybrid approach
efficientlycondensesa1024Ã—1024resolutionimage(whichsufficesinmostusecases)into576
tokens. Thistokencountstrikesabalancebetweenrichvisualrepresentationandtokeneconomy,
makingitfeasibleforbothtext-imageinterleavingandmulti-turninferencescenarios.
During the pretraining of multimodal models, a common challenge encountered is the
potential degradation of language capabilities when the training process is overly reliant on
vision-languagedata. Ourresearchrevealsthatmaintainingasignificantproportionoflanguage
dataâ€”specifically,atleast70%â€”isessentialtopreservetheintegrityoflanguageknowledge
withinthemodel. Thisbalanceiscriticalforachievingarobustmultimodalcapabilitythatdoes
notcompromiselanguageperformance. Moreover,weintroduceanovelâ€œmodalitywarm-upâ€
strategy. This approach carefully adjusts the ratio of modalities during training, gradually
incorporatingmorevision-languagedata. Thecarefultuningofthemodalityratioalongwith
thewarm-upstrategyresultsinabalancedperformanceofbothmodalities.
Wheniteratingonourmodel,Weconductexperimentsonasmallscalebeforescalingtoa
largermodelsize. However,asmallermodel,e.g.,,1Bmodel,cannotdemonstratereasonable
performanceonbenchmarks(Schaefferetal.,2024)andfaithfullyreflectthemodelâ€™sperformance.
Weadopttwoapproachestoaddressthis. First,wemodifytheevaluationprotocolfrommulti-
choicetocomparetheperplexityofoptions. Also,topreventtheinstructionfollowingability
frombecomingthebottleneck,wemixasmallproportionofinstructiontuningdataduringthe
pretrainingphase. Inthisway,wecanachievereasonableperformanceusingthe1Bmodeland
moreaccuratelymeasuretheimpactofeachiterationduringtheexperiment.
Throughextensiveevaluationsofgeneralvisionandlanguagebenchmarks,theDeepSeek-VL
familyshowcasessuperioruserexperiencesinreal-worldapplicationsandachievesstate-of-
the-artorcompetitiveperformanceacrossawiderangeofvisual-languagebenchmarksatthe
samemodelsize,whilemaintainingrobustlanguage-centricperformance. Tofosterinnovation
andenableawiderangeofapplications,wehavemadetwoversionsofourours,1.3Band7B,
publiclyaccessible,inthehopeoffacilitatingtheneedsofvaryingcomputationalcapabilities.
52. Data Construction
Adiverseandlargedatasetisthemostimportantingredientofvisuallanguagemodeltraining.
Our dataset can be divided into two parts: Vision-Language pretraining Data and Vision-
LanguageSupervisedFine-TuningData. VLpretrainingDataiscomposedofvisual-textdata
fromvarioussources,aimedatenhancingthemodelâ€™sfundamentalcross-modalunderstanding
capabilities; whileVLSupervisedFine-TuningDatahasarelativelysmallersizeandaimsto
teach the model to complete specific downstream tasks. By design, VL pretraining Data is
usedtowarmupthevision-languageadaptorintrainingstage1andjointlypretrainthevision-
languagemodelinstage2,andVLSupervisedFine-TuningDataisexploitedintrainingstage3,
i.e.,visionlanguagesupervisedfine-tuning.
2.1. Vision-LanguagepretrainingData
Thepretrainingdatasetutilizedinourstudyencompassesadiverserangeofpubliclyaccessible
sources,inadditiontoaselectionofproprietarydata. Weprovideacomprehensiveoverviewof
thedatasources employed duringthejointvisionandlanguagepretrainingstageinTable1.
SuchadatasetcanfacilitateLLMâ€™scomprehensionoftheentitiesportrayedintheimages.
Furthermore,wepresentadetailedbreakdownofthecompletedataset,whichisorganized
intothefollowingcategories:
Interleaved image-text data enable the models to have a better capability for in-context
learningofmulti-modalityinputs,andweutilizethreepublicdatasetsMMC4(Zhuetal.,2024),
Wiki(Burnsetal.,2023)andWikihow(Yangetal.,2021).
Imagecaptiondatacomefromthreehigh-qualityimage-textpaireddatasets: Capsfusion(Yu
etal.,2023a),TaiSu(Liuetal.,2022b)andDetailedCaption(echo840,2024).
Tableandchartdataenablethemodelstolearnthecapabilityforgeneraltableandchart
image understanding. It encompasses a diverse range of public data sources, including
Chart2text (Kantharaj et al., 2022), Geo170K (Gao et al., 2023), Unichart (Masry et al., 2023),
Ureader(Yeetal.,2023),M-paper(mPLUG,2024),ScienceQA(Luetal.,2022b),ScreenQA(Hsiao
etal.,2022), SciGraphQA-295K(LiandTajbakhsh,2023), Paper2figure100k(Rodriguezetal.,
2023),WidgetCaptioning(Lietal.,2020),Screen2words(Wangetal.,2021),andRefexp(Mao
etal.,2016).
WebCodedataempowersmodelswiththecapabilitytoreconstructcodefromgraphical
interfacesorvisualplots. LeveragingWebsight(HuggingFaceM4,2024)forUIInverseRendering,
weadoptedastrategyakintothatusedinMATCHA(Liuetal.,2022a)forvisualplotsinverse
rendering. Thisinvolvedtheprocessingofapproximately1.46millionJupyternotebooksfrom
theStackdataset(Kocetkovetal.,2023). Byextractingthesenotebooksandcollatingalldiagrams
alongwiththeircorrespondingprecedingcodesegments,wesucceededincuratingacollection
featuring 2 million pairs of images and codes. For better data quality, we filter 1.1 million
instances, each comprising a singular image coupled with a minimum of 5 lines of code, to
constituteourprimarytrainingdataset.
DocumentOpticalCharacterRecognition(OCR)datafacilitatestherecognitionofoptical
characters at the document level, even in challenging real-world scenarios. To the best of
ourknowledge,thereiscurrentlynopubliclyavailablelarge-scaledatasetencompassingboth
English and Chinese documents. Despite the existence of the publicly accessible small-scale
datasetLatex-OCR(Blecher,2024),weadditionallyconstructedacomprehensiveEnglishand
6Table1 | Summaryofdatasetsusedinthejointvisionandlanguagepretrainingstage.
Category Dataset Ratio
Interleavedimage-text MMC4(Zhuetal.,2024) 15.2%
WikipediaEN&CN(Foundation)
Wikihow(Yangetal.,2021)
in-housePDFandEpubtextbooks
Imagecaption Capsfusion(Yuetal.,2023a) 11.1%
TaiSu(Liuetal.,2022b)
DetailedCaption(echo840,2024)
Tableandchart Chart2text(Kantharajetal.,2022) 2.1%
Geo170K(Gaoetal.,2023)
Ureader(Yeetal.,2023)
Unichart(Masryetal.,2023)
M-paper(mPLUG,2024)
ScienceQA(Luetal.,2022b)
ScreenQA(Hsiaoetal.,2022)
SciGraphQA-295K(LiandTajbakhsh,2023)
Paper2figure100k(Rodriguezetal.,2023)
WidgetCaptioning(Lietal.,2020)
Screen2words(Wangetal.,2021)
Refexp(Maoetal.,2016)
WebCode Websight(HuggingFaceM4,2024) 0.4%
pythonplotsscrapedfromGithubnotebook
ScenetextOCR ArT(Chngetal.,2019) 1.2%
MLT-17(Nayefetal.,2017)
LSVT(Sunetal.,2019)
UberText(Zhangetal.,2017)
Coco-text(Veitetal.,2016)
RCTW-17(Shietal.,2017)
ReCTS(Zhangetal.,2019)
TextOCR(Singhetal.,2021)
OpenVINO(Krylovetal.,2021)
HierText(Longetal.,2022)
DocumentOCR arXivrenderedmarkdown
Text-onlycorpus DeepSeek-LLM2Ttextcopus(DeepSeek-AI,2024) 70.0%
ChinesedocumentOCRdataset. Itiscomprisedoftwoparts: 1): arXivArticles: Wecollected
sourcecodeandcompiledPDFsfrom1.4millionarXivarticles. Utilizingpre-processingtools
fromNougat(Blecheretal.,2023),werenderedthesearticlesintopairedimagesandtexts;2):
E-booksandEducationalMaterials: Wecleaned860KEnglishand180KChinesee-booksfrom
Annaâ€™sArchive(Annaâ€™sArchive,2024)alongsidemillionsofK-12educationexamquestions.
Subsequently,weemployedHTMLrenderingtools(KulkarniandTruelsen)toconvertthese
HTMLfileswithdifferenttemplatesintopairedimageandtextformats.
ScenetextOCRdataaugmentthecapabilityofthemodeltorecognizeandextracttextfrom
imagesinwhichthetextisintegratedintotheenvironment. Thedatasetiscomposedofmultiple
7Table2 | Summaryofdatausedinourjointvisionandlanguagesupervisedfine-tuningstage.
Class Dataset Ratio
In-houseData SFTdatabasedontaxonomy(Figure3) 10.5%
GeneralMulti-modality ShareGPT4V(Chenetal.,2023) 35.5%
LAION-GPTV(LAION,2023)
LVIS-Instruct4V(Wangetal.,2023a)
textOCR-GPT4V(Carter,2024)
LLaVA1.6-GPT4V(Liuetal.,2024a)
IconQA(Luetal.,2021)
Tableandchart Ureader(Yeetal.,2023) 4.1%
Geo170K(Gaoetal.,2023)
ScienceQA(Luetal.,2022b)
WebCode Screen-to-code(Abi,2024) 2.0%
ScreenQA(Hsiaoetal.,2022)
Text-onlySFT DeepSeek-LLM(DeepSeek-AI,2024) 47.9%
publicdatasets,includingArT(Chngetal.,2019),MLT-17(Nayefetal.,2017),LSVT(Sunetal.,
2019), UberText (Zhang et al., 2017), Coco-text (Veit et al., 2016), RCTW-17 (Shi et al., 2017),
ReCTS(Zhangetal.,2019),TextOCR(Singhetal.,2021),OpenVINO(Krylovetal.,2021)and
HierText(Longetal.,2022).
Text-onlycorpusservestomaintainproficiencyinlanguage-centrictasks. Inthisstudy,we
employthesametextcorpuswithDeepSeek-LLM(DeepSeek-AI,2024).
2.2. SupervisedFine-tuningData
Thesupervisedfine-tuningdatasetsutilizedinourstudyencompassadiverserangeofmulti-
modalityandlanguagedatasources,includingwell-knownopen-sourcesharedgpt4vdatasets
suchasShareGPT4V(Chenetal.,2023),LAION-GPTV(LAION,2023),LVIS-Instruct4V(Wang
etal.,2023a),textOCR-GPT4V(Carter,2024),LLaVA1.6-GPT4V(Liuetal.,2024a)andIconQA(Lu
etal.,2021). Additionally,weincorporatepartialtableandchartdataextractedfrompretraining
datasetssuch asUreader(Ye etal.,2023), ScreenQA(Hsiaoet al.,2022), Geo170K (Gaoetal.,
2023),andScienceQA(Luetal.,2022b). Moreover,weintegratetheUICodedatasetobtained
fromScreen-to-code(Abi,2024)tasks. Toenhancethequalityofourmulti-modalitySFTdata,
we have also curated a portion of high-quality in-house multi-modality SFT data, some of
which are in the Chinese language. Our in-house instruction-tuning dataset is meticulously
designed to reflect real-world usage scenarios and cover a wide range of tasks. We start by
collectingadiversesetofauthentictestcasesforGPT-4VandGeminifromvariousonlinesources.
These test cases are then carefully analyzed and organized into a comprehensive taxonomy,
whichencompassesmultiplecategories,suchasrecognition,conversion,analysis,reasoning,
evaluation,andsafety,asdetailedinTable3. Thisstructuredtaxonomyservesasaguideline
forselectingrepresentativepromptsforeachtestimage,ensuringthatourinstruction-tuning
datasetisbothpracticalandrelevanttoreal-worldapplications. Moreover, thistaxonomyis
alsoemployedtoconstructabalancedandcomprehensiveevaluationdataset,whichallowsus
toeffectivelyassessthemodelâ€™sperformanceacrossdifferenttasksandcategories. Byfollowing
thissystematicapproach,weensurethatthecategoriescoveredbyourin-housemulti-modality
SFTdataarewell-alignedwiththetaxonomyandrepresentativeofreal-worldusagescenarios.
8MainCategory Description SecondaryCategory TertiaryCategory
Recognition Thispartoftheusecasesmainlyexaminestheunder- GlobalDescription ThemeDescription,Event/BehaviorDescription,Lo-
standinganddescriptionabilityoflargemodelsforim- cation/SceneDescription,Emotion/MoodDescription,
agecontent,whichdoesnotrequirehighknowledge StyleRecognition,FoodRecognition,Others
reserveandreasoningabilityofthemodel,andsome
taskscanbecompletedusingtraditionalmachinelearn-
ingmodels.
LocalDescription Pointing Description, Position Description, Person
Recognition,ObjectAttributeDescription,LogoRecog-
nition,Counting,CurrencyRecognition
OCRandTranscription PrintedTextTranscription,HandwrittenTextTranscrip-
tion,SpecifiedFormatTranscription,SpecifiedLanguage
Transcription
Conversion Thistypeofusecaserequiresthemodeltobeableto ImagetoCode UItoCode,CharttoCode,PhototoSVG/p64Encoding,
describeandrecognizeimagecontent,andusespecific FormulatoCode,FlowcharttoCode
knowledge(e.g.,codeknowledge,promptengineering
knowledge)toconvertimagecontentintoanotherform.
ImagetoText ImagetoPrompt,TextSummary,Image-basedCreation,
TextInterpretation
Analysis Thistypeofusecaserequiresthemodeltousespecific DataChartAnalysis GraphInterpretation,TableInterpretation
knowledgeandlogicalabilitytomakereasonableanal-
ysisandunderstandingbasedonimagecontent,and
describetheimageaccordingtoinstructions.
ProfessionalChartAnalysis CircuitDiagram,Flowchart,Map,MusicScore,Financial
Chart,FloorPlan,Others
ProfessionalImageAnalysis SensorImage,BiologicalandMedicalImage,Voiceprint
Image,PointCloudImage
EncyclopediaKnowledgeAnaly- Art and Culture Knowledge, Natural Environment
sis Knowledge,Food/Clothing/Housing/Transportation
RelatedKnowledge,EntertainmentRelatedKnowledge,
HistoricalKnowledge
Commonsense Rea- Thistypeofusecasemainlyteststhemodelâ€™sunder- RelationshipReasoning InterpersonalRelationship,SpatialRelationship,Size
soning standingandmasteryofcommonsenseinlife,which Relationship,SpeciesRelationship
requiresreasoningbasedontheinterpretationandanal-
ysisofimagecontentcombinedwithcommonsense.
FunctionReasoning HardwareFunctionReasoning,SoftwareFunctionRea-
soning
EnvironmentReasoning EnvironmentStateAnalysis,Environment-basedBehav-
iorReasoning,EmbodiedIntelligence
AnomalyReasoning IdentifyingAnomaliesinImages,DefectDetection,Ac-
cidentJudgment
HumorReasoning -
OtherCommonsenseReasoning StateReasoning,CauseReasoning,AttributeCompari-
son,OpticalIllusion,FunGames,IntentionInterpreta-
tion,BehaviorPrediction
LogicalReasoning Thistypeofusecaserequiresthemodeltocombinethe MathematicalReasoning AlgebraandOperation,PlaneGeometry,SolidGeometry
understandingofimages,comprehensivelyusedomain
knowledgeandlogicalreasoningabilitytocomplete
correspondingtasks.
OtherLogicalReasoning Physics,Chemistry,Biology,Code,IQQuestions
Evaluation Thistypeofusecaserequiresthemodeltoevaluatethe - RealityEvaluation,SimilarityEvaluation,AestheticEval-
imagecontentaccordingtospecificcriteria. uation,Open-endedEvaluation,ImprovementSugges-
tions
Multi-graph Thistypeofusecaseexaminesthemodelâ€™sabilityto TemporalSequenceUnderstand- EventPrediction,ImageSequencing,BehaviorAnalysis
analyzeandunderstandmultipleimages. ing
Multi-graphComparison AttributeComparison,Image-TextMatching,Finding
Associations,SpottingDifferences,ImageDiscrimina-
tion
Safety Thistypeofusecaseexaminesthemodelâ€™sperformance - SuggestiveQuestioning,CounterfactualQuestioning,
intermsofsafety. PromptInjection
Table3 | Ourtaxonomyforthein-houseSFTdata. Thecategoriescoveredbyourhigh-quality
in-housemulti-modalitySFTdataarecomprehensivelyrepresentedinthistaxonomy.
9Furthermore, we include the text-only SFT data employed in DeepSeek-LLM (DeepSeek-AI,
2024)aspartofourjointvisionandlanguageSFTdata.
3. Approach
3.1. Architecture
Oursystemcontainsthreemodules: ahybridvisionencoder,avisionadaptor,andalanguage
model. Weintroduceeachpartinthissection.
Hybrid Vision Encoder. We employ SigLIP as the vision encoder to extract high-level
semanticfeaturerepresentationsfromvisualinputs. However,weobservethatasingleSigLIP
encoderstrugglestoaddressallreal-worldquestionscomprehensively. Visionencodersinthe
CLIPfamily,includingSigLIP,areprimarilydesignedforsemanticvisualrepresentationsbut
arechallengedbyambiguousencoding,resultinginvisuallydistinctimagesbeingencodedas
similarduetowhatisreferredtoas"CLIP-blindpairs"Tongetal.(2024). Meanwhile,theCLIP
familyofmodelsislimitedbyitsrelativelylow-resolutioninputs(e.g.,224x224,336x336,384
x384,512x512),whichhinderstheirabilitytohandletasksrequiringmoredetailedlow-level
featureslikedenseOCRandvisualgroundingtask.
To address these limitations, we additionally utilize a vision-only encoder based on the
SAM-B (Kirillov et al., 2023), a pre-trained ViTDet (Li et al., 2022) image encoder to process
low-levelfeatures,whichacceptshigh-resolution1024x1024imageinputs. Inadditiontothe
SAM-B encoder, we retain the SigLIP-L vision encoder with low-resolution 384 x 384 image
inputs. Consequently,ourhybridvisionencodercombinestheSAM-BandSigLIP-Lencoders,
efficiently encoding high-resolution 1024 x 1024 images while preserving both semantic and
detailedinformation. Specifically,ahigh-resolutionSAM-Bvisionencoderfirstresizestheimage
into1024x1024andresultsina64x64x256featuremap. Thenwewillinterpolateitinto96x
96x256andemploytwoconvolutionallayerswithastrideof2,producingafeaturemapof
24x24x1024,andfinallyreshapeitto576x1024. Thelow-resolutionSigLIP-Lvisionencoder
firstresizestheimageinto384x384,encodesitintoa24x24x1024featuremap,andfinally
reshapesitinto576x1024.
Inthecaseofahigh-resolutionfeaturemapofsize,64x64x256generatedbySAM-B,the
VLAdaptorinitiallyinterpolatesitintoasizeof96x96x256. Subsequently,itemploystwo
convolutionallayerswithastrideof2,producingafeaturemapof24x24x1024,andreshapes
itto576x1024. Alongsidethis,thelow-resolutionfeaturemapofsize576x1024generatedby
SigLIP-Lisconcatenatedwiththehigh-resolutionfeatures,resultingin576visualtokenswith
2048dimensions. Thesevisualtokenspossessasubstantialcapacityforenhancinghigh-level
semanticvisualrecognitionandlow-levelvisualgroundingtasks. ThentheyundergoGeLU
activation and are directed through an embedding layer to establish a connection with the
languagemodel.
Vision-LanguageAdaptor. Weemployatwo-layerhybridMLPtobridgethevisionencoder
andtheLLM.Initially,distinctsingle-layerMLPsareusedtoprocesshigh-resolutionfeatures
andlow-resolutionfeaturesseparately. Subsequently,thesefeaturesareconcatenatedalongtheir
dimensionsandthentransformedintotheLLMâ€™sinputspacethroughanotherlayerofMLP.
LanguageModel. OurlanguagemodelisbuiltuponDeepSeekLLM(DeepSeek-AI,2024)
whose micro design largely follows the design of LLaMA (Touvron et al., 2023a,b), adopt-
ing a Pre-Norm structure with RMSNorm (Zhang and Sennrich, 2019) function and using
SwiGLU(Shazeer,2020)astheactivationfunctionfortheFeed-ForwardNetwork(FFN),with
10Prompt
Is the cyclist on the left or right side of the woman's handbag?
Response
The cyclist is on the left side of the woman's handbag. This can be determined by observing the relative positions of the objects in
the image:
1. The woman is holding a handbag that is positioned to her right side, as we view the image.
2. There is a cyclist depicted on the road behind the woman and to her left when looking at the image.
3. The cyclist appears to be moving away from the viewpoint, given their direction and the motion blur effect on their wheels
4. The cyclist is further down the road compared to the woman, which supports the conclusion that they are on the left side of the
handbag based on our perspective.
These visual clues confirm that the cyclist is indeed con the left side of the woman's handbag.
Figure 2 | Visualization results. DeepSeek-VL is capable of capturing tiny object and giving
organizedexplanations.
anintermediatelayerdimensionof 8ð‘‘ ð‘šð‘œð‘‘ð‘’ð‘™. ItalsoincorporatesRotaryEmbedding(Suetal.,
3
2024)forpositionalencodingandusesthesametokenizerwithDeepSeek-LLM.Weintroduce
a family of DeepSeek-VL models. Given our objective of conducting joint pretraining with
multimodalandlanguage,weselectanintermediatecheckpointfromDeepSeekâ€™spretrained
modelstocontinuepretraining.
Specifically, the DeepSeek-VL-1B model is constructed based on the DeekSeek-LLM-1B
model,whichunderwenttrainingwithanapproximatecorpusof500billiontexttokens. And
theDeekSeek-VL-7BmodelisdevelopedleveragingtheDeepSeek-LLM-7Bmodeltrainedwith
anestimated2trilliontexttokens.
11Stage 1: Training VL Adaptor Stage 2: Joint VL Pre-training Stage 3: Supervised Finetuning
DeepSeek LLM DeepSeek LLM DeepSeek LLM
Vision-Language Vision-Language Vision-Language
Adaptor Adaptor Adaptor
SAM-B SigLIP-L SAM-B SigLIP-L SAM-B SigLIP-L
Hybrid Vision Hybrid Vision Hybrid Vision
Encoder Encoder Encoder
Interleaved VL + VL Chat Data +
Image-Text Pairs
Pure Language Sequences Pure Language Chat Data
Figure 3 | Ourtraining pipelinesconsistof threestages. Stage1 involvestrainingtheVision-
Language (VL) adaptor while keeping the hybrid vision encoder and language model fixed.
Stage2isthecrucialpartofthejointvisionandlanguagepretraining,wherebothVLadaptor
andlanguagemodelaretrainable. Stage3isthesupervisedfine-tuningphase,duringwhichthe
low-resolutionvisionencoderSigLIP-L,VLadaptor,andlanguagemodelwillbetrained.
3.2. TrainingPipelines
WetrainourDeepSeek-VLinthreeconsecutivestagesasshowninFigure3: vision-language
adaptorwarmup,jointvision-languagepretraining,andsupervisedfine-tuning. Wecurrently
focusonvisualunderstandingcapabilitiesandonlycalculatethenexttokenpredictionlosson
thelanguagepart.
3.2.1. Stage1: TrainingVision-LanguageAdaptor
Theprimaryobjectiveofthisstageistoestablishaconceptuallinkbetweenvisualandlinguistic
elementswithintheembeddingspace,therebyfacilitatingthecomprehensiveunderstanding
ofdepictedentitiesintheimagesbytheLargeLanguageModel(LLM).Consistentwithprior
researchconductedbyLLaVA(Liuetal.,2024b)andInstruct-BLIP(Daietal.,2023),weadopta
similarapproachinwhichboththevisionencoderandtheLLMremainfrozenduringthisstage,
whilesolelyallowingthetrainableparameterswithinthevision-languageadaptor. Weutilizea
datasetcomprising1.25millionimage-textpairedcaptionsobtainedfromShareGPT4V,along
with2.5millionDocumentOCRrenderingpairstotraintheVLadaptor.
Nevertheless,comparedtoLargeLanguageModels(LLMs),vision-languageadaptors(e.g.,
a2-layerMLP)haveasignificantlysmallerparametercapacity. Thislimitationinmodelcapacity
restrictsthecapabilitiesthatcanbelearnedduringthisstage. Anaturalquestionarises: Canthe
lawofdatascalingbeeffectiveatthisstage? Toaddressthisquestion,weconductedasimple
experimentinTable8. Theresultsdemonstratethatexpandingthedatascaleatthisstagedoes
notprovidebenefitsandmayevenleadtoinferiorperformance. Consequently,weproceedto
unfreezetheLargeLanguageModel(LLM)andinvestigateefficientvision-languagepretraining
approachesduringstage2.
123.2.2. Stage2: JointVision-Languagepretraining
Inthisstage,weexploreeffectivepretrainingstrategieswhichcanbeconsideredasanadditional
stagetoenableLargeLanguageModels(LLMs)tocomprehendmultimodalinputs. Wekeepthe
visionencoderfrozenandoptimizethelanguagemodelandVLadaptor.
Initially, we attempt to directly train the LLM with multimodal data. However, we find
while the metrics for multimodal performance incrementally improved, there is a stark and
severedeclineinlanguagemetricsasillustratedinFigure4(Multimodal:Language=100%:0%),.
Thisunderscorestheinherentchallengeindirectlyconductingmultimodalpretrainingonthe
foundationofanLLM,revealingacriticaltrade-offbetweenenhancingmultimodalabilitiesand
preservinglinguisticproficiency.
Wehypothesizethattheobservedphenomenonstemsfromtwoprimaryfactors: firstly,the
majorityofmultimodalcorpora,areoverlysimplisticandexhibitasignificantdivergencefrom
thecomplexityanddistributionoflinguisticdata. Secondly,thereappearstobeacompetitive
dynamicbetweenmultimodalandlinguisticmodalities,leadingtowhatcanbedescribedas
catastrophicforgettingoflanguagecapabilitieswithintheLLM.
JointLanguage-multimodalTrainingToaddressthischallenge,wedeviseastraightforward
yeteffectivejointlanguage-multimodaltrainingstrategy. Duringtraining,wenotonlyengage
in multimodal data training but also incorporate a large proportion of language data into
thetraining. Thisapproachaimstobalancethetrainingfocus,mitigatingtheadverseeffects
observed. WeconductexperimentsontheDeepSeek-VL1BmodelinFigure4toexplorethe
impactofvaryingthemodalitymixingratios.
The analysis of the graph yields several key conclusions: (1). Integrating language data
significantlyalleviatesthedeclineinlanguagecapabilities,demonstratingasubstantialimprove-
mentinthemodelâ€™slinguisticperformance. (2). Theinclusionoflanguagedatadoesnotleadto
asignificantlossinmultimodalperformance,indicatingthatthemodelretainsitsmultimodal
processing abilities. (3). The performance of different modalities is strongly correlated with
theirrespectiveproportionsinthetrainingdataset,substantiatingthecompetitiverelationship
betweenthetwomodalities. Ultimately,weoptforatrainingratiooflanguagetomultimodal
dataofroughly7:3forourfinalmodel. Thisratioenablesthemodeltomaintainitslanguage
capabilitieswhilesimultaneouslyachievingbetterpretrainingonmultimodaldata,effectively
balancingthedevelopmentofbothlanguageandmultimodalproficiencies.
ScalingVision-LanguagePretrainingNevertheless,thepretrainingstageofthemodelincursa
substantialcomputationalcost,andperformingiterationsonthe7Bmodelrequiresanexcessive
amountofcomputingpowerandtime. Onesuitablestrategyinvolvesconductingexperiments
onasmallermodel,specificallythe1.3Bmodel,andsubsequentlyscalingituptothe7Bmodel.
Fortunately,wehaveobservedthatasignificantportionoftheoutcomesobtainedfromthe1.3B
modelscanbeeffectivelytransferredtothe7BmodelthroughtheutilizationofSFT(e.g.,the
encoderdesign). However,duringthestage2trainingphase,wehaveencounteredconsiderable
fluctuationsinthegenerativemetricsofthe1.3Bmodel,renderingitchallengingtosupervise
thetrainingprocesseffectively. Andthishasbeendiscussedin Schaefferetal.(2024),"sharp
andunpredictablechangesmightbeinducedbytheresearcherâ€™schoiceofmeasurement,even
thoughthemodelfamilyâ€™sper-tokenerrorratechangessmoothly,continuouslyandpredictably
withincreasingscale."Subsequentexperimentshaveledustoidentifytherootcausesofthis
issue: thelimitedcapacityofthe1.3BmodelandtheabsenceofSFTdatawithinthetraining
dataset,bothofwhichhinderthemodelâ€™sabilitytoaccuratelyfollowinstructions. Evenwhen
themodelpossessesknowledgeofthecorrectoptions,itstrugglestogeneratethemprecisely.
13SeedBench MMBench
45.0 Multimodal:Langauge=10%:90% 50.0 Multimodal:Langauge=10%:90%
Multimodal:Langauge=25%:75% Multimodal:Langauge=25%:75%
42.5 Multimodal:Langauge=60%:40% 47.5 Multimodal:Langauge=60%:40%
Multimodal:Langauge=75%:25% Multimodal:Langauge=75%:25%
40.0 Multimodal:Langauge=100%:0% 45.0 Multimodal:Langauge=100%:0%
37.5 42.5
35.0 40.0
32.5 37.5
30.0 35.0
27.5 32.5
0 2000 4000 6000 8000 10000 12000 14000 16000 0 2000 4000 6000 8000 10000 12000 14000 16000
Step Step
MMBench_CN MMLU
Multimodal:Langauge=10%:90%
42 Multimodal:Langauge=25%:75% 0.330
Multimodal:Langauge=60%:40%
40 Multimodal:Langauge=75%:25%
Multimodal:Langauge=100%:0% 0.325 38
36
0.320
34
32 0.315 Multimodal:Langauge=10%:90%
Multimodal:Langauge=25%:75%
30
Multimodal:Langauge=60%:40%
28 0.310 Multimodal:Langauge=75%:25%
Multimodal:Langauge=100%:0%
0 2000 4000 6000 8000 10000 12000 14000 16000 0 2000 4000 6000 8000 10000 12000 14000 16000
Step Step
HellaSwag Pile-test
Multimodal:Langauge=10%:90%
0.578 Multimodal:Langauge=25%:75% 2.24
Multimodal:Langauge=60%:40%
0.576 Multimodal:Langauge=75%:25%
Multimodal:Langauge=100%:0% 2.22
0.574
Multimodal:Langauge=10%:90%
0.572 2.20 Multimodal:Langauge=25%:75%
Multimodal:Langauge=60%:40%
0.570 Multimodal:Langauge=75%:25%
2.18 Multimodal:Langauge=100%:0%
0.568
0.566 2.16
0.564
2.14
0 2000 4000 6000 8000 10000 12000 14000 16000 0 2000 4000 6000 8000 10000 12000 14000 16000
Step Step
Figure4 | Comparativeperformanceresultsondifferentmodalityfusionratio. Anexcessively
largeproportionofmultimodaldata(multimodal:language=100%:0%)leadstosignificantfor-
gettingoflanguagecapabilitiesinLLMs. Asuitableratio(multimodal:language=70%:30%)can
effectivelymitigatetheissueoflanguageforgettingwhilesimultaneouslyenhancingthemodelâ€™s
multimodalabilities.
Tomitigatethesechallenges,weadopteadual-prongedapproach. Firstly,weemploythe
Multi-choicePPLmethodologytomonitorthemodelâ€™sprogress. Thisinvolvesinputtingnot
onlythepromptandimageintothenetworkbutalsoalltheanswerassociatedwiththequestion.
Subsequently, we calculate the PPL for each answer position (e.g., A, B, C, D) and select the
optiondeemedcorrectbythemodelasthefinalanswer. Secondly,weintroduceSFTdatainto
thetrainingdatasetataminimalproportion,allowingthemodeltoacquiresomeproficiencyin
followinginstructions. Thecombinationofthesetwoapproachesensuresthemaintenanceof
stabletrainingmetricsforthe1.3Bmodelandbringbetterperformanceafterstage3.
3.2.3. Stage3: SupervisedFine-tuning
Inthisphase,wefinetunethepretrainedDeepSeek-VLmodelwithinstruction-basedfine-tuning
tobolsteritsabilitytofollowinstructionsandengageindialogue,culminatinginthecreationof
theinteractiveDeepSeek-VL-Chatmodel. Weoptimizethelanguagemodel,VLadaptor,and
14
ycaruccA
LPPCM
ycaruccA
ycaruccA
LPPCM
ycaruccA
LPPCM
ycaruccA
LPPML
LPPPrompt Response â€“DeepSeek-VL
Explain the code, step by step. The code provided is a Python function that calculates the longest palindrome substring
of a given string. The function takes in two parameters, which are not defined within this
snippet but presumably represent the input strings to be processed.
Here's a step-by-step explanation of how the code works:
1. It defines a helper function calledlongestPalIndromewith two arguments,self(which
is likely a reference to an object or class instance) ands. This function returns a value,
which we can assume is the length of the longest palindrome substring found ins.
2. The main function starts by checking if the length ofsis less than 1. If true, it
immediately returnssasthere is no palindrome substring for empty or single character
strings.
3. Next, it initializes variablesMax_LenandMax_Strto store the maximum length of a
palindrome substring found so far.
4. A loop (for iin range(len(s)-1):) iterates over all possible starting points of substrings
ins. For each starting pointi, another nested loop (for j in range(i+1, len(s))) checks
whether the substring from indexitojforms a palindrome.
The conditionif j -i+ 1 > Max_Lenand s[i:j] == s[j:i][::-1]ensures that the substring is
both longer than the current maximum length and reads the same backward as forward.
If this condition is met, it updatesMax_LenandMax_Strwith the new maximum length
and substring.
5. After iterating through all possible substrings, the function returnsMax_Str, which
contains the longest palindrome substring found.
6. Finally, the linereturn Max_Strindicates that the function will return the result of its
computation.
This function uses dynamic programming principles to find the longest palindromic
subsequence efficiently by storing intermediate results and avoiding redundant
calculations.
Figure5 | Visualizationresults. DeepSeek-VLcanunderstandPythoncodeandprovidedetailed
andorganizedexplanations.
hybridvisionencoderwiththevision-languageSFTdataasshowninTable2,SAM-Bremains
frozenduetothelimitedGPUmemory. Weonlysuperviseanswersandspecialtokensandmask
thesystemanduserprompts. Toguaranteethemodelâ€™scomprehensiveproficiencyindialogue,
weutilizeablendofmultimodaldataandpuretextdialoguedatausedinDeepSeek-LLM.This
approachensuresthemodelâ€™sversatilityacrossvariousdialoguescenarios.
3.3. HyperparametersandInfrastructures
ThedetailedhyperparametersofallstagesareillustratedinTable4. Wetrainandevaluateour
DeepSeek-VLwithHAI-LLM(High-flyer,2023),alightweightandefficientdistributedtraining
framework. Sinceweusevisualencoderstoconvertimagesintoembeddingvectorsandthen
treatimageembeddingsandtextembeddingsuniformly,wecaneasilyadaptpipelineparal-
lelismtoVLmodeltraining: allweneedtodoistoviewvisualencodersandtextembeddingas
asinglemoduleandtakeitasthefirstlayeroftheresultingmodel. Thisveryfirstlayerhasa
complicatedmodelstructureandprecludesstandardtensorparallelismtechnique,butluckily
itrequiresrelativelysmallcomputationcomparedtoupperstandardtransformerblocks. We
thereforesimplyrecomputethevisualencoderforwardpassinalltensorparallelranks. The
existenceofvisualencodersalsoleadstonon-uniformexecutiontimeacrossmodellayers,so
were-dividemodellayersbetweenpipelineparallelismrankstoachievebetterloadbalanceand
throughput. TheupperlayersofDeepSeek-VLareexactlythesameasthoseinDeepSeek-LLM.
With such minor modification, we can now perform canonical 3D parallelism techniques as
inMegatron(Korthikantietal.,2023;Narayananetal.,2021;Shoeybietal.,2019)andoverlap
computationandcommunicationasinDeepSeek-LLM(DeepSeek-AI,2024). DeepSeek-VL-7B
consumed 5 days on a cluster of 64 Nvidia A100 processing units, each comprising 8 cards,
whileDeepSeek-VL-1Bconsumed7daysonasetupinvolving16NvidiaA100processingunits.
15DeepSeek-VL1B DeepSeek-VL7B
VisionEncoder SigLIP SigLIP+SAM
Hyperparameters Stage1 Stage2 Stage3 Stage1 Stage2 Stage3
Learningrate 1.0Ã—10âˆ’3 3Ã—10âˆ’5 2.0Ã—10âˆ’5 1.0Ã—10âˆ’3 4.2Ã—10âˆ’5 2.0Ã—10âˆ’5
LRscheduler Cosine Step Cosine Cosine Step Cosine
Weightdecay 0.0 0.0 0.0 0.0 0.0 0.0
Gradientclip 1.0 1.0 1.0 1.0 1.0 1.0
Optimizer AdamW(ð›½ =0.9,ð›½ =0.95) AdamW(ð›½ =0.9,ð›½ =0.95)
1 2 1 2
Warm-upsteps 128 2000 256 128 2000 256
Trainingsteps 15000 96000 10000 15000 42000 10000
Batchsize 256 1024 256 256 2304 256
Sequencelength 512 4096 4096 512 4096 4096
Sequencepacking Ã— âœ“ Ã— Ã— âœ“ Ã—
Pipelineparallelism Ã— Ã— Ã— Ã— âœ“ âœ“
Table4 | DetailedhyperparametersofourDeepSeek-VL.
4. Evaluation
4.1. PublicMultimodalBenchmarksEvaluation
Weevaluateourmodelsonaseriesofpublicbenchmarks:
Multimodal comprehensive understanding datasets: MMMU (Yue et al., 2023), CM-
MMU (Zhang et al., 2024), MMBench (Liu et al., 2023a), MMBench-CN (Liu et al., 2023a),
SeedBench (Li et al., 2023a) and MMV (Yu et al., 2023b). We compare DeepSeek-VL with
competitorsonMMB/MMC-devascurrentofficialtestdownloadlinkisnolongeractive.
Chart/tableunderstandingdatasets: OCRBench(Liuetal.,2023b);
Hallucinationdatasets: POPE(Lietal.,2023b);
Scientificproblemdatasets: ScienceQA(Luetal.,2022a)andMathVista(Luetal.,2023).
Weapplygeneration-basedevaluationwithgreedydecoding. Thegeneration-basedevalua-
tionherereferstolettingthemodelgeneratefreetextsandparsingresultsfromgeneratedtexts.
Thecomparativeresults,asillustratedinTable5,showthatDeepSeek-VL-7Bsurpassesmost
open-sourcemodelsofsimilarsizeacrossawiderangeofbenchmarks.
DeepSeek-VLoutperformsopen-sourcemodelsofsimilarsizeinbenchmarkssuchasMMB,
MMC,andSEEDbench,evenapproachingproprietarymodels(DeepSeek-VLvs. GPT-4V=70.4
vs. 71.6onseedbench),demonstratingitspowerfulnaturalimagecomprehensioncapability. The
modelalsosurpassesallopen-sourcemodelsinmathematicallogic,butstilllagssignificantly
behindproprietarymodelslikeGPT-4V(36.1vs. 47.8onMathVista). Thisdifferencecouldbe
attributedtothevarianceinbasemodelsizes.
Furthermore, as shown in Table 6, DeepSeek-VL-1.3B significantly outperforms models
ofcomparablesize. Itdemonstratessuperiorperformancecomparedtoleadingopen-source
modelsintheMMBbenchmarktest,whileutilizingonlyclosetohalftheparameters(1.3Bvs.
2.7B),indicatingitsrobustnaturalimagecomprehensioncapability. DeepSeek-VL-1.3Beven
achieves comparable results to 7B open-source models on MathVista, further validating the
powerfullogicalunderstandingcapabilitiesoftheDeepSeek-VLfamily.
16LLM MMMU CMMMU MMB MMC SEED OCRB POPE MathV MMVet
Close-sourceLMMs:
GeminiPro Unk 48.9 - 75.2 74.0 70.7 659 - 45.2 59.2
GPT-4V Unk 56.8 42.5 75.0 74.7 71.6 659 - 47.8 49.9
Qwen-VL-Plus Unk 45.2 39.5 66.2 69.6 72.7 - - 43.3 55.7
Qwen-VL-MAX Unk 51.4 - 78.1 76.4 72.7 - - 51.0 61.8
Open-source13BLMMs:
LLaVA-1.5 13B 36.4 - 68.2 61.9 68.2 331 85.9 26.4 38.3
VILA 13B - - 70.3 64.3 - - 84.2 - 38.8
LLaVA-Next 13B 36.2 - 70.0 64.4 71.9 - 86.7 35.3 48.4
Open-source7BLMMs:
EMU2-Chat 7B 36.3 23.8 63.6 45.9 68.9 - - 30.0 31.0
Qwen-VL-Chat 7B 37.0 - 60.6 56.7 64.8 - - 33.8 47.3
CogVLM 7B 37.3 24.8 63.7 53.8 68.8 - - 34.7 54.5
LLaVA-Next 7B 35.8 - 67.4 60.0 70.2 - 86.5 34.6 43.9
Yi-VL 6B 37.8 35.8 68.2 68.9 67.6 - - 28.0 31.1
DeepSeek-VL(ours) 7B 36.6 37.9 73.2 72.8 70.4 456 88.1 36.1 41.5
Table5 | Thecomparisonbetweendifferentmulti-modalmodels. Thetophalfareproprietary
models,whilethebottomareopen-sourcemodels.
LLM MMMU CMMMU MMB MMC SEED OCRB POPE MathV MMVet
TinyModel:
MobileVLM 1.4B - - 53.2 - - - 84.5 - -
MobileVLM 2.7B - - 59.6 - - - 84.9 - -
MobileVLMV2 1.4B - - 59.6 - - - 84.3 - -
MobileVLMV2 2.7B - - 63.2 - - - 84.7 - -
LLaVA-Phi 2.7B - - 59.5 - - - 85.0 - 28.9
DeepSeek-VL(ours) 1.3B 32.2 27.4 64.6 61.3 66.7 409 87.6 31.1 34.8
Table6 | Thecomparisonbetweentinymulti-modalmodels.
4.2. PublicLanguageBenchmarksEvaluation
Weevaluateourmodelsonthefollowingpubliclanguagebenchmarks:
Multi-subjectmultiple-choicedatasetsincludingMMLU(Hendrycksetal.,2020).
LanguageunderstandingandreasoningdatasetsincludingHellaSwag(Zellersetal.,2019).
LanguagemodelingdatasetsincludingPile(Gaoetal.,2020).
MathdatasetsincludingGSM8K(Cobbeetal.,2021).
CodedatasetsincludingMBPP(Austinetal.,2021).
StandardizedexamsincludingAGIEval(Zhongetal.,2023).
Weapplyperplexity-basedevaluationtodatasetsthatrequireanswerstobechosenfrom
severaloptions. ThesedatasetsincludeHellaSwagandMMLU.Theperplexity-basedevaluation
here refers to calculating the perplexity of each option and selecting the lowest one as the
17DeepSeek-VL DeepSeek-VL DeepSeek-LLM
Version
1BChat 7BChat 7BChat
Encoder SigLIP SigLIP+SAM None
HellaSwag 56.0 68.4 68.5
MMLU 32.5 52.4 49.4
GSM8K 18.0 55.0 63.0
Benchmark
MBPP 10.0 35.2 35.2
AGIEval 14.0 27.8 19.3
Table7 | Theperformanceonlanguagebenchmarks.
modelprediction. Perplexity-basedevaluationhelpstodistinguishsubtleprobabilitydifference
betweenmodelpredictionsandavoidsdiscontinuityofexactmatchstyleevaluation. Weapply
generation-basedevaluationwithgreedydecodingforGSM8KandAGIEval. Thegeneration-
basedevaluationherereferstolettingthemodelgeneratefreetextsandparsingresultsfrom
generated texts. We apply language-modeling-based evaluation for Pile-test, which means
calculatingthebits-per-byteonthetestcorpus. AndtheresultsareillustratedinTable7
Itcanbeobservedthatacrossthemajorityoflanguagebenchmarks,DeepSeek-VLperforms
comparablyto,orevensurpasses,DeepSeek-7B.Forinstance,itachievesscoresof68.4vs. 68.5
on HellaSwag, which serves as a general benchmark for evaluating general language ability.
DeepSeek-VLoutperformsDeepSeek-7BonmetricssuchasMMLUandAGIEval,indicatingthat
multimodaltrainingmethodsmayevenaidinlanguagetasks. Nevertheless,DeepSeek-VL-7B
showsacertaindegreeofdeclineinmathematics(GSM8K),whichsuggeststhatdespiteefforts
topromoteharmonybetweenvisionandlanguagemodalities,therestillexistsacompetitive
relationship between them. This could be attributed to the limited model capacity (7B), and
largermodelsmightalleviatethisissuesignificantly. Overall,DeepSeek-VLstrivestoachieve
thegoalofminimizingdeclinesinlanguagecapabilitywhileaddressingthesechallenges.
4.3. HumanEvaluation
TofurtherexplorethecapabilitiesofourDeepSeek-VL,weindependentlyconstructadataset
for manual evaluation. This dataset comprises 100 questions, divided into seven categories,
each encompassing specific tasks. These categories and tasks are same as our taxonomy for
the in-house SFT data, as shown in Table 3. This approach ensures that the tasks we test are
universalandencompassthemajorityofusecasesformultimodalmodels.
Moreover,basedonthecategoriesandtasksdescribedinexistingreports,wecollectsimilar
imagematerialsanddevelopedprompts. Thesourcesfortheseimagematerialsincluderoyalty-
freeimagecommunitiesandphotographstakenbytheresearchers. Thismethodicalcollection
andpromptformulationprocessensuresourdatasetisbothcomprehensiveandrepresentative
ofreal-worldmultimodalmodelapplications.
We compare our DeepSeek-VL 7B with InternLM2-VL, CogVLM and GPT-4V as shown
inFigure6(andwealsoprovidevisualizationresultsinAppendixA).GPT-4Vdemonstrates
exceptionalperformanceacrossmostdimensions. Allopen-sourcemodelsarestillfarbehind
GPT-4Vinlogicalreasoning,highlightingthenecessityofscalingupthesizeofLargeLanguage
Models (LLMs). DeepSeek-VL 7B achieves better results in overall performance, reaching
outcomesclosetoGPT-4VinRecognition,Conversion,andCommonsenseReasoning.
18InternLM2-VL-7B CogVLM-17B DeepSeek-VL-7B GPT4V
TOTAL SCORE RECOGNITION CONVERSION ANALYSIS COMMONSENSE LOGICAL MULTI-IMAGES EVALUATION
Figure6 | HumanevaluationresultsonInternLM2-VL,CogVLM,DeepSeek-VLandGPT-4V.
Stage1,TrainingStep MMB MMC SEED POPE MMMU Average
2K 59.0 54.0 61.8 82.3 30.3 57.5
8K 58.0 45.0 58.5 84.9 29.2 55.1
20K 56.0 52.3 59.0 81.7 28.6 55.5
80K 58.1 55.0 58.6 78.6 27.9 55.6
Table8 | ComparativedirectlySFTperformanceresultsonscalingupstage1data. Theresults
demonstratethatexpandingthedatascaleatthisstagedoesnotyieldbenefits,orevenresultsin
worseperformance.
Stage1 Stage2 Stage3 MMB MMC SEED POPE MMMU Average
âœ“ âœ“ 59.4 54.2 61.4 82.5 29.2 57.4
âœ“ âœ“ 63.4 60.5 65.9 87.1 31.8 61.7
âœ“ âœ“ âœ“ 64.3 61.3 66.7 87.6 32.2 62.4
Table9 | Analysisofmodelperformanceacrosstrainingstages.
4.4. AblationStudy
Scale Up Projector Training We expand the dataset for stage 1 (projector warmup) and sub-
sequently apply supervised fine-tuning. The results, depicted in Figure 8, demonstrate that
augmentingthetrainingdatavolumedoesnotenhanceperformanceatthisstage. Thisimplies
thattheprojectorâ€™scapacityisinherentlyconstrained,renderingitincapableofcapturingthe
extensiveknowledgenecessaryformultimodaltasks.
TrainingStageInTable9,weexaminethecontributionsofeachstagetothemodelâ€™sperformance.
Itâ€™sevidentthatcombiningstage1,stage2,andstage3yieldssignificantlybetterresultsacross
allmetricscomparedtocombiningstage1andstage3alone,demonstratingtheeffectivenessof
multimodalpretraining. Additionally,thecombinationofstage2andstage3stillslightlylags
behindthecombinedperformanceofstage1,stage2,andstage3,indicatingthatvision-language
adaptorwarmupstageremainsmeaningful.
19
55.4 56.4
56.5
3.6
17.5
67.4
10.7 41.7
90.4 90.4
28.6
37.7
12.4
62.5
47.4 74.4
22.5
69.6
25.6
47.6
12.3
34.1
92.4
63.5
57.3 57.3
31.3
31.8
57.3
5.2
57.3 57.3Pile-test MMBench MMBench_CN
2.170
w/o group by modality 50.0 w/o group by modality w/o group by modality
w/ group by modality w/ group by modality 42 w/ group by modality
2.165 47.5 40
45.0 38
2.160 42.5 36
40.0 34
2.155
37.5 32
35.0 30
2.150
32.5 28
0 2000 4000 6000 8000 10000120001400016000 0 2000 4000 6000 8000 10000120001400016000 0 2000 4000 6000 8000 10000120001400016000
Step Step Step
Figure7 | Comparativeanalysisofmodalitywarmuponlanguage(Pile-test)andmultimodal
(MMBenchandMMBench_CN)benchmarksdemonstratesthatmodalitygroupingconsistently
surpassesthenon-groupedmodalityapproachinlanguagetasks,whilesimultaneouslypreserv-
ingperformanceonmultimodaltasks(Multimodal:Language=60%:40%).
ModalityGroupTrainingWhenmixinglanguageandmultimodaldata,weobservethatdirectly
blendingthematthebatchlevelsignificantlyreducestrainingefficiency. Thisinefficiencyarises
becauseeachbatchgradientbackpropagationprocesswaitsfortheslowestsampletocomplete.
As a result, the predominantly faster-to-process pure language data ends up waiting for the
multimodalsamplestofinish,leadingtoadecreaseinoveralltrainingefficiency.
Toaddressthisissue,weexperimentwithgroupingdifferentmodalitiesofdataateachglobal
step,samplingdistinctmodalitiesseparately. Thisapproachinvolvesorganizingthetraining
datasothatbatchesarecomposedeitherentirelyoflanguagedataorentirelyofmultimodal
dataatdifferenttrainingsteps,ratherthanmixingthemwithinthesamebatch.
TheresultsareshowninFigure7,weobservethatthismethoddoesnotcompromisethe
modelâ€™sperformancewhileenhancingthemodelâ€™strainingefficiencyby20%. Thisstrategyeffec-
tivelycircumventsthebottleneckcausedbythedisparateprocessingtimesbetweenmodalities,
optimizingthetrainingworkflow.
ModalityWarmupConsideringthatourapproachinvolvesmultimodaltrainingonthefounda-
tionofalanguagemodel,directlymixingmultimodaldatainafixedproportionfromtheoutset
candestabilizethemodel. Tocounteractthisissue,weproposeasimpleyeteffectivemodality
warm-upstrategy. Initially,wesetthelanguagedataratioto1,andthengraduallydecreaseitto
thetargetratioforthefinalmodeltraining(e.g.,0.7).
Ourexperiments,asillustratedinFigure8,demonstratethatthisstrategyeffectivelyprevents
asignificantdeclineinlanguagecapabilitiesatthebeginningoftraining,whilealsoyielding
comparativelysuperioroutcomesinthefinalphasesforboththelanguageandmultimodaldo-
mains. Thisgradualadaptationenablesthemodeltomoreseamlesslyadjusttotheincorporation
ofmultimodaldata,therebyimprovingoveralltrainingstabilityandperformance.
VisionEncoderSelectionInordertobetteracquireandutilizeimageinformation,wecompare
the training loss of different vision encoders under our training settings except for reducing
trainingstepsofstage2to8000forefficiency. AsillustratedinFigure9,theincorporationof
vision-onlyself-supervisedencodershasbeenfoundtosignificantlyenhanceperformanceon
trainingloss. Tomoreeffectivelyprocesshigh-resolutionimages,ourresearchultimatelyadopts
ahybridvisionencoderstrategy,combiningSigLIPwithSAMforourmodelâ€™simplementation.
Vision-LanguageAdaptorDesignToimprovetheefficiencyofextractinginformationfromthe
visualencoderwhileadheringtocurrenttokenlengthconstraints,adjustmentscanbemadeto
20
LPP
ycaruccA ycaruccAPile-test MMBench MMBench_CN
w/o modality warmup 50.0 w/o modality warmup w/o modality warmup
2.165 w/ modality warmup w/ modality warmup 42 w/ modality warmup
47.5
40
2.160
45.0
38
2.155 42.5 36
40.0
2.150 34
37.5
32
2.145 35.0
30
32.5
0 2000 4000 6000 8000 10000120001400016000 0 2000 4000 6000 8000 10000120001400016000 0 2000 4000 6000 8000 10000120001400016000
Step Step Step
Figure8 | Comparativeperformanceresultsonlanguage(Pile-test)andmultimodal(MMBench
andMMBench_CN)benchmarksformodalitywarmup. Modalitywarmupconsistentlymatches
or surpasses the performance of approaches without modality warmup across all evaluated
tasks(Multimodal:Language=60%:40%).
CLIP
SigLIP
2.6 SigLIP+DINO
SigLIP+SAM
2.4
2.2
2.0
1.8
0 1000 2000 3000 4000 5000 6000 7000 8000
Step
Figure9 | Comparativeanalysisofdifferentvisionencodersontraininglossesinstage2.
theVision-Languageadaptorintwomainways: themethodusedtocombinevisualfeatures
andthedesignoftheMLPadaptor.
Previous studies (Tong et al., 2024) have indicated that combining visual features along
thesequencedimensioncanleadtobettermodelperformance,althoughthiscomeswiththe
trade-offofincreasedcomputationalrequirementsduetoalongersequenceofvisualfeature
tokens. AsdemonstratedinthetopsectionofTable10,reducingthesequencelengthbystacking
visualfeaturesalongtheimageâ€™swidthorheightdimensionsbeforesequenceconcatenation,
in order to keep the sequence length constant, does not achieve better results compared to
simplymergingthemalongtheembeddingdimensioninmostmetrics. Intermsoftheadaptor
architecture, employing separate MLP adaptors for each vision feature encoder allows for
more precise adjustments to the specific values and distribution patterns of visual features,
facilitating smoother model training. Conversely, using a shared MLP adaptor for different
visionencoderscontributestoadequatefeaturefusion. Weadoptamixedstrategyandreport
stableandimprovedperformance,asoutlinedinthelowersectionofTable10.
21
LPP
ssoL
ycaruccA ycaruccAArchitecture MMB MMC SEED POPE ScienceQA MMMU OCRB Average
SequenceConcatenation:
TokenPooling-W 61.2 59.6 61.6 86.5 57.7 31.6 304 55.5
TokenPooling-H 59.9 58.3 61.6 83.8 55.0 32.0 291 54.2
EmbeddingConcatenation:
HybridMLP 61.7 60.1 62.9 87.8 56.6 31.3 309 55.9
SharedMLP 62.0 58.9 62.5 86.6 54.7 30.2 318 55.2
SeparateMLP 57.5 58.7 63.1 86.5 56.6 29.0 299 54.5
Table10|ComparisonofdifferentAdaptorarchitecturesusingSigLIPandSAMashybridvision
encoder,HybridMLPareusedforsequenceconcatenationexperiments. Boldedentriesrepresent
the best results, while underlined entries denote the second-best results. For calculating the
averagescore,wedividetheOCRBenchbythetotalnumberofquestions.
5. Conclusion, Limitation, and Future Work
Inthistechnicalreport,wehaveintroducedDeepSeek-VL,aseriesofMultimodalLargeLan-
guage Models, available in scales of 1.3B and 6.7B parameters. This report has unveiled the
limitations inherent in the predominant projector-based pretraining methodologies, setting
thestagefortheinnovativeapproachadoptedbyDeepSeek-VL.Byprioritizingajointvision
andlanguage(VL)pretrainingphase,DeepSeek-VLtranscendstraditionalmodelsbyensuring
thattheintegrationofmultimodaldatadoesnotcompromisethelinguisticcapabilitiesofthe
LargeLanguageModels(LLMs). Thisisachievedthroughastrategicwarm-updataratioand
theintroductionofahybridvisionencoder,whichtogetherenabletheefficientprocessingof
high-resolutionimageswithoutlosingsightofsemanticrichness.
Theincorporationofahybridvisionencoder,capableofhandling1024x1024imageswithin
aconstrainedtokenbudget, underscoresourcommitmenttopreservingthenuanceddetails
andsemanticintegrityacrossdiversetasks. Asaresult,DeepSeek-VLemergesasapioneering
model that not only meets but exceeds the standards set by generalist models in its class. It
showcasesexceptionalperformanceacrossawiderangeofvisually-centricbenchmarkswhile
sustainingformidableproficiencyinlanguage-centricevaluations.
In making DeepSeek-VL publicly available, we aim to catalyze further innovation and
explorationwithintheresearchcommunity,providingarobustfoundationuponwhichfuture
studiescanbuild. Thisgestureofopennessisintendedtofacilitatethecollectiveadvancement
ofourunderstandingandcapabilitiesinhandlingmultimodaldata.
Lookingahead,weareexcitedtoannounceplanstoscaleupDeepSeek-VLtolargersizes,
incorporatingMixtureofExperts(MoE)technology. Thisforthcomingexpansionpromisesto
furtherenhancethemodelâ€™sefficiencyandeffectiveness,openingupnewhorizonsforresearch
andapplicationinthefieldofAI.
References
01-ai. Yi-34Bvisionlanguagemodel. https://huggingface.co/01-ai/Yi-VL-34B,2024.
Abi. Screenshottocode. https://github.com/abi/screenshot-to-code,2024.
22Annaâ€™sArchive. Annaâ€™sarchive. https://annas-archive.org/,2024.
Anthropic. IntroducingClaude,2023. URLhttps://www.anthropic.com/index/introd
ucing-claude.
J.Austin,A.Odena,M.Nye,M.Bosma,H.Michalewski,D.Dohan,E.Jiang,C.Cai,M.Terry,
Q.Le,etal. Programsynthesiswithlargelanguagemodels. arXivpreprintarXiv:2108.07732,
2021.
J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A
versatilevision-languagemodelforunderstanding,localization,textreading,andbeyond.
arXivpreprintarXiv:2308.12966,2023.
R.Bavishi,E.Elsen,C.Hawthorne,M.Nye,A.Odena,A.Somani,andS.TasÂ¸Ä±rlar. Introducing
ourmultimodalmodels,2023. URLhttps://www.adept.ai/blog/fuyu-8b.
L.Blecher. Latex-ocr. GitHubrepository,2024. URLhttps://github.com/lukas-blecher
/LaTeX-OCR.
L.Blecher,G.Cucurull,T.Scialom,andR.Stojnic. Nougat: Neuralopticalunderstandingfor
academicdocuments. arXivpreprintarXiv:2308.13418,2023.
A.Burns,K.Srinivasan,J.Ainslie,G.Brown,B.A.Plummer,K.Saenko,J.Ni,andM.Guo. A
suite of generative tasks for multi-level multimodal webpage understanding. In The 2023
Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. URL
https://openreview.net/forum?id=rwcLHjtUmn.
J.Carter. Textocr-gpt4v. https://huggingface.co/datasets/jimmycarter/textocr-g
pt4v,2024.
L.Chen,J.Li,X.Dong,P.Zhang,C.He,J.Wang,F.Zhao,andD.Lin. Sharegpt4v: Improving
largemulti-modalmodelswithbettercaptions. arXivpreprintarXiv:2311.12793,2023.
C. K. Chng, Y. Liu, Y. Sun, C. C. Ng, C. Luo, Z. Ni, C. Fang, S. Zhang, J. Han, E. Ding, et al.
Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art. In 2019 International
ConferenceonDocumentAnalysisandRecognition(ICDAR),pages1571â€“1576.IEEE,2019.
K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,
J.Hilton, R.Nakano, etal. Trainingverifierstosolvemathwordproblems. arXivpreprint
arXiv:2110.14168,2021.
W.Dai,J.Li,D.Li,A.M.H.Tiong,J.Zhao,W.Wang,B.Li,P.Fung,andS.Hoi. Instructblip:
Towardsgeneral-purposevision-languagemodelswithinstructiontuning,2023.
DeepSeek-AI. Deepseekllm: Scalingopen-sourcelanguagemodelswithlongtermism. arXiv
preprintarXiv:2401.02954,2024. URLhttps://github.com/deepseek-ai/DeepSeek-L
LM.
echo840. Detailedcaptiondataset. https://huggingface.co/datasets/echo840/Deta
iled_Caption,2024.
W.Foundation. Wikimediadownloads. URLhttps://dumps.wikimedia.org.
J. Gao, R. Pi, J. Zhang, J. Ye, W. Zhong, Y. Wang, L. Hong, J. Han, H. Xu, Z. Li, et al. G-
llava: Solving geometric problem with multi-modal large language model. arXiv preprint
arXiv:2312.11370,2023.
23L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,
N.Nabeshima,etal. ThePile: An800GBdatasetofdiversetextforlanguagemodeling. arXiv
preprintarXiv:2101.00027,2020.
Google. AnimportantnextsteponourAIjourney,2023. URLhttps://blog.google/tech
nology/ai/bard-google-ai-search-updates/.
D.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,andJ.Steinhardt. Measuring
massivemultitasklanguageunderstanding. arXivpreprintarXiv:2009.03300,2020.
High-flyer. Hai-llm: é«˜æ•ˆä¸”è½»é‡çš„å¤§æ¨¡åž‹è®­ç»ƒå·¥å…·,2023. URLhttps://www.high-flyer.c
n/en/blog/hai-llm.
Y.-C.Hsiao,F.Zubach,M.Wang,etal. Screenqa: Large-scalequestion-answerpairsovermobile
appscreenshots. arXivpreprintarXiv:2209.08199,2022.
HuggingFaceM4. Websightdataset. https://huggingface.co/datasets/HuggingFaceM
4/WebSight,2024.
S. Kantharaj, R. T. Leong, X. Lin, A. Masry, M. Thakkar, E. Hoque, and S. Joty. Chart-to-
text: A large-scale benchmark for chart summarization. In S. Muresan, P. Nakov, and
A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for
ComputationalLinguistics(Volume1: LongPapers),pages4005â€“4023,Dublin,Ireland,May
2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/2022.acl-long.277. URL
https://aclanthology.org/2022.acl-long.277.
A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,S.Whitehead,A.C.
Berg,W.-Y.Lo,etal. Segmentanything. arXivpreprintarXiv:2304.02643,2023.
D.Kocetkov,R.Li,L.B.Allal,J.Li,C.Mou,C.M.Ferrandis,Y.Jernite,M.Mitchell,S.Hughes,
T.Wolf,D.Bahdanau,L.vonWerra,andH.deVries. Thestack: 3tbofpermissivelylicensed
sourcecode. InTransactionsonMachineLearningResearch,2023.
V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro.
Reducing activation recomputation in large transformer models. Proceedings of Machine
LearningandSystems,5,2023.
I.Krylov,S.Nosov,andV.Sovrasov. Openimagesv5textannotationandyetanothermasktext
spotter. InAsianConferenceonMachineLearning,pages379â€“389.PMLR,2021.
A.KulkarniandJ.Truelsen. wkhtmltopdf. https://wkhtmltopdf.org/. Projectmaintained
byAshishKulkarni,originallycreatedbyJakobTruelsen.Accessed: 2024-02-22.
LAION. Gpt-4v dataset. https://huggingface.co/datasets/laion/gpt4v-dataset,
2023.
B.Li,R.Wang,G.Wang,Y.Ge,Y.Ge,andY.Shan. Seed-bench: Benchmarkingmultimodalllms
withgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023a.
S. Li and N. Tajbakhsh. Scigraphqa: A large-scale synthetic multi-turn question-answering
datasetforscientificgraphs,2023.
Y.Li,G.Li,L.He,J.Zheng,H.Li,andZ.Guan. Widgetcaptioning: Generatingnaturallanguage
descriptionformobileuserinterfaceelements. arXivpreprintarXiv:2010.04295,2020.
24Y.Li,H.Mao,R.Girshick,andK.He. Exploringplainvisiontransformerbackbonesforobject
detection. InEuropeanConferenceonComputerVision,pages280â€“296.Springer,2022.
Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen. Evaluating object hallucination in
largevision-languagemodels. arXivpreprintarXiv:2305.10355,2023b.
J.Lin,H.Yin,W.Ping,Y.Lu,P.Molchanov,A.Tao,H.Mao,J.Kautz,M.Shoeybi,andS.Han.
Vila: Onpre-trainingforvisuallanguagemodels. arXivpreprintarXiv:2312.07533,2023.
F.Liu,F.Piccinno,S.Krichene,C.Pang,K.Lee,M.Joshi,Y.Altun,N.Collier,andJ.M.Eisenschlos.
Matcha: Enhancingvisuallanguagepretrainingwithmathreasoningandchartderendering.
arXivpreprintarXiv:2212.09662,2022a.
H.Liu,C.Li,Y.Li,B.Li,Y.Zhang,S.Shen,andY.J.Lee. Llava-next: Improvedreasoning,ocr,
and world knowledge, January 2024a. URL https://llava-vl.github.io/blog/202
4-01-30-llava-next/.
H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. Advancesinneuralinformation
processingsystems,36,2024b.
Y.Liu,G.Zhu,B.Zhu,Q.Song,G.Ge,H.Chen,G.Qiao,R.Peng,L.Wu,andJ.Wang. Taisu: A
166mlarge-scalehigh-qualitydatasetforchinesevision-languagepre-training. InS.Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural
Information Processing Systems, volume 35, pages 16705â€“16717. Curran Associates, Inc.,
2022b. URLhttps://proceedings.neurips.cc/paper_files/paper/2022/file/6
a386d703b50f1cf1f61ab02a15967bb-Paper-Datasets_and_Benchmarks.pdf.
Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al.
Mmbench: Isyourmulti-modalmodelanall-aroundplayer? arXivpreprintarXiv:2307.06281,
2023a.
Y.Liu,Z.Li,H.Li,W.Yu,M.Huang,D.Peng,M.Liu,M.Chen,C.Li,L.Jin,etal. Onthehidden
mysteryofocrinlargemultimodalmodels. arXivpreprintarXiv:2305.07895,2023b.
S.Long,S.Qin,D.Panteleev,A.Bissacco,Y.Fujii,andM.Raptis. Towardsend-to-endunified
scene text detection and layout analysis. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,2022.
P. Lu, L. Qiu, J. Chen, T. Xia, Y. Zhao, W. Zhang, Z. Yu, X. Liang, and S.-C. Zhu. Iconqa: A
newbenchmarkforabstractdiagramunderstandingandvisuallanguagereasoning. arXiv
preprintarXiv:2110.13214,2021.
P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan.
Learntoexplain: Multimodalreasoningviathoughtchainsforsciencequestionanswering.
InThe36thConferenceonNeuralInformationProcessingSystems(NeurIPS),2022a.
P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan.
Learntoexplain: Multimodalreasoningviathoughtchainsforsciencequestionanswering.
AdvancesinNeuralInformationProcessingSystems,35:2507â€“2521,2022b.
P.Lu,H.Bansal,T.Xia,J.Liu,C.Li,H.Hajishirzi,H.Cheng,K.-W.Chang,M.Galley,andJ.Gao.
Mathvista: Evaluatingmathematicalreasoningoffoundationmodelsinvisualcontexts. arXiv
preprintarXiv:2310.02255,2023.
25J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy. Generation and
comprehensionofunambiguousobjectdescriptions. InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,pages11â€“20,2016.
A.Masry,P.Kavehzadeh,X.L.Do,E.Hoque,andS.Joty. Unichart: Auniversalvision-language
pretrainedmodelforchartcomprehensionandreasoning. arXivpreprintarXiv:2305.14761,
2023.
mPLUG. M-paperdataset. https://huggingface.co/datasets/mPLUG/M-Paper,2024.
D.Narayanan,M.Shoeybi,J.Casper,P.LeGresley,M.Patwary,V.Korthikanti,D.Vainbrand,
P.Kashinkunti,J.Bernauer,B.Catanzaro,etal. Efficientlarge-scalelanguagemodeltraining
ongpuclustersusingmegatron-lm. InProceedingsoftheInternationalConferenceforHigh
PerformanceComputing,Networking,StorageandAnalysis,pages1â€“15,2021.
N.Nayef,F.Yin,I.Bizid,H.Choi,Y.Feng,D.Karatzas,Z.Luo,U.Pal,C.Rigaud,J.Chazalon,
et al. Icdar2017 robust reading challenge on multi-lingual scene text detection and script
identification-rrc-mlt. In201714thIAPRinternationalconferenceondocumentanalysisand
recognition(ICDAR),volume1,pages1454â€“1459.IEEE,2017.
OpenAI. Chatgpt: Optimizinglanguagemodelsfordialogue. 2022. URLhttps://openai.c
om/blog/chatgpt.
OpenAI. GPT-4technicalreport. arXiv,2023a.
R.OpenAI. Gpt-4v(ision)systemcard. 2023b.
J. A. Rodriguez, D. Vazquez, I. Laradji, M. Pedersoli, and P. Rodriguez. Ocr-vqgan: Tam-
ing text-within-image generation. In Proceedings of the IEEE/CVF Winter Conference on
ApplicationsofComputerVision,pages3689â€“3698,2023.
R. Schaeffer, B. Miranda, and S. Koyejo. Are emergent abilities of large language models a
mirage? AdvancesinNeuralInformationProcessingSystems,36,2024.
N.Shazeer. Gluvariantsimprovetransformer. arXivpreprintarXiv:2002.05202,2020.
B.Shi,C.Yao,M.Liao,M.Yang,P.Xu,L.Cui,S.Belongie,S.Lu,andX.Bai.Icdar2017competition
on reading chinese text in the wild (rctw-17). In 2017 14th iapr international conference on
documentanalysisandrecognition(ICDAR),volume1,pages1429â€“1434.IEEE,2017.
M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm:
Trainingmulti-billionparameterlanguagemodelsusingmodelparallelism. arXivpreprint
arXiv:1909.08053,2019.
A. Singh, G. Pang, M. Toh, J. Huang, W. Galuba, and T. Hassner. Textocr: Towards large-
scaleend-to-endreasoningforarbitrary-shapedscenetext. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages8802â€“8812,2021.
J.Su,M.Ahmed,Y.Lu,S.Pan,W.Bo,andY.Liu. Roformer: Enhancedtransformerwithrotary
positionembedding. Neurocomputing,568:127063,2024.
Q. Sun, Q. Yu, Y. Cui, F. Zhang, X. Zhang, Y. Wang, H. Gao, J. Liu, T. Huang, and X. Wang.
Generativepretraininginmultimodality. arXivpreprintarXiv:2307.05222,2023.
26Y. Sun, Z. Ni, C.-K. Chng, Y. Liu, C. Luo, C. C. Ng, J. Han, E. Ding, J. Liu, D. Karatzas, et al.
Icdar2019competitiononlarge-scalestreetviewtextwithpartiallabeling-rrc-lsvt. In2019
InternationalConferenceonDocumentAnalysisandRecognition(ICDAR),pages1557â€“1562.
IEEE,2019.
G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut,J.Schalkwyk,A.M.Dai,
A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805,2023.
S. Tong, Z. Liu, Y. Zhai, Y. Ma, Y. LeCun, and S. Xie. Eyes wide shut? exploring the visual
shortcomingsofmultimodalllms. arXivpreprintarXiv:2401.06209,2024.
H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.RoziÃ¨re,N.Goyal,
E.Hambro,F.Azhar,etal. LLaMA:Openandefficientfoundationlanguagemodels. arXiv
preprintarXiv:2302.13971,2023a.
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,
P.Bhargava,S.Bhosale,D.Bikel,L.Blecher,C.Canton-Ferrer,M.Chen,G.Cucurull,D.Esiobu,
J.Fernandes,J.Fu,W.Fu,B.Fuller,C.Gao,V.Goswami,N.Goyal,A.Hartshorn,S.Hosseini,
R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,
M.Lachaux,T.Lavril,J.Lee,D.Liskovich,Y.Lu,Y.Mao,X.Martinet,T.Mihaylov,P.Mishra,
I.Molybog,Y.Nie,A.Poulton,J.Reizenstein,R.Rungta,K.Saladi,A.Schelten,R.Silva,E.M.
Smith,R.Subramanian,X.E.Tan,B.Tang,R.Taylor,A.Williams,J.X.Kuan,P.Xu,Z.Yan,
I.Zarov,Y.Zhang,A.Fan,M.Kambadur,S.Narang,A.Rodriguez,R.Stojnic,S.Edunov,and
T.Scialom. Llama2: Openfoundationandfine-tunedchatmodels. CoRR,abs/2307.09288,
2023b. doi: 10.48550/arXiv.2307.09288. URLhttps://doi.org/10.48550/arXiv.2307.
09288.
A.Veit,T.Matera,L.Neumann,J.Matas,andS.Belongie. Coco-text: Datasetandbenchmarkfor
textdetectionandrecognitioninnaturalimages. arXivpreprintarXiv:1601.07140,2016.
B. Wang, G. Li, X. Zhou, Z. Chen, T. Grossman, and Y. Li. Screen2words: Automatic mobile
uisummarizationwithmultimodallearning. InThe34thAnnualACMSymposiumonUser
InterfaceSoftwareandTechnology,pages498â€“510,2021.
J.Wang,L.Meng,Z.Weng,B.He,Z.Wu,andY.-G.Jiang. Toseeistobelieve: Promptinggpt-4v
forbettervisualinstructiontuning. arXivpreprintarXiv:2311.07574,2023a.
W.Wang,Q.Lv,W.Yu,W.Hong,J.Qi,Y.Wang,J.Ji,Z.Yang,L.Zhao,X.Song,etal. Cogvlm:
Visualexpertforpretrainedlanguagemodels. arXivpreprintarXiv:2311.03079,2023b.
Y.Yang,A.Panagopoulou,Q.Lyu,L.Zhang,M.Yatskar,andC.Callison-Burch. Visualgoal-step
inferenceusingwikihow. arXivpreprintarXiv:2104.05845,2021.
J. Ye, A. Hu, H. Xu, Q. Ye, M. Yan, G. Xu, C. Li, J. Tian, Q. Qian, J. Zhang, et al. Ureader:
Universalocr-freevisually-situatedlanguageunderstandingwithmultimodallargelanguage
model. arXivpreprintarXiv:2310.05126,2023.
Q.Yu,Q.Sun,X.Zhang,Y.Cui,F.Zhang,Y.Cao,X.Wang,andJ.Liu. Capsfusion: Rethinking
image-textdataatscale. arXivpreprintarXiv:2310.20550,2023a.
W.Yu,Z.Yang,L.Li,J.Wang,K.Lin,Z.Liu,X.Wang,andL.Wang. Mm-vet: Evaluatinglarge
multimodalmodelsforintegratedcapabilities. arXivpreprintarXiv:2308.02490,2023b.
27X.Yue,Y.Ni,K.Zhang,T.Zheng,R.Liu,G.Zhang,S.Stevens,D.Jiang,W.Ren,Y.Sun,etal.
Mmmu: Amassivemulti-disciplinemultimodalunderstandingandreasoningbenchmarkfor
expertagi. arXivpreprintarXiv:2311.16502,2023.
R.Zellers,A.Holtzman,Y.Bisk,A.Farhadi,andY.Choi. HellaSwag: Canamachinereallyfinish
yoursentence? InA.Korhonen,D.R.Traum,andL.MÃ rquez,editors,Proceedingsofthe57th
ConferenceoftheAssociationforComputationalLinguistics,ACL2019,Florence,Italy,July
28-August2,2019,Volume1: LongPapers,pages4791â€“4800.AssociationforComputational
Linguistics,2019. doi: 10.18653/v1/p19-1472. URLhttps://doi.org/10.18653/v1/p1
9-1472.
B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural
InformationProcessingSystems,32,2019.
G. Zhang, X. Du, B. Chen, Y. Liang, T. Luo, T. Zheng, K. Zhu, Y. Cheng, C. Xu, S. Guo, et al.
Cmmmu: Achinesemassivemulti-disciplinemultimodalunderstandingbenchmark. arXiv
preprintarXiv:2401.11944,2024.
R. Zhang, Y. Zhou, Q. Jiang, Q. Song, N. Li, K. Zhou, L. Wang, D. Wang, M. Liao, M. Yang,
et al. Icdar 2019 robust reading challenge on reading chinese text on signboard. In 2019
international conference on document analysis and recognition (ICDAR), pages 1577â€“1581.
IEEE,2019.
Y. Zhang, L. Gueguen, I. Zharkov, P. Zhang, K. Seifert, and B. Kadlec. Uber-text: A large-
scale dataset for optical character recognition from street-level imagery. In SUNw: Scene
UnderstandingWorkshop-CVPR2017,Hawaii,U.S.A.,2017. URLhttp://sunw.csail.m
it.edu/abstract/uberText.pdf.
W.Zhong,R.Cui,Y.Guo,Y.Liang,S.Lu,Y.Wang,A.Saied,W.Chen,andN.Duan. AGIEval: A
human-centricbenchmarkforevaluatingfoundationmodels. CoRR,abs/2304.06364,2023.
doi: 10.48550/arXiv.2304.06364. URLhttps://doi.org/10.48550/arXiv.2304.06364.
W.Zhu,J.Hessel,A.Awadalla,S.Y.Gadre,J.Dodge,A.Fang,Y.Yu,L.Schmidt,W.Y.Wang,
andY.Choi. Multimodalc4: Anopen,billion-scalecorpusofimagesinterleavedwithtext.
AdvancesinNeuralInformationProcessingSystems,36,2024.
28A. Appendix
Prompt Response â€“DeepSeek-VL
å°†è¿™æ®µæµç¨‹å›¾è½¬æ¢ä¸ºpythonç¨‹åº è¿™æ®µæµç¨‹å›¾å±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨Pythonç¼–ç¨‹è¯­è¨€ç¼–å†™çš„çŒœæ•°å­—æ¸¸æˆã€‚æ¸¸æˆçš„ç›®çš„æ˜¯çŽ©å®¶çŒœæµ‹ä¸€
ä¸ªåœ¨1åˆ°10ä¹‹é—´çš„éšæœºæ•°ï¼Œå¦‚æžœçŒœå¯¹äº†ï¼Œæ¸¸æˆå°±ä¼šç»“æŸå¹¶æ˜¾ç¤ºâ€œä½ èµ¢äº†ï¼â€ï¼ˆYou got it!ï¼‰ã€‚
å¦‚æžœçŽ©å®¶çš„çŒœæµ‹è¶…è¿‡äº†æˆ–ä½ŽäºŽè¿™ä¸ªèŒƒå›´ï¼Œæ¸¸æˆä¼šæç¤ºâ€œå¤ªé«˜äº†ï¼â€ï¼ˆToo high!ï¼‰æˆ–â€œå¤ªä½Ž
äº†ï¼â€ï¼ˆToo low!ï¼‰ï¼Œç„¶åŽçŽ©å®¶å¯ä»¥å†æ¬¡å°è¯•ã€‚
æ ¹æ®æµç¨‹å›¾çš„æè¿°ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæ¸¸æˆå®žçŽ°ä¸ºä»¥ä¸‹Pythonä»£ç ï¼š
åœ¨è¿™ä¸ªç¨‹åºä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†random.randint()å‡½æ•°æ¥ç”Ÿæˆä¸€ä¸ª1åˆ°10ä¹‹é—´çš„éšæœºæ•´æ•°ä½œä¸º
ç­”æ¡ˆã€‚ç„¶åŽï¼Œæˆ‘ä»¬è¿›å…¥ä¸€ä¸ªå¾ªçŽ¯ï¼Œç›´åˆ°çŽ©å®¶çš„çŒœæµ‹ç­‰äºŽç­”æ¡ˆä¸ºæ­¢ã€‚åœ¨æ¯æ¬¡çŒœæµ‹åŽï¼Œæˆ‘ä»¬ä¼š
æ£€æŸ¥çŽ©å®¶çš„çŒœæµ‹æ˜¯å¦åœ¨èŒƒå›´å†…ï¼Œä»¥åŠå®ƒæ˜¯å¦ç­‰äºŽç­”æ¡ˆã€‚å¦‚æžœæ˜¯ï¼Œæˆ‘ä»¬å°±æ‰“å°å‡ºâ€œä½ èµ¢
äº†ï¼â€ï¼›å¦‚æžœä¸æ˜¯ï¼Œæˆ‘ä»¬å°±æ‰“å°å‡ºâ€œå¤ªé«˜äº†ï¼â€æˆ–â€œå¤ªä½Žäº†ï¼â€ï¼Œç„¶åŽè®©çŽ©å®¶å†æ¬¡çŒœæµ‹ã€‚
ä¸€æ—¦çŽ©å®¶çš„çŒœæµ‹ç­‰äºŽç­”æ¡ˆï¼Œæ¸¸æˆå°±ç»“æŸäº†ã€‚
Response â€“Open-source Model 1
sorry, as a language model AI, I cannot process image into any other content.
Response â€“Open-source Model 2
å½“ç„¶ï¼è¿™é‡Œæœ‰ä¸€ä¸ªPython ä»£ç ç‰‡æ®µå¯ä»¥å¤åˆ¶æµç¨‹å›¾ï¼š```python
Figure 10 | Visualization results. DeepSeek-VL can understand childrenâ€™s programming dia-
gramsfromtherealworldandprovidedetailedandorganizedexplanations.
29Prompt Response â€“DeepSeek-VL
Explain the image. The image is a line graph that compares the performance of two different learning rate schedulers,
namely the Multi Step Learning Rate Scheduler and the Cosine Learning Rate Scheduler. The x-axis
represents the number of processed tokens in billions, ranging from 0 to 100 billion. The y-axis
represents the training loss, which ranges from 2.0 to 3.0.
The blue line represents the Multi Step Learning Rate Scheduler with three steps of increasing
rates: (80% + 10% + 10%). This line starts at a high point near 3.0 on the y-axis when the number
of processed tokens is 0 billion, indicating a higher initial loss. As the number of processed tokens
increases, this line shows a steep decline in training loss, suggesting rapid improvement in model
performance as more data is processed. However, after approximately 40 billion processed tokens,
the line begins to level off, indicating diminishing returns on further processing.
The orange line represents the Cosine Learning Rate Scheduler. It also starts at a high point near
3.0 when the number of processed tokens is 0 billion but has a much smoother descent than the
blue line. This suggests a slower but steady decrease in training loss as more tokens are processed.
After around 60 billion processed tokens, this line levels off as well, similar tothe blue line.
Both lines show a general trend of decreasing training loss as the number of processed tokens
increases, indicating improved model performance over time. However, the Multi Step Learning
Rate Scheduler appears to achieve lower losses more quickly compared to the Cosine Learning
Rate Scheduler, especially in the early stages of processing tokens.
Response â€“Open-source Model 1
Line 1 expresses the number of tokens, following a nonlinear trend. Line 2 starts at value 80, then
is decreasing and increases by 10% until reaching a value where it then increases by 10%, before it
flattens out. Line 3 also starts in increasing part, then has some constant decreases between
different points, and after that express an increasing part.
Response â€“Open-source Model 2
The image displays a line graph comparing the training losses of two different learning rates
schedulers: the Multi Step Learning Rate Scheduler (80% + 10% = 10%) and the Cosine Learning
Rate Scheduler. The y-axis represents the training loss in terms of billions, ranging from
approximately 2.2 to 3.0. The x-axis represents the number of processed tokens, with a range
from 0 to 100. The Multi Step Learning Rate Scheduler's line is blue, showing a sharp decline
initially and then stabilizing. The Cosine Learning Rate Scheduler's line is orange, showing a more
gradual decline.
Prompt Response â€“DeepSeek-VL
Help me find the bug. The image shows a piece of code written in Python, which is intended to find the maximum value in
a sequence. The function definition starts with "def" indicating that this is a new function being
defined. The name of the function is "find_max_in_sequence".
The first line checks if the input is not a sequence; if it is, the function returns None. This is an
important check because functions should only operate on sequences (like lists or tuples) and not
other types of data structures. If the input is not a sequence, the function will return no result, which
can be useful for handling invalid inputs.
Next, there's a statement defining what a sequence is: max_value= sequence [1]. This seems
incorrect as it assigns the index 1 to max_valueinstead of the actual element at position 1 in the
sequence.
Then we have a loop that iterates over each number in the sequence. It uses an "if" condition to
check whether the current number (num) is greater than the current maximum value (max_value). If
true, it updates max_valueto be equal to num. This part of the code appears correct, as it correctly
compares each number against the current maximum value and updates the maximum when
necessary.
Finally, after the loop ends, the function returns the updated max_value, which contains the
maximum value found in the sequence.
Based on the visual clues provided by the code snippet, the bug lies in the assignment of max_value
where it incorrectly assigns the index 1 rather than the value at that index in the sequence. The
corrected version would look like this: max_value= sequence[0].
This error could potentially lead to unexpected results if the sequence has more than one element,
as the function might fail to find the maximum value due to the wrong assignment of max_value.
Response â€“Open-source Model 1
The function name in the code is "find_max_in_sequence", but it has not actually been called as
the condition of the if-statement checks for 'not sequence' instead of 'if not sequence'. The actual
line where the function should be called is missing from the provided code.
Response â€“Open-source Model 2
The bug in the code is that the condition for returning 'None' is set to 'if not sequence', which is
incorrect. The correct condition should be 'if not max_value', to ensure that the function returns
'None' when the sequence is not found or when the max value is not found within the sequence.
Figure11 | Visualizationresults. DeepSeek-VLhasstrongunderstandingcapabilitiesforcode
andchartsintherealworld.
30Prompt Response â€“DeepSeek-VL
What coin is that? The coin is a 10 Baht coin from Thailand.
Response â€“Open-source Model 1
The coin is an Asian coin, which features a cityscape on it.
Response â€“Open-source Model 2
answering does not require reading text in the image
Prompt Response â€“DeepSeek-VL
è¯·æ ¹æ®è¿™å¼ å›¾ç‰‡ï¼Œåˆ›ä½œä¸€é¦–ä¸ƒè¨€ç»å¥ å±±å·æš®è‰²æ˜ æ–œé˜³ï¼Œ
æž—é—´é›ªè¦†å¯’æ„é•¿ã€‚
å­¤æ ‘å½±æ–œäººæœªå½’ï¼Œ
ä¸€æŠ¹æ®‹éœžç…§æ–­è‚ ã€‚
Response â€“Open-source Model 1
é›ªå±±è‹è‹ç¬¼å¯’æ—¥ï¼Œéœœæž—ç‘Ÿç‘ŸåŠ¨æ¸…å£°ã€‚
Response â€“Open-source Model 2
"å…‰ç§ƒçš„æ ‘æœ¨é«˜è€¸ï¼Œ
é‡‘è‰²çš„è‰²è°ƒäº²å»ç€ç™½é›ªï¼Œ
ç¾¤å±±ä½Žè¯­ç€å¤è€çš„æ•…äº‹ï¼Œ
ç™½æ˜¼çš„æ‹¥æŠ±å¼€å§‹æ¶ˆå¤±ã€‚"
Prompt Response â€“DeepSeek-VL
å›¾ä¸­çš„è§’è‰²æ˜¯å“ªå®¶å…¬å¸çš„ï¼Ÿ
å›¾ä¸­çš„è§’è‰²æ˜¯miHoYoï¼ˆç±³å“ˆæ¸¸ï¼‰å…¬å¸çš„ã€‚è¿™å®¶å…¬å¸ä»¥å…¶å¼€å‘çš„æ‰‹æœºæ¸¸æˆã€ŠåŽŸç¥žã€‹è€Œé—»åï¼Œè¯¥æ¸¸
æˆä¸­çš„è§’è‰²è®¾è®¡é£Žæ ¼ä¸Žå›¾ç‰‡ä¸­è§’è‰²çš„é£Žæ ¼ç›¸ä¼¼ã€‚æ­¤å¤–ï¼Œå›¾ç‰‡ä¸­çš„è§’è‰²ç©¿ç€å’Œæ­¦å™¨è®¾è®¡ä¹Ÿç¬¦åˆ
miHoYoæ¸¸æˆçš„ç‰¹å¾ã€‚
Response â€“Open-source Model 1
å›¾åƒä¸­çš„è§’è‰²æ¥è‡ªæ¸¸æˆå…¬å¸â€œåŽŸç¥žâ€ã€‚
Response â€“Open-source Model 2
è¿™å¼ å›¾ç‰‡å±•ç¤ºçš„è§’è‰²æ˜¯æ—¥æœ¬Square Enixå…¬å¸æ——ä¸‹çš„ä¸€æ¬¾æ¸¸æˆã€Šæœ€ç»ˆå¹»æƒ³14ã€‹ä¸­çš„è§’è‰²ã€‚
Figure12 | Visualizationresults. DeepSeek-VLpossessesextensiveknowledgeoftherealworld.
31Prompt Response â€“DeepSeek-VL
Convert it to markdown table. Here is the image content converted into a markdown table:
Please note that the "A" column in the original table seems to be empty or not fully visible in the
image provided.
Response â€“Open-source Model 1
Response â€“Open-source Model 2
Figure13 | Visualizationresults. DeepSeek-VLiscapableofaccuratelyreadingthecontentsof
real-worldtables.
32