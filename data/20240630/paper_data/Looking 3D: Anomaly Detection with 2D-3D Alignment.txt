Looking 3D: Anomaly Detection with 2D-3D Alignment
AnkanBhunia ChangjianLi HakanBilen
UniversityofEdinburgh
https://groups.inf.ed.ac.uk/vico/research/Looking3D
Abstract Reference Query {‚Äúnormal‚Äù,‚Äúanomaly‚Äù}
3D Shape Image
Automaticanomalydetectionbasedonvisualcuesholds Classify
practical significance in various domains, such as manu-
Localize
facturingandproductqualityassessment. Thispaperintro-
ducesanewconditionalanomalydetectionproblem,which
Figure 1. We propose a new conditional AD task that aims to
involves identifying anomalies in a query image by com-
identifyandlocalizeanomaliesinaqueryimagebycomparingit
paring it to a reference shape. To address this challenge,
to a reference shape. The anomalous region is shown in a yel-
wehavecreatedalargedataset,BrokenChairs-180K,con- lowboundingbox. Forinstance, therightlegofthebluesofais
sisting of around 180K images, with diverse anomalies, rectangularunlikethecylindricaloneinitsreferenceshape.
geometries, and textures paired with 8,143 reference 3D
shapes. To tackle this task, we have proposed a novel instance. The AD here depends on whether the chair in-
transformer-basedapproachthatexplicitlylearnsthecorre- stancewasoriginallydesignedtohavethreelegs.
spondencebetweenthequeryimageandreference3Dshape
Motivated by the intuition above, this paper introduces
viafeaturealignmentandleveragesacustomizedattention
anovelconditionalADtask, alongwithanewbenchmark
mechanismforanomalydetection. Ourapproachhasbeen
andaneffectivesolution,thataimstoidentifyandlocalize
rigorously evaluated through comprehensive experiments,
anomaliesfromaphotoofanobjectinstance(i.e.,thequery
servingasabenchmarkforfutureresearchinthisdomain.
image),inrelationtoareference3Dmodel(seeFig.1).The
3D model provides the reference shape for the regular ob-
ject instance, and hence a clear definition of regularity for
thequeryimage.Thissettingismotivatedbyreal-worldap-
1.Introduction
plicationsininspectionandqualitycontrol,whereanobject
Anomalydetection(AD)[9,26],identifyinginstancesthat instance is manufactured based on a reference 3D model,
are irregular or significantly deviate from the normality, is whichcanthenbeusedtoidentifyanomalies(e.g.,produc-
an actively studied problem in several fields. In standard tionfaults,damages)fromaphotooftheinstance.
visionADbenchmarks,‚Äòirregularities‚Äôaretypicallycaused Theproposedtaskgoesbeyondthesingleimageanaly-
by either high-level (or semantic) variations such as pres- sis in standard AD benchmarks and requires the detection
ence of objects from unseen categories [1, 5, 8], defects ofsubtleanomaliesinshapebycomparingtwomodalities,
suchasscratches,dentsonobjects[4],low-levelvariations animagewithitsreference3Dmodel,whichischallenging
in color, shape, size [10], or pixel-level noise [16]. The for three reasons. First, we would like our model to de-
standard approach has been to learn representations along tect anomalies in previously unseen object instances from
with classifiers that are robust to the variations within the image-shape pairs at test time. Generalizing to unseen in-
regular set of instances, and, at the same time, sensitive to stances demands learning rich representations encoding a
theonescausingirregularities.However,thisparadigmper- diverse set of 3D shapes and appearances while enabling
formspoorlywhentheirregularitiesarearbitraryandcondi- accurate localization of anomalies. Second, the reference
tionaltothecontextand/orindividualcharacteristicsofthe 3D model contains only shape but not texture information
instancewhichmaynotbeknowninpriororobserved. For tosimulatearealisticscenariowherethe3Dmodelcanbe
instance,inanobjectcategorysuchas‚Äòchair‚Äôthatcontains used to produce instances with different materials, colors,
visually very diverse instances with huge intra-class varia- andtextures.Theresultingdomaingapbetweentwomodal-
tion, havingthreelegsmayimplyamissinglegandhence ities requires learning representations that are invariant to
ananomalyforachairinstance,whileregularityforanother such appearance changes and sensitive to variations in ge-
4202
nuJ
72
]VC.sc[
1v39391.6042:viXra
ylamonA noitceteDometry. Finally, in our benchmark, the viewpoint of the text. Wealsostudyaspecificapplicationoftheconditional
object instances in query images is not available in train- AD problem where the context information is instance-
ing. This requires the model to establish the local corre- specificandcomesfromareference3Dshape.
spondencesbetweenthemodalities,i.e.,corresponding3D AD image benchmarks. A major problem in the devel-
locationforeachimagepatchinanunsupervisedmanner. opment of AD is the lack of large datasets with realistic
To tackle the first challenge, we propose a new large- anomalies. For semantic anomalies, a common practice
scale dataset, BrokenChairs-180K, consisting of around (e.g.,[7,30])istoselectanarbitrarysubsetofclassesfrom
180K query images with diverse anomalies, geometries, an existing classification dataset (e.g., MNIST [20], CI-
andtexturespairedwith8,143reference3Dshapes. Train- FAR10[18]),treatthemasananomalousclass,andtraina
ing on such a diverse dataset enables learning rich multi- modelonlyontheremainingclasses.Therealsoexistmulti-
modal representations to generalize to unseen objects. To pledatasetsthatcontainreal-worldanomalousinstancesin-
addressthedomaingapbetweenthequeryimagesandref- cludingirregularlyshapedobjects[31],objectswithvarious
erence shapes, we follow two strategies. First, we render defectssuchasscratches,dents,contaminations[3],various
eachreferenceshapefrommultipleviewpointstogenerate defectsinnanofibrousmaterial[6]whichfocusononesam-
a set of multi-view images to represent the 3D shape and pleatatime. Aconcurrentwork,PAD[38]targetsasimilar
use them as input along with a query image to our model. objective with ours, while our task has fewer assumptions
The multi-view representation facilitates learning domain- and is designed to detect fine-grained geometrical anoma-
invariantrepresentationsthroughsharingthesameencoder lies. Moreover, compared to the PAD dataset consisting
across query and multi-view images. Second, our model, of only 20 LEGO bricks of animal toys, ours comprises a
Correspondence Matching Transformer (CMT) learns to large-scalecollectionofrealisticchairswithdiversegeome-
capture cross-modality relationships by applying a novel tries,textures,andawiderrangeoffine-grainedanomalies.
cross-attentionmechanismthroughasparsesetoflocalcor- 2D-3D cross-modal correlation. Image-based 3D shape
respondences. Finally, to address the third challenge, we retrieval [13, 14, 23] is a related problem that aims to re-
use an auxiliary task that forces the model to learn view- trieve the most similar shape for a given 2D image. Most
pointinvariantrepresentationsforeachlocalpatchinquery existing works learn to embed 2D images and 3D shapes
and multi-view images enabling our method to align local into a common feature space and perform metric learning
features corresponding to the same 3D location regardless using a triplet loss. Different to the retrieval task that pri-
ofitsviewpointwithoutgroundtruthcorrespondences. marilyinvolvesglobal-levelmatching,ourfocusiscompre-
Insummary,ourmaincontributionsarethreefold,intro- hendingthecorrelationoffine-grainedlocaldetailsbetween
ducinganovelADtask,alarge-scalebenchmarktoprovide theshapeandtheimagetodetectanomalieswithintheim-
atestbedforfutureresearch,andacustomizedsolution.Our age. Another related area focuses on learning of 2D-3D
model includes multiple technical innovations including a correspondences [12, 21, 28, 36] by matching 2D and 3D
hybrid2D-3Drepresentationfor3Dshapes,atransformer- locally with a triplet loss [12, 28], matching images and
basedarchitecturethatjointlylearnstodenselyalignquery pointcloudswithacoarse-to-fineapproach[21],improving
and multi-view images from image-level supervision and matching robustness using a global-to-local Graph Neural
detect anomalies. Our results in extensive ablation studies Network [36]. 2D-3D correlation is also studied for spe-
clearly demonstrate that 3D information along with corre- cific applications such as object pose estimation [22, 37],
spondence matching yields significant improvements. We 3Dshapeestimation[15]andobjectdetectioninimagesby
also perform an additional perceptual study that evaluates usingasetof3Dmodels[2]. Unlikethemethodsdiscussed
the human performance on the task, showing that the pro- here,ourobjectiveistoidentifyandlocalizeanomaliesina
posed task is challenging. Finally, we evaluate our tech- given2Dqueryimageinrelationtoareference3Dmodel.
niqueonrealimagesshowingpromisingresults.
3.BuildingBrokenChairs-180K Dataset
2.RelatedWork
Tothebestofourknowledge,thereisnopriorlargepublic
ADmethods. Wereferto[9,26]fordetailedliteraturere- dataset with paired 3D shapes and images. Hence we in-
views. Unlike the standard AD techniques, we focus on a troduceBrokenChairs-180K,anewbenchmarkforthepro-
conditionalandmulti-modalADproblemwhichrequiresa posed conditional AD task. Our dataset focuses on gen-
jointanalysisofaqueryimagewithareference3Dshapeto erating samples from one category, namely ‚Äòchair‚Äô, which
detectlocalirregularitiesintheimage. includesvarioussubcategorieslikesofas,officechairs,and
Conditional/referentialAD.InmanyADapplications,the stools, while our generation pipeline is general and appli-
anomalyofaninstancedependsonitsspecificcontext[32]. cabletoothercategories. Wepickedthiscategoryaschairs
Forinstance, anomaloustemperaturechangescanbemore containaverywiderangeofshapes,appearances,andma-
accuratelydetectedinaparticularspatialandtemporalcon- terial combinations making them appealing for our exper-Rotational Missing Positional Rotational Swapped Swapped Broken Positional Rotational Broken Rotational
Figure2. ExampleanomalyinstancesfromourBrokenChairs-180K dataset. Ourdatasetconsistsofaround100K anomalyimages.
Inthetoprow,someexampleanomalyinstancesareshown,alongwiththegroundtruthboundingboxesandsegmentationmasksinthe
bottomrow. Theredmaskisusedtoindicatepartswithanomalies,andagreencontourlinehighlightstheirrespectiveregionspriorto
applyinganyanomaly,andtheboundingboxisshownasbluerectangularboxes.(figurebestviewedinzoom)
iments. In the following, we describe the generation pro- Table1.Datasetstatistics.Thefirstrowshowsthenumberofdis-
cedure,includinganomalycreationandrealisticimageren- tinct3Dchairinstancesutilizedinourdataset,thefollowingrow
indicatesthetotalnumberofimagesrenderedfromtheseshapes,
dering. Moredetailscanbefoundinthesupplementary.
andthethirdrowdenotes[min.,max.,median]valuescorrespond-
3.1.CreatingAnomalyfrom3DObjects ingtothenumberofviewsrenderedforeachshapeobject.
DifferenttypesofAnomalies
3D shape collection. To cover a wide variety of fine- Position Rotation Broken Swapped Missing #Anomaly #Normal
grained anomalies across various parts of chairs (e.g., leg, #Shapes 5,646 5,551 6,113 6,427 5,182 28,919 8,143
arm,andheadrest),westrivetocollect3Dshapesthatcome #Images 20,023 20,017 20,008 20,010 20,023 100,076 77,994
#Views [1,7,4] [1,7,4] [1,5,4] [1,7,4] [1,8,4] [1,8,4] [1,12,8]
withpartannotationsandthusopttoutilizethePartNet[25]
asourstartingpoint. PartNetisalarge-scaledatasetof3D able realistic rendering, we use photo-realistic relightable
objectsannotatedwithfine-grainedpartlabels.Itschaircat- materials from [27] represented as SVBRDF. In total, we
egoryisamongthemostpopulous,providingarichsource utilize 400 publicly available SVBRDF materials, encom-
of 3D shapes for our task. In particular, we use 8,143 3D passingvarioustypessuchaswood,plastic,leather,fabric,
chair shapes from PartNet. Given a 3D model of a chair and metal. Following PhotoShape [27], we automatically
and its part annotation, we automatically create anomalies assignamaterialtoeachsemanticpartofa3Dshape, and
byapplyinggeometricdeformationsasdescribedbelow. useBlender‚Äôs‚ÄúSmartUVprojection‚Äùalgorithmtoestimate
Generation of anomaly shapes. Our dataset covers five theUVmapsneededfortexturing.
anomaly scenarios (see Fig. 2) relevant to real-world ap- Renderingandviewselection.Werendereachshapefrom
plications. (1) Positional anomalies pertain to deviations variousviewpointssampledfromahemispherearoundthe
fromthedesignatedpositionofchairparts. Tocreateapo- object. The viewpoint is parameterized in spherical coor-
sitionalanomaly, werandomlyselectapartfromanormal dinates where azimuth values are sampled uniformly over
3D model and apply random translation. (2) Rotational [0,2œÄ) with an interval of œÄ/10 and elevation values are
anomalies are created by applying a 3D rotational trans- uniformly sampled in [œÄ/9,2œÄ/9]. The radius is fixed at
formation to a randomly selected 3D part. (3) Broken or 2.5 for all views. For anomaly shapes, we only keep the
damagedpartsconsistcasesthatstructuralcomponentsare rendering if the anomalous part is visible from the camera
broken or damaged. We synthetically generate breaks us- view.Weemployaqualitycontrolandverificationstep(see
ingBooleansubtractionfollowing[19]wherewefracturea supplementary)todiscardbad-qualitysamples.
chairpartbysubtractingarandomsphericalorcubicalgeo- Dataset Statistics. Our dataset comprises a total of 8,143
metricprimitivefromthepartmesh. (4)Componentswap- reference3Dshapes(normal),alongwitharound180Kim-
pinginvolvesswappingcommonpartsacrossdifferentchair agesrenderedataresolutionof256√ó256pixels. Among
instances(e.g.,‚Äòback-connector‚Äôofonechairisexchanged theseimages,100K containsanomalies,whiletheremain-
witha‚Äòback-connector‚Äôfromanotherchair),simulatingan ing are categorized as normal. Since in our solution, we
incorrectassemblyduringmanufacturing. (5)Missingcom- use textureless multi-view images to represent the refer-
ponentsinvolverandomlychoosingonepartandremoving ence3Dshape,wefurtherprovidegrayscalemulti-viewim-
it from the 3D shape. Next, we discuss the generation of ages1 rendered from 20 regularly sampled viewpoints for
queryimageswithphoto-realistictexture. each reference shape. However, the 3D representation is
not necessary to be multi-view images, alternative repre-
3.2.Photo-realisticRenderingof3Dobjects
sentationslikemesh,pointcloud,orvoxelcanbeobtained
Assigningmaterialsto3Dshapes. TheshapesinPartNet 1In practice, we clone grayscale values and convert each view to a
onlycontainbasictexturesbutnorealisticmaterials. Toen- three-channelimagebeforefeedingthemasinputtoourmodel.Correspondence Matching Transformer (CMT) View-Agnostic Local Feature Alignment (VLFA)
ùíó‚Äô
En3 cD
o
dP ino gs it (i 3o Dna Pl
E )
‚Ñù#√ó%! ‚Ñí!" ùúë ‚Ñù!√ó%√ó& ùõΩ ‚Ñù!√ó# ‚Ä¶$
Reference
3D Shape ùíó$ ùõæ ‚Ñù! ùë∑( ùõΩ InTo dp ic- eùëò s ùúë ùõΩ ‚Ñí!"(ùí´,ùí©)
‚Ä¶
‚Ñù#√ó%!
C
ùíó#
ùúë K V Pseudo Labeling (Unsupervised) ‚Ñí!"(ùí´(,ùí©))
ùë≠( [tok] {0,1} ùúë ùõΩ
MR ue ln tid -ve ir ee wd
s
shared ‚Ñù#√ó%"
Q
SA TKCA
‚Ñí#$%
‚Ä¶
ùúë √óùêµ Query
ùíá) ùúô: Correspondence-
Query Image (ùíí) Guided Attention (CGA) Known Labels Pseudo Labels
ùõæ 3DPE: Fourier Encoding + MLP ùúë Encoder: ResNet18-FPN ùõΩ MLP Projection Head TKCA Top-ùëòSparse Cross-Attention SA Self-Attention
‚Ñí!" View-Agnostic Alignment Loss ‚Ñí#$%Classification Loss (binary cross entropy) C Concatenation [tok]Learnable Token ùêµ Number of CGA blocks
Figure3. OverallarchitectureofourproposedCMTframeworkforconditionalADtask. OurCMTtakesthefollowinginputs: thequery
imageq andtherenderedmulti-viewimages{v }N . Weextractqueryfeaturesfq andmulti-viewfeaturesFv usingtheencoderœÜ.
n n=1
Additionally,weuse3Dpositionalencoding(3DPE)toobtain3DpositionalfeaturesPvforthemulti-viewimages.Next,FvandPvare
concatenatedandfedtothecorrespondence-guidedattention(CGA)network,denotedasœï,alongwiththequeryfeaturesfq. TheCGA
networkselectivelyconditionsthefinalpredictiononasmallsubsetofthemostrelatedpatchesfrommulti-viewimagesthroughatop-k
sparsecross-attention(TKCA)mechanism.Theview-agnosticlocalfeaturealignment(VLFA)servestoaligntheencoderoutputfeatures
toachieveview-agnosticrepresentationthroughsemi-supervisedlearning.
fromthereferenceshapeandadoptedbyfuturealgorithms Anidealclassifierœàmustidentifysubtleshapeirregular-
whensolvingtheconditionalADproblem. itiesinqbyfindingtherelevantpatchesinV foreachpatch
A detailed breakdown of these statistics is provided in in q and comparing them. One straightforward design to
Tab.1.Wedividedthedatasetintothreedistinctsets:138K relatepatchesacrossqueryandmulti-viewimagesistouse
images for training, 13K for validation, and 26K for test- the cross-attention module [34]. In particular, one can use
ing. Each set contains rendered images from a set of mu- localfeaturesextractedfromqasqueryandonesfromV as
tually exclusive 3D shapes. Hence, the evaluation is per- key and value matrices as input to the scaled dot-product
formed on previously unseen 3D shapes. Our dataset also attention in [34] to cross-correlate them while predicting
contains bounding box and segmentation mask, localizing theanomalylabel. Whilethisdesigncanimplicitlycapture
anyanomalousregion. such cross-correlations between patches from only image-
level supervision when trained with the loss in Eq. (1), it
4.ProposedMethod fails to perform better than a similar model that is trained
onlyonthequeryimagesinpractice(seeSec.5). Weposit
4.1.Overview
thatthefailuretoutilizeV isduetothedifficultyinestab-
Let q ‚àà R3√óH√óW be an H √ó W dimensional RGB im- lishingthecorrectcorrespondencesfromnoisycorrelations
age of an object captured from an unknown viewpoint between all patches pairs across query and multi-view im-
and V = {v }N be a set of H √ó W dimensional im- agesonlyfromimage-levelsupervision.
n n=1
ages that are rendered from the reference shape at N reg-
Toaddressthischallenge,weproposeanewmodel,cor-
ularly sampled viewpoints on a hemisphere. We assume
respondence matching transformer (CMT) that consists of
the model has access to the camera pose and depth map
a CNN encoder, a 3D positional encoding (3DPE) mod-
of each multi-view image. We wish to learn a classifier
œà : R3√óH√óW √ó RN√ó3√óH√óW ‚Üí [0,1] that takes in q ule, a correspondence-guided attention (CGA) network,
and lastlya view-agnostic localfeature alignment (VLFA)
and V and predicts the ground-truth binary anomaly label
mechanism (see Fig. 3). While the 3DPE module encodes
y ‚àà {0,1}. Given a labeled training set D including |D|
the 3D location of the patches in multi-view images and
query,multi-view,andlabeltriplets(q,V,y),theclassifier
facilitates finding local correspondences across views, the
canbeoptimizedbyminimizingthelossterm:
CGA network selectively conditions the final prediction
(cid:88) on a small subset of the most related patches from multi-
L (D)= ‚Ñì (œà(q,V),y) (1)
bce bce
viewimagesthroughatop-ksparsecross-attention(TKCA)
(q,V,y)‚ààD
mechanism. Finally, VLFA provides a richer supervision
where‚Ñì isthebinarycross-entropylossfunction. signaltoestablishcorrespondencesbetweensimilarregions
bce
)ùëß,ùë¶,ùë•(
nettalf
nettalf
...
...
...
segamI
weiv-itluMmatrixpv. Forthenextsteps,wegatherfv andpv overN
views, andconcatenateeachsetalongtheirseconddimen-
sions, resultingin Fv ‚àà Rd√ónv and Pv ‚àà Rd√ónv respec-
tivelywherenv =N√ónq.AugmentingFvwithPvresults
inanovelhybrid2D-3Drepresentationbyincorporatingex-
plicit3Dinformationintothe2Dmulti-viewimages.
Correspondence-Guided Attention (CGA). The CGA
network œï, as illustrated in Fig. 4, takes in fq, Fv, Pv
andpredictstheanomalylabelwhileefficientlycomputing
the correlations across two modalities. CGA comprises B
consecutive transformer blocks where each block contains
multiple operations and is indexed by subscript b. In par-
Figure4. Ourproposedcorrespondence-guidedattention(CGA).
ticular, the block b starts with concatenating Fv and Pv
The CGA comprises B transformer-based blocks, each consist-
along their first dimension, then the resulting 2d√ónv di-
ingofastandardself-attentionmodulefollowedbyatop-ksparse
cross-attention(TKCA)module. mensionalmatrixisreducedtod√ónv dimensionalF¬Øv ma-
trix through a linear projection layer Œ±(b) : R2d ‚Üí Rd
in the query and the multi-view images by using semi- (Eq. (2)). After self-attention operation (SA) is applied to
supervisedlearning. Next,wedescribethemindetail. thequeryfeaturesfq wherefq = fq (Eq.(3)), itcom-
(b) (1)
putesthequeryQ ‚àà Rd√ónq (Eq.(4))andkey-valuema-
4.2.CorrespondenceMatchingTransformer (b)
tricesK ‚àà Rd√ónv,V ‚àà Rd√ónv (Eq.(5))byapplying
(b) (b)
CMT uses ResNet18 feature pyramid network [24] as the thelinearprojectionsWQ,K,V ‚ààRd√ódrespectively.
feature encoder, which is denoted as œÜ : R3√óH√óW ‚Üí (cid:20) Fv(cid:21)
Rd√óh√ów where the input is down-scaled 8 times through F¬Ø (v b) ‚ÜêŒ± (b)( Pv ) (2)
the network (h = H/8 and w = W/8). Once we extract
f¬Øq ‚ÜêSA(fq ) (3)
the features of q and for each v, œÜ(q) and œÜ(v) respec- (b) (b)
tively, we reshape each of them to be d√ónq dimensional Q ‚ÜêWQf¬Øq (4)
matrices, denote them as fq and fv respectively, where (b) (b)
nq = h√ów. Each column in fq and fv corresponds to K (b) ‚ÜêWKF¬Ø (v b), V (b) ‚ÜêWVF¬Ø (v b) (5)
addimensionallocalfeature. Weusef[.j]notationtoindi- O ‚ÜêTKCA(Q ,K ,V ,M) (6)
(b) (b) (b) (b)
catej-thlocalfeatureorpatchencoding,aseachencoding
O ‚ÜêNorm(O +Q ) (7)
(b) (b) (b)
approximatelycorrespondstoalocalpatchintheinputim-
O ‚ÜêNorm(FFN(O )+O ) (8)
age due to locality in the convolutional encoder. Next, we (b) (b) (b)
describe key components of the CMT including the 3DPE f (q b+1) ‚ÜêO (b) (9)
andCGAmodules.
Next, we pass Q ,K ,V to our top-k sparse cross-
3D Positional Encoding (3DPE). While the multi-view (b) (b) (b)
attention(TKCA)module(seeEq.(6)). Unlikethevanilla
representation allows for a simple and efficient model de-
cross-attentionmoduleinstandardtransformers[11,34]in-
sign through a shared feature encoder for our task, it also
gesting all tokens for the attention computation, which is
makes 3D information less accessible and hence hampers
inefficientforourtaskandmayintroducenoisyinteractions
relating local features across different views accurately.
withirrelevantfeatures,potentiallydegradingperformance,
To mitigate this problem, we propose complementing the
TKCA calculates the attention between query and only a
multi-viewimageswith3Dinformation. Foreachpatchen-
smallsubsetofrelevantmulti-viewfeaturesusingasimilar-
codingfv[.j],wefirstlocatethecorrespondingimagepatch
itymatrixM:
in v and then compute the 3D position of the correspond-
ing patch x j ‚àà R3 in the world coordinates 3D using the TKCA(Q,K,V,M)=softmax(cid:18) TM(Q ‚àöK )(cid:19)
V (10)
known camera parameters and depth maps. Then we use k
d
Fourierencodingtoobtainahigher-dimensionalvectorfor
eachx andfurtherprocessitthroughanMLPblocktoob- whereTM isgivenby:
j k
tain a d dimensional 3DPE. Formally, we denote the joint
(cid:40)
mappingbyŒ≥ :R3 ‚ÜíRd. A[ij], ifM[ij]‚ààtop (M[i.])
TM(A)[ij]= k (11)
Compared to the 2D standard positional encoding used k ‚àí‚àû, otherwise
in transformer models [11], 3DPE encodes 3D object ge-
ometryintheworldspace. Foreachfv includingnq patch where top (M[i.]) operation selects the k most similar
k
encodings,wecomputeacorrespondingd√ónqdimensional featuresfrommulti-viewrepresentation(seeFig.5)fori-thQuery Image (ùêà!) view 1 view 2 view 3 view ùëÅ feature in the reference view to each local feature in the
query at each training step, after mapping their features to
theview-invariantspaceandnormalizingthem:
‚Ä¶ cÀÜ =argmaxzqTzv, (13)
i i j
j
wherezq = Œ≤(fq[.i]) andzv = Œ≤(fv[.j]) . Wecompute
i ‚à•Œ≤(fq[.i])‚à• j ‚à•Œ≤(fv[.j])‚à•
Figure 5. Top-k sparse attention-span visualization. For the thepseudo-labelforeachzq andstoretheminalook-upta-
query point (yellow), similarity heatmaps (first row) and top-k i
blePÀÜ(q,v,i)=cÀÜ. InanotheroneNÀÜ(q,v,i),westorethe
attention-span(secondrow)acrossmultipleviewsareshown. i
remaining set of reference view and index values that are
queryfeature.TocomputeM,weuseanauxiliaryfunction not the corresponding location. Then, using PÀÜ,NÀÜ as pos-
Œ≤ : Rd ‚Üí Rd,instantiatedasafourlayeredMLPfollowed itive and negative correspondences respectively, we mini-
byafinalchannel-wisenormalization,thatprojectsfq and mizeacontrastiveloss‚Ñì (q,v)overeachq-vpair:
va
eachviewinFvtoaview-agnosticfeaturespacewhereim-
ages corresponding to same object part in 3D has similar (cid:88)nq exp(zqTzv/œÑ)
representationsregardlessoftheirviewpoint. Toobtainthe ‚àílog i + ,
exp(zqTzv/œÑ)+ (cid:80) exp(zqTzv/œÑ)
similarity between the query and multi-view patches, we i=1 i + i j
j‚ààNÀÜ(q,v,i)
computethedotproductbetweentheirprojectedfeatures:
(14)
M
=Œ≤(fq)TŒ≤(Fv)‚ààRnq√ónv
. (12)
where œÑ is a temperature parameter and z +v = z Pv ÀÜ(q,v,i).
Due to the cost of computing the pseudo-correspondences
In contrast to other transformer architectures using sparse for all query features across all views, we compute them
attention [35], TCKA chooses the top-k elements based onlyforarandomsubsetofqueryfeaturesacrossrandomly
on a different source of information, geometric correspon- sampledviewsateachtrainingiteration.
dencescomputedacrosstwomodalities,andenablesanef- Learningcorrespondencesthroughself-learningalonein
ficientcomputationofthecross-attention,asthesameM is thepresenceofthedomaingapbetweenqueryandreference
used throughout the transformer blocks. After the cross- viewsisanoisyprocess. Hence,wealsoexploittheknown
correlation, the standard residual addition, normalization viewpoints of the multi-view images by densely aligning
and feedforward (FFN) layers are applied to obtain fq as theirlocalfeaturesineachviewpairv,v‚Ä≤ aftercomputing
input to the next block b +1 (Eqs. (7) to (9)). Note that thegroundtruthdensecorrespondencesbetweenthemand
weusemultipleheads,concatenatetheoutputsfrommulti- discarding the ones that are occluded in one of the views.
head attention and then derive the final attention results Thekeyassumptionhereisthataligningdifferentviewsby
throughlinearprojection. Weappendalearnabletokende- usingtheirgroundtruthlabelsenablesamoreaccuratecor-
noted as [tok] to construct the query inputs of the CGA respondence learning between query images and views, as
network. Through the transformer blocks, the output state theparametersoftheprojectionŒ≤aresharedacrosstwodo-
ofthe[tok]tokendevelopsaconsolidatedrepresentation mains. Asbefore,weformtwolook-uptablesP(v,v‚Ä≤)and
enrichedbylearnedshape-imagecorrelation,whichisused N(v,v‚Ä≤)tostorethepositiveandnegativecorrespondences
asinputtotheclassificationhead. between two views, and randomly subsample them. After
mappingthemtotheview-invariantspaceandnormalizing
4.3.View-AgnosticLocalFeatureAlignment
them, we compute and minimize Eq. (14) for the pairs in
As discussed above, image-level supervision alone is too thelook-uptables.
weak to capture fine localized correlations between q and TheobjectiveinEq.(1)canberewrittenas:
V. Thus, we introduce an auxiliary task, VLFA that aims
todenselyaligncorrespondingpartsbetweenqueryimages L (D)+aL (PÀÜ,NÀÜ)+(1‚àía)L (P,N) (15)
bce va va
andrelatedviews. ThroughŒ≤,welearntomapfq andfv
to a view-agnostic space such that their local features cor- where L (PÀÜ,NÀÜ) and L (P,N) are the alignment loss
va va
respondingtothesameobjectpartaremappedtoasimilar functions over query-view pairs and view-view pairs re-
point regardless of the viewpoint from which the image is spectively,aisalossbalancingweightsetto0.5.
captured. As the viewpoint of q is unknown, the ground-
truth correspondences between query and reference views 5.Experiments
cannotbeobtainedthroughinverserendering.
To this end, we use a self-labeling strategy to generate ImplementationDetails: TheencoderœÜtakesa3√ó256√ó
pseudo-correspondences by finding the most similar local 256 image as input and returns a 128 √ó 32 √ó 32 featureGT: anomaly GT: anomaly GT: anomaly GT: normal GT: normal GT: anomaly GT: normal GT: anomaly GT: anomaly GT: normal GT: anomaly GT: normal GT: anomaly
Pred: anomaly Pred: normal Pred: anomaly Pred: normal Pred: normal Pred: anomaly Pred: normal Pred: anomaly Pred: anomaly Pred: anomaly Pred: anomaly Pred: normal Pred: anomaly
Figure6.AnomalydetectionresultsonthetestsetusingourproposedCMTframework.Incorrectpredictionsaremarkedinred.
Table 2. Quantitative Comparison of the proposed CMT frame- Table3.Ablationofview-agnosticalignmentloss.Thelossfunc-
workwithseveralbaselinesintermsofareaundertheROCcurve tioncomprisestwocomponents: L (P,N)andL (PÀÜ,NÀÜ). Op-
va va
(AUC)andaccuracyscore.Forbothmetrics,higherisbetter. timalperformanceisachievedwhenbothlossesarecombined.
3DRef. Methods AUC(‚Üë) Accuracy(‚Üë) Lva(P,N) Lva(PÀÜ,NÀÜ) AUC(‚Üë) Accuracy(‚Üë)
ResNet18-FPN[24] 74.6 64.7 ‚úó ‚úó 78.5 68.3
‚úó ResNet18-FPNw/SAblocks 75.2 65.1 ‚úó ‚úì 78.6 68.1
VisionTransformer(ViT) [11] 75.4 65.2 ‚úì ‚úó 81.6 73.7
LFD[CVPR‚Äô19][14] - 64.9 ‚úì ‚úì 84.7 75.4
Linetal.[ICCV‚Äô21][23] - 67.8
A:CMT(w/oCGA,VLFA,3DPE) 76.1 66.8 3D shape retrieval techniques [14, 23] that learn to embed
‚úì B:CMT(w/oVLFA,3DPE) 76.3 67.1 2Dimagesand3Dshapesintoacommonfeaturespaceand
C:CMT(w/oCGA,3DPE) 80.8 72.3
D:CMT(w/o3DPE) 82.6 73.7 performmetriclearningusingatripletloss. Oncewetrain
Ours:CMT 84.7 75.4 theminourdataset,weevaluatethembyusingthedistance
between the query and reference shape embeddings to ob-
taintheclassificationscoreafterathresholdingstep. Based
block. Within the CGA network, we employ three trans-
onresultsinTab.2,wearguethatthesemethodsfailtolo-
former blocks (B = 3), and each applies 8-headed atten-
cate subtle variations in geometry, as the cross-modal cor-
tion.ThevalueofkinTCKAissetto100.Duringtraining,
relations are only learned at the image level missing fine-
we randomly select a subset of N = 10 views, and dur-
grainedlocalcorrespondencelearning.
ing testing, we utilize all 20 views. We apply basic data
Ablation of CGA, VLFA and 3DPE. Our first baseline
augmentationtothequeryimages, whichincludesrandom
(A) includes none of the three components but a standard
horizontalflipsandrandomcroppingof224√ó224regions,
cross-attention module to relate two modalities using all
followed by resizing the cropped regions back to the orig-
local patches. Surprisingly, A obtains only a 1.6% accu-
inal size of 256√ó256. We train our model for 20 epochs
racyimprovement overthequery-only baseline, indicating
using 4 Titan RTX GPUs, maintaining a batch size of 8 in
itsinabilitytofullyleveragethereferenceshape. Baseline
eachGPU,andusetheAdamoptimizerwithalearningrate
BincludesonlytheCGAcomponentwiththetop-k sparse
of2√ó10‚àí5.Werefertothesupplementaryformoredetails.
cross-attention, baseline C, contains the VLFA but with a
standard cross-attention. While baseline B does not show
5.1.Results
much improvement over A, baseline C performs signifi-
Sincethereisnorelatedpublicbenchmarkforourtask,we cantly better than A obtaining a 5.2% accuracy gain. This
define several challenging baselines to evaluate our CMT. clearly demonstrates the importance of the auxiliary task
Wereportthequantitativeresultsusingtwoevaluationmet- wherewelearnmatchingcorrespondencesfortheADtask
rics‚ÄìtheareaundertheROCcurve(AUC)andaccuracyin and that the CGA fails to acquire meaningful correspon-
Tab.2,andprovidequalitativeresultsinFig.6. dencesintheabsenceoftheVLFA.BaselineDthatemploys
Importance of 3D reference shape. To assess the signif- both CGA and VLFA further boosts the performance of C
icanceofusingthereferenceshape, weestablishbaselines through its selective sparse attention mechanism. Finally,
thatsolelyrelyonthequeryimagefordetectinganomalies. ourmodel,whichincludesallthecomponents,outperforms
As our first baseline, we use a ResNet18-FPN model that D thankstotheintroductionof3DPEthatfacilitatescorre-
takesinonlyqueryimagesasinput. Forthenexttwobase- spondingmatchingacrossdifferentviews.
lines, we add three self-attention blocks to ResNet18-FPN Ablationoflossfunctions. Tab.3reportsanablationstudy
anduseaViT[11]respectively. Tab.2showsthattherefer- on the loss functions used for learning the view-agnostic
ence 3D shape is crucial to good performance while CMT representation. Utilizing only query-view alignment loss
outperformsthebaselinesbymorethan10%inaccuracy. (L (PÀÜ,NÀÜ)) does not yield any advantages (row 2) over
va
Comparisonwithrelatedwork. Asthereisnopriorwork not employing any alignment loss (row 1). However, em-
designedforourproblem,wetaketworecentimage-based ploying the view-view alignment (L (P,N)) alone leads
va
.feR
D3
yreuQ85 84.6 84.7 86 ùëÅ!"#$%=5 ùëÅ!"#$%=10 normalorirregularinstanceswithbroken,removed,ormis-
84 83.4 84.6 85.5 aligned parts from various viewpoints. Background pix-
83 85 els in query images are removed by using a segmentation
82
ùëò=ùëò!"#
84.5 method [17] in a preprocessing step and also a synthetic
81.3
81 84 shadowwasaddedtomatchthetrainingimages. Toobtain
1 25 50 100 200 5 10 15 20
ùëò ùëÅ$%&$ thereference3Dshapes,wetakemultiplephotosofobject
Figure7.(left)AUCscoreunderdifferentvaluesofk.(right)Im-
instances while walking around them, use the 3D recon-
pactofnumbersofmulti-viewimagesduringtrainingandtesting.
structionsoftware[33],andfinallyapplyLaplaciansmooth-
ing to post-process it. Fig. 8 illustrates the results for two
3D Rref. Query 1 Query 2 Query 3 3D Ref. Query 1 Query 2 Query 3
regular reference shapes, each paired with three query im-
ages. In5outof6cases,ourmethodsuccessfullyclassifies
andlocalizestheanomalousparts,whileinthefailurecase,
GT: anomaly GT: anomaly GT: anomaly GT: anomaly GT: normal GT: normal
Pred: anomaly Pred: anomalyPred: anomaly Pred: anomaly Pred: normal Pred: anomaly itincorrectlyrelatestheself-occludedarmwithananomaly.
Figure 8. Evaluation on real data. The predicted anomalies are
shownintheblueboundingboxes. Anomalylocalization. Hereweadoptourmodeltolocal-
izeanomaliesintheformofaboundingbox,useabound-
toimprovedresults(row3). Theoptimalresultisachieved ing box regression head (a 4-layer MLP), and jointly train
whenbothcomponentsarecombined(row4). itwiththeothernetworkparametersbyusingL1regression
Sensitivitytok. Weanalyzetheperformanceunderdiffer- andgeneralizedIoUloss[29]. Thismodelachieves56.5%
entvaluesofk inFig.7(left). Comparedtothemaximum average precision on our dataset, outperforms a ViT base-
possiblek thatisN √ó32√ó32,weanalyzesignificantly line that is trained only on the query images, and obtains
max
smallerkvaluesand,amongthem,showk =100yieldsthe 42.6%. Moreover,jointlylearningtheclassificationandlo-
bestresult. Usingallavailabletokensresultsindeteriorated calization further boosts the classification performance to
performance(shownasthedottedhorizontalline)showing 85.9(+1.2)AUCand77.3%(+1.9)accuracy.
thatourtop-ksparseattentioniseffectiveineliminatingthe
User Perceptual Study. We also evaluated human perfor-
noisypatchesbyusingonlythektop-relatedones.
mance in our task and conducted a study with 100 partici-
SensitivitytoN. Fig.7(right)depictstheanalysisonthe pants. We presented each participant with 10 pairs of ref-
number of input views for training and testing, N train and erence shapes and query images, with each pair randomly
N test respectively. Tothisend, wetraintwoseparateCMT selected from a random subset of 200. We observe a hu-
modelswith5and10views,andevaluateeachusing5,10, man accuracy of 70.6%, showing that the proposed task is
15, and 20 views at test time. The plot shows that, while challenging,whileourCMTobtainsasuperioraccuracyof
increasingviewsinbothtrainingandtestinghelps,training 74.8%onthesamesubset.
with few views and testing on more views can provide a
goodtradeoffbetweentrainingtimeandperformance.
Viewpoint prediction. As a side product of establishing 6.Conclusion
the correspondences across the query image and views in
our model, we could estimate the camera viewpoint in the In this paper, we have introduced a new AD task, a new
queryimagew.r.t.areferenceshape. Tothisend,wecom- benchmark, and a customized solution inspired by qual-
pute dense correspondences between the query image and ity control and inspection scenarios in manufacturing. We
each view image, and then calculate the distance between showed that an accurate detection of fine-grained anoma-
thepixelcoordinatesofeachpointinthe queryimageand liesingeometryrequiresacarefulstudyofbothmodalities
their predicted correspondences in the multi-view images, jointly. Our method achieves this goal by learning dense
andchoosetheviewwiththelowestaveragedistanceasthe correspondenceacrossthosemodalitiesfromlimitedsuper-
approximate viewpoint. As a baseline, we train a ResNet vision. Our benchmark and method also have limitations.
withtheviewpointsupervisiononthenormalimagesonly, Duetothedifficultyandcostofobtainingrealdamagedob-
andevaluateitonthetestnormalqueryimages.Ourmodel, jects, our dataset contains only shapes and images of syn-
trained with no viewpoint supervision, achieves a signifi- thetic objects and currently is limited to a single yet very
cantly better accuracy (47% vs 89%) when predicting the diverse category of ‚Äòchair‚Äô, presence of only one anomaly
closestviewsuggestingthatourmodelimplicitlylearnsto ineachqueryimage,focusingonlyonshapeanomaliesex-
relatethequeryimagewiththeclosestviews. cludingtheappearancebasedonessuchasfading,discolor,
Evaluationonrealdata. Hereweapplyourmodel,which textureanomaly. Moreover,ourmodelassumesthatobject
is trained on the synthetic BrokenChairs-180K dataset, on instances are rigid, and cannot deal with articulations and
asmallsetofrealchairsamplesthatcontainmultiplepairs deformations,andrequiresanaccuratereference3Dshape
of the reference 3D shape, query images containing either foraccuratedetection.
CUA CUAReferences [17] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
[1] Faruk Ahmed and Aaron Courville. Detecting semantic
head,AlexanderCBerg,Wan-YenLo,etal. Segmentany-
anomalies. InAAAI,2020. 1
thing. arXivpreprintarXiv:2304.02643,2023. 8
[2] MathieuAubry,DanielMaturana,AlexeiAEfros,BryanC
[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
Russell, and Josef Sivic. Seeing 3d chairs: exemplar part-
layersoffeaturesfromtinyimages. 2009. 2
based2d-3dalignmentusingalargedatasetofcadmodels.
[19] NikolasLamb,SeanBanerjee,andNatashaKholgadeBaner-
InCVPR,2014. 2
jee. Deepjoin: Learningajointoccupancy,signeddistance,
[3] Paul Bergmann, Michael Fauser, David Sattlegger, and
and normal field function for shape repair. In ACM TOG,
Carsten Steger. Mvtec ad‚Äìa comprehensive real-world
2022. 3
datasetforunsupervisedanomalydetection.InCVPR,2019.
[20] Yann LeCun, Le¬¥on Bottou, Yoshua Bengio, and Patrick
2
Haffner.Gradient-basedlearningappliedtodocumentrecog-
[4] PaulBergmann,KilianBatzner,MichaelFauser,DavidSat-
nition. ProceedingsoftheIEEE,1998. 2
tlegger, and Carsten Steger. The mvtec anomaly detection
[21] Minhao Li, Zheng Qin, Zhirui Gao, Renjiao Yi, Chenyang
dataset:acomprehensivereal-worlddatasetforunsupervised
Zhu, YulanGuo, andKaiXu. 2d3d-matr: 2d-3dmatching
anomalydetection. IJCV,2021. 1
transformer for detection-free registration between images
[5] Hermann Blum, Paul-Edouard Sarlin, Juan Nieto, Roland
andpointclouds. InICCV,2023. 2
Siegwart, and Cesar Cadena. The fishyscapes benchmark:
[22] Joseph J Lim, Hamed Pirsiavash, and Antonio Torralba.
Measuring blind spots in semantic segmentation. IJCV,
Parsingikeaobjects: Fineposeestimation. InICCV,2013.
2021. 1
2
[6] DiegoCarrera,FabioManganini,GiacomoBoracchi,andEt-
[23] Ming-Xian Lin, Jie Yang, He Wang, Yu-Kun Lai, Rongfei
toreLanzarone. Defectdetectioninsemimagesofnanofi-
Jia, Binqiang Zhao, and Lin Gao. Single image 3d shape
brousmaterials. IEEETransactionsonIndustrialInformat-
retrieval via cross-modal instance and category contrastive
ics,2016. 2
learning. InICCV,2021. 2,7
[7] RaghavendraChalapathy,AdityaKrishnaMenon,andSan-
[24] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,
jayChawla. Anomalydetectionusingone-classneuralnet-
Bharath Hariharan, and Serge Belongie. Feature pyramid
works. arXivpreprintarXiv:1802.06360,2018. 2
networksforobjectdetection. InCVPR,2017. 5,7
[8] Robin Chan, Krzysztof Lis, Svenja Uhlemeyer, Hermann
[25] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna
Blum, Sina Honari, Roland Siegwart, Pascal Fua, Math-
Tripathi,LeonidasJGuibas,andHaoSu. Partnet: Alarge-
ieu Salzmann, and Matthias Rottmann. Segmentmeifyou-
scalebenchmarkforfine-grainedandhierarchicalpart-level
can:Abenchmarkforanomalysegmentation.arXivpreprint
3dobjectunderstanding. InCVPR,2019. 3
arXiv:2104.14812,2021. 1
[9] Varun Chandola, Arindam Banerjee, and Vipin Kumar. [26] GuansongPang, ChunhuaShen, LongbingCao, andAnton
Anomaly detection: A survey. ACM computing surveys, VanDenHengel. Deeplearningforanomalydetection: A
2009. 1,2 review. ACMcomputingsurveys,2021. 1,2
[10] Lucas Deecke, Lukas Ruff, Robert A Vandermeulen, and [27] Keunhong Park, Konstantinos Rematas, Ali Farhadi, and
Hakan Bilen. Transfer-based semantic anomaly detection. Steven M Seitz. Photoshape: Photorealistic materials for
InICML,2021. 1 large-scaleshapecollections. InSIGGRAPHAsia,2018. 3
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, [28] Quang-Hieu Pham, Mikaela Angelina Uy, Binh-Son Hua,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, DucThanhNguyen,GemmaRoig,andSai-KitYeung. Lcd:
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- Learned cross-domain descriptors for 2d-3d matching. In
vain Gelly, et al. An image is worth 16x16 words: Trans- AAAI,2020. 2
formers for image recognition at scale. In ICLR, 2021. 5, [29] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir
7 Sadeghian, Ian Reid, and Silvio Savarese. Generalized in-
[12] Mengdan Feng, Sixing Hu, Marcelo H Ang, and Gim Hee tersectionoverunion: Ametricandalossforboundingbox
Lee. 2d3d-matchnet:Learningtomatchkeypointsacross2d regression. InCVPR,2019. 8
imageand3dpointcloud. InICRA,2019. 2 [30] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas
[13] AlexanderGrabner,PeterMRoth,andVincentLepetit. 3d Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Em-
pose estimation and 3d model retrieval for objects in the manuel Mu¬®ller, and Marius Kloft. Deep one-class classifi-
wild. InCVPR,2018. 2 cation. InICML,2018. 2
[14] AlexanderGrabner,PeterMRoth,andVincentLepetit. Lo- [31] Babak Saleh, Ali Farhadi, and Ahmed Elgammal. Object-
cationfielddescriptors: Singleimage3dmodelretrievalin centric anomaly detection by attribute-based reasoning. In
thewild. In3DV,2019. 2,7 CVPR,2013. 2
[15] MohsenHejratiandDevaRamanan.Analyzing3dobjectsin [32] XiuyaoSong,MingxiWu,ChristopherJermaine,andSanjay
clutteredimages. NeurIPS,2012. 2 Ranka. Conditionalanomalydetection. IEEETKDE,2007.
[16] Dan Hendrycks and Kevin Gimpel. A baseline for detect- 2
ingmisclassifiedandout-of-distributionexamplesinneural [33] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,
networks. InICLR,2017. 1 Brent Yi, Terrance Wang, Alexander Kristoffersen, JakeAustin, KamyarSalahi, AbhikAhuja, etal. Nerfstudio: A
modular framework for neural radiance field development.
InACMSIGGRAPH,2023. 8
[34] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
reit,LlionJones,AidanNGomez,≈ÅukaszKaiser,andIllia
Polosukhin. Attentionisallyouneed. InNeurIPS,2017. 4,
5
[35] Pichao Wang, Xue Wang, Fan Wang, Ming Lin, Shuning
Chang,HaoLi,andRongJin. Kvt:k-nnattentionforboost-
ingvisiontransformers. InECCV,2022. 6
[36] Shuzhe Wang, Juho Kannala, and Daniel Barath. Dgc-
gnn: Descriptor-free geometric-color graph neural network
for2d-3dmatching.arXivpreprintarXiv:2306.12547,2023.
2
[37] JianxiongXiao, BryanRussell, andAntonioTorralba. Lo-
calizing3dcuboidsinsingle-viewimages. NeurIPS,2012.
2
[38] QiangZhou,WeizeLi,LihanJiang,GuoliangWang,Guyue
Zhou, Shanghang Zhang, and Hao Zhao. Pad: A dataset
and benchmark for pose-agnostic anomaly detection. In
NeurIPS,2024. 2