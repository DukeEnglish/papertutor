Dataset Size Recovery from LoRA Weights
MohammadSalama JonathanKahana EliahuHorwitz YedidHoshen
SchoolofComputerScienceandEngineering
TheHebrewUniversityofJerusalem,Israel
https://vision.huji.ac.il/dsire/
{ mohammad.salama3, jonathan.kahana, eliahu.horwitz, yedid.hoshen}@mail.huji.ac.il
Abstract
Modelinversionandmembershipinferenceattacksaimtoreconstructandverify
thedatawhichamodelwastrainedon. However,theyarenotguaranteedtofind
alltrainingsamplesastheydonotknowthesizeofthetrainingset. Inthispaper,
weintroduceanewtask: datasetsizerecovery,thataimstodeterminethenumber
of samples used to train a model, directly from its weights. We then propose
DSiRe,amethodforrecoveringthenumberofimagesusedtofine-tuneamodel,
in the common case where fine-tuning uses LoRA. We discover that both the
normandthespectrumoftheLoRAmatricesarecloselylinkedtothefine-tuning
datasetsize;weleveragethisfindingtoproposeasimpleyeteffectiveprediction
algorithm. ToevaluatedatasetsizerecoveryofLoRAweights,wedevelopand
releaseanewbenchmark,LoRA-WiSE,consistingofover25,000weightsnapshots
frommorethan2,000diverseLoRAfine-tunedmodels. Ourbestclassifiercan
predictthenumberoffine-tuningimageswithameanabsoluteerrorof0.36images,
establishingthefeasibilityofthisattack.
1 Introduction
Dataisthetopfactorforthesuccessofmachinelearningmodels. Modelinversion[13,52,14]and
membershipinferenceattacks[5,42,25]aimtoreconstructandverifythetrainingdataofamodel,
using its weights[14, 11, 36]. While these methods may discover some of the training data, they
arenotguaranteedtorecoveralltrainingsamples. Onefundamentallimitthatpreventsthemfrom
discoveringtheentiretyofthetrainingdataisthattheydonothaveahaltingcondition,astheydo
notknowthesizeofthetrainingset[14]. E.g.,inmembershipinference,theattackersequentially
testsasetofimagesformembershipinthetrainingsetbutdoesnotknowwhentohaltthealgorithm,
effectivelymakingitsruntimeinfinite.
Discoveringthesizeofatrainingdatasetgiventhemodelweightsisimportant,evenwithoutexplicit
reconstructionoftheimagesthemselves. AstockphotographyprovidersuchasGettyorShuterstock,
mayallowuserstousetheirdataforfine-tuningpersonalizedgenerativemodelsandchargethem
forthenumberofimagesactuallyusedfortraining. Quantifyingthedatasetsizewouldthereforebe
essentialforbilling. Understandingthenumberofimagesusedtotrainorfine-tunemodelsisalsoof
greatinteresttoresearchers,whowishtounderstandthecostsofreplicatingamodelâ€™sperformance.
Wethereforeproposeanewtask: DatasetSizeRecovery,whichaimstorecoverthenumberoftraining
imagesbasedonthemodelâ€™sweights. Wetackletheimportantspecialcaseofrecoveringthenumber
ofimagesusedtofine-tuneamodel,wherefine-tuningusesLow-RankAdaption(LoRA)[21]. These
LoRApersonalizedtext-to-imagefoundationmodels[39]areamongthemostcommonlytrained
models,asevidentbythesuccessofmarketplacesforpublicsharingofthemsuchasCivitAIand
HuggingFace.
Preprint. Underreview.
4202
nuJ
72
]VC.sc[
1v59391.6042:viXraDSiRe
Finetuning
P r e M-T or da ein led LoRA
ð‘‘WL
eoð‘Ÿ
iR gA hts
SVD ðœŽ ðœŽðœŽ
ðœŽ . .
.1 2
3
ð‘Ÿ
KNN
ð‘ ð‘ ð‘ 
. .
.
ð¿21
M Va ojo tr eity ð‘›âˆ—
ð‘› samples ð¿
ð‘€ð´ð¸
ð’
2
1
Invisible to Attacker
Figure1: DSiRe: Weintroducethetaskofdatasetsizerecovery,whichaimstorecoverthedataset
sizeusedtoLoRAfine-tuneamodelbasedonitsweights. DSiReextractsthesingularvaluesofeach
LoRAmatrixandtreatsthemasfeatures. Thesefeaturesarethenusedtotrainasetoflayer-specific
nearest-neighborclassifierswhichpredictthedatasetsize.
WefirstanalyzetherelationshipbetweenLoRAfine-tuningweightsandtheircorrespondingdataset
size. OurobservationssuggestthattheFrobeniusnormofLoRAmatricesishighlypredictiveoftheir
fine-tuningdatasetsize. Asasinglescalarperweightmatrixprovideslimitedexpressivity,singular
valuesofeachweightmatrixserveasmoreexpressivefeaturevectors. Withthisinmind,wepresent
DSiRe(DatasetSizeRecovery). Themethodrecoversthefine-tuningdatasetsizefromtheLoRA
weightspectrum. DSiReclassifiesthedatasetsizeusingatrainedclassifierontopoftheweight
spectrumofthelayer. Inpractice,wefoundthataverysimpleclassifiersuffices;inourexperiments,
DSiReusesthenearestneighborclassifier. ForanoverviewofthemethodseeFig1.
Toenabletheevaluationofthisworkandencouragefutureresearch,wereleaseanew,large-scale,
anddiversedataset: LoRA-WiSE.Thedatasetcomprisesover25,000weightscheckpointsdrawnfrom
morethan2,000independentLoRAmodels,spanningdifferentdatasetsizes,backbones,ranks,and
personalizationsets. OnLoRA-WiSE,DSiRerecoversthedatasetsizesfromLoRAweightswitha
MeanAbsoluteError(MAE)of0.36,demonstratingthatourmethodishighlyeffectiveinrealistic
settings.
Tosummarize,ourmaincontributionsare:
1. Introducingthetaskofdatasetsizerecovery.
2. PresentingDSiRe,amethodforrecoveringdatasetsizeforLoRAfine-tuning.
3. ReleasingLoRA-WiSE,acomprehensivedatasetsizerecoveryevaluationsuitebasedon
Stable-Diffusion-fine-tunedLoRAs.
2 Relatedwork
2.1 ModelFine-Tuning
Modelfine-tuning[58,56,3]adaptsamodelforadownstreamtaskandisconsideredacornerstone
inmachinelearning. Theemergenceoflargefoundationmodels[37,45,4,38]hasmadestandard
fine-tuningcostlyandunattainablewithoutsubstantialresources. Parameter-EfficientFine-Tuning
(PEFT)methodswerethenproposed[21,10,20,29,28,32,16,31,26,59,47,24],offeringvarious
waystofine-tunemodelswithfeweroptimizedparameters. Amongthesemethods,LoRA[21]stands
out, proposing to train additive low-rank weight matrices while keeping the pre-trained weights
frozen. LoRAwasfoundtobeveryeffectiveacrossseveralmodalities[48,53,2]. Recently,Horwitz
etal.[19]identifiedasecurityissueinLoRAfine-tuning,demonstratingthatmultipleLoRAscan
2be used to recover the original pre-trained weights. In this paper, we uncover a new use case of
LoRAfine-tuning,specificallyfocusingontherecoveryofthedatasetsizefromtext-to-imagemodels
fine-tunedviaLoRA.
2.2 MembershipInference&ModelInversionAttacks
Two privacy vulnerabilities found in machine learning models are Membership Inference Attack
(MIA)[41,5,23,42,25]andModelInversion[13,52,17,55,14]. Firstpresentedby[44],MIAs
aimtoverifywhetheracertainimagewasinthetrainingdatasetofagivenmodel. Typically,MIAs
assumesthattrainingsamplesareover-fittedproposingvariousmembershipcriteria;eitherbylooking
forlowerlossvalues[40,54]orsomeothermetrics[49,5]. Ingenerativemodels,MIAshavebeen
extensivelyresearchedaswell[18,15,6],includingrecentattacksagainstdiffusionmodels[35,22].
Modelinversionisasimilarattack,inadata-freesetting. Introducedby[13],modelinversionmethods
wishtogeneratetrainingsamplesfromscratch,insteadofaskingwhetheraknownspecificimagewas
inthetrainingset. Modelinversionisalsousedforsettingswheredataisunavailable,e.g.,data-free
quantization[7,51,30]anddata-freedistillation[33,60,57,12,43].
Haimetal.[14]emphasizedtheimportanceofrecoveringthetrainingsetsizeformodelinversion
applications. Whenthissizeisunknown,itpreventsmodelinversionattacksfromreconstructing
the entire dataset a model was trained on, as it is unclear how many samples are sufficient. Our
workspecificallyaddressesthisissuebyuncoveringanewvulnerabilityinfine-tunedmodels,which
enablesustoinferthesizeofthedatasetusedforfine-tuning.
3 Motivation
3.1 Background: LoRAfine-tuning.
Fine-tuninglargefoundationmodelscanbeacomputationallyexpensiveprocess,asitmodifieseach
weightmatrixofthepre-trainedmodelW âˆˆRdÃ—k,byanadditivefine-tuningmatrixâˆ†W,asfollows:
Wâ€² =W +âˆ†W (1)
Recently,Huetal.[21]introducedLow-RankAdaptation(LoRA)forefficientfine-tuning. InLoRA,
thefine-tuningmatrixâˆ†W islow-rank,i.e.,wechooseitsrankrs.t. r â‰ªmin(d,k). Anefficient
i
andSGDamenablewaytoimplementlow-rankmatricesistoparameterizethemastheproductoftwo
rectangularmatricessothatB âˆˆRdÃ—r,A âˆˆRrÃ—k. Thefine-tuningmatrixisthereforegivenby:
i i
âˆ†W =B A (2)
i i i
3.2 AnalysingtheLoRASpectrum.
Ourhypothesisisthatthedifferencebetweenpre-fine-tuningandpost-fine-tuningweights,denoted
asâˆ†W ,encodesinformationaboutthesizeofthefine-tuningdataset. Here,wefocusonthecase
i
wherefine-tuningusesLoRA,i.e.,whereâˆ†W islow-rank,whichisemergingasthemostpopular
i
fine-tuningparadigmforfoundationmodels.
Webeginbyconsideringaverysimplestatisticofeachfine-tuningmatrix,itsFrobeniusnormthatwe
denotes .
F
(cid:88)
s = |âˆ†W |2 (3)
F ij
ij
Thenormofaweightmatrixcorrelateswiththefunctioncomplexitythenetworkcanexpress. For
example,weightdecay,thateffectivelyconstrainsthenormisacommonwayforregularizingmodels.
Wesuggestasimpleexperimenttoanalyzethecorrelationbetweens andthefine-tuningdataset
F
size. Wefine-tuneStableDiffusion(SD)1.5on50micro-datasetsofsizes1to6images,keeping
all hyper-parameters, apart from the dataset size, fixed. Fig. 2a shows the range of values of the
Frobeniusnormstatistics foreachdatasetsize. Itisclearthats isnegativelycorrelatedtothe
F F
datasetsize. Wemotivatethiscorrelationbyover-fitting,i.e.,modelsover-fitfastertosmallerdataset
sizes,leadingtolargersizesofs .
F
Toobtainadeeperunderstanding,wemonitoranotherstatisticofthefine-tuningmatrix,itssingular
valuesspectrum. AsweconsiderthecaseofLoRAfine-tuning,thespectrumofâˆ†W hasatmostr
3(a) (b)
Figure2: NormandSpectrumofFine-TuningWeightsvs. DatasetSize. Analysisof210Stable
Diffusion1.5modelsfine-tunedondatasetsofsizesfrom1âˆ’6. (a)Frobeniusnormrangeperdataset
size(b)Singularvaluesperdatasetsize. Thereisaclearnegativecorrelationbetweenweight/spectrum
magnitudesandthesizeofthefine-tuningdataset.
(a) (b)
Figure3: SpectrumRangesof2DifferentLayers. Singularvaluesdistributionoftwolayerson
oppositesidesofStableDiffusion1.5UNet,fine-tunedondatasetsofsizes1âˆ’6. (a)Firstdownblock
(b)Lastupperblock. Thelastupperblockshowsgreaterseparationofsingularvaluescomparedto
thefirstdownblock,highlightingthatnotalllayersarebornequallyfordatasetsizerecovery.
non-zerovalues(whereristherank). WedenotethemthspectralvalueasÏƒ ,wheremâˆˆ[1,...,r].
m
TherangeofvaluesofwereplottedÏƒ acrossdifferentvaluesofmanddifferentdatasetsizesin
m
Fig.2b. Wenotethereisabetterseparationbetweendifferentdatasetsizesforthelargestsingular
values. This suggests that the spectrum is more discriminative than the scalar Frobenius norm.
Overall,boths andthespectrumindicatelargervaluesforsmalldatasetsizes.
F
Finally,weanalyzedhowdiscriminativedifferentlayersareforpredictingfine-tuningdatasetsize.
Weplotthespectraoftwodifferentlayers-inthefirstdownblockandthelastupblockinFig.3. We
canseethatthelayerismorediscriminativethantheformerone. Indeed,theuplayersarefoundtobe
morediscriminativethanthedownones. Withoutaclearexplanation,itcanhypothesizethatthe
UNetdecoderismorepronetoover-fittingthantheencoder. Itisnoteworthythatourexperiments
revealedthatnosinglelayerisdiscriminativeforallmodels,suggestingthatweightingtheresults
fromalllayersisbetter.
4 Method
4.1 TaskDefinition: DatasetSizeRecovery
WeintroducethetaskofDatasetSizeRecoveryforfine-tuneddataset,anewattackvectoragainst
fine-tuned models. Formally, given the fine-tuning weights of all layers of a model denoted as
4âˆ†W = [âˆ†W ,âˆ†W ...âˆ†W ], our task is to recover the number of images n that the model was
1 2 L
fine-tunedon. Moreformally,wewishtofindafunctionf,suchthat:
n=f(âˆ†W) (4)
TheeffectivenessofthisattackwasmeasuredbytheMAEbetweenf(âˆ†W)andnacrossasetof
models.
4.2 DSiRe
WeproposeDSiRe(DatasetSizeRecovery),asupervisedmethodforrecoveringdatasetsizefrom
LoRAfine-tunedweights. Ourapproachfirstconstructsatrainingdatasetbyfine-tuningmultiple
LoRAmodelsonconceptpersonalizationsetsacrossarangeofdatasetsizes(seeSec. 5). Itthen
trainsapredictorfunctionf thatactsonasetoffine-tuningweightsofeachmodelandoutputsthe
predicteddatasetsizen. Attesttime,itgeneralizestounseenmodelstrainedwithdifferentconcepts.
ThemethodcanbeseeninFig.1
Trainingsetsynthesis. Wefirstsynthesizeatrainingsetbyfine-tuningourmodeloneachofN
train
datasets,eachcontainingasetoftrainingimages. Thedatasetsspanarangeofsizes;inthispaper,we
testedtheranges1âˆ’6,1âˆ’50,and1âˆ’1000. TheresultisasetofN modelsâˆ†W ,eachwitha
train m
correspondinglabelofthedatasetsizen .
m
Predictortraining. GiventhesetofN labeledmodels,wewishtotrainapredictorthatmaps
train
thefine-tuningweightsW todatasetsizen . Motivatedbytheresultsofouranalysis(seeSec.3),
m m
wedescribetheweightsofeachfine-tuninglayerusingitsspectrumÎ£consistingofrsingularvalues.
LetusdefinethefeaturesofeachmodelasthesetofspectraofallitsLlayersf =[Î£ ,Î£ ..Î£ ]. We
1 2 L
testedmanydifferentpredictorsandablatedtheminTab.5. Overall,thesimpleNearestNeighbor
(NN)ensembleperformedthebest. Duringinference,givenanewmodel,foreachlayer,weretrieve
theNNlayerthathasthemostsimilarspectrumtothelayerofthetargetmodel. Eachlayervotesfor
thedatasetsizeofitsNNlayer. Theoverallpredictionisthedatasetsizethatmostlayersvotedfor.
5 LoRAWiSEBenchmark
WepresenttheLoRAWeightSizeEvaluation(LoRA-WiSE)benchmark,acomprehensivebenchmark
specificallydesignedtoevaluateLoRAdatasetsizerecoverymethods,forgenerativemodels. More
specifically,itfeaturestheweightsof2350StableDiffusion[38]models,whichwereLoRAfine-tuned
byastandard,popularprotocol[39,1]. Ourbenchmarkincludesversions1.5and2ofStableDiffusion,
having2050and300trainedmodelsforeachversionrespectively.
Wefine-tunethemodelsusingthreedifferentrangesofdatasetsize: (i)Lowdatarange: 1âˆ’6images.
(ii)Mediumdatarange: 1âˆ’50images. (iii)Highdatarange: 1âˆ’1000. Foreachrange,weusea
discretesetoffine-tuningdatasetsizes. Inthelowandmediumranges,wealsoprovideotherversions
ofthesebenchmarkswithdifferentLoRAranksandbackbones. SeeTab.1fortheprecisebenchmark
details.
Forourlowdatarangeset,wechooseConcept101[27],apreviouslycollectedsetofmicro-datasets
(3âˆ’15images)designedforpersonalizationresearch. Forourmediumandhighdatarangesweuse
differentclassesofImageNet[9]asthedatasource. Thisselectionofdatasetsaimstoensurethatthe
fine-tunedmodelsaredrawnfromadiversesetofconcepts,spanningvariouscategories.
Eachmicro-datasetisusedtofine-tunethemodelsforeachdatasetsize. Theimagesarerandomly
selectedfromthemicro-dataset. EachStableDiffusionmodelconsistsof132adaptedlayers(pairsof
A ,B ),includingvariouslayertypes,suchasself-attention,cross-attention,andMLPs. WesaveA ,
i i i
B separately,i.e.,eachmodelprovidesatotalof264uniqueweightmatrices. Wethenspliteach
i
rangeofthisnewbenchmark(low,medium,andhighranges)intotrainandtestsetsbasedonthe
micro-datasets. formoredetailsseeappendix12.1
5Table1: LoRAWiSEBenchmarkOverview. Thedatasetcomprisesover25,000weightscheckpoints
drawnfrommorethan2000independentLoRAmodels,spanningdifferentdatasetsizes,backbones,
ranks,andpersonalizationsets.
DataRange DatasetSizes Source Backbone LoRARank #ofModels
8 300
Low 1,2,3,4,5,6 Concept101 SD1.5 16 300
32 300
16 300
SD1.5 32 300
Medium 1,10,20,30,40,50 ImageNet
64 300
SD2 32 300
High 1,50,100,500,1000 ImageNet SD1.5 32 250
6 Experiments
6.1 ExperimentalSetup
WeevaluateDSiReontheLoRA-WiSEbenchmark. Unlessmentionedotherwise, weuseStable
Diffusion1.5asthepre-trainedmodel, whichwefine-tuneusingaLoRAofrank32(denotedas
aboveâˆ†W). Asforthetrainingset,weuse15differentpersonalizationsets,totraineachâˆ†W model,
resultinginatrainingsetof90weightsamplesforlowandmediumdatasetsizes, and75weight
samplesforourhigher(1âˆ’1000)range. WeevaluateDSiReusing35additionalpersonalizationsets.
Wenote,Werepeatthisexperiment10times,includingsubsetsampling. Thereportedperformance
metricsaretheaverageandstandarddeviationovertheexperiments.
Baseline. WecompareDSiRetoabaseline,denotedasFrobenius-NN,whichpredictsthedataset
sizeusinganearestneighborclassifierontopoftheFrobeniusnormsofthelayersâ€™LoRAweights.
SimilartoDSiRe,theFrobenius-NNisfittedseparatelytoeachlayer,andthenamajorityvoterule
isappliedtoselectthepredictionfromalllayer-wisepredictions. TheanalysisinSec3provides
motivationforthisbaseline.
Evaluationmetrics. AsdescribedinSec. 4.1,ourmainevaluationmetricisMeanAbsoluteError
(MAE).Forcompleteness,wechoosetoreporttwocomplementarymetricsaswell: (i)Accuracy. (ii)
MeanAbsolutePercentageError(MAPE).SinceDSiRepredictsdatasetsizes,simpleaccuracydoes
notadequatelymeasureitseffectiveness,e.g.,predicting4whenthetruevalueis5isnotasbadas
predicting1. WethereforeprovideMAPEscoresaswell,whichcomputethepercentilefromthe
groundtruththatisequaltotheabsoluteerror.
6.2 Results
Webeginbyevaluatingon1âˆ’6fine-tuningimages. Whenusingboththesingularvaluesasfeatures
forDSiReandtheFrobeniusnormforFrobenius-NN,wefindthattheyyieldrelativelysuccessful
resultsinrecoveringthedatasetsize.
These low data range results are presented in Table 2. As DSiRe outperforms Frobenius-NN by
asmallmargin(> 3%),weconcludethattheresultsalignwithouranalysis(seeSec.3.2),which
demonstratethatbothsingularvaluesandFrobeniusnormareindeedpredictiveofthedatasetsize.
Mid-rangefine-tuningdatasetsizes(10âˆ’50images)arecommoninartisticLoRAfine-tuning. We
thereforepresenttheresultsofourmethodusing1âˆ’50fine-tuningimagesinTable2,showingthat
DSiReperformswellwithanMAEof1.48. Inthisdatarange,theFrobenius-NNbaselineachieves
comparableresultstoDSiReacrossallmetrics,demonstratinggoodperformance. Whiletheabsolute
MAEvalueislargerthaninthelowdatarangecase,itisrelativelysmallcomparedtotherangeof
datasizes. Theaccuracyandmeanabsolutepercentageerror(MAPE)scoresofbothmethodsfurther
supportthisobservation. Fig. 4showsanotherfavorablepropertyofourapproach: itsmistakesare
usuallynearhits,i.e.,largeerrorsbetweengroundtruthandpredictedlabelsarerare.
6Table2: PerformanceComparisonofDatasetSizeRecoveryMethodsacrossDifferentRanges.
PerformanceofFrobenius-NNandDSiReacrossdifferentdataranges(1âˆ’6,1âˆ’50,1âˆ’1000)using
StableDiffusion1.5. Thesedecentresultsalignswithouranalysis(seeSec3),showedthatbothSVD
andFrobeniusnormareeffectivefeaturesfordatasetsizerecovery. However,DSiReoutperfomrsthe
Frobenius-NNonallevaluationmetrics.
DataRange Method MAEâ†“ MAPE(%)â†“ Acc(%)â†‘
Frobenius-NN 0.43Â±0.04 15.14Â±2.12 65.29Â±2.42
1-6
DSiRe 0.36Â±0.04 11.36Â±1.55 69.30Â±3.83
Frobenius-NN 1.56Â±0.19 4.16Â±0.75 85.33Â±1.81
1-50
DSiRe 1.48Â±0.21 3.97Â±0.73 86.10Â±1.99
Frobenius-NN 68.62Â±5.53 9.25Â±1.21 86.51Â±1.12
1-1000
DSiRe 41.77Â±6.61 5.96Â±1.46 91.90Â±1.28
Table3: RobustnessofDatasetSizeRecoveryMethodsonStableDiffusion2.0. DSiRerecovers
datasetsizemoreeffectivelythanFrobenius-NNforthemediumdatarange(1âˆ’50)usingStable
Diffusion2.0. Thissupportsthebenefitfromamoreexpressiverepresentationgivenbythesingular
values,independenttothespecificbackbonemodelused.
Method MAEâ†“ MAPE(%)â†“ Acc(%)â†‘
Frobenius-NN 2.95Â±0.28 11.99Â±3.93 73.90Â±2.21
DSiRe 2.51Â±0.22 7.46Â±0.95 77.43Â±1.70
At larger data quantities, dataset size recovery could aid in better understanding data collection
quantitiesneededforfine-tuning. Therefore,weconductedanadditionalexperimentusingmodels
trainedwithhigherdataranges,having1,50,100,500and1000imagesamplespermodel(notethat
herewehave5datasetsizeclasses). Results,presentedinTab. 2,showsDSiReisabletodetectthe
datasetsizewithmorethan90%accuracy,andaMAPEscoreofonly6%. Additionally,inFig. 6we
showtheconfusionmatrixgeneratedbyDSiRe,whereweseethatmostoftheerrorshappenbetween
adjacentclasses.
OtherBackbone TheLoRAfine-tuningtechniqueiscommonlyusedbypopulartext-to-image
models. Adesirableaspectofourparadigmisbeingrobusttomodelarchitecture. Inthispart,We
testtherobustnessofDSiRetothebackbonemodelbyevaluatingitonStableDiffusion2.0. Wenote
thatthesemodelsdonotsharepre-trainingweights,asStableDiffusion2.0wasnotfine-tunedfroma
previousversion. Tab. 3showsthatDSiREperformswellonStableDiffusion2.0,reachingaround
77%accuracy. Thisprovidesevidenceforthecorrelationbetweenthesingularvaluesanddatasetsize
isnotspecifictoonebackbonealone.
7 Ablationstudies
7.1 NumberofMicro-Datasets
Whileourattackisdatadrivenandrequiresaccesstothepre-trainedmodel,wefindthatonlyafew
examplesareneededforDSiRetoperformwell. E.g.,inourmediumdatasizerange,ourmodelcan
reach86.4%accuracyusingonly5micro-datasetsfortraining. Thefullresults,presentedinFig.5,
showcasesthatwhilemoresamples(fine-tunedmodels)improvestheaccuracyofourpredictor,even
asinglemicro-datasetissufficienttoachievearound80%accuracy. Thisshowsthatourmethodis
robusttothenumberofmicro-datasetsused,eventoverysmallnumbers.
7.2 RobustnesstoLoRAHyper-Parameters
While DSiRe performs well in our settings, it is important to show that it remains robust to
hyperparameters. Inthissection,wewillablateourmethodusingtheLoRA-WiSEbenchmarkby
testingDSiRewithmodelsfine-tunedusingotherrecipes. Wechecktwocommonvariationsofthe
7Figure4: DSiReConfusionMatrixforMedium
Figure5: DSiResMicro-DatasetSizevs. Accu-
DataRangeinasingleexperiment. Illustrating
racy,reportedonmediumdatasizerange(1âˆ’50).
DSiResaccuracyintherangeof1âˆ’50samples,
Evenasinglemicro-datasetissufficientforDSiRe
shows that most of the errors are near misses,
to reach 80% accuracy. This demonstrates its
highlighting DSiReâ€™s precision in dataset size
effectivenesswithlimitedtrainingdata.
recovery.
Table4: DSiRePerformancewithDifferentLoRARanks. Desireconsistentlyachieveshighaccuracy
acrossbothlowandmediumranges,indicatingitsrobustnessregardlessofLoRArankvariations.
DataRange LoRARank MAEâ†“ MAPE(%)â†“ Acc(%)â†‘
8 0.43Â±0.04 14.8Â±2.3 66Â±3.08
1âˆ’6 16 0.42Â±0.03 12.4Â±1.11 67.7Â±2.3
32 0.36Â±0.04 11.36Â±1.55 69.30Â±3.83
16 1.67Â±0.17 4.32Â±0.46 84.04Â±1.85
1âˆ’50 32 1.48Â±0.21 3.97Â±0.73 86.10Â±1.99
64 1.41Â±0.39 3.90Â±1.30 86.58Â±3.45
LoRAmodels: (1)LoRArank(2)seeding. Foradditionalablationstudiesonthetrainingsteps,batch
size,andusedclassifierseeappendix. 11.2.
LoRARank. StartingwiththeLoRarankablation,wetrainDSiRefordifferentvaryingLoRAranks.
Tab. 4showstheresultsformediumandlowdataranges. OurmethodisrobusttotheLoRArank,
achievingsimilarresultsinall3testedranksforbothranges.
Seeding. Whileinthestandardrecipe,allmodelsuseseed = 0,wealsotestedthecasewhereall
seedswereselectedrandomly. Tab. 7showsthatthevariationinseedsonlyreducesaccuracyby
around4%,andthatMAEdecreasesbylessthan0.5. Thisisnotasmallchange,giventhatthegap
betweenpossibledatasetsizevaluesis10.
7.3 ChoiceofClassifier
Wetestedvariouspredictors,includingparametricandnon-parametricclassifiers,asshowninTable5.
ExcepttheNN-fullmodelbaseline,ourpipelineremainsunchanged,allclassifiersarefittedseparately
toeachlayerandamajorityvoteruleselectsthelabelfromalllayer-wisepredictions. Incontrast,
theNN-fullmodelusesakNNclassifierwhichisfittedtoallthelayerssimultaneously. Theresults
showthatthechoiceofclassifieraffectstheperformancesignificantly. Furthermore,theseresults
confirmourhypothesisfromSec. 3: whileeachlayerispredictiveofthedatasetsize,theaccuracyof
individuallayersisnotsufficient. Asthelayer-wiseNNpredictorcanpoolinformationacrosslayers,
ithelpsDSiReperformbetterthantheothermethods.
8 Discussion
PerformanceatLowDataRanges. Whileourapproachshowspromisingresults,thereisroomfor
improvementinlowerdataregimes,whereDSiRereacheslessthan80%accuracy. Improvingthese
resultswillprovidetighterupperboundsformembershipinferenceandmodelinversionattacks.
8Table 5: Performance of various predictors, medium dataset size range (1âˆ’50). DSiRe with
majorityvotingperformsbestbycombiningpredictionsfrommultiplelayers,opposedtothenearest
neighbor-fullmodelbaseline,whichusesthespectraofalllayerstogetherasfeaturesforasingle
prediction.
Model MAEâ†“ MAPE(%)â†“ Acc(%)â†‘
NN-fullmodel 7.33Â±0.75 33.37Â±7.71 48.61Â±3.53
RidgeRegression 8.05Â±0.08 38.95Â±6.24 37.09Â±0.63
GDA 2.89Â±0.48 7.87Â±1.30 74.14Â±3.65
DSiRe-averagevote 3.57Â±0.36 19.05Â±3.37 64.62Â±3.58
DSiRe-majorityvote 1.48Â±0.21 3.97Â±0.73 86.10Â±1.99
Table6: DefenseagainstDSiRe,onmediumdatarange(1âˆ’50). Weshowhowthesimplestdata
augmentation(horizontalflipping),canharmDSiResperformance,reducingitsaccuracybymore
than26%.
Ablation MAEâ†“ MAPE(%)â†“ Acc(%)â†‘
Defense 4.62Â±0.83 17.62Â±4.82 59.71Â±5.88
DSiRe 1.48Â±0.21 3.97Â±0.73 86.10Â±1.99
DataDrivenSolution. Ourmethodisdatadrivenasitrequirestrainingmultiplemodelsfromeach
datasetsize. However,ouranalysisshowsthereiscorrelationbetweentheFrobeniusnormanddataset
size(seeFig. 2a). Thisinsightcouldbeasteppingstoneindevelopingadata-freesolution.
Pre-training dataset size recovery. Another interesting application of dataset size recovery is
forpre-trainingcases. Lowerboundingtherequirednumberoftrainingsetsamplesforfoundation
modelswillhaveasubstantialimpactontheresearchcommunity. Answeringthisquestionwould
requirescalingupourmethodtomuchlargerdatasetsizesandweightmatrixdimensions.
DefenseagainstDSiRe. Dataaugmentationaimstoartificiallyincreasethesizeofthedatasetby
generatingmoreimagetransformations,suchasflippingandrotationonexistingtrainingimages.
Motivatedbythisobservation,weemploydataaugmentationduringthefine-tuningprocesstoinflate
theactualnumberofuniqueimagesusedforfine-tuning,addingmorevariationtoeveryimageinthe
dataset. Toavoidhurtingmodelconvergenceduringfine-tuningorthequalityofthegeneratedimages,
weappliedonlytheflipaugmentationtechniquetoeachimageinthedataset. Thisprocessreduced
thecorrelationbetweenthesingularvaluesandthetruenumberofimages,asillustratedinFig. 6.
9 Socialimpact
Webelieveournewlyproposedtaskcanpositivelyimpactboththeresearchanddigitalartscommunities.
Whendetectingsmallandmidrangedatasetsizes,ourmotivationistoestablishanupperboundfor
membershipinferenceattacks,promotingprivacyawaredeploymentofLoRAmodels. Forlarger
datasetsizes,itwillincreasetheinformationontheresourcesrequirestosuccessfullytrainmodels.
Thisisoftenforresearchersthatneedtocollectexpensivedatasetsfornewfine-tuningtaskse.g.,[50]
and[8].
10 Conclusion
Weintroducedthenewtaskofdatasetsizerecoveryandproposedamethod,DSiRe,forlearninga
predictorforthistaskformodelsthatuseLoRAfine-tuning. Ourmethodshowedpromisingresults
onanewlarge-scaledatasetthatwereleased. Webelieveourworkcanprovideanupperboundfor
modelinversionandmembershipinferenceattacks. Itcanalsoprovidebothresearchersandstock
photographyownerswithaquantitativeanalyticestimatingthedatacostforfine-tuningmodels.
9References
[1] Dreambooth training readme. https://github.com/huggingface/diffusers/blob/
main/examples/dreambooth/README.md. Accessed: 01/02/24.
[2] OmriAvrahami,KfirAberman,OhadFried,DanielCohen-Or,andDaniLischinski. Break-a-
scene: Extractingmultipleconceptsfromasingleimage. InSIGGRAPHAsia2023Conference
Papers.AssociationforComputingMachinery,2023.
[3] OmriAvrahami,ThomasHayes,OranGafni,SonalGupta,YanivTaigman,DeviParikh,Dani
Lischinski, Ohad Fried, andXi Yin. Spatext: Spatio-textualrepresentation for controllable
imagegeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),2023.
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877â€“1901,2020.
[5] NicholasCarlini,SteveChien,MiladNasr,ShuangSong,AndreasTerzis,andFlorianTramer.
Membershipinferenceattacksfromfirstprinciples. In2022IEEESymposiumonSecurityand
Privacy(SP),pages1897â€“1914.IEEE,2022.
[6] DingfanChen,NingYu,YangZhang,andMarioFritz. Gan-leaks: Ataxonomyofmembership
inferenceattacksagainstgenerativemodels.InProceedingsofthe2020ACMSIGSACconference
oncomputerandcommunicationssecurity,pages343â€“362,2020.
[7] KanghyunChoi,DeokkiHong,NoseongPark,YoungsokKim,andJinhoLee.Qimera: Data-free
quantization with synthetic boundary supporting samples. Advances in Neural Information
ProcessingSystems,34:14835â€“14847,2021.
[8] XiaoliangDai,JiHou,Chih-YaoMa,SamTsai,JialiangWang,RuiWang,PeizhaoZhang,Simon
Vandenhende,XiaofangWang,AbhimanyuDubey,etal. Emu: Enhancingimagegeneration
modelsusingphotogenicneedlesinahaystack. arXivpreprintarXiv:2309.15807,2023.
[9] JiaDeng, WeiDong, RichardSocher, Li-JiaLi, KaiLi, andLiFei-Fei. Imagenet: Alarge-
scalehierarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpattern
recognition,pages248â€“255.Ieee,2009.
[10] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient
finetuningofquantizedllms. arXivpreprintarXiv:2305.14314,2023.
[11] JinhaoDuan, FeiKong, ShiqiWang, XiaoshuangShi, andKaidiXu. Arediffusionmodels
vulnerabletomembershipinferenceattacks? InInternationalConferenceonMachineLearning,
pages8717â€“8730.PMLR,2023.
[12] GongfanFang,KanyaMo,XinchaoWang,JieSong,ShitaoBei,HaofeiZhang,andMingliSong.
Upto100xfasterdata-freeknowledgedistillation. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,volume36,pages6597â€“6604,2022.
[13] MattFredrikson,SomeshJha,andThomasRistenpart. Modelinversionattacksthatexploit
confidenceinformationandbasiccountermeasures. InProceedingsofthe22ndACMSIGSAC
conferenceoncomputerandcommunicationssecurity,pages1322â€“1333,2015.
[14] NivHaim,GalVardi,GiladYehudai,OhadShamir,andMichalIrani. Reconstructingtraining
datafromtrainedneuralnetworks. AdvancesinNeuralInformationProcessingSystems,35:
22911â€“22924,2022.
[15] JamieHayes,LucaMelis,GeorgeDanezis,andEmilianoDeCristofaro. Logan: Membership
inferenceattacksagainstgenerativemodels. arXivpreprintarXiv:1705.07663,2017.
[16] JunxianHe,ChuntingZhou,XuezheMa,TaylorBerg-Kirkpatrick,andGrahamNeubig.Towards
a unified view of parameter-efficient transfer learning. ArXiv, abs/2110.04366, 2021. URL
https://api.semanticscholar.org/CorpusID:238583580.
10[17] ZechengHe,TianweiZhang,andRubyBLee. Modelinversionattacksagainstcollaborative
inference. InProceedingsofthe35thAnnualComputerSecurityApplicationsConference,pages
148â€“162,2019.
[18] BenjaminHilprecht, MartinHÃ¤rterich, andDanielBernau. Montecarloandreconstruction
membershipinferenceattacksagainstgenerativemodels. ProceedingsonPrivacyEnhancing
Technologies,2019.
[19] EliahuHorwitz,JonathanKahana,andYedidHoshen. Recoveringthepre-fine-tuningweights
ofgenerativemodels. arXivpreprintarXiv:2402.10208,2024.
[20] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,
AndreaGesmundo,MonaAttariyan,andSylvainGelly. Parameter-efficienttransferlearningfor
nlp. InInternationalConferenceonMachineLearning,pages2790â€“2799.PMLR,2019.
[21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprintarXiv:2106.09685,2021.
[22] Hailong Hu and Jun Pang. Membership inference of diffusion models. arXiv preprint
arXiv:2301.09956,2023.
[23] Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S Yu, and Xuyun Zhang.
Membershipinferenceattacksonmachinelearning: Asurvey.ACMComputingSurveys(CSUR),
54(11s):1â€“37,2022.
[24] NamHyeon-Woo,MoonYe-Bin,andTae-HyunOh. Fedpara: Low-rankhadamardproductfor
communication-efficientfederatedlearning. arXivpreprintarXiv:2108.06098,2021.
[25] MatthewJagielski,MiladNasr,KatherineLee,ChristopherAChoquette-Choo,NicholasCarlini,
andFlorianTramer. Studentsparrottheirteachers: Membershipinferenceonmodeldistillation.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[26] MenglinJia,LumingTang,Bor-ChunChen,ClaireCardie,SergeBelongie,BharathHariharan,
andSer-NamLim. Visualprompttuning. InEuropeanConferenceonComputerVision,pages
709â€“727.Springer,2022.
[27] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-
conceptcustomizationoftext-to-imagediffusion. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages1931â€“1941,2023.
[28] BrianLester,RamiAl-Rfou,andNoahConstant. Thepowerofscaleforparameter-efficient
prompttuning. arXivpreprintarXiv:2104.08691,2021.
[29] XiangLisaLiandPercyLiang. Prefix-tuning: Optimizingcontinuouspromptsforgeneration.
arXivpreprintarXiv:2101.00190,2021.
[30] ZhikaiLi,MengjuanChen,JunruiXiao,andQingyiGu. Psaq-vitv2: Towardaccurateand
generaldata-freequantizationforvisiontransformers. IEEETransactionsonNeuralNetworks
andLearningSystems,2023.
[31] HaokunLiu,DerekTam,MohammedMuqeeth,JayMohta,TenghaoHuang,MohitBansal,and
ColinARaffel. Few-shotparameter-efficientfine-tuningisbetterandcheaperthanin-context
learning. AdvancesinNeuralInformationProcessingSystems,35:1950â€“1965,2022.
[32] XiaoLiu,YananZheng,ZhengxiaoDu,MingDing,YujieQian,ZhilinYang,andJieTang. Gpt
understands,too. AIOpen,2023.
[33] RaphaelGontÄ³oLopes,StefanoFenu,andThadStarner. Data-freeknowledgedistillationfor
deepneuralnetworks. arXivpreprintarXiv:1710.07535,2017.
[34] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and
Benjamin Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https:
//github.com/huggingface/peft,2022.
11[35] TomoyaMatsumoto,TakayukiMiura,andNaotoYanai. Membershipinferenceattacksagainst
diffusionmodels. In2023IEEESecurityandPrivacyWorkshops(SPW),pages77â€“83.IEEE,
2023.
[36] Ngoc-BaoNguyen,KeshigeyanChandrasegaran,MiladAbdollahzadeh,andNgai-ManCheung.
Re-thinking model inversion attacks against deep neural networks. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages16384â€“16393,2023.
[37] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748â€“8763.PMLR,2021.
[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.
High-resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10684â€“10695,2022.
[39] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman.
Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
22500â€“22510,2023.
[40] AlexandreSablayrolles, MatthÄ³sDouze, CordeliaSchmid, YannOllivier, andHervÃ©JÃ©gou.
White-boxvsblack-box: Bayesoptimalstrategiesformembershipinference. InInternational
ConferenceonMachineLearning,pages5558â€“5567.PMLR,2019.
[41] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael
Backes. Ml-leaks: Modelanddataindependentmembershipinferenceattacksanddefenseson
machinelearningmodels. arXivpreprintarXiv:1806.01246,2018.
[42] AvitalShafran,ShmuelPeleg,andYedidHoshen. Membershipinferenceattacksareeasieron
difficultproblems. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages14820â€“14829,2021.
[43] RenrongShao,WeiZhang,JianhuaYin,andJunWang. Data-freeknowledgedistillationfor
fine-grainedvisualcategorization. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pages1515â€“1525,2023.
[44] RezaShokri,MarcoStronati,CongzhengSong,andVitalyShmatikov. Membershipinference
attacksagainstmachinelearningmodels. In2017IEEEsymposiumonsecurityandprivacy
(SP),pages3â€“18.IEEE,2017.
[45] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothÃ©e
Lacroix,BaptisteRoziÃ¨re,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[46] PatrickvonPlaten,SurajPatil,AntonLozhkov,PedroCuenca,NathanLambert,KashifRasul,
MishigDavaadorj,DhruvNair,SayakPaul,WilliamBerman,YiyiXu,StevenLiu,andThomas
Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/
diffusers,2022.
[47] Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon
Kim. Multitaskprompttuningenablesparameter-efficienttransferlearning. arXivpreprint
arXiv:2303.02861,2023.
[48] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu.
Prolificdreamer: High-fidelityanddiversetext-to-3dgenerationwithvariationalscoredistilla-
tion. ArXiv,abs/2305.16213,2023. URLhttps://api.semanticscholar.org/CorpusID:
258887357.
[49] LaurenWatson,ChuanGuo,GrahamCormode,andAlexSablayrolles. Ontheimportanceof
difficultycalibrationinmembershipinferenceattacks. arXivpreprintarXiv:2111.08440,2021.
12[50] DanielWinter,MatanCohen,ShlomiFruchter,YaelPritch,AlexRav-Acha,andYedidHoshen.
Objectdrop: Bootstrappingcounterfactualsforphotorealisticobjectremovalandinsertion,2024.
[51] ShoukaiXu,HaokunLi,BohanZhuang,JingLiu,JiezhangCao,ChuangrunLiang,andMingkui
Tan. Generativelow-bitwidthdatafreequantization. InComputerVisionâ€“ECCV2020: 16th
EuropeanConference, Glasgow, UK,August23â€“28, 2020, Proceedings, PartXII16, pages
1â€“17.Springer,2020.
[52] Ziqi Yang, Jiyi Zhang, Ee-Chien Chang, and Zhenkai Liang. Neural network inversion in
adversarial setting via background knowledge alignment. In Proceedings of the 2019 ACM
SIGSACConferenceonComputerandCommunicationsSecurity,pages225â€“240,2019.
[53] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,Anwen
Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language
modelswithmultimodality. arXivpreprintarXiv:2304.14178,2023.
[54] SamuelYeom,IreneGiacomelli,MattFredrikson,andSomeshJha. Privacyriskinmachine
learning: Analyzing the connection to overfitting. In 2018 IEEE 31st computer security
foundationssymposium(CSF),pages268â€“282.IEEE,2018.
[55] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem,
NirajKJha,andJanKautz. Dreamingtodistill: Data-freeknowledgetransferviadeepinversion.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages8715â€“8724,2020.
[56] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander
Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
18123â€“18133,2022.
[57] LinZhang,LiShen,LiangDing,DachengTao,andLing-YuDuan.Fine-tuningglobalmodelvia
data-freeknowledgedistillationfornon-iidfederatedlearning. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10174â€“10183,2022.
[58] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-image
diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer
Vision(ICCV),pages3836â€“3847,October2023.
[59] QingruZhang,MinshuoChen,AlexanderBukharin,PengchengHe,YuCheng,WeizhuChen,
andTuoZhao. Adaptivebudgetallocationforparameter-efficientfine-tuning. arXivpreprint
arXiv:2303.10512,2023.
[60] ZhuangdiZhu,JunyuanHong,andJiayuZhou.Data-freeknowledgedistillationforheterogeneous
federated learning. In International conference on machine learning, pages 12878â€“12889.
PMLR,2021.
1311 Appendix
11.1 AdditionalAblationStudies
Weprovidemoreablationstudiesofourmethod. Specifically,wetestthetrainingsteps,batchsize,
usedclassifiertypeandusedLoRAmatrices.
11.2 RobustnesstoLoRAHyper-Parameters
BatchSize. Weablatethebatchsize,resultsatshowninTab. 7. Despitethechangeinbatchsize,
DSiRedemonstratesrobustperformance,achievingaMAEscoreof1.94comparedtotheoriginal
1.48. Additionally,theaccuracyonlydecreasesbylessthan5%,indicatingthatourmethodmaintains
comparableeffectivenessevenwithdifferentbatchsizes.
Table7: DSiReperformanceusingdifferentLoRAhyper-parameters. Mediumdatarange
Ablation MAEâ†“ MAPE(%)â†“ Acc(%)â†‘
Batchsize 1.94Â±0.26 9.35Â±1.34 81.50Â±2.55
Baseline 1.48Â±0.21 3.97Â±0.73 86.10Â±1.99
TrainingSteps. TotrainDSiRe,wefirstfine-tuneasetofLoRAmodels. Thesemodelsfollow
acertainrecipe,withaspecificamountoftrainingsteps. Toevaluaterobustness,wetestedDSiRe
onmodelsfine-tunedatdifferentsteps,with1200stepsasourbaseline. AsshowninTab8,DSiRe
consistintlyachievescomparableresultsacrossdifferentfintuningsteps. e.g. theMAEscoreranges
from2.43at300stepsto1.40at1400steps,withaccuracyvariationswithin10%.
Table8: DSiReperformanceondifferentcheckpointsofStableDiffusion1.5rank16range1âˆ’50
#Steps MAEâ†“ MAPE(%)â†“ Acc(%)â†‘
300 2.43Â±0.20 6.82Â±0.78 77.90Â±1.49
400 2.39Â±0.20 6.72Â±0.76 78.38Â±1.49
500 2.05Â±0.15 5.55Â±0.59 81.33Â±1.60
600 1.86Â±0.10 4.59Â±0.34 82.76Â±0.86
700 1.89Â±0.21 5.13Â±0.77 82.00Â±2.01
800 1.71Â±0.29 4.59Â±0.89 83.67Â±2.68
900 1.60Â±0.22 4.21Â±0.69 85.14Â±2.04
1000 1.62Â±0.21 4.69Â±0.70 85.10Â±1.77
1100 1.58Â±0.19 4.50Â±0.90 84.48Â±1.32
1200 1.48Â±0.21 3.97Â±0.73 86.10Â±1.99
1300 1.46Â±0.15 3.84Â±0.51 86.29Â±1.55
1400 1.40Â±0.20 3.73Â±0.76 86.76Â±2.08
11.3 ChoiceofLoRAMatrices
SeeinginSec. 3thatnotalllayersaresimilarinbehavior,wetesttoseeifdifferentLoRAmatrices
alsocapturedifferentinformation. InTab. 9,wefindthatindeeddifferentLoRAmatricescapture
differentinformation,andleadtosubstantiallyotherperformances. Unsurprisingly,wealsofindthat
usingalltheLoRAmatricescombinedyieldsthebestresult.
Table9: DSiReperformanceondifferentlayersofLoRAoftheUNetinStableDiffusion1.5,range
1âˆ’50:
#LayerType MAE â†“ MAPE(%)â†“ Acc(%)
A 1.9Â±0.29 5.63Â±1.26 82.52Â±2.20
B 1.57Â±0.19 4.07Â±0.65 84.90Â±2.09
BA 1.61Â±0.16 4.22Â±0.47 85.00Â±1.45
fullmodel 1.48Â±0.21 3.97Â±0.73 86.10Â±1.99
1412 HigherDataRegimesAnalysis
To better understand the results on higher data regimes we provide here the confusion matrix of
DSiReusing1âˆ’1000trainingsamples. Wecanseethatmostoftheerrorsareinlargerdataclasses.
Figure6: DSiReConfusionmatrixinHighdataregime. IllustratingDSiReâ€™saccuracyintherange
datasize(1âˆ’1000)forasingleexperiment,showingthatmostpredictionsarecorrectornearmisses,
highlightingtheDSiReâ€™sprecisionindatasetsizerecovery.
12.1 Implementationsdetails
12.1.1 LoRA-WiSE.
wenowelaboratetheimplementationsdetailsoftheLoRA-WiSEbenchdataset.
Datasets in all ranges 1 âˆ’ 6, 1 âˆ’ 50, 1 âˆ’ 1000. As the Pre-Ft models we use
runwayml/stable-diffusion-v1-5 and stabilityai/stable-diffusion-2 [38]. We fine-
tunethemodelsusingthePEFTlibrary[34]. Weusethescripttrain_dreambooth_lora.py[39]
withthediffuserslibrary[46]. weusethestandardrecipetofine-tunethemodelsinallranges[1]see
tab11. weusebatchsize8forrange1-1000forcomputationalresourcesand1000trainingstep. in
theablationswedonâ€™tchangeanyhyper-parameterexcepttheablateone.
Eachmodeltookapproximately30-50minutestofine-tune. WeusedGPUswith16-21GBofRAM,
suchastheNVIDIARTXA5000. TheDSiReprocess,however,doesnotrequireGPUsandcanrun
onCPUs.
ExperimentSettings. InadditiontotheexperimentsettingsdescribedinSection6,weusedthe
followingconfigurationsforourmodels:
-Formodelsintheranges1-6and1-50,weusedthecheckpointatiteration1200. -Formodelsinthe
range1-1000,weusedthecheckpointatiteration1000.
Weusedafixedseedof42tosplitthetrainandtestdataforeveryexperiment.
Layerweighmatrices InlinewithouranalysisseeSec.3,givennexampleweightsforeachA ,B
i i
wewishtobuildaseparateclassifierforeachone. Knowingtherelationbetweenthesingularvalues
andthedatasetsize,wedecomposeeachmatrixusingthesingularvaluedecomposition(SVD),and
usetheorderedsetofsingularvaluesasfeaturesforourclassifiers. Formally,wenotethesingular
15Table10: ranges1-6and1-50 Table 11: range 1-1000 Hyper-
parameters
Name Value
Name Value
lora_rank(r) r
lr 1eâˆ’4 lora_rank(r) 32
batch_size 1 lr 1eâˆ’4
gradient_accumulation_steps 1 batch_size 8
learning_rate_scheduler Constant gradient_accumulation_steps 1
training_steps 1400 learning_rate_scheduler Constant
warmup_ratio 0 training_steps 1000
imagenet[9] warmup_ratio 0
dataset
concept101[27] dataset imagenet
seeds 0 seeds 0
values of A as Î£ and the singular values of B as Î£ . We include the singular values of
ij Aij ij Bij
B Â·A denotedasÎ£ . Additionally,ourobservationsindicatethattheproductB Â·A also
ij ij BijÂ·Aij i i
providesusefulinformationfordatasizerecovery. Thus,foreachLoRAmatrix,weobtainadataset
withnsamples,whereeachsampleisavectorofsingularvaluesÎ£ ,pairedwithacorresponding
ij
labely . OurmethodthentrainsthreeseparatekNN-classifierswithK =1foreachlayerover(i)A
j i
(ii)B and(iii)B A . Atinferencetime,thepredictionsfromallclassifiersaremergedbymajority
i i i
voting.
16