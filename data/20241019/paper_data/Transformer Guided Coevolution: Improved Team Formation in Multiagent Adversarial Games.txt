Transformer Guided Coevolution: Improved Team Formation in
Multiagent Adversarial Games
PranavRajbhandari PrithvirajDasgupta DonaldSofge
CarnegieMellonUniversity NavalResearchLaboratory NavalResearchLaboratory
Pittsburgh,PA,UnitedStates Washington,D.C.,UnitedStates Washington,D.C.,UnitedStates
prajbhan@alumni.cmu.edu prithviraj.dasgupta.civ@us.navy.mil donald.a.sofge.civ@us.navy.mil
ABSTRACT Researchershaveaddressedtheteamselectionprobleminmulti-
Weconsidertheproblemofteamformationwithinmultiagentad- agentteamformationusingevolutionarycomputation-basedap-
versarialgames.WeproposeBERTeam,anovelalgorithmthatuses proaches[14,31],albeitfornon-adversarialsettingslikesearchand
atransformer-baseddeepneuralnetworkwithMaskedLanguage reconnaissance.Inthispaper,weconsidertheuseofatransformer
Modeltrainingtoselectthebestteamofplayersfromatrainedpop- basedneuralnetworktopredictthesetofagentswhichformateam.
ulation.Weintegratethiswithcoevolutionarydeepreinforcement WenamethistechniqueBERTeam,andinvestigateitssuitability
learning,whichtrainsadiversesetofindividualplayerstochoose forteamformationinmultiagentadversarialgames.
teamsfrom.Wetestouralgorithminthemultiagentadversarial Incontrasttoevolution-basedapproachesinliterature,ourtech-
gameMarineCapture-The-Flag,andwefindthatBERTeamlearns nique considers team selection as a token sequence generation
non-trivialteamcompositionsthatperformwellagainstunseenop- process.Wegenerateateambybeginningwithamaskedsequence
ponents.Forthisgame,wefindthatBERTeamoutperformsMCAA, ofmembers,anditerativelyqueryingthetransformertopredictthe
analgorithmthatsimilarlyoptimizesteamformation. nextmaskedagentâ€™sidentity.Wecontinueuntilthespecifiedteam
sizeisreached.WetrainBERTeamonadatasetofwell-performing
KEYWORDS teams,sothetransformerwillattempttomatchthisdistribution.
AlongsidetrainingBERTeam,weevolveapopulationofagents
Multiagentreinforcementlearning,TeamFormation,Adversarial
usingCoevolutionaryDeepReinforcementLearning[12,24].This
games,Coevolution,Transformers,SequenceGeneration
methodutilizesself-play,samplinggamesbetweenteamsselected
ACMReferenceFormat: fromthepopulation.Thetrainingdatafromthesegamesupdatesa
PranavRajbhandari,PrithvirajDasgupta,andDonaldSofge.2025.Trans- setofReinforcementLearning(RL)algorithmsandguidesastan-
formerGuidedCoevolution:ImprovedTeamFormationinMultiagentAd- dardevolutionaryalgorithm.
versarialGames.InProc.ofthe24thInternationalConferenceonAutonomous Weempiricallyvalidateourproposedtechniquewiththeğ‘˜-v-ğ‘˜
AgentsandMultiagentSystems(AAMAS2025),Detroit,Michigan,USA,May adversarial game Marine Capture-the-Flag (MCTF). Our results
19â€“23,2025,IFAAMAS,11pages. showthatBERTeamisaneffectivemethodforteamselectionin
thisdomain.WefoundthatBERTeamisabletolearnanon-trivial
1 INTRODUCTION distribution,favoringwellperformingteams.Wealsofindthatfor
MCTF,BERTeamoutperformsMultiagentCoevolutionforAsym-
Weinspectmultiagentadversarialgames,characterizedbyanenvi-
metricAgents(MCAA),anotherteamformationalgorithm.
ronmentwithmultipleteamsofagents,eachworkingtoachievea
teamgoal.Theirperformanceisevaluatedbyanoutcome,areal
numberassignedtoeachteamattheendofagame(episode). 2 RELATEDWORKS
Variouscomplexteamgamescanbeformulatedasamultiagent Self-Play:Self-playisacentralconceptfortrainingautonomous
adversarialgame,includingpursuit-evasiongames[10,18,40,49], agentsforadversarialgames.Themainideaofself-playistokeep
roboticfootball[17,23,41],androboticcapture-the-flag[32].The trackofasetofpoliciestoplayagainstduringtraining[20].These
problemofcreatingacooperativeteamofrobotsisusefulforsolving policiesareusuallycurrentorpastversionsofagentsbeingtrained.
thesegames,aswellasapplicationssuchassearchandrescue. Extendingtheconceptofself-play,Alpha-Star[21]utilizedleague
A crucial problem in adversarial multiagent team formation play,atechniquewhereadiversesetofagentswithdifferentper-
is the selection of teams. Given a set of agents and potentially formancelevelsplayinatournamentstylegamestructure.League
informationabouttheenvironment,ateammustbeselectedto playimprovedtheadaptabilityoftrainedagentstoplayagainst
performbestagainstopponents.Thisproblemisdifficultsincea differentopponentdifficultiesintheStarcraft-IIreal-timestrategy
goodteamformationalgorithmmustconsiderbothintra-teamand videogame.Similartotheleague-playconcept,inourproposed
inter-teaminteractionstoselectanoptimalteam.Additionally,the technique,weusecoevolutionofagentstoimprovetheiradaptabil-
setofagentsmustoftenlearntheirindividualpolicies,increasing ityagainstvaryingopponentstrategiesanddifficultylevels.
thecomplexityoftheproblem. TeamFormationinAdversarialGames:In[27],researchers
proposedTeam-PSRO(PolicySpaceResponseOracle),atechnique
Proc.ofthe24thInternationalConferenceonAutonomousAgentsandMultiagentSystems withinaframeworkcalledTMECor(TeamMax-minEquilibrium
(AAMAS2025),A.ElFallahSeghrouchni,Y.Vorobeychik,S.Das,A.Nowe(eds.),May19 withCorrelationdevice),formulti-playeradversarialgames.In
â€“23,2025,Detroit,Michigan,USA.Â©2025InternationalFoundationforAutonomous
Team-PSRO,agentsimprovetheirpoliciesiterativelythroughre-
AgentsandMultiagentSystems(www.ifaamas.org).Thisworkislicencedunderthe
CreativeCommonsAttribution4.0International(CC-BY4.0)licence. peatedgame-play.Ineachiteration,thepolicyforeachagentina
4202
tcO
71
]IA.sc[
1v96731.0142:viXrateamisselectedasacomponentofabest-responsepolicyforthe Transformers:Transformersareasequence-to-sequencedeep
entireteam,calculatedbyabestresponseoracle.Inourtechnique, neuralnetworkarchitecturedesignedtocreatecontext-dependent
theroleofthisoracleisperformedbyBERTeamâ€™steamselection. embeddingsofinputtokensinasequence[45].Theyuseanencoder-
Anotherapproachforlearningtoplayadversarialteamgames decoderarchitecture[1,11,42],takingasinputtwosequences.The
trainsagentsincrementallyviacurriculumlearning,thenadapts outputisanembeddedsequencecorrespondingtoeachelementof
thoselearnedstrategiesforadversarialsettingsviaself-play[25]. oneinitialsequence.Afinallayercanbeaddedconvertingeachele-
Strategiesarestochasticallymodifiedtointroducediversityandto mentintoaprobabilitydistributionovertokensinavocabularyand
increaseadaptabilityagainstunseenopponentstrategies.Whilethis allowingforsequencegeneration.Thisarchitectureiswidelyused
techniqueadaptspreviouslytrainedpoliciesforadversarialplay,our inNaturalLanguageProcessing(NLP),intaskssuchasgeneration,
approachcreatesapopulationofpoliciestrainedinanadversarial classification,andtranslation[8,9,22,26].
environment,thenselectsthebestteamfromthispopulation.The BidirectionalEncoderRepresentationsfromTransformers(BERT)
largenumberofpotentialteamstoselectfromallowsustoadapt [13]isanupdatetotheoriginaltransformerâ€˜nexttokenpredictionâ€™
ourteamselectiontoneweropponentstrategies. trainingstructure.BERTisinsteadtrainedwithMaskedLanguage
EvolutionaryAlgorithmsforMultiagentGames:Evolution- Modeling(MLM),inspiredbytheClozetask[43].Thisforcesitto
aryalgorithmshavebeenusedfordecadesformultiagentgames predictrandomlymaskedtokensgivenbidirectionalcontext,and
duetotheiradaptabilityandperformanceindomainslikesoccer thusimprovesrobustnessinthemodel.Weuseasimilarapproach,
[48].Coevolutioninparticular[46,47]hastheadvantageofevolv- associatingasetofagentstotokensandusinganMLMtraining
ingindependentpopulationsofagentsforspecializedskills(e.g. schemetoproducesequencesofagentstoformstrongteams.
defending,passing,shooting).However,adownsidetothisisthat
thecorrectspecializedagentsmustbechosenforeachenvironment. 3 TEAMSELECTIONINADVERSARIAL
Toaddressthis,authorsin[49]proposedatechniquetouseanas- GAMES
sessortochooseteamcompositionsintelligently.Theassessorwas
Preliminaries:AMarkovDecisionProcess(MDP)isaframework
amodeltrainedtopredicttheoutcomeofgamesbetweenknownop-
capturingabroadrangeofoptimizationtasks.AnMDPisdescribed
ponents.Thisallowedthemtoevolvespecializedagentswhilealso byatuple(ğ‘†,ğ´,T,R,ğ›¾),containingastatespaceğ‘†,anactionspace
beingabletochooseoptimalteamsagainstknownopponents.How- ğ´,atransitionfunctionT(ğ‘† | ğ‘† Ã—ğ´),arewardfunctionR:ğ‘† Ã—
ever,toselecttheoptimalteamagainstaknownopponentteam, ğ´Ã—ğ‘† â†’R,andadiscountfactorğ›¾ âˆˆ [0,1).Anagentisaplayer
theymustsearchthespaceofallpossibleteams,queryingtheir inanMDPandisdescribedbyitspolicyğœ‹(ğ´ |ğ‘†).Thesequence
modeleachtime.Toselectagoodteamagainstapartiallyknown (ğ‘  0,ğ‘ 0,ğ‘Ÿ 0),(ğ‘  1,ğ‘ 1,ğ‘Ÿ 1),... isreferredtoasatrajectory,whereğ‘ ğ‘– is
opponentteam,theyrepeatedlyupdatethecurrentandopponent sampledfromğœ‹(ğ‘ |ğ‘  ğ‘–âˆ’1),ğ‘  ğ‘–issampledfromthetransitionfunction
teams,continuinguntilconvergence.Thus,thiscausesdifficultyin T(ğ‘  | ğ‘  ğ‘–âˆ’1,ğ‘ ğ‘–âˆ’1),andğ‘Ÿ ğ‘¡ = R(ğ‘  ğ‘¡,ğ‘ ğ‘¡,ğ‘  ğ‘¡+1).Anagentâ€™sobjectiveis
scalabilitytolargerpopulations,andreducesadaptabilityincases optimizingthesumofdiscountedrewardsinatrajectory:E[(cid:205)ğ›¾ğ‘–ğ‘Ÿ ğ‘–].
whereopponentpoliciesarenotknownapriori. ğ‘–
Intheirresearch,Dixitetal.achieveasimilargoalofdiversifying ReinforcementLearning(RL)isonemethodofoptimizingapol-
individualpoliciesandselectinganoptimalteamcompositionfrom icyforanMDP.Whiletherearevariousdifferenttechniquesfor
thesediverseagents[14].Todiversifyagents,theyusedQualityDi- different classes of MDPs, most of them keep track of a policy
versity(QD)methodslikeMAP-Elites,anevolutionaryalgorithm
ğœ‹ ğœƒ(ğ´ |ğ‘†)andarewardestimatorğ‘Ÿ
ğœ™
:ğ‘† â†’R.Aftersamplingan
that ensures the population evolved has sufficiently diverse be-
episode,therewardparametersğœ™areupdatedtowardstheobserved
havior [31]. MAP-Elites works by projecting each policy into a discountedrewardsfromeachstate.Thepolicyparametersğœƒ are
lowdimensionalbehaviorspace,thenconsideringonlyfitnessesof updatedusingtherewardestimatortooptimizetheexpecteddis-
agentsthatbehavesimilarlywhenupdatingthepopulation. countedrewardsfromeachstate.DeepRLutilizesDeepNeural
DixitusedQDonafewindependentislandsofRLagents.To Networkstocreatepolicyandrewardnetworksğœ‹ ğœƒ,ğ‘Ÿ ğœƒ.
selectoptimalteamcompositions,theirmainlandalgorithmkeeps We are concerned with multiagent ğ‘˜-v-ğ‘˜ adversarial games.
trackofadistributionoftheproportionofmembersfromeach Giventhepoliciesofallagentsplaying,thisscenariocanbeformally
islandthatcomposeanoptimalteam.Totrainthisdistribution,they definedasaninstanceofanMDPforeachagent,withadistinct
repeatedlyevaluateagentsinteamgames,thenusetheoutcomesto rewardfunctionforeachagent.Ateachtimestep,theactionsof
ranktheteams.Theyusethecompositionsofthebestfewteamsto allotheragentsareconsideredinthetransitionfunctionT.We
updatethedistribution,andtheindividualfitnessesandRLtraining dividetheagentsintoteamsandadditionallydefineanoutcome
examplestoupdatetheindividualpoliciesoneachisland. evaluationthatconsidersthetrajectoriesofagameandreturnsa
MCAAisdesignedandevaluatedforcooperativetaskslikevis-
setofteamsthatâ€˜wonâ€™.1WeassumetheMDPrewardsofanagent
iting a set of targets with robots that have different navigation correlatewithitsteamâ€™soutcome,soagentsthatgethighrewards
capabilities(e.g.dronesandrovers).SimilartoMCAA,ourpro- inagamearelikelytobeonwinningteams.Weusethisframework
posedalgorithmusestwoseparatecomponentsforevolvingagent asopposeddec-MDPs,aformalizationusedinteamreach-avoid
skillsandevaluatingagentperformance.However,wedivergefrom games[5,10,18,39]sincewewouldlikeeachagenttohavetheir
MCAAasweconsideradversarialteamsandcaseswheretheagents ownrewardstructure,andfortheserewardstobeseparatefromthe
areuniformandonlydifferentiatedbythebehavioroftheirlearned gameoutcomes.Withinourframework,theproblemweconsider
policies.Wecomparethemaintechniquesofourproposedalgo-
1Anaturalextensionofthisistohavetheoutcomebearealnumberforeachteam.
rithmwithanalogouscomponentsofMCAA. WediscussawaytopotentiallyhandlethisinAppendixD.1ishowtobestcreateateamofagentswhosepoliciesarelikelyto
winagainstavarietyofopponents.
Thisproblemisdifficulttosolveduetothecooperationrequired
ofpolicieswithinateamandthevarietyofpotentialopponent
policies.Therearetwogeneraldirectionstoaddressthisproblem.
Thefirstmethodmaintainsafixednumberofagentsperteamcorre-
spondingtotheteamsize;eachagentâ€™spolicyistrainediteratively
viatechniqueslikeself-play[25,27].However,maintainingafixed
setofagentpolicieslimitstheadaptabilityofeachagentaswellas
thediversityoftheteam.Itmightbedifficulttoquicklyformateam
thatcanplaysuccessfullyagainstanopponentstrategythatmay
havebeenâ€˜forgottenâ€™duringtraining.Thesecondapproach[21], Figure1:BERTeamâ€™score,atransformernetwork
whichweadoptinthisresearch,istomaintainalargersetofagent
policiesandselectafewagentpoliciestoformateam.Thiscreates
vastdiversityinpossibleteamswiththedownsideofintroducing ofvectorsonwhichtoconditionBERTeamâ€™soutput.4 Sincethe
theadditionaloverheadofteamselection.Thisisanon-trivialprob- architectureofthisinputembeddingdependsontheformofthe
lemastheteamselectionmustselectpoliciesthatcooperatewell, observations,thismodelmustbetailoredtoeachusecase.Asingle
whileremainingcognizantoftheopponentâ€™spossiblestrategies.At queryofthemodelwilltakeasinputapartiallymaskedteamand
thesametime,policiesmustbecontinuouslyimprovedviaself-play anyenvironmentobservations.Theoutputwillbeapredicteddistri-
learningtoremaincompetitiveagainstneweropponentstrategies. butionoveragentsforeachelementofthesequence,asillustrated
Ourproposedtechniqueistousecoevolutiontotrainalarge inFigure1.
setorpopulationofagentpolicies,anduseatransformer-based Duringevaluation,themodelwillbeusedasagenerativepre-
sequencegenerationtechniquetoselectthebestteamsfromthe trainedmodel(asin[45])tosampleateam.Forthegeneralcase,
population.ThesetwotechniquesareillustratedinFigure2and itcouldbeusefultoeitherincludetheteamindexasinputforthe
describedinmoredetailinthefollowingsections. inputembedding,ormakeseparateinstancesofBERTeamforeach
team.However,thereisoftensymmetrybetweenteams,sothe
3.1 TransformerbasedSequenceCompletion sameinstanceofBERTeamcanbeused.
forTeamSelection 3.1.2 TrainingProcedure. TheBERTeammodelusesMLMtraining,
WhyTransformers? Transformersaregenerativemodelsthatcanbe whichrequiresadatasetofâ€˜correctsequencesâ€™.Themodelistrained
usedtoqueryâ€˜nexttokenâ€™conditionaldistributions.Asizeğ‘˜team tocompletemaskedsequencestomatchthisdistribution.
canbegeneratedinğ‘˜queries,givingusefficientuseofthemodel. Togeneratethisdataset,weconsidertheoutcomesofgames
Theformoftheoutputalsoallowsusmakespecializedqueriesfor playedbetweenvariousteams.Sinceourgoalisforthemodelto
caseswherevalidteamsmayhavespecificconstraints.2Transform- predictgoodteams,wefillthedatasetwithonlyteamsthatwin
ersarealsoabletoconditiontheiroutputoninputembeddings, games,alongwiththeobservationsoftheirplayers.Wealsoinclude
allowingustoformteamsdependentonobservations(ofanyform) examplesofwinningteamswithoutanyobservationstotrainthe
oftheopponent.Wemayalsopassinanemptyinputsequencefor modeltogenerateunconditionedoutput.
unconditionalteamformation.Weexpectthatthroughtheirinitial Sinceitisusuallyinfeasibletosampleallpossibleteampairs
embeddings,transformerswillencodesimilaritiesbetweenagents uniformly,wemustdecidewhichgamestosampletoefficiently
andallowthemodeltoinfermissingdata.Thisissupportedbythe createadataset.WedothisbyusingBERTeamâ€™ssamplingtocreate
behaviorofwordembeddingsinthedomainofNLP[28]. adatasetofteamsthatwinagainstBERTeamâ€™scurrentdistribution
ThemainissuewithtransformersistheirÎ˜(ğ‘˜2)complexityfor ofopponents.Themotivationofthisisthatisisnaturaltofavorgen-
asequenceofsizeğ‘˜.However,inpractice,sequencelengthsofup eratingteamsthatwinagainstpowerfulopponents,asopposedto
to512areeasilycalculated[13].Thisindicatesthatouralgorithm teamsthatwinagainstpoorlycoordinatedopponents.Additionally,
canscaleuptothisteamsize.3 Evenforteamsizesbeyondthis undercertainassumptionsanddatasetweights,BERTeamimitating
limit,thereexistworkaroundslikesliding-windowattention[3]. adatasetformedinthiswaywillconvergetoaNashEquilibrium.
WeproposeBERTeam,amethodforselectingateamofagents WedescribethisinAppendixD.1,butdonotimplementthisinour
inamulti-agentadversarialgameutilizingatransformermodel. experiment,asitmaycausepoorlybehavedlearning.Thisappendix
alsodescribesastrategyforgameswithgeneraloutcomes,where
3.1.1 ModelArchitecture. ThecoreofBERTeamisatransformer cannotnecessarilydistinguishaâ€˜winningâ€™team.
modelwhosetokensrepresenteachpossibleagentinthepopu- Thus,asinFigure2,wewilltrainBERTeamalongsidegenerating
lation,alongwitha[MASK]token.Aseparateinputembedding itsdataset,andutilizethepartiallytrainedmodeltosamplegames.
modeltransformsobservationsofanyformintoashortsequence Ourdatasetwilltaketheformofareplaybuffersothatoutdated
examples are eventually replaced by newer ones. One concern
is that filling the dataset with teams generated from BERTeam
2Anexampleofthiswouldbeateamthatmustbepartitionedbyagenttypes.Inthis
case,wewouldusetheoutputtosampleeachmemberfromasetofvalidchoices. 4WhilethisisacapabilityofBERTeam,wechoosetoanalyzeBERTeamasanuncondi-
3Thiswouldcomewithanincreaseintrainingdatarequiredforsensibleoutput. tionalteamgenerator.Weplantoexploreinputembeddingsinfuturework.mayresultinstagnation,asthedatasetwillmatchtrendsinthe Algorithm1:Algorithmfortrainingapopulationofagents
distribution.Tomitigatethis,weuseinverseprobabilityweighting viacoevolutionself-play
[19],weightingraresamplesmore.WediscussthisinAppendixD. input :ğ‘ƒğ‘œğ‘:agentpopulation
ğ‘˜:sizeofeachteam
output:ğ‘ƒğ‘œğ‘:updatedagentpopulationviacoevolution
1
Proceduretrain-pop-coevolution(ğ‘ƒğ‘œğ‘,ğ‘˜)
2 ğ‘“ ğ‘– â†1000 âˆ€ğ‘– âˆˆğ‘ƒğ‘œğ‘
3 for1...ğ‘› ğ‘’ğ‘ğ‘œğ‘â„ğ‘  do
4
Tğ‘– â†âˆ… âˆ€ğ‘– âˆˆğ‘ƒğ‘œğ‘
5 for1,...,ğ‘› ğ‘”ğ‘ğ‘šğ‘’ğ‘  do
6
(ğ‘‡,ğ‘‡â€²)â†sampleğ‘˜agentsperteamfromğ‘ƒğ‘œğ‘
7
ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘â€² â†selectcaptainsforteamsğ‘‡,ğ‘‡â€²
8
ğ‘”â†gameplayedbetweenteamsğ‘‡,ğ‘‡â€²
9
ğ‘‚ â†outcomesforteamsğ‘‡,ğ‘‡â€²fromgameğ‘”
10
Tğ‘– â†Tğ‘–âˆªtrajectoriesfromğ‘”foragentğ‘–,
âˆ€ğ‘– âˆˆ{ğ‘‡ âˆªğ‘‡â€²}
Figure2:TrainingofBERTeamalongsidecoevolutionaryRL
11
ğ‘“ ğ‘ğ‘ğ‘,ğ‘“
ğ‘ğ‘ğ‘â€²
â†updatefitness(ğ‘ğ‘ğ‘,ğ‘ğ‘ğ‘â€²,ğ‘‚)
12 end
3.1.3 TrainingalongwithCoevolution. TheBERTeammodelisable
tobetrainedalongsidecoevolution,asitspastknowledgeofthe 13 Updateğœ‹ ğ‘– usingRLalgorithmonexperienceinTğ‘–
agentpoliciescanbeutilizedandupdatedwithrecentinformation. 14 Pğ‘ğ‘™ğ‘œğ‘›ğ‘’ â†Cloneğ‘› ğ‘Ÿğ‘’ğ‘š agentsfromğ‘ƒğ‘œğ‘selected
usingEqn.1
The outcomes of games sampled in coevolution can be used in
thedatasetofBERTeam,andtheBERTeampartiallytrainedmodel 15 Pğ‘Ÿğ‘’ğ‘š â†Selectğ‘› ğ‘Ÿğ‘’ğ‘š agentsfromğ‘ƒğ‘œğ‘usingEqn.2
canbeusedtosamplebetterteams,acycledisplayedinFigure2. 16
ğ‘ƒğ‘œğ‘ â†{ğ‘ƒğ‘œğ‘\Pğ‘Ÿğ‘’ğ‘š}âˆªPğ‘ğ‘™ğ‘œğ‘›ğ‘’
WhilewedetailthecoevolutionalgorithminSection3.2,themain 17 end
thingwemustdefineusingBERTeamisanindividualagentfitness 18 returnğ‘ƒğ‘œğ‘
function.Thisistricky,aswehaveonlyassumedtheexistenceofa
comparativeteamoutcome.WesolvethisbyutilizingElo,amethod
ofassigningeachplayeravaluefromtheresultsofpairwise1v1
ThepartsthatdeviatefromthecoevolutionaryRLalgorithm
games[15].Givenateamselector(i.e.BERTeam)thatcansample
in[12]arethefitnessupdates(lines7and11)andthegeneration
fromthesetofallpossibleteamscontainingsomespecifiedagent
update(lines14-16).Thedeviationinfitnessupdatesisaresultof
(thecaptain),wedefinethefitnessofeachagentastheexpectedElo
onlyassumingtheexistenceofateamevaluationfunction,andis
ofateamchosenwiththatagentascaptain.Toupdatethesevalues,
discussedinSection3.1.3.Thedeviationinpopulationupdatesis
wesampleteamstoplaytraininggames,choosingourcaptains
duetoconsiderationswithtrainingalongsideBERTeam.BERTeam
byaddingnoisetoBERTeamâ€™sinitialdistribution.Weupdatethe
assumessimilaritiesinbehavioroftheagentassignedtoeachtoken,
fitnessesofeachteamcaptainwithastandardEloupdate(Appendix
andifwereplaceorrearrangeeverymemberofthepopulation,
A).Sinceitisconfusingtodistinguishtheseindividualfitnesses
thetrainingoftheBERTeammodelwouldberendereduseless.By
fromteamElos,wewillrefertothemasfitnessvaluesfromnow
controllingreplacementrate,weensuremostoftheinformation
on.
learnedbyBERTeamretainsrelevance.Todothiswhilebestim-
3.2 CoevolutionForTrainingAgentPolicies itating[12],westochasticallychooseğ‘› ğ‘Ÿğ‘’ğ‘š agentstoreplaceand
ğ‘› ğ‘Ÿğ‘’ğ‘š toclone5usingthefollowingequations:
Themainideaincoevolutionisthatinsteadofoptimizingasingle
team,apopulationofagentslearnsstrategiesplayingingames
betweensampledteams.WechoosetouseCoevolutionaryDeepRL P{cloneagentğ‘–}=
exp(ğ‘“ ğ‘–)
(1)
sinceitallowsagentstobetrainedagainstavarietyofopponent (cid:205) exp(ğ‘“ ğ‘—)
ğ‘—âˆˆğ‘ƒğ‘œğ‘
policies,addressingperformanceagainstanunseenopponent.
Thus,weuseAlgorithm1,heavilyinspiredby[12],toproducea P{removeagentğ‘–}=
exp(âˆ’ğ‘“ ğ‘–)
(2)
populationoftrainedagents.Theinputisaninitializedpopulation (cid:205) exp(âˆ’ğ‘“ ğ‘—)
ğ‘—âˆˆğ‘ƒğ‘œğ‘
ofagents,aswellasparametersliketeamsize.Ineachepoch,we
sampleğ‘› ğ‘”ğ‘ğ‘šğ‘’ğ‘  games,whicheachconsistofselectingteamsand 4 EXPERIMENTS
captainsusingateamselector(lines6,7),playingthegame(line8),
andcollectingtrajectoriesandoutcomes(lines9,10).Theoutcomes Tobetteranalyzetheeffectivenessofthisalgorithm,weconsider
aresenttotheteamselectorfortrainingandalsousedtoupdate 2v2 team games. This small team size allows us to more easily
individualagentfitnessesinline11(seeFigure1).Attheendof analyzethetotaldistributionlearnedbyBERTeam.
eachepoch,thetrajectoriescollectedupdateeachagentpolicyin
placeinline13,andthepopulationisupdatedinlines14-16. 5Anagentmightbeselectedforreplacementandcloning,resultinginnochange.4.1 Aquaticus 4.3 TeamSelectionwithCoevolutionofAgents
WetrainBERTeamalongsideAlgorithm1,asillustratedinFigure
2.6Foreachindividualagent,weuseProximalPolicyOptimization
(PPO),anon-policyRLalgorithm[38].Wedetailexperimentpa-
rametersinAppendixE.TofindtheElosofeachpossibleteam,we
utilizetheEloscalculatedforthefixedpolicyteamsasabaseline.
Foreachofthe1275possibleteams,7wetestagainstallpossible
teamsoffixedpolicyagents.Wethenusetheresultsofthesegames
tocalculatethetrueElosofourteamsoftrainedagents.Wedonot
updatetheElosofourfixedpolicyagentsduringthiscalculation.
(a)MOOS-IvPEnvironment (b)Pyquaticusenvironment
Forpolicyoptimization,weusestable_baselines3[33],astan-
dardRLlibrary.Sincethislibraryissingle-agent,wecreateunsta-
Figure3:Aquaticusgame,anditssimulatedversion ble_baselines3,8awrapperallowingasetofindependentlearning
algorithmstotraininaPettingZoomultiagentenvironment.
AquaticusisaCapture-the-Flagcompetitionplayedwithteams 4.3.1 Aggression Metric. The BERTeam model is difficult to in-
ofautonomousboats[32].Weareinterestedinthistaskbecauseit terpretinthecaseoflearnedpolicies,astheteamcompositionis
isanexampleofateam-basedmultiagentadversarialgame.Each unclear.Thus,wecreateanaggressionmetrictoestimatethebehav-
agenthaslow-levelmotorcontrol,andthusanycomplexbehaviors
iorofeachagentinagame.Specifically,thismetricforagentğ‘is
mustbelearnedthroughamethodlikeRL.Additionally,thereare
1+2(#ğ‘capturesflag)+1.5(#ğ‘grabsflag)+(#ğ‘istagged)
,where (#ğ‘event)
1+(#ğ‘tagsopponent)
variousstrategies(e.g.offensive/defensive)thatagentsmayadopt denotesthenumberoftimesthateventhappenedtoagentğ‘inthe
thatperformwellincompetition.Finally,webelievethatoptimal game.Weevaluatetheaggressivenessofeachtrainedagentbycon-
teamcompositioninthisgameisnon-trivial,andexpectthatagood sideringagamewhereoneteamiscomposedofonlythatagent.We
teamiscomposedofabalancedsetofstrategies. evaluatetheaverageaggressionmetricagainstallpossibleteamsof
fixedpolicyagents.Weexpectaggressiveagentstohaveametric
4.1.1 Pyquaticus. Duetothedifficultyoftestingonrealrobotic largerthan1,anddefensiveagentstohaveametriclessthan1.9
platforms,wetestandevaluateourmethodsonPyquaticus,asim-
ulatedversionofAquaticus[2].Theplatformisimplementedasa
4.4 ComparisonwithMCAA
PettingZooenvironment[44],thestandardmultiagentextensionof
OpenAIGymnasium[6].Inexperiments,weusetheMDPstructure WenoticethattheMCAAmainlandteamselectionalgorithmis
implementedinPyquaticus.Weterminateanepisodewhenateam playingananalogousroletoBERTeamteamformation,andthatthe
capturesaflag,oraftersevenin-gameminutes. MAP-Elitespolicyoptimizationoneachislandisanalogoustoour
coevolutionaryRLalgorithm.SincebothouralgorithmandMCAA
distinguishteamformationandindividualpolicyoptimizationas
4.2 TeamSelectionwithFixedPolicyAgents
separatealgorithms,wemayhybridizethemethodsandcompare
TotesttheeffectivenessofBERTeamindependentofcoevolution,
theresultsoffouralgorithms.Thealgorithmsaredistinguishedby
weusefixedpolicyagentspredefinedinPyquaticus:arandomagent,
choosingMAP-ElitesorCoevolutionaryDeepRLforpolicyopti-
threedefendingagents,andthreeattackingagents.Theattacking
mization,andBERTeamorMCAAforteamselection.Thehybrid
anddefendingagentseachcontainaneasy,medium,andhardpolicy.
trialswillallowustoindividuallyevaluateBERTeamasateam
Theeasypoliciesmoveinafixedpath,andthemediumpolicies
selectionmethod,independentofthepolicyoptimization.
utilizepotentialfieldcontrollerstoeitherattacktheopponentflag
WemustmakesomechangestobeabletoimplementMCAAand
whileavoidingopponents,orcaptureopponentsbyfollowingthem.
MAP-Elitesinanadversarialscenario.First,astepoftheMCAAal-
Thehardpoliciesarethemediumpolicieswithfastermovement
gorithmranksasetofgeneratedteamswithateamfitnessfunction.
speed.WefollowthetrainingalgorithminFigure2withoutthe
Toapproximatethisinanadversarialenvironment,weplayasetof
coevolutionupdate.Weconductanexperimentwiththe7agents,
gameseachepoch,andconsidertheteamsthatwonasâ€˜toprankedâ€™,
anddetailexperimentparametersinAppendixE.
andincludethemintheMCAAtrainingdata.Similarly,MCAA
Throughouttraining,werecordtheoccurrenceprobabilityof
assumesanevaluationfunctionforfitnessofindividualagents.We
all possible teams using BERTeam. We do this exhaustively, by
approximatethisbyconsideringtheteamseachindividualhasbeen
consideringallpossiblesequencesdrawnfromthesetofagentswith
includedin.Wetakeamovingaverageofthecomparativeteam
replacement.WeexpectthatasBERTeamtrains,thiswillgradually
evaluations,andassignthisaveragetotheselectedagent.
approachadistributionthatfavorswellperformingteams.
ForMAP-Elites,weadaptouraggressionmetricasabehavior
projection.Whilethiscouldbemademorecomplex,webelievethis
4.2.1 EloCalculation. Sincewehaveafixedsetofpolicies,wecan
issufficient,asourworkisfocusedontheteamselectionaspect.
computetrueElosofall28unordered2agentteams(seeAppendix
B),obtainedfromanexhaustivetournamentofallpossibleteam
pairings.Fortheseresults,weperform10experimentsforeach
6Ourimplementation,alongwithexperiments,isavailableat[34,35]
7Weuseapopulationof50agentsandateamsizeof2,seeAppendixB.
choiceoftwoteams.WeusethescalingofstandardchessElo[15], 8Codeavailableat[36]
andshiftallElossothatthemeanis1000. 9Wetunedourmetrictodistinguishourfixedpolicyagents.Additionally,weadapttheMAP-Elitesalgorithmtoremoveğ‘›
ğ‘Ÿğ‘’ğ‘š
poorlyperformingagentsfromthepopulation:
â€¢ Projecteachpolicyintoalowdimensionalbehaviorspace.
Letthebehaviorvectorofagentğ‘beğµ ğ‘.
â€¢ Protectâ€˜uniqueâ€™agentsfromdeletion.Givenaneighborhood
sizeğœ†,weconsideranğ‘uniqueifğµ ğ‘isğœ†-farfromanyother
ğµ ğ‘â€².Wemayincreaseğœ†iftoomanyagentsareunique.
â€¢ Obtainfitnessscoresğ‘“ ğ‘ foreachagent,andfitnesspredic-
tionsğ‘“ ğ‘â€²usingeachagentsneighborhoodaverage.
â€¢ Deleteğ‘› ğ‘Ÿğ‘’ğ‘š agentsstochasticallyusingEquation2onrela- (a)Totaldistribution (b)Captaindistribution
tivefitnessğ‘“ ğ‘âˆ’ğ‘“ ğ‘â€².
WedothistokeepthespiritofMAP-Eliteswhileallowingitto Figure4:BERTeamdistributionsthroughouttraining,sorted
maintainafixedpopulationsize,whichisnecessaryforthehybrid byprobability(largestonbottom)
trial with BERTeam. The original MAP-Elites algorithm can be
recoveredbyremovingstochasticityandrepeatedlyrunningour teamscontainingitbecameheavilyfavoredinthefirstfewepochs.
versionwithfixedğœ†untileveryagentisunique. Initially,itseemsBERTeamfavored(2,2),theteamcomposedof
AnotherconsiderationisthatintheMCAApaper,islandswere onlyhardattackingagents.However,aroundepoch500,thedis-
distinguishedbyhavingdifferentproportionsofvarioustypesof tributionshiftedandteam(2,2)sharplydecreasedinoccurrence
robots(i.e.dronesandrovers).Inourcase,wecannotdothisasall probability,infavorofteam(2,5),thestrongbalancedteam.After
agentsarehomogeneous.Toimitatethisvariation,weimplement this,therewerenomajorchangesinBERTeamâ€™soutputdistribution.
adifferentRLalgorithmoneachisland,varyingthealgorithmtype
aswellasthenetworksizeused.Wedescribethesealgorithms, True True Predicted BERTeam
Team
alongwithotherexperimentparametersinAppendixE. Rank Elo Rank Occurrence
Onceallfouralgorithmshavebeentrained,wefixthelearned (2,5) 1 1388 1 0.14
agentpoliciesandteamselectionpolicies.WedefinetheEloofan (2,2) 2 1337 2 0.13
algorithmastheexpectedEloofateamsampledfromthealgo- (2,3) 3 1135 7 0.06
rithmâ€™steamselector.Weevaluatetherelativeperformanceoftwo (1,2) 4 1112 4 0.10
algorithmsbysamplingteamsandevaluatingthegamesplayed. (0,2) 5 1097 3 0.10
Wesample10000gamesforeachofthe6algorithmcomparisons. (2,4) 6 1087 5 0.10
WeadditionallygroundourEloestimatesbysampling1000games (2,6) 7 1035 6 0.07
againstourfixedpolicyteams.WhendoingElocalculations,wedo
(0,5) 8 975 13 0.03
notupdatethefixedpolicyteams.Wesamplesinceforapopulation
Table1:Comparisonoftrueranksandpredictedranks
sizeofğ‘›,thereareÎ˜(ğ‘›2)possibleteams,resultinginaninfeasible
Î˜(ğ‘›4)possiblepairingsbetweenalgorithms.
ToinspecttheperformanceofBERTeamâ€™sfavoredteamcompo-
5 RESULTS
sitions,weconsiderthetrueElosofeachteam.InTable1,welist
InMCTF,reorderingthemembersofateamhasnoeffectonthe theeightbestperformingteamsandtheirtrueElosalongsidethe
teamâ€™sperformance.However,BERTeamisasequencegeneration rankingsandoccurrenceprobabilitiesfromBERTeam.Wefindthat
model,soitdoesdistinguishorder.Tomakeresultsmorereadable, thetoptwochoicesmadebyBERTeamarecorrect,beingthebal-
we assume any reordering of a given team is equivalent to the anced(2,5),andtheaggressive(2,2)respectively.Theiroccurrence
originalteam,andcalculatedistributionsandElosaccordingly.A probabilities(14%,13%)arealsoreasonablylargerthanthethird
caveattothisisthatteamswithtwodistinctagentsarecounted rankat10%.BERTeamdoescorrectlyselectthenextfive,though
twiceinatotaldistribution,whileteamswithtwocopiesofone theorderisshuffled.Withtheexceptionofteam(2,3),theyallare
agentarecountedonce.Tocompensateforthisduringanalysis, reasonablyclosetotheircorrectpositions.
wedoublethedistributionvalueofthesecondtypeofteam,and InFigure4(b),weweconsiderthedistributionofteamcaptains
normalize.ThisisrelevantmainlyinFigure4(a)andTable1,where chosenbyBERTeam.Thisistheexpectedoutputdistributiongiven
weinspectthetotaldistributionofasmallnumberofteams.For acompletelymaskedsequence.Wefindthatagent2isstrongly
theanalysisofcaseswithmanyagents,thiseffectisnegligible. favoredthroughouttraining,indicatingthatitisalikelychoice
in a top performing team. This can be related to NLP, with an
5.1 TeamSelectionwithFixedPolicyAgents
analogousproblemofgeneratingthefirstwordofanunknown
Throughouttraining,weinspectthetotaldistributionofteams sentence.Justasarticlesandprepositions(e.g.â€˜Theâ€™)arestrong
learnedbyBERTeam.FromFigure4(a),wenoticethatourproposed choicesforthistask,BERTeambelievesagent2isastrongchoice
trainingalgorithmcertainlyseemstolearnsomethingnon-uniform. toleadateam.ThisissupportedbythetopseventeamsinTable1
Thetopsevenoutof21possibleteamcompositionsaccountfor beingallteamscontainingagent2.Thus,justasinNLP,BERTeamis
about75%ofthetotaldistribution.ItseemslikeBERTeamimme- abletoaccuratelydetermineagentslikelytobeinwell-performing
diatelyfavoredteamscontainingthehardattackingagent,asthe teams,andchoosethemasteamcaptains.TheseresultsindicateBERTeamisabletolearnanon-trivialteam 5.2.1 Team Elos. We calculate the true Elos for all 1275 teams,
composition,asitpredictedtopteamswithreasonableaccuracy. usingthefixedpolicyagentteamsasabaseline.Weplotthesein
Figure6,alongwithBERTeamâ€™soccurrenceprobability.Weparti-
5.2 TeamSelectionwithCoevolutionofAgents tionallteamsintoâ€˜Defensiveâ€™,â€˜Balancedâ€™,andâ€˜Aggressiveâ€™based
onwhethertheyhavezero,one,ortwoaggressiveagents.Wealso
conductalinearregressiononall1275teamsandplottheline.
WefindthatthereisacorrelationwiththetrueEloofateam
and BERTeamâ€™s probability of outputting that team. Our linear
regressionhadacorrelationcoefficientğ‘…2 â‰ˆ .25,implyingthat
about25%ofthevarianceinBERTeamâ€™soutputisexplainedbythe
performanceoftheteam.Thisindicatesthedistributionlearnedby
BERTeam,whilenoisy,favorsteamsthatperformwell.
WefindthatthetrueEloofthebestperformingteamisaround
1017,indicatingaperformanceslightlybetterthananaveragefixed
policyteam.Thisspecificteam(composedoftwodistinctaggressive
(a)Clusteringbasedonaggression (b)BERTeamteamcomposition(ordered) agents)isalsothemostprobableoutputofBERTeam.
Thus,wefindthatBERTeam,trainedalongsidecoevolution,was
Figure5:BERTeamlearneddistributionontrainedagents abletoproduceandrecognizeateamthatperformedcompetitively
againstpreviouslyunseenopponents.Infact,fromtherankingsin
Table1,weseethatthetopchoicefromBERTeamoutperformsany
After training, we evaluate the evolved agents based on the
teamthatdoesnotcontainagent2(thehardoffensiveagent).
aggressionmetricdescribedinSection4.3.1.WeobservefromFigure
Whiletheperformanceoftheteamslearnedfromself-playare
5(a)thatthemetricclusterstheevolvedagentsintotwocleargroups.
lowerthanthetopfixedpolicyagents,thismaybearesultfrom
Fromourpopulationof50agents,weclassify36asdefensiveand14
difficultiesintheenvironment,suchasalackofcorrelationbetween
asaggressive.Thesimilardistributionofagentfitnessacrosseach
gameoutcomesandMDPrewards.Thiscouldalsopotentiallybe
populationindicatesthatthisdiversityisnotcorrelatedwithagent
improvedbyhyperparametertuningineitherthebaseRLalgorithm,
performance.Itisinsteadlikelyduetospecializationfordifferent
thecoevolutionalgorithm,orinBERTeam.
subtasks.Thisindicatesthatourtrainingschemesupportsdiversity
Overall,thereasonableperformanceoftopcoevolvedteams,as
inagentbehaviorsduringcoevolution,evenwhentheMDPreward
wellasthepositivecorrelationinFigure6,indicatethatBERTeam
structureforeachagentisidentical.
trainedalongsidecoevolutionissuccessfulatoptimizingpolicies
Weusethegroupingofagentsbyaggressiontopartitionthe
foramultiagentadversarialgameagainstunknownopponents.
teamdistributionlearnedbyBERTeam.WenoticefromFigure5(b)
thatthetotaldistributionofBERTeamheavilyfavorsabalanced
5.2.2 AgentEmbeddings. Recallthatthefirststepofatransformer
team,composedofadefensiveandanaggressivemember.This
istoassigneachtokenavectorembedding.Wedirectlyinspectthe
pairingaccountsforabout75%ofthetotaldistribution.Thesecond
agentembeddingslearnedbyBERTeam.Forasubsetofthetotal
mostcommoncompositionistwoaggressivemembers,accounting
population,weconsidertheaveragecosinesimilarityofeachpair
forabout20%ofthetotaldistribution.
chosenfromthesubset.Weusethisasanestimateofhowsimilar
ThedistributionlearnedbyBERTeamalignswithourobserva-
BERTeambelievesagentsinthatsubsetare.
tioninthefixedpolicyexperiment,whereabalancedteamperforms
Foroursubsetchoices,wedividethetotalpopulationintoag-
thebest(Table1).ThisresultalsoimpliesthatBERTeamislearning
gressiveanddefensiveagents,asinFigure5(a).Wefurtherdivide
anon-trivialteamcomposition,sinceasolutionsuchasâ€˜always
eachsubsetintoâ€˜strongâ€™andâ€˜weakâ€™basedonwhethertheirElo
choosethebestagentâ€™favorsateamcomposedofoneagenttype.
isabovethepopulationaverage.WeexpectthatBERTeamâ€™sem-
beddingsofeachclassofagentshavemoresimilaritythanasubset
chosenuniformlyatrandom.Wecalculatethecosinesimilarityof
auniformrandomsubsetinAppendixC.
FromtheresultsinTable2,wecanseethatthemajorityofthe
subsetswechosehaveastrongersimilaritythanarandomsubset
ofthesamesize.Theonlysubsetsthatdonotsupportthisarethe
â€˜Defensiveâ€™andâ€˜WeakDefensiveâ€™subsets,whichareslightlylower.
This suggests that the initial vector embeddings learned by
BERTeamarenotsimplyuniquelydistinguishingeachagent.The
agentsthatperformsimilarlyareviewedassimilarbyBERTeam.
Thisindicatesknownpropertiesoftokenembeddingsinthedomain
ofNLP(suchaswordvectorslearnedin[28])applyinourcase
aswell.ThevectorslearnedbyBERTeamencodeaspectsofeach
agentâ€™sbehavior,andsimilaritiesinagentscanbeinferredthrough
Figure6:BERTeamtotaldistributionandElos
similaritiesintheirinitialembeddings.ThissuggestsBERTeamcanSubset AvgCosineSimilarity Size significantcostisjustifiedbyitsstrongerperformance,andthat
UniformRandom -0.00306 Any itcanbetrainedofflinewithoutinterferingwiththeflowofthe
Aggressive 0.0153 14 restofthealgorithms.Finally,wefindourimplementationofMAP-
Elitesiscostly,aswesampleseparategamestoperformbehavior
Defensive -0.00449 36
projection.WecouldmitigatethisbyimplementingMAP-Elites
StrongAggressive 0.01984 8
moresimilartotheoriginalimplementation.
WeakAggressive 0.03846 6
StrongDefensive 0.00741 17
6 CONCLUSION
WeakDefensive -0.00842 19
Table2:AveragecosinesimilarityofBERTeamâ€™slearnedini- In this paper, we propose BERTeam, an algorithm and training
tialembeddingsacrossvariouspopulationsubsets procedurethatisabletolearnteamcompositioninmultiagentad-
versarialgames.Thealgorithmiseffectivebothinchoosingteamsof
fixedpolicyagentsandwhenbeingtrainedalongwiththepolicies
ofindividualagents.BERTeamcanalsotakeininput,allowingitto
accountforincompletetrainingdatabylearningwhichagentshave generateteamsconditionalonobservationsofopponentbehavior.
similarbehaviors,andmaybeinterchangeableinateam. WeevaluatethisalgorithmonPyquaticus,asimulatedcapture-
the-flaggameplayedbyboatrobots.WetestBERTeambothwith
5.3 ComparisonwithMCAAandMAP-Elites
fixedpolicyagentsandtrainingalongsideacoevolutionarydeepRL
Wedirectlycomparethetrainedpoliciesusingouralgorithm,MCAA, algorithm.Wefindthatinbothcases,BERTeameffectivelylearns
andthehybridmethods.Weproduceteamsusingthespecifiedteam strongnon-trivialteamcomposition.Forfixedpolicyagents,we
formationmethod,andusetheoutcomesofthesegamestoestimate findthatBERTeamlearnsthecorrectoptimalteam.Inthecoevolu-
theircomparativeexpectedElosinTable3. tioncase,wefindthatBERTeamlearnstoformabalancedteamof
agentsthatperformscompetitively.Uponfurtherinspection,we
Policy Team Avg.updatetimeof findthatlikeitsinspirationinthefieldofNLP,BERTeamlearns
Elo
Optimizer Selection Agents TeamDist. similaritiesbetweenagentbehaviorsthroughinitialembeddings.
Coevolution BERTeam 919 13s/epoch 46s/update Thisallowsittoaccountformissingdatabyinferringthebehav-
Coevolution MCAA 817 13s/epoch â‰ˆ0s/update iorofagents.WealsofindthatBERTeamoutperformsMCAA,an
MAP-Elites BERTeam 883 36s/epoch 45s/update algorithmdesignedforteamselection.
MAP-Elites MCAA 809 35s/epoch â‰ˆ0s/update Overall, BERTeam is a strong team selection algorithm with
Table3:RelativeperformanceofMCAA,ouralgorithm,and rootsinspiredbyNLPtextgenerationmodels.BERTeamâ€™sability
hybridalgorithms tolearnsimilaritiesinagentbehaviorresultsinefficienttraining,
andallowsBERTeamtotrainalongsideindividualagentpolicies.
6.1 FutureResearch
We find that trials that used BERTeam as a team formation
methodoutperformedtrialsthatusedtheMCAAmainlandalgo- â€¢ WedonotexplorethecapabilitytoconditionBERTeamâ€™s
rithm.Onepossibleexplanationforthisisthelackofspecificityin outputonobservationsoftheopponent.Infutureresearch,
theMCAAmainlandalgorithm.WhileBERTeamlearnsthedistri- weplantoshowthatteamsgeneratedthroughconditioning
butiononanindividualagentlevel,MCAAchoosestheproportion outperformteamsgeneratedwithnoinformation.
ofeachislandincludedinateam.Thismethodseemstobemost â€¢ Wetestonlysize2teamstoeasilyanalyzetheoutputof
effectivewhentheislandsaredistinct(i.e.intheoriginalMCAA BERTeam.Weplantotestlargerteamsinfutureresearch.
paper,islandshaddifferentproportionsofrobottypes).However, â€¢ WecanchangeourweightingandtrainingsothatBERTeam
inourcase,itseemsvaryingtheRLalgorithmdidnothaveasimilar willconvergeonaNashEquilibrium,assumingcertainprop-
effect.AnotherpossibleexplanationiswhileBERTeammaylearn ertiesofthegameoutcomes(AppendixD.1).Wedonotmake
anarbitrarilyspecifictotaldistribution,MCAAcanonlylearnato- thesechangesbecausetheymaybeincompatiblewithMLM
taldistributionthatisindependentforeachposition.Thisrestricts training.Weplantoexplorethisinfutureresearch.
MCAAfromlearningdistributionsthatdonotfactorinthisway. â€¢ BERTeamisapplicabletogameswithmorethantwoteams.
Additionally,thissupportspreviousresults.Whentakingthe Infutureexperiments,itwouldbeinterestingtoevaluate
weightedaverageofElosinFigure6,wefindBERTeamâ€™sexpected theperformanceofBERTeamonmulti-teamgames.
Eloisabout930.Thisisclosetotheresultof917,andtheminor â€¢ Forcoevolution,weonlyconsiderabasicevolutionaryalgo-
differencecanbeexplainedbythefactwetrainedforlessepochs. rithmwithreproductionthroughcloning,asin[12].How-
Asforruntimecomplexity,weseparatelyinspecttheclocktime ever,thereisavastliteratureofevolutionaryalgorithmvari-
ofteamtrainingandofagentpolicyupdates.Thedifferencein antsthatcouldreplacethis.Itwouldbeinterestingtoexplore
usingBERTeamorMCAAtogenerateteamsforpolicyupdateswas whicharemostcompatiblewithourtrainingscheme.
negligable.Theruntimewasdominatedbyconductingthesample â€¢ Wedonotfocusonoptimizinghyperparametersinoural-
gamesandconductingtheRLupdates.Forteamtraining,wefind gorithms or their interactions. It would be interesting to
theupdateofMCAAtookalmostnotime.Incontrast,trainingthe optimizetheseacrossawidevarietyofprobleminstances,
BERTeammodeltookabout46secondsonabatchsizeof512.This andinspecttheirrelationwithaspectsofeachinstance.REFERENCES
[29] DovMondererandLloydShapley.1996.FictitiousPlayPropertyforGameswith
[1] DzmitryBahdanauetal.2016.NeuralMachineTranslationbyJointlyLearning IdenticalInterests.JournalofEconomicTheory68,1(1996),258â€“265.
toAlignandTranslate. arXiv:1409.0473[cs.CL] [30] DovMondererandLloydS.Shapley.1996.PotentialGames.GamesandEconomic
[2] Jordan Beason et al. 2024. Evaluating Collaborative Autonomy in Behavior14,1(1996),124â€“143.
Opposed Environments using Maritime Capture-the-Flag Competitions. [31] Jean-BaptisteMouretandJeffClune.2015.Illuminatingsearchspacesbymapping
arXiv:2404.17038[cs.RO] elites. arXiv:1504.04909[cs.AI]
[3] Iz Beltagy et al. 2020. Longformer: The Long-Document Transformer. [32] MichaelNovitzkyetal.2019. Aquaticus:PubliclyAvailableDatasetsfroma
arXiv:2004.05150[cs.CL] MarineHuman-RobotTeamingTestbed.In201914thACM/IEEEInternational
[4] UlrichBerger.2007.Brownâ€™soriginalfictitiousplay.JournalofEconomicTheory ConferenceonHuman-RobotInteraction(HRI).InstituteofElectricalandElectron-
135,1(2007),572â€“578. icsEngineers,392â€“400.
[5] AurÃ©lieBeynieretal.2013.DEC-MDP/POMDP.JohnWiley&Sons,Ltd,Chapter9, [33] AntoninRaffinetal.2021.Stable-Baselines3:ReliableReinforcementLearning
277â€“318. Implementations.JournalofMachineLearningResearch22,268(2021),1â€“8.
[6] GregBrockmanetal.2016.OpenAIGym. arXiv:1606.01540[cs.LG] [34] Pranav Rajbhandari. 2024. BERTeam. https://github.com/pranavraj575/
[7] GeorgeBrown.1951.IterativeSolutionofGamesbyFictitiousPlay.InActivity BERTeam.
AnalysisofProductionandAllocation,T.C.Koopmans(Ed.).Wiley. [35] PranavRajbhandari.2024.TransformerbasedCoevolver.https://github.com/
[8] TomBrownetal.2020.Languagemodelsarefew-shotlearners.InProceedingsof pranavraj575/coevolution.
the34thInternationalConferenceonNeuralInformationProcessingSystems(NIPS [36] PranavRajbhandari.2024.unstable_baselines3.https://github.com/pranavraj575/
â€™20).CurranAssociatesInc.,Article159,25pages. unstable_baselines3.
[9] Wei-ChengChangetal.2020.TamingPretrainedTransformersforExtremeMulti- [37] JuliaRobinson.1951.AnIterativeMethodofSolvingaGame.AnnalsofMathe-
labelTextClassification.InProceedingsofthe26thACMSIGKDDInternational matics54,2(1951),296â€“301.
ConferenceonKnowledgeDiscovery&DataMining(KDDâ€™20).Associationfor [38] John Schulman et al. 2017. Proximal Policy Optimization Algorithms.
ComputingMachinery,3163â€“3171. arXiv:1707.06347[cs.LG]
[10] MoChenetal.2017.MultiplayerReach-AvoidGamesviaPairwiseOutcomes. [39] LloydShapley.1953.Stochasticgames.ProceedingsoftheNationalAcademyof
IEEETrans.Automat.Control62,3(2017),1451â€“1457. Sciences39,10(1953),1095â€“1100.
[11] KyunghyunChoetal.2014.LearningPhraseRepresentationsusingRNNEncoder- [40] DaigoShishikaetal.2019. TeamCompositionforPerimeterDefensewith
DecoderforStatisticalMachineTranslation.InProceedingsofthe2014Conference PatrollersandDefenders.In2019IEEE58thConferenceonDecisionandControl
onEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).Associationfor (CDC).InstituteofElectricalandElectronicsEngineers,7325â€“7332.
ComputationalLinguistics,1724â€“1734. [41] YanSongetal.2024.BoostingStudiesofMulti-AgentReinforcementLearning
[12] DavidCottonetal.2020.CoevolutionaryDeepReinforcementLearning.In2020 onGoogleResearchFootballEnvironment:ThePast,Present,andFuture.In
IEEESymposiumSeriesonComputationalIntelligence(SSCI).InstituteofElectrical Proceedingsofthe23rdInternationalConferenceonAutonomousAgentsandMulti-
andElectronicsEngineers,2600â€“2607. agentSystems(AAMASâ€™24).InternationalFoundationforAutonomousAgents
[13] JacobDevlinetal.2019.BERT:Pre-trainingofDeepBidirectionalTransformers andMultiagentSystems,1772â€“1781.
forLanguageUnderstanding.InProceedingsofNAACL-HLT,Vol.1.Association [42] IlyaSutskeveretal.2014.Sequencetosequencelearningwithneuralnetworks.In
forComputationalLinguistics,2. Proceedingsofthe27thInternationalConferenceonNeuralInformationProcessing
[14] GauravDixitetal.2022. Diversifyingbehaviorsforlearninginasymmetric Systems-Volume2(NIPSâ€™14).MITPress,3104â€“3112.
multiagentsystems.InProceedingsoftheGeneticandEvolutionaryComputation [43] WilsonTaylor.2016."ClozeProcedure":ANewToolForMeasuringReadability.
Conference(GECCOâ€™22).AssociationforComputingMachinery,350â€“358. InJournalismQuarterly.SageJournals,415â€“433.
[15] ArpadElo.1978.TheRatingofChessplayers,PastandPresent.ArcoPub. [44] J.K.Terryetal.2021.PettingZoo:GymforMulti-AgentReinforcementLearning.
[16] PhilippeFlajoletandRobertSedgewick.2013.AnalyticCombinatorics.Cambridge InAdvancesinNeuralInformationProcessingSystems,Vol.34.CurranAssociates,
UniversityPress. Inc.,15032â€“15043.
[17] VanessaFrÃ­as-MartÃ­nezandElizabethSklar.2004.Ateam-basedco-evolutionary [45] AshishVaswanietal.2017. Attentionisallyouneed.InProceedingsofthe
approachtomultiagentlearning.InProceedingsofthe2004AAMASWorkshop 31stInternationalConferenceonNeuralInformationProcessingSystems(NIPSâ€™17).
onLearningandEvolutioninAgentBasedSystems.Citeseer,AutonomousAgents CurranAssociatesInc.,6000â€“6010.
andMultiagentSystems. [46] ChernHanYongandRistoMiikkulainen.2001. CooperativeCoevolutionof
[18] EloyGarciaetal.2020.OptimalStrategiesforaClassofMulti-PlayerReach-Avoid Multi-AgentSystems.
DifferentialGamesin3DSpace.IEEERoboticsandAutomationLetters5,3(2020), [47] ChernHanYongandRistoMiikkulainen.2009. CoevolutionofRole-Based
4257â€“4264. CooperationinMultiagentSystems.IEEETransactionsonAutonomousMental
[19] MorrisH.HansenandWilliamN.Hurwitz.1943.OntheTheoryofSamplingfrom Development1,3(2009),170â€“186.
FinitePopulations.TheAnnalsofMathematicalStatistics14,4(1943),333â€“362. [48] HaoyuZhaoetal.2021.Multi-ObjectiveOptimizationforFootballTeamMember
[20] JohannesHeinrichandDavidSilver.2016.DeepReinforcementLearningfrom Selection.IEEEAccess9(2021),90475â€“90487.
Self-PlayinImperfect-InformationGames. arXiv:1603.01121[cs.LG] [49] YueZhao,LushanJu,andJosÃ¨HernÃ¡ndez-Orallo.2024.Teamformationthrough
[21] MaxJaderbergetal.2019.Human-levelperformancein3Dmultiplayergames anassessor:choosingMARLagentsinpursuit-evasiongames.Complex&Intelli-
withpopulation-basedreinforcementlearning.Science364,6443(2019),859â€“865. gentSystems10,3(2024),3473â€“3492.
[22] MarcinJunczys-Dowmunt.2019.MicrosoftTranslatoratWMT2019:Towards
Large-ScaleDocument-LevelNeuralMachineTranslation.InProceedingsofthe
FourthConferenceonMachineTranslation(Volume2:SharedTaskPapers,Day1).
AssociationforComputationalLinguistics,225â€“233.
[23] HiroakiKitanoetal.1997.TheRoboCupsyntheticagentchallenge97.InProceed-
ingsofthe15thInternationalJointConferenceonArtificalIntelligence-Volume1
(IJCAIâ€™97).MorganKaufmannPublishersInc.,24â€“29.
[24] DaanKlijnandA.E.Eiben.2021. Acoevolutionaryapproachtodeepmulti-
agentreinforcementlearning.InProceedingsoftheGeneticandEvolutionary
ComputationConferenceCompanion(GECCOâ€™21).AssociationforComputing
Machinery,283â€“284.
[25] FanqiLinetal.2023.TiZero:MasteringMulti-AgentFootballwithCurriculum
LearningandSelf-Play.InProceedingsofthe2023InternationalConferenceonAu-
tonomousAgentsandMultiagentSystems(AAMASâ€™23).InternationalFoundation
forAutonomousAgentsandMultiagentSystems,67â€“76.
[26] XiaodongLiuetal.2020.VeryDeepTransformersforNeuralMachineTranslation.
arXiv:2008.07772[cs.CL]
[27] StephenMcAleeretal.2023.Team-PSROforLearningApproximateTMECor
inLargeTeamGamesviaCooperativeReinforcementLearning.InAdvances
inNeuralInformationProcessingSystems,A.Oh,T.Naumann,A.Globerson,
K.Saenko,M.Hardt,andS.Levine(Eds.),Vol.36.CurranAssociates,Inc.,45402â€“
45418.
[28] TomasMikolovetal.2013.Efficientestimationofwordrepresentationsinvector
space. arXiv:1301.3781[cs.CL]A ELOUPDATEEQUATION D DATASETWEIGHTING/SAMPLING
Consider a 1v1 game where outcomes for each player are non- Considerthegeneralcasewithğ‘šteamsinanadversarialgame.We
negativeandsumto1.Elosareamethodofassigningvaluesto assumeBERTeamhasacurrentdistributionforeachteam,andwe
eachagentinapopulationbasedontheirabilityinthegame[15]. wouldliketosampleadatasetfortheteaminğ‘–thposition.Wealso
Ifagents1and2withelos ğ‘“ 1 and ğ‘“ 2 playagame,weexpect assumewehaveanotionofaâ€˜winningâ€™teaminacertaingame.Our
agent1towinwithprobability10ğ‘Œ 1:= 1+exp(1
ğ‘“ 2âˆ’ğ‘“
1).Whenagame goalisthattheoccurrenceofateaminthedatasetisproportional
betweenagents1and2issampled,weupdatetheEloofagentğ‘– toitswinprobabilityagainstopponentsselectedbyBERTeam.
usingthegameoutcomeğ‘† ğ‘– withthefollowingequation: Forteamğ´,denotethiswinprobabilityğ‘Š(ğ´).Letthesetofall
validteamsfortheğ‘–thpositionbeTğ‘–.
ğ‘“ ğ‘–â€² =ğ‘“ ğ‘– +ğ‘(ğ‘† ğ‘– âˆ’ğ‘Œ ğ‘–). (3) ThenaÃ¯veapproachistosimplysampleuniformlyfromTğ‘– and
includeteamsthatwinagainstopponentssampledfromBERTeam.
Notethatiftheoutcomeğ‘† ğ‘– islarger(resp.smaller)thanourexpec- Whilethisresultsinthecorrectdistribution,thismethodwillrarely
tationğ‘Œ ğ‘–,weincrease(resp.decrease)ourEloestimate.Wesetğ‘as findasuccessfulteam,asweassumeBERTeamâ€™schoicesarestrong.
thescaletodeterminethemagnitudeoftheupdates. To increase the probability of finding a successful team, we
maysampleusingBERTeaminsteadofuniformlyfrom Tğ‘–.This
B NUMBEROFPOSSIBLETEAMSOFSIZEK increasesoursuccessrate,butfailstogeneratethecorrectdistri-
bution.Inparticular,theoccurrenceofteamğ´isproportionalto
Wewillfindthenumberofpossiblesizeğ‘˜teamswithindistinguish-
P{ğ´ âˆ¼ BERTeam}Â·ğ‘Š(ğ´).Tofixthis,weuseinverseprobability
ablemembersfromğ‘›totalpolicies.
weighting[19],weightingeachinclusionofteamğ´bytheinverseof
Wefirstpartitionallpossibleteamsofğ‘˜membersbasedontheir
itsselectionprobability.Thisensuresthattheweightedoccurrence
number of distinct policiesğ‘–. In the case ofğ‘– policies, we must
choosewhichoftheğ‘›policiestoinclude((cid:0)ğ‘› ğ‘–(cid:1)
choices).Wethen
oft Te ha im sağ´ ddin itit oh ne ad lla yta cs re et ati es sğ‘Š sy( mğ´) m.
etry,sinceeachteamissampled
assignanagenttoeachoftheğ‘˜ teammembers.Sincewedonot
fromBERTeam.Thus,wemayconsiderğ‘šdatasetsandeachgame
careaboutorder,wemustconsiderthenumberofwaystoassignğ‘˜
updatethedatasetscorrespondingtothewinningteams.Thisin-
indistinguishableobjects(members)intoğ‘–distinctbins(policies).
Thereare(cid:0)ğ‘˜ ğ‘–âˆ’âˆ’ 11(cid:1)
waystodothisbyâ€˜starsandbarsâ€™[16].Thus,overall
c or ue ra ese xs peth rie mn eu nm tsb ,e wr ho ef rs eam thp ele ps law ye erg se at np der og pa pm one eb ny tsa af ra ect so yr mo mfğ‘š et. rI icn
,
thereare(cid:0)ğ‘› ğ‘–(cid:1)(cid:0)ğ‘˜ ğ‘–âˆ’âˆ’ 11(cid:1) teamchoiceswithğ‘–distinctmembers.Wesum
wekeeponedatasetanddothisimplicitly.
ğ‘˜
thisoverallpossiblevaluesofğ‘–toobtain ğ‘–(cid:205) =1(cid:0)ğ‘› ğ‘–(cid:1)(cid:0)ğ‘˜ ğ‘–âˆ’âˆ’ 11(cid:1) .
Ifwedocareaboutorder(i.e.membersaredistinguishable),there D.1 RelationtoNashEquilibria
aretriviallyğ‘›ğ‘˜ waystochooseasequenceofğ‘˜agentsfromğ‘›with Weanalyzethegeneralcasewherethereareğ‘šteamsinagame,and
repeats.Anyintermediateorderconsiderationsmustfallbetween thesetsofvalidteamsareT 1,...,Tğ‘š.Theactofchoosingteams
thesetwoextremes.Ineithercase,withfixedğ‘˜,thereareÎ˜(ğ‘›ğ‘˜)
toplaymultiagentadversarialmatchessuggeststhestructureof
possiblechoicesofateamofsizeğ‘˜fromğ‘›totalagents. a normal form game withğ‘š players. Theğ‘–th playerâ€™s available
actionsareteamsinTğ‘–,andtheutilitiesofachoiceofğ‘šteamsare
C COSINESIMILARITYOFRANDOMSUBSET theexpectedoutcomesofeachteaminthematch.
Arandomsubsetofsizeğ‘˜chosenuniformlyfromapopulationof ThedistributionofBERTeamdefinesamixedstrategyofaplayer
ğ‘– in this game (i.e. a distribution over all teams, an element of
vectorswillhavethesameaveragecosinesimilarityasthewhole
thesimplex Î”(Tğ‘–)).Ideally,wewouldlikethetrainingtocause
population.
BERTeam to approach a Nash Equilibrium of the game. This is
Foraproof,wemayconsiderafullyconnectedgraphwhere
eachnodeisoneofğ‘›vectors,andeachedgejoiningtwoagentsis possible,undersomeassumptions.
Consideraweighteddatasetğ‘†ofteamssampleduniformlyfrom
weightedbytheircosinesimilarity.Theabovestatementisequiva-
lenttosayingtheexpectedaverageweightofedgesinarandom Tğ‘–.Weconstructavectorğ‘£ ğ‘† âˆˆR|Tğ‘–| suchthatthedimensioncor-
inducedsubgraphofsizeğ‘˜istheoverallaverageedgeweight. respondingtoteamğ‘‡ âˆˆ Tğ‘– istheweightedoccurrenceofğ‘‡ inğ‘†.
Considertakingtheexpectationacrossallpossibleğ‘˜ subsets. AssumeBERTeamâ€™sdistributionisğ‘ âˆˆÎ”(Tğ‘–).Wedefineğ‘£ ğ‘†,ğ‘ asthe
E ofa ğ‘˜ch sued bg see tw sce oig nh tat iw nii nll gb ie tc (co hu on ot se ed t(cid:0) hğ‘› ğ‘˜ eâˆ’âˆ’ r22 e(cid:1) mti am ine is n, gas ğ‘˜t âˆ’hi 2s fis rot mhe tn hu em ğ‘›b âˆ’e 2r p ar bo oje uc nt dio an ryof oğ‘£ fğ‘† Î”o (Tn ğ‘–t )o at nh det ğ‘£a ğ‘†n ,ğ‘ge en xt its sp ta hc ee so imfğ‘ plw exrt ,. wÎ” e(T mğ‘–) a. yIf nğ‘ eeis do tn
o
othernodes).Whentakingtheedgeaverageineachğ‘˜subset,we insteadprojectğ‘£ ğ‘† ontothetangentspaceofğ‘ withrespecttoa
divideby(cid:0)ğ‘˜(cid:1) ,andwhentakingtheexpectationacrossallğ‘˜subsets,
lowerdimensionalfaceofÎ”(Tğ‘–).
2 OurfirstassumptionisthatthereisaBERTeamarchitectureand
wedivideby(cid:0)ğ‘› ğ‘˜(cid:1) .Thus,anedgecontributes (( ğ‘˜ğ‘› ğ‘˜ )âˆ’âˆ’ (22 ğ‘›)
)
= (cid:0)ğ‘› 2(cid:1)âˆ’1 times trainingschemesuchthatupdatingwithdatasetğ‘†isequivalent(in
itsweighttotheexpectation,thesameasitw2 ouğ‘˜
ldinanaverage
expectation)toupdatingğ‘inthedirectionofğ‘£ ğ‘†,ğ‘.
(cid:0)ğ‘›(cid:1) Nowconsiderweightingeachteaminğ‘†accordingtoitsexpected
acrossall edges.
2 outcomeagainstadistributionğ‘ âˆˆ (cid:206)Î”(Tğ‘—).Weclaimthatthe
ğ‘—â‰ ğ‘–
resultingvectorğ‘£ ğ‘†,ğ‘ isinthedirectionofanupdatethatimproves
10Elosarescaledinchessbyafactorof 400 ,butignoringthisiscleaner theexpectedutilityofğ‘againstğ‘unlessğ‘isoptimal.
log10Considerthevectorinğ‘…|Tğ‘–| wherethedimensioncorresponding Parameter Value Justification
toteamğ‘‡ istheexpectedoutcomeofğ‘‡ againstğ‘.Byconstruction Epochs 3000 Plotconverged
ofğ‘†,thisisexactlyğ‘£ ğ‘†.Sinceexpectationsarelinear,thechange Gamesperepoch 25 Consistency
inexpectedutilitybyupdatingğ‘ intheğ‘£ ğ‘†,ğ‘ directionisexactly BERTeamTransformer
ğ‘£ ğ‘† Â·ğ‘£ ğ‘†,ğ‘.However,sinceğ‘£ ğ‘†,ğ‘ isaprojectionofğ‘£ ğ‘†,thisisalways Encoder/Decoderlayers 3/3 1 PyTorchdefault
non-negative.Additionally,ğ‘£ ğ‘† Â·ğ‘£ ğ‘†,ğ‘ =0onlywhenğ‘£ ğ‘† iszero,or Embeddingdim 128 2 1 PyTorchdefault
whenğ‘ cannotincreaseprobabilityfortheteamswithmaximal Feedforwarddim 512 4 1 PyTorchdefault
expectedoutcome.Thus,ğ‘£
ğ‘†,ğ‘
improvestheexpectedutilityofğ‘
Attentionheads 4
4
1 PyTorchdefault
againstğ‘unlessğ‘isoptimal. 2
Dropout 0.1 PyTorchdefault
Thisimpliesupdatingwithğ‘†weightedinthiswaywillresultin
Trainfrequency Every10epochs Consistency
animprovingupdatetoğ‘basedonempiricalevidenceofopponent
Batch/Minibatchsize 1024/256
strategies.Thesameholdsforanyoftheğ‘šplayers.
InputEmbedding(Unused)
This process is equivalent to fictitious play, a process where
playersupdatetheirstrategybasedonempiricalestimationsof Networkarchitecture LSTM
opponentstrategies[4,7].Therehasbeenmuchresearchintofor Layers 2
whatclassesofgamesfictitiousplayresultsinconvergencetoa Embeddingdim 128 SameasBERTeam
Nashequilibrium(e.g.zero-sumgameswithfinitestrategies[37], Dropout 0.1 SameasBERTeam
orpotentialgames[29,30]).Ifourdefinedgamehappenstofall Table4:BERTeamexperimentparameters(Section4.2)
inanyofthesecategories,thisprocesswillconvergetoaNash
equilibrium.Thisisoursecondassumption.
Toformadatasetğ‘†suchthattheweightedoccurrenceofeach
Parameter Value Justification
teamisitsexpectedoutcomeagainstadistributionğ‘,wecanfill
Epochs 8000
ğ‘† with teams in Tğ‘– weighed by sampled outcomes againstğ‘. In
Gamesperepoch 25
expectation,thiswillachieveourdesiredweighting.11
ChangesinBERTeamParameters
Weimplicitlyassumethatthebehaviorsofagentsarefixed,so
Batch/Minibatchsize 512/64 Loweredforspeed
thatgamesbetweentwoteamshaveconstantexpectedoutcome.
Thisiscertainlynottrueforconfigurationswhereweupdateagent Coevolution
policies,butcanbeâ€˜trueenoughâ€™ifthepolicieshaveconverged. PopulationSize 50
Oursecondassumptiondependsontheactualoutcomesdefined Replacements Drasticchangesmay
1
inthemultiagentadversarialgame.However,eveninsituations pergeneration destabilizeBERTeam
wherethisassumptiondoesnothold,updatingBERTeaminthis Protectionof Decentpoliciesrequire
500epochs
waywillresultinfictitiousplay,whichseemslikeareasonable newagents âˆ¼500gamesoftraining
updatestrategy.InthecaseofPyquaticus,wecansimplyredefine Eliteagents 3 Protectbestpolicies
ouroutcomevaluestoformazero-sumgame. ReinforcementLearning
Our first assumption is untrue for MLM training. Instead of RLalgorithm PPO
updatingtowardsavector,MLMtrainingusescross-entropyloss, Networkhiddenlayers (64,64) stable_baselinesdefault
encouragingthemodeltomatchadistribution.Whilesomething
Table5:Coevolutionexperimentparameters(Section4.3)
similartoMSElosswouldachievethedesiredgradient,thereis
littlesupportinliteratureforusinglossesotherthancross-entropy
losswhenworkingwithdistributions.Thus,usingadifferentloss
Parameter Value Justification
orchangingthemodelmayresultinpoorlybehavedtraining.
Epochs 4000 Decreasedforspeed
E EXPERIMENTPARAMETERS GamesperEpoch 16 Decreasedforspeed
Islands 4 Sameasexperimentsin[14]
IslandSize 15 4Â·15â‰ˆ50
EliteAgentsperIsland 1 1Â·4â‰ˆ3
Island0
RLalgorithm PPO
Networkhiddenlayers (64,64) stable_baselinesdefault
Island1
RLalgorithm PPO
11IfwealsomustweighttoaccountforsamplingbiasasinAppendixD,wecansimply Networkhiddenlayers (96,96) Slightlymorecomplex
multiplytheinverseprobabilityweightwiththeoutcomeweighttoensurebothgoals. Island2
RLalgorithm DQN
Networkhiddenlayers (64,64) stable_baselinesdefault
Island3
RLalgorithm DQN
Networkhiddenlayers (96,96) Slightlymorecomplex
Table6:Comparisonexperimentparameters(Section4.4)