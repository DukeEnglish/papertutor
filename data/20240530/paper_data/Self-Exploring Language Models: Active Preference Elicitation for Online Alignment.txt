Self-Exploring Language Models:
Active Preference Elicitation for Online Alignment
ShenaoZhang1 DonghanYu2 HiteshiSharma2
ZiyiYang2 ShuohangWang2 HanyHassan2 ZhaoranWang1
1NorthwesternUniversity 2Microsoft
Abstract
Preferenceoptimization,particularlythroughReinforcementLearningfromHuman
Feedback(RLHF),hasachievedsignificantsuccessinaligningLargeLanguage
Models (LLMs) to adhere to human intentions. Unlike offline alignment with
a fixeddataset,online feedbackcollection from humans orAI on modelgener-
ations typicallyleads to more capable rewardmodels andbetter-alignedLLMs
throughaniterativeprocess.However,achievingagloballyaccuraterewardmodel
requiressystematicexplorationtogeneratediverseresponsesthatspanthevast
spaceofnaturallanguage.Randomsamplingfromstandardreward-maximizing
LLMs alone is insufficient to fulfill this requirement. To address this issue,we
proposeabilevelobjectiveoptimisticallybiasedtowardspotentiallyhigh-reward
responsestoactivelyexploreout-of-distributionregions.Bysolvingtheinner-level
problemwiththereparameterizedrewardfunction,theresultingalgorithm,named
Self-ExploringLanguageModels(SELM),eliminatestheneedforaseparateRM
anditerativelyupdatestheLLMwithastraightforwardobjective. Comparedto
DirectPreferenceOptimization(DPO),theSELMobjectivereducesindiscriminate
favorofunseen extrapolations andenhances exploration efficiency. Ourexperi-
mentalresultsdemonstratethatwhenfinetunedonZephyr-7B-SFTandLlama-3-
8B-Instructmodels,SELM significantlyboosts theperformanceon instruction-
followingbenchmarkssuchasMT-BenchandAlpacaEval2.0,aswellasvarious
standard academic benchmarks in different settings. Our code and models are
availableathttps://github.com/shenao-zhang/SELM.
1 Introduction
LargeLanguageModels(LLMs)haverecentlyachievedsignificantsuccesslargelyduetotheirability
tofollowinstructionswithhumanintent.AsthedefactomethodforaligningLLMs,Reinforcement
LearningfromHumanFeedback(RLHF)worksbymaximizingtherewardfunction,eitheraseparate
model[44;5;18]orreparameterizedbytheLLMpolicy[49;48;4;70],whichislearnedfromthe
prompt-response preference data labeled by humans. The key to the success of alignment is the
responsediversitywithinthepreferencedata,whichpreventsrewardmodels(RMs)fromgetting
stuckinlocaloptima,therebyproducingmorecapablelanguagemodels.
Offlinealignmentmethods[49;54]attempttomanuallyconstructdiverseresponsesforfixedprompts
[11;24;72],which,unfortunately,strugglestospanthenearlyinfinitespaceofnaturallanguage.On
theotherhand,onlinealignmentfollowsaniterativeprocedure:samplingresponsesfromtheLLM
andreceivingfeedbacktoformnewpreferencedataforRMtraining[44;21].Theformerstephelps
exploreout-of-distribution(OOD)regionsthroughrandomnessinsampling.However,instandard
onlineRLHFframeworks,maximizingtheexpectedrewardlearnedfromthecollecteddataisthe
onlyobjectivefortheLLM,samplingfromwhichoftenleadstoresponsesclusteredaroundlocal
optima.Thispassiveexplorationmechanismcansufferfromoverfittingandprematureconvergence,
leavingthepotentiallyhigh-rewardregionsunexplored.
4202
yaM
92
]GL.sc[
1v23391.5042:viXraa Figure1:Intuitionofourmethod.Forafixedprompt
dat
optimisticRM x,arewardmodelr(x,y)triestofittheground-truth
d
ve rewardrâˆ—(x,y).TheblueandgreenRMsareequally
er
obs
vanillaRM
goodwhenusingstandardreward-fittinglossL lr,since
the observed preference data (red stars) are fitted
equally well. However, the green RM has a larger
uncertainty ground-truth max r(x,y) and thus a lower optimistically biased
y
lossL âˆ’Î±max r(x,y).Therefore,theresponsey
lr y u
ğ‘¦! atwhichtheuncertaintyishighcanbeelicitedandthen
responseğ‘¦ proceededforhumanfeedbacktoreduceuncertainty.
To address this issue,we propose an active exploration method for online alignment that elicits
novelfavorableresponses. Initssimplestform,anoptimismtermÎ±max r(x,y)isaddedtothe
y
reward-fittingobjective(e.g.,logisticregressionondatasetD),denotedasâˆ’L ,resultinginabilevel
lr
optimizationobjectivefortherewardmodelr:
maxmaxÎ±r(x,y)âˆ’L (r;D), (1.1)
lr
r y
whereÎ±isahyperparametercontrollingthedegreeofoptimism.TheintuitionisillustratedinFigure
1.Specifically,minimizingthevanillareward-fittinglossL islikelytogivealocallyaccurateRM
lr
thatoverfitstheobserveddataandgetsstuckinlocalminima.Randomsamplingfromthisvanilla
RMmaytakealongtimetoexploretheOODregionsthatcontainthebestresponse.Byincorporating
theoptimismterm,weobtainanRMthatbothfitsthedatawellandhasalargemax r(x,y).This
y
ensuresthatthegreedyresponsey fromitiseithergloballyoptimalwhenuncertaintyinhigh-reward
u
regionsiseliminated,orpotentiallygoodinunexploredareaswherer(x,y )canbearbitrarilyhuge
u
duetotherelaxedreward-fittingloss.Feedbackfromhumansontheseresponsesy canthenreduce
u
uncertaintyandtrainamoreaccurateRM.
Inthispaper,weformulatethisideawithinthecontextofonlinedirectalignment,wheretheLLMis
iterativelyupdatedwithoutaseparateRM.WefirstintroducetwomodificationstothebilevelRM
objectivein1.1,namelyaddingKLconstraintsandusingrelativemaximumreward.Thenwederive
asimpleLLMtrainingobjectivebyapplyingtheclosed-formsolutionoftheinner-levelproblem
andreparameterizingtherewardwiththeLLMpolicy. Theresultingiterativealgorithmiscalled
Self-Exploring Language Models (SELM). We show that the policy gradient of SELM is biased
towardsmorerewardingareas.Furthermore,byreducingthechanceofgeneratingresponsesthatare
assignedlowimplicitrewards,SELMmitigatestheindiscriminatefavoringofunseenextrapolations
foundinDPO[49;48]andenhancesexplorationefficiency.
In experiments,we implement SELM using Zephyr-7B-SFT [57] and Llama-3-8B-Instruct [38]
asbasemodels.ByfinetuningsolelyontheUltraFeedback[11]datasetandusingthesmall-sized
PairRM[25]foriterativeAIfeedback,SELMbooststheperformanceofZephyr-7B-SFTandLlama-
3-8B-Instruct by a large margin on AlpacaEval 2.0 [14] (+16.24% and +11.75% LC win rates)
andMT-Bench[71](+2.31and+0.32).SELMalsodemonstratesstrongperformanceonstandard
academicbenchmarksandachieveshigherpairwiseLCwinratesagainsttheiterativeDPObaseline.
Ourcodeandmodelsareavailableathttps://github.com/shenao-zhang/SELM.
2 Background
2.1 LargeLanguageModels
A language modelÏ€ âˆˆ âˆ†X typically takes the promptx âˆˆ X as inputandoutputs the response
Y
y âˆˆ Y.Here,X andY arefinitespacesofpromptsandresponses,respectively.Giventheprompt
xâˆˆX,adiscreteprobabilitydistributionÏ€(Â·|x)âˆˆâˆ† isgenerated,whereâˆ† isthesetofdiscrete
Y Y
distributions overY. Modern recipes fortraining LLMs consistofpre-training andpost-training
procedures,whereduringpre-training,LLMslearntopredictthenextwordonahugeanddiverse
datasetoftextsequences in orderto understandthe underlying patterns andstructures ofnatural
languageinanunsupervisedmanner.Thepost-trainingprocedureaimstoalignbettertoendtasks
and human preferences with two phases happening in order: Supervised Fine-Tuning (SFT) and
humanpreferencealignment.Here,SFTfine-tunesthepre-trainedLLMwithsupervisedlearning
2
draweron high-qualitydata to followinstructions on downstream tasks andobtain a modelÏ€SFT. In the
followingofthispaper,wefocusmainlyonpreferencealignment.
2.2 RewardModelingandPreferenceOptimization
ReinforcementLearningfromHumanFeedback(RLHF). StandardRLHFframeworksconsist
oflearningarewardmodelandthenoptimizingtheLLMpolicyusingthelearnedreward.
Specifically,apoint-wiserewardr(x,y):X Ã—Y â†’RrepresentstheEloscore[16]oftheresponse
ygiventhepromptx.ThenthepreferencedistributioncanbeexpressedbytheBradley-Terrymodel
thatdistinguishesbetweenthepreferredresponsey andthedispreferredresponsey givenprompt
w l
x,denotedasy â‰»y |x,usingthelogisticfunctionÏƒ:
w l
p(y â‰»y |x):=E (cid:2) 1(hprefersy overy givenx)(cid:3)
w l h w l
(cid:0) (cid:1)
exp r(x,y )
(cid:0) (cid:1) w
=Ïƒ r(x,y )âˆ’r(x,y ) = , (2.1)
w l (cid:0) (cid:1) (cid:0) (cid:1)
exp r(x,y ) +exp r(x,y )
w l
wherehdenotesthehumanraterandtheexpectationisoverhtoaccountfortherandomnessofthe
choicesofhumanratersweaskfortheirpreference.WhenprovidedastaticdatasetofN comparisons
D ={x ,y ,y }N ,theparameterizedrewardmodelcanbelearnedbyminimizingthefollowing
i w,i l,i i=1
logisticregressionloss:
L (r;D)=âˆ’E (cid:2) logÏƒ(cid:0) r(x,y )âˆ’r(x,y )(cid:1)(cid:3) . (2.2)
lr (x,yw,yl)âˆ¼D w l
Usingthelearnedreward,theLLMpolicyÏ€ âˆˆâˆ†X isoptimizedwithreinforcementlearning(RL)to
Y
maximizetheexpectedrewardwhilemaintainingasmalldeviationfromsomebasereferencepolicy
Ï€ ,i.e.,maximizingthefollowingobjective
ref
J(Ï€;D)=E (cid:2) r(x,y)(cid:3) âˆ’Î²D (Ï€||Ï€ ), (2.3)
xâˆ¼D,yâˆ¼Ï€(Â·|x) KL ref
whereÎ² isahyperparameterandD (Ï€||Ï€ ):=E [KL(Ï€(Â·|x)||Ï€ (Â·|x))]istheexpected
KL ref xâˆ¼D ref
Kullback-Leibler(KL)divergence.AnidealÏ€ isthepolicythathelpsmitigatethedistributionshift
ref
issue[49;21]betweenthetruepreferencedistributionandthepolicyÏ€ duringtheoff-policyRL
training.SinceweonlyhaveaccesstothedatasetDsampledfromtheunavailabletruepreference
distribution,Ï€ canbeobtainedbyfine-tuningonthepreferredresponsesinD orsimplysetting
ref
Ï€ =Ï€SFTandperformingRLHFbasedontheSFTmodel.
ref
DirectAlignmentfromPreference. Withthemotivationtogetridofaseparaterewardmodel,
whichiscomputationallycostlytotrain,recentworks[49;4;70;57;17]derivedthepreferenceloss
asafunctionofthepolicybychangingofvariables.Amongthem,DPO[49]showsthatwhentheBT
modelin(2.1)canperfectlyfitthepreference,theglobaloptimizersoftheRLHFobjectivein(2.3)
andthefollowinglossareequivalent:
(cid:20) (cid:18) (cid:19)(cid:21)
Ï€(y |x) Ï€(y |x)
L (Ï€;D)=âˆ’E logÏƒ Î²log w âˆ’Î²log l .
DPO (x,yw,yl)âˆ¼D Ï€ (y |x) Ï€ (y |x)
ref w ref l
3 Self-ExploringLanguageModels
3.1 RM-FreeObjectiveforActiveExploration
Inthissection,wepresentseveralmodificationstotheoptimisticallybiasedobjective(1.1)motivated
intheintroduction.ThenwederiveanRM-freeobjectivefortheLLMpolicyandanalyzehowactive
explorationworksbyexaminingitsgradient.
First,we considerthe equivalence of(1.1): max âˆ’L (r;D)+Î±max E [r(x,y)],where the
r lr Ï€ yâˆ¼Ï€
innerÏ€isdeterministicwhenoptimal.ToaccountforthechangeofÏ€relativetothereferencepolicy
Ï€ ,weintroducetwomodifications:(1)replacingtheoptimisticbiastermmax E [r(x,y)]with
ref Ï€ yâˆ¼Ï€
max E [r(x,y)âˆ’r(x,yâ€²)],and(2)incorporatingaKL-divergencelosstermbetweenÏ€
Ï€ yâˆ¼Ï€,yâ€²âˆ¼Ï€ref
andÏ€ .ThesechangesensurethattheresultingoptimisticRMelicitsresponseswithhighpotential
ref
unknowntothereferencepolicyÏ€ whileminimizingthedeviationbetweenÏ€andÏ€ .
ref ref
3Formally,fortherewardfunctionr,thebileveloptimizationproblemwithoptimismisformulatedas:
(cid:32) (cid:33)
(cid:104) (cid:105)
maxâˆ’L (r;D )+Î±max E r(x,y)âˆ’r(x,yâ€²) âˆ’Î²D (Ï€||Ï€ ) , (3.1)
r
lr t
Ï€
xâˆ¼ yD â€²âˆ¼t, Ï€y reâˆ¼ f(Ï€ Â·|( xÂ· )|x) KL ref
(cid:124) (cid:123)(cid:122) (cid:125)
F(Ï€;r)
whereD ={x ,yt ,yt }N istheassociateddatasetatiterationtandL isthelogisticregression
t i w,i l,i i=1 lr
loss defined in (2.2). The nested optimization in (3.1) can be handled by first solving the inner
optimizationF(Ï€;r)toobtainÏ€ thatisoptimalunderr.Thesolutionisasfollowsandwedeferall
r
thederivationsinthissectiontoAppendixA.
1 (cid:0) (cid:1)
Ï€ (y |x):=argmaxF(Ï€;r)= Ï€ (y |x)exp r(x,y)/Î² ,
r Z(x) ref
Ï€
(cid:80)
where the partition function Z(x) = Ï€ (y|x)exp(r(x,y)/Î²). By substituting Ï€ = Ï€ into
y ref r
F(Ï€;r),wecanrewritethebilevelobjectivein(3.1)asasingle-levelone:
maxâˆ’L (r;D )+Î±F(Ï€ ;r).
lr t r
r
Following the implicit reward formulation in DPO,we reparameterize the reward function with
Î¸ âˆˆ Î˜asr (x,y) = Î²(logÏ€ (y | x)âˆ’logÏ€ (y | x)),whichistheoptimalsolutionof(2.3)and
(cid:98)Î¸ Î¸ ref
canexpressallrewardclassesconsistentwiththeBTmodelasprovedin[49].Withthischangeof
variable,weobtaintheRM-freeobjectivefordirectpreferencealignmentwithoptimism:
maxâˆ’L (Ï€ ;D )âˆ’Î±Î²E (cid:2) logÏ€ (y |x)(cid:3) . (3.2)
Ï€Î¸
DPO Î¸ t xâˆ¼D,yâˆ¼Ï€ref(Â·|x) Î¸
Wenowanalyzehowthisnewobjectiveencouragesactiveexploration.Specifically,wederivethe
gradientof(3.2)withrespecttoÎ¸as
âˆ’Î²E (cid:104) Ïƒ(cid:0) r (x,y )âˆ’r (x,y )(cid:1)(cid:0) âˆ‡ logÏ€ (y |x)âˆ’âˆ‡ logÏ€ (y |x)(cid:1)(cid:105)
(x,yw,yl)âˆ¼Dt (cid:98)Î¸ l (cid:98)Î¸ w Î¸ Î¸ w Î¸ Î¸ l
(cid:124) (cid:123)(cid:122) (cid:125)
âˆ‡Î¸LDPO(Ï€Î¸;Dt)
âˆ’Î±Î²E (cid:2) exp(cid:0) âˆ’r (x,y)/Î²(cid:1) âˆ‡ logÏ€ (y |x)(cid:3) . (3.3)
xâˆ¼D,yâˆ¼Ï€Î¸(Â·|x) (cid:98)Î¸ Î¸ Î¸
We note that the second line,corresponding to the gradient of the optimism term,decreases the
log-likelihoodofresponseygeneratedbyÏ€ thathasalowvalueofexp(âˆ’r (x,y)/Î²).Therefore,
Î¸ (cid:98)Î¸
theaddedoptimismtermbiasesthegradienttowardparameterregionsthatcanelicitresponsesywith
highimplicitrewardr ,consistentwithourintuitionoutlinedinFigure1.
(cid:98)Î¸
This also explains why E [logÏ€ ] is minimized in our objective (3.2), which is equivalent to
Ï€ref Î¸
maximizingtheKLdivergencebetweenÏ€ andÏ€ ,whilethereverseKLinthepolicyoptimization
ref Î¸
objective(2.3)isminimized.FortheDPOgradientâˆ‡ L (Ï€ ;D ),thedegreeofdeviationofpolicy
Î¸ DPO Î¸ t
Ï€ fromÏ€ onlyaffectsthepreferenceestimatedwithr .Inotherwords,Ïƒ(r (x,y )âˆ’r (x,y ))
Î¸ ref (cid:98)Î¸ (cid:98)Î¸ l (cid:98)Î¸ w
isascalarvalueandthepolicydeviationonlydeterminesthestepsizeofthepolicygradient,instead
ofitsdirection.Ontheotherhand,ouraddedexplorationtermdirectlycontrolsthedirectionofthe
gradienttowardpotentiallymorerewardingareaswhilestillfittingthepreferencedatainD . As
t
morefeedbackdataiscollectediteratively,deviatingfromtheunbiasedlyfittedmodelincursahigher
DPOloss,whichultimatelydominatesourobjectiveatconvergence.Thismechanismensuresthat
theresultingLLMeffectivelybalancesbetweenexploringnovelresponsesandexploitingpreviously
observedones,leadingtoamoreaccurateandalignedmodel.
3.2 Algorithm
Withtheoptimisticallybiasedobjectivederivedabove,thelanguagemodelcanactivelygenerate
OODresponsesworthexploring.HumanorAIfeedbackfollowstoreducetheuncertaintyinthese
regions.Thesetwostepsareexecutediterativelytogetamoreandmorealignedmodel.
Inpractice,wesplittheofflinepreferencedatasetintothreeportionswithequalsizes,oneforeach
iteration. Besides,we use AI rankers,such as external RMs,to provide feedback on the model-
generatedresponseandtheoriginalchosen,rejectedresponses. Thecompletepseudocodeofour
algorithm,namedSelf-ExploringLanguageModels(SELM),isoutlinedinAlgorithm1.
4Algorithm1Self-ExploringLanguageModels(SELM)
Input: ReferencemodelÏ€ ,preferencedatasetD,onlineiterationsT,optimismcoefficientÎ±.
ref
1: foriterationt=1,2,...,T do
2: SetD tasthet-thportionofDandgeneratey âˆ¼Ï€ ref(Â·|x)foreachpromptxinD t.
3: Rank{y,y w,y l}andupdateD ttocontainthebest(chosen)andworst(rejected)responses.
4: TraintheLLMÏ€ Î¸t =argmax Ï€Î¸âˆ’L DPO(Ï€ Î¸;D t)âˆ’Î±E xâˆ¼Dt[logÏ€ Î¸(y |x)]andletÏ€ ref =Ï€ Î¸t.
5: endfor
3.3 Self-ExplorationReducesIndiscriminateFavorofUnseenExtrapolations
Ithasbeenobservedrecently[48;46;64]thatDPOdecreasesthelikelihoodofresponsesgenerated
bythereferencepolicy.Itisbecauseforanypromptx,atconvergencewhenÏ€ Ì¸=Ï€ ,itholdsthat
Î¸ ref
E (cid:2) r (x,y)/Î²(cid:3) =E (cid:2) logÏ€ (y |x)âˆ’logÏ€ (y |x)(cid:3) =âˆ’KL(cid:0) Ï€ (Â·|x)||Ï€ (Â·|x)(cid:1) <0,
yâˆ¼Ï€ref (cid:98)Î¸ yâˆ¼Ï€ref Î¸ ref ref Î¸
while at the beginning of training when Ï€ = Ï€ ,the above terms are zero. Thus,the expected
Î¸ ref
implicitrewardr aswellasthelikelihoodofÏ€ willdecreaseonthereferencemodelâ€™sresponses.
(cid:98)Î¸ Î¸
ThisindicatesthatDPOstimulatesabiaseddistributionfavoringunseenextrapolatedresponses.Inthe
onlineiterativesettingthatweconsider,theLLMpolicygeneratesresponsesandreceivespreference
feedbackalternately,wherebiasingtowardsOODregionsmaysometimeshelpdiscoveroutstanding
novelresponses.However,DPOindiscriminatelyfavorsunseenextrapolationsandpassivelyexplores
basedpurelyontherandomnessinherentinsamplingfromtheLLM.Asaconsequence,thevastspace
ofnaturallanguagemakesitalmostimpossibletoexhaustivelyexploreallthepossibleresponsesand
identifythosethatmosteffectivelybenefitalignment.
Next,wedemonstratethatSELMmitigatesthisissuebyperformingguidedexploration.Specifically,
considertheproposedself-explorationobjectivein(3.2),which,inadditiontothestandardDPOloss,
alsominimizesE [logÏ€ (y |x)].Wenowinvestigatehowtheprobabilitydistributionchanges
x,yâˆ¼Ï€ref Î¸
withthistermincorporated.
Theorem 3.1. For any Ï âˆˆ Î˜ in the policy parameter space, let r (x,y) = Î²(logÏ€ (y|x) âˆ’
(cid:98)Ï Ï
logÏ€ (y|x)) be the reparameterized implicit reward. Denote Ï€min as the policy that minimizes
ref Ï
theexpectedimplicitrewardundertheKLconstraint,i.e.,
Ï€min(Â·|x):=argminE (cid:2) r (x,y)(cid:3) +Î²D (Ï€||Ï€ ). (3.4)
Ï x,yâˆ¼Ï€(Â·|x) (cid:98)Ï KL Ï
Ï€
ThenminimizingE [logÏ€ (y|x)]decreasesthelikelihoodofresponsessampledfromÏ€min:
x,yâˆ¼Ï€ref Î¸ Ï
minE (cid:2) logÏ€ (y |x)(cid:3) =minE (cid:2) logÏ€ (y |x)(cid:3) .
Ï€Î¸
x,yâˆ¼Ï€ref(Â·|x) Î¸
Ï€Î¸
x,yâˆ¼Ï€ Ïmin(Â·|x) Î¸
TheabovetheoremstatesthatmaximizingthedivergencebetweenÏ€ andÏ€ isessentiallyreducing
Î¸ ref
the probability ofgenerating responses withlow implicitrewards reparameterizedby any policy
parameterÏduringtraining.Inotherwords,thepolicynotonlyexploitstheexistingpreferencedata
butalsolearnstoavoidgeneratingthetextythatisassignedalowrewardvalue.Thisprocessoccurs
ineveryiterationwithupdatedreferencemodels.Consequently,responseswithhighpotentialrewards
areselectivelypreferredandmanycommonplaceresponsesreceiveasmallprobabilitymass,thus
mitigatingtheindiscriminatefavoringofunseenresponsesandimprovingexplorationefficiency.
4 RelatedWork
DataSynthesisforLLMs. Akeychallengeforfine-tuninglanguagemodelstoalignwithusersâ€™
intentionsliesinthecollectionofdemonstrations,includingboththeSFTinstruction-followingexpert
dataandtheRLHFpreferencedata.Gatheringsuchdatafromhumanlabelersisexpensive,time-
consuming,andsometimessuffersfromvariantquality[44;29].Toaddressthisissue,syntheticdata
[34]hasbeenusedforaligningLLMs.Onelineofworkfocusesongeneratingplausibleinstruction
promptsforunlabeleddatabyregardingthetargetoutputasinstruction-followingresponses[31;
60; 27; 55]. Besides, high-quality data can also be distilled from strong models for fine-tuning
weakerones[20;1;32;12;47].ToconstructsyntheticdatasetsforofflineRLHF,apopularpipeline
[11;57;59;24;72]involvesselectingresponsessampledfromvariousLLMsonasetofpromptsin
5thehopetoincreasethediversityofthedatathatcanspanthewholelanguagespace.However,data
manuallycollectedinsuchapassivewaydoesnotconsiderwhatimprovesthemodelmostthrough
itstraining,leavingthepotentiallyhigh-rewardregionsunexplored.
IterativeOnlinePreferenceOptimization ComparedtoofflineRLHFalgorithms[49;70;4]that
collectpreferencedatasetsaheadoftraining,onlineRLHF[44;21],especiallytheiterative/batched
onlineRLHF[5;63;19;22;62;6;50]hasthepotentialtogatherbetterandbettersyntheticdataasthe
modelimproves.Asaspecialcase,self-alignmentlanguagemodelsaligntheirresponseswithdesired
behaviors,suchasmodel-generatedfeedback[66;67;53;58]. Unfortunately,theabovemethods
stillpassivelyexplorebyrelyingontherandomnessduringsamplingandeasilygetstuckatlocal
optimaandoverfittothecurrentdataduetothevastspaceofnaturallanguage.Anotableexception
is [15],which proposed to use ensembles of RMs to approximately measure the uncertainty for
posterior-samplingactiveexploration.Onthecontrary,ourmethodexploresbasedontheoptimistic
biasanddoesnotestimatetheuncertaintyexplicitly,bypassingtheneedtofitmultipleRMs.
ActiveExploration. Infact,activeexplorationhasbeenwidelystudiedbeyondLLMs.Similarto
[15],mostexistingsample-efficientRLalgorithmsfirstestimatetheuncertaintyoftheenvironment
usinghistoricaldataandthenplanwithoptimism[3;51;26],orselectingtheoptimalactionfroma
statisticallyplausiblysetofactionvaluessampledfromitsposteriordistribution[52;41;42;69].The
proposedself-explorationobjectivecanbecategorizedasanoptimism-basedexplorationmethod.
However,mostpreviousworksrequiretheestimationoftheupperconfidencebound,whichisoften
intractable.Ensemblemethods[43;8;37]canserveasapproximationstotheuncertaintyestimation
butarestillcomputationallyinefficient.MEX[35]proposedtocombineestimationandplanningina
singleobjectivesimilartooursandestablishedtheoreticalguaranteesundertraditionalRLsetups.
Concurrenttoourwork,RPO[36]alsoproposedtouseanadversariallychosenrewardmodelfor
policyoptimization,butfocusesonmitigatingoveroptimizationinofflineRLHFsettings.
5 Experiments
5.1 ExperimentSetup
WeselecttheUltraFeedback[11]datasetasourtrainingset,whichcontains61kpreferencepairsof
single-turnconversations.FortherankerprovidingAIfeedbackduringonlinealignment,wechoose
thesmall-sizedPairRM(0.4B)[25].Allexperimentsareconductedon8xA100GPUs.
Duetotheabsenceofperformantopen-sourceonlinedirectalignmentcodebasesatthetimeofthis
study,wefirstimplementaniterativeversionofDPOasthebaseline,adheringtothesamesteps
as Algorithm 1 but training the LLM with the standard DPO objective. Then we conduct a grid
searchoverhyperparameters,suchasthebatchsize,learningrate,anditerationnumber,toidentify
theoptimalsettingsfortheiterativeDPObaseline.Detailsonthehyperparametersandgridsearch
resultsareprovidedinAppendixC.WefollowthesebestsettingstotrainSELMforafaircomparison.
Inaddition,thetopchoiceforthebasemodelsofSELMareLLMsthatarefinetunedwithRLHF
afterSFT,sincetheyaretypicallymorecapablethanSFT-onlyandpertrainedmodels.Weconsider
twoseriesofLLMs:Zephyr[57]andLlama-3[38],todemonstratetherobustnessofSELM.Since
theofficialZephyr-7B-Î² modelisfinetunedwithDPOonthesameUltraFeedbackdataset,toavoid
overoptimization,wechooseZephyr-7B-SFT1asthebasemodelandperform3iterationsofSELM
afterasingleiterationofstandardDPOtrainingonthefirstportionofthetrainingdata(wereferto
thismodelasZephyr-7B-DPO).ForLlama-3-8B-Instruct2thatisalreadyfinetunedwithRLHF,we
directlyapply3iterationsofSELMtraining.
5.2 ExperimentResults
WefirstreporttheperformanceofSELMandthebaselinesontheinstruction-followingchatbench-
marksAlpacaEval2.0[14]andMT-Bench[71]inTable1.WecanobservethatforAlpacaEval2.0,
SELMsignificantlyboostsZephyr-7B-SFTandLlama-3-8B-Instruct,achievinglength-controlled
(LC)winrateimprovementsof+16.24%and+11.75%,respectively.Thisenhancementresultsin
1https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta
2https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
6AlpacaEval2.0 MT-Bench
Model LCWinRate WinRate Avg.len Avgerage 1stTurn 2ndTurn
Zephyr-7B-SFT 8.01 4.63 916 5.30 5.63 4.97
Zephyr-7B-DPO 15.41 14.44 1752 7.31 7.55 7.07
DPOIter1(Zephyr) 20.53 16.69 1598 7.53 7.81 7.25
DPOIter2(Zephyr) 22.12 19.82 1717 7.55 7.85 7.24
DPOIter3(Zephyr) 22.19(â†‘14.18) 19.88 1717 7.46(â†‘2.16) 7.85 7.06
SELMIter1(Zephyr) 20.52 17.23 1624 7.53 7.74 7.31
SELMIter2(Zephyr) 21.84 18.78 1665 7.61 7.85 7.38
SELMIter3(Zephyr) 24.25(â†‘16.24) 21.05 1694 7.61(â†‘2.31) 7.74 7.49
Llama-3-8B-Instruct 22.92 22.57 1899 7.93 8.47 7.38
DPOIter1(Llama3-It) 30.89 31.60 1979 8.07 8.44 7.70
DPOIter2(Llama3-It) 33.91 32.95 1939 7.99 8.39 7.60
DPOIter3(Llama3-It) 33.17(â†‘10.25) 32.18 1930 8.18(â†‘0.25) 8.60 7.77
SELMIter1(Llama3-It) 31.09 30.90 1956 8.09 8.57 7.61
SELMIter2(Llama3-It) 33.53 32.61 1919 8.18 8.69 7.66
SELMIter3(Llama3-It) 34.67(â†‘11.75) 34.78 1948 8.25(â†‘0.32) 8.53 7.98
SPIN 7.23 6.54 1426 6.54 6.94 6.14
Orca-2.5-SFT 10.76 6.99 1174 6.88 7.72 6.02
DNO(Orca-2.5-SFT) 22.59 24.97 2228 7.48 7.62 7.35
Mistral-7B-Instruct-v0.2 19.39 15.75 1565 7.51 7.78 7.25
SPPO(Mistral-it) 28.53 31.02 2163 7.59 7.84 7.34
Yi-34B-Chat 27.19 21.23 2123 7.90 - -
Llama-3-70B-Instruct 33.17 33.18 1919 9.01 9.21 8.80
GPT-4Turbo(04/09) 55.02 46.12 1802 9.19 9.38 9.00
Table1:ResultsonAlpacaEval2.0andMT-Bench.Namesinsidethebracketsarethebasemodels
thatarealignedbasedupon.Theredarrowsindicatetheincrementordecrementfromthebasemodel.
ComparedtoiterativeDPOandotheronlinealignmentbaselines,SELMgainsmoreimprovements
basedontheweakerZephyr-7B-SFTmodelandachievessuperiorperformancethatiscompetitive
withmuchlargerSOTAmodelswhenfinetunedfromLlama-3-8B-Instruct.
modelsthatarecompetitivewithorevensuperiortomuchlargerLLMs,suchasYi-34B-Chat[65]
andLlama-3-70B-Instruct.Forthemulti-turnMT-Bench,whichexhibitshighervariance,wereport
theaveragescoresofSELMandDPObaselinesacross3runs. WeobservethatSELMimproves
thescoresby+2.31and+0.32,respectively.Furthermore,theproposedmethodself-exploresand
enhances the model monotonically,with consistent performance improvements in each iteration.
Thisvalidatestherobustnessofouralgorithm.Comparedtootheriterativepost-trainingalgorithms,
suchasSPIN[7],DNO[50],andSPPO[61],SELMgainsmoreimprovementsonbothbenchmarks
whenusingtheweakerbasemodel(Zephyr-7B-SFT),andachievesthebestperformancewhenusing
Llama-3-8B-Instructasthebasemodel.
Figure2:Pairwiselength-controlledwinratescomparisonbetweenSELM,iterativeDPO,andbase
models on the AlpacaEval2.0 benchmark. Scores representthe LC win rates ofthe row models
againstthecolumnmodels.ModelspositionedinhigherrowshavehigherLCwinratesagainstthe
basemodelandthusbetterperformance.
7GSM8K HellaSwag ARC TruthfulQA EQ OBQA
Models Average
(8-sCoT) (10-s) (25-s) (0-s) (0-s) (10-s)
Zephyr-7B-SFT 43.8 82.2 57.4 43.6 39.1 35.4 50.3
Zephyr-7B-DPO 47.2 84.5 61.9 45.5 65.2 38.0 57.0
DPOIter1(Zephyr) 45.5 85.2 62.1 52.4 68.4 39.0 58.8
DPOIter2(Zephyr) 44.9 85.4 62.0 53.1 69.3 39.4 59.0
DPOIter3(Zephyr) 43.2 85.2 60.8 52.5 69.1 39.6 58.4
SELMIter1(Zephyr) 46.3 84.8 62.9 52.9 68.8 39.6 59.2
SELMIter2(Zephyr) 46.2 85.4 62.1 53.1 69.3 39.6 59.3
SELMIter3(Zephyr) 43.8 85.4 61.9 52.4 69.9 39.8 58.9
Llama-3-8B-Instruct 76.7 78.6 60.8 51.7 61.8 38.0 61.3
DPOIter1(Llama3-It) 78.5 81.7 63.9 55.5 64.1 42.6 64.4
DPOIter2(Llama3-It) 79.4 81.7 64.4 56.4 64.3 42.6 64.8
DPOIter3(Llama3-It) 80.1 81.7 64.1 56.5 64.1 42.6 64.8
SELMIter1(Llama3-It) 78.7 81.7 64.5 55.4 64.1 42.4 64.5
SELMIter2(Llama3-It) 79.3 81.8 64.7 56.5 64.2 42.6 64.9
SELMIter3(Llama3-It) 80.1 81.8 64.3 56.5 64.2 42.8 65.0
SPIN 44.7 85.9 65.9 55.6 54.4 39.6 57.7
Mistral-7B-Instruct-v0.2 43.4 85.3 63.4 67.5 65.9 41.2 61.1
SPPO(Mistral-it) 42.4 85.6 65.4 70.7 56.5 40.0 60.1
Table2:PerformancecomparisonbetweenSELMandthebaselinesonacademicmulti-choiceQA
benchmarksinstandardzero-shot,few-shot,andCoTsettings.Here,n-sreferston-shot.Theredand
bluetextsrepresentthebestandthesecond-bestresults.
WealsoconductpairwisecomparisonsbetweenSELM,iterativeDPO,andthebasemodelstovalidate
theeffectivenessofourmethod.TheresultsforAlpacaEval2.0areshowninFigure2.Weobserve
thatwiththesamenumberoftrainingiterationsanddata,SELMconsistentlyoutperformstheiterative
DPOcounterpart.Additionally,whenusingZephyr-7B-SFTasthebasemodel,SELMoutperforms
iterativeDPOevenwhenthelatteristrainedwithtwicethedata.
Beyond instruction-following benchmarks,we also evaluate SELM and the baselines on several
academicbenchmarks,includingGSM8K[10],HellaSwag[68],ARCchallenge[9],TruthfulQA[33],
EQ-Bench[45],andOpenBookQA(OBQA)[39].TobetterreflectthecapabilitiesofLLMs,weadopt
varioussettingsforthesebenchmarks,includingzero-shot,few-shot,andfew-shotChain-of-Thought
(CoT)settings.Theaccuracyresultsforthesemultiple-choiceQAbenchmarksareprovidedinTable
2.ItcanbeobservedthatbothourmethodandthebaselinescandegradeaftertheRLHFphaseon
somebenchmarks,whichisknownasthealignmenttax[2;40;30].Nevertheless,ourmethodisstill
abletoimprovethebasemodelsonmostofthebenchmarksandoffersthebestoverallperformance.
WenotethatSELMisoneoftheinstantiationsoftheproposedself-explorationobjectivein(1.1),with
reparameterizedrewardfunctionsandalgorithm-specificdesignsdescribedinSection3.2,suchasthe
datasetpartitionandupdaterule.However,thisobjectiveisnotrestrictedtothecurrentimplementation
andcanalsobedirectlyappliedtoanyotheronlinealignmentframework,withorwithoutaseparate
rewardmodel,regardlessofdifferencesinalgorithmdesigns.Thus,theproposedmethodisorthogonal
toandcanbeintegrateddirectlyintotherecentonlineRLHFworkflows[13;62;23]thatincorporate
additionaldelicatedesignswithcarefullycurateddatasets.
5.3 AblationStudy
Wefirstprovideablationstudiestobetterunderstandtheexplorativeoptimismterm.Webeginby
investigatingtheeffectoftheoptimismcoefficientÎ±.InFigure3(Left),weplottheLCwinratesof
SELMwhenusingZephyr-7B-SFTasthebasemodelfordifferentÎ±intheAlpacaEval2.0benchmark.
WefindthatsettingasmallÎ±,suchas0.0001,leadstoverysimilarbehaviorstotheiterativeDPO
(Î± = 0) baseline,while SELM with a large Î± may become overly optimistic and thus not very
effective.Theseresultsmeetourexpectations,suggestingthatpropervaluesofÎ±areessentialfor
achievingthebesttrade-offbetweenexplorationandexploitation.
Next,westudythedifferenceinrewarddistributionswithvaryingÎ±anditerations.Specifically,we
greedilysamplefromtheLLMusingpromptsfromtheholdouttestset(2kintotal)ofUltraFeedback
andgeneraterewardsfortheseresponseswithPairRM.Wethencalculatethefractionofdatathatlies
ineachpartitionofrewardvalues.TheresultsfordifferentÎ±valuesofSELMIter2(Zephyr)are
8Figure3:AblationontheoptimismcoefficientÎ±andthechangeoftherewarddistribution.Left:The
length-controlledwinratesofSELMwithdifferentÎ±onAlpacaEval2.0.Middle:Comparisonof
rewarddistributionsatiteration2withdifferentÎ±.Right:SELMinitiallyexploresandthenshiftsto
higher-rewardregionsasmoretrainingiterationsareperformed.
showninFigure3(Middle),whichindicatethatincreasingÎ±resultsindistributionsthataremore
concentratedinhigher-rewardregions.
Additionally,Figure3(Right)demonstratesthattherewarddis-
tributionshiftstotheright(higher)asmoretrainingiterations
areperformed.Thisshiftcorrespondstoaninitialexploration
phase,wheretheLLMgeneratesuncertainresponsesofvary-
ingquality,followedbyanexploitationphaseasfeedbackis
incorporatedandmoretrainingdataiscollected.
Wealsoconductablationstudiesontheimplicitrewardcaptured
by the SELM and DPO models. Recall that for both SELM
and DPO, the implicit reward takes the form of r (x,y) =
(cid:98)Î¸
Î²(logÏ€ (y | x)âˆ’logÏ€ (y | x)). We calculate the reward
Î¸ ref
differencer (x,y)âˆ’r (x,y)foreachpromptxinthe
(cid:98)SELM (cid:98)DPO
UltraFeedback holdout test set. Here, we study the implicit
rewardofthegood(chosen)andbad(rejected)responses,so
y =y ory =y .Wethensorttherewarddifferenceandplot
w l Figure4:Differenceofimplicitre-
theresultsforZephyr-basedmodelsafteriteration1inFigure
wardbetweenSELMandDPOon
4. The plot clearly shows that for both chosen and rejected
thechosenandrejectedresponses.
responses,SELMproduceshigherimplicitrewardscompared
SELMassignsahigherimplicitre-
toDPO,aligningwiththeproposedoptimisticallybiasedself-
wardthanDPOforbothresponses.
explorationobjective.
6 Conclusion&FutureWork
Inthispaper,weintroducedanactivepreferenceelicitationmethodfortheonlinealignmentoflarge
languagemodels.Byincorporatinganoptimismtermintothereward-fittingobjective,theproposed
bilevelself-exploringobjectiveeffectivelybalancesbetweenexploitingobserveddataandexploring
potentiallyhigh-rewardregions.UnlikestandardonlineRLHFalgorithmsthatpassivelyexplorethe
responsespacebysamplingfromthetrainingLLM,whosesoleobjectiveismaximizingtheexpected
learnedreward,ourmethodactivelyseeksdiverseandhigh-qualityresponses.Thisself-exploration
mechanismhelpsmitigatetheriskofprematureconvergenceandoverfittingwhentherewardmodel
is onlylocallyaccurate. To optimize this bilevelobjective,we solve the inner-levelproblem and
reparameterizetherewardwiththeLLMpolicy,resultinginasimpleyetnoveliterativealignment
algorithmcalledSelf-ExploringLanguageModels(SELM).ComparedtoDPO,SELMimproves
theexplorationefficiencybyselectivelyfavoringresponseswithhighpotentialrewardsratherthan
indiscriminatelysamplingunseenresponses.
Ourexperiments,conductedwithZephyr-7B-SFTandLlama-3-8B-Instructmodels,demonstrated
the efficacy of SELM. Finetuning on the UltraFeedback dataset and leveraging PairRM for AI
feedback,SELMachievedsubstantialimprovementsinperformanceonAlpacaEval2.0,MT-Bench,
andacademicbenchmarks.TheseresultsunderscoretheabilityofSELMtoenhancethealignment
andcapabilitiesoflargelanguagemodelsbypromotingmorediverseandhigh-qualityresponses.
SincetheproposedtechniqueisorthogonaltotheadoptedonlineRLHFworkflow,itwillbeinteresting
9toapplyourmethodwithinmoresophisticatedalignmentframeworkswithadvanceddesigns,which
wewouldliketoleaveasfuturework.
References
[1] MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,JyotiAneja,AhmedAwadallah,Hany
Awadalla,NguyenBach,AmitBahree,ArashBakhtiari,HarkiratBehl,etal. Phi-3technicalre-
port:Ahighlycapablelanguagemodellocallyonyourphone. arXivpreprintarXiv:2404.14219,
2024.
[2] AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,DeepGanguli,TomHenighan,Andy
Jones,NicholasJoseph,BenMann,NovaDasSarma,etal. Agenerallanguageassistantasa
laboratoryforalignment. arXivpreprintarXiv:2112.00861,2021.
[3] PeterAuer.Usingconfidenceboundsforexploitation-explorationtrade-offs.JournalofMachine
LearningResearch,3(Nov):397â€“422,2002.
[4] MohammadGheshlaghiAzar,MarkRowland,BilalPiot,DanielGuo,DanieleCalandriello,
MichalValko,andReÂ´miMunos. Ageneraltheoreticalparadigmtounderstandlearningfrom
humanpreferences. arXivpreprintarXiv:2310.12036,2023.
[5] YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,Dawn
Drain,StanislavFort,DeepGanguli,TomHenighan,etal. Trainingahelpfulandharmless
assistantwithreinforcementlearningfromhumanfeedback. arXivpreprintarXiv:2204.05862,
2022.
[6] DanieleCalandriello,DanielGuo,RemiMunos,MarkRowland,YunhaoTang,BernardoAvila
Pires,PierreHarveyRichemond,CharlineLeLan,MichalValko,TianqiLiu,etal. Human
alignmentoflargelanguagemodelsthroughonlinepreferenceoptimisation. arXivpreprint
arXiv:2403.08635,2024.
[7] ZixiangChen,YiheDeng,HuizhuoYuan,KaixuanJi,andQuanquanGu. Self-playfine-tuning
convertsweaklanguagemodelstostronglanguagemodels. arXivpreprintarXiv:2401.01335,
2024.
[8] KurtlandChua,RobertoCalandra,RowanMcAllister,andSergeyLevine. Deepreinforcement
learning in a handful of trials using probabilistic dynamics models. Advances in neural
informationprocessingsystems,31,2018.
[9] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,
andOyvindTafjord. Thinkyou have solvedquestion answering? tryarc,the ai2 reasoning
challenge. arXivpreprintarXiv:1803.05457,2018.
[10] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifiersto
solvemathwordproblems. arXivpreprintarXiv:2110.14168,2021.
[11] GanquCui,LifanYuan,NingDing,GuanmingYao,WeiZhu,YuanNi,GuotongXie,Zhiyuan
Liu,andMaosongSun. Ultrafeedback:Boostinglanguagemodelswithhigh-qualityfeedback.
arXivpreprintarXiv:2310.01377,2023.
[12] NingDing,YulinChen,BokaiXu,YujiaQin,ZhiZheng,ShengdingHu,ZhiyuanLiu,Maosong
Sun,andBowenZhou. Enhancingchatlanguagemodelsbyscalinghigh-qualityinstructional
conversations. arXivpreprintarXiv:2305.14233,2023.
[13] HanzeDong,WeiXiong,BoPang,HaoxiangWang,HanZhao,YingboZhou,NanJiang,Doyen
Sahoo,CaimingXiong,andTongZhang. Rlhfworkflow:Fromrewardmodelingtoonlinerlhf.
arXive-prints,pagesarXivâ€“2405,2024.
[14] YannDubois,BalaÂ´zsGalambosi,PercyLiang,andTatsunoriBHashimoto. Length-controlled
alpacaeval:Asimplewaytodebiasautomaticevaluators. arXivpreprintarXiv:2404.04475,
2024.
10[15] VikranthDwaracherla,SeyedMohammadAsghari,BotaoHao,andBenjaminVanRoy.Efficient
explorationforllms. arXivpreprintarXiv:2402.00396,2024.
[16] ArpadEEloandSamSloan. Theratingofchessplayers:Pastandpresent. IshiPressInterna-
tional,1978.
[17] Kawin Ethayarajh,Winnie Xu,Niklas Muennighoff,Dan Jurafsky,andDouwe Kiela. Kto:
Modelalignmentasprospecttheoreticoptimization. arXivpreprintarXiv:2402.01306,2024.
[18] LeoGao,JohnSchulman,andJacobHilton. Scalinglawsforrewardmodeloveroptimization.
InInternationalConferenceonMachineLearning,pages10835â€“10866.PMLR,2023.
[19] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts,
AbhishekSharma,AdityaSiddhant,AlexAhern,MiaosenWang,ChenjieGu,etal. Reinforced
self-training(rest)forlanguagemodeling. arXivpreprintarXiv:2308.08998,2023.
[20] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio CeÂ´sar Teodoro Mendes, Allie Del Giorno,
SivakanthGopi,Mojan Javaheripi,Piero Kauffmann,Gustavo de Rosa,Olli Saarikivi,et al.
Textbooksareallyouneed. arXivpreprintarXiv:2306.11644,2023.
[21] ShangminGuo,BiaoZhang,TianlinLiu,TianqiLiu,MishaKhalman,FelipeLlinares,Alexandre
Rame,ThomasMesnard,YaoZhao,BilalPiot,etal. Directlanguagemodelalignmentfrom
onlineaifeedback. arXivpreprintarXiv:2402.04792,2024.
[22] BradenHancockHoangTran,ChrisGlaze. Snorkel-mistral-pairrm-dpo. 2024.
[23] JianHu,XibinWu,WeixunWang,Xianyu,DehaoZhang,andYuCao.Openrlhf:Aneasy-to-use,
scalableandhigh-performancerlhfframework,2024.
[24] HamishIvison,YizhongWang,ValentinaPyatkin,NathanLambert,MatthewPeters,Pradeep
Dasigi,JoelJang,DavidWadden,NoahASmith,IzBeltagy,etal. Camelsinachangingclimate:
Enhancinglmadaptationwithtulu2. arXivpreprintarXiv:2311.10702,2023.
[25] Dongfu Jiang,Xiang Ren,and Bill Yuchen Lin. Llm-blender: Ensembling large language
modelswithpairwiserankingandgenerativefusion. arXivpreprintarXiv:2306.02561,2023.
[26] ChiJin,ZhuoranYang,ZhaoranWang,andMichaelIJordan. Provablyefficientreinforcement
learningwithlinearfunctionapproximation. InConferenceonlearningtheory,pages2137â€“
2143.PMLR,2020.
[27] MartinJosifoski,MarijaSakota,MaximePeyrard,andRobertWest. Exploitingasymmetry
forsynthetictrainingdatageneration:Synthieandthecaseofinformationextraction. arXiv
preprintarXiv:2303.04132,2023.
[28] Dahyun Kim,Yungi Kim,Wonho Song,Hyeonwoo Kim,Yunsu Kim,Sanghoon Kim,and
ChanjunPark. sdpo:Donâ€™tuseyourdataallatonce. arXivpreprintarXiv:2403.19270,2024.
[29] AndreasKoÂ¨pf,YannicKilcher,DimitrivonRuÂ¨tte,SotirisAnagnostidis,ZhiRuiTam,Keith
Stevens,AbdullahBarhoum,DucNguyen,OliverStanley,RichaÂ´rdNagyfi,etal. Openassistant
conversations-democratizinglargelanguagemodelalignment. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[30] Shengzhi Li, Rongyu Lin, and Shichao Pei. Multi-modal preference alignment remedies
regressionofvisualinstructiontuningonlanguagemodel. arXivpreprintarXiv:2402.10884,
2024.
[31] Xian Li,Ping Yu,Chunting Zhou,Timo Schick,Luke Zettlemoyer,Omer Levy,Jason We-
ston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint
arXiv:2308.06259,2023.
[32] YuanzhiLi,SeÂ´bastienBubeck,RonenEldan,AllieDelGiorno,SuriyaGunasekar,andYinTat
Lee. Textbooksareallyouneedii:phi-1.5technicalreport. arXivpreprintarXiv:2309.05463,
2023.
11[33] Stephanie Lin,JacobHilton,andOwain Evans. Truthfulqa: Measuring howmodels mimic
humanfalsehoods. arXivpreprintarXiv:2109.07958,2021.
[34] RuiboLiu,JerryWei,FangyuLiu,ChengleiSi,YanzheZhang,JinmengRao,StevenZheng,
DaiyiPeng,DiyiYang,DennyZhou,andAndrewM.Dai. Bestpracticesandlessonslearned
onsyntheticdataforlanguagemodels,2024.
[35] ZhihanLiu,MiaoLu,WeiXiong,HanZhong,HaoHu,ShenaoZhang,SiruiZheng,Zhuoran
Yang,and Zhaoran Wang. Maximize to explore: One objective function fusing estimation,
planning,andexploration. AdvancesinNeuralInformationProcessingSystems,36,2024.
[36] ZhihanLiu,MiaoLu,ShenaoZhang,BoyiLiu,HongyiGuo,YingxiangYang,JoseBlanchet,
andZhaoranWang. Provablymitigatingoveroptimizationinrlhf:Yoursftlossisimplicitlyan
adversarialregularizer,2024.
[37] Xiuyuan Lu andBenjamin Van Roy. Ensemble sampling. Advances in neuralinformation
processingsystems,30,2017.
[38] Meta. Introducingmetallama3:Themostcapableopenlyavailablellmtodate. 2024.
[39] TodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal. Canasuitofarmorconduct
electricity?anewdatasetforopenbookquestionanswering. arXivpreprintarXiv:1809.02789,
2018.
[40] MichaelNoukhovitch,SamuelLavoie,FlorianStrub,andAaronCCourville. Languagemodel
alignmentwithelasticreset. AdvancesinNeuralInformationProcessingSystems,36,2024.
[41] IanOsband,DanielRusso,andBenjaminVanRoy. (more)efficientreinforcementlearningvia
posteriorsampling. AdvancesinNeuralInformationProcessingSystems,26,2013.
[42] IanOsband,ZhengWen,SeyedMohammadAsghari,VikranthDwaracherla,MortezaIbrahimi,
XiuyuanLu,andBenjaminVanRoy. Approximatethompsonsamplingviaepistemicneural
networks. InUncertaintyinArtificialIntelligence,pages1586â€“1595.PMLR,2023.
[43] IanOsband,ZhengWen,SeyedMohammadAsghari,VikranthDwaracherla,MortezaIbrahimi,
XiuyuanLu,andBenjaminVanRoy. Epistemicneuralnetworks. AdvancesinNeuralInforma-
tionProcessingSystems,36,2024.
[44] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelsto
followinstructionswithhumanfeedback. Advancesinneuralinformationprocessingsystems,
35:27730â€“27744,2022.
[45] SamuelJPaech. Eq-bench:Anemotionalintelligencebenchmarkforlargelanguagemodels.
arXivpreprintarXiv:2312.06281,2023.
[46] ArkaPal,DeepKarkhanis,SamuelDooley,ManleyRoberts,SiddarthaNaidu,andColinWhite.
Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint
arXiv:2402.13228,2024.
[47] BaolinPeng,ChunyuanLi,PengchengHe,MichelGalley,andJianfengGao. Instructiontuning
withgpt-4. arXivpreprintarXiv:2304.03277,2023.
[48] RafaelRafailov,JoeyHejna,RyanPark,andChelseaFinn. Fromrtoqâˆ—:Yourlanguagemodel
issecretlyaq-function. arXivpreprintarXiv:2404.12358,2024.
[49] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,and
ChelseaFinn. Directpreferenceoptimization:Yourlanguagemodelissecretlyarewardmodel.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[50] CorbyRosset,Ching-AnCheng,ArindamMitra,MichaelSantacroce,AhmedAwadallah,and
Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with
generalpreferences. arXivpreprintarXiv:2404.03715,2024.
12[51] DanielRussoandBenjaminVanRoy.Eluderdimensionandthesamplecomplexityofoptimistic
exploration. AdvancesinNeuralInformationProcessingSystems,26,2013.
[52] MalcolmStrens. Abayesianframeworkforreinforcementlearning. InICML,volume2000,
pages943â€“950,2000.
[53] ZhiqingSun,YikangShen,QinhongZhou,HongxinZhang,ZhenfangChen,DavidCox,Yiming
Yang,andChuangGan. Principle-drivenself-alignmentoflanguagemodelsfromscratchwith
minimalhumansupervision. AdvancesinNeuralInformationProcessingSystems,36,2024.
[54] Yunhao Tang,Daniel Zhaohan Guo,Zeyu Zheng,Daniele Calandriello,Yuan Cao,Eugene
Tarassov,ReÂ´miMunos,BernardoAÂ´vilaPires,MichalValko,YongCheng,etal. Understand-
ing the performance gap between online and offline alignment algorithms. arXiv preprint
arXiv:2405.08448,2024.
[55] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,Percy
Liang,andTatsunoriB.Hashimoto. Stanfordalpaca:Aninstruction-followingllamamodel.
https://github.com/tatsu-lab/stanford_alpaca,2023.
[56] LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,ShengyiHuang,Kashif
Rasul,AlexanderM.Rush,andThomasWolf. Thealignmenthandbook. https://github.
com/huggingface/alignment-handbook,2023.
[57] LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,KashifRasul,Younes
Belkada,ShengyiHuang,LeandrovonWerra,CleÂ´mentineFourrier,NathanHabib,etal. Zephyr:
Directdistillationoflmalignment. arXivpreprintarXiv:2310.16944,2023.
[58] XiyaoWang,JiuhaiChen,ZhaoyangWang,YuhangZhou,YiyangZhou,HuaxiuYao,Tianyi
Zhou,Tom Goldstein,Parminder Bhatia,Furong Huang,and Cao Xiao. Enhancing visual-
languagemodalityalignmentinlargevisionlanguagemodelsviaself-improvement,2024.
[59] YizhongWang,HamishIvison,PradeepDasigi,JackHessel,TusharKhot,KhyathiChandu,
DavidWadden,KelseyMacMillan,NoahASmith,IzBeltagy,etal. Howfarcancamelsgo?
exploringthestateofinstructiontuningonopenresources. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[60] ShengguangWu,KemingLu,BenfengXu,JunyangLin,QiSu,andChangZhou. Self-evolved
diversedatasamplingforefficientinstructiontuning. arXivpreprintarXiv:2311.08182,2023.
[61] YueWu,ZhiqingSun,HuizhuoYuan,KaixuanJi,YimingYang,andQuanquanGu. Self-play
preferenceoptimizationforlanguagemodelalignment. arXivpreprintarXiv:2405.00675,2024.
[62] Wei Xiong,Hanze Dong,Chenlu Ye,Han Zhong,Nan Jiang,andTong Zhang. Gibbs sam-
pling from human feedback: A provable kl-constrained framework forrlhf. arXiv preprint
arXiv:2312.11456,2023.
[63] Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more
cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint
arXiv:2312.16682,2023.
[64] ShushengXu,WeiFu,JiaxuanGao,WenjieYe,WeilinLiu,ZhiyuMei,GuangjuWang,Chao
Yu,andYiWu. Isdposuperiortoppoforllmalignment?acomprehensivestudy. arXivpreprint
arXiv:2404.10719,2024.
[65] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,
Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai.
arXivpreprintarXiv:2403.04652,2024.
[66] WeizheYuan,RichardYuanzhePang,KyunghyunCho,SainbayarSukhbaatar,JingXu,and
JasonWeston. Self-rewardinglanguagemodels. arXivpreprintarXiv:2401.10020,2024.
[67] RichardYuanzhePang,WeizheYuan,KyunghyunCho,HeHe,SainbayarSukhbaatar,andJason
Weston. Iterativereasoningpreferenceoptimization. arXive-prints,pagesarXivâ€“2404,2024.
13[68] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag:Cana
machinereallyfinishyoursentence? arXivpreprintarXiv:1905.07830,2019.
[69] ShenaoZhang. Conservativedualpolicyoptimizationforefficientmodel-basedreinforcement
learning. Advancesinneuralinformationprocessingsystems,35:25450â€“25463,2022.
[70] YaoZhao,RishabhJoshi,TianqiLiu,MishaKhalman,MohammadSaleh,andPeterJLiu. Slic-
hf:Sequencelikelihoodcalibrationwithhumanfeedback. arXivpreprintarXiv:2305.10425,
2023.
[71] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,
ZiLin,ZhuohanLi,DachengLi,EricXing,etal. Judgingllm-as-a-judgewithmt-benchand
chatbotarena. AdvancesinNeuralInformationProcessingSystems,36,2024.
[72] BanghuaZhu,EvanFrick,TianhaoWu,HanlinZhu,andJiantaoJiao. Starling-7b:Improving
llmhelpfulnessandharmlessnesswithrlaif,November2023.
14A DerivationsinSection3.1
We begin by deriving (3.2). The solution for the inner-level optimization problem of (3.1) is as
follows:
(cid:104) (cid:105)
maxF(Ï€;r)=maxE r(x,y)âˆ’r(x,yâ€²) âˆ’Î²D (Ï€||Ï€ )
Ï€ Ï€
xâˆ¼ yD â€²âˆ¼t, Ï€y reâˆ¼ f(Ï€ Â·|( xÂ· )|x) KL ref
=E (cid:104) Î²logE (cid:2) exp(r(x,y)/Î²)(cid:3)(cid:105) âˆ’E (cid:2) r(x,yâ€²)(cid:3) (A.1)
xâˆ¼Dt yâˆ¼Ï€ref(Â·|x) xâˆ¼Dt,yâ€²âˆ¼Ï€ref(Â·|x)
Whentherewardrisreparameterizedbyr (x,y)=Î²(logÏ€ (y |x)âˆ’logÏ€ (y |x)),wehavethat
(cid:98)Î¸ Î¸ ref
thefirsttermin(A.1)is0.Thebilevelobjective(3.1)thenbecomes
maxâˆ’L (r;D )âˆ’Î±E (cid:2) r(x,yâ€²)(cid:3) .
r
lr t xâˆ¼D,yâ€²âˆ¼Ï€ref(Â·|x)
ByreparameterizingtherewardwiththeLLM,weobtainthedesiredresultsin(3.2).
Thenweprovidethederivationof(3.3).Weprimarilyconsiderthegradientofthenewlyincorporated
termE [logÏ€ (y |x)].Specifically,wehave
xâˆ¼D,yâˆ¼Ï€ref(Â·|x) Î¸
âˆ‡ E (cid:2) logÏ€ (y |x)(cid:3) =E (cid:104)(cid:88) Ï€ (y |x)âˆ‡ logÏ€ (y |x)(cid:105)
Î¸ xâˆ¼D,yâˆ¼Ï€ref(Â·|x) Î¸ xâˆ¼D ref Î¸ Î¸
y
(cid:104)Ï€ (y |x) (cid:105)
=E ref âˆ‡ logÏ€ (y |x)
xâˆ¼D,yâˆ¼Ï€Î¸ Ï€ (y |x) Î¸ Î¸
Î¸
=E (cid:104) exp(cid:0) âˆ’r (x.y)/Î²(cid:1) âˆ‡ logÏ€ (y |x)(cid:105) .
xâˆ¼D,yâˆ¼Ï€Î¸ (cid:98)Î¸ Î¸ Î¸
ForthederivationoftheDPOgradientâˆ‡ L (Ï€ ;D ),wereferthereadersto[49].
Î¸ DPO Î¸ t
B ProofofTheorem3.1
Proof. ThesolutiontotheKL-constrainedrewardminimizationobjective(3.4)is
Ï€min(y |x)=Ï€ (y |x)exp(cid:0) âˆ’r (x,y)/Î²(cid:1) /Z(x),
Ï Ï (cid:98)Ï
whereZ(x)=(cid:80) Ï€ (y |x)exp(âˆ’r (x,y)/Î²)=1.ThenwehaveÏ€min(y |x)=Ï€ (y |x),i.e.,
y Ï (cid:98)Ï Ï ref
thereferencepolicyÏ€ achievesthelowestimplicitrewardreparameterizedbyanyÏ.
ref
C ExperimentSetup
Inexperiments,weusetheAlignmentHandbook[56]frameworkasourcodebase.Wefindthebest
hyperparametersettingsbyconductingagridsearchovertheiterationnumber,batchsize,learning
rate,andlabelupdaterulefortheiterativeDPObaseline.TheresultsfortheZephyr-basedmodels
areshowninFigure5.Specifically,wefindthatusingthesameamountofdata,updatingthemodel
toomanyiterationscanleadtoinstability.Sowesettheiterationnumberto3forLlama3-It-based
andZephyr-basedmodels(excludingthefirstiterationofDPOtraining).Besides,weobservethat
choosingdifferentbatchsizeshasalargeeffectonthemodelsâ€™performanceandtheoptimalbatchsize
heavilydependsonthemodelarchitecture.Inexperiments,wesetthebatchsizeto256and128for
theZephyr-basedandLlama3-It-basedmodels,respectively.Forthelearningrate,weconsiderthree
designchoices:cycliclearningratewithconstantcycleamplitude,linearlydecayedcycleamplitude,
anddecayedcycleamplitudeatthelastiteration.Wefindthatadecayingcycleamplitudeperforms
betterthanconstantamplitudesingeneral.Thus,forZephyr-basedmodels,wesetthelearningto
5eâˆ’7forthefirstthreeiterationsand1eâˆ’7forthelastiteration.Ineachiteration,thewarmupratio
is0.1.ForLlama3-It-basedmodels,weusealinearlydecayedlearningratefrom5eâˆ’7to1eâˆ’7
within3iterationswiththesamewarmupratio.Wealsotesttwoupdatewaysforthepreferencedata.
Oneistoranky ,y ,y andkeepthebestandworstresponsesintheupdateddataset,whichisthe
w l ref
settingthatisdescribedinthemainpaper.Theotheristocomparey andy andreplacethechosen
w ref
orrejectedresponsebyy basedonthecomparisonresult.Wefindthattheformerdesignperforms
ref
15Figure5:AblationoftheiterativeDPObaseline.Weconductagridsearchovertheiterationnumber,
batchsize,learningrate,anddesignsofthedatasetupdaterule.
betterthanthelatter.WealsocomparedwithstepwiseDPO[28],whichupdatesthereferencemodel
at each iteration but uses the original dataset instead of the updated one. This demonstrates that
exploringandcollectingnewdataisnecessary.
FortheproposedSELMmethod,wefollowtheabovehyperparametersettingsforafaircomparison.
TheoptimismcoefficientÎ±issearchedover0.005,0.001,0.0005,and0.0001andisselectedbased
on the average external reward on the holdout test set of UltraFeedback. We set Î± = 0.001 for
Zephyr-basedSELMandÎ±=0.0001forLlama3-It-basedSELM.FortrainingSELMbasedonother
models,werecommendsettingÎ±=0.005or0.001asitshowsminimalsensitivitytovariations.
16