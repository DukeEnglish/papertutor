NPGA: Neural Parametric Gaussian Avatars
SIMONGIEBENHAIN,TechnicalUniversityofMunich,Germany
TOBIASKIRSCHSTEIN,TechnicalUniversityofMunich,Germany
MARTINRÃœNZ,Synthesia,Germany
LOURDESAGAPITO,UniversityCollegeLondon,UnitedKingdom
MATTHIASNIESSNER,TechnicalUniversityofMunich,Germany
Fig.1. NPGA:WeutilizetherichexpressionspaceofNeuralParametricHeadModelstocreatehigh-fidelityavatarswithfine-grainedexpressioncontrol.Our
avatarsconsistofadynamicsmoduleandacanonicalGaussianpointcloud,whichisaugmentedwithper-primitivefeaturesthatencodevaluablesemantic
information,asindicatedontheleft.Ontheright,wedemonstrateahighlydetailedcross-reenactmentusingtheinsetimageasadrivingexpression.
Thecreationofhigh-fidelity,digitalversionsofhumanheadsisanimportant In particular, there is a strong motivation to reconstruct digital
steppingstoneintheprocessoffurtherintegratingvirtualcomponents avatarsfromreal-worldcaptures,suchasmulti-viewrecordings,
intooureverydaylives.Constructingsuchavatarsisachallengingresearch to obtain a digital copy of a specific real person. The resulting
problem,duetoahighdemandforphoto-realismandreal-timerendering digitalavatarscanthenbeanimatedandrenderedfromarbitrary
performance.Inthiswork,weproposeNeuralParametricGaussianAvatars viewpointswhileexpectinghighvisualfidelity;e.g.,withrespectto
(NPGA),adata-drivenapproachtocreatehigh-fidelity,controllableavatars
photo-realisticcolorsanddetails,preservationofidentity,andthe
frommulti-viewvideorecordings.Webuildourmethodaround3DGaussian
adoptionofperson-specificmannerisms.Atthesametime,many
splattingforitshighlyefficientrenderingandtoinheritthetopologicalflexi-
avatarapplicationsdemandreal-timerenderingcapabilitieswithout
bilityofpointclouds.Incontrasttopreviouswork,weconditionouravatarsâ€™
dynamicsontherichexpressionspaceofneuralparametricheadmodels compromisingvisualquality.
(NPHM),insteadofmesh-based3DMMs.Tothisend,wedistillthebackward Recentadvancesattheintersectionofcomputergraphicsand
deformationfieldofourunderlyingNPHMintoforwarddeformationswhich vision research have steadily improved methods to digitally re-
arecompatiblewithrasterization-basedrendering.Allremainingfine-scale, construct3Dobjectswithphoto-realisticrenderingquality[Kerbl
expression-dependentdetailsarelearnedfromthemulti-viewvideos.To etal.2023a;Mildenhalletal.2021;MÃ¼lleretal.2022].Inpartic-
increasetherepresentationalcapacityofouravatars,weaugmentthecanon- ular,3DGaussianSplatting(3DGS)[Kerbletal.2023a]hasbeen
icalGaussianpointcloudusingper-primitivelatentfeatureswhichgovern quicklyadoptedinrecentworkondigitalhumans,e.g.[Lietal.2024;
itsdynamicbehavior.Toregularizethisincreaseddynamicexpressivity,we
Zielonkaetal.2023]andvirtualheadavatars,e.g.,[Qianetal.2023;
proposeLaplaciantermsonthelatentfeaturesandpredicteddynamics.We
Xuetal.2024],duetoitsefficientrenderingandphoto-realisticre-
evaluateourmethodonthepublicNeRSembledataset,demonstratingthat
constructions.Atthesametimerecentpubliclyavailablemulti-view
NPGAsignificantlyoutperformsthepreviousstate-of-the-artavatarsonthe
self-reenactmenttaskbyâ‰ˆ2.6PSNR.Furthermore,wedemonstrateaccurate datasets[IÅŸÄ±ketal.2023;Kirschsteinetal.2023b;Panetal.2024;Wuu
animationcapabilitiesfromreal-worldmonocularvideos. etal.2022],offeranevermoreexcitingbasisforavatarresearch.A
centralquestionishowcontrollabilitycanbeachieved.Themost
1 INTRODUCTION prominentapproachforheadsistoutilizea3Dmorphablemodel
(3DMM),whichofferscompactdescriptionsoffacesusingdisentan-
Creatingphoto-realistic3Davatarsisoneofthecorechallenges
gledparametricspacesforidentityandfacialexpressions.When
incomputergraphicsandincludesawiderangeofapplications
utilizingtheexpressionsofanunderlying3DMM,e.g.,[Gafnietal.
suchasmovies,games,AR/VRteleconferencing,andthemetaverse.
2021;Grassaletal.2022;Qianetal.2023;Xuetal.2024;Zielonka
ProjectWebsite:https://simongiebenhain.github.io/NPGA/
4202
yaM
92
]VC.sc[
1v13391.5042:viXra2 â€¢ SimonGiebenhain,TobiasKirschstein,MartinRÃ¼nz,LourdesAgapito,andMatthiasNieÃŸner
etal.2022],theavatarisoptimizedtofollowageneralizedexpres- NeuralRadianceFields[Mildenhalletal.2021]providesuchflexi-
sionspacewhichenablesexpressiontransferoranimationthrough bilityandcanbeextendedtodeformablescenes.Thisextensionis
trackinginmonocularvideos[Thiesetal.2016].While3DMMsoffer eitherachievedbymodelingtemporalchangesviaadeformation
acompactparameterization,theirlinearnatureinherentlylimitsthe fieldthataccompaniesacanonicalframe[Parketal.2021a,b],by
fidelityofrepresentedexpressions.Atthesametime,wearguethat addingtimeasaconditioningvariable[Lietal.2022],ordirectlyde-
theunderlyingexpressionspaceplaysacrucialroleindetermining composingthe4Dscenevolumeintoacomputationallymanageable
thequalityofthecreatedavatars.Itnotonlyinfluencesthecon- representation[Attaletal.2023;Songetal.2023].Theextensive
trollabilityoftheresultingavatarsbutitalsolimitsthesharpness useofMLPsinneuralradiancefieldsentailsahighcomputational
ofdetails.Ifthestreamofinputexpressioncodesisinsufficiently burden,however,andstillprovescostlyafterimprovementssuchas
correlatedwiththeobservedimages,theoptimizationproblemcan hashencodings[MÃ¼lleretal.2022],triplanes[Chanetal.2022]or
becomefundamentallyill-posedandleadtooverfitting. otherlow-rankapproximations[Shaoetal.2023].MVP[Lombardi
Tothisend,weproposeNPGA,anewavatarrepresentationthat etal.2021]usesaCNNforamortizeddecodingofsmallprimitives
leveragesalearneddeformationrepresentationwhileensuringthat thatcanbecomparedtotheGaussiansin3DGS[Kerbletal.2023a],
thepredictedfacialdynamicsstayclosetothepriorofanunderly- but typically at lower sharpness.One approach to extend 3DGS
ingneuralparametricheadmodel(NPHM)[Giebenhainetal.2023, todynamicscenesistooptimizeparameterslikepositionsover
2024].NPHMprovidesouravatarswithmorefine-grainedexpres- time[Luitenetal.2024],resultinginarepresentationthatishardto
sioncontrolcomparedtoclassical,public3DMMs[Lietal.2017; control.Thislimitationcanbeovercomebyanimatingacanonical
Paysanetal.2009],whichwerepreviouslyusedforavatarcreation. 3DGSrepresentationwithadeformationfield[Yangetal.2023],
NPGAconsistsofacanonicalGaussianpointcloud,thatcanbe similartotheNerfies[Parketal.2021a]approach.NPGAadopts
forward-deformedusinganexpressioncodeandrenderedusing thisparadigmtoo.
3DGS,similartopreviouswork[Qianetal.2023;Xuetal.2024;
Zielonkaetal.2023].Asarasterization-basedapproach,3DGScan- 2.2 3DMorphableModels
notbeefficientlycombinedwiththebackward deformationfield
Traditionalmorphablemodels[BlanzandVetter1999;Paysanetal.
ofMonoNPHM.Therefore,weproposeadistillationstrategyto
2009]learnarepresentationofbodygeometryviaPCA.Theyare
invertthedeformationdirectionofMonoNPHMâ€™sexpressionprior
oneoftheprimarytoolstodrivehumananimationandareheavily
usingacycleconsistencyloss.Theresultingforwarddeformation
usedinindustryapplications,makingthemacorebuildingblockfor
fieldbecomescompatiblewiththerasterization-basedrenderingof
workonvirtualavatars.Whilesomemodelsarededicatedtospecific
3DGS.Toincreasetheoveralldynamicexpressivityofouravatars,
regionsliketheface[Paysanetal.2009]andhead[Lietal.2017],
wefurtherproposetoaugmentourcanonicalGaussianwithper-
somevariantsincludetheneck[Zhangetal.2023]oreventheentire
Gaussianlatentfeatures,whichenableourdeformationmodule
body[Pavlakosetal.2019].Morerecently,neuralequivalentsof
tooperateinahigherdimensionalspace,thatcandescribefacial
3DMMs,suchasi3DMM[Yenamandraetal.2021],ImFace[Zheng
dynamicsmoreeffectively.Weshowthatthisaddedexpressivity
et al. 2022a] or NPHM [Giebenhain et al. 2023], have improved
resultsinhigher-fidelityimagesynthesis,butisrequiredtobeappro-
upontheexpressionfidelitycomparedtoclassicalPCA-basedmod-
priatelyregularizedtoachieveartifact-freerenderings.Tothisend,
els.Theyencodeacanonicalrepresentationofthegeometryvia
weformulateLaplaciansmoothnesstermsonthelatentfeatures
asigneddistancefunctions(SDF),whichcanbemappedtoarbi-
andpredicteddynamics,basedonthek-nearestneighborgraph
traryexpressionsviaadeformationfield.NPGAutilizesNPHMas
incanonicalspace.Furthermore,wemodifytheadaptivedensity
itoffersseveralbeneficialcharacteristics:Itmodelsthefacedensely
controlstrategyof3DGSformoredetailedavatarreconstructions.
includingeyes,hair,andteeth,itcaptureslocaldetailswellandit
Tosummarize,ourcontributionsarethefollowing:
disentanglesshapefromexpressions.
â€¢ WeproposeadistillationstrategytoutilizeNPHMâ€™srichex-
pressionpriorformoreexpressiveandcontrollableavatars.
2.3 HumanHeadReconstructionandAnimation
â€¢ Weintroduceper-Gaussianlatentfeaturesthatlifttheinput
domainofourdeformationmoduletoahigher-dimensional Existingapproachesforanimatingavatarsmainlydifferintwofun-
space,yieldingincreaseddynamiccapacity. damentalaspects:first,theutilized3Drepresentationincombina-
â€¢ Ouravatarsoutperformthepreviousstate-of-the-artby2.6 tionwithitsrenderingmechanism,andsecond,howtheexpression
PSNRand0.021SSIMontheself-reenactmenttask.Further- codesaretransferredintoscenedynamics.Existingworkhasex-
more,wedemonstrateaccurateavatarreconstructionfrom plored,amongothers,meshesincombinationwithdeferredneural
monocularRGBsequences. rendering [Grassal et al. 2022; Kim et al. 2018], neural radiance
fields [Athar et al. 2022; Gafni et al. 2021; Zielonka et al. 2022],
and3DCNNsincombinationwithvolumerendering[Caoetal.
2 RELATEDWORK
2022;Lombardietal.2019,2021].Recently,therehasbeenalotof
2.1 DynamicSceneRepresentations
interestinpoint-basedrepresentationsandrendering[Qianetal.
Sincehumansareinherentlydynamic,andsubjecttotopological 2023;Xuetal.2024;Zhengetal.2023],especiallysinceKerbletal.
variationwhenforexampleopeningandclosingthemouth,general [2023a]proposed3DGaussianSplatting,whichwealsoadoptinour
dynamicscenerepresentationsaimtosolveasetofsimilarproblems work,duetoitsefficientrenderingandtopologicalflexibility.While
withtheexemptionofcontrollability.Implicitapproachesbasedon someapproacheschoosetoexplainthefaceâ€™smovementexplicitlyNPGA:NeuralParametricGaussianAvatars â€¢ 3
b) Cycle-Consistency Distillation c) NeuralParametricGaussianAvatars
Time CanonicalGaussians DeformedGaussians Rendering
3DGS
CanonicalSpace
a) NPHM Tracking
: per Gaussianfeatures
d) Dynamics : Gaussiancenter
Fig.2. MethodOverview:Thebasisofouravataroptimizationaremulti-viewvideorecordingsalongsideaMonoNPHMtrackingthereof,see(a).Next,we
extractaforward-deformationpriorFfromMonoNPHMâ€™sbackwarddeformationfieldBusingacycle-consistencyloss,see(b).Ouravatarsconsistofa
canonicalGaussianpointcloud(c),whichiswarpedintoposedspaceusingourdynamicsmoduleD,consistingofthecoarsepre-trainedcomponentFanda
detailnetworkG.WeconditionbothnetworksonperGaussianfeatures,whichdictateeachprimitiveâ€™sbehavior.Afterrenderingtheavatarwith3DGS,we
employascreen-spaceCNNtosuppresssmall-scaleartifacts.
throughtheunderlyingmeshofa3DMM,e.g.,[Atharetal.2022; deformationfield
Qianetal.2023;Zielonkaetal.2022],otherschoosetheopposing ğ‘¥
ğ‘
=B(ğ‘¥ ğ‘; zid,zexp) (2)
extremeofamoredata-drivenapproachbyfreelylearningtheface
thatwarpspointsğ‘¥ inposedspaceintocanonicalspaceğ‘¥ .
ğ‘ ğ‘
movementusinganeuralcomponent,whichisdirectlyconditioned
ontheexpressioncodes[Gafnietal.2021;Lombardietal.2021;Xu 4 METHOD
etal.2024].NPGAadoptsthelatterideabutreplacesthe3DMM
Fig.2showsanoverviewofourproposedrepresentationandmethod-
withaneuralparametricmodel.
ologytobuildourNeuralParametricGaussianAvatars(NPGA).As
describedinSection4.1,ouravatarsarecomposedoftwokeycompo-
3 PRELIMINARIES
nents:acanonicalGaussianpointcloudAğ‘ andadynamicsmodule
3.1 3DGaussianSplatting(3DGS)
DwhichdeformstheGaussianswhenprovidedwithanexpression
3DGSusesapoint-basedscenerepresentation,whereeachpoint code,similartorecentwork[Qianetal.2023;Xuetal.2024].In
representsaGaussianprimitivethatisdescribedbyapositionğœ‡, Section4.2wedescribeourdistillationstrategythatallowsNPGAto
rotationq,scaleS,opacityğ›¼ andsphericalharmonicscoefficients leveragetherichlatentexpressionspaceanddetailedmotionprior
SH.Inthefollowing,weletthefollowingnotation ofMonoNPHM[Giebenhainetal.2024].Givenmulti-viewvideo
recordings alongside tracked MonoNPHM expression codes, we
A={ğœ‡,q,S,ğ›¼,SH}, ğ¼ =3dGS(cid:0)A,ğœ‹ ğ¾,ğ¸(cid:1) (1)
jointlyoptimizeforourcanonicalGaussiansanddynamicsmodule,
denotethesetofattributesAcomposingtheGaussianpointcloud, asdescribedinSection4.3.
anditstile-baseddifferentiablerasterizationintoanimageğ¼ under
thecameraprojectiondescribedbyintrinsicandextrinsicparame- 4.1 NeuralParametricGaussianAvatars
tersğ¾ andğ¸respectively. 4.1.1 CanonicalRepresentation. Comparedtothedefaultscenerep-
resentationof3DGS,outlinedinEq.(1),weaugmentourcanonical
3.2 NeuralParametricHeadModels
Gaussianpointcloud
3DMMsdescribethegeometry(andappearance)offaces(orheads)
usingdisentangledparametricspacesforidentityandexpression
Ağ‘ ={ğœ‡,q,S,ğ›¼,SH,}âˆª{f} (3)
variations.NPHMisaspecialcaseofa3DMM,whichrepresentsa withper-Gaussianfeaturesf âˆˆRğ‘Ã—8.Whilethesefeaturesarestatic
personâ€™sheadgeometryusinganeuralSDFanddeformationfield themselves,theyprovidecrucialsemanticinformationtodescribe
conditionedonidentitylatentcodeszidandexpressioncodeszexp, thedynamicbehavioroftherespectiveprimitives.Insomesense,
respectively.Inparticular,ourworkbuildsontopofMonoNPHM our per-Gaussian features serve a similar purpose as positional
formulation,whichdescribesexpressionsusinganeuralbackward encodings[Mildenhalletal.2021;MÃ¼lleretal.2022],whichare
CCNNNN4 â€¢ SimonGiebenhain,TobiasKirschstein,MartinRÃ¼nz,LourdesAgapito,andMatthiasNieÃŸner
uncorrelatedwithspatialcoordinates,haveinfinitespatialresolution expressioncodeszexp.Note,thatinEq.(8)weomitthedependence
anddonotrequireadditionaldatastructures. onexpressioncodeszexpforclarity.Duringthisstage,thecanonical
spaceisnotyetdiscretizedintoasetofGaussianprimitives.Hence,
4.1.2 DynamicsModule. Wemodelfacialexpressionsusingady-
weutilizeafeaturefield
namicsmodelDwhichisdecomposedintotwoMulti-LayerPer-
ceptrons(MLPs),acoarseprior-basednetworkF andanetwork f(ğ‘¥ ğ‘)=TriPlane(ğ‘¥ ğ‘) (9)
Gresponsibleformodelingallremainingdetails.Ourprior-guided
represented as low-resolution 64x64 triplanes [Chan et al. 2022;
forwarddeformationfield
Pengetal.2020],whichcanbeevaluatedatarbitrarypointsğ‘¥ in
ğ‘
ğ›¿ ğœ‡F =F(ğœ‡,f; zexp) âˆˆR3 (4) canonicalspace.
isoptimizedtoactastheinverseofMonoNPHMâ€™sbackwarddefor- WetrainF once,usingthetrackedsequencesof20peoplein
mationsB,aswedescribelaterinSection4.2.F isacoordinate- theNeRSembledataset[Kirschsteinetal.2023b],andusethesame
basednetworkwhichpredictsoffsetsğ›¿F totheGaussiancentersğœ‡ F forallouravatars.Duringtraining,eachpersonhastheirown
ğœ‡
andisconditionedonspatialcoordinatesğœ‡,featuresf andexpres- TriPlane,whichweregularizeusingatotalvariationloss.Further-
sioncodezexp.Note,thatF actsindependentlyoneachprimitive, more,weregularizethenormofpredictedoffsetsâˆ¥F(ğ‘¥ ğ‘,f(ğ‘¥ ğ‘)âˆ¥2
whichweomitinEq.(4)andbelowforclarity. tobesmall.
TorepresentdynamicsbeyondNPHMâ€™sprior,suchasfine-scaled
4.3 AvatarOptimizationStrategy
expression-dependentwrinkles,andappearancechanges,e.g.due
toambientocclusionsandchangesinbloodflowconcentration,we After obtaining a forward deformation field F using our cycle-
relyonasecondMLP consistencydistillationstrategy,weaimtojointlyoptimizeforthe
ğ›¿ ğ‘G =Gğ‘(ğœ‡,f; zexp) (âˆ€ğ‘âˆˆAğ‘), (5) c ea nn eo rgn yic ta el rp ma .ra Tm oe it ne ir ts iaA lizğ‘ ea tn hd eM caL nP onG ict ao lm Gain ui sm sii az ne sa cp eh no teto rsm ğœ‡et wri ec
whichpredictsoffsetsforallcanonicalGaussianattributesğ‘.Weuse sample30.000pointsuniformlyontheiso-surfaceofthetracked
thesamearchitectureforGasforF,besideshavingmoreoutput MonoNPHMmodel.TheperGaussianfeaturesareinitializedby
channelsduetotheincreasednumberofattributeoffsets.Intotal, queryingTriPlane(ğœ‡)atthesampledGaussiancenters.Allremain-
givenanexpressioncodezexp âˆˆR100weobtaintheGaussianpoint ingattributesareinitializedusingthedefault3DGSprocedure.In
cloudinposedspaceAğ‘ byaddingğ›¿F andğ›¿G totheirrespective practice,weobservedthatkeepingF frozenresultsinsub-optimal
canonicalattributes,whichwedenoteas performance,whichislikelycausedthroughtopologicalissuesdur-
ingdistillationinthemouthregion.Hence,wedecidetofurther
Ağ‘ =D(Ağ‘; zexp). (6)
optimize F alongside G and Ağ‘, however, using a significantly
4.1.3 Screen-SpaceRefinement. Finally,afterrenderingtheposed smallerlearningrateandawarm-upschedulethatencouragesthe
GaussiansAğ‘ usingthedifferentiablerasterizerfromKerbletal. preservationofthedistilledprior.
[2023b],weapplyascreen-spaceCNNnetwork[Xuetal.2023]: Ouroptimizationstrivestominimizethephotometricdataterm
[ğ¼Ë† rgb,ğ¼ h] =3dGS(Ağ‘; ğœ‹ ğ¾,ğ¸), ğ¼Ë† cnn=CNN([ğ¼Ë† rgb,ğ¼ h]), (7) L=âˆ¥ğ¼âˆ’ğ¼Ë† rgbâˆ¥1+ğœ†(cid:16) 1âˆ’SSIM(ğ¼,ğ¼Ë† rgb)(cid:17) +ğœ†(cid:16) 1âˆ’SSIM(ğ¼,ğ¼Ë† cnn)(cid:17) , (10)
wheretheğ¼Ë† denotesrenderedRGBcolors.ğ¼ isalatentimage
rgb â„
usedfortheCNNrefinementmodule,whichisobtainedbyrendering whereğ¼ denotesarandomlysamplegroundtruthimagefromthe
h+ğ›¿G,whereharelatentCNNfeatureswhichweadditionally NeRSembledatasetwithcorrespondingexpressioncodeszexp.
h
includeinAğ‘.Note,thatcomparedto[Xuetal.2024]wedonot
4.3.1 Regularization. Wefindthatregularizingbothourcanonical
performsuper-resolution,butmotivatetheuseoftheCNNthrough
representationAğ‘,aswellasourdynamicsmoduleDiscrucialto
increasedperformanceinourablationstudy.
avoidoverfittingtothetrainingexpressions.ToregularizeNPGA
weutilizeaLaplaciansmoothnesstermbasedontheğ‘˜NNgraphof
4.2 Cycle-ConsistencyDistillation
canonicalGaussiancenters.Tothisendlet
OneofourcoreideasistoleverageMonoNPHMâ€™smotionpriorand
(cid:13) (cid:13)2
e fix ep ldre Bss ,i won his cp hac we a. rH po sw poev ine tr s,s inin toce cait nu ot nil ii cz ae ls sa pab ca ec ,k ww ear cd and ne ofo tr dm ira et ci to ln y Rlap(ğ‘¥)=(cid:13) (cid:13) (cid:13) (cid:13)|N1
ğ‘–|
(cid:169) (cid:173)âˆ‘ï¸ ğ‘¥ ğ‘—(cid:170) (cid:174)âˆ’ğ‘¥ ğ‘–(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:169) (cid:173)ğ‘¥ âˆˆ{f}âˆª (cid:216) ğ›¿ ğ‘G(cid:170) (cid:174), (11)
incorporatethisdeformationpriorinourpipeline.Instead,weneed (cid:13) (cid:171)ğ‘—âˆˆNğ‘—
(cid:172)
(cid:13)2
(cid:171)
ğ‘âˆˆAğ‘
(cid:172)
forwarddeformations,whichwarppointsintoposedspace,suchthat denoteaLaplaciansmoothnessterm,whichweusetoregularize
theycanbedirectlyrasterized.Whileitispossibletonumerically theperGaussianfeaturesf,aswellas,theoffsetpredictionsğ›¿ ğ‘G for
approximatetheinverseofBusingiterativeroot-finding[Chenetal.
allattributesğ‘âˆˆAğ‘.Note,thatwheneverthenumberofcanonical
2023,2021],wesearchforamorecomputationallyefficientmethod.
Gaussianschangesduetodensificationorpruning,werecompute
Instead,weproposetodistillaforwarddeformationnetworkF as
theğ‘˜NNgraph.
theinverseofBusingacycleconsistencyloss
Inadditiontothesesmoothnessterms,weencourageF andG
Lcyc(ğ‘¥ ğ‘)=âˆ¥B(F(ğ‘¥ ğ‘,f(ğ‘¥ ğ‘)))âˆ’ğ‘¥ ğ‘âˆ¥2 2. (8) topredictsmalloffsets
U Bsi fn og raE rq b. it( r8 a) rw ilyec sa an md pi lr ee dct pl oy insu tspe ğ‘¥r ğ‘vi âˆˆse RF 3 iw ni cth ant oh ne ik can lo sw pl ae cd ege ano df Rğ›¿ = ğœ† ğœ‡Fâˆ¥ğ›¿ ğœ‡Fâˆ¥2 2 + ğ‘âˆ‘ï¸ âˆˆAğœ† ğ‘Gâˆ¥ğ›¿ ğ‘Gâˆ’eğ‘âˆ¥2 2, (12)NPGA:NeuralParametricGaussianAvatars â€¢ 5
whereeğ‘denotestheneutralelementforthegroupoperationact- totheFLAME3DMMmodel[Lietal.2017].Therefore,GaussianA-
ingonattributeğ‘.Similarly,weimposeregularizationontheper vatarscanbeextremelyefficientlyanimated,sincethereisnoneed
GaussianattributesR
f
=âˆ¥fâˆ¥2 2toremainsmall,andutilizethescale toevaluateacostlyneuralcomponent.Onthedownside,GaussianA-
regularizationlossof[Saitoetal.2024],whichpunishesscalesğ‘† vatarsarelimitedtothefacialmovementslyinginsidetheFLAME
lyingoutsideofawell-behavedrange. expressionspace.
4.3.2 AdaptiveDensityControl(ADC). Acentralingredienttothe GaussianHeadAvatar (GHA) [Xu et al. 2024]: GHA is another
successof3DGSisitsstrategytoadaptivelyaddandpruneGaussians recent3DGS-basedavatarmethod,whichalsolearnsdeformation
inareaswheretheyareneededorredundant,basedonasetofsimple fieldsfrommulti-viewvideoandiscontrolledthroughtheircustom
yeteffectiveheuristicsthatareperiodicallyinvoked.Therulesof multi-viewBFM[Paysanetal.2009]tracking.WhileGHAalsouses
ADChavebeendesignedwithstaticscenesinmindandwefind perGaussianfeatures,weallowthesefeaturestoinfluencethepre-
thedefaultsettingstobesuboptimalforouravatarcreation.Inthe dictedmovementandallotherattributes,whileGHArestrictsthem
dynamicscenario,therecanbeareasthatremainhiddenforlarge toinfluenceonlydynamicappearancechanges.Anotherdifference
partsofthetrainingsequence,suchasthemouthinterior.Therefore, toourworkisthatweuseADC,whileGHAassumesafixedsetof
weadjusttheADCbyemployingageneralizedmean Gaussians,andGHAperformssuper-resolution.Forcomputational
(cid:32) (cid:33)1/ğ‘’ reasonsweremoveoneupsamplinglayer,resultinginatraining
ğ‘€ ğ‘–ğ‘’ = ğ‘1 âˆ‘ï¸ ğœ tğ‘’ (13) r Ge Hso Alu wtio hn ico hf i1 s0 c2 o4 nx d1 i0 ti2 o4 nf eo dr oG nH oA u. rF tu rr at ch ke er dm Mor oe n, ow Ne Pa Hdd Ma ev xe pr rs ei so sn ioo nf
ğ‘¡âˆˆğ‘‡
toaggregatetheview-spacegradientsğœ
ğ‘¡
oftheğ‘–-thprimitiveof codes,indicatedasGHA NPHM.
allframesğ‘‡ betweeninvocationsoftheADCmechanism.Note,
MixtureofVolumetricPrimitives(MVP)[Lombardietal.2021]:
thatğ‘’ = 1resultsinthedefault3DGSsettings.Byincreasingthe MVPutilizesacombinationofvolumerenderingandahead-geometry
exponentğ‘’theaggregationğ‘€ğ‘’ becomesclosertoamaximumfunc-
ğ‘– awareCNNthatcreatesavolumetricpayloadinanamortizedfash-
tion.Therefore,afewvisibleframescanbesufficientfortheADC
ion.Incontrasttoourmethodandtheotherbaselines,MVPutilizes
totriggerdensification,whichisespeciallyimportantforregions
aVariationalAuto-Encoder(VAE)[KingmaandWelling2014]to
liketheteethandmouthinterior.Wefindthatğ‘’ = 2alreadyre- learnalatentexpressionencodingbasedontheir3DMMtracking.
sultsinanincreasednumberofGaussians,leadingtomoredetailed
Note, however, that we do not provide MVP with view-average
reconstruction,especiallyinthemouthinterior.
texturestoobtainamorecomparableevaluationsetting.
Furthermore,wereplacethehardopacityresetmechanismof
3DGS, which we find to be harmful to our optimization, with a 5.1.2 Metrics. Toevaluatetheself-reenactmenttaskweusethe
softervariantproposedin[BulÃ²etal.2024].Insteadofinfrequently PeakSignal-to-NoiseRatio(PSNR),structuralsimilarityindexmea-
setting the ğ›¼ values to be almost transparent, the opacities get sure(SSIM),andperceptualLPIPS[Zhangetal.2018]metrics.For
reducedfrequentlyforasmallamountonly,i.e.inourexperiments thesakeofcompleteness,wealsoreportnumbersforadynamic
by0.01. novelviewsynthesis(NVS)scenario,wherewecompareallmeth-
ods on the held-out camera view of the training sequences. We
5 RESULTS focusourevaluationonthefacialregion,sinceneckandtorsoare
Forourexperiments,weusethestate-of-the-art,publicmulti-view notaccuratelyexplainedbyNPHMandtheunderlying3DMMsof
videoNeRSembledataset[Kirschsteinetal.2023b],fromwhichwe ourbaselines.Tothisend,weleveragesegmentationmasksfrom
chooseadiversesetofsixsubjectsperformingchallengingfacial Facer[Zhengetal.2022b]tomaskouttheneckandtorsobefore
expressions.Afterprovidingadditionaldetailsontheperformed computingthemetrics.Furthermore,wecomputemetricsataresolu-
experimentsandbaselinemethodsinSections5.1and5.2,wepresent tionof550x802.SincewetrainGHAon1024x1024wedownsample
our main results on the tasks of self- and cross-reenactment in andcropthegeneratedimagesaccordingly.
Sections5.3and5.4,respectively.Finally,inSection5.5weablatea
5.2 ImplementationDetails
seriesofexperimentsvalidatingourproposedmodelcomponents.
Wehighlyencouragethereadertoconsultoursupplementalvideo Hyper-Parameters. ForbothourdeformationnetworksF andG
forcompleteresultsincludingtemporalinformation. weuse6-layerMLPswithahiddendimensionalityof256.Inorder
topreservethepriorthatF obtainedinourdistillationprocedure,
5.1 ExperimentalSetup
wesetitslearningrateto4ğ‘’âˆ’5,whileGisequippedwithamuch
TheNeRSembledatasetprovides16synchronizedandcalibrated higherlearningrateof2ğ‘’âˆ’3.Additionally,wefreezethenetwork
videos, from which we choose 15 cameras for training and the parametersofF forthefirst5.000optimizationsteps.Wedecayboth
frontalcameraforevaluation.Furthermore,wetrainouravatarson learningratestwicebyafactorof2duringthecourseof800.000
allsequences,exceptforthe"FREE"-sequencewhichwekeepasa optimizationsteps.Furthermore,weemployweightdecayonF
held-outevaluationsequencefortheself-reenactmenttask. andG,usingaweightof0.1asanadditionalregularizationmeasure.
WeperformanADCstepevery5.000iterations,andmultiplythe
5.1.1 Baselines.
gradientthresholdforthedensificationbyafactorof2,toaccom-
GaussianAvatars(GA)[Qianetal.2023]: GaussianAvatarsisa modateforthefactthatourlosscombinesthelossesoftheRGB
recentmethodthatcreates3DGS-basedavatars,bybindingthem renderingandCNN-refinedpredictions.6 â€¢ SimonGiebenhain,TobiasKirschstein,MartinRÃ¼nz,LourdesAgapito,andMatthiasNieÃŸner
MVP GaussianAvatars GHA Ours GroundTruth
Fig.3. Self-Reenactment:Qualitativecomparisonofdifferentmethodsontheheld-outsequence.
Runtime. Whilewedonotfocusonefficienttraining,animation, resolutionforanother5hoursoftrainingtime.Furthermore,we
orrenderingofavatars,weacknowledgetheimportanceoffast maskoutthetorso,sinceitisneithercontainedinNPHMâ€™sexpres-
animationandrendering.Inourunoptimizedimplementation,we sionspacenorthefocusofourwork.
canrenderimagesat31framespersecond(FPS)for550x802and
18FPSat1100x1604onanNVIDIARTX3080graphicscard,which
includesdeformation,rendering,andCNN.WhenomittingtheCNN Table1. QuantitativeComparison:Wecompareagainstourbaselines
thespeedincreasesto43and38FPS,respectively.Asacomparison, onself-reenactmentusingaheld-outsequence.Forcompletenesswealso
GHArunsat22FPSat1024x1024onthesamemachine.Wetrainall reportmetricsontheheld-outcameraofthetrainingsequences,denoted
ouravatars,andbaselines,untilconvergence,whichroughlytakes asnovel-viewsynthesis(NVS).
7hoursforGA(onanRTX2080),30hours(onanRTX3090)forGHA
andourmethod,and60hours(onanRTX2080)forMVP. NVS Self-Reenactment
Method
PSNRâ†‘ SSIMâ†‘ LPIPSâ†“ PSNRâ†‘ SSIMâ†‘ LPIPSâ†“
DataPreparation. WeobtainMonoNPHMtrackingsontheNeRSem- MVP 33,42 0,957 0,083 27.19 0.919 0.114
bledatasetusingapurelygeometricconstraintbetweentheMonoN-
GA 32,95 0,956 0,080 27.77 0.926 0.104
PHMâ€™s predicted surface and a point cloud reconstructed using
GHA 33,92 0,953 0,045 26.81 0.914 0.077
COLMAP[SchÃ¶nbergerandFrahm2016].Weutilizethesametrack-
GHA 33,09 0,952 0,049 26.60 0.911 0.078
NPHM
ingalgorithmthathasbeenpreviouslyusedin[Anejaetal.2024;
Ours 37,68 0,973 0,032 30.42 0.935 0.057
Kirschsteinetal.2023a].Fortrainingandquantitativeevaluation,
weusearesolutionof550x802,thesameasweuseforMVPandGA.
Forourqualitativeresults,wefine-tuneouravatarson1100x1604NPGA:NeuralParametricGaussianAvatars â€¢ 7
DrivingExpression MVP GaussianAvatars GHA Ours
Fig.4. Cross-Reenactment:Qualitativecomparisonoftransferringadrivingexpressionfromadifferentidentity(left)toanavatar.
5.3 Self-Reenactment 5.4 Cross-Reenactment
Ourmainevaluationisconcernedwiththeself-reenactmenttask. Anothercrucialtaskiscross-reenactment,wheredrivingexpres-
Forthispurpose,allavatarsaretrainedonasetof21trainingse- sionsfromanotherpersonaretransferredtotheavatar.Sincea
quencesalongsidetheirrespectivetrackingresults.Toevaluatethe ground truth for cross-reenactment does not exist, we only re-
avatars,theyareanimatedusingthetrackedexpressionsfroma port a qualitative comparison, which is presented in Fig. 4. We
held-outtestsequence.Wepresentqualitativeandquantitativere- observethatallmethodssuccessfullydisentangleidentityandex-
sultsinFig.3andTable1,respectively,andrecommendthereaderto pressioninformation,allowingforeffectivecross-reenactment.Our
considerthesupplementalvideofortemporalresults.Ourpredicted avatars,however,preservethemostdetailsfromthedrivingex-
self-reenactmentsportraytheunseenexpressionmoreaccurately pressions.Todemonstratereal-worldapplicability,Fig.6depicts
andcontainsharperdetailsinrelativelystaticareaslikethehair cross-reenactmentanimationsofouravatarsusingmonocularRGB
region.Interestingly,GHA performsslightlyworsethanGHA, videosfromacommoditycameraunderreal-worldcircumstances.
NPHM
indicatingthatMonoNPHMexpressioncodesalonedonotimme- Tothisend,weutilizethemonocularMonoNPHMtrackerproposed
diatelyboostperformance.Instead,wehypothesizethatwithout byGiebenhainetal.[2024].
NPHMâ€™smotionpriorasinitialization,NPHMâ€™slatentexpression
distributionmightprovideamorecomplicatedtrainingsignalcom-
5.5 AblationsStudy
paredtothelinearblendshapesofBFM.
InordertoverifyseveralimportantcomponentsofNPGA,weper-
formablationexperimentsusingthreesubjects.Quantitativeand
qualitativeresultsofourablationscanbefoundinTable2andFig.5,
respectively.First,weruna"vanilla"versionofNPGAthatserves8 â€¢ SimonGiebenhain,TobiasKirschstein,MartinRÃ¼nz,LourdesAgapito,andMatthiasNieÃŸner
Vanilla +p.G.F +Lap.smoothness Ours Ours-ADC GroundTruth
Fig.5. AblationStudy:WithoututilizingperGaussiansfeatures("Vanilla"),theavatarsfailtorepresentfineexpressiondetailsandcomplicatedregions
liketheeyesandbottomteeth.AddingperGaussianfeatures(p.G.F.)resultsinsignificantlysharperreconstructionsbutispronetoartifactsunderextreme
expressions.AddingourLaplacianregularization("+Lap.smoothness")andascreen-spaceCNN("Ours")finallyresolvesallartifacts.Furthermore,"Ours-ADC"
demonstratesthatthedefaultdensificationstrategyinhibitsdetailedreconstructions.
Table2. Ablations:Weperformourablationexperimentsonasubsetof
threesubjects.WereportNovel-ViewSynthesis(NVS)forcompleteness.
NVS Self-Reenactment
Method
PSNRâ†‘ SSIMâ†‘ LPIPSâ†“ PSNRâ†‘ SSIMâ†‘ LPIPSâ†“
Vanilla 35.80 0.965 0.048 30.16 0.927 0.067
+PGF 37.04 0.970 0.037 30.54 0.929 0.059
+Lap.smooth 36.85 0.969 0.038 30.56 0.928 0.059
Ours 37.23 0.972 0.033 30.65 0.933 0.053
Ours-ADC 36.12 0.967 0.045 30,49 0,933 0.070
removedusingourproposedLaplaciansmoothnessterms.Table2in-
dicatesthatusingthisregularizationsignificantlyshrinksthegener-
alizationgapbetweentraining(NVS)andtesting(self-reenactment)
Driving
Expression Cross-Reenactments sequences.Comparedtothismodel,"Ours"alsoincludesaCNN,
whichfurtherboostsmetricsandvisualquality.
Fig.6. Real-WorldApplication:WeutilizethemonocularRGBtracking
fromMonoNPHMtoanimateourhigh-fidelityavatars,demonstratingthe AdaptiveDensityControl. Finally,weshowtheimportanceof
applicabilityofouravatarsoutsideofmulti-viewcapturestudios. using an adopted ADC strategy. Surprisingly, using the default
ADC settings with a densification interval of 5000 steps and an
opacityresetintervalof50.000stepsalmostcompletelydiminishes
theimprovementsofourothercontributions.Whilewedonotclaim
asabaseline.ThisversiondoesnotutilizeperGaussianfeatures, thatusingğ‘’ =2inEq.(13)isnecessaryforgreatperformance,we
Laplacian smoothness terms, and usesğ‘’ = 1 in Eq. (13) for the simplynotethatfindingasettingthatletsenoughGaussiansappear
ADC.Thismodelfailstoproducesharprenderingsforfine-scale isimportant,especiallyforfine-scaledwrinklesandteeth.
details,andareasthatarecomplicatedduetofrequentocclusions
6 LIMITATIONSANDFUTUREWORK
andreflections,likethebottomteethandeyes.
Inourexperiments,weshowthatNPGAcancreatecontrollableand
Per-GaussianFeatures. WhenaddingperGaussianfeaturestothe high-fidelityvirtualheadavatarsfrommulti-viewvideodata.How-
vanillamodel,denotedas"+p.G.F.",theincreasedrepresentational ever,bothcontrollabilityandreconstructionqualityofouravatars
capacityresultsinsharperreconstructions.Atthesametime,we arefundamentallyrestrictedtowhattheunderlying3DMMcan
occasionallyobserveartifactsof"free-floating"primitives,ashigh- explain.Therefore,regionsliketheneck,torso,tongue,andeyeball
lightedinthesecondcolumnofFig.5.Theseartifactscanbelargely rotation, which are not explained by NPHMâ€™s expression codes,NPGA:NeuralParametricGaussianAvatars â€¢ 9
cannotbeanimatedasreliablyormightevenleadtoartifactsdue REFERENCES
tooverfitting.Possiblesolutionsareextensionsoftheunderlying Shivangi Aneja, Justus Thies, Angela Dai, and Matthias NieÃŸner. 2024. Fac-
3DMMtoprovideamorecompletedescriptionofapersonâ€™sstate, eTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models.
arXiv:2312.08459[cs.CV]
e.g.theinclusionoftheneck[Zhangetal.2023]oreventorsoand
ShahRukhAthar,ZexiangXu,KalyanSunkavalli,EliShechtman,andZhixinShu.2022.
completebodies[Pavlakosetal.2019]. RigNeRF:FullyControllableNeural3DPortraits.InProceedingsoftheIEEE/CVF
Furthermore,asadata-drivenapproachtoavatarcreation,our ConferenceonComputerVisionandPatternRecognition(CVPR).20364â€“20373.
BenjaminAttal,Jia-BinHuang,ChristianRichardt,MichaelZollhoefer,JohannesKopf,
methodislimited,tosomedegree,totheavailabletrainingdataper MatthewOâ€™Toole,andChangilKim.2023.HyperReel:High-Fidelity6-DoFVideo
person.Webelievethatrecentlarge-scalemulti-viewvideodataset withRay-ConditionedSampling.InConferenceonComputerVisionandPattern
ofhumanheads[Kirschsteinetal.2023b;Panetal.2024]openup
Recognition(CVPR).
VolkerBlanzandThomasVetter.1999.Amorphablemodelforthesynthesisof3Dfaces.
opportunitiestolearnageneralizedheadmodel,suchas[Caoetal. InProceedingsofthe26thannualconferenceonComputergraphicsandinteractive
2022],withmuchhigherfidelitythanNPHMandotheravailable techniques.187â€“194.
SamuelRotaBulÃ²,LorenzoPorzi,andPeterKontschieder.2024.RevisingDensification
3DMMs,throughtheuseofphotometricoptimizationandefficient
inGaussianSplatting. arXiv:2404.06109[cs.CV]
rendering,like3DGS. ChenCao,TomasSimon,JinKyuKim,GabeSchwartz,MichaelZollhoefer,Shun-Suke
Saito,StephenLombardi,Shih-EnWei,DanielleBelko,Shoou-IYu,YaserSheikh,and
7 CONCLUSION JasonSaragih.2022.AuthenticVolumetricAvatarsfromaPhoneScan.ACMTrans.
Graph.41,4,Article163(jul2022),19pages. https://doi.org/10.1145/3528223.3530143
Inthiswork,wehaveproposedNeuralParametricGaussianAvatars EricR.Chan,ConnorZ.Lin,MatthewA.Chan,KokiNagano,BoxiaoPan,ShaliniDe
Mello,OrazioGallo,LeonidasGuibas,JonathanTremblay,SamehKhamis,Tero
(NPGA),amethodforcreatingaccuratelycontrollableandhigh-
Karras,andGordonWetzstein.2022. EfficientGeometry-aware3DGenerative
fidelityvirtualheadavatars.Akeycomponentofourworkisthe AdversarialNetworks.InCVPR.
usageofMonoNPHMâ€™srichexpressionspaceandmotionprior.To XuChen,TianjianJiang,JieSong,MaxRietmann,AndreasGeiger,MichaelJ.Black,
andOtmarHilliges.2023. Fast-SNARF:AFastDeformerforArticulatedNeural
thisend,weproposeacycle-consistencystrategytodistillafor- Fields.PatternAnalysisandMachineIntelligence(PAMI)(2023).
ward deformation field from MonoNPHM, such that it becomes XuChen,YufengZheng,MichaelJBlack,OtmarHilliges,andAndreasGeiger.2021.
SNARF:DifferentiableForwardSkinningforAnimatingNon-RigidNeuralImplicit
compatiblewith3DGaussianSplatting.Furthermore,wehavein-
Shapes.InInternationalConferenceonComputerVision(ICCV).
troduced per Gaussian features, a simple technique, to help our GuyGafni,JustusThies,MichaelZollhÃ¶fer,andMatthiasNieÃŸner.2021.DynamicNeural
deformationmodulesexplainthedynamicbehaviorofouravatarâ€™s RadianceFieldsforMonocular4DFacialAvatarReconstruction.InProceedingsofthe
canonicalGaussianprimitives.Weproposedaneffectiveregular-
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).8649â€“8658.
SimonGiebenhain,TobiasKirschstein,MarkosGeorgopoulos,MartinRÃ¼nz,Lourdes
izationstrategyandadjustedtheadaptivedensitycontrolstrategy, Agapito,andMatthiasNieÃŸner.2023.LearningNeuralParametricHeadModels.In
bothofwhichareimportantforidealavatarquality.Inourexper- Proc.IEEEConf.onComputerVisionandPatternRecognition(CVPR).
SimonGiebenhain,TobiasKirschstein,MarkosGeorgopoulos,MartinRÃ¼nz,Lourdes
iments,wesignificantlyoutperformthepreviousstate-of-the-art Agapito,andMatthiasNieÃŸner.2024. MonoNPHM:DynamicHeadReconstruc-
avatarsonself-reenactment.Finally,weshowedtheapplicability tionfromMonocularVideos.InProc.IEEEConf.onComputerVisionandPattern
ofouravatarsbeyondacontrolledmulti-viewset-upbyanimating
Recognition(CVPR).
Philip-WilliamGrassal,MaltePrinzler,TitusLeistner,CarstenRother,MatthiasNieÃŸner,
themfrommonocularRGBvideotrackings. andJustusThies.2022.NeuralheadavatarsfrommonocularRGBvideos.InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
Acknowledgements. ThisworkwasfundedbySynthesiaandsup- 18653â€“18664.
portedbytheERCStartingGrantScan2CAD(804724),theGerman MustafaIÅŸÄ±k,MartinRÃ¼nz,MarkosGeorgopoulos,TarasKhakhulin,JonathanStarck,
LourdesAgapito,andMatthiasNieÃŸner.2023.Humanrf:High-fidelityneuralradi-
ResearchFoundation(DFG)ResearchUnitâ€œLearningandSimulation
ancefieldsforhumansinmotion.arXivpreprintarXiv:2305.06356(2023).
inVisualComputingâ€.Wewouldliketothankourresearchassistant BernhardKerbl,GeorgiosKopanas,ThomasLeimkÃ¼hler,andGeorgeDrettakis.2023a.
MohakMansharamani,andAngelaDaiforthevideovoice-over. 3DGaussianSplattingforReal-TimeRadianceFieldRendering.ACMTransactionson
Graphics42,4(July2023).https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/
BernhardKerbl,GeorgiosKopanas,ThomasLeimkÃ¼hler,andGeorgeDrettakis.2023b.
3DGaussianSplattingforReal-TimeRadianceFieldRendering.ACMTransactionson
Graphics42,4(July2023).https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/
HyeongwooKim,PabloGarrido,AyushTewari,WeipengXu,JustusThies,Matthias
NieÃŸner,PatrickPÃ©rez,ChristianRichardt,MichaelZollÃ¶fer,andChristianTheobalt.
2018.DeepVideoPortraits.ACMTransactionsonGraphics(TOG)37,4(2018),163.
Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational
Bayes. In 2nd International Conference on Learning Representations, ICLR
2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.
arXiv:http://arxiv.org/abs/1312.6114v10[stat.ML]
TobiasKirschstein,SimonGiebenhain,andMatthiasNieÃŸner.2023a. DiffusionA-
vatars:DeferredDiffusionforHigh-fidelity3DHeadAvatars. arXivpreprint
arXiv:2311.18635(2023).
TobiasKirschstein,ShenhanQian,SimonGiebenhain,TimWalter,andMatthiasNieÃŸner.
2023b. NeRSemble:Multi-ViewRadianceFieldReconstructionofHumanHeads.
ACMTrans.Graph.42,4,Article161(jul2023),14pages. https://doi.org/10.1145/
3592455
TianyeLi,TimoBolkart,Michael.J.Black,HaoLi,andJavierRomero.2017.Learninga
modeloffacialshapeandexpressionfrom4Dscans.ACMTransactionsonGraphics,
(Proc.SIGGRAPHAsia)36,6(2017),194:1â€“194:17. https://doi.org/10.1145/3130800.
3130813
TianyeLi,MiraSlavcheva,MichaelZollhÃ¶fer,SimonGreen,ChristophLassner,Changil
Kim,TannerSchmidt,StevenLovegrove,MichaelGoesele,RichardNewcombe,
andZhaoyangLv.2022. Neural3DVideoSynthesisFromMulti-ViewVideo.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR).5521â€“5531.10 â€¢ SimonGiebenhain,TobiasKirschstein,MartinRÃ¼nz,LourdesAgapito,andMatthiasNieÃŸner
ZheLi,ZerongZheng,LizhenWang,andYebinLiu.2024. AnimatableGaussians: YuelangXu,HongwenZhang,LizhenWang,XiaochenZhao,HuangHan,QiGuojun,
LearningPose-dependentGaussianMapsforHigh-fidelityHumanAvatarModeling. andYebinLiu.2023.LatentAvatar:LearningLatentExpressionCodeforExpressive
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition NeuralHeadAvatar.InACMSIGGRAPH2023ConferenceProceedings.
(CVPR). ZiyiYang,XinyuGao,WenZhou,ShaohuiJiao,YuqingZhang,andXiaogangJin.2023.
StephenLombardi,TomasSimon,JasonSaragih,GabrielSchwartz,AndreasLehrmann, Deformable3DGaussiansforHigh-FidelityMonocularDynamicSceneReconstruc-
andYaserSheikh.2019.NeuralVolumes:LearningDynamicRenderableVolumes tion.arXivpreprintarXiv:2309.13101(2023).
fromImages.ACMTrans.Graph.38,4,Article65(July2019),14pages. TarunYenamandra,AyushTewari,FlorianBernard,Hans-PeterSeidel,MohamedEl-
StephenLombardi,TomasSimon,GabrielSchwartz,MichaelZollhoefer,YaserSheikh, gharib,DanielCremers,andChristianTheobalt.2021.i3DMM:DeepImplicit3D
andJasonSaragih.2021. MixtureofVolumetricPrimitivesforEfficientNeural MorphableModelofHumanHeads.InProceedingsoftheIEEE/CVFConferenceon
Rendering.ACMTrans.Graph.40,4,Article59(jul2021),13pages. https://doi.org/ ComputerVisionandPatternRecognition.12803â€“12813.
10.1145/3450626.3459863 LongwenZhang,ZijunZhao,XinzhouCong,QixuanZhang,ShuqiGu,YuchongGao,
JonathonLuiten,GeorgiosKopanas,BastianLeibe,andDevaRamanan.2024.Dynamic RuiZheng,WeiYang,LanXu,andJingyiYu.2023.HACK:LearningaParametric
3DGaussians:TrackingbyPersistentDynamicViewSynthesis.In3DV. HeadandNeckModelforHigh-FidelityAnimation.ACMTrans.Graph.42,4,Article
BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRa- 41(jul2023),20pages. https://doi.org/10.1145/3592093
mamoorthi,andRenNg.2021.Nerf:Representingscenesasneuralradiancefields RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.2018.
forviewsynthesis.Commun.ACM65,1(2021),99â€“106. TheUnreasonableEffectivenessofDeepFeaturesasaPerceptualMetric.InCVPR.
ThomasMÃ¼ller,AlexEvans,ChristophSchied,andAlexanderKeller.2022. Instant MingwuZheng,HongyuYang,DiHuang,andLimingChen.2022a.ImFace:ANonlinear
NeuralGraphicsPrimitiveswithaMultiresolutionHashEncoding. ACMTrans. 3DMorphableFaceModelwithImplicitNeuralRepresentations.InProceedingsof
Graph.41,4,Article102(July2022),15pages. https://doi.org/10.1145/3528223. theIEEE/CVFConferenceonComputerVisionandPatternRecognition.
3530127 YinglinZheng,HaoYang,TingZhang,JianminBao,DongdongChen,YangyuHuang,Lu
DongweiPan,LongZhuo,JingtanPiao,HuiwenLuo,WeiCheng,YuxinWang,Siming Yuan,DongChen,MingZeng,andFangWen.2022b.Generalfacialrepresentation
Fan,ShengqiLiu,LeiYang,BoDai,ZiweiLiu,ChenChangeLoy,ChenQian,Wayne learninginavisual-linguisticmanner.InProceedingsoftheIEEE/CVFConferenceon
Wu,DahuaLin,andKwan-YeeLin.2024. RenderMe-360:ALargeDigitalAsset ComputerVisionandPatternRecognition.18697â€“18709.
LibraryandBenchmarksTowardsHigh-fidelityHeadAvatars.AdvancesinNeural YufengZheng,WangYifan,GordonWetzstein,MichaelJ.Black,andOtmarHilliges.
InformationProcessingSystems36(2024). 2023.PointAvatar:DeformablePoint-basedHeadAvatarsfromVideos.InProceedings
KeunhongPark,UtkarshSinha,JonathanTBarron,SofienBouaziz,DanBGoldman, oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).
StevenMSeitz,andRicardoMartin-Brualla.2021a. Nerfies:Deformableneural Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael ZollhÃ¶fer, Jus-
radiancefields.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer tus Thies, and Javier Romero. 2023. Drivable 3D Gaussian Avatars. (2023).
Vision.5865â€“5874. arXiv:2311.08581[cs.CV]
KeunhongPark,UtkarshSinha,PeterHedman,JonathanT.Barron,SofienBouaziz, WojciechZielonka,TimoBolkart,andJustusThies.2022. InstantVolumetricHead
DanBGoldman,RicardoMartin-Brualla,andStevenM.Seitz.2021b.HyperNeRF: Avatars. arXiv:2211.12499[cs.CV]
AHigher-DimensionalRepresentationforTopologicallyVaryingNeuralRadiance
Fields.ACMTrans.Graph.40,6,Article238(dec2021).
GeorgiosPavlakos,VasileiosChoutas,NimaGhorbani,TimoBolkart,AhmedA.A.
Osman,DimitriosTzionas,andMichaelJ.Black.2019.ExpressiveBodyCapture:3D
Hands,Face,andBodyfromaSingleImage.InProceedingsIEEEConf.onComputer
VisionandPatternRecognition(CVPR).10975â€“10985.
PascalPaysan,ReinhardKnothe,BrianAmberg,SamiRomdhani,andThomasVetter.
2009.A3Dfacemodelforposeandilluminationinvariantfacerecognition.In2009
sixthIEEEinternationalconferenceonadvancedvideoandsignalbasedsurveillance.
Ieee,296â€“301.
SongyouPeng,MichaelNiemeyer,LarsMescheder,MarcPollefeys,andAndreasGeiger.
2020. ConvolutionalOccupancyNetworks.InEuropeanConferenceonComputer
Vision(ECCV).
ShenhanQian,TobiasKirschstein,LiamSchoneveld,DavideDavoli,SimonGiebenhain,
andMatthiasNieÃŸner.2023. GaussianAvatars:PhotorealisticHeadAvatarswith
Rigged3DGaussians.arXivpreprintarXiv:2312.02069(2023).
ShunsukeSaito,GabrielSchwartz,TomasSimon,JunxuanLi,andGiljooNam.2024.
RelightableGaussianCodecAvatars.InCVPR.
JohannesLutzSchÃ¶nbergerandJan-MichaelFrahm.2016. Structure-from-Motion
Revisited.InConferenceonComputerVisionandPatternRecognition(CVPR).
RuizhiShao,ZerongZheng,HanzhangTu,BoningLiu,HongwenZhang,andYebin
Liu.2023.Tensor4D:EfficientNeural4DDecompositionforHigh-fidelityDynamic
ReconstructionandRendering.InProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition.
LiangchenSong,AnpeiChen,ZhongLi,ZhangChen,LeleChen,JunsongYuan,YiXu,
andAndreasGeiger.2023.NeRFPlayer:AStreamableDynamicSceneRepresentation
withDecomposedNeuralRadianceFields.IEEETransactionsonVisualizationand
ComputerGraphics29,5(2023),2732â€“2742. https://doi.org/10.1109/TVCG.2023.
3247082
J.Thies,M.ZollhÃ¶fer,M.Stamminger,C.Theobalt,andM.NieÃŸner.2016.Face2Face:
Real-timeFaceCaptureandReenactmentofRGBVideos.InProc.ComputerVision
andPatternRecognition(CVPR),IEEE.
Cheng-hsinWuu,NingyuanZheng,ScottArdisson,RohanBali,DanielleBelko,Eric
Brockmeyer,LucasEvans,TimothyGodisart,HyowonHa,XuhuaHuang,Alexan-
derHypes,TaylorKoska,StevenKrenn,StephenLombardi,XiaominLuo,Kevyn
McPhail,LauraMillerschoen,MichalPerdoch,MarkPitts,AlexanderRichard,Ja-
sonSaragih,JunkoSaragih,TakaakiShiratori,TomasSimon,MattStewart,Au-
tumnTrimble,XinshuoWeng,DavidWhitewolf,ChengleiWu,Shoou-IYu,and
YaserSheikh.2022. Multiface:ADatasetforNeuralFaceRendering.InarXiv.
https://doi.org/10.48550/ARXIV.2207.11243
YuelangXu,BenwangChen,ZheLi,HongwenZhang,LizhenWang,ZerongZheng,
andYebinLiu.2024. GaussianHeadAvatar:UltraHigh-fidelityHeadAvatarvia
DynamicGaussians.InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR).