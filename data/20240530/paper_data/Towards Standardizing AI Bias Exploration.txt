Towards Standardizing AI Bias Exploration
EmmanouilKrasanakis*, SymeonPapadopoulos
CentreforResearch&TechnologyHellas,6thkmCharilaou-Thermi,Thessaloniki,Greece,57001
Abstract
CreatingfairAIsystemsisacomplexproblemthatinvolvestheassessmentofcontext-dependentbias
concerns.Existingresearchandprogramminglibrariesexpressspecificconcernsasmeasuresofbias
thattheyaimtoconstrainormitigate. Inpractice,oneshouldexploreawidevarietyof(sometimes
incompatible)measuresbeforedecidingwhichoneswarrantcorrectiveaction,buttheirnarrowscope
meansthatmostnewsituationscanonlybeexaminedafterdevisingnewmeasures.Inthiswork,we
presentamathematicalframeworkthatdistilsliteraturemeasuresofbiasintobuildingblocks,hereby
facilitating new combinations to cover a wide range of fairness concerns, such as classification or
recommendationdifferencesacrossmultiplemulti-valuesensitiveattributes(e.g.,manygendersand
races,andtheirintersections).Weshowhowthisframeworkgeneralizesexistingconceptsandpresent
frequentlyusedblocks.Weprovideanopen-sourceimplementationofourframeworkasaPythonlibrary,
calledFairBench,thatfacilitatessystematicandextensibleexplorationofpotentialbiasconcerns.
Keywords
Measuringbias,Auditingtools,Algorithmicframeworks,Multidimensionalbias
1. Introduction
ArtificialIntelligence(AI)systemsseewidespreadadoptionacrossmanyapplicationsthataffect
peopleâ€™slives. Sincetheytendtopickupandexacerbatereal-worldbiasesordiscrimination,
aswellasspuriouscorrelationsbetweenpredictedvaluesandsensitiveattributes(e.g.,gender,
race,age,financialstatus),makingthemfairisasubjectofintensiveresearchandregulatory
efforts. These include quantification of bias concerns through appropriate measures so that
unfairbehaviorcanbedetectedandcorrected. Tothisend,severalmeasuresofbiashavebeen
proposedinresearchpapersandimplementedasreusablecomponentswithinprogramming
librariesortoolkits(Section2). Recognizingthatbiasand,moregenerally,fairnessdependson
thesocialcontextandtheparticularsettingsinwhichAIsystemsaredeployed,eachcreated
measureislimitedtoassessingadifferenttypeofconcern. Intheend,researchanddevelopment
focusesonmitigatingorconstrainingmeasureswhenthoserevealfairnessissues,butnoton
howasystematicexplorationofmanymeasurescouldbecarriedouttoperformfairnessaudits.
Therefore, and despite the obvious value of presenting reusable algorithmic solutions to
specificproblems,thereisaneedformethodsthatcriticallyexaminereal-worldsystemsacross
awiderangeofconcernsandnotjustafewofthem. Themainbarrierinpursuingsuchmethods
is that measures of bias are designed in a monolithic manner and rarely consider how they
AIMMES2024WorkshoponAIbias:Measurements,Mitigation,ExplanationStrategies|co-locatedwithEUFairness
ClusterConference2024,Amsterdam,Netherlands
*Correspondingauthor.
$maniospas@iti.gr(E.Krasanakis);papadop@iti.gr(S.Papadopoulos)
(cid:26)0000-0002-3947-222X(E.Krasanakis);0000-0002-5441-7341(S.Papadopoulos)
Â©2024Copyrightforthispaperbyitsauthors.UsepermittedunderCreativeCommonsLicenseAttribution4.0International(CCBY4.0).
4202
yaM
92
]GL.sc[
1v22091.5042:viXracouldbegeneralizedorportedtodifferentsettings. Forexample,differentialfairness[1]was
onlyrecentlyproposedasameansofgeneralizingdisparateimpactassessmenttointersectional
fairness,whichrecognizesthecumulativeeffectofsensitiveattributes(e.g.,multiplegenders
andraces),despitedisparateimpactmeasuresbeingaroundfordecades[2].
Inthiswork,weassistsystematicexplorationoffairnessconcernsbydecomposingmeasures
ofbiasintosimplebuildingblocks. Thesecanberecombinedtocreatenewmeasurescovering
awiderangeofcontextsandconcerns. Forexample,ablockthataggregatesclassificationbias
inmultidimensionalsettings(e.g.,withmultipleracesandgenders)canbecombinedwitha
blockthatanothermeasureusestoassessrecommendationbiasbetweenonlytwogroupsof
people(e.g.,onlywhitesvsnon-whites)tocreateanewmeasurethatassessesmultidimensional
recommendation bias. We implement the proposed framework in a Python library, called
FairBench, that standardizes how measures of bias are defined by combining interoperable
blocksofeachkind. Thelibraryâ€™sfunctionalinterfacesetsupafixedrepresentationofexisting
andnewmeasuresofbias,andcancreatebiasreportswhiletracingprospectivefairnessissues
totherawquantitiescomputedoverpredictiveoutcomes. Ourcontributionisthreefold:
a) Wepresentamathematicalframeworkthatsystematicallycombinesbuildingblocksto
constructmanyexistingandnewmeasuresofbias.
b) Weexpressseveralbuildingblocksofliteraturemeasuresofbiaswithinthisframework.
c) WeintroducetheFairBenchPythonlibrarythatimplementstheaboveblocksandcombina-
tionmechanismstocomputemeasuresofbiasinawiderangeofcomputingenvironments.
This paper is structured as follows. After this sectionâ€™s introduction, Section 2 presents
theoretical background and related work. Section 3 describes our proposed mathematical
framework. Section 4 extracts several bias building blocks from the AI fairness literature.
Section5introducestheprogramminginterfaceprovidedbyourFairBenchPythonlibraryto
explorebias. Finally,Section6summarizesourworkandpointstofuturedirections.
2. Background and Related Work
2.1. FairAI
The problem of creating fair AI systems is the subject has attracted attention as part of the
broaderthemeofTrustworthyAIinworldwideregulationandethicalguidelines,liketheEUâ€™s
AssessmentListforTrustworthyAI[3],andtheNISTâ€™sAIRiskManagementFramework[4].
Evaluatingsystemfairnessisacomplexproblemthatdependsonthecontextbeingexamined
andthesystemsbeingcreated. Apartofansweringthisproblemconsistsofmathematicalor
algorithmicexplorationthatenablesautomatedsystemassessmentandoversightwithpractices
likeTrustAIOps[5],whichmonitortheevolutionofsystembiasinthedeploymentcontext.
Mathematicaldefinitionsoffairnessareoftencategorizedintothefollowingtypes[6,7,8,9]:
i) Groupfairnessfocusesonequaltreatmentbetweenpopulationgroupsorsubgroups. This
isthesubjectofmostresearchandcoveredextensivelyinthenextsubsection.
ii) Individual fairness [10] focuses on the fair treatment of individuals, for example by
obtaining similar predictions for those with similar features. This can be modelled as
groupfairnesstoo,byconsideringeverypersontobelongtotheirowngroup.iii) Counterfactualfairness[11,12]learnscausalmodelsofpredictivemechanismsthataccount
for discrimination and then makes predictions in a would-be fair reality. Although
originally coined as a variation of individual fairness, recent understanding [13] also
suggeststhatcounterfactualfairnessisakindofgroupfairness.
Making(e.g.,training)AItobefairtypicallyinvolvesmeasuresofbiasthatquantifynumerical
deviation from exact definitions of fairness. Measures are either minimized or subjected to
constraints[14],butidentifyingwhichareimportantisnotonlyamatterofapplyingscientific
principles. Inparticular,itismathematicallyimpossibletosimultaneouslysatisfyallconceivable
definitions of fairness [15, 16], which means that systems should only address the fairness
concernsthatmattertohumans(e.g.,stakeholdersorpolicymakers)intheparticularsituation.
Therearealsotrade-offsbetweensatisfyingfairnessandmaintainingpredictiveperformance.
Inthisworkweproposethatmanytypesofbiasshouldbemonitoredsimultaneouslytoreveal
concerningtrendsthatrequirefurthercontext-dependenthumanevaluation.
Inpractice,AIsystemsemployfairnesslibrariesortoolkitstocomputepopularmeasuresof
biasandeitherconstrainpredictivetaskstoachievetheacceptedmeasurevaluesorcarryout
trade-offsofthelatterwithpredictiveperformance. Somepopularsoftwareprojectsforfair
AIareAIF360[17],FairLearn[18],Whatif[19],andAequitas[20]. Eachofthesefocuseson
supportingdifferentcomputationalbackendsanddatastructures. AIF360andFairLearnare
programminglibrariesandprovidebiasmitigationalgorithms. WhatIfandAequitasfocuson
assessmentoffairness,andspecializeinassessingcounterfactualandgroupfairnessrespectively.
All libraries and toolkits implement ad-hoc measures of bias that capture fairness concerns
well-studied in the literature, where the latter typically account for very restricted types of
evaluationthatdonotmodelcomplexconcerns.
2.2. Measuresofbias
We now present common measures of bias, starting from ones that quantify group fairness
concerns for classifiers [21]. We organize people with the same sensitive attribute values
intopopulationgroups. Allpresentedmeasuresassumevaluesintherange[0,1]and,when
necessary,weshowthecomplementsofmeasuresoffairnesswithrespectto1toletthemassess
biasinstead,i.e.,avalueof0indicatesperfectfairness. Forexample,belowweuse1âˆ’pruleas
ameasureofbias,insteadofprule,whichisameasureoffairness.
Measuresofclassificationbias. Themeasuresof1âˆ’prule[2]andCalders-Verwerdisparity
cv[22]quantifydisparateimpactconcerns,i.e.,predictionrateinequalitybetweentwogroupsof
samplesğ’®andğ’®â€² = ğ’® âˆ–ğ’®,wherethelattercomplementstheformerwithinthepopulationğ’® .
ğ‘ğ‘™ğ‘™ ğ‘ğ‘™ğ‘™
Thismodelsonebinarysensitiveattributeindicatingmembershiptothegroup. Mathematically:
{ï¸ }ï¸
prule = min P(ğ‘(ğ‘¥)=1|ğ‘¥âˆˆğ’®) , P(ğ‘(ğ‘¥)=1|ğ‘¥âˆˆğ’®â€²) , cv = |P(ğ‘(ğ‘¥) = 1|ğ‘¥ âˆˆ ğ’®)âˆ’P(ğ‘(ğ‘¥) = 1|ğ‘¥ âˆˆ ğ’®â€²)|
P(ğ‘(ğ‘¥)=1|ğ‘¥âˆˆğ’®â€²) P(ğ‘(ğ‘¥)=1|ğ‘¥âˆˆğ’®)
whereP(Â·|Â·)denotesconditionalprobabilityandğ‘(ğ‘¥) âˆˆ {0,1}isthebinaryoutcomeofclassi-
fyingdatasampleğ‘¥. Disparateimpactiseliminatedwhen1âˆ’prule = cv = 0. Anotherconcern
isdisparatemistreatment[23],whichcapturesmisclassificationdifferencesbetweengroupsper:
|Î”ğ‘š| = |ğ‘š(ğ’®)âˆ’ğ‘š(ğ’®â€²)|where ğ‘š(Â·) denotes a misclassification measure over groups of samples, such as their false
positive rate (fpr) or their false negative rate (fnr). The same formula can express cv or ac-
commodate error measures for other predictive tasks, and therefore constitutes a building
blockofgeneralizedfairnessevaluationframeworks[24](alsoseeSubsection4.3). Anexam-
pleofdifference-basedbiasforprobabilisticmeasuresofpredictiveperformanceisequalized
opportunitydifference[25,26]:
|Î”ğ‘’ğ‘œ| = |P(ğ‘(ğ‘¥) = 1|ğ‘¥ âˆˆ ğ’®,ğ‘Œ(ğ‘¥) = ğ‘¦)âˆ’P(ğ‘(ğ‘¥) = 1|ğ‘¥ âˆˆ ğ’®â€²,ğ‘Œ(ğ‘¥) = ğ‘¦)|
whereğ‘¦ âˆˆ {0,1}andğ‘Œ(ğ‘¥)isthetruetest/validationdatalabelcorrespondingtosampleğ‘¥.
Measuresofscoringandrecommendationbias. Thesameunderlyingprinciplescanbe
portedtootherpredictivetasks[5,14]suchasrecommendationandscoring.Inrecommendation,
thebasemeasureğ‘š(Â·)in|Î”ğ‘š|maybereplacedbysomeformofexposureofitemstogroup
membersorthefractionoftop-ğ‘˜ recommendationsthataremembersofthegroup. Inscoring
tasks, ğ‘š(Â·) may represent the fraction of score mass concentrated on groups, which can be
comparedtoadesiredfractionorbetweengroupstosatisfyconcernssimilartodisparateimpact
[27]. Aggregatestatistics,liketheareaundertheroc(auc)orscoremeans,mayalsobecompared
betweengroups[28]. Adefinitionthatweuselaterforreceiveroperatingcharacteristic(roc)
curvesofgroupsğ’® accountsforallpairsoffalsepositiverate(fpr)andtruepositiverate(tpr)
ğ‘–
valuesobtainedforthegroupsoverdifferentthresholdsğœƒ ofwhetherscoresareinterpretedas
positivepredictionsornot(weusethemapletarrowtoexpressrocsasmapsfromfprtotpr):
roc(ğ’® ) = {fpr(ğ’® ,ğœƒ) â†¦â†’ tpr(ğ’® ,ğœƒ) : ğœƒ âˆˆ (âˆ’âˆ,âˆ)} (1)
ğ‘– ğ‘– ğ‘–
Morefine-grainedapproachessummarizethedifferencesbetweencurvesordistributions(a
mathematicalexpressionforthisappearsinSubsection4.3).Forexample,viablemeasuresofbias
aretheabsolutebetweennessareabetweenroccurves(abroca)[29]andtheKullback-Leibler
divergencebetweendistributionsofsystemoutcomesforeachgroup[30].
2.3. Frameworkstoassessmultidimensionalbias
Fairness concerns may span several sensitive groups and their intersections [31]. This set-
ting is known as multidimensional fairness. For example, there may be multiple protected
demographics(e.g.,genders,races,andtheirintersections). Twoframeworksforexpressing
several measures of bias in the multidimensional setting are a) what we later call groups vs
all comparison[24]andb)worst-casebias[30]. Inthiswork,wegeneralizetheseframeworks
underamoreexpressiveone. Ouranalysisalsoaccountsforc)individualfairnessbytreatingit
underamultidimensionalframeworkwhereeachindividualisaseparategroupofoneelement.
Groupsvsall. Thissetsupthefollowinggenericframeworkformeasuresofbias:
ğ¹ ğ‘ğ‘–ğ‘ğ‘ (S) = âŠ™ ğ’®ğ‘–âˆˆSğ¹ ğ‘ğ‘ğ‘ ğ‘’(ğ’® ğ‘–) (2)
whereSareallgroups,ğ¹ (ğ’® )isameasureofbiasforonegroupğ’® âˆˆ S(thiscorresponds
ğ‘ğ‘ğ‘ ğ‘’ ğ‘– ğ‘–
to treating that group as having only one binary sensitive attribute), and âŠ™ is a reductionmechanism,suchastheminimumormaximum. Groupsmaybeoverlappingwhenexamining
each sensitive attribute value independently without considering their intersections [32, 33,
34,35]. Ithasbeenproposedthatgroupintersectionsmayalsoreplacethegroupswithinthis
analysis,thereforecreatingsubgroupsintheirplace[1,31,30].
In Equation 2, the base measure of bias compares one group against the total population.
Examplemeasuresthatemploythisschemearestatisticalparitysubgroupfairness(spsf)and
falsepositivesubgroupfairness(fpsf)[31]. Forthese,eachsubgroupiscomparedtothetotal
populationtocreatemultidimensionalvariationsofÎ”ğ‘’ğ‘œandÎ”fprrespectively. Bothemploya
reductionmechanismthatweighscomparisonsbygroupsize.
Worst-casebias. Thisframeworkstartsfrompairwisegrouporsubgroupcomparisonsand
keepstheworstcase. Wecomputeworst-casebias(wcb)foranyprobabilisticmeasureğ¹(ğ’® ),
ğ‘–
suchasofaccurateorerroneouspredictionsovergroupsorsubgroupsğ’® âˆˆ S,per:
ğ‘–
wcb(S) = 1âˆ’ min ğ¹(ğ’® âŸ©) (3)
(ğ’®ğ‘–,ğ’®ğ‘—)âˆˆS2 ğ¹(ğ’® |)
Forexample,differentialfairness[1]statesthatpruleshouldresideintherange[ğ‘’âˆ’ğœ–,ğ‘’ğœ–]for
someğœ– â‰¥ 0whenitcomparesallsubgroupspairwise.Aviablemeasureofbiasforthisdefinition,
whichwecalldifferentialbias(db),isgivenforsubgroupsSwhenğ¹(ğ’® ) = P(ğ‘(ğ‘¥) = 1|ğ‘¥ âˆˆ ğ’® ),
ğ‘– ğ‘–
forwhichdifferentialfairnessisequivalenttosatisfyingwcb(S) â‰¤ 1âˆ’ğ‘’âˆ’ğœ–.
Individual fairness Multidimensional discrimination can also model individual fairness
[10,36]bysettingeachindividualasaseparategroup. Inthiscase,andgiventhatindividual
fairnessisformallydefinedtosatisfyğ‘‘ (ğ‘(ğ‘¥ ),ğ‘(ğ‘¥ )) â‰¤ ğ‘‘ (ğ‘¥ ,ğ‘¥ )acrossallpairsofindividuals
ğ‘¦ ğ‘– ğ‘— ğ‘¥ ğ‘– ğ‘—
(ğ‘¥ ,ğ‘¥ ),whereğ‘isapredictivemechanismandğ‘‘ ,ğ‘‘ aresomedistancemetrics,aviablemeasure
ğ‘– ğ‘— ğ‘¦ ğ‘¥
ofindividualbias(ib)thatsatisfiesindividualfairnesswhenib â‰¤ 1is:
ib = max ğ‘‘ğ‘¦(ğ‘(ğ‘¥ğ‘–),ğ‘(ğ‘¥ğ‘—)) (4)
ğ‘¥ğ‘–,ğ‘¥ğ‘— ğ‘‘ğ‘¥(ğ‘¥ğ‘–,ğ‘¥ğ‘—)
3. A General Bias Measurement Framework
Inthissectionwepresentaframeworkfordefiningmeasuresofbiasfromfundamentalbuilding
blocks. Thisletsusdecomposeexistingmeasuresintoblocks,introducevariations,andcreate
novelcombinations. Werecognizefourtypesofblocks,whichcorrespondtosuccessivecompu-
tationalsteps: a)selectingwhichpairsofpopulation(sub)groupstocompare,b)basemeasures
thatassesssomesystempropertyoneachgroup,c)comparisonsbetweengroupassessments,
andd)reductionsthatconvertmultiplepairwisecomparisonstoonevalue.
Mathematically,weannotatebasemeasuresasğ¹(ğ’® )andcomputethemovergroupsofdata
ğ‘–
samples ğ’® âŠ† ğ’® of some population ğ’® . We consider any information necessary for this
ğ‘– ğ‘ğ‘™ğ‘™ ğ‘ğ‘™ğ‘™
computation(e.g.,predictedandgroundtruthlabels)directlyretrievablefromtherespective
samples. We also annotate the set of all these groups as S = {ğ’® : ğ‘– = 0,1,...}. The base
ğ‘–
measurescouldbeerrorratesofpredictionsorunsupervisedstatistics,likepositiverates. Then,
weconsiderasetofsubgrouppairstocompareğ¶(S,ğ’® ğ‘ğ‘™ğ‘™) âˆˆ (ï¸€ 2ğ’® ğ‘ğ‘™ğ‘™)ï¸€2,where2ğ’® ğ‘ğ‘™ğ‘™ denotesthepowerset. Thecreatedsetofsubgrouppairscomprisesanynecessarydetectedcomparisons
betweengroupsorsubgroupswithinğ’® ,eventhosethatdonotdirectlyresideinSbutmay
ğ‘ğ‘™ğ‘™
be derived from the latter, such as subgroups. We will make pairwise comparisons ğ‘“ =
ğ‘–ğ‘—
ğ¹(ğ’® )âŠ˜ğ¹(ğ’® )between(sub)groups(ğ’® ,ğ’® ) âˆˆ ğ¶(S,ğ’® ). Finally,thereductionmechanism
ğ‘– ğ‘— ğ‘– ğ‘— ğ‘ğ‘™ğ‘™
willbeexpressedasâŠ™ ğ‘“ acrossallpairs(ğ‘† ,ğ‘† )andoutcomesofcomparingthemğ‘“ .
(ğ‘†ğ‘–,ğ‘†ğ‘—) ğ‘–ğ‘— ğ‘– ğ‘— ğ‘–ğ‘—
Puttingtheaboveinoneformulayieldsourframeworkforexpressingmeasuresofbias:
ğ¹ = âŠ™ (ï¸€ ğ¹(ğ’® )âŠ˜ğ¹(ğ’® ))ï¸€ (5)
ğ‘ğ‘–ğ‘ğ‘  (ğ’®ğ‘–,ğ’®ğ‘—)âˆˆğ¶(S,ğ’® ğ‘ğ‘™ğ‘™) ğ‘– ğ‘—
This is a direct generalization of Equation 2 in that it supports more varied strategies for
expressinggroupcomparisons. Atthesametime,itsystematizesthecomparisonmechanisms
betweengroupsofpeoplethroughtheoperationâŠ˜. Ourframeworkisalsoageneralizationof
Equation3inthat,inadditiontothebasemeasure(whichweallowtoalsobenon-probabilistic,
ifsodesired),itprovidesflexibilityinthereductionandcomparisonmechanismsbeyondthe
choicesofminimumandfractionalcomparisonthatcharacterizetheworstcase.
Eachtupleofchoices(ğ¹,ğ¶,âŠ˜,âŠ™)createsadifferentmeasureofbias. Inthenextsection,we
delveintohoweachbuildingblocktypeamongthemembersofthistuplecouldvarybetween
contexts that comprise different fairness concerns and predictive tasks. For every building
block,asystematicexplorationwouldconsiderallpossiblecombinationswithothersofdifferent
kindssothateventuallywenotonlyreconstructtheoriginalmeasuresofbias,butalsocreate
variationsthataddressdifferentsettings. Table1exemplifieshowthemeasuresdiscussedin
Subsection2.2arisefromspecificchoices. Creatingafulltaxonomyofexistingmeasuresunder
ourframeworkisnotthisworkâ€™sobjective(weonlyaimtodemonstrateitswideexpressive
breadth)andislefttofuturework.
Measure ğ¹(ğ’® ) ğ¶(S,ğ’® ) ğ‘“ âŠ˜ğ‘“ âŠ™
âŸ© ğ‘ğ‘™ğ‘™ ğ‘– ğ‘—
Examplemeasures
|Î”fpr| [23] P(ğ‘(ğ‘¥)=1|ğ‘¥âˆˆğ’® ,ğ‘Œ(ğ‘¥)=0) {ğ’®,ğ’® âˆ–ğ’®}2forS={ğ’®} ğ‘“ âˆ’ğ‘“ max
ğ‘– ğ‘ğ‘™ğ‘™ ğ‘– ğ‘—
|Î”fnr| [23] P(ğ‘(ğ‘¥)=0|ğ‘¥âˆˆğ’® ,ğ‘Œ(ğ‘¥)=1) {ğ’®,ğ’® âˆ–ğ’®}2forS={ğ’®} ğ‘“ âˆ’ğ‘“ max
ğ‘– ğ‘ğ‘™ğ‘™ ğ‘– ğ‘—
|Î”ğ‘’ğ‘œ| [25] P(ğ‘(ğ‘¥)=1|ğ‘¥âˆˆğ’® ) {ğ’®,ğ’® âˆ–ğ’®}2forS={ğ’®} ğ‘“ âˆ’ğ‘“ max
ğ‘– ğ‘ğ‘™ğ‘™ ğ‘– ğ‘—
spsf [31] P(ğ‘(ğ‘¥)=1|ğ‘¥âˆˆğ’® ) SÃ—{ğ’® }âˆª{ğ’® }Ã—S |ğ‘“ âˆ’ğ‘“ | wmean
ğ‘– ğ‘ğ‘™ğ‘™ ğ‘ğ‘™ğ‘™ ğ‘– ğ‘—
fpsf [31] P(ğ‘(ğ‘¥)=1|ğ‘¥âˆˆğ’® ,ğ‘Œ(ğ‘¥)=0) SÃ—{ğ’® }âˆª{ğ’® }Ã—S |ğ‘“ âˆ’ğ‘“ | wmean
ğ‘– ğ‘ğ‘™ğ‘™ ğ‘ğ‘™ğ‘™ ğ‘– ğ‘—
cv [22] P(ğ‘(ğ‘¥)=1|ğ‘¥âˆˆğ’® ) {ğ’®,ğ’® âˆ–ğ’®}2 ğ‘“ âˆ’ğ‘“ max
ğ‘– ğ‘ğ‘™ğ‘™ ğ‘– ğ‘—
1âˆ’prule [2] P(ğ‘(ğ‘¥)=1|ğ‘¥âˆˆğ’® ) {ğ’®,ğ’® âˆ–ğ’®}2 1âˆ’ğ‘“ /ğ‘“ max
ğ‘– ğ‘ğ‘™ğ‘™ ğ‘– ğ‘—
db [1] P(ğ‘(ğ‘¥)=1|ğ‘¥âˆˆğ’® ) S2 1âˆ’ğ‘“ /ğ‘“ max
ğ‘– ğ‘– ğ‘—
Multidimentionalbiasframeworks
ğ¹ [24] any* SÃ—{ğ’® }âˆª{ğ’® }Ã—S any any
ğ‘ğ‘–ğ‘ğ‘  ğ‘ğ‘™ğ‘™ ğ‘ğ‘™ğ‘™
wcb [30] any S2 1âˆ’ğ‘“ /ğ‘“ max
ğ‘– ğ‘—
ib [10] ğ‘(ğ‘¥) {{ğ‘¥}:ğ‘¥âˆˆğ’® } ğ‘‘ (ğ‘“ ,ğ‘“ )/ğ‘‘ (ğ’® ,ğ’® ) max
ğ‘ğ‘™ğ‘™ ğ‘¦ ğ‘– ğ‘— ğ‘¥ ğ‘– ğ‘—
Table1
ExpressingthemeasuresofclassificationbiasofSubsection2.2andmultidimensionalbiasframeworks
ofSubsection2.3underourbiasmeasuredefinitionframework. Interpretationofreductionscanbe
foundinSubsection4.4.Frameworksallowforanyblocksofcertainkinds.*ğ¹ doesnotexplicitly
ğ‘ğ‘–ğ‘ğ‘ 
acknowledgebasemeasurebuildingblocks.4. Bias building blocks
Here we present building blocks that occur from decomposing popular measures of bias of
Sections2.2and2.3. Combinationsunderourframeworkcancreatenewmeasures.
4.1. Selectinggroupsorsubgroupstocompare
Thegroupselectionmechanismswestudyincludea)baseselection,b)accountingforindividual
fairness[10],andc)accountingforintersectionality. Moreblockscanbecreatedinthefuture.
Baseselection. Severalfairnessmeasuresaredefinedbycomparingeachpopulationgroup
orsubgroupwiththetotalpopulationğ’® . Giventhatthepairwisecomparisonmechanism
ğ‘ğ‘™ğ‘™
isnotsymmetric, i.e., itmayholdthatğ‘“ âŠ˜ğ‘“ Ì¸= ğ‘“ âŠ˜ğ‘“ , weaccountforboth(ğ’® ,ğ’® )and
ğ‘– ğ‘— ğ‘— ğ‘– ğ‘– ğ‘ğ‘™ğ‘™
(ğ’® ,ğ’® )forsubgroupsğ’® âˆˆ S. Mathematically,wedefineagroupvsanysampleselectionper:
ğ‘ğ‘™ğ‘™ ğ‘– ğ‘–
vsany(S,ğ’® ) = SÃ—{ğ’® }âˆª{ğ’® }Ã—S (6)
ğ‘ğ‘™ğ‘™ ğ‘ğ‘™ğ‘™ ğ‘ğ‘™ğ‘™
Alternatively,onemayconsiderallgrouppairs(weletreductionremoveanyself-comparisons):
pairs(S,ğ’® ) = S2 (7)
ğ‘ğ‘™ğ‘™
Forone-dimensionalfairness,i.e.,onlyonesensitiveattributewithonlytwopotentialvalues,
such as indicating which samples belong to a protected group of people and which do not,
eachgroupğ’® istypicallycomparedtoitscomplement withinthewholepopulationğ’® âˆ–ğ’® .
ğ‘ğ‘™ğ‘™ ğ‘–
Inthiscase,S = {ğ’®}andwewritethepairwiseselectionbetweenitanditscomplementper
compl(S,ğ’® ) = {ğ’®,ğ’® âˆ–ğ’®}2. Onegeneralizationtomultidimensionalsettingswouldbeto
ğ‘ğ‘™ğ‘™ ğ‘ğ‘™ğ‘™
considerğ’® âˆ–ğ’® asagroupandrecreateEquation7. However,anequallyvalidgeneralization
ğ‘ğ‘™ğ‘™
ittocomparegroupstotheircomplementsonly:
â‹ƒï¸
compl(S,ğ’® ) = {ğ’® ,ğ’® âˆ–ğ’® }2 (8)
ğ‘ğ‘™ğ‘™ ğ‘– ğ‘ğ‘™ğ‘™ ğ‘–
ğ’®ğ‘–âˆˆS
Accountingforindividualfairness. Individualfairnessdisregardsthenotionofpopulation
groupsorsubgroupsandinsteadcomparesallindividualspairwise. Thisisaninstanceofthe
pairsmechanism,whereeachindividualisassignedtotheirowngroup. Mathematically,this
wouldoccurifwespecifiedsubgroupsS = {{ğ‘¥} : ğ‘¥ âˆˆ ğ’® }. Weherebyextendallcomparison
ğ‘ğ‘™ğ‘™
mechanismsğ¶ tofollowthisschemawhenanemptysetofgroupsâˆ…isprovided1 per:
ğ¶(âˆ…,ğ’® ) = ğ¶({{ğ‘¥} : ğ‘¥ âˆˆ ğ’® },ğ’® ) (9)
ğ‘ğ‘™ğ‘™ ğ‘ğ‘™ğ‘™ ğ‘ğ‘™ğ‘™
Accounting for intersectionality. Measures of bias like db, spsf, and spsf account for
subgroupsthatformintersectionsofgroupstoo. Tomodelthisscenario,wedefineintersectional
comparisonsbetweengroupswhileignoringemptyintersections(thisconvenientlyignores
intersectionsbetweenmutuallyexclusivesensitiveattributevalues)per:
ğ¶ (S,ğ’® ) = ğ¶({ğ’® = ğ’® âˆ©ğ’® âˆ©Â·Â·Â· : ğ’® âˆˆ S,ğ’® =Ì¸ âˆ…},ğ’® ) (10)
ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ ğ‘’ğ‘ğ‘¡ ğ‘ğ‘™ğ‘™ âŸ©âˆ âŸ©âˆˆ âŸ©
â€–
ğ‘ğ‘™ğ‘™
1Theemptysetindicatesnoknowledgeofagroup,inwhichcaseourfallbackbecomesindividualfairness.4.2. Basemeasures
Ourframeworkacceptsanymeasureofpredictiveperformance. Inthissubsection,wedemon-
stratetwoconcepts: a)computingaggregateassessmentstofeedas-areincomparisonmecha-
nisms,andb)trackingunderlyingmetadatathatcouldbeusedbyotherbuildingblockswithout
altering Equation 5. Base measures can also be ported from fairness definitions that do not
specifymeasuresofbias,suchasequalgroupbenefit[37]. Ourframeworkcansupplytherestof
buildingblockstotryallavailableoptionsatdefiningbias,suchasallcomparisonmechanisms.
Aggregateassessment. Here,welistbasemeasuresofaggregatesystemassessmentoften
usedtodefinebias. Probabilisticdefinitionsforthemcanbefoundindomainliterature. When
working with classifiers, popular base measures are positive rates (pr), fpr, and fnr. Other
misclassification measures are also used in expressions of disparate mistreatment [23]. In
multiclass settings, each class can be treated as a different group of data samples with its
own target labels. Bias has been assessed for the top-ğ‘˜ individual/item recommendations
bycomparingthebasemeasuresofhitrateandskew[38,39]acrossgroupswhenchecking
for proportional representation. Recommendation correctness measures may be similarly
accommodated,suchastheclick-throughrate[39]. Recommendationisassessedacrossseveral
valuesofğ‘˜,whichleadstomeasuresthatkeeptrackofmetadata(seebelow).
Keeping track of metadata. An emerging concern is that measures of bias affirm the
presence of certain biases but not their absence. To verify the latter, one needs to look at
nuances of underlying distributions [30]. For example, it has been argued that statistical
testsshouldreplacethepruletoshownon-discriminationincertainlegalsettings[40]. Thus,
our framework retains building block metadata to be used in subsequent computations. To
understandthis,thinkoftheabrocameasure[29]thatcomparestheareabetweentheroccurves
ofEquation1insteadofcomparingaucsthroughdifferencesorfractions.
We model curve comparisons by tracking computational metadata and retrieving them
throughanappropriatepredicatecurve(Â·)definedalongsidebasemeasures. Forexample,aucâ€™s
definitionasabuildingblockshouldincludethestatement:
curve(auc(ğ’® )) = roc(ğ’® )
ğ‘– ğ‘–
We do not directly return the metadata (e.g., the roc curves) in order to compare aggregate
assessmentswithasmanymechanismsaspossible. Forinstance,alsocomputing|auc(ğ’® )âˆ’
ğ‘–
auc(ğ’® )|andcheckingdifferenceswithabrocamaycreatehigh-levelinsights(e.g.,ifthesetwo
ğ‘—
quantitiesareequalbetweentwogroups,thecorrespondingroccurvesdonotintersect).
We now present a second base measure used in recommendation systems to assess top
predictionsandcontainsacurve,namelyaveragerepresentation(ar)withinthetoppredictions:
ğ¾
âˆ‘ï¸
ar = 1 P(ğ‘¥ âˆˆ ğ’® |ğ‘¥ âˆˆ top )
ğ¾ ğ¾ ğ‘– ğ‘˜
ğ‘˜=1
Tokeepworkingwithintegratablecurveswithcontinuoushorizontalaxes,weuseDiracâ€™sdeltafunctionğ›¿(ğ‘˜) = 0forğ‘˜ Ì¸= 0andâˆ«ï¸€âˆ ğ›¿(ğ‘˜)dğ‘˜ = 1todefinetheinjection:
âˆ’âˆ
curve(ar ) = {ï¸€ ğ‘˜ â†¦â†’ P(ğ‘¥ âˆˆ ğ’® |ğ‘¥ âˆˆ top )ğ›¿(0)ifğ‘˜ âˆˆ {1,2,...,ğ¾},0otherwise : ğ‘˜ âˆˆ (âˆ’âˆ,âˆ)}ï¸€
ğ¾ ğ‘– ğ‘˜
4.3. Comparisonmechanisms
We examine three types of comparison to serve as building blocks in our framework: a) no
comparison,b)numericerrors(differencesandfractions),andc)curvecomparisons. Thelast
typeencompassesmechanismsthatcomputeweighedcurvedifferences. Othermechanisms
canbereadilycreated,andwedemonstratethresholdedvariations.
Nocomparison. Thisisavalidoptionwhenaimingtoremoveconfoundingbias,i.e.,bias
thatdirectlyleadstoerroneoussystempredictions. Forexample,theworstmeasureassessment
acrossgroupsorsubgroupsismaximizedifallgroupsexhibithighpredictiveperformance[41].
Inthiscase,ourframeworkjustassessesAIperformanceforallgroups. Mathematically:
ğ‘“ âŠ˜ ğ‘“ = ğ‘“
ğ‘– ğ‘›ğ‘œğ‘›ğ‘’ ğ‘— ğ‘–
Numericerrors. Thesecomputesomedeviationbetweenğ‘“ andğ‘“ ,suchasabsoluteerror
ğ‘– ğ‘—
(abs),relativeerror(rel),andtheirsignedvalues(sabsandsrelrespectively):
ğ‘“ âŠ˜ ğ‘“ = |ğ‘“ âˆ’ğ‘“ |, ğ‘“ âŠ˜ ğ‘“ = |1âˆ’ğ‘“ /ğ‘“ |, ğ‘“ âŠ˜ ğ‘“ = ğ‘“ âˆ’ğ‘“ , ğ‘“ âŠ˜ ğ‘“ = 1âˆ’ğ‘“ /ğ‘“
ğ‘– ğ‘ğ‘ğ‘  ğ‘— ğ‘– ğ‘— ğ‘– ğ‘Ÿğ‘’ğ‘™ ğ‘— ğ‘– ğ‘— ğ‘– ğ‘ ğ‘ğ‘ğ‘  ğ‘— ğ‘– ğ‘— ğ‘– ğ‘ ğ‘Ÿğ‘’ğ‘™ ğ‘— ğ‘– ğ‘—
Curvecomparisons. Letusconsiderthatthecurvepredicatecanextractfrombasemeasure
assessmentcurvesğ‘ğ‘Ÿğ‘£ = curve(ğ‘“ ),ğ‘ğ‘Ÿğ‘£ = curve(ğ‘“ )withaknowncommondomainğ’Ÿ. We
ğ‘– ğ‘– ğ‘— ğ‘—
cancomparethesegivenaweightingmechanismğ¼(ğ‘˜)forthedomainandcomparisonâŠ˜ as:
ğ‘ğ‘Ÿğ‘£
âˆ«ï¸
ğ‘“ âŠ˜ğ‘“ = 1 ğ¼(ğ‘˜)Â·(ï¸€ ğ‘ğ‘Ÿğ‘£ (ğ‘˜)âŠ˜ ğ‘ğ‘Ÿğ‘£ (ğ‘˜))ï¸€dğ‘˜
ğ‘– ğ‘— âˆ«ï¸€ ğ¼(ğ‘˜)dğœƒ ğ‘– ğ‘ğ‘Ÿğ‘£ ğ‘—
ğ’Ÿ ğ’Ÿ
Essentially, we are building curve comparisons from the simpler components (ğ¼,âŠ˜ ). As
ğ‘ğ‘Ÿğ‘£
an example, we reconstruct the abroca pairwise group comparison [29] based on the auc
basemeasureofSubsection4.2giventhecurvecomparisondefinedbythepairofoperations
âŠ˜ = (ğ¼ (ğ‘˜) = 1,âŠ˜ ). Measuresthatretaincurvesoftop-ğ‘˜ recommendationsmay
ğ‘ğ‘ğ‘Ÿğ‘œğ‘ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ ğ‘ğ‘ğ‘ 
alsoconsiderNDCG-likeweightingğ¼ (ğ‘˜) = (ï¸€ 1 ifğ‘˜ > 0,0otherwise)ï¸€[38,39].
ğ·ğ¶ğº log(1+ğ‘˜)
Thresholdedvariations. Oftentimes,smalldeviationsfromperfectfairnessareaccepted.
Settlingforthresholdsunderwhichmeasuresofbiasoutput0cantakeplaceeitherattheendof
theassessment,orduringintermediatesteps. Ifthelastoptionischosen,comparisonsğ‘“ âŠ˜ğ‘“
ğ‘– ğ‘—
needtobereplacedwiththefollowingvariationsofmaximumacceptablethresholdğœ– â‰¥ 0:
ğ‘“ âŠ˜ ğ‘“ = max{0,ğ‘“ âŠ˜ğ‘“ âˆ’ğœ–}
ğ‘– ğœ– ğ‘— ğ‘– ğ‘—4.4. Reductionmechanisms
Thefinalstepofourframeworkconsistsofreducingtoonequantityallpairwisecomparisons
ğ‘“ betweengroupsorsubgroups(ğ’® ,ğ’® ) âˆˆ ğ¶(S,ğ’® ). Commonreductionsarethemaximum,
ğ‘–ğ‘— ğ‘– ğ‘— ğ‘ğ‘™ğ‘™
minimum,orarithmeticmeanofallvalues. Especiallythemaximumisacornerstoneofthe
worst-casebiasframework,althoughitdoesnotdifferentiatebetweensystemsthathavethe
sameworstcase. Alternatively,reductioncouldadoptsomeformulathatbecomestheweighted
meaninthegroupvsallcase[31]. Here,wedemonstrateonefeasibleoption,whichobtains
weights1âˆ’|P(ğ‘¥ âˆˆ ğ‘† )âˆ’P(ğ‘¥ âˆˆ ğ‘† ) = 1âˆ’|P(ğ‘¥ âˆˆ ğ‘† )âˆ’1| = P(ğ‘¥ âˆˆ ğ‘† )undervsanybut
ğ‘– ğ‘ğ‘™ğ‘™ ğ‘– ğ‘–
alsoignoresself-comparisonsamonggroupsunderpairs(Â·,Â·)orcompl(Â·,Â·):
âˆ‘ï¸
wmean ğ‘“ = (1âˆ’|P(ğ‘¥ âˆˆ ğ‘† )âˆ’P(ğ‘¥ âˆˆ ğ‘† )|)ğ‘“
ğ‘–,ğ‘— ğ‘–ğ‘— ğ‘– ğ‘— ğ‘–ğ‘—
ğ‘“ğ‘–ğ‘—:ğ’®ğ‘–Ì¸=ğ’®ğ‘—
5. The FairBench Library
Weimplementourbiasmeasuredefinitionframeworkwithinanopen-sourcePythonlibrary
called FairBench.2 This focuses on ease-of-use, comprehensibility, and compatibility with
popularworkingenvironments. Toachievethese,weadoptedaforward-orientedparadigm[42]
todesigntheprogramminginterfacesofcodeblocksascallablemethodsthatcanbecombined
inreportingmechanisms. Blockparametersaretransferredthroughkeywordarguments.
5.1. Forksofsensitiveattributevalues
Ourdesigniscenteredarounduniformlyrepresentingmanygroupsofdatasamples. Previous
fairnesslibrariestendtoparselistsofsensitiveattributenamesandmatchthesewithcolumnsof
clearlyunderstoodprogrammingdatatypes,suchasPandasdataframes[43]. However,coupling
dataloadingandfairnessassessmentcreatesinflexibleusagepatternsandsourcecodethatis
hardertoextend. Forexample,itneedsnewmethodsandclassestosupportgraphorimageAI.
Tobypassthisissue,FairBenchparsesvectorsofpredictions(e.g.,scores),groundtruth(e.g.,
classificationlabels),andbinarymembershiptoprotectedgroups. Vectorscancomefromany
computationalbackendwithaduck-typeextensionofPythonâ€™s interface;theycould
Iterable
beNumPyarrays[44],PyTorchtensors[45],TensorFlowtensors[46],JAXtensors[47],Pandas
columns[43],orPythonlists. Wesupportend-to-endintegrationwithautomaticdifferentiation
frameworksbyextendingtheircommonfunctionalinterfaceprovidedbytheEagerPylibrary
[48]andsettingtheinternalcomputationalbackendper .
fairbench.backnend(name)
Wesimplifyhandlingofmultipleprotectedgroupsbyorganizingthemintoonedatastructure,
which we call a of the sensitive attribute. This is a programming equivalent of S. All
Fork
interfaces accept a sensitive attribute fork, and base measures run for every group. Thus,
everygroupbeinganalysedbecomesacomputationalbranchofthefork. Weprovidedynamic
constructorpatternstoeasilydeclareforksincommonsetups. Onepatternisdemonstrated
below,whereafork letsuscomputetheaccuracyfork thatholdsassessmentoutcomesfor
s acc
bothwhitesandblacks:
2ThedocumentationofFairBenchisavailableat:https://fairbench.readthedocs.ioimport fairbench as fb
race, predictions, labels = ... # arrays, tensors, etc
s = fb.Fork(white=..., black=...) # branches hold binary iterables
acc = fb.accuracy(predictions=..., labels=..., sensitive=s)
fb.visualize(acc) # visualize accuracy, which is also a fork
Forksmayalsobeconstructedfromcategoricaliterables(e.g.,Pandasdataframecolumns
ornativelistslike )withtheconstructorpattern
cats=[â€™manâ€™, â€™womanâ€™, â€™manâ€™, â€™nonbinaryâ€™]
. Forksofmultidimensionalsensitiveattributescanbecreatedby
fb.Fork(fb.categories@cats)
addingallcategoricalparsingtotheconstructoraspositionalarguments. Intersectionsbetween
forkbranchesperEquation10canbeachievedbycallingacorrespondingmethod:
races, genders = ... # categorical iterables
s = fb.Fork(fb.categories@races, fb.categories@genders)
s = s.intersectional()
print(s.sum()) # visualization is not very instructive for many branches
5.2. Fairnessreports
Multidimensional fairness reports for several default popular measures, comparisons, and
reductionscanbecomputedanddisplayedwiththefollowingsnippet:
vsany = fb.unireport(predictions=..., labels=..., sensitive=s)
pairs = fb.multireport(predictions=..., labels=..., sensitive=s)
report = fb.combine(pairs, vsany)
fb.describe(report) # or print or fb.visualize
ThiscombinesthecomparisonsofEquation6(unireport)andEquation7(multireport). Formore
reporttypesorhowtodeclareonespecificmeasureofbias,refertothelibraryâ€™sdocumentation.
Thebasemeasurestoanalysearedeterminedfromkeywordarguments. Forexample,theabove
codesnippetcomputesclassificationbasemeasures,butifweaddeda arguments
scores=...
(in addition to or instead of ) recommendation measures would be obtained too.
predictions
Reportscanalsoparseacustomselectionofbasemeasures,includingexternallydefinedones.
TheoutcomeofrunningtheabovecodeispresentedinFigure1.Columnnamescorrespondto
thecombinationofreductionandcomparisonmechanisms,whereasrowstothebasemeasures.
Familiarizationwithboththelibraryandourmathematicalframeworkisrequiredtounderstand
FairBenchreports,butthesepresent-toourknowledge-thefirstsystematicwayoflookingat
manybiasassessmentsatonce,regardlessoftheexactsetting. Forexample,inthereportbelow
itiseasytodetectthesmallminimumratio(minratio)ofpr,whichcorrespondstoprule,even
iftherearegenerallybalancedtprandtnr(equivalenttobalancedfnrandfpr).
Figure1:ExampleofacombinedFairBenchreport.
Finally,reportscanbeinteractivelyexploredthrough . Thisusesthe
fb.interactive(report)
Bokehlibrary[49]toruninnotebookoutputsornewbrowsertabsandcreatevisualizationsimilartoFigure2. Throughthevisualinterface,userscannotonlyseereportvaluesbutalso
focus on columns or rows, and delve into explanations of which raw values contributed to
computations,asindicatedviathe partoftheexplorationpathontop. Explanations
.explain
correspond to tracking metadata values through predicates per Subsection 4.2. The same
explorationcanbeperformedpurelyprogrammaticallytoo.
Figure2:ExamplestepsduringtheinteractiveexplorationofaFairBenchreport.
6. Conclusions
Inthisworkweprovidedamathematicalframeworkthatstandardizesthedefinitionofmany
existingandnewmeasuresofbiasbysplittingtheminbasebuildingblocksandcompilingall
possiblecombinations. ThisframeworksystematicallyexploresAIfairnessthroughmeasures
that account for a wide range of settings and bias concerns, beyond the confines of ad-hoc
exploration. ImplementationofpopularbuildingblocksareprovidedintheFairBenchlibrary
viainteroperablePythoninterfaces. Thelibraryprovidesadditionalfeatures,likevisualization
andcomputationbacktracking,thatenablein-depthexplorationofawiderangeofbiasconcerns.
Itcanalsobeusedalongsidepopularcomputationalbackends.
Futureresearchanddevelopmentcouldworktowardsidentifyingmorebiasbuildingblocks
by decomposing more measures from the literature, as well as additional block types that
mayariseinthefuture. FairBenchisopensourceandwealsoencourageimplementationsof
suchanalysisfromthecommunity. Wefurtherplantoextendcurrentlyexperimentalfeatures,
likecreatingfairnessmodelcardsthatincludecaveatsandrecommendationsobtainedfrom
interdisciplinarycollaborationswithsocialscientists,andcreatinghigher-levelassessments.
Acknowledgement
ThisresearchworkwasfundedbytheEuropeanUnionundertheHorizonEuropeMAMMOth
project,GrantAgreementID:101070285. UKparticipantinHorizonEuropeProjectMAMMOth
issupportedbyUKRIgrantnumber10041914(TrilateralResearchLTD).References
[1] J. R. Foulds, R. Islam, K. N. Keya, S. Pan, An intersectional definition of fairness, in:
2020 IEEE 36th International Conference on Data Engineering (ICDE), IEEE, 2020, pp.
1918â€“1921.
[2] D.Biddle,Adverseimpactandtestvalidation:Apractitionerâ€™sguidetovalidanddefensible
employmenttesting,Routledge,2017.
[3] P.Ala-PietilÃ¤,Y.Bonnet,U.Bergmann,M.Bielikova,C.Bonefeld-Dahl,W.Bauer,L.Bouarfa,
R.Chatila,M.Coeckelbergh,V.Dignum,etal.,Theassessmentlistfortrustworthyartificial
intelligence(ALTAI),EuropeanCommission,2020.
[4] N.AI, Artificialintelligenceriskmanagementframework(airmf1.0)(2023).
[5] B.Li,P.Qi,B.Liu,S.Di,J.Liu,J.Pei,J.Yi,B.Zhou, Trustworthyai: Fromprinciplesto
practices, ACMComputingSurveys55(2023)1â€“46.
[6] E.Ntoutsi,P.Fafalios,U.Gadiraju,V.Iosifidis,W.Nejdl,M.-E.Vidal,S.Ruggieri,F.Turini,
S.Papadopoulos,E.Krasanakis,etal., Biasindata-drivenartificialintelligencesystemsâ€”an
introductory survey, Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery10(2020)e1356.
[7] S. Barocas, M. Hardt, A. Narayanan, Fairness and machine learning: Limitations and
opportunities,MITPress,2023.
[8] S. Mitchell, E. Potash, S. Barocas, A. Dâ€™Amour, K. Lum, Algorithmic fairness: Choices,
assumptions, and definitions, Annual Review of Statistics and Its Application 8 (2021)
141â€“163.
[9] N.Mehrabi,F.Morstatter,N.Saxena,K.Lerman,A.Galstyan, Asurveyonbiasandfairness
inmachinelearning, ACMcomputingsurveys(CSUR)54(2021)1â€“35.
[10] C.Dwork, M.Hardt, T.Pitassi, O.Reingold, R.Zemel, Fairnessthroughawareness, in:
Proceedingsofthe3rdinnovationsintheoreticalcomputerscienceconference,2012,pp.
214â€“226.
[11] M.J.Kusner,J.Loftus,C.Russell,R.Silva, Counterfactualfairness, Advancesinneural
informationprocessingsystems30(2017).
[12] A.N.Carey,X.Wu, Thecausalfairnessfieldguide: Perspectivesfromsocialandformal
sciences, FrontiersinBigData5(2022)892837.
[13] L.Rosenblatt, R.T.Witter, Counterfactualfairnessisbasicallydemographicparity, in:
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume37,2023,pp.14461â€“
14469.
[14] V.XinyingChen, J.Hooker, Aguidetoformulatingfairnessinanoptimizationmodel,
AnnalsofOperationsResearch(2023)1â€“39.
[15] J.Kleinberg,S.Mullainathan,M.Raghavan, Inherenttrade-offsinthefairdetermination
ofriskscores, arXivpreprintarXiv:1609.05807(2016).
[16] T.Miconi, Theimpossibilityof"fairness": ageneralizedimpossibilityresultfordecisions,
arXivpreprintarXiv:1707.01195(2017).
[17] R.K.Bellamy,K.Dey,M.Hind,S.C.Hoffman,S.Houde,K.Kannan,P.Lohia,J.Martino,
S. Mehta, A. MojsiloviÄ‡, et al., Ai fairness 360: An extensible toolkit for detecting and
mitigatingalgorithmicbias, IBMJournalofResearchandDevelopment63(2019)4â€“1.
[18] S.Bird,M.DudÃ­k,R.Edgar,B.Horn,R.Lutz,V.Milan,M.Sameki,H.Wallach,K.Walker,Fairlearn: A toolkit for assessing and improving fairness in ai, Microsoft, Tech. Rep.
MSR-TR-2020-32(2020).
[19] J.Wexler,M.Pushkarna,T.Bolukbasi,M.Wattenberg,F.ViÃ©gas,J.Wilson, Thewhat-if
tool: Interactiveprobingofmachinelearningmodels, IEEEtransactionsonvisualization
andcomputergraphics26(2019)56â€“65.
[20] P.Saleiro,B.Kuester,L.Hinkson,J.London,A.Stevens,A.Anisfeld,K.T.Rodolfa,R.Ghani,
Aequitas: Abiasandfairnessaudittoolkit, arXivpreprintarXiv:1811.05577(2018).
[21] A. Castelnovo, R. Crupi, G. Greco, D. Regoli, I. G. Penco, A. C. Cosentini, The zoo of
fairnessmetricsinmachinelearning(2021).
[22] T.Calders,S.Verwer, Threenaivebayesapproachesfordiscrimination-freeclassification,
Dataminingandknowledgediscovery21(2010)277â€“292.
[23] M.B.Zafar,I.Valera,M.GomezRodriguez,K.P.Gummadi, Fairnessbeyonddisparate
treatment&disparateimpact: Learningclassificationwithoutdisparatemistreatment, in:
Proceedingsofthe26thinternationalconferenceonworldwideweb,2017,pp.1171â€“1180.
[24] A.Roy,J.Horstmann,E.Ntoutsi, Multi-dimensionaldiscriminationinlawandmachine
learning-acomparativeoverview,in:Proceedingsofthe2023ACMConferenceonFairness,
Accountability,andTransparency,2023,pp.89â€“100.
[25] M.Hardt,E.Price,N.Srebro, Equalityofopportunityinsupervisedlearning, Advancesin
neuralinformationprocessingsystems29(2016).
[26] M.Donini,L.Oneto,S.Ben-David,J.S.Shawe-Taylor,M.Pontil, Empiricalriskminimiza-
tionunderfairnessconstraints, Advancesinneuralinformationprocessingsystems31
(2018).
[27] S. Tsioutsiouliklis, E. Pitoura, P. Tsaparas, I. Kleftakis, N. Mamoulis, Fairness-aware
pagerank, in: ProceedingsoftheWebConference2021,2021,pp.3815â€“3826.
[28] T.Calders,A.Karim,F.Kamiran,W.Ali,X.Zhang, Controllingattributeeffectinlinear
regression, in: 2013IEEE13thinternationalconferenceondatamining,IEEE,2013,pp.
71â€“80.
[29] J. Gardner, C. Brooks, R. Baker, Evaluating the fairness of predictive student models
throughslicinganalysis, in: Proceedingsofthe9thinternationalconferenceonlearning
analytics&knowledge,2019,pp.225â€“234.
[30] A.Ghosh,L.Genuit,M.Reagan, Characterizingintersectionalgroupfairnesswithworst-
casecomparisons, in: ArtificialIntelligenceDiversity,Belonging,Equity,andInclusion,
PMLR,2021,pp.22â€“34.
[31] M.Kearns,S.Neel,A.Roth,Z.S.Wu, Preventingfairnessgerrymandering: Auditingand
learningforsubgroupfairness, in: Internationalconferenceonmachinelearning,PMLR,
2018,pp.2564â€“2572.
[32] M.Kearns,S.Neel,A.Roth,Z.S.Wu, Anempiricalstudyofrichsubgroupfairnessfor
machine learning, in: Proceedings of the conference on fairness, accountability, and
transparency,2019,pp.100â€“109.
[33] J.Ma,J.Deng,Q.Mei, Subgroupgeneralizationandfairnessofgraphneuralnetworks,
AdvancesinNeuralInformationProcessingSystems34(2021)1048â€“1061.
[34] N.L.Martinez,M.A.Bertran,A.Papadaki,M.Rodrigues,G.Sapiro, Blindparetofairness
andsubgrouprobustness, in: InternationalConferenceonMachineLearning,PMLR,2021,
pp.7492â€“7501.[35] C.Shui,G.Xu,Q.Chen,J.Li,C.X.Ling,T.Arbel,B.Wang,C.GagnÃ©, Onlearningfairness
andaccuracyonmultiplesubgroups, AdvancesinNeuralInformationProcessingSystems
35(2022)34121â€“34135.
[36] M.P.Kim, A.Korolova, G.N.Rothblum, G.Yona, Preference-informedfairness, arXiv
preprintarXiv:1904.01793(2019).
[37] J. Gartner, A new metric for quantifying machine learn-
ing fairness in healthcare, https://www.closedloop.ai/blog/
a-new-metric-for-quantifying-machine-learning-fairness-in-healthcare/, 2020. Ac-
cessed: 17-2-2024.
[38] M. Zehlike, K. Yang, J. Stoyanovich, Fairness in ranking: A survey, arXiv preprint
arXiv:2103.14000(2021).
[39] E. Pitoura, K. Stefanidis, G. Koutrika, Fairness in rankings and recommendations: an
overview, TheVLDBJournal(2022)1â€“28.
[40] E.A.Watkins,M.McKenna,J.Chen, Thefour-fifthsruleisnotdisparateimpact: awoeful
tale of epistemic trespassing in algorithmic fairness, arXiv preprint arXiv:2202.09519
(2022).
[41] I. Sarridis, C. Koutlis, S. Papadopoulos, C. Diou, Flac: Fairness-aware representation
learning by suppressing attribute-class associations, arXiv preprint arXiv:2304.14252
(2023).
[42] E. Krasanakis, A. L. Symeonidis, Forward oriented programming: A meta-dsl for fast
developmentofcomponentlibraries, AvailableatSSRN4180025(????).
[43] W.McKinney,etal., pandas: afoundationalpythonlibraryfordataanalysisandstatistics,
Pythonforhighperformanceandscientificcomputing14(2011)1â€“9.
[44] C.R.Harris,K.J.Millman,S.J.VanDerWalt,R.Gommers,P.Virtanen,D.Cournapeau,
E.Wieser,J.Taylor,S.Berg,N.J.Smith,etal., Arrayprogrammingwithnumpy, Nature
585(2020)357â€“362.
[45] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, et al., Pytorch: An imperative style, high-performance deep
learninglibrary, Advancesinneuralinformationprocessingsystems32(2019).
[46] M.Abadi,A.Agarwal,P.Barham,E.Brevdo,Z.Chen,C.Citro,G.S.Corrado,A.Davis,
J. Dean, M. Devin, et al., Tensorflow: Large-scale machine learning on heterogeneous
distributedsystems, arXivpreprintarXiv:1603.04467(2016).
[47] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula,
A.Paszke,J.VanderPlas,S.Wanderman-Milne,Q.Zhang,JAX:composabletransformations
ofPython+NumPyprograms,2018.URL:http://github.com/google/jax.
[48] J.Rauber,M.Bethge,W.Brendel, Eagerpy: Writingcodethatworksnativelywithpytorch,
tensorflow,jax,andnumpy, arXivpreprintarXiv:2008.04175(2020).
[49] C.BokehDevelopmentTeam,Bokeh: Pythonlibraryforinteractivevisualization,2014.