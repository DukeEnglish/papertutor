Approximation-Aware Bayesian Optimization
NatalieMaus KyuraeKim GeoffPleiss
UniversityofPennsylvania UniversityofPennsylvania UniversityofBritishColumbia
nmaus@seas.upenn.edu
DavidEriksson JohnP.Cunningham JacobR.Gardner
Meta ColumbiaUniversity UniversityofPennsylvania
Abstract
High-dimensionalBayesianoptimization(BO)taskssuchasmoleculardesignoften
require>10,000functionevaluationsbeforeobtainingmeaningfulresults. While
methodslikesparsevariationalGaussianprocesses(SVGPs)reducecomputational
requirementsinthesesettings,theunderlyingapproximationsresultinsuboptimal
dataacquisitionsthatslowtheprogressofoptimization. Inthispaperwemodify
SVGPstobetteralignwiththegoalsofBO:targetinginformeddataacquisition
rather than global posterior fidelity. Using the framework of utility-calibrated
variational inference, we unify GP approximation and data acquisition into a
jointoptimization problem, thereby ensuring optimaldecisionsunderalimited
computational budget. Our approach can be used with any decision-theoretic
acquisition function and is compatible with trust region methods like TuRBO.
Wederiveefficientjointobjectivesfortheexpectedimprovementandknowledge
gradient acquisition functions in both the standard and batch BO settings. Our
approachoutperformsstandardSVGPsonhigh-dimensionalbenchmarktasksin
controlandmoleculardesign.
1 Introduction
Bayesianoptimization(BO;Frazier,2018;Garnett,2023;Jonesetal.,1998;Mockus,1982;Shahriari
etal.,2015)castsoptimizationasasequentialdecision-makingproblem. ManyrecentsuccessesofBO
haveinvolvedcomplexandhigh-dimensionalproblems. Incontrasttoâ€œclassicâ€low-dimensionalBO
problemsâ€”whereexpensiveblack-boxfunctionevaluationsfarexceededcomputationalcostsâ€”these
modernproblemsnecessitatetensofthousandsoffunctionevaluations,anditisoftenthecomplexity
anddimensionalityofthesearchspacethatmakesoptimizationchallenging,ratherthanalimited
evaluationbudget(Erikssonetal.,2019;GriffithsandHernÃ¡ndez-Lobato,2020;Mausetal.,2022,
2023;Stantonetal.,2022). Becauseofthesescenarios,BOisenteringaregimewherecomputational
costsarebecomingaprimarybottleneck(Maddoxetal.,2021;Mausetal.,2023;Mossetal.,2023;
Vakilietal.,2021),astheGaussianprocess(GP;RasmussenandWilliams,2005)surrogatemodels
thatunderpinmostofBayesianoptimizationscalecubicallywiththenumberofobservations.
In this new regime, we require scalable GP approximations, an area that has made tremendous
progressoverthelastdecade. Inparticular,sparsevariationalGaussianprocesses(SVGP;Hensman
et al., 2013; QuiÃ±onero-Candela and Rasmussen, 2005; Titsias, 2009) have seen an increase in
use(GriffithsandHernÃ¡ndez-Lobato,2020;Maddoxetal.,2021;Mausetal.,2022,2023;Stanton
etal.,2022;Trippetal.,2020;Vakilietal.,2021),butmanychallengesremaintoeffectivelydeploy
SVGPs for large-budget BO. In particular, the standard SVGP training objective is not aligned
withthegoalsofblack-boxoptimization. SVGPsconstructaninducingpointapproximationthat
maximizesthestandardvariationalevidencelowerbound(ELBO;Jordanetal.,1999),yieldinga
posteriorapproximationğ‘âˆ—(ğ‘“)thatmodelsallobserveddata(Matthewsetal.,2016;Mossetal.,2023).
Preprint. Underreview.
4202
nuJ
6
]GL.sc[
1v80340.6042:viXraHowever,theoptimalposteriorapproximationğ‘âˆ—issuboptimalforthedecision-makingtasksinvolved
inBO(Lacosteâ€“Julienetal.,2011). InBO,wedonotcareaboutposteriorfidelityatthemajority
ofpriorobservations;rather,weonlycareaboutthefidelityofdownstreamfunctionsinvolvingthe
posterior,suchastheexpectedutility. Toillustratethispointintuitively,considerusingthecommon
expectedimprovement(EI;Jonesetal.,1998)acquisitionfunctionforselectingnewobservations.
MaximizingtheELBOmightresultinaposteriorapproximationthatmaintainsfidelityfortraining
examplesinregionsofvirtuallyzeroEI,thuswastingâ€œapproximationbudget.â€
Tosolvethisproblem,wefocusonthedeepconnectionsbetweenstatisticaldecisiontheory(Robert,
2001;Wasserman,2013,Â§12)andBayesianoptimization(Garnett,2023,Â§6-7),whereacquisition
maximizationcanbeviewedasmaximizingposterior-expectedutility. Followingthisperspective,
we leverage the utility-calibrated approximate inference framework (Jaiswal et al., 2020, 2023;
Lacosteâ€“Julienetal.,2011),andsolvetheaforementionedproblemthroughavariationalbound(Blei
etal.,2017;Jordanetal.,1999)â€“the(log)expectedutilitylowerbound(EULBO)â€”ajointfunctionof
thedecision(theBOquery)andtheposteriorapproximation(theSVGP).Whenoptimizedjointly,
theEULBOautomaticallyyieldstheapproximatelyoptimaldecisionthroughtheminorize-maximize
principle (Lange, 2016). The EULBO is reminiscent of the standard variational ELBO (Jordan
etal.,1999),andcanindeedbeviewedasastandardELBOforageneralizedBayesianinference
problem(Bissirietal.,2016;Knoblauchetal.,2022)whereweseektoapproximatetheutility-weighted
posterior. Thisworkrepresentsthefirstapplicationofutility-calibratedapproximateinferencetowards
BOdespiteitsinherentconnectionwithutilitymaximization.
The benefits of our proposed approach are visualized in Fig. 1. Furthermore, it can be applied
toacquisitionfunctionthatadmitsadecision-theoreticinterpretation,whichincludesthepopular
expectedimprovement(EI;Jonesetal.,1998)andknowledgegradient(KG;Wuetal.,2017)acquisition
functions,andistriviallycompatiblewithlocaloptimizationtechniqueslikeTuRBO(Erikssonetal.,
2019)forhigh-dimensionalproblems. WedemonstratethatourjointSVGP/acquisitionoptimization
approachyieldssignificantimprovementsacrossnumerousBayesianoptimizationbenchmarks. Asan
addedbenefit,ourapproachcansimplifytheimplementationandreducethecomputationalburdenof
complex(decision-theoretic)acquisitionfunctionslikeKG.Wedemonstrateanovelalgorithmderived
fromourjointoptimizationapproachforcomputingandoptimizingtheKGthatexpandsrecentwork
onone-shotKG(Balandatetal.,2020)andvariationalGPposteriorrefinement(Maddoxetal.,2021).
Overall,ourcontributionsaresummarizedasfollows:
âˆ™
Weproposeutility-calibratedvariationalinferenceofSVGPsinthecontextoflarge-budgetBO.
âˆ™
Westudythisframeworkintwospecialcasesusingtheutilityfunctionsoftwocommonacquisition
functions: EIandKG.Foreach,wederivetractableEULBOexpressionsthatcanbeoptimized.
âˆ™ For KG, we demonstrate that the computation of the EULBO takes only negligible additional
workovercomputingthestandardELBObyleveraginganonlinevariationalupdate. Thus,asa
byproductofoptimizingtheEULBO,optimizingKGbecomescomparabletothecostoftheEI.
âˆ™ We extend this framework to be capable of running in batch mode, by introducing q-EULBO
analogsofq-KGandq-EIascommonlyusedinpractice(Wilsonetal.,2018).
âˆ™
WedemonstratetheeffectivenessofourproposedmethodagainststandardSVGPstrainedwith
ELBOmaximizationonhigh-dimensionalbenchmarktasksincontrolandmoleculardesign,
wherethedimensionalityandevaluationbudgetgoupto256and80k,respectively.
2 Background
Black-Boxoptimization referstoproblemsoftheform: maximize ğ¹(ğ’™),whereğ’³ âŠ‚â„ğ‘‘ is
ğ’™âˆˆğ’³
somecompactdomain,andweassumethatonlyzeroth-orderinformationisavailable. Generally,we
assumethatobservationsoftheobjectivefunction(ğ’™,ğ‘¦ =ğ¹Ë†(ğ’™))havebeencorruptedbyGaussian
ğ‘– ğ‘– ğ‘–
noiseğ¹Ë†(ğ’™)â‰œğ¹(ğ’™)+ğœ–,whereğœ– âˆ¼ğ’©(0,ğœ2). Thenoisevarianceğœ2 iscommonlyunknownand
ğ‘– ğ‘– ğ‘› ğ‘›
inferredfromthetrainingdata.
BayesianOptimization (BO)isanditerativeapproachtoblack-boxoptimizationthatutilizesthe
followingsteps: â¶Ateachstepğ‘¡ â‰¥ 0,weuseasetofobservationsğ’Ÿ =
{(
ğ’™,ğ‘¦
=ğ¹Ë†(ğ’™)) }ğ‘›ğ‘¡
of
ğ‘¡ ğ‘– ğ‘– ğ‘– ğ‘–=1
ğ¹Ë†to fit a surrogate supervised model ğ‘“. Typically ğ‘“ is taken to be a Gaussian process, with the
function-valuedposteriordistributionğœ‹(ğ‘“ âˆ£ğ’Ÿ)asoursurrogatemodelatstepğ‘¡. â·Thesurrogate
2ELBO fit (m=4) ELBO EULBO (Ours)
Data True function Inducing points GP mean EI (approx) EI (exact)
Figure1: (Left.) FittinganSVGPmodelwithonlyğ‘š=4inducingpointssacrificesmodelingareas
ofhighEI(fewdatapointsatright)becausetheELBOfocusesonlyonglobaldataapproximation
(left data) and is ignorant of the downstream decision making task. (Middle.) Because of this,
(normalized)EIwiththeSVGPmodelpeaksinanincorrectlocationrelativetotheexactposterior.
(Right.) UpdatingtheGPfitandselectingacandidatejointlyusingtheEULBO(ourmethod)resultsin
candidateselectionmuchclosertotheexactmodel.
is then used to form a decision problem where we choose which point we should evaluate next,
ğ’™ =ğ›¿ (ğ’Ÿ ),bymaximizinganacquisitionfunctionğ›¼ âˆ¶ğ’³ â†’â„as
ğ‘¡+1 ğ›¼ ğ‘¡
ğ›¿ (ğ’Ÿ )â‰œargmax ğ›¼(ğ’™;ğ’Ÿ ). (1)
ğ›¼ ğ‘¡ ğ‘¡
ğ’™âˆˆğ’³
â¸Afterselectingğ’™ ,ğ¹Ë†isevaluatedtoobtainthenewdatapoint(ğ’™ ,ğ‘¦ =ğ¹Ë†(ğ’™ )). Thisis
ğ‘¡+1 ğ‘¡+1 ğ‘¡+1 ğ‘¡+1
thenaddedtothedataset,formingğ’Ÿ =ğ’Ÿ âˆª(ğ’™ ,ğ‘¦ )tobeusedinthenextiteration.
ğ‘¡+1 ğ‘¡ ğ‘¡+1 ğ‘¡+1
UtilityPerspectiveofAcquisitionFunctions. Manycommonlyusedacquisitionfunctions,includ-
ingEIandKG,canbeexpressedasposterior-expectedutilityfunctions
ğ›¼(ğ’™;ğ’Ÿ)â‰œâˆ« ğ‘¢(ğ’™,ğ‘“;ğ’Ÿ )ğœ‹(ğ‘“ âˆ£ğ’Ÿ)dğ‘“, (2)
ğ‘¡
whereğ‘¢(ğ’™,ğ‘“;ğ’Ÿ )âˆ¶ğ’³Ã—â„± â†’â„issomeutilityfunctionassociatedwithğ›¼(Garnett,2023,Â§6-7).
ğ‘¡
Indecisiontheory,posterior-expectedutilitymaximizationpoliciessuchasğ›¿ areknownasBayes
ğ›¼
policies. These are important because, for a given utility function, they attain certain notions of
statisticaloptimalitysuchasBayesoptimalityandadmissibility(Robert,2001,Â§2.4;Wasserman,
2013,Â§12). However,thisonlyholdstrueifwecanexactlycomputeEq.(2)overtheposterior. Once
approximateinferenceisinvolved,makingoptimalBayesdecisionsbecomeschallenging.
SparseVariationalGaussianProcesses Whiletheğ’ª(ğ‘›3)complexityofexactGaussianprocess
model selection and inference is not necessarily a roadblock in the traditional regression setting
with 10,000-50,000 training examples, BO amplifies the scalability challenge by requiring us to
sequentiallytrainorupdatemanylargescaleGPsasweiterativelyacquiremoredata.
Toaddressthis,sparsevariationalGPs(SVGP;Hensmanetal.,2013;Titsias,2009)havebecome
commonlyusedinhigh-throughputBayesianoptimization. SVGPsmodifytheoriginalGPprior
fromğ‘(ğ‘“)toğ‘(ğ‘“ âˆ£ ğ’–)ğ‘(ğ’–),whereweassumethelatentfunctionğ‘“ isâ€œinducedâ€byafinitesetof
inducingvaluesğ’–=(ğ‘¢ ,â€¦,ğ‘¢ )âˆˆâ„ğ‘š locatedatinducingpointsğ’› âˆˆğ’³ forğ‘– =1,â€¦,ğ‘š. Inference
1 ğ‘š ğ‘–
is done through variational inference (Blei et al., 2017; Jordan et al., 1999) the posterior of the
inducingpointsisapproximatedusingğ‘ (ğ’–)=ğ’©(ğ’–;ğ€=(ğ’,ğ‘º))andthatofthelatentfunctions
ğ€
withğ‘(ğ‘“ âˆ£ğ’–)=ğ‘(ğ‘“ âˆ£ğ’–). TheresultingELBOobjective,whichcanbecomputedinclosedform
(Hensmanetal.,2013),isthen
â„’ (ğ€;ğ’Ÿ )â‰œğ”¼
[âˆ‘ğ‘›ğ‘¡
logğ“(ğ‘¦
âˆ£ğ‘“(ğ’™))]
âˆ’D (ğ‘ (ğ’–),ğ‘(ğ’–)), (3)
ELBO ğ‘¡ ğ‘ğœ†(ğ‘“) ğ‘–=1 ğ‘– ğ‘– KL ğœ†
where ğ“(ğ‘¦ âˆ£ ğ‘“(ğ’™)) = ğ’©(ğ‘¦ âˆ£ğ‘“(ğ’™),ğœ ) is a Gaussian likelihood. The marginal variational
ğ‘– ğ‘– ğ‘– ğ‘– ğœ–
approximationcanbecomputedas
ğ‘ (ğ‘“)=âˆ« ğ‘ (ğ‘“,ğ’–)dğ’–=âˆ« ğ‘(ğ‘“ âˆ£ğ’–)ğ‘ (ğ’–)dğ’–
ğœ† ğ€ ğœ†
suchthat
( )
ğ‘ (ğ‘“(ğ’™))=ğ’© ğ‘“(ğ’™); ğœ‡ (ğ’™)â‰œğ‘² ğ‘²âˆ’1ğ’, ğœ2(ğ’™)â‰œğ‘˜Ëœ +ğ’ŒâŠ¤ ğ‘²âˆ’1ğ‘ºğ‘²âˆ’1ğ’Œ , (4)
ğœ† ğ‘“ ğ’™ğ’ ğ’ğ’ ğ‘“ ğ’™ğ’™ ğ’™ğ’ ğ’ğ’ ğ’ğ’ ğ’ğ’™
withğ‘˜Ëœ â‰œğ‘˜(ğ’™,ğ’™)âˆ’ğ’Œ ğ‘²âˆ’1ğ’ŒâŠ¤ ,thevectorğ’Œ âˆˆâ„ğ‘š isformedas[ğ’Œ ] =ğ‘˜(ğ’›,ğ’™),andthe
ğ’™ğ’™ ğ’™ğ’ ğ’ğ’ ğ’ğ’™ ğ’ğ’™ ğ’ğ’™ ğ‘– ğ‘–
matrixğ‘² âˆˆâ„ğ‘šÃ—ğ‘š isformedas[ğ‘² ] =ğ‘˜(ğ’›,ğ’› ). Additionally,theGPlikelihoodandkernel
ğ’ğ’ ğ’ğ’ ğ‘–ğ‘— ğ‘– ğ‘—
3containhyperparameters,whichwedenoteasğœ½ âˆˆÎ˜,andwecollectivelydenotethesetofinducing
pointlocationsasğ’ ={ğ’› ,â€¦,ğ’› }âˆˆâ„ğ‘‘ğ‘š. WethereforedenotetheELBOasâ„’ (ğ€,ğ’,ğœ½;ğ’Ÿ ).
1 ğ‘š ELBO ğ‘¡
3 Approximation-AwareBayesianOptimization
WhenSVGPsareusedinconjunctionwithBO(Maddoxetal.,2021;Mossetal.,2023),acquisition
functionsoftheformofEq.(2)arenaÃ¯velyapproximatedas
ğ›¼(ğ’™;ğ’Ÿ)â‰ˆâˆ« ğ‘¢(ğ’™,ğ‘“;ğ’Ÿ )ğ‘ (ğ‘“)dğ‘“,
ğ‘¡ ğ€
whereğ‘ (ğ‘“)istheapproximateSVGPposteriorgivenbyEq.(4). Theacquisitionpolicyimpliedby
ğ€
thisapproximationcontainstwoseparateoptimizationproblems:
ğ’™ ğ‘¡+1 =arg ğ’™âˆˆm ğ’³axâˆ« ğ‘¢(ğ’™,ğ‘“;ğ’Ÿ ğ‘¡)ğ‘ ğ€âˆ— ELBO(ğ‘“)dğ‘“ and ğ€âˆ— ELBO =arg ğ€âˆˆm Î›axâ„’ ELBO(ğ€;ğ’Ÿ ğ‘¡). (5)
Treating these optimization problems separately creates an artificial bottleneck that results in
suboptimaldataacquisitiondecisions. Intuitively,ğ€âˆ— ischosentofaithfullymodelallobserved
ELBO
data(Matthewsetal.,2016;Mossetal.,2023),withoutregardforhowtheresultingmodelperformsat
selectingthenextfunctionevaluationintheBOloop. Foranillustrationofthis,seeFigure1. Instead,
weproposeamodificationtoSVGPsthatcouplestheposteriorapproximationanddataacquisition
throughajointproblemoftheform:
(ğ’™ , ğ€âˆ—)=argmaxâ„’ (ğ€,ğ’™;ğ’Ÿ ). (6)
ğ‘¡+1 EULBO ğ‘¡
ğ€âˆˆÎ›,ğ’™âˆˆğ’³
Thisresultsinğ’™ directlyapproximatingasolutiontoEq.(2),wheretheexpectedutilitylower-
ğ‘¡+1
bound(EULBO)isanELBO-likeobjectivefunctionderivedbelow.
3.1 ExpectedUtilityLower-Bound
ConsideranacquisitionfunctionoftheformofEq.(2)wheretheutilityğ‘¢âˆ¶ğ’³Ã—â„± â†’â„ isstrictly
>0
positive. Wecanderiveasimilarvariationalformulationoftheacquisitionfunctionmaximization
problemfollowingLacosteâ€“Julienetal.(2011). Thatis,givenanydistributionğ‘ indexedbyğ€âˆˆÎ›
ğ€
andconsideringtheSVGPprioraugmentationğ‘(ğ‘“)â†’ğ‘(ğ‘“ âˆ£ğ’–)ğ‘(ğ’–),theacquisitionfunctioncanbe
lower-boundedthroughJensenâ€™sinequalityas
logğ›¼(ğ’™;ğ’Ÿ )=logâˆ« ğ‘¢(ğ’™,ğ‘“;ğ’Ÿ )ğœ‹(ğ‘“ âˆ£ğ’Ÿ )dğ‘“
ğ‘¡ ğ‘¡ ğ‘¡
ğ‘ (ğ‘“,ğ’–)
ğ€
=logâˆ« ğ‘¢(ğ’™,ğ‘“;ğ’Ÿ )ğœ‹(ğ‘“,ğ’–âˆ£ğ’Ÿ ) dğ‘“dğ’–
ğ‘¡ ğ‘¡ ğ‘ (ğ‘“,ğ’–)
ğ€
ğ‘ (ğ’–)ğ‘(ğ‘“ âˆ£ğ’–)
ğ€
=logâˆ« ğ‘¢(ğ’™,ğ‘“;ğ’Ÿ ) ğ“(ğ’Ÿ âˆ£ğ‘“)ğ‘(ğ‘“ âˆ£ğ’–)ğ‘(ğ’–) dğ‘“dğ’–âˆ’logğ‘
ğ‘¡ ğ‘¡ ğ‘ (ğ’–)ğ‘(ğ‘“ âˆ£ğ’–)
ğ€
ğ‘¢(ğ’™,ğ‘“;ğ’Ÿ )ğ“(ğ’Ÿ âˆ£ğ‘“)ğ‘(ğ’–)
ğ‘¡ ğ‘¡
â‰¥âˆ« log( )ğ‘(ğ‘“ âˆ£ğ’–)ğ‘ (ğ’–) dğ‘“dğ’–âˆ’logğ‘, (7)
ğ‘ (ğ’–) ğ€
ğ€
whereğ‘isanormalizingconstant. Notethatthisderivationissimilartothederivationofexpectation-
maximization(Dempsteretal.,1977)andvariationallowerbounds(Jordanetal.,1999). Thus,this
lowerboundimpliesthat,bytheminorize-maximizeprinciple(Lange,2016),maximizingthelower
boundwithrespecttoğ’™andğ€approximatelysolvestheoriginalproblemofmaximizingthe(exact)
posterior-expectedutility.
ExpectedUtilityLower-Bound Uptoaconstantandrearrangingterms,maximizingEq.(7)is
equivalenttomaximizing
â„’ (ğ€,ğ’™;ğ’Ÿ )â‰œğ”¼ [logğ“(ğ’Ÿ âˆ£ğ‘“)+logğ‘(ğ’–)âˆ’logğ‘ (ğ’–)+logğ‘¢(ğ’™,ğ‘“;ğ’Ÿ )]
EULBO ğ‘¡ =ğ”¼ğ‘(ğ‘“âˆ£ğ’–) [ğ‘ âˆ‘ğœ†(ğ’– ğ‘›)
ğ‘¡
logğ“(ğ‘¦ğ‘¡
âˆ£ğ‘“)]
âˆ’D (ğ‘
(ğ’–),ğ‘(ğ’–ğœ†
))+ğ”¼
logğ‘¢(ğ’™,ğ‘“ğ‘¡
;ğ’Ÿ )
ğ‘ğœ†(ğ‘“) ğ‘–=1 ğ‘– KL ğ€ ğ‘ğœ†(ğ‘“) ğ‘¡
=â„’ (ğ€;ğ’Ÿ )+ğ”¼ logğ‘¢(ğ’™,ğ‘“;ğ’Ÿ ), (8)
ELBO ğ‘¡ ğ‘ğ€(ğ‘“) ğ‘¡
whichisthejointobjectivefunctionalludedtoinEq.(6). WemaximizeEULBOtoobtain(ğ’™ ,ğ€âˆ—)=
ğ‘¡+1
argmax â„’ (ğ’™,ğ€),whereğ’™ correspondsournextBOâ€œqueryâ€.
ğ’™âˆˆğ’³,ğ€âˆˆÎ› EULBO ğ‘¡+1
FromEq.(8),theconnectionbetweentheEULBOandELBOisobvious: theEULBOisnowâ€œnudgingâ€
theELBOsolutiontowardhighutilityregions. Analternativeperspectiveisthatweareapproximating
4ageneralizedposterior weightedbytheutility(Table. 1byKnoblauchetal.,2022; Bissirietal.,
2016). Furthermore,Jaiswaletal.(2020,2023)provethattheresultingactionssatisfyconsistency
guaranteesunderassumptionstypicalinsuchresultsforvariationalinference(WangandBlei,2019).
HyperparametersandInducingPointLocations Forthehyperparametersğœ½ andinducingpoint
locationsğ’,weusethemarginallikelihoodtoperformmodelselection,whichiscommonpracticein
BO(Shahriarietal.,2015,Â§V.A).(Optimizingoverğ’ waspopularizedbySnelsonandGhahramani,
2005.) Followingsuit,wealsooptimizetheEULBOasafunctionofğœ½ andğ’ as
{ }
maximize â„’ (ğ€,ğ’™,ğœ½,ğ’;ğ’Ÿ )â‰œâ„’ (ğ€,ğ’,ğœ½;ğ’Ÿ )+ğ”¼ logğ‘¢(ğ’™,ğ‘“;ğ’Ÿ ) .
ğ€,ğ’™,ğœ½,ğ’
EULBO ğ‘¡ ELBO ğ‘¡ ğ‘ğ€(ğ‘“) ğ‘¡
WeemphasizeherethattheSVGP-associatedparametersğ€,ğœ½,ğ’ havegradientsthataredetermined
bybothtermsabove. Thus,theexpectedlog-utilitytermğ”¼ logğ‘¢(ğ’™,ğ‘“;ğ’Ÿ )simultaneously
ğ‘“âˆ¼ğ‘ğ€(ğ‘“) ğ‘¡
resultsinacquisitionofğ’™ anddirectlyinfluencestheunderlyingSVGPregressionmodel.
ğ‘¡+1
3.2 EULBOforExpectedImprovement(EI)
TheEIacquisitionfunctioncanbeexpressedasaposterior-expectedutility,wheretheunderlying
â€œimprovementâ€utilityfunctionisgivenbythedifferencebetweentheobjectivevalueofthequery,
ğ‘“(ğ’™),andthecurrentbestobjectivevalueğ‘¦âˆ— =max {ğ‘¦ âˆ£ğ‘¦ âˆˆğ’Ÿ }:
ğ‘¡ ğ‘–=1,â€¦,ğ‘¡ ğ‘– ğ‘– ğ‘¡
( )
ğ‘¢ (ğ’™,ğ‘“;ğ’Ÿ )â‰œReLU ğ‘“(ğ’™)âˆ’ğ‘¦âˆ— , (EI;Jonesetal.,1998) (9)
EI ğ‘¡ ğ‘¡
whereReLU(ğ‘¥)â‰œmax(ğ‘¥,0). Unfortunately,thisutilityisnotstrictlypositivewheneverğ‘“(ğ’™)â‰¤ğ‘¦âˆ—.
Thus,wecannotimmediatelyplugğ‘¢ intotheEULBO.Whileitispossibletoaddasmallpositive
EI
constanttoğ‘¢ andmakeitstrictlypositiveasdonebyKuÅ›mierczyketal.(2019),thisresultsina
EI
looserJensengapinEq.(7),whichcouldbedetrimental. Thisalsointroducestheneedfortuningthe
constant,whichisnotstraightforward. Instead,definethefollowingâ€œsoftimprovementâ€utility:
( )
ğ‘¢ (ğ’™,ğ‘“;ğ’Ÿ )â‰œsoftplus ğ‘“(ğ’™)âˆ’ğ‘¦âˆ— ,
SEI ğ‘¡ ğ‘¡
wherewereplacetheReLUinEq.(9)withsoftplus(ğ‘¥)â‰œlog(1+exp(ğ‘¥)). softplus(ğ‘¥)converges
totheReLUinbothextremesofğ‘¥ â†’âˆ’âˆandğ‘¥ â†’âˆ. Thus,ğ‘¢ behavessimilarlytoğ‘¢ ,butwill
SEI EI
beslightlymoreexplorativeduetopositivity.
ComputingtheEULBOanditsderivativesnowrequiresthecomputationofğ”¼ logğ‘¢ (ğ’™,ğ‘“;ğ’Ÿ ),
ğ‘“âˆ¼ğ‘ğ€(ğ‘“) SEI ğ‘¡
which, unlike EI, does not have a closed-form. However, since the utility function only depends
onthefunctionvaluesofğ‘“,theexpectationcanbeefficientlycomputedtohighprecisionthrough
one-dimensionalGauss-Hermitequadrature.
3.3 EULBOforKnowledgeGradient(KG)
Althoughnon-trivial,theKGacquisitionisalsoaposterior-expectedutility,wheretheunderlying
utilityfunctionisgivenbythemaximumpredictivemeanvalueanywhereintheinputdomainafter
conditioningonanewobservation(ğ’™,ğ‘¦):
[ ]
ğ‘¢ (ğ’™,ğ‘¦;ğ’Ÿ )â‰œmax ğ”¼ ğ‘“(ğ’™â€²)âˆ£ğ’Ÿ âˆª{(ğ’™,ğ‘¦)} . (KG;Frazier,2009;Garnett,2023)
KG ğ‘¡ ğ‘¡
ğ’™â€²âˆˆğ’³
Notethattheutilityfunctionasdefinedaboveisnotnon-negative: themaximumpredictivemeanofa
Gaussianprocesscanbenegative. Forthisreason,theutilityfunctioniscommonly(andoriginally,
e.g. Frazier,2009,Eq. 4.11)writtenintheliteratureasthedifferencebetweenthenewmaximum
meanafterconditioningon(ğ’™,ğ‘¦)andthemaximummeanbeforehand:
[ ]
ğ‘¢ (ğ’™,ğ‘¦;ğ’Ÿ )â‰œmax ğ”¼ ğ‘“(ğ’™â€²)âˆ£ğ’Ÿ âˆª{(ğ’™,ğ‘¦)} âˆ’ğœ‡+,
KG ğ‘¡ ğ’™â€²âˆˆğ’³ ğ‘¡ ğ‘¡
[ ]
whereğœ‡+ â‰œmax ğ¸ ğ‘“(ğ’™â€²â€²)âˆ£ğ’Ÿ . Noteğœ‡+playstheroleofasimpleconstantasitdependson
ğ‘¡ ğ’™â€²â€²âˆˆğ’³ ğ‘¡ ğ‘¡
neitherğ’™norğ‘¦. SimilarlytotheEIacquisition,thisutilityisnotstrictlypositive,andwethusdefine
itsâ€œsoftâ€variant:
( )
ğ‘¢ (ğ’™,ğ‘¦;ğ’Ÿ )â‰œsoftplus ğ‘¢ (ğ’™,ğ‘¦;ğ’Ÿ )âˆ’ğ‘+ .
SKG ğ‘¡ KG ğ‘¡
Here,ğ‘+actsasğœ‡+bymakingğ‘¢ positiveasoftenaspossible. Thisisparticularlyimportantwhen
ğ‘¡ KG
theGPpredictivemeanisnegativeasaconsequenceoftheobjectivevaluesbeingnegative. One
naturalchoiceofconstantisusingğœ‡+;however,wefindthatsimplychoosingğ‘+ =ğ‘¦+workswell
ğ‘¡ ğ‘¡
andismorecomputationallyefficient.
5One-shotKGEULBO. TheEULBOusingğ‘¢ resultsinanexpensivenestedoptimizationproblem.
SKG
Toaddressthis,weuseanapproachsimilartotheone-shotknowledgegradientmethodofBalandat
etal.(2020). Forclarity,wewilldefinethereparameterizationfunction
ğ‘¦ (ğ’™;ğœ–)â‰œğœ‡ (ğ’™)+ğœ (ğ’™)ğœ–,
ğ€ ğ‘– ğ‘ğœ† ğ‘ğœ† ğ‘–
where, f (or an i.i.d. sam ) ple ğœ– ğ‘– âˆ¼ ğ’©(0,1), computing ğ‘¦ ğ‘– = ğ‘¦ ğ€(ğ’™,ğœ– ğ‘–) is equivalent to sampling
ğ‘¦ âˆ¼ğ’© ğœ‡ (ğ’™),ğœ (ğ’™) . Thisenablestheuseofthereparameterizationgradientestimator(Kingma
ğ‘– ğ‘ğœ† ğ‘ğœ†
andWelling,2014;Rezendeetal.,2014;TitsiasandLÃ¡zaro-Gredilla,2014). Now,noticethattheKG
acquisitionfunctioncanbeapproximatedthroughMonteCarloas
1 âˆ‘ğ‘† 1 âˆ‘ğ‘† [ ]
ğ›¼ (ğ’™;ğ’Ÿ)â‰ˆ ğ‘¢ (ğ’™,ğ‘¦ (ğ’™;ğœ–);ğ’Ÿ )= maxğ”¼ ğ‘“(ğ’™â€²)âˆ£ğ’Ÿ âˆª{ğ’™,ğ‘¦ (ğ’™;ğœ–)} ,
KG ğ‘† KG ğ€ ğ‘– ğ‘¡ ğ‘† ğ’™â€² ğ‘¡ ğ€ ğ‘–
ğ‘–=1 ğ‘–=1
where ğœ– âˆ¼ ğ’©(0,1) are i.i.d. for ğ‘– = 1,â€¦,ğ‘†. The one-shot KG approach absorbs the nested
ğ‘–
optimizationoverğ’™â€²intoasimultaneousjointoptimizationoverğ’™andameanmaximizerforeachof
theSsamples,ğ’™â€² 1,...,ğ’™â€²
ğ‘†
suchthatmax ğ’™ğ›¼ KG(ğ’™;ğ’Ÿ ğ‘¡)â‰ˆmax ğ’™,ğ’™â€² 1,...,ğ’™â€² ğ‘†ğ›¼ 1-KG(ğ’™;ğ’Ÿ),where
1 âˆ‘ğ‘† 1 âˆ‘ğ‘† [ ]
ğ›¼ (ğ’™;ğ’Ÿ )â‰œ ğ‘¢ (ğ’™,ğ’™â€²,ğ‘¦ (ğ’™;ğœ–);ğ’Ÿ )= ğ”¼ ğ‘“(ğ’™â€²)âˆ£ğ’Ÿ âˆª{ğ’™,ğ‘¦ (ğ’™;ğœ–)} ,
1-KG ğ‘¡ ğ‘† 1-KG ğ‘– ğ€ ğ‘– ğ‘¡ ğ‘† ğ‘– ğ‘¡ ğ€ ğ‘–
ğ‘–=1 ğ‘–=1
Evidently,thereisnolongeraninneroptimizationproblemoverğ’™â€². Toestimatetheğ‘–thtermofthis
sum,wedrawasampleoftheobjectivevalueofğ’™,ğ‘¦ (ğ’™;ğœ–),andconditionthemodelonthissample.
ğ€ ğ‘–
Wethencomputethenewposteriorpredictivemeanatğ’™â€². Aftersumming,wecomputegradients
ğ‘–
withrespecttoboththecandidateğ’™andthemeanmaximizersğ’™â€²,...,ğ’™â€². Again,weusetheâ€œsoftâ€
1 ğ‘†
versionofone-shotKGinourEULBOoptimizationproblem:
( ) ( [ ] )
ğ‘¢ ğ’™,ğ’™â€²,ğ‘¦;ğ’Ÿ =softplus ğ”¼ ğ‘“(ğ’™â€²)âˆ£ğ’Ÿ âˆª{(ğ’™,ğ‘¦)} âˆ’ğ‘+ ,
1-SKG ğ‘¡ ğ‘¡
wherethisutilityfunctioniscruciallyafunctionofbothğ’™andafreeparameterğ’™â€². Aswithğ›¼ ,
1-KG
maximizingtheEULBOcanbesetupasajointoptimizationproblem:
1 âˆ‘ğ‘† ( )
maximize â„’ (ğ€,ğ’,ğœ½)+ logğ‘¢ ğ’™,ğ’™â€²,ğ‘¦ (ğ’™;ğœ–);ğ’Ÿ (10)
ğ’™,ğ’™â€²,...,ğ’™â€²,ğ€,ğ’,ğœ½ ELBO ğ‘† 1-SKG ğ‘– ğ€ ğ‘– ğ‘¡
1 ğ‘† ğ‘–=1
EfficientKG-EULBOComputation. Computingthenon-ELBOterminEquation10isdominated
[ ]
byhavingtocomputeğ”¼ ğ‘“(ğ’™â€²)âˆ£ğ’Ÿ âˆª{(ğ’™,ğ‘¦ (ğ’™;ğœ–))} ğ‘†-times. Noticethatweonlyneedtocompute
ğ‘– ğ‘¡ ğ€ ğ‘–
anupdatedposteriorpredictivemean,andcanignorepredictivevariances. Forthis,wecanleverage
onlineupdating(Maddoxetal.,2021). Inparticular,thepredictivemeancanbeupdatedinğ’ª(ğ‘š2)
time using a simple Cholesky update. The additional ğ’ª(ğ‘†ğ‘š2) cost of computing the EULBO is
thereforeamortizedbytheoriginalğ’ª(ğ‘š3)costofcomputingtheELBO.
3.4 Extensiontoq-EULBOforBatchBayesianOptimization
TheEULBOcanbeextendedtosupportbatchBayesianoptimizationbyusingtheMonteCarlobatch
modeanalogsofutilityfunctionsasdiscussede.g. byBalandatetal.(2020);Wilsonetal.(2018).
[ ]
Givenasetofcandidatesğ‘¿ = ğ’™ ,...,ğ’™ ,theğ‘-improvementutilityfunctionisgivenby:
1 ğ‘
( ( ) )
ğ‘¢ (ğ‘¿,ğ’‡;ğ’Ÿ )â‰œ max ReLU ğ‘“ ğ’™ âˆ’ğ‘¦âˆ— (q-EI;Balandatetal.,2020;Wilsonetal.,2018)
q-I ğ‘¡ ğ‘— ğ‘¡
ğ‘—=1...ğ‘
Thisutilitycanagainbesoftenedas:
( ( ) )
ğ‘¢ (ğ‘¿,ğ’‡;ğ’Ÿ )â‰œ max softplus ğ‘“ ğ’™ âˆ’ğ‘¦âˆ—
q-SI ğ‘¡ ğ‘— ğ‘¡
ğ‘—=1â€¦ğ‘
Because this is now a ğ‘-dimensional integral, Gauss-Hermite quadrature is no longer applicable.
However,wecanapplyMonteCarloas
1 âˆ‘ğ‘† ( )
ğ”¼ logğ‘¢ (ğ‘¿,ğ’‡;ğ’Ÿ )â‰ˆ max softplus ğ‘¦ (ğ’™;ğœ–)âˆ’ğ‘¦âˆ— .
ğ‘ğ€(ğ‘“) ğ‘-SI ğ‘¡ ğ‘† ğ‘—=1...ğ‘ ğ€ ğ‘– ğ‘¡
ğ‘–=1
AsdoneintheBoTorchsoftwarepackage(Balandatetal.,2020),weobservethatfixingthesetof
basesamplesğœ– ,...,ğœ– duringeachBOiterationresultsinbetteroptimizationperformanceatthecost
1 ğ‘†
ofnegligibleq-EULBObias. Now,optimizingtheq-EULBOisdoneoverthefullsetofğ‘candidates
ğ’™ ,...,ğ’™ jointly,aswellastheGPhyperparameters,inducingpoints,andvariationalparameters.
1 ğ‘
6KnowledgeGradient. TheKGversionoftheEULBOcanbesimilarlyextended. Theexpectedlog
utilityterminthemaximizationproblemEq.(10)becomes:
1
âˆ‘ğ‘†
maximize â„’ (ğ€,ğ’,ğœ½)+ max logğ‘¢ (ğ’™ ,ğ’™â€²,ğ‘¦ (ğ’™;ğœ–);ğ’Ÿ ),
ğ’™1,...,ğ’™ğ‘,ğ’™â€² 1,...,ğ’™â€² ğ‘†,ğ€,ğ’,ğœ½ ELBO ğ‘† ğ‘–=1ğ‘—=1..ğ‘ 1-SKG ğ‘— ğ‘– ğ€ ğ‘– ğ‘¡
resultinginasimilaranalogtoq-KGasdescribedbyBalandatetal.(2020).
3.5 OptimizingtheEULBO
OptimizingtheELBOforSVGPsisknowntobechallenging(Galy-FajouandOpper,2021;Terenin
etal.,2024)astheoptimizationlandscapefortheinducingpointsisnon-convex,multi-modal,and
non-smooth. Naturally,thesearealsochallengesforEULBO.Inpractice,wefoundthatcaremustbe
takenwhenimplementingandinitializingtheEULBOmaximizationproblem. Inthissubsection,we
outlinesomekeyideas,whileadetaileddescriptionwithpseudocodeispresentedinAppendixA.
InitializationandWarm-Starting. Wewarm-starttheEULBOmaximizationprocedurebysolving
theconventionaltwo-stepschemeinEq.(5): AteachBOiteration, weobtaintheâ€œwarmâ€initial
valuesfor(ğ€,ğ’,ğœ½)byoptimizingthestandardELBO.Then,weusethistomaximizetheconventional
acquisitionfunctioncorrespondingtothechosenutilityfunctionğ‘¢(theexpectationofğ‘¢overğ‘ (ğ‘“)),
ğ€
whichprovidesthewarm-startinitializationforğ’™.
AlternatingMaximizationScheme. Tooptimizeâ„’ (ğ’™,ğ€,ğ’,ğœ½),wealternatebetweenopti-
EULBO
mizingoverthequeryğ’™andtheSVGPparametersğ€,ğ’,ğœ½. Wefindblock-coordinatedescenttobe
morestableandrobustthanjointlyupdatingallparameters,thoughthereasonwhythisismorestable
thanjointlyoptimizingallparametersrequiresfurtherinvestigation.
4 Experiments
We evaluate EULBO-based SVGPs on a number of benchmark BO tasks, described in detail in
Section4.1. Thesetasksincludestandardlow-dimensionalBOproblems,e.g.,the6DHartmann
function,aswellas7high-dimensionalandhigh-throughputoptimizationtasks.
Baselines. WecompareEULBOtoseveralbaselineswiththemaingoalofachievingahighreward
usingasfewfunctionevaluationsaspossible. OurprimarypointofcomparisonisELBO-basedSVGPs.
Weconsidertwoapproachesforinducingpointlocations: 1.optimizinginducingpointlocationsvia
theELBO(denotedasELBO),2.placingtheinducingpointsusingthestrategyproposedbyMoss
etal.(2023)ateachstageofELBOoptimization(denotedasMossetal.). Thelatteroffersimproved
BOperformanceoverstandardELBO-SVGPinBOsettings,yetâ€”unlikeourmethodâ€”itexclusively
targetsinducingpointplacementanddoesnotaffectvariationalparametersorhyperparametersofthe
model. Inaddition,wecomparetoBOusingexactGPsusing2,000functionevaluationsastheuse
ofexactGPisintractablebeyondthispointduetotheneedtorepeatedlyfitmodels.
AcquisitionfunctionsandBOalgorithms. ForEULBO,wetesttheversionsbasedonboththe
Expected Improvement (EI) and Knowledge Gradient (KG) acquisition functions as well as the
batchvariant. WetestthebaselinemethodsusingEIonly. Onhigh-dimensionaltasks(taskswith
dimensionalityabove10),werunEULBOandbaselinemethodswithstandardBOandwithtrustregion
Bayesianoptimization(TuRBO)(Erikssonetal.,2019). Forthelargesttasks(Lasso,Molecules)we
useacquisitionbatchsizeof20(ğ‘ =20),andbatchsize1(ğ‘ =1)forallothers.
ImplementationDetailsandHyperparameters. Wewillprovidecodetoreproduceallresults
inthepaperinapublicGitHubrepository. WeimplementEULBOandbaselinemethodsusingthe
GPyTorch(Gardneretal.,2018)andBoTorch(Balandatetal.,2020)packages. Forallmethods,we
initializeusingasetof100datapointssampleduniformlyatrandominthesearchspace. Weusethe
sametrustregionhyperparametersasin(Erikssonetal.,2019). InAppendixB.1,wealsoevaluatean
additionalinitializationstrategyforthemoleculardesigntasks. Thisalternativeinitializationmatches
priorworkinusing10,000moleculesfromtheGuacaMoldatasetBrownetal.(2019)ratherthanthe
detailsweusedaboveforconsistencyacrosstasks,butdoesachievehigheroverallperformance.
4.1 Tasks
Hartmann6D. ThewidelyusedHartmannbenchmarkfunction(SurjanovicandBingham,2013).
7Figure 2: Optimization results on the 8 considered tasks. We compare all methods for both
standardBOandTuRBO-basedBO(onalltasksexceptHartmann). Eachline/shadedregionrepresents
themean/standarderrorover20runsSeesubsectionB.1foradditionalmoleculeresults.
Figure 3: Ablation study measuring the impact of EULBO optimization on various SVGP
parameters. At each BO iteration, we use the standard ELBO objective to optimize the SVGP
hyperparameters,variationalparameters,andinducingpointlocations. Wethenrefinesomesubsetof
theseparametersbyfurtheroptimizingthemwithrespecttotheEULBOobjective.
LunarLander. Thegoalofthistaskistofindanoptimal12-dimensionalcontrolpolicythatallows
an autonomous lunar lander to consistently land without crashing. The final objective value we
optimizeistherewardobtainedbythepolicyaveragedoverasetof50randomlandingterrains. For
thistask,weusethesamecontrollersetupusedbyErikssonetal.(2019).
Rover. TherovertrajectoryoptimizationtaskintroducedbyWangetal.(2018)consistsoffinding
a60-dimensionalpolicythatallowsarovertomovealongsometrajectorywhileavoidingasetof
obstacles. WeusethesameobstaclesetupasinMausetal.(2023).
LassoDNA. Weoptimizethe180âˆ’dimensionalDNAtaskfromtheLassoBenchlibrary(Å ehiÄ‡
etal.,2022)ofbenchmarksbasedonweightedLASSOregression(Gassoetal.,2009).
Moleculardesigntasks(x4). WeselectfourchallengingtasksfromtheGuacamolbenchmark
suiteofmoleculardesigntasks(Brownetal.,2019): OsimertinibMPO,FexofenadineMPO,Median
Molecules1,andMedianMolecules2. WeusetheSELFIES-VAEintroducedbyMausetal.(2022)
toenablecontinuous256dimensionaloptimization.
84.2 Optimizationresults
InFigure2,weplottherewardofthebestpointfoundbytheoptimizerafteragivennumberoffunction
evaluations. Errorbarsshowthestandarderrorofthemeanover20replicateruns. EULBOwithTuRBO
outperformstheotherbaselineswithTuRBO.Similarly, EULBOwithstandardBOoutperformsthe
otherstandardBObaselines. Onenoteworthyobservationisthatneitheracquisitionfunctionappears
toconsistentlyoutperformtheother. However,EULBO-SVGPalmostalwaysdominatesELBO-SVGP
andoftenrequiresasmallfractionofthenumberoforaclecallstoachievecomparableperformance.
These results suggest that coupling data acquisition with approximate inference/model selection
resultsinsignificantlymoresample-efficientoptimization.
4.3 AblationStudy
WhiletheresultsinFig.2demonstratethatEULBO-SVGPimprovestheBOperformanceitisnot
immediatelycleartowhatextentjointoptimizationmodifiestheposteriorapproximationbeyondwhat
isobtainedbystandardELBOoptimization. Tothatend,inFig.3werefineanELBO-SVGPmodel
withvaryingdegreesofadditionalEULBOoptimization. AteveryBOiterationwebeginbyobtaininga
SVGPmodel(wherethevariationalparameters,inducingpointlocations,andGPhyperparametersare
allobtainedbyoptimizingthestandardELBOobjective). Wethenrefinesomesubsetofparameters
(eithertheinducingpoints,thevariationalparameters,theGPhyperparameters,oralloftheabove)
throughadditionaloptimizationwithrespecttotheEULBOobjective. Interestingly,wefindthattasks
responddifferentlytothevaryinglevelsofEULBOrefinement. InthecaseofLassoDNA,thereisnot
muchofadifferencebetweenEULBOrefinementonallparametersversusrefinementonthevariational
parametersalone. Ontheotherhand,theperformanceonMedianMolecules2isclearlydominatedby
refinementonallparameters. Nevertheless,weseethatEULBOisalwaysbeneficial,whetherapplied
toallparametersorsomesubset.
5 RelatedWork
ScalingBayesianoptimizationtotheLarge-BudgetRegime. BOhastraditionallybeenconfined
tothesmall-budgetoptimizationregime. However,recentinterestinhigh-dimensionaloptimization
problemshasdemonstratedtheneedtoscaleBOtolargedataacquisitionbudgets. Forproblems
withğ‘‚(103)dataacquisitions, HernÃ¡ndez-Lobatoetal.(2017);Snoeketal.(2015);Springenberg
et al. (2016) consider Bayesian neural networks (BNN; Neal, 1996), McIntire et al. (2016) use
SVGP,andWangetal.(2018)turntoensemblesofsubsampledGPs. Forproblemswithâ‰« 1000
acquisitions,SVGPhasbecomethedefactoapproachtoalleviatecomputationalcomplexity(Griffiths
andHernÃ¡ndez-Lobato,2020;Mausetal.,2022,2023;Stantonetal.,2022;Trippetal.,2020;Vakili
etal.,2021). Asinthispaper, manyworkshaveproposedmodificationstoSVGPtoimproveits
performanceinBOapplications. Mossetal.(2023)proposedaninducingpointplacementbasedona
heuristicmodificationofdeterminantalpointprocesses(KuleszaandTaskar,2012),whichweused
forinitialization,whileMaddoxetal.(2021)proposedamethodforafastonlineupdatestrategyfor
SVGPs,whichweutilizefortheKGacquisitionstrategy.
Utility-CalibratedApproximateInference. ThiswasfirstproposedbyLacosteâ€“Julienetal.(2011),
wheretheauthorsuseacoordinateascentalgorithmtoperformloss-calibratedvariationalinference.
Sincethen,variousextensionshavebeenproposed: KuÅ›mierczyketal.(2019)leverageblack-box
variationalinference(Ranganathetal.,2014;TitsiasandLÃ¡zaro-Gredilla,2014); MoraisandPillow
(2022)useexpectation-propagation(EP;Minka,2001); Abbasnejadetal.(2015)employimportance
sampling; Cobbetal.(2018)andLiandZhang(2023)deriveaspecificvariantforBNNs;and (Wei
et al., 2021) derive a specific variant for GP classification. Closest to our work is the GP-based
recommendationmodellearningalgorithmbyAbbasnejadetal.(2013),whichsparsifiesanEP-based
GPapproximationbymaximizingautilitysimilartothoseusedinBO.
6 LimitationsandDiscussion
Themainlimitationofourproposedapproachisincreasedcomputationalcost. WhileEULBO-SVGP
stillretainstheğ‘‚(ğ‘š3)computationalcomplexityofstandardSVGP,ourpracticalimplementation
requiresawarm-start: firstfittingSVGPwiththeELBOlossandthenmaximizingtheacquisition
functionbeforejointlyoptimizingwiththeEULBOloss. Furthermore,EULBOoptimizationcurrently
requires multiple tricks such as clipping and block-coordinate updates. In future work, we aim
9to develop a better understanding of the EULBO geometry in order to develop developing more
stable,efficient,andeasy-to-useEULBOoptimizationschemes. Nevertheless,ourresultsinSection4
demonstratethattheadditionalcomputationofEULBOyieldssubstantialimprovementsinBOdata-
efficiency, a desirable trade-off in many applications. Moreover, EULBO-SVGP is modular, and
ourexperimentscaptureafractionofitspotentialuse. Itcanbeappliedtoanydecision-theoretic
acquisitionfunction,anditislikelycompatiblewithnon-standardBayesianoptimizationproblems
suchascost-constrainedBO(Snoeketal.,2012),causalBO(Agliettietal.,2020),andmanymore.
Moreimportantly,ourpaperhighlightsanewavenueforresearchinBO,whereourpaperisthefirst
tojointlyconsidersurrogatemodeling,approximateinference,anddataselection. Extendingthis
ideatoGPapproximationsbeyondSVGPandacquisitionfunctionsbeyondEI/KGmayyieldfurther
improvements,especiallyintheincreasinglypopularhigh-throughputBOsetting.
10References
EhsanAbbasnejad,JustinDomke,andScottSanner. Loss-calibratedMonteCarloactionselection. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume29ofAAAI.AAAIPress,
March2015. (page9)
M.EhsanAbbasnejad, EdwinV.Bonilla, andScottSanner. Decision-theoreticsparsificationfor
Gaussianprocesspreferencelearning.InMachineLearningandKnowledgeDiscoveryinDatabases,
volume13717ofLNCS,pages515â€“530,Berlin,Heidelberg,2013.Springer. (page9)
VirginiaAglietti,XiaoyuLu,AndreiPaleyes,andJavierGonzÃ¡lez. CausalBayesianoptimization. In
ProceedingsoftheInternationalConferenceonArtificialIntelligenceandStatistics,volume108of
PMLR,pages3155â€“3164.JMLR,June2020. (page10)
Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, An-
drew Gordon Wilson, and Eytan Bakshy. BoTorch: A framework for efficient Monte-Carlo
Bayesianoptimization. InAdvancesinNeuralInformationProcessingSystems,volume33,pages
21524â€“21538.CurranAssociates,Inc.,2020. (pages2,6,7,16)
P.G.Bissiri,C.C.Holmes,andS.G.Walker. Ageneralframeworkforupdatingbeliefdistributions.
JournaloftheRoyalStatisticalSocietySeriesB:StatisticalMethodology,78(5):1103â€“1130,2016.
(pages2,5)
DavidM.Blei,AlpKucukelbir,andJonD.McAuliffe.Variationalinference: Areviewforstatisticians.
JournaloftheAmericanStatisticalAssociation,112(518):859â€“877,April2017. (pages2,3)
NathanBrown,MarcoFiscato,MarwinH.S.Segler,andAlainC.Vaucher. Guacamol: Benchmarking
models for de novo molecular design. Journal of Chemical Information and Modeling, 59(3):
1096â€“1108,Mar2019. (pages7,8)
Adam D. Cobb, Stephen J. Roberts, and Yarin Gal. Loss-Calibrated Approximate Inference in
BayesianNeuralNetworks. arXivPreprintarXiv:1805.03901,arXiv,May2018. (page9)
A.P.Dempster,N.M.Laird,andD.B.Rubin. Maximumlikelihoodfromincompletedataviathe
EMalgorithm. JournaloftheRoyalStatisticalSociety: SeriesB(Methodological),39(1):1â€“22,
September1977. (page4)
DavidEriksson,MichaelPearce,JacobGardner,RyanDTurner,andMatthiasPoloczek. Scalable
globaloptimizationvialocalBayesianoptimization. InAdvancesinNeuralInformationProcessing
Systems,volume32,pages5496â€“5507.CurranAssociates,Inc.,2019. (pages1,2,7,8)
PeterIFrazier. Knowledge-gradientmethodsforstatisticallearning. PhDthesis,PrincetonUniversity
Princeton,2009. (page5)
PeterIFrazier. AtutorialonBayesianoptimization. arXivPreprintarXiv:1807.02811,ArXiv,2018.
(page1)
ThÃ©oGaly-FajouandManfredOpper. AdaptiveinducingpointsselectionforGaussianprocesses.
arXivPreprintarXiv:2107.10066,arXiv,2021. (page7)
JacobGardner,GeoffPleiss,KilianQ.Weinberger,DavidBindel,andAndrewG.Wilson. GPyTorch:
Blackboxmatrix-matrixGaussianprocessinferencewithGPUacceleration. InAdvancesinNeural
Information Processing Systems, volume 31, pages 7576â€“7586. Curran Associates, Inc., 2018.
(pages7,16)
RomanGarnett. BayesianOptimization. CambridgeUniversityPress,Cambridge,UnitedKingdom;
NewYork,NY,2023. (pages1,2,3,5)
GillesGasso,AlainRakotomamonjy,andStÃ©phaneCanu. Recoveringsparsesignalswithacertain
familyofnonconvexpenaltiesandDCprogramming. IEEETransactionsonSignalProcessing,57
(12):4686â€“4698,2009. (page8)
Ryan-RhysGriffithsandJosÃ©MiguelHernÃ¡ndez-Lobato. ConstrainedBayesianoptimizationfor
automatic chemical design using variational autoencoders. Chemical Science, 11(2):577â€“586,
2020. (pages1,9)
JamesHensman,NicoloFusi,andNeilD.Lawrence. Gaussianprocessesforbigdata. InProceedings
of the Conference on Uncertainty in Artificial Intelligence, pages 282â€“290. AUAI Press, 2013.
(pages1,3)
11JosÃ©MiguelHernÃ¡ndez-Lobato,JamesRequeima,EdwardO.Pyzer-Knapp,andAlÃ¡nAspuru-Guzik.
ParallelanddistributedThompsonsamplingforlarge-scaleacceleratedexplorationofchemical
space. InProceedingsoftheInternationalConferenceonMachineLearning,volume70ofPMLR,
pages1470â€“1479.JMLR,July2017. (page9)
PrateekJaiswal,HarshaHonnappa,andVinayakA.Rao. Asymptoticconsistencyofloss-calibrated
variationalBayes. Stat,9(1):e258,2020. (pages2,5)
PrateekJaiswal,HarshaHonnappa,andVinayakRao. Onthestatisticalconsistencyofrisk-sensitive
bayesiandecision-making. InAdvancesinNeuralInformationProcessingSystems,volume36,
pages53158â€“53200.CurranAssociates,Inc.,December2023. (pages2,5)
DonaldR.Jones,MatthiasSchonlau,andWilliamJ.Welch.Efficientglobaloptimizationofexpensive
black-boxfunctions. JournalofGlobalOptimization,13(4):455â€“492,1998. (pages1,2,5)
MichaelI.Jordan,ZoubinGhahramani,TommiS.Jaakkola,andLawrenceK.Saul. Anintroduction
tovariationalmethodsforgraphicalmodels. MachineLearning,37(2):183â€“233,1999. (pages1,2,
3,4)
DiederikP.KingmaandJimmyBa. Adam: AMethodforStochasticOptimization. InProceedings
oftheInternationalConferenceonLearningRepresentations,SanDiego,California,USA,2015.
(pages15,16)
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the
InternationalConferenceonLearningRepresentations,Banff,AB,Canada,April2014. (page6)
JeremiasKnoblauch,JackJewson,andTheodorosDamoulas. Anoptimization-centricviewonBayesâ€™
rule: Reviewingandgeneralizingvariationalinference. JournalofMachineLearningResearch,23
(132):1â€“109,2022. (pages2,5)
AlexKuleszaandBenTaskar. Determinantalpointprocessesformachinelearning. Foundationsand
TrendsÂ®inMachineLearning,5(2â€“3):123â€“286,2012. (page9)
TomaszKuÅ›mierczyk,JosephSakaya,andArtoKlami. VariationalBayesiandecision-makingfor
continuousutilities. InAdvancesinNeuralInformationProcessingSystems,volume32,pages
6395â€“6405.CurranAssociates,Inc.,2019. (pages5,9)
Simon Lacosteâ€“Julien, Ferenc HuszÃ¡r, and Zoubin Ghahramani. Approximate inference for the
loss-calibratedBayesian. InProceedingsoftheInternationalConferenceonArtificialIntelligence
andStatistics,volume15ofPMLR,pages416â€“424.JMLR,June2011. (pages2,4,9)
Kenneth Lange. MM Optimization Algorithms. Society for Industrial and Applied Mathematics,
Philadelphia,2016. (pages2,4)
BolianLiandRuqiZhang. Long-tailedClassificationfromaBayesian-decision-theoryPerspective.
arXivPreprintarXiv:2303.06075,arXiv,2023. (page9)
WesleyJMaddox,SamuelStanton,andAndrewGWilson. ConditioningsparsevariationalGaussian
processesforonlinedecision-making. InAdvancesinNeuralInformationProcessingSystems,
volume34,pages6365â€“6379.CurranAssociates,Inc.,2021. (pages1,2,4,6,9)
Alexander G. de G. Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani. On
sparsevariationalmethodsandtheKullback-Leiblerdivergencebetweenstochasticprocesses. In
ProceedingsoftheInternationalConferenceonArtificialIntelligenceandStatistics,volume51of
PMLR,pages231â€“239.JMLR,May2016. (pages1,4)
NatalieMaus,HaydnJones,JustonMoore,MattJ.Kusner,JohnBradshaw,andJacobGardner. Local
latent space Bayesian optimization over structured inputs. In Advances in Neural Information
ProcessingSystems,volume35,pages34505â€“34518,December2022. (pages1,8,9)
NatalieMaus,KaiwenWu,DavidEriksson,andJacobGardner. Discoveringmanydiversesolutions
withBayesianoptimization.InProceedingsoftheInternationalConferenceonArtificialIntelligence
andStatistics,volume206,pages1779â€“1798.PMLR,April2023. (pages1,8,9)
Mitchell McIntire, Daniel Ratner, and Stefano Ermon. Sparse Gaussian Processes for Bayesian
Optimization. InProceedingsoftheConferenceonUncertaintyinArtificialIntelligence,Jersey
City,NewJersey,USA,2016.AUAIPress. (page9)
ThomasP.Minka. Expectationpropagationforapproximatebayesianinference. InProceedingsof
theConferenceonUncertaintyinArtificialIntelligence,pages362â€“369,SanFrancisco,CA,USA,
2001.MorganKaufmannPublishersInc. (page9)
12JonasMockus. TheBayesianapproachtoglobaloptimization. InSystemModelingandOptimization,
pages473â€“481.Springer,1982. (page1)
MichaelJ.MoraisandJonathanW.Pillow. Loss-calibratedexpectationpropagationforapproximate
Bayesiandecision-making. TechnicalReportarXiv:2201.03128,arXiv,January2022. (page9)
HenryB.Moss,SebastianW.Ober,andVictorPicheny. InducingpointallocationforsparseGaussian
processesinhigh-throughputBayesianoptimisation.InProceedingsoftheInternationalConference
onArtificialIntelligenceandStatistics,volume206ofPMLR,pages5213â€“5230.JMLR,April
2023. (pages1,4,7,9,16,17,18)
RadfordM.Neal. BayesianLearningforNeuralNetworks,volume118ofLectureNotesinStatistics.
SpringerNewYork,NewYork,NY,1996. (page9)
JoaquinQuiÃ±onero-CandelaandCarlEdwardRasmussen. Aunifyingviewofsparseapproximate
Gaussianprocessregression. JournalofMachineLearningResearch,6(65):1939â€“1959,2005.
(page1)
RajeshRanganath,SeanGerrish,andDavidBlei. Blackboxvariationalinference. InProceedingsof
theInternationalConferenceonArtificialIntelligenceandStatistics,volume33ofPMLR,pages
814â€“822.JMLR,April2014. (page9)
CarlEdwardRasmussenandChristopherK.I.Williams. GaussianProcessesforMachineLearning.
TheMITPress,November2005. (page1)
DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. Stochasticbackpropagationand
approximateinferenceindeepgenerativemodels. InProceedingsoftheInternationalConference
onMachineLearning,volume32ofPMLR,pages1278â€“1286.JMLR,June2014. (page6)
ChristianP.Robert. TheBayesianChoice: FromDecision-TheoreticFoundationstoComputational
Implementation. SpringerTextsinStatistics.Springer,NewYorkBerlinHeidelberg,2.ededition,
2001. (pages2,3)
BobakShahriari,KevinSwersky,ZiyuWang,RyanPAdams,andNandoDeFreitas. Takingthe
human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1):
148â€“175,2015. (pages1,5)
Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In
AdvancesinNeuralInformationProcessingSystems,volume18,pages1257â€“1264.MITPress,
2005. (page5)
JasperSnoek,HugoLarochelle,andRyanPAdams. PracticalBayesianoptimizationofmachine
learningalgorithms. Advancesinneuralinformationprocessingsystems,25:2951â€“2959,2012.
(page10)
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
MostofaPatwary,MrPrabhat,andRyanAdams. ScalableBayesianoptimizationusingdeepneural
networks. InProceedingsoftheInternationalConferenceonMachineLearning,volume37of
PMLR,pages2171â€“2180.JMLR,June2015. (page9)
JostTobiasSpringenberg,AaronKlein,StefanFalkner,andFrankHutter. BayesianOptimization
withRobustBayesianNeuralNetworks. InAdvancesinNeuralInformationProcessingSystems,
volume29,pages4134â€“4142.CurranAssociates,Inc.,2016. (page9)
SamuelStanton,WesleyMaddox,NateGruver,PhillipMaffettone,EmilyDelaney,PeytonGreenside,
andAndrewGordonWilson. AcceleratingBayesianoptimizationforbiologicalsequencedesign
withdenoisingautoencoders.InProceedingsoftheInternationalConferenceonMachineLearning,
volume162ofPMLR,pages20459â€“20478.JMLR,June2022. (pages1,9)
SonjaSurjanovicandDerekBingham. Virtuallibraryofsimulationexperiments: Testfunctionsand
datasets,2013. (page7)
AlexanderTerenin,DavidR.Burt,ArtemArtemev,SethFlaxman,MarkvanderWilk,CarlEdward
Rasmussen,andHongGe. NumericallystablesparseGaussianprocessesviaminimumseparation
usingcovertrees. JournalofMachineLearningResearch,25(26):1â€“36,2024. (page7)
Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In
ProceedingsoftheInternationalConferenceonArtificialIntelligenceandStatistics,volume5of
PMLR,pages567â€“574.JMLR,April2009. (pages1,3)
13MichalisTitsiasandMiguelLÃ¡zaro-Gredilla. DoublystochasticvariationalBayesfornon-conjugate
inference. InProceedingsoftheInternationalConferenceonMachineLearning,volume32of
PMLR,pages1971â€“1979.JMLR,June2014. (pages6,9)
AustinTripp,ErikDaxberger,andJosÃ©MiguelHernÃ¡ndez-Lobato. Sample-efficientoptimization
in the latent space of deep generative models via weighted retraining. In Advances in Neural
InformationProcessingSystems,volume33,pages11259â€“11272.CurranAssociates,Inc.,2020.
(pages1,9)
Sattar Vakili, Henry Moss, Artem Artemev, Vincent Dutordoir, and Victor Picheny. Scalable
ThompsonsamplingusingsparseGaussianprocessmodels. InAdvancesinNeuralInformation
ProcessingSystems,volume34,pages5631â€“5643,2021. (pages1,9)
KenanÅ ehiÄ‡,AlexandreGramfort,JosephSalmon,andLuigiNardi.Lassobench: Ahigh-dimensional
hyperparameteroptimizationbenchmarksuiteforLASSO. InProceedingsoftheInternational
ConferenceonAutomatedMachineLearning,volume188ofPMLR,pages2/1â€“24.JMLR,25â€“27
Jul2022. (page8)
YixinWangandDavidM.Blei.FrequentistconsistencyofvariationalBayes.JournaloftheAmerican
StatisticalAssociation,114(527):1147â€“1161,July2019. (page5)
ZiWang,ClementGehring,PushmeetKohli,andStefanieJegelka. Batchedlarge-scalebayesian
optimization in high-dimensional spaces. In Proceedings of the International Conference on
ArtificialIntelligenceandStatistics,volume84ofPMLR,pages745â€“754.JMLR,March2018.
(pages8,9)
LarryWasserman. Allofstatistics: aconcisecourseinstatisticalinference. SpringerScience&
BusinessMedia,2013. (pages2,3)
YadiWei,RishitSheth,andRoniKhardon. DirectlossminimizationforsparseGaussianprocesses.
InProceedingsoftheInternationalConferenceonArtificialIntelligenceandStatistics,volume130
ofPMLR,pages2566â€“2574.JMLR,March2021. (page9)
JamesWilson,FrankHutter,andMarcDeisenroth. MaximizingacquisitionfunctionsforBayesian
optimization. InAdvancesinNeuralInformationProcessingSystems,pages9884â€“9895.Curran
Associates,Inc.,2018. (pages2,6)
Jian Wu, Matthias Poloczek, Andrew G Wilson, and Peter Frazier. Bayesian optimization with
gradients. InAdvancesinNeuralInformationProcessingSystems,volume30,pages5267â€“5278.
CurranAssociates,Inc.,2017. (page2)
14A ImplementationDetails
Wewillnowprovideadditionaldetailsontheimplementation. Fortheimplementation, wetreat
the SVGP parameters, such as the variational parameters ğ€, inducing point locations ğ’, and
hyperparametersğœ½,equally. Therefore,forclarity,wewillcollectivelydenotethemasğ’˜=(ğ€,ğ’,ğœ½)
suchthatğ’˜âˆˆğ’² =Î›Ã—ğ’³ğ‘šÃ—Î˜,andtheresultingSVGPvariationalapproximationasğ‘ . Then,
ğ’˜
theELBOandEULBOareequivalentlydenotedasfollows:
â„’ (ğ’˜;ğ’Ÿ)â‰œâ„’ (ğ€,ğ’,ğœ½;ğ’Ÿ)
ELBO ELBO
â„’ (ğ’™,ğ’˜;ğ’Ÿ ,ğ’Ÿ )â‰œğ”¼ logğ‘¢(ğ’™,ğ‘“;ğ’Ÿ )+â„’ (ğ’˜;ğ’Ÿ ).
EULBO ğ’™ ğ’˜ ğ‘“âˆ¼ğ‘ğ’˜(ğ‘“) ğ’™ ELBO ğ’˜
Also,noticethattheâ„’ separatelydenotethedatasettobepassedtotheutilityandtheELBO.
EULBO
(Settingğ’Ÿ =ğ’Ÿ =ğ’Ÿ retrievestheoriginalformulationinEq.(8).)
ğ‘¡ ğ’˜ ğ’™
AlternatingUpdates Weperformblock-coordinateascentontheEULBObyalternatingbetween
maximizingoverğ’™asğ’˜. Usingvanillagradientdescent,theğ’™-updateisequivalentto
ğ’™â†ğ’™+ğ›¾ âˆ‡ â„’ (ğ’™,ğ’˜;ğ’Ÿ)=ğ’™+ğ›¾ âˆ‡ ğ”¼ logğ‘¢(ğ’™,ğ‘“;ğ’Ÿ),
ğ’™ ğ’™ EULBO ğ’™ ğ’™ ğ‘“âˆ¼ğ‘ğ’˜(ğ‘“)
whereğ›¾ isthestepsize. Ontheotherhand,fortheğ’˜-update,wesubsamplethedatasuchthatwe
ğ’™
optimizetheELBOoveraminibatchğ‘† âŠ‚ğ’Ÿofsizeğµ =|ğ‘†|as
( )
ğ’˜â†ğ’˜+ğ›¾ âˆ‡ â„’ (ğ’™,ğ’˜;ğ‘†,ğ’Ÿ)=ğ’˜+ğ›¾ âˆ‡ ğ”¼ logğ‘¢(ğ’™,ğ‘“;ğ’Ÿ)+â„’ (ğ’˜;ğ‘†) ,
ğ’˜ ğ’˜ EULBO ğ’˜ ğ’˜ ğ‘“âˆ¼ğ‘ğ’˜(ğ‘“) ELBO
whereğ›¾ isthestepsize. Naturally,theğ’˜-updateisstochasticduetominibatching,whiletheğ’™-update
ğ’˜
isdeterministic. Inpractice,weleveragetheAdamupdaterule(KingmaandBa,2015)insteadof
simplegradientdescent. Togetherwithgradientclipping,thisalternatingupdateschemeismuch
morerobustthanjointlyupdating(ğ’™,ğ’˜).
Algorithm1:EULBOMaximizationPolicy
Input: SVGPparametersğ’˜ =(ğ€ ,ğ’ ,ğœ½ ),Datasetğ’Ÿ ,BOutilityfunctionğ‘¢,
0 0 0 0 ğ‘¡
Output: BOqueryğ’™
ğ‘¡+1
1
âŠ³ Compute Warm-Start Initializations
2 ğ’˜â†argmax ğ’˜âˆˆğ’²â„’ ELBO(ğ’˜;ğ’Ÿ ğ‘¡)withğ’˜ 0asinitialization.
3 ğ’™â†argmax ğ’™âˆˆğ’³âˆ« ğ‘¢(ğ’™,ğ‘“;ğ’Ÿ ğ‘¡)ğ‘ ğ’˜(ğ‘“)dğ‘“
4
âŠ³ Maximize EULBO
5 repeat
âŠ³ Update posterior approximation ğ‘
ğ’˜
6 Fetchminibatchğ‘†fromğ’Ÿ ğ‘¡
7 Computeğ’ˆ ğ’˜ â†âˆ‡ ğ’˜â„’ EULBO(ğ’™,ğ’˜;ğ‘†,ğ’Ÿ ğ‘¡)
8 Clipğ’ˆ ğ’˜withthresholdğº clip
9 ğ’˜â†AdamStep ğ›¾ğ’˜(ğ’™,ğ’ˆ ğ’˜)
10
âŠ³ Update BO query ğ’™
11 Computeğ’ˆ ğ’™ â†âˆ‡ ğ’™â„’ EULBO(ğ’™,ğ’˜;ğ‘†,ğ’Ÿ ğ‘¡)
12 Clipğ’ˆ ğ’™withthresholdğº clip
13 ğ’™â†AdamStep ğ›¾ğ’™(ğ’™,ğ’ˆ ğ’™)
14 ğ’™â†proj (ğ’™)
ğ’³
15 untiluntilconverged
16 ğ’™ ğ‘¡+1 â†ğ’™
17
OverviewofPseudocode. Thecompletehigh-levelviewofthealgorithmispresentedinAlgorithm1,
exceptfortheacquisition-specificdetails. AdamStep (ğ’™,ğ’ˆ)appliestheAdamstepsizerule(Kingma
ğ›¾
andBa,2015)tothecurrentlocationğ’™withthegradientestimateğ’ˆandthestepsizeğ›¾. Inpractice,
Adamisaâ€œstatefulâ€optimizer,whichmaintainstwoscalar-valuedstatesforeachscalarparameter.
Forthis,were-initializetheAdamstatesatthebeginningofeachBOstep.
15Initialization. IntheinitialBOstepğ‘¡ = 0, weinitializeğ’ withtheDPP-basedinducingpoint
0
selectionstrategyofMossetal.(2023). FortheremainingSVGPparametersğ€ andğœ½ ,weusedthe
0 0
defaultinitializationofGPyTorch(Gardneretal.,2018). FortheremainingBOstepsğ‘¡ >0,weuseğ’˜
fromthepreviousBOstepastheinitializationğ’˜ ofthecurrentBOstep.
0
Warm-Starting. Duetothenon-convexityandmulti-modalityofboththeELBOandtheacquisition
function,itiscriticaltoappropriatelyinitializetheEULBOmaximizationprocedure. Asmentioned
inSection3.5,towarm-starttheEULBOmaximizationprocedure,weusetheconventional2-step
schemeEq.(5), wherewemaximizetheELBOandthenmaximizetheacquisitionfunction. For
ELBOmaximization,weapplyAdam(KingmaandBa,2015)withthestepsizesetasğ›¾ untilthe
ğ‘¤
convergencecriteria(describedbelow)aremet. Foracquisitionfunctionmaximization,weinvokethe
highlyoptimizedBoTorch.optimize.optimize_acqffunction(Balandatetal.,2020).
MinibatchSubsamplingStrategy. Ascommonlydone,weusethereshufflingsubsamplingstrategy
wherethedatasetğ’Ÿ isshuffledandpartitionedintominibatchesofsizeğµ. Thenumberofminibatches
ğ‘¡
constitutesanâ€œepoch.â€ Thedatasetisreshuffled/repartitionedaftergoingthroughafullepoch.
ConvergenceDetermination. ForbothmaximizingtheELBOduringwarm-startingandmaximizing
theEULBO,wecontinueoptimizationuntilwestopmakingprogressorexceedğ‘˜ numberof
epochs
epochs. ThatisiftheELBO/EULBOfunctionvaluefailstomakeprogressforğ‘› numberofsteps.
fail
Table1: ConfigurationsofHyperparametersusedfortheExperiments
Hyperparameter Value Description
ğ›¾ 0.001 ADAMstepsizeforthequeryğ’™
ğ’™
ğ›¾ 0.01 ADAMstepsizefortheSVGPparametersğ’˜
ğ’˜
ğµ 32 Minibatchsize
ğº 2.0 Gradientclippingthreshold
clip
ğ‘˜ 30 Maximumnumberofepochs
epochs
ğ‘› 3 Maximumnumberoffailuretoimprove
fail
ğ‘š 100 Numberofinducingpoints
ğ‘› =|ğ’Ÿ | 100 NumberofobservationsforinitializingBO
0 0
#quad. 20 NumberofGauss-Hermitequadraturepoints
optimize_acqf: restarts 10
optimize_acqf: raw_samples 256
optimize_acqf: batch_size 1âˆ•20 Dependsontask,seedetailsinSection4
Hyperparameters. ThehyperparametersusedinourexperimentsareorganizedinTable1. For
the full-extent of the implementation details and experimental configuration, please refer to the
supplementarycode.
16B AdditionalPlots
Weprovideadditionalresultsandplotsthatwereomittedfromthemaintext.
B.1 AdditionalResultsonMoleculeTasks
InFig.4,weprovideplotsonadditionalresultsthataresimilartothoseinFig.2. Onthreeofthe
moleculetasks,weuse10,000randommoleculesfromtheGuacaMoldatasetasinitialization. Thisis
moreconsistentwithwhathasbeendoneinpreviousworksandachievesbetteroveralloptimization
performance.
Figure4: Additionaloptimizationresultsonthreemoleculetasksusing10,000randommolecules
fromtheGuacaMoldatasetasinitialization. Eachline/shadedregionrepresentsthemean/standard
errorover20runs. Wecountoraclecallsstartingaftertheseinitializationevaluationsforallmethods.
B.2 SeparatePlotsforBOandTuRBOResults
Inthissection,weprovideadditionalplotsseparatingoutBOandTuRBOresultstomakevisualization
easier.
Figure 5: BO-only optimization results of Fig. 2. We compare EULBO-SVGP, ELBO-SVGP,
ELBO-SVGPwithDPPinducingpointplacement(Mossetal.,2023),andexactGPs. Thesearea
subsetofthesameresultsshowninFig.2. Eachline/shadedregionrepresentsthemean/standard
errorover20runs.
17Figure6: TuRBO-onlyoptimizationresultsofFig.2. WecompareEULBO-SVGP,ELBO-SVGP,
ELBO-SVGPwithDPPinducingpointplacement(Mossetal.,2023),andexactGPs. Thesearea
subsetofthesameresultsshowninFig.2. Eachline/shadedregionrepresentsthemean/standard
errorover20runs.
B.3 EffectofNumberofInducingPoints
Fortheresultswithapproximate-GPsinSection4,weusedğ‘š=100inducingpoints. InFig.7,we
evaluatetheeffectofusingalargernumberofinducingpoints(ğ‘š = 1024)forEULBO-SVGPand
ELBO-SVGP.
Lasso DNA
TuRBO (EULBO EI) w/ 1024 inducing points
TuRBO (EULBO EI)
0.29 TuRBO (ELBO EI) w/ 1024 inducing points
TuRBO (ELBO EI)
0.30
0.31
0.32
0.33
0.34
0 5000 10000 15000 20000
Number of Oracle Calls
Figure7: AblatingthenumberofinducingpointsusedbyEULBO-SVGPandELBO-SVGP.
AsinFig.2,wecomparerunningTuRBOwithEULBO-SVGPandwithELBO-SVGPusingğ‘š=100
inducingpointsusedforbothmethods. WeaddtwoadditionalcurvesforTuRBOwithEULBO-SVGP
andTuRBOwithELBO-SVGPusingğ‘š=1024inducingpoints. Eachline/shadedregionrepresents
themean/standarderrorover20runs.
Fig.7showsthatthenumberofinducingpointshaslimitedimpactontheoverallperformanceof
TuRBO,andEULBO-SVGPoutperformsELBO-SVGPregardlessofwhichthenumberofinducing
pointsused.
18
draweR
naeMC ComputeResources
Table2: InternalClusterSetup
Type ModelandSpecifications
SystemTopology 20nodeswith2socketseachwith24logicalthreads(total48threads)
Processor 1IntelXeonSilver4310,2.1GHz(maximum3.3GHz)persocket
Cache 1.1MiBL1,30MiBL2,and36MiBL3
Memory 250GiBRAM
Accelerator 1NVIDIARTXA5000pernode,2GHZ,24GBRAM
TypeofComputeandMemory. AllresultsinthepaperrequiredtheuseofGPUworkers(one
GPU per run of each method on each task). The majority of runs were executed on an internal
cluster,wheredetailsareshowninTable2,whereeachnodewasequippedwithanNVIDIARTX
A5000 GPU. In addition, we used cloud compute resources for a short period leading up to the
subsmissionofthepaper. Weused40RTX4090GPUworkersfromrunpod.io,whereeachGPU
hadapproximately24GBofGPUmemory. Whileweused24GBGPUsforourexperiments,each
runofourexperimentsonlyrequiresapproximately15GBofGPUmemory.
ExecutionTime. Eachoptimizationrunfornon-moleculetaskstakesapproximatelyonedayto
finish. Sincewerunthemoleculetasksouttoamuchlargernumberoffunctionevaluationsthan
othertasks(80000totalfunctionevaluationsforeachmoleculeoptimizationtask),eachmolecule
optimizationtaskruntakesapproximately2daysofexecutiontime. Withalleighttasks,tenmethods
run,and20runscompletedpermethod,resultsinFig.2include1600totaloptimizationruns(800
for molecule tasks and 800 for non-molecule tasks). Additionally, the two added curves in each
plotinFig.3required160additionalruns(120formoleculetasksand40fornon-moleculetask).
Completingalloftherunsneededtoproducealloftheresultsinthispaperthereforerequiredroughly
2680totalGPUhours.
ComputeResourcesusedDuringPreliminaryInvestigations. Inadditiontothecomputational
resourcesrequiredtoproduceexperimentalresultsinthepaperdiscussedabove,wespentapproximately
500hoursofGPUtimeonpreliminaryinvestigations. Thiswasdoneontheaforementionedinternal
clustershowninTable2.
19