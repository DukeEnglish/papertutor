Transformers are SSMs: Generalized Models and Efficient Algorithms
Through Structured State Space Duality
TriDaoâˆ—1 andAlbertGuâˆ—2
1DepartmentofComputerScience,PrincetonUniversity
2MachineLearningDepartment,CarnegieMellonUniversity
tri@tridao.me,agu@cs.cmu.edu
Abstract
WhileTransformershavebeenthemainarchitecturebehinddeeplearningâ€™ssuccessinlanguagemodeling,state-space
models(SSMs)suchasMambahaverecentlybeenshowntomatchoroutperformTransformersatsmalltomediumscale.
We show that these families of models are actually quite closely related, and develop a rich framework of theoretical
connectionsbetweenSSMsandvariantsofattention,connectedthroughvariousdecompositionsofawell-studiedclass
ofstructuredsemiseparablematrices. Ourstatespaceduality(SSD)frameworkallowsustodesignanewarchitecture
(Mamba-2)whosecorelayerisanarefinementofMambaâ€™sselectiveSSMthatis2-8Ã—faster, whilecontinuingtobe
competitivewithTransformersonlanguagemodeling.
1 Introduction
Transformers,inparticulardecoder-onlymodels(e.g.GPT(Brownetal.2020),Llama(Touvron,Lavril,etal.2023))which
process input sequences in a causal fashion, are one of the main drivers of modern deep learningâ€™s success. Numer-
ousapproachesattempttoapproximatethecoreattentionlayertoaddressitsefficiencyissues(Tayetal.2022),suchas
scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during
autoregressivegeneration.Inparallel,aclassofalternativesequencemodels,structuredstate-spacemodels(SSMs),have
emerged with linear scaling in sequence length during training and constant state size during generation. They show
strongperformanceonlong-rangetasks(e.g. S4(Gu,Goel,andRÃ©2022))andrecentlymatchedorbeatTransformerson
languagemodeling(e.g.Mamba(GuandDao2023))atsmalltomoderatescale.However,thedevelopmentofSSMshave
appeareddisjointfromthecommunityâ€™scollectiveefforttoimproveTransformers,suchasunderstandingthemtheoreti-
callyaswellasoptimizingthemonmodernhardware.Asaresult,itismoredifficulttounderstandandexperimentwith
SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an
algorithmicandsystemsperspective.
OurmaingoalistodeveloparichbodyoftheoreticalconnectionsbetweenstructuredSSMsandvariantsofattention.This
willallowustotransferalgorithmicandsystemsoptimizationsoriginallydevelopedforTransformerstoSSMs,towards
thegoalofbuildingfoundationmodelsthatperformbetterthanTransformerswhilescalingmoreefficientlyinsequence
length.AmilestonecontributioninthisdirectionwastheLinearAttention(LA)framework(Katharopoulosetal.2020),
whichderivedaconnectionbetweenautoregressiveattentionandlinearRNNsbyshowingtheequivalencebetweenâ€œdual
formsâ€ofquadratickernelizedattentionandaparticularlinearrecurrence. Thisdualityallowsnewcapabilitiessuchas
the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this
paperprovidesmultipleviewpointsconnectinglinear-complexitySSMswithquadratic-complexityformstocombinethe
strengthsofSSMsandattention.1
âˆ—Alphabeticalbylastname.
1Technicallyspeaking,theseconnectionsonlyrelatetocertainflavorsofattention;thetitleofthispaperisanhomagetoKatharopoulosetal.(2020)
whichfirstshowedthatâ€œTransformersareRNNsâ€.
1
4202
yaM
13
]GL.sc[
1v06012.5042:viXraState Space Duality. Our framework connecting struc-
Efficient
tured SSMs and variants of attention, which we call struc-
Algorithms
tured state space duality (SSD), is made through the
abstractions of structured matrices: matrices with sub-
quadraticparametersandmultiplicationcomplexity. Wede- Sec. 6
veloptwobroadframeworksforrepresentingsequencemod-
els,oneasmatrixtransformationsandoneastensorcontrac- Structured
tions,whicheachrevealdifferentperspectivesoftheduality. Matrices
Ourtechnicalcontributionsinclude:
Semiseparable Structured Masked
â€¢ Weshowanequivalencebetweenstatespacemodelsanda
Matrices Attention (SMA)
well-studiedfamilyofstructuredmatricescalledsemisep- Sec. 3 Sec. 4
arable matrices (Section 3). This connection is at the
heart our framework, revealing new properties and algo-
State Space Sec. 5
rithms for SSMs. A central message of this paper is that Attention
Models (SSM)
differentmethodsofcomputingstatespacemodelscanbere-
State Space
framedasvariousmatrixmultiplicationalgorithmsonstruc-
turedmatrices. Duality (SSD)
â€¢ We significantly improve the theory of linear atten-
Sec. 7
tion (Katharopoulos et al. 2020). We first provide an in-
cisiveproofofitsrecurrentformthroughthelanguageof
Mamba-2
tensorcontractions,andthengeneralizeittoanewfamily
of structuredmaskedattention(SMA)(Section4). Figure 1: (Structured State-Space Duality.) This paper
fleshes out the relationship between state space models
â€¢ WeconnectSSMsandSMA,showingthattheyhavealarge
andattentionthroughthebridgeofstructuredmatrices.
intersection that are duals of each other, possessing both
SSM-like linear and attention-like quadratic forms (Sec-
tion 5). We also prove that any kernel attention method
possessingafastrecurrentformmustbeanSSM.
Beyonditsintrinsictheoreticalvalue,ourframeworkopensupabroadsetofdirectionsforunderstandingandimproving
sequencemodels.
EfficientAlgorithms. Firstandmostimportantly,ourframeworkexposesnewefficientandeasily-implementablealgo-
rithmsforcomputingSSMs(Section6).WeintroduceanewSSDalgorithm,basedonblockdecompositionsofsemisepa-
rablematrices,thattakesadvantageofboththelinearSSMrecurrenceandquadraticdualform,obtainingoptimaltradeoffs
onallmainefficiencyaxes(e.g. trainingandinferencecompute,memoryusage,andabilitytoleveragematrixmultipli-
cationunitsonmodernhardware).AdedicatedimplementationofSSDis2âˆ’8Ã—fasterthantheoptimizedselectivescan
implementationofMamba,whilesimultaneouslyallowingformuchlargerrecurrentstatesizes(8Ã—thesizeofMambaor
evenhigher,withminimalslowdown). SSDishighlycompetitivewithoptimizedimplementationsofsoftmaxattention
(FlashAttention-2(Dao2024)),crossingoveratsequencelength2Kand6Ã—fasteratsequencelength16K.
Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to
Transformers,suchashardware-efficientoptimizationandparallelismtechniquesforlarge-scaletraining.Ourframework
allowsusingestablishedconventionsandtechniquesforattentiontobuildavocabularyofarchitecturedesignchoicesfor
SSMs, andfurtherimprovethem(Section7). Forexample, weintroducetheanalogofheadsfrommulti-headattention
(MHA)toSSMs. WeshowthattheMambaarchitectureisamulti-inputSSM(MIS)thatturnsouttobeanalogousto
multi-valueattention(MVA),andcompareothervariantsofMambawithdifferentheadstructures.
WealsousetheseideastomakeslightmodificationstotheMambablock, whichallowstensorparallelismtobeimple-
mented(e.g. inthestyleofMegatron(Shoeybietal.2019)). Themainideasincludeintroducinggrouped-valueattention
(GVA)headstructure,andmovingalldata-dependentprojectionstooccurinparallelatthebeginningoftheblock.
The combination of the modified parallel Mamba block, together with using SSD as the inner SSM layer, results in the
Mamba-2architecture. WeinvestigateChinchillascalinglawsforMamba-2inthesamesettingasMamba,findingthat
itParetodominatesMambaandTransformer++inbothperplexityandwall-clocktime.Weadditionallytrainafamilyof
2Mamba-2 models at varying sizes on the Pile, showing that it matches or outperforms Mamba and open source Trans-
formers on standard downstream evaluations. For example, Mamba-2 with 2.7B parameters trained on 300B tokens on
thePileoutperformsMamba-2.8B,Pythia-2.8BandevenPythia-6.9Btrainedonthesamedataset.
SystemsOptimizations. TheSSDframeworkconnectsSSMsandTransformers,allowingustoleveragearichbodyof
workonsystemsoptimizationsdevelopedforTransformers(Section8).
â€¢ For example, Tensor Parallelism (TP) is an important model parallelism technique to train large Transformer models
bysplittingeachlayeracrossGPUsonthesamenode. WedesignMamba-2tobeTP-friendly,reducingthenumberof
synchronizationpointperblockbyhalf.
â€¢ Forverylongsequenceswhoseactivationsdonotfitononedevice, sequenceparallelismhasbeendevelopedforthe
attentionblocks. WedescribehowtotrainSSMsingeneralandMamba-2inparticularwithsequenceparallelism,by
passingtherecurrentstatesbetweendevices.
â€¢ Forfinetuningwithexamplesofdifferentlengths,forbestefficiency,Transformerrequiressophisticatedtechniquesto
remove padding tokens and perform attention on variable length sequences. We show how Mamba-2 can be trained
withvariablesequencelengthsefficiently,requiringnopaddingtokens.
Section9empiricallyvalidatesMamba-2onlanguagemodeling,trainingefficiency,andadifficultmulti-queryassociative
recalltask(Arora,Eyuboglu,Zhang,etal.2024). Finally,inSection10,weprovideanextendedrelatedworkanddiscuss
potentialresearchdirectionsopenedupbyourframework.
Modelcodeandpre-trainedcheckpointsareopen-sourcedathttps://github.com/state-spaces/mamba.
2 Background and Overview
2.1 StructuredStateSpaceModels
Structuredstatespacesequencemodels(S4)arearecentclassofsequencemodelsfordeeplearningthatarebroadlyrelated
to RNNs, CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a
1-dimensionalsequenceğ‘¥ âˆˆRT â†¦â†’ğ‘¦ âˆˆRTthroughanimplicitlatentstateâ„ âˆˆR(T,N).
AgeneraldiscreteformofstructuredSSMstakestheformofequation(1).
â„
ğ‘¡
=ğ´â„ ğ‘¡âˆ’1+ğµğ‘¥
ğ‘¡
(1a) â„
ğ‘¡
=ğ´ ğ‘¡â„ ğ‘¡âˆ’1+ğµ ğ‘¡ğ‘¥
ğ‘¡
(2a)
ğ‘¦
ğ‘¡
=ğ¶âŠ¤â„
ğ‘¡
(1b) ğ‘¦
ğ‘¡
=ğ¶ ğ‘¡âŠ¤â„
ğ‘¡
(2b)
whereğ´ âˆˆ R(N,N),ğµ âˆˆ R(N,1),ğ¶ âˆˆ R(N,1). Structured SSMs are so named because theğ´ matrix controlling the temporal
dynamicsmustbestructuredinordertocomputethissequence-to-sequencetransformationefficientlyenoughtobeused
indeepneuralnetworks.Theoriginalstructuresintroducedwerediagonalpluslow-rank(DPLR)(Gu,Goel,andRÃ©2022)
anddiagonal(Gu,Gupta,etal.2022; Gupta,Gu,andBerant2022; J.T.Smith,Warrington,andLinderman2023),which
remainsthemostpopularstructure.
In this work, we use the term state space model (SSM) to refer to structured SSMs. There are many flavors of such
SSMs, with deep ties to several major paradigms of neural sequence models such as continuous-time, recurrent, and
convolutionalmodels(Gu, Johnson, Goel, etal.2021). Weprovideabriefoverviewbelow, andrefertopriorworkfor
morecontextanddetails(Gu2023;GuandDao2023).
Continuous-timeModels. TheoriginalstructuredSSMsoriginatedascontinuous-timemapsonfunctionsğ‘¥(ğ‘¡) âˆˆRâ†¦â†’
ğ‘¦(ğ‘¡) âˆˆ R,ratherthanoperatingdirectlyonsequences. Inthecontinuous-timeperspective,inequation(1a)thematrices
(ğ´,ğµ) arenotdirectlylearnedbutgeneratedfromunderlyingparameters (ğ´Ëš,ğµËš ),alongwithaparameterizedstepsizeÎ”.
Theâ€œcontinuousparametersâ€(Î”,ğ´Ëš,ğµËš )areconvertedtoâ€œdiscreteparametersâ€(ğ´,ğµ)throughfixedformulasğ´= ğ‘“ ğ´(Î”,ğ´Ëš )
andğµ = ğ‘“ ğµ(Î”,ğµËš ),wherethepair(ğ‘“ ğ´,ğ‘“ ğµ)iscalledadiscretizationrule.
Remark1. Whileourmainmodelsadoptthesameparameterizationanddiscretizationstepaspriorwork(seeGuandDao
(2023) for details), for simplifying exposition and notation we omit it in the rest of this paper. We note that prior work on
3structuredSSMsreferredtothecontinuousparameters (ğ´Ëš,ğµËš ) anddiscreteparameters (ğ´,ğµ) as (ğ´,ğµ) and (ğ´Â¯,ğµÂ¯ ) instead;we
havechangednotationtosimplifythepresentationandfocusdirectlyonthediscreteparameters,whichgovernthemainSSM
recurrence.
RecurrentModels. Equations(1)and(2)taketheformofarecurrencewhichislinearinitsinputğ‘¥. StructuredSSMs
canthereforebeviewedastypesofrecurrentneuralnetworks(RNNs),wherethelinearityendowsthemwithadditional
properties and allows them to avoid the sequential computation of traditional RNNs. Conversely, despite this simplifi-
cation,SSMsarestillfullyexpressiveassequencetransformations(inthesenseofuniversalapproximation)(Kaul2020;
Orvietoetal.2023;ShidaWangandXue2023).
ConvolutionalModels. WhentheSSMâ€™sdynamicsareconstantthroughtimeasinequation(1), themodeliscalled
lineartime-invariant(LTI).Inthiscase,theyareequivalenttoconvolutions. Thus,SSMscanalsobeviewedastypes
ofCNNs,butwhere(i)theconvolutionkernelsareimplicitlyparameterizedthroughtheSSMparameters(ğ´,ğµ,ğ¶)and(ii)
theconvolutionkernelsaregenerallyglobalinsteadoflocal. Conversely, throughclassicalsignalprocessingtheoryall
sufficientlywell-behavedconvolutionscanberepresentedasSSMs.
Commonly, previousLTISSMswouldusetheconvolutionalmodeforefficientparallelizabletraining(wherethewhole
inputsequenceisseenaheadoftime),andswitchedintorecurrentmode(1)forefficientautoregressiveinference(where
theinputsareseenonestepatatime).
SelectiveStateSpaceModels. Theform(2)wherethe parameters (ğ´,ğµ,ğ¶) canalsovaryintime wasintroducedin
MambaastheselectiveSSM.ComparedtothestandardLTIformulation(1),thismodelcanselectivelychoosetofocus
onorignoreinputsateverytimestep. ItwasshowntoperformmuchbetterthanLTISSMsoninformation-densedata
such as language, especially as its state size N increases allowing for more information capacity. However, it can only
be computed in recurrent instead of convolutional mode, and requires a careful hardware-aware implementation to be
efficient. Evenso,itisstilllessefficientthanhardware-friendlymodelssuchasCNNsandTransformersbecauseitdoes
notleveragematrixmultiplicationunits,whichmodernacceleratorssuchasGPUsandTPUsarespecializedfor.
Whiletime-invariantSSMsarecloselyrelatedtocontinuous,recurrent,andconvolutionalsequencemodels,theyarenot
directlyrelatedtoattention. Inthispaper,weshowadeeperrelationshipbetweenselectiveSSMsandattention,anduse
ittosignificantlyimprovethetrainingspeedofSSMswhilesimultaneouslyallowingformuchlargerstatesizesN.
StructuredSSMsasSequenceTransformations.
Definition2.1. Weusethetermsequencetransformationtorefertoaparameterizedmaponsequencesğ‘Œ = ğ‘“ ğœƒ(ğ‘‹)where
ğ‘‹,ğ‘Œ âˆˆ R(T,P) andğœƒ isanarbitrarycollectionofparameters. Trepresentsthesequenceor timeaxis;subscriptsindexintothe
firstdimension,e.g.ğ‘‹ ğ‘¡,ğ‘Œ ğ‘¡ âˆˆRP .
Sequence transformations (e.g. SSMs, or self-attention) are the cornerstone of deep sequence models, where they are
incorporated into neural network architectures (e.g. Transformers). The SSM in (1) or (2) is a sequence transformation
withP=1;itcanbegeneralizedtoP>1bysimplybroadcastingacrossthisdimension(inotherwords,viewingtheinput
as P independent sequences and applying the SSM to each). One can think of P as a head dimension, which we will
elaborateoninSection7.
Definition 2.2. We define the SSM operator SSM(ğ´,ğµ,ğ¶) = SSM(ğ´ 0:ğ‘‡,ğµ 0:ğ‘‡,ğ¶ 0:ğ‘‡) as the sequence transformationğ‘‹ âˆˆ
R(T,P) â†¦â†’ğ‘Œ âˆˆR(T,P) definedbyequation(2).
InSSMs,theNdimensionisafreeparametercalledthestatesizeorstatedimension.Wealsocallitthestateexpansion
factor,becauseitexpandsthesizeoftheinput/outputbyafactorofğ‘,withimplicationsforthecomputationalefficiency
ofthesemodels.
Finally,weremarkthatmanytypesofsequencetransformations,suchasattention,canberepresentedasasinglematrix
multiplicationacrossthesequencedimension.
Definition2.3. Wecallasequencetransformationğ‘Œ = ğ‘“ ğœƒ(ğ‘‹) amatrixtransformationifitcanbewrittenintheform
ğ‘Œ =ğ‘€ ğœƒğ‘‹ whereğ‘€ isamatrixdependingontheparametersğœƒ. Weidentifythesequencetransformationwiththematrixğ‘€,
andoftendropthedependenceonğœƒ whenclearfromcontext.
42.2 Attention
Attentionbroadlyreferstoatypeofcomputationthatassignsscorestoeverypairofpositionsinasequence, allowing
eachelementtoâ€œattendâ€totherest.Byfarthemostcommonandimportantvariantofattentionissoftmaxself-attention,
whichcanbedefinedas
ğ‘Œ =softmax(ğ‘„ğ¾âŠ¤)Â·ğ‘‰
forğ‘„,ğ¾,ğ‘‰ âˆˆ R(T,P). Themechanismofpairwisecomparisons(inducedbymaterializingğ‘„ğ¾âŠ¤)leadstothecharacteristic
quadratictrainingcostofattention.
Manyvariantsofattentionhavebeenproposed,butallsharetheunderlyingcoreoftheseattentionscores,withvarious
approximations(Tayetal.2022).Themostimportantvariantforthisworkislinearattention(Katharopoulosetal.2020).
Roughlyspeaking,thisfamilyofmethodsdropsthesoftmaxbyfoldingitintoakernelfeaturemap,andusesassociativity
ofmatrixmultiplicationtorewrite (ğ‘„ğ¾âŠ¤) Â·ğ‘‰ = ğ‘„ Â· (ğ¾âŠ¤ğ‘‰). Moreover, intheimportantcaseofcausal(autoregressive)
attention,theyshowthatwhenthecausalmaskisincorporatedintotheleft-handsideas (ğ¿â—¦ğ‘„ğ¾âŠ¤) Â·ğ‘‰,whereğ¿ isthe
lower-triangular1â€™smatrix,thentheright-handsidecanbeexpandedasarecurrence.Severalrecentandconcurrentworks
suchasRetNet(Y.Sunetal.2023)andGateLoop(Katsch2023)strengthenthistomoregeneralformsofğ¿(Section10).In
thiswork,ourformulationofstructuredmaskedattentionwillstronglygeneralizetheseideas.
2.3 StructuredMatrices
General matrices ğ‘€ âˆˆ R(T,T) require T2 parameters to represent and ğ‘‚(T2) time to perform basic operations such as
matrix-vectormultiplication.Structuredmatricesarethosethat
(i) canberepresentedinsubquadratic(ideallylinear)parametersthroughacompressedrepresentation,and
(ii) havefastalgorithms(mostimportantlymatrixmultiplication)byoperatingdirectlyonthiscompressedrepresen-
tation.
Perhapsthemostcanonicalfamiliesofstructuredmatricesaresparseandlow-rankmatrices.However,thereexistmany
other families, such as Toeplitz, Cauchy, Vandermonde, and butterfly matrices, which have all been used in machine
learningforefficientmodels(Dao,Gu,etal.2019;D.Fuetal.2024;Gu,Gupta,etal.2022;Thomasetal.2018).Structured
matrices are a powerful abstraction for efficient representations and algorithms. In this work, we will show that SSMs
areequivalenttoanotherclassofstructuredmatricesthathavenotpreviouslybeenusedindeeplearning,andusethis
connectiontoderiveefficientmethodsandalgorithms.
2.4 Overview: StructuredStateSpaceDuality
WhilethispaperdevelopsamuchricherframeworkofconnectionsbetweenSSMs,attention,andstructuredmatrices,we
provideabriefsummaryofthemainmethod,whichisactuallyquiteself-containedandsimplealgorithmically.
Recurrent(Linear)Form. Thestatespacedual(SSD)layercanbedefinedasaspecialcaseoftheselectiveSSM(2).
The standard computation of an SSM as a recurrence (or parallel scan) can be applied, which has linear complexity in
sequencelength.ComparedtotheversionusedinMamba,SSDhastwominordifferences:
â€¢ Thestructureonğ´isfurthersimplifiedfromdiagonaltoscalartimesidentitystructure.Eachğ´
ğ‘¡
canalsobeidentified
withjustascalarinthiscase.
â€¢ WeusealargerheaddimensionP,comparedtoP = 1usedinMamba. TypicallyP = {64,128} ischosenwhichis
similartoconventionsformodernTransformers.
Compared to the original selective SSM, these changes can be viewed as slightly decreasing the expressive power in
return for significant training efficiency improvements. In particular, our new algorithms will allow the use of matrix
multiplicationunitsonmodernaccelerators.
5Dual(Quadratic)Form. ThedualformofSSDisaquadraticcomputationcloselyrelatedtoattention,definedas
(cid:40)
(ğ¿â—¦ğ‘„ğ¾âŠ¤)Â·ğ‘‰ ğ¿ ğ‘–ğ‘— = ğ‘ ğ‘– Ã—Â·Â·Â·Ã—ğ‘ ğ‘—+1 ğ‘– â‰¥ ğ‘—
0 ğ‘– < ğ‘—
whereğ‘
ğ‘–
areinput-dependentscalarsboundedin [0,1].
Comparedtostandardsoftmaxattention,therearetwomaindifferences
â€¢ Thesoftmaxisdropped.
â€¢ Theattentionmatrixismultipliedelementwise-wisebyanadditionalmaskmatrixğ¿.
Both of these changes can be viewed as addressing problems in vanilla attention. For example, the softmax has been
recently observed to cause problems in attention scores, such as the â€œattention sinkâ€ phenomenon (Darcet et al. 2024;
Xiaoetal.2024). Moreimportantly, themaskmatrixğ¿ canbeviewedasreplacingtheheuristicpositionalembeddings
ofTransformerswithadifferentdata-dependentpositionalmaskthatcontrolshowmuchinformationistransferedacross
time.
Morebroadly,thisformisaninstanceofourstructuredmaskedattentiongeneralizationoflinearattention,definedin
Section4.
MatrixFormandSSDAlgorithm. ThevariousformsofSSDareconnectedthroughaunifiedmatrixrepresentation,
byshowingthatSSMshaveamatrixtransformationformğ‘Œ =ğ‘€ğ‘‹ foramatrixğ‘€ ğœƒ âˆˆR(T,T) thatdependsonğœƒ = (ğ´,ğµ,ğ¶).
Inparticular,thedualformofSSDisequivalenttonaive(quadratic-time)multiplicationbythematrixğ‘€,andtherecurrent
formisaparticularefficient(linear-time)algorithmthatleveragesthestructureinğ‘€.
Goingbeyondthese,any algorithmformultiplicationbyğ‘€ canbeapplied. Ourproposedhardware-efficientSSDalgo-
rithm(Section6)isanewstructuredmatrixmultiplicationmethodthatinvolvesblockdecompositionsofğ‘€,whichobtains
better efficiency tradeoffs than either the pure linear or quadratic forms. It is relatively simple and easy-to-implement
comparedtogeneralselectiveSSMs(GuandDao2023); Listing1providesacompleteimplementationinafewlinesof
code.
Figure1providesasimpleroadmapoftherelationshipsbetweentheconceptspresentedinthispaper.
2.5 Notation
Throughoutthispaper,wepreferusingprecisenotationthatcanbemappedtocode.
MatricesandVectors. Wegenerallyuselowercasetodenotevectors(i.e.tensorswithasingleaxis)anduppercaseto
denotematrices(i.e.tensorswithmorethanoneaxes). Wedonotboldmatricesinthiswork. Sometimes,ifamatrixis
tiedorrepeatedalongoneaxis(andhencecanalsobeviewedasavector),wemayuseeitherupperorlowercaseforit.2
Â·denotesscalarormatrixmultiplicationwhileâ—¦denotesHadamard(elementwise)multiplication.
Indexing. WeusePython-styleindexing,e.g.ğ‘– : ğ‘— referstotherange(ğ‘–,ğ‘–+1,...,ğ‘—âˆ’1)whenğ‘– < ğ‘— and(ğ‘–,ğ‘–âˆ’1,...,ğ‘—+1)
whenğ‘– > ğ‘—. For example, for any symbolğ‘£ we letğ‘£ ğ‘—:ğ‘– for ğ‘— â‰¥ ğ‘– denote the sequence (ğ‘£ ğ‘—,...,ğ‘£ ğ‘–+1). [ğ‘–] is equivalent to
0:ğ‘– = (0,...,ğ‘–âˆ’1).Forshorthand,wealsoletğ‘£Ã— ğ‘—:ğ‘– denotetheproductğ‘£ ğ‘— Ã—Â·Â·Â·Ã—ğ‘£ ğ‘–+1.3
Dimensions. Todistinguishfrommatricesandtensors,weoftenusecapitallettersintypewriterfonts(e.g. D,N,T) to
denote dimensions and tensor shapes. Instead of the traditional notation ğ‘€ âˆˆ Rğ‘‡Ã—ğ‘‡ we frequently use ğ‘€ âˆˆ R(T,T) to
reflecttensorshapesincode.
TensorContractions. Wewillheavilyrelyontensorcontractionoreinsumnotationbothforclarityandasacentral
toolinstatingandprovingourresults. Weassumethereadertobefamiliarwiththisnotation,whichiscommonlyused
2Inthiswork,thishappensonlywiththeğ´parameterofSSMs.
3Insomecontexts,itisalwaysclearthatthenotationğ‘ğ‘–:ğ‘— orğ´ğ‘–:ğ‘— meansğ‘ ğ‘–Ã— :ğ‘—,andthesuperscriptisomitted.
6in modern tensor libraries such as numpy. For example, we can use contract(MN,NKâ†’MK) to denote the matrix-matrix
multiplicationoperator,andinournotationcontract(MN,NKâ†’MK)(ğ‘‹,ğ‘Œ)(whichisequivalenttoğ‘‹ Â·ğ‘Œ)canbetranslated
tocodeasnumpy.einsum(â€²mn,nkâ†’mkâ€²,X,Y).
AlargeglossaryofnotationisincludedinAppendixA.
3 State Space Models are Structured Matrices
Thissectionexploresdifferentperspectivesofthestatespacemodelasasequencetransformation,andoutlinesproperties
and algorithms of such maps. The main results of this section are about the equivalence between state space models
and a family of structured matrices called semiseparable matrices, which imply new efficiency results (Theorems 3.5
and3.7).
3.1 TheMatrixTransformationFormofStateSpaceModels
RecallthatourdefinitionofanSSMisdefinedasaparameterizedmapdefinedthrough(2). Ourtheoreticalframework
startsbysimplywritingthistransformationasamatrixmultiplicationmappingthevectorsğ‘¥ âˆˆRT â†¦â†’ğ‘¦ âˆˆRT.
Bydefinition,â„
0
=ğµ 0ğ‘¥ 0.Byinduction,
â„
ğ‘¡
=ğ´
ğ‘¡
...ğ´ 1ğµ 0ğ‘¥ 0+ğ´
ğ‘¡
...ğ´ 2ğµ 1ğ‘¥ 1+Â·Â·Â·+ğ´ ğ‘¡ğ´ ğ‘¡âˆ’1ğµ ğ‘¡âˆ’2ğ‘¥ ğ‘¡âˆ’2+ğ´ ğ‘¡ğµ ğ‘¡âˆ’1ğ‘¥ ğ‘¡âˆ’1+ğµ ğ‘¡ğ‘¥
ğ‘¡
ğ‘¡
âˆ‘ï¸
= ğ´ ğ‘¡Ã— :ğ‘ ğµ ğ‘ ğ‘¥ ğ‘ .
ğ‘ =0
Multiplyingbyğ¶ ğ‘¡ toproduceğ‘¦ ğ‘¡ andvectorizingtheequationoverğ‘¡ âˆˆ [T],wederivethematrixtransformationformof
SSMs.
ğ‘¡
âˆ‘ï¸
ğ‘¦ ğ‘¡ = ğ¶ ğ‘¡âŠ¤ğ´ ğ‘¡Ã— :ğ‘ ğµ ğ‘ ğ‘¥ ğ‘ 
ğ‘ =0 (3)
ğ‘¦ =SSM(ğ´,ğµ,ğ¶)(ğ‘¥) =ğ‘€ğ‘¥
ğ‘€
ğ‘—ğ‘–
(cid:66)ğ¶âŠ¤
ğ‘—
ğ´ ğ‘—Â·Â·Â·ğ´ ğ‘–+1ğµ
ğ‘–
3.2 SemiseparableMatrices
ğ‘€ in equation (3) is a particular representation of a class of matrices known as semiseparable matrices. Semiseparable
matricesareafundamentalmatrixstructure.Wefirstdefinethesematricesandtheirproperties.
Definition3.1. A(lowertriangular)matrixğ‘€isN-semiseparableifeverysubmatrixcontainedinthelowertriangularportion
(i.e.onorbelowthediagonal)hasrankatmostN.WecallNtheorderor rankofthesemiseparablematrix.
Definition 3.1, and other forms of related â€œseparableâ€ structure (e.g. quasiseparable matrices and other definitions of
semiseparablematrices)aresometimescalledstructuredrankmatrices(orrank-structuredmatrices)becausetheyare
characterized by rank conditions on their submatrices. Semiseparable matrices have many structured representations
includingthehierarchicalsemiseparable(HSS),sequentialsemiseparable(SSS),andBruhatforms(PernetandStorjohann
2018).WewillprimarilyusetheSSSform.
3.2.1 TheSequentiallySemiseparable(SSS)Representation
Definition3.2. Alowertriangularmatrixğ‘€ âˆˆ R(T,T) hasaN-sequentiallysemiseparable(SSS)representationifitcan
bewrittenintheform
ğ‘€
ğ‘—ğ‘–
=ğ¶âŠ¤
ğ‘—
ğ´ ğ‘—Â·Â·Â·ğ´ ğ‘–+1ğµ
ğ‘–
(4)
forvectorsğµ 0,...,ğµ Tâˆ’1,ğ¶ 0,...,ğ¶
Tâˆ’1
âˆˆRN andmatricesğ´ 0,...,ğ´
Tâˆ’1
âˆˆR(N,N).
WedefinetheoperatorSSSsothatğ‘€ =SSS(ğ´ 0:T,ğµ 0:T,ğ¶ 0:T).
7AfundamentalresultofsemiseparablematricesisthattheyareexactlyequivalenttomatriceswithSSSrepresentations.
Onedirectioncanbededucedwithasimpleconstructiveproof.
Lemma3.3. AnN-SSSmatrixğ‘€ withrepresentation(4)isN-semiseparable.
Proof. Consideranyoff-diagonalblockğ‘€ ğ‘—:ğ‘—â€²,ğ‘–â€²:ğ‘– where ğ‘—â€² > ğ‘— â‰¥ğ‘– >ğ‘–â€².Thishasanexplicitrank-Nfactorizationas
ğ¶âŠ¤ğ´Ã— ğµ ... ğ¶âŠ¤ğ´Ã— ğµ ğ¶âŠ¤ğ´Ã—
ï£® ğ‘— ğ‘—:ğ‘–â€² ğ‘–â€² ğ‘— ğ‘—:ğ‘–âˆ’1 ğ‘–âˆ’1 ï£¹ ï£® ğ‘— ğ‘—:ğ‘— ï£¹
ï£¯ ï£¯ ï£¯ . . . . . . ï£º ï£º ï£º = ï£¯ ï£¯ ï£¯ . . . ï£º ï£º ï£ºğ´Ã— ğ‘—:ğ‘–âˆ’1 (cid:2)ğ´ ğ‘–Ã— âˆ’1:ğ‘–â€²ğµ ğ‘–â€² Â·Â·Â· ğ´ ğ‘–Ã— âˆ’1:ğ‘–âˆ’1ğµ ğ‘–âˆ’1(cid:3). (5)
ï£¯ğ¶âŠ¤ ğ´Ã— ğµ ... ğ¶âŠ¤ ğ´Ã— ğµ ï£º ï£¯ğ¶âŠ¤ ğ´Ã— ï£º
ï£¯
ï£°
ğ‘—â€²âˆ’1 ğ‘—â€²âˆ’1:ğ‘–â€² ğ‘–â€² ğ‘—â€²âˆ’1 ğ‘—â€²âˆ’1:ğ‘–âˆ’1 ğ‘–âˆ’1ï£º
ï£»
ï£¯
ï£°
ğ‘—â€²âˆ’1 ğ‘—â€²âˆ’1:ğ‘—ï£º
ï£»
â–¡
Equation (5) will be used extensively in deriving our fast algorithms for sequence models. The other direction is well-
establishedintheliteratureonsemiseparablematrices.
Proposition3.4. EveryN-semiseparablematrixhasaN-SSSrepresentation.
Furthermore,notethatalthoughDefinition3.2involvesğ‘‚(N2T)parametersfortherepresentation(inparticulartostorethe
ğ´matrices),itcanactuallybecompresseddowntoğ‘‚(NT) parameters,whichisasymptoticallytight(Pernet,Signargout,
and Villard 2023). Therefore in the rest of this paper we will conflate the structured matrix class (Definition 3.1) and a
particularrepresentationofit(Definition3.2);wewillalwaysusethisrepresentationinsteadofothercandidates.Inturn
wewilluseN-SStorefertoanN-semiseparablematrixinSSSform.
Semiseparablematricesareafundamentalmatrixstructureandhavemanyimportantproperties.Theyaredeeplyrelated
torecurrencesatlarge,andcanbedefinedbymultiplecharacterizations(e.g.Definitions3.1and3.2)whichrevealdifferent
connectionsandefficientalgorithmsforthem.WementionsomeoftheirotherpropertiesinAppendixC.1.
Remark 2. The notion of semiseparability is very broad and many similar but subtlely different definitions appear in the
literature;ourdefinitionsmaydifferslightlyfromotherconventions.First,becauseweareprimarilyconcernedwithcausalor
autoregressivesettingsinthispaper,wehaverestrictedthedefinitionofsemiseparabilitytothetriangularcase;Definition3.1
moreformallymightbecalled(N,0)-semiseparabilitybysomeauthors.Someauthorsmayalsoinsteadrefertoitasaformof
quasiseparability(EidelmanandGohberg1999;Pernet2016).SeeVandebriletal.(2005)forabriefsurvey.
3.2.2 1-SemiseparableMatrices:theScalarSSMRecurrence
Wewillsingleoutthespecialcaseof1-SSmatrices. Notethatinthiscase,theğ¶ andğµ arescalars,andcanbefactored
ğ‘— ğ‘–
outoftheSSSrepresentation(4)(wealsouselower-casetoemphasizethattheparametersarescalarsinthiscase)
SSS(ğ‘,ğ‘,ğ‘) =diag(ğ‘)Â·ğ‘€ Â·diag(ğ‘) where ğ‘€
ğ‘—ğ‘–
=ğ‘Ã— ğ‘—:ğ‘–.
Since diagonal matrices are easy to handle (e.g. multiplication by a diagonal matrix is the same as elementwise scalar
multiplication),wecanignoretheseterms.Thusourbasicrepresentationofa1-SSmatrixisğ‘€
ğ‘—ğ‘–
=ğ‘
ğ‘—:ğ‘–
or
1
ï£® ï£¹
ï£¯ ğ‘ 1 ï£º
ï£¯ 1 ï£º
ï£¯ ğ‘ ğ‘ ğ‘ 1 ï£º
ğ‘€ =1SS(ğ‘ 0:ğ‘‡) (cid:66) ï£¯
ï£¯ ï£¯
2
. . .
1
. .
.2
... ...
ï£º
ï£º
ï£º. (6)
ï£¯ ï£º
ï£¯ğ‘ ...ğ‘ ğ‘ ...ğ‘ ... ğ‘ 1ï£º
ï£¯ ğ‘‡âˆ’1 1 ğ‘‡âˆ’1 2 ğ‘‡âˆ’1 ï£º
ï£° ï£»
The importance of 1-SS matrices lies in their equivalence to the minimal form of a scalar recurrence â€“ the case of a
degenerateSSMwithstatedimensionN=1andno (ğµ,ğ¶) projections. Notethatmultiplicationğ‘¦ =ğ‘€ğ‘¥ canbecomputed
bytherecurrence
ğ‘¦
ğ‘¡
=ğ‘ ğ‘¡:0ğ‘¥ 0+Â·Â·Â·+ğ‘ ğ‘¡:ğ‘¡ğ‘¥
ğ‘¡
=ğ‘
ğ‘¡
(ğ‘ ğ‘¡âˆ’1:0ğ‘¥ 0+Â·Â·Â·+ğ‘ ğ‘¡âˆ’1:ğ‘¡âˆ’1ğ‘¥ ğ‘¡âˆ’1)+ğ‘ ğ‘¡:ğ‘¡ğ‘¥
ğ‘¡
(7)
=ğ‘ ğ‘¡ğ‘¦ ğ‘¡âˆ’1+ğ‘¥ ğ‘¡.
8Outputs ğ‘Œ
Sequence
Transformation
Matrix ğ‘€
Sequencedim. T
Head dim.
P State Space Models are Semiseparable Matrix Transformations
Inputs ğ‘‹
Figure2: (StateSpaceModelsareSemiseparableMatrices.) Assequencetransformations,statespacemodelscanbe
representedasamatrixtransformationğ‘€ âˆˆR(T,T) actingonthesequencedimensionT,sharingthesamematrixforeach
channel in a head (Left). This matrix is a semiseparable matrix (Right), which is a rank-structured matrix where every
submatrixcontainedon-and-belowthediagonal(Blue)hasrankatmostN,equaltotheSSMâ€™sstatedimension.
Wethusalsorefertomatrixmultiplicationby1-SSmatricesasthescalarSSMrecurrenceorthecumprodsum(cumu-
lativeproductsum; ageneralizationofcumulativeproductandcumulativesum)operator. Asthefundamentalformof
recurrence,multiplicationby1-SSmatricesisimportantasabuildingblockforourmainalgorithms.
We emphasize that one of the central themes of this paper is that many algorithms on sequence models can be reduced
to structured matrix multiplication algorithms. 1-SS matrices exemplify this connection: there are many fast algorithms
forcomputingtheprimitivescalarrecurrenceorcumprodsumoperator,andallofthemturnouttobeequivalenttodif-
ferentstructuredfactorizationof1-SSmatrices. WededicateAppendixBtothesealgorithmsfor1-SSmatrixmultiplica-
tion.
3.3 StateSpaceModelsareSemiseparableMatrices
RecallthatourdefinitionofanSSMisdefinedasaparameterizedmapdefinedthroughDefinition2.1. Theconnection
between SSMs and semiseparable matrices follows from simply writing this transformation as a matrix multiplication
mappingthevectorsğ‘¥ â†¦â†’ğ‘¦ âˆˆRT.
Equation(3)directlyestablishesthelinkbetweenstatespacemodelsandthesequentiallysemiseparablerepresentation,
whichinturnareequivalenttosemiseparablematricesingeneral(Lemma3.3andProposition3.4).
Theorem3.5. Thestatespacemodeltransformationğ‘¦ =SSM(ğ´,ğµ,ğ¶)(ğ‘¥)withstatesizeNisidenticaltomatrixmultiplica-
tionbyanN-SSmatrixinsequentiallysemiseparablerepresentationğ‘¦ =SSS(ğ´,ğµ,ğ¶)Â·ğ‘¥.
InotherwordsthesequencetransformationoperatorSSM(Definition2.2)coincideswiththematrixconstructionoper-
atorSSS(Definition3.2),andweusetheminterchangeably(orsometimesSSasshorthand). Furthermoreâ€”byatwistof
fateâ€”structuredstatespacemodelsandsequentiallysemiseparablematriceshavethesameacronyms,underscoringtheir
equivalence!ConvenientlywecanuseanyoftheseacronymsSSM(statespacemodelorsemiseparablematrix),SSS(struc-
turedstatespaceorsequentiallysemiseparable),orSS(statespaceorsemiseparable)interchangeablytounambiguously
refertoeitherconcept. However,wewillgenerallyusetheconventionthatSSMreferstostatespacemodel,SSrefersto
semiseparable,andSSSreferstosequentiallysemiseparable.
Figure2illustratesthesequencetransformationperspectiveofstatespacemodelsassemiseparablematrices.
9
noitacilpitlum
xirtaM3.4 ComputingStateSpaceModelsthroughStructuredMatrixAlgorithms
The reason Theorem 3.5 is important is that it will allow us to reduce the problem of efficient computation of SSMs (and
other sequence models) into efficient algorithms for structured matrix multiplication. We briefly provide an overview and
deferourmainnewalgorithmtoSection6,aftershowingtheequivalenceofSSMstoothersequencemodelsinSections4
and5.
Aspreviouslydefined,semiseparablematrices(i.e.rank-structuredmatrices)areaclassicaltypeofstructuredmatrix:
(i) TheyhavecompressedrepresentationssuchastheSSSformwhichhasonlyğ‘‚(T)insteadofğ‘‚(T2)parameters.
(ii) Theyhavefastalgorithmsoperatingdirectlyonthecompressedrepresentation.
Furthermore,theparameterizationandmatrixmultiplicationcostcanbetightinthesemiseparableorder.
Proposition3.6(Pernet,Signargout,andVillard(2023)). AnN-SSmatrixofsizeTcanberepresentedinğ‘‚(NT)parameters
andhasmatrix-vectormultiplicationintimeandspaceğ‘‚(NT).
Forexample,1-SSmatricesillustratetheessenceofthisconnection. Thematrixğ‘€ = 1SS(ğ‘) isdefinedbyexactlyTâˆ’1
parametersğ‘
0:Tâˆ’1
=ğ‘ 1,...,ğ‘ Tâˆ’1,andcanbecomputedinğ‘‚(T)timebyfollowingthescalarrecurrence(7).
3.4.1 TheLinear(Recurrent)Mode
Proposition 3.6 can be easily seen in the case of diagonal structured SSMs (S4D (Gu, Gupta, et al. 2022)), simply by
leveragingthestatespacemodelformulation(2)andunrollingtherecurrence.Weprovidetheformaltensor-contraction
algorithmin(8),wherethedimensionSisequaltoT4.
ğ‘ =contract(SP,SNâ†’SPN)(ğ‘‹,ğµ) (S,P,N) (8a)
ğ» =contract(TSN,SPNâ†’TPN)(ğ¿,ğ‘) (T,P,N) (8b)
ğ‘Œ =contract(TN,TPNâ†’TP)(ğ¶,ğ») (T,P) (8c)
Here,ğ¿ âˆˆR(T,T) isdefinedas1SS(ğ´),orinotherwordsğ¿ 0:T,0:T =1SS(ğ´ 0:T)forğ‘– âˆˆ [N].Thisalgorithminvolvesthreesteps
correspondingto(2):
(i) expandingtheinputğ‘‹ bytheinputmatrixğµ(8a),
(ii) unrollingindependentscalarSSMrecurrences(8b),and
(iii) contractingthehiddenstateğ» bytheoutputmatrixğ¶ (8c).
NotethatwehaveusedtheequivalencebetweenscalarSSMsand1-SSmatricesinstep(8b).
Remark 3. We note that (8) is a special case of the Mamba (S6) model. however, a naive implementation is slow because
oftheexpandedtensorsğ‘ andğ» ofsize (T,P,N);GuandDao(2023)introducedahardware-awareimplementationtoavoid
materializingthesetensors.
Surprisingly, Theorem 3.5 and Proposition 3.6 immediately imply that all SSMs have the same asymptotic efficiency as
algorithm(8).
Theorem3.7. Anystatespacemodel(Definition2.2)ofstatesizeNonsequencelengthTcanbecomputedintimeğ‘‚(TN)(not
accountingforpotentialpreprocessing).
WenotethatthisresultisnewtothestructuredSSMliterature. Inparticular,givendenseunstructuredğ´ matrices,the
ğ‘¡
total representation alone seems to be of sizeğ‘‚(TN2). Thus Theorem 3.7 states the non-trivial result that with a pre-
processingstep,evenanunstructuredSSMcanbecomputedoptimallyefficiently,withupperboundmatchingthelower
boundğ‘‚(TN)givenbythesizeofğµandğ¶.
Remark 4. Theorem 3.7 is perhaps not too surprising in light of the fact that almost all dense matrices over
R(N,N)
are
diagonalizableoverC,leadingtotheresultthat almostalldenserealSSMsareequivalenttoadiagonalcomplexSSM.This
factunderliesthereasonwhydiagonalSSMsarethemostpopularformofstructuredSSM(Gu,Gupta,etal.2022;Gupta,Gu,
4Adifferentsymbolisrequiredforthecontractionnotation.
10andBerant2022;J.T.Smith,Warrington,andLinderman2023). However,Theorem3.7impliesthemuchstrongerresultfor
allrealSSMs(notjustthediagonalizableones),aswellasdenseSSMsoverotherfields(includingCitself).
In practice, efficiently computable SSMs still require additional structure onğ´, particularly to avoid the expensive pre-
processingstep(whichbothhasorderNextraFLOPsandinvolveshardware-inefficientoperationssuchassingularvalue
decompositions). ThesestructuresarethefocusofpastworkonstructuredSSMs(e.g.S4(D)andMamba)aswellasour
newalgorithms. Inparticular,whenslightlystrongerstructureisimposedonğ´,wewilldesignveryhardware-efficient
algorithmsthroughblockdecompositionsoftheSSMmatrixğ‘€ =SSS(ğ´,ğµ,ğ¶)inSection6.
3.4.2 TheQuadratic(Naive)Mode
WenotethatthereisanotherwaytocomputeanSSMexposedbyournewmatrixpointofview.Anaivecomputationof
thematrixSSMrepresentation(3)involvessimplymaterializingthesequencetransformationmatrixğ‘€ = SSS(ğ´,ğµ,ğ¶).
Thisisa(T,T)matrix,andthereforethisnaivealgorithmwillscalequadraticallyinsequencelength.However,whenthe
sequencelengthTisshort, thiscanactuallybemoreefficientthanthelinearalgorithmduetoconstantfactorsandthe
hardware-friendlinessofthecomputationpattern(e.g. leveragingmatrix-matrixmultiplications). Infact,foraparticular
caseofstructuredSSMs,thislooksverysimilartoaquadraticattentioncomputation(Section5).
3.4.3 Summary
Many sequence models are explicitly motivated or defined as matrix sequence transformations â€“ most notably Trans-
formers,wherethematrixmixeristheattentionmatrix. Ontheotherhand,RNNsandSSMshavenotpreviouslybeen
describedinthisway. Byprovidinganexplicitmatrixtransformationformofstatespacemodels,werevealnewwaysof
understandingandusingthem. Fromacomputationalperspective,anymethodofcomputingtheforwardpassofastate
space model can be viewed as a matrix multiplication algorithm on semiseparable matrices. The semiseparable matrix
perspective provides one lens into state space duality (SSD), where the dual modes respectively refer to a linear-time
semiseparablematrixmultiplicationalgorithmandquadratic-timenaivematrixmultiplication.
Moreover,leveragingtherichstructureofsemiseparablematricescanleadtoevenbetteralgorithmsandmoreinsights(e.g.
Section6andAppendixB).InAppendixC.1,wedescribesomeadditionalpropertiesofsemiseparablematrices.
4 Structured Masked Attention: Generalizing Linear Attention
with Structured Matrices
Inthissectionwerevisitthelinearattentionframeworkfromfirstprinciples.Themainresultsinthissectionareasimple
tensor-contraction-basedproofoflinearattention(Proposition4.1),andourgeneralizedabstractionofstructuredmasked
attentioninDefinition4.2. Wenotethatthissectionderivesthemaindualityresultsfromadifferentdirectionthanstate
spacemodelsandcanbereadcompletelyindependentlyofSection3.
â€¢ Section4.1setsupourframeworkforvariantsofattention,withaparticularfocusonkernelattentionandmasked
kernelattention.
â€¢ Section 4.2 provides our first main attention result, a simple proof of linear attention through the lens of tensor
contractions.
â€¢ Section4.3definesstructuredmaskedattention,ourgeneralizationofpriorattentionvariantsthroughstructured
matrices.
114.1 TheAttentionFramework
4.1.1 Attention
Thebasicformof(single-head)attentionisamaponthreesequencesofvectors(ğ‘„,ğ¾,ğ‘‰) â†¦â†’ğ‘Œ.
ğ‘„ =input (T,N)
ğ¾ =input (S,N)
ğ‘‰ =input (S,P)
(9)
ğº =ğ‘„ğ¾âŠ¤ (T,S)
ğ‘€ = ğ‘“(ğº) (T,S)
ğ‘Œ =ğºğ‘‰ (T,P)
Weuseâ€œshapeannotationsâ€toindicatethedimensionsoftensors,e.g.ğ‘„ âˆˆ R(T,N).Inthisgeneralform,SandTrepresent
sourceandtarget sequencelengths,Nrepresentsthefeaturedimension,andPrepresentstheheaddimension.
Themostcommonvariantof softmaxattentionusesasoftmaxactivation ğ‘“ = softmaxtonormalizetherowsoftheğº
matrix.
4.1.2 Self-Attention
Ourtreatmentismotivatedbythemostimportantcaseofself-attention,where
(i) thesourceandtargetsequencesarethesame(i.e.S=T),
(ii) usuallythefeatureandheaddimensionsarethesame(i.e.N=P),
(iii) andğ‘„,ğ¾,ğ‘‰ aregeneratedbylinearprojectionsonthesameinputvector(ğ‘„ =ğ‘Š
ğ‘„
Â·ğ‘‹,ğ¾ =ğ‘Š
ğ¾
Â·ğ‘‹,ğ‘‰ =ğ‘Š
ğ‘‰
Â·ğ‘‹).
However,ourpresentationabstractsawaythesechoicesandbeginsfromtheğ‘„,ğ¾,ğ‘‰ matrices.
Remark 5. Our focus is on the self-attention case with equal head and feature dimensions (i.e. S = T and N = P), which
should be used as the running example. We define the general formulation of attention not only so that our framework
captures variants such as cross-attention, but also because separating the notation for dimensions (e.g. S and T) makes the
contractionnotationproofsofourmainresultsinthissectionmoreclear.
Remark6. Whileattentionisusuallyframedasanoperationonthesethreeinputsğ‘„,ğ¾,ğ‘‰ whichareviewedsymmetrically,
theinputandoutputdimensionsin(9)indicateotherwise. Inparticular,thefeaturedimensionNisnotpresentintheoutput;
therefore in the case when S = T (e.g. self-attention), we viewğ‘‰ as the main input, so that (9) defines a proper sequence
transformationğ‘‰ â†¦â†’ğ‘Œ (Definition2.1).
4.1.3 KernelAttention
ThestepwherethesoftmaxfunctionisappliedtotheGrammatrixğº canbedecomposedintotwoparts:
1. Exponentiatingtheğº matrix.
2. Normalizingtheğº matrixontheSaxis.
Wecanignorethenormalizationtermfornow,asitamountstosimplypassinginğ‘‰ = 1anddividing(werevisitthisin
Section7.3).Theexponentiationtermcanbeviewedasakerneltransformation:thereisan(infinite-dimensional)feature
mapğœ‘ such that exp(ğ‘„ğ¾âŠ¤) = ğœ‘(ğ‘„)ğœ‘(ğ¾)âŠ¤. By abstracting away the feature map into the definition ofğ‘„ and ğ¾ itself
(i.e.defineğ‘„,ğ¾ asthepost-transformedversions),wecanignorethesoftmaxtransformation,andassumethatğ‘„,ğ¾ are
arbitrarilygeneratedbykernelfeaturemapsandpotentiallyNâ‰ P.
Manyinstantiationsofkernelattentionhavebeenproposed,including:
â€¢ TheoriginalLinearAttention(Katharopoulosetal.2020)definesthekernelfeaturemapasanarbitrarypointwise
activationfunction,suchasğ‘¥ â†¦â†’1+elu(ğ‘¥).
â€¢ Random Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature map to approximate softmax
attention(i.e. theexpfeaturemap)usingtherandomFourierfeatureapproximationofGaussiankernels(Rahimi
12andRecht2007).Thisinvolvesrandomprojections(i.e.multiplyingğ‘„andğ¾byarandomprojectionğ‘Š andapplying
theactivationğ‘¥ â†¦â†’ (cos(ğ‘¥),sin(ğ‘¥)).
â€¢ Performer(Choromanskietal.2021)proposesthefastattentionviapositiveorthogonalrandomfeatures(FAVOR+).
Thepositiverandomfeatures(PRF)partchoosesthekernelfeaturemaptobearandomprojectionfollowedbythe
featuremapğ‘¥ â†¦â†’2âˆ’1/2(exp(ğ‘¥),exp(âˆ’ğ‘¥)). Thischoiceismotivatedsothatthekernelelementsarepositive-valued
andprovablyapproximatesthesoftmaxattention.[Italsoproposeschoosingtherandomprojectionsinorthogonal
directions,whichwedonotconsider.]
â€¢ cosFormer (Qin, Weixuan Sun, et al. 2022) augment RFA with a cosine reweighting mechanism that incorpo-
rates positional information to emphasize locality. This effectively passes ğ‘„ ğ‘¡,ğ¾ ğ‘¡ through the feature map ğ‘¥ â†¦â†’
(ğ‘¥cos(ğœ‹ğ‘¡/2ğ‘‡),sin(ğœ‹ğ‘¡/2ğ‘‡)).
â€¢ Linear Randomized Attention (Zheng, C. Wang, and Kong 2022) generalize RFA from the perspective of impor-
tance sampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-
transformednumerator).
OtherrelatedattentionvariantsincludeLinformer(SinongWangetal.2020)andNystrÃ¶former(Xiongetal.2021),which
bothuselow-rankapproximationsoftheattentionmatrixğ‘€(andarethuscompatiblewithequation(9)),throughrandom
projections(Johnson-Lindenstrauss)andkernelapproximation(theNystrÃ¶mmethod)respectively.
4.1.4 Masked(Kernel)Attention
Letğ¿ beamaskofshape (T,S). Mostcommonly,intheautoregressiveself-attentioncasewhenS = T,ğ¿ maybealower-
triangularmatrixof1â€™srepresentingacausalmask.Besidesenforcingcausality,manyothertypesofmaskscanbeapplied
â€“inparticularvarioussparsitypatternssuchasbanded,dilated,orblockdiagonalâ€“whicharemotivatedbyreducingthe
complexityofdenseattention.
Maskedattentionisusuallywritteninmatrixnotationas
ğ‘¦ = (ğ¿â—¦(ğ‘„ğ¾âŠ¤))Â·ğ‘‰. (10)
Moreprecisely,withshapeannotationsandbreakingthisdownintotheprecisesequenceofcomputations:
ğº =ğ‘„ğ¾âŠ¤ (T,S)
ğ‘€ =ğº â—¦ğ¿ (T,S) (11)
ğ‘Œ =ğ‘€ğ‘‰ (T,P)
Ourimprovedderivationofattentionvariantsinthissectionstartsbynoticingthatthisformulacanbewrittenasasingle
contraction:
ğ‘Œ =contract(TN,SN,SP,TSâ†’TP)(ğ‘„,ğ¾,ğ‘‰,ğ¿) (12)
andthealgorithmin(11)canbereframedascomputing(12)byaparticularorderingofpairwisecontractions
ğº =contract(TN,SNâ†’TS)(ğ‘„,ğ¾) (T,S) (13a)
ğ‘€ =contract(TS,TSâ†’TS)(ğº,ğ¿) (T,S) (13b)
ğ‘Œ =contract(TS,SPâ†’TP)(ğ‘€,ğ‘‰) (T,P) (13c)
4.2 LinearAttention
Linearattention, andmanyothervariantsofefficientattention, isoftenmotivatedbychangingtheorderofmatrixas-
sociativity in the core attention computation (ğ‘„ğ¾âŠ¤)ğ‘‰ = ğ‘„(ğ¾âŠ¤ğ‘‰). However when the mask is added, the derivation is
somewhatlessstraightforward(forexample,theoriginalpaper(Katharopoulosetal.2020)andvariants(Y.Sunetal.2023)
statetheformulawithoutproof).
Roughly, thelinearattentionmethodclaimsthatthefollowingformulaisequivalentto(10), whichmustbeverifiedby
expandingthesumandtrackingindicescarefully.
ğ‘Œ =ğ‘„ Â·cumsum(ğ¾âŠ¤ğ‘‰) (14)
13Proposition4.1((Katharopoulosetal.2020)). Autoregressivekernelattention,i.e.maskedkernelattentionwiththecausal
mask,canbecomputedinğ‘‚(ğ‘‡)timebyarecurrencetakingconstanttimeperstep.
4.2.1 ATensorContractionProofofLinearAttention
Wepresentasimpleandrigorousderivationoflinearattentionthatwillalsoimmediatelyrevealhowtogeneralizeit.The
mainideaistoperformthecontraction(12)inanalternateorder.Weavoidambiguousmatrixnotationandworkdirectly
withcontractionnotation:
ğ‘ =contract(SP,SNâ†’SPN)(ğ‘‰,ğ¾) (S,P,N) (15a)
ğ» =contract(TS,SPNâ†’TPN)(ğ¿,ğ‘) (T,P,N) (15b)
ğ‘Œ =contract(TN,TPNâ†’TP)(ğ‘„,ğ») (T,P) (15c)
Intuitively,weinterpretthiscontractionorderasfollows.
The first step (15a) performs an â€œexpansionâ€ into more features, by a factor of the feature dimension N. The third step
(15c)contractstheexpandedfeaturedimensionaway. Ifğ¾ isviewedastheinput(Remark6),thenğ‘‰ andğ‘„ performthe
expansionandcontraction,respectively.
Thesecondstepisthemostcritical,andexplainsthelinear partoflinearattention. Firstnoticethat(15b)isjustadirect
matrixmultiplicationbyğ¿ (sincethe (P,N) axescanbeflattened). Alsonotethatthisistheonlytermthatinvolvesboth
T and S axes, hence should have Î©(TS) complexity (i.e. quadratic in sequence length). However, when the mask ğ¿ is
thestandardcausalattentionmask(lowertriangular1â€™s),matrix-vectormultiplicationbyğ¿isidenticaltoafeature-wise
cumulativesum
1
ï£® ï£¹
ğ‘¦ = ï£¯ ï£¯. . . ... ï£º ï£ºğ‘¥ â‡â‡’ ğ‘¦ 0 =ğ‘¥ 0 .
ï£¯
ï£¯1 ...
1ï£º
ï£º
ğ‘¦
ğ‘¡
=ğ‘¦ ğ‘¡âˆ’1+ğ‘¥
ğ‘¡
ï£¯ ï£º
ï£° ï£»
4.3 StructuredMaskedAttention
Withthe tensorcontraction perspectiveof maskedattention (15), we canimmediately seethat thecrux ofthe original
linearattentionisthefactthatmatrix-vectormultiplicationbythecausalmaskisequivalenttothecumulativesumopera-
tor.
However,weobservethatthereisnoreasontheattentionmaskhastobeall1â€™s.Allthatisnecessaryforlinearattention
tobefastisforğ¿tobeastructuredmatrix,whichbydefinitionarethosethathavefastmatrixmultiplication(Section2.3).
In particular, we can use any mask matrix ğ¿ that has sub-quadratic (ideally linear) matrix-vector multiplication, which
wouldhavethesamecomplexityasstandardlinearattentionbyspeedingupthebottleneckequation(15b).
Definition 4.2. Structured masked attention (SMA) (or structured attention for short) is defined as a function on
queries/keys/valuesğ‘„,ğ¾,ğ‘‰ as well as any structured matrix ğ¿ (i.e. has sub-quadratic matrix multiplication), through the
4-waytensorcontraction
ğ‘Œ =contract(TN,SN,SP,TSâ†’TP)(ğ‘„,ğ¾,ğ‘‰,ğ¿).
The SMA quadratic mode algorithm is the sequence of pairwise contractions defined by (13), which corresponds to the
standard(masked)attentioncomputation.
The SMA linear mode algorithm is the sequence of pairwise contractions defined by (15), where step (15b) is optimized
throughthesubquadraticstructuredmatrixmultiplication.
We can instantiate structured masked attention to any given class of matrix structure. Some examples include (Fig-
ure3):
â€¢ Linearattentionusesacausalmask.
â€¢ RetNet(Y.Sunetal.2023)usesadecaymaskğ¿ ğ‘–ğ‘— =ğ›¾ğ‘–âˆ’ğ‘— Â·I[ğ‘— â‰¥ğ‘–] forsomedecayfactorğ›¾ âˆˆ [0,1].
14Sequence Transformation
Structured Mask $ Matrix %
Keys
"
Causal Mask Linear Attention
Decay Mask Retentive Network
SSD
Queries =
1-semiseparable 1-SS Structured Attention
!
Toeplitz Toeplitz Structured Attention
Discrete Fourier
Fourier Structured Attention
Transform
Figure3: (StructuredMaskedAttention.) SMAconstructsamaskedattentionmatrixğ‘€ =ğ‘„ğ¾âŠ¤â—¦ğ¿foranystructured
matrixğ¿,whichdefinesamatrixsequencetransformationğ‘Œ =ğ‘€ğ‘‰. AllinstancesofSMAhaveadualsubquadraticform
inducedbyadifferentcontractionordering,combinedwiththeefficientstructuredmatrixmultiplicationbyğ¿. Previous
examplesincludeLinearAttention(Katharopoulosetal.2020)andRetNet(Y.Sunetal.2023).BeyondSSD(1-semiseparable
SMA),thefocusofthispaper,manyotherpotentialinstantiationsofstructuredattentionarepossible.
â€¢ ThedecaymaskcouldbegeneralizedtoaToeplitzmatrixğ¿
ğ‘–ğ‘—
=ğ›¼
ğ‘–âˆ’ğ‘—
forsomelearnable(orinput-dependent)setof
parametersğ›¼ âˆˆRT.Thiscanbeinterpretedasaformofrelativepositionalencoding,reminiscentofothermethods
suchasAliBi(Press,N.Smith,andLewis2022)butmultiplicativeinsteadofadditive.
â€¢ AnothervariantcoulduseaFouriermatrixğ¿
ğ‘–ğ‘—
=ğœ”ğ‘–ğ‘—/Ttoencodepositionalstructureadifferentway.
InSection5,weconsidersemiseparableSMA,whichdefinesourmainSSDmodel.
4.3.1 Summary:TheDualFormsofMaskedAttention
Standard(maskedkernel)attentionis oftenconflatedbetweenafunctionandanalgorithm. Separating thisdistinction
presentsaclearwaytounderstanddifferentvariantsofattention.
â€¢ Weviewmaskedattentionasaparticularfunction(12).
â€¢ Thestandardquadraticattentioncomputation(13)canbeviewedasanalgorithmtocomputethefunction.
â€¢ Linearattention(15)isanalternatealgorithmtocomputethesamefunction.
Moreover,inthiscase
â€¢ Themaskedattentionfunctionissimplyaparticularcontractiononfourterms.
â€¢ Thequadraticandlinearattentionalgorithmsaresimplytwodifferentorderstoperformthecontractions.
Itisknownthatcontractionorderingscanmakelargedifferencesincomputationcomplexity,leadingtothequadraticvs.
linearsplit. Justasstatespacemodelsareatransformationthatcanbecomputedinmultipleways,withdualquadratic
vs.linearforms(Section3.4),linearattentionhasasimilardualitythatresultsfromtwocontractionorders. Infact,these
turnouttobedifferentperspectivesonthesameunderlyingduality,whichwemakeexplicitinSection5.
155 State Space Duality
In Sections 3 and 4, we defined structured state space models and structured attention, discussed their properties, and
showedthattheybothhaveaquadraticalgorithmandalinearalgorithm.Thissectionconnectsthemtogether.Ourmain
result is showing that a particular case of structured state space models coincides with a particular case of structured
attention,andthatthelinear-timeSSMalgorithmandquadratic-timekernelattentionalgorithmaredualformsofeach
other.
â€¢ Section5.1specializesstatespacemodelstoscalarstructure,wherethenaivequadraticcomputationcanbeseenas
aninstanceofkernelattention.
â€¢ Section 5.2 specializes structured masked attention to semiseparable SMA, which characterizes masked attention
withefficientautoregression.
â€¢ Section 5.3 summarizes the connection between structured masked attention and structured state space models,
termedstructuredstatespaceduality.
5.1 Scalar-IdentityStructuredStateSpaceModels
InSection3weshowedthatstatespacemodelsareequivalenttosemiseparablematrixtransformations,resultinginboth
alinearrecurrentformandquadraticnaiveform.
RecallthatSSMsaredefinedbyğ‘¦ =SSM(ğ´,ğµ,ğ¶)(ğ‘¥),andthematrixformofSSMsusestheSSS(sequentiallysemisepara-
ble)representationğ‘€ =SSS(ğ´,ğµ,ğ¶)whereğ‘€
ğ‘—ğ‘–
=ğ¶âŠ¤
ğ‘—
ğ´ ğ‘—:ğ‘–ğµ
ğ‘–
(equation(3)).
Nowletusconsiderthecasewhereğ´ issimplyascalar;inotherwords,aninstantiationofastructuredSSMwherethe
ğ‘—
ğ´matricesareextremelystructured:ğ´=ğ‘ğ¼ forscalarğ‘andidentitymatrixğ¼.Thenwecanrearrange
ğ‘€
ğ‘—ğ‘–
=ğ´
ğ‘—:ğ‘–
Â·(ğ¶âŠ¤
ğ‘—
ğµ ğ‘–).
Andthiscanbevectorizedinto
ğ¿ (cid:66) 1SS(ğ‘)
ğ‘€ =ğ¿â—¦(ğ¶ğµâŠ¤)
whereğµ,ğ¶ âˆˆR(T,N).
Usingthisformulation,thefulloutputğ‘Œ =ğ‘€ğ‘‹ iscomputedpreciselyas
ğº =contract(TN,SNâ†’TS)(ğ¶,ğµ) (T,S)
ğ‘€ =contract(TS,TSâ†’TS)(ğº,ğ¿) (T,S) (16)
ğ‘Œ =contract(TS,SPâ†’TP)(ğ‘€,ğ‘‹) (T,P)
whereS=T.Butthisisexactlythesameasoriginaldefinitionofmaskedkernelattentiondefinition(13)!
Therefore, as alluded to in Section 3.4, naively computing the scalar structured SSMâ€”by materializing the semiseparable
matrix ğ‘€ and performing quadratic matrix-vector multiplicationâ€”is exactly the same as quadratic masked kernel atten-
tion.
5.2 1-SemiseparableStructuredMaskedAttention
Structuredmaskedattentionallowsfortheuseofanystructuredmaskğ¿.Whenğ¿isthecausalmask,itisstandardlinear
attention.Notethatthecausalmaskisğ¿ =SS(1 ğ‘‡),i.e.the1-SSmaskisgeneratedbyğ‘
ğ‘¡
=1indefinition(6).Thismotivates
generalizingğ¿ totheclassof1-semiseparablemasks,or1-semiseparablestructuredmaskedattention(1-SSSMA),
wherethecumsuminlinearattentionâ€™srecurrenceisreplacedbyamoregeneralrecurrenceâ€“thescalarSSMscan, i.e.
1-semiseparablematrixmultiplication(Section3.2.2).
Finally, the most important reason we consider 1-semiseparable SMA is because the linear form for computing it is a
special case of diagonal state space model. The linear form of SMA is algorithm (15), where the bottleneck step (15b)
16canbeviewedasmatrixmultiplicationbythe1-SSmask. InSection3,wealsowroteoutthecomputationforadiagonal
SSM(8),wherethebottleneckstep(8b)isascalarSSMrecurrencewhichisequivalentto1-SSmultiplication. Theonly
differenceisthat(8b)hasanextraNdimensioninğ¿,becausethematrixğ´isadiagonalmatrixofsizeN.ThisNdimension
woulddisappearifalldiagonalentriesofğ´arethesame,whichresultsinCorollary5.1.
Corollary5.1. 1-SSSMA(maskedattentionwith1-semiseparablestructuredmatricesğ¿)(15)isaspecialcaseofadiagonal
SSM (8)wherethediagonalmatrixisascalarmultipleoftheidentity.
WhileCorollary5.1saysthat1-SSSMAhasanefficientrecurrentform,wecanalsoshowaconverseresultthatcharac-
terizeswhichinstancesofSMAhasefficientautoregression.
Theorem 5.2. For any instantiation of structured masked attention (Definition 4.2) that is an autoregressive process with
boundedorder,thestructuredmaskğ¿mustbeasemiseparablematrix.
Inotherwords,efficientautoregressiveattentionisgeneralsemiseparableSMA.Theorem5.2isprovedinAppendixC.2.
Remark7. While1-semiseparableSMAisaspecialcaseofastatespacemodel,generalsemiseparableSMAisstrictlymore
expressivethan1-SSSMA,andcannotbedescribedbyastandardSSM.However,thesemiseparablemultiplicationbyğ¿ and
thelinearformofSMA(equation(15a))eachinvolveanexpansionandcontractionstep,andcanbeabsorbedintoasimilar
instanceof1-SSSMAwithasingle(larger)expansion.
Insummary,1-semiseparablestructuredattentionisthemostimportantcaseofSMA,becauseitis:
â€¢ anaturalgeneralizationoflinearattentionwithaninput-dependentrecurrence.
â€¢ thesimplestcaseofgeneralsemiseparableattention,whichisequivalenttoefficientautoregressiveattention.
â€¢ aspecialcaseofadiagonalstatespacemodel.
5.3 StructuredState-SpaceDuality(SSD)
Tosummarizeourresults:
â€¢ Structuredstate-spacemodels(Section3)areamodelusuallydefinedthroughalinear-timerecurrence. However,
byexpandingthematrixformulationcharacterizingitslinearsequence-to-sequencetransformation,onecanderive
aquadraticform.
â€¢ Attentionvariants(Section4)areamodeldefinedthroughquadratic-timepairwiseinteractions.However,byview-
ingitasafour-waytensorcontractionandreducinginadifferentorder,onecanderivealinearform.
â€¢ A natural special case of each one â€“ more precisely, state space models with scalar-identity structure on the ğ´
matrices,andstructuredmaskedattentionwith1-semiseparablestructureonitsğ¿ maskâ€“aredualsofeachother
withtheexactsamelinearandquadraticforms.
Figure4summarizesthedualitybetweenthesetworepresentations.
Anextendedrelatedworkanddiscussion(Section10)describestherelationshipbetweenSSDandgeneralSSMs/attention
inmoredetail.
6 A Hardware-Efficient Algorithm for SSD Models
ThebenefitsofdevelopingthetheoreticalSSDframeworkbetweenSSMs,attention,andstructuredmatricesliesinusing
theconnectionstoimprovethemodelsandalgorithms. Inthissection,weshowhowvariousalgorithmsforcomputing
SSDmodelsefficientlycanbederivedfromvariousalgorithmsforcomputingstructuredmatrixmultiplication.
OurmaincomputationalresultisanalgorithmforcomputingSSDmodelsthatcombinesboththelinear(recurrent)mode
andquadratic(attention)mode.ThisalgorithmisascomputationefficientasSSMs(linearscalinginsequencelength)and
ashardware-friendlyasattention(primarilyusesmatrixmultiplications).
Theorem6.1. ConsideranSSDmodelwithstateexpansionfactorNandheaddimensionP = N. Thereexistsanalgorithm
forcomputingthemodelonanyinputğ‘‹ âˆˆR(T,P) whichonlyrequiresğ‘‚(TN2)trainingFLOPs,ğ‘‚(TN)inferenceFLOPs,ğ‘‚(N2)
inferencememory,andwhoseworkisdominatedbymatrixmultiplications.
17StructuredStateSpaceModel StructuredMaskedAttention Structured State Space Model (SSM) Structured
State Space
ğ¶ (contractionmatrix) ğ‘„ (queries) S4 Diagonal State Space Model Duality (SSD)
ğµ (expansionmatrix) ğ¾ (keys)
ğ‘‹ (inputsequence) ğ‘‰ (values) DSS Scalar-Identity SSM
ğ´ ğ‘—:ğ‘– (statematrix) ğ¿ ğ‘—ğ‘– (mask) S4D RetNet GateLoop
N (stateexpansiondim.) N (kernelfeaturedim.) S5 TransNormer
ğ» (hiddenstates(8b)) S6 Linear Attention
SMAlineardual(15) 1-Semiseparable SMA
=ğ¿Â·ğ‘‹ğµ (linearmode)
Efficient Autoregressive Attention
ğº (Grammatrix(13a)) Semiseparable SMA
SSMquadraticdual(16)
=ğ‘„Â·ğ¾âŠ¤ (quadraticmode)
Structured Masked Attention (SMA)
Figure 4: (Structured State Space Duality.) State space duality describes the close relationship between state space
modelsandmaskedattention.(Left)GeneralSSMsandSMAbothpossesslinearandquadraticforms,withdirectanalogs
innotation.(Right)SSMsandSMAintersectatalargeclassofstatespacedualmodels(SSD)whichcapturemanysequence
modelsasspecialcases.
Note that all of these bounds are tight, because a state space model with state expansion N operating on a head size of
NhastotalstatesizeN2 (yieldingthelowerboundsfortrainingandinferenceFLOPsofğ‘‚(TN2) andğ‘‚(N2) respectively).
Furthermoretheinputğ‘‹ itselfhasTNelements,yieldingthememorylowerbound.
ThemainideabehindTheorem6.1isonceagainviewingtheproblemofcomputingastatespacemodelasasemiseparable
matrixmultiplication,butleveragingitsstructureinanewway.Insteadofcomputingthewholematrixineitherrecurrent
orattentionmode,weperformablockdecompositionofthematrix. Thediagonalblockscanbecomputedusingthedual
attentionmode,whichcanbeefficientlydonewithmatrixmultiplications,whiletheoff-diagonalblockscanbefactored
bytherank-structureofsemiseparablematricesandreducedtoasmallerrecurrence.WehighlightthatListing1provides
a self-contained implementation of the SSD algorithm. Compared to the general selective SSM of Gu and Dao (2023),
thisimplementationismuchsimpler,andrelativelyefficienteveninnativePyTorchwithoutrequiringspeciallow-level
kernels.
Tobegin,wepartitionthematrixğ‘€ intoa T Ã— T gridofsubmatricesofsizeQÃ—Q,forsomeblocksizeQ. Notethatthe
Q Q
off-diagonalblocksarelow-rankbythedefiningpropertyofsemiseparablematrices(Definition3.1).5
ğ‘€(0,0)
ï£® ï£¹
ï£¯ ğ‘€(1,0) ğ‘€(1,1) ï£º
ï£¯ ï£º
(BlockDecomposition) ğ‘€ = ï£¯ ï£¯ . . . . . . ... ï£º ï£º
ï£¯ ï£º
ï£¯ ï£¯ğ‘€(T/Qâˆ’1,0) ğ‘€(T/Qâˆ’1,1) ... ğ‘€(T/Qâˆ’1,T/Qâˆ’1)ï£º ï£º
ï£° ï£»
(DiagonalBlock) ğ‘€(ğ‘—,ğ‘—) =SSM(ğ´ ğ‘—Q:(ğ‘—+1)Q,ğµ ğ‘—Q:(ğ‘—+1)Q,ğ¶ ğ‘—Q:(ğ‘—+1)Q)
ğ¶âŠ¤ğ´ ğµâŠ¤ğ´ âŠ¤
ï£® ğ‘—Q ğ‘—Q:ğ‘—Qâˆ’1 ï£¹ ï£® ğ‘–Q (ğ‘–+1)Qâˆ’1:ğ‘–Q ï£¹
ï£¯ . ï£º ï£¯ . ï£º
(Low-RankBlock) ğ‘€(ğ‘—,ğ‘–) = ï£¯ ï£¯ . . ï£º ï£ºğ´ ğ‘—Qâˆ’1:(ğ‘–+1)Qâˆ’1 ï£¯ ï£¯ . . ï£º ï£º
ï£¯ğ¶âŠ¤ ğ´ ï£º ï£¯ğµâŠ¤ ğ´ ï£º
ï£¯
ï£° (ğ‘—+1)Qâˆ’1
(ğ‘—+1)Qâˆ’1:ğ‘—Qâˆ’1ï£º
ï£»
ï£¯
ï£° (ğ‘–+1)Qâˆ’1
(ğ‘–+1)Qâˆ’1:(ğ‘–+1)Qâˆ’1ï£º
ï£»
Thisiseasiestillustratedthroughanexample,e.g.forT = 9anddecomposingintochunksoflengthQ = 3. Theshaded
5Notethattheblockdecompositionisvalidevenwithpartitionsofvaryingsize,e.g.ifQâˆ¤T,butweassumeevendivisibilityforsimplicity.
18cellsarelow-rankfactorizationsoftheoff-diagonalblocksofthesemiseparablematrix.
ğ¶âŠ¤ğ´ ğµ
ï£® 0 0:0 0 ï£¹
ï£¯ ï£º
ï£¯ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ï£º
ï£¯ 1 1:0 0 1 1:1 1 ï£º
ï£¯ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ï£º
ï£¯ 2 2:0 0 2 2:1 1 2 2:2 2 ï£º
ï£¯ ï£º
ï£¯ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ï£º
ï£¯ 3 3:0 0 3 3:1 1 3 3:2 2 3 3:3 3 ï£º
ğ‘€ = ï£¯ ï£¯ğ¶ 4âŠ¤ğ´ 4:0ğµ 0 ğ¶ 4âŠ¤ğ´ 4:1ğµ 1 ğ¶ 4âŠ¤ğ´ 4:2ğµ 2 ğ¶ 4âŠ¤ğ´ 4:3ğµ 3 ğ¶ 4âŠ¤ğ´ 4:4ğµ 4 ï£º ï£º
ï£¯ ï£º
ï£¯ ï£¯ğ¶ 5âŠ¤ğ´ 5:0ğµ 0 ğ¶ 5âŠ¤ğ´ 5:1ğµ 1 ğ¶ 5âŠ¤ğ´ 5:2ğµ 2 ğ¶ 5âŠ¤ğ´ 5:3ğµ 3 ğ¶ 5âŠ¤ğ´ 5:4ğµ 4 ğ¶ 5âŠ¤ğ´ 5:5ğµ 5 ï£º
ï£º
ï£¯ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ï£º
ï£¯ 6 6:0 0 6 6:1 1 6 6:2 2 6 6:3 3 6 6:4 4 6 6:5 5 6 6:6 6 ï£º
ï£¯ ï£º
ï£¯ ï£¯ğ¶ 7âŠ¤ğ´ 7:0ğµ 0 ğ¶ 7âŠ¤ğ´ 7:1ğµ 1 ğ¶ 7âŠ¤ğ´ 7:2ğµ 2 ğ¶ 7âŠ¤ğ´ 7:3ğµ 3 ğ¶ 7âŠ¤ğ´ 7:4ğµ 4 ğ¶ 7âŠ¤ğ´ 7:5ğµ 5 ğ¶ 7âŠ¤ğ´ 7:6ğµ 6 ğ¶ 7âŠ¤ğ´ 7:7ğµ 7 ï£º
ï£º
ï£¯ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ï£º
ï£¯ 8 8:0 0 8 8:1 1 8 8:2 2 8 8:3 3 8 8:4 4 8 8:5 5 8 8:6 6 8 8:7 7 8 8:8 8ï£º
ï£° ï£»
ğ¶âŠ¤ğ´ ğµ
ï£® 0 0:0 0 ï£¹
ï£¯ ï£º
ï£¯ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ï£º
ï£¯ 1 1:0 0 1 1:1 1 ï£º
ï£¯ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ğ¶âŠ¤ğ´ ğµ ï£º
ï£¯ 2 2:0 0 2 2:1 1 2 2:2 2 ï£º
ï£¯ ï£º
ï£¯ ï£¯ ï£®ğ¶ 3âŠ¤ğ´ 3:2ï£¹ ï£®ğµ 0âŠ¤ğ´ 2:0ï£¹âŠ¤ ğ¶ 3âŠ¤ğ´ 3:3ğµ 3 ï£º ï£º
= ï£¯ ï£¯ ï£¯ ï£¯ğ¶ 4âŠ¤ğ´ 4:2ï£º ï£ºğ´ 2:2 ï£¯ ï£¯ğµ 1âŠ¤ğ´ 2:1ï£º ï£º ğ¶ 4âŠ¤ğ´ 4:3ğµ 3 ğ¶ 4âŠ¤ğ´ 4:4ğµ 4 ï£º ï£º
ï£¯ ï£¯ ï£º ï£¯ ï£º ï£º
ï£¯
ï£¯
ï£¯ ï£°ğ¶ 5âŠ¤ğ´ 5:2ï£º
ï£»
ï£¯ ï£°ğµ 2âŠ¤ğ´ 2:2ï£º
ï£»
ğ¶ 5âŠ¤ğ´ 5:3ğµ 3 ğ¶ 5âŠ¤ğ´ 5:4ğµ 4 ğ¶ 5âŠ¤ğ´ 5:5ğµ 5 ï£º
ï£º
ï£¯ ï£¯ ï£¯ ï£¯ ï£¯ ï£® ï£¯ ï£¯ ï£¯ğ¶ğ¶ 76 âŠ¤âŠ¤ ğ´ğ´ 76 :: 55ï£¹ ï£º ï£º ï£ºğ´ 5:2 ï£® ï£¯ ï£¯ ï£¯ğµ ğµ0 1âŠ¤ âŠ¤ğ´ ğ´2 2: :0 1ï£¹ ï£º ï£º ï£ºâŠ¤ ï£® ï£¯ ï£¯ ï£¯ğ¶ ğ¶6 7âŠ¤ âŠ¤ğ´ ğ´6 7: :5 5ï£¹ ï£º ï£º ï£ºğ´ 5:5 ï£® ï£¯ ï£¯ ï£¯ğµ ğµ3 4âŠ¤ âŠ¤ğ´ ğ´5 5: :3 4ï£¹ ï£º ï£º ï£ºâŠ¤ ğ¶ ğ¶6 7âŠ¤ âŠ¤ğ´ ğ´6 7: :6 6ğµ ğµ6 6 ğ¶ 7âŠ¤ğ´ 7:7ğµ 7 ï£º ï£º ï£º ï£º ï£º
ï£¯ ï£¯ ï£¯ ï£°ğ¶ 8âŠ¤ğ´ 8:5ï£º ï£» ï£¯ ï£°ğµ 2âŠ¤ğ´ 2:2ï£º ï£» ï£¯ ï£°ğ¶ 8âŠ¤ğ´ 8:5ï£º ï£» ï£¯ ï£°ğµ 5âŠ¤ğ´ 5:5ï£º ï£» ğ¶ 8âŠ¤ğ´ 8:6ğµ 6 ğ¶ 8âŠ¤ğ´ 8:7ğµ 7 ğ¶ 8âŠ¤ğ´ 8:8ğµ 8ï£º ï£º
ï£° ï£»
From here we can reduce the problem into these two parts. These can also be interpreted as dividing the output of a
â€œchunkâ€ğ‘¦ intotwocomponents:theeffectofinputswithinthechunkğ‘¥ ,andtheeffectofinputsbeforethe
ğ‘—Q:(ğ‘—+1)Q ğ‘—Q:(ğ‘—+1)Q
chunkğ‘¥ .
0:ğ‘—Q
6.1 DiagonalBlocks
Thediagonalblocksareeasytohandle, becausetheyaresimplyself-similarproblemsofasmallersize. The ğ‘—-thblock
represents computing the answer SSM(ğ´ ğ‘…,ğµ ğ‘…,ğ¶ ğ‘…)(ğ‘¥ ğ‘…) for the range ğ‘… = ğ‘—Q : (ğ‘— +1)Q = (ğ‘—Q,ğ‘—Q+1,...,ğ‘—Q+Qâˆ’1).
The key is that this block can be computed using any desired method. In particular, for small chunk lengths Q, this
problemiscomputedmoreefficientlyusingthedualquadraticSMAform. Additionally,thechunkscanbecomputedin
parallel.
Thesesubproblemscanbeinterpretedas: whatistheoutputperchunksupposingthattheinitialstate(tothechunk)is0.
Inotherwordsforchunk ğ‘—,thiscomputesthecorrectoutputstakingintoaccountonlythechunkinputsğ‘¥ .
ğ‘—Q:(ğ‘—+1)Q
6.2 Low-RankBlocks
The low-rank factorizations consist of 3 terms, and there are correspondingly three pieces of the computation. In this
factorization,wewillusetheterminology
ğµâŠ¤ğ´ âŠ¤
ï£® 0 2:0ï£¹
â€¢ Thetermslike ï£¯ğµâŠ¤ğ´ ï£º arecalledtherightfactorsorğµ-block-factors.
ï£¯ 1 2:1ï£º
ï£¯ ï£º
ï£¯ ï£°ğµ 2âŠ¤ğ´ 2:2ï£º
ï£»
â€¢ Thetermslikeğ´ arecalledthecenterfactorsorğ´-block-factors.
5:2
ğ¶âŠ¤ğ´
ï£® 6 6:5ï£¹
â€¢ Thetermslike ï£¯ğ¶âŠ¤ğ´ ï£º arecalledtheleftfactorsorğ¶-block-factors.
ï£¯ 7 7:5ï£º
ï£¯ ï£º
ï£¯ ï£°ğ¶ 8âŠ¤ğ´ 8:5ï£º
ï£»
19Semiseparable Matrix ğ‘€
Block Decomposition
DiagonalBlock: Input â†’ Output
Low-Rank Block: Input â†’ State
Outputs ğ‘Œ Low-Rank Block: State â†’ State
Low-Rank Block: State â†’ Output
States ğ»
Inputs ğ‘‹
Figure 5: (SSD Algorithm.) By using the matrix transformation viewpoint of state space models to write them as
semiseparablematrices(Section3),wedevelopamorehardware-efficientcomputationoftheSSDmodelthroughablock-
decomposition matrix multiplication algorithm. The matrix multiplication also has an interpretation as a state space
model,whereblocksrepresentchunkingtheinputandoutputsequence. Diagonalblocksrepresentintra-chunkcompu-
tationsandtheoff-diagonalblocksrepresentinter-chunkcomputations,factoredthroughtheSSMâ€™shiddenstate.
RightFactors. Thisstepcomputesthemultiplicationbytherightğµ-block-factorsofthelow-rankfactorization. Note
that for each chunk, this is a (N,Q) by (Q,P) matrix multiplication, where N is the state dimension and ğ‘ƒ is the head
dimension. Theresultisa (N,P) tensorforeachchunk,whichhasthesamedimensionalityastheexpandedhiddenstate
â„.
This can be interpreted as: what is the final state per chunk supposing that the initial state (to the chunk) is 0. In other
wordsthiscomputesâ„ ğ‘—Q+Qâˆ’1assumingthatğ‘¥
0:ğ‘—Q
=0.
CenterFactors. Thisstepcomputestheeffectofthecenterğ´-block-factorstermsinthelow-rankfactorization.Inthe
previousstep,thefinalstatesperchunkhavetotalshape(T/Q,N,P).Thisisnowmultipliedbya1-SSmatrixgeneratedby
ğ´Ã— ,ğ´Ã— ,...,ğ´Ã— .
2Qâˆ’1:Qâˆ’1 3Qâˆ’1:2Qâˆ’1 Tâˆ’1:Tâˆ’Qâˆ’1
This step can be computed by any algorithm for computing 1-SS multiplication (also known as the scalar SSM scan or
cumprodsumoperator).
Thiscanbeinterpretedas:whatistheactualfinalstateperchunktakingintoaccountallpreviousinputs;inotherwords,
thiscomputesthetruehiddenstateâ„ takingintoaccountallofğ‘¥ .
ğ‘—Q 0:(ğ‘—+1)Q
LeftFactors. Thisstepcomputesthemultiplicationbytheleftğ¶-block-factorsofthelow-rankfactorization. Foreach
chunk,thiscanberepresentedbyamatrixmultiplicationcontract(QN,NPâ†’QP).
Thiscanbeinterpretedas: whatistheoutputperchunktakingintoaccountthecorrectinitialstateâ„ ğ‘—Qâˆ’1,andsupposing
theinputsğ‘¥ ğ‘—Q:(ğ‘—+1)Qare0.Inotherwordsforchunk ğ‘—,thiscomputesthecorrectoutputstakingintoaccountonlytheprior
inputsğ‘¥ .
0:ğ‘—Q
6.3 ComputationalCost
We define the notation BMM(B,M,N,K) to define a batched matrix multiplication contract(MK,KNâ†’MN) with batch di-
mensionB.Fromthisnotationwecaninferthreeaspectsoftheefficiency:
â€¢ Computationcost:totalofğ‘‚(BMNK)FLOPs.
â€¢ Memorycost: totalofğ‘‚(B(MK+KN+MN))space.
20Listing1FullPyTorchexampleofthestatespacedual(SSD)model.
def segsum(x):
"""Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
which is equivalent to a scalar SSM."""
T = x.size(-1)
x_cumsum = torch.cumsum(x, dim=-1)
x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)
x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
return x_segsum
def ssd(X, A, B, C, block_len=64, initial_states=None):
"""
Arguments:
X: (batch, length, n_heads, d_head)
A: (batch, length, n_heads)
B: (batch, length, n_heads, d_state)
C: (batch, length, n_heads, d_state)
Return:
Y: (batch, length, n_heads, d_head)
"""
assert X.dtype == A.dtype == B.dtype == C.dtype
assert X.shape[1] % block_len == 0
# Rearrange into blocks/chunks
X, A, B, C = [rearrange(x, "b (c l) ... -> b c l ...", l=block_len) for x in (X, A, B, C)]
A = rearrange(A, "b c l h -> b h c l")
A_cumsum = torch.cumsum(A, dim=-1)
# 1. Compute the output for each intra-chunk (diagonal blocks)
L = torch.exp(segsum(A))
Y_diag = torch.einsum("bclhn,bcshn,bhcls,bcshp->bclhp", C, B, L, X)
# 2. Compute the state for each intra-chunk
# (right term of low-rank factorization of off-diagonal blocks; B terms)
decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))
states = torch.einsum("bclhn,bhcl,bclhp->bchpn", B, decay_states, X)
# 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries
# (middle term of factorization of off-diag blocks; A terms)
if initial_states is None:
initial_states = torch.zeros_like(states[:, :1])
states = torch.cat([initial_states, states], dim=1)
decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
new_states = torch.einsum("bhzc,bchpn->bzhpn", decay_chunk, states)
states, final_state = new_states[:, :-1], new_states[:, -1]
# 4. Compute state -> output conversion per chunk
# (left term of low-rank factorization of off-diagonal blocks; C terms)
state_decay_out = torch.exp(A_cumsum)
Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states, state_decay_out)
# Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
Y = rearrange(Y_diag+Y_off, "b c l h p -> b (c l) h p")
return Y, final_state
â€¢ Parallelization: largerM,N,Ktermscanleveragespecializedmatrixmultiplicationunitsonmodernaccelerators.
CenterBlocks. ThecostofthequadraticSMAcomputationconsistsofthreesteps(equation(16)):
â€¢ Computingthekernelmatrixğ¶âŠ¤ğµ,whichhascostBMM(T/Q,Q,Q,N).
â€¢ Multiplyingbythemaskmatrix,whichisanelementwiseoperationontensorsofshape(T/Q,Q,Q).
â€¢ Multiplyingbytheğ‘‹ values,whichhascostBMM(T/Q,Q,P,N)
21Low-RankBlocks:RightFactors. ThisstepisasinglematrixmultiplicationwithcostBMM(T/Q,N,P,Q).
Low-Rank Blocks: Center Factors. This step is a scalar SSM scan (or 1-SS multiplication) of length T/Q on (N,P)
independentchannels.TheworkofthisscanisTNP/Q,whichisnegligiblecomparedtotheotherfactors.
NotethatbecauseoftheblockingwhichreducesthelengthofthesequencefromTtoT/Q,thisscanhasQtimessmallercost
thanapureSSMscan(e.g.theselectivescanofMamba).Thusweobservethatonmostproblemlengths,otheralgorithms
(AppendixB)maybemoreefficientormucheasiertoimplementwithoutasignificantslowdown. Forexample,anaive
implementationofthisvia1-SSmatrixmultiplicationhascostBMM(1,T/Q,NP,T/Q),whichismucheasiertoimplement
andcanbemoreefficientthananaiverecurrence/scanimplementation.
Low-RankBlocks:LeftFactors. ThisstepisasinglematrixmultiplicationwithcostBMM(T/Q,Q,P,N).
TotalCost. IfwesetN=P=Q(inotherwordsthestatedimension,headdimension,andchunklengthareequal),then
allBMMtermsabovebecomeBMM(T/N,N,N,N).Thecomputationalchacteristicsofthisare:
â€¢ TotalFLOPcountofğ‘‚(TN2).
â€¢ Totalmemoryofğ‘‚(TN).
â€¢ Theworkconsistsprimarilyofmatrixmultiplicationsonmatricesofshape(N,N).
Notice that the memory consumption is tight; the inputs and outputs ğ‘¥,ğ‘¦ have shape (T,P) = (T,N). Meanwhile the
flop count reflects an extra factor of N, which is cost incurred by the autoregressive state size and is common to all
models.
Asidefromthematmuls,thereisascalarSSMscanonNP=N2featuresandsequencelengthT/Q.Thishascostğ‘‚(T/QN2)
FLOPsandğ‘‚(log(T/Q))depth.Althoughitdoesnotusematrixmultiplications,itisstillparallelizableandthetotalwork
doneisnegligiblecomparedtotheothersteps;thishasanegligiblecostinourGPUimplementation.
ComparisontoPureSSMandAttentionModels. Quadraticattentionisalsoveryhardwareefficientbyonlylever-
agingmatrixmultiplications,buthasT2ğ‘ totalFLOPs. Itsslowercomputationspeedatbothtrainingandinferencecan
directlybeseenasaconsequenceofhavingalargerstatesizeâ€“standardattentionhasastatesizescalingwithsequence
lengthTbecauseitcachesitshistoryanddoesnotcompressitsstate.
LinearSSMshaveTNP = TN2 totalFLOPs, whichisthesameasSSD.However, anaiveimplementationrequiresastate
expansion (15a) that materializes extra memory, and a scalar operation (15b) that does not leverage matrix multiplica-
tions.
Attention SSM SSD
Statesize T N N
TrainingFLOPs T2N TN2 TN2
InferenceFLOPs TN N2 N2
(Naive)memory T2 TN2 TN
Matrixmultiplication âœ“ âœ“
Wenotethatmanyothermatrixdecompositionsarepossible(forexample,seeAppendixBforacompendiumofalgorithms
for1-SSmultiplicationthroughdifferentstructuredmatrixdecompositions)whichmayleadtomorealgorithmsforSSDs
thatcouldbebetterforotherspecializedsettings. Evenmorebroadly, wenotethatsemiseparablematriceshavearich
literature and many more representations besides the SSS form that we use (Definition 3.2), and even more efficient
algorithmsmaybepossible.
7 The Mamba-2 Architecture
ByconnectingSSMsandattention,theSSDframeworkallowsustodevelopasharedvocabularyandlibraryoftechniques
for both. In this section we discuss some examples of understanding and modifying SSD layers using ideas originally
22X N
Y
X
SSM
Y
A X BC SSM Linear projection
A X BC
Sequence transformation
! ! ! !
Nonlinearity (activation,
Conv Conv normalization, multiplication)
Sequential Mamba Block Parallel Mamba Block
Figure 6: (Mamba-2 Architecture.) The Mamba-2 block simplifies the Mamba block by removing sequential linear
projections;theSSMparametersğ´,ğµ,ğ¶areproducedatthebeginningoftheblockinsteadofasafunctionoftheSSMinput
ğ‘‹. An additional normalization layer is added as in NormFormer (Shleifer, Weston, and Ott 2021), improving stability.
Theğµandğ¶ projectionsonlyhaveasingleheadsharedacrosstheğ‘‹ heads,analogoustomulti-valueattention(MVA).
developed for Transformers. We discuss several design choices, resulting in the Mamba-2 architecture. These axes of
variationareablatedinSection9.4.
7.1 BlockDesign
Wefirstdiscussmodificationstotheneuralnetworkblockthatareindependentoftheinnersequencemixinglayer(i.e.
outsidethecoreSSDlayer).
ParallelParameterProjections. Mamba-1wasmotivatedbyanSSM-centricpointofviewwheretheselectiveSSM
layerisviewedasamapfromğ‘‹ â†¦â†’ğ‘Œ.TheSSMparametersğ´,ğµ,ğ¶ areviewedassubsidiaryandarefunctionsoftheSSM
inputğ‘‹.Thusthelinearprojectionsdefining(ğ´,ğµ,ğ¶)occuraftertheinitiallinearprojectiontocreateğ‘‹.
InMamba-2,theSSDlayerisviewedasamapfromğ´,ğ‘‹,ğµ,ğ¶ â†¦â†’ğ‘Œ.Itthereforemakessensetoproduceğ´,ğ‘‹,ğµ,ğ¶inparallel
withasingleprojectionatthebeginningoftheblock.Notetheanalogytostandardattentionarchitectures,whereğ‘‹,ğµ,ğ¶
correspondtotheğ‘„,ğ¾,ğ‘‰ projectionsthatarecreatedinparallel.
Notethatadoptingparallelprojectionsfortheğ´,ğµ,ğ¶,ğ‘‹ inputstotheSSMslightlyreducesparametersandmoreimpor-
tantlyismoreamenabletotensorparallelismforlargermodels,byusingstandardMegatronshardingpatterns(Shoeybi
etal.2019)).
ExtraNormalization. Inpreliminaryexperiments,wefoundthatinstabilitieswerepronetoarisinginlargermodels.
Wewereabletoalleviatethisbyaddinganextranormalizationlayer(e.g.LayerNorm,GroupNorm,orRMSNorm)tothe
blockrightbeforethefinaloutputprojection. ThisusageofanormalizationismostdirectlyrelatedtotheNormFormer
architecture (Shleifer, Weston, and Ott 2021), which also added normalization layers at the end of the MLP and MHA
blocks.
WealsonotethatthischangeissimilartootherrecentmodelsrelatedtoMamba-2thatwerederivedfromalinearattention
viewpoint. Theoriginallinearattentionformulationnormalizesbyadenominatortermthatemulatesthenormalization
ofthesoftmaxfunctioninstandardattention.TransNormerLLM(Qin,DongLi,etal.2023)andRetNet(Y.Sunetal.2023)
findthatthisnormalizationisunstableandaddanextraLayerNormorGroupNormafterthelinearattentionlayer. Our
extranormalizationlayerdiffersslightlyfromthese,occuringafterthemultiplicativegatebranchinsteadofbefore.
237.2 MultiheadPatternsforSequenceTransformations
RecallthatSSMsaredefinedasasequencetransformation(Definition2.1)where:
â€¢ ğ´,ğµ,ğ¶ parametershaveastatedimensionN.
â€¢ TheydefineasequencetransformationRT â†’RT,whichforexamplecanberepresentedasamatrixğ‘€ âˆˆR(T,T).
â€¢ Thistransformationoperatesoveraninputsequenceğ‘‹ âˆˆR(T,P),independentlyoverthePaxis.
Onecanviewthisasdefiningonehead ofthesequencetransformation.
Definition7.1(Multiheadpatterns). AmultiheadsequencetransformationconsistsofHindependentheads,foratotalmodel
dimensionofD=d_model.Theparametersmaybetiedacrossheads,leadingtoaheadpattern.
ThestatesizeNandheaddimensionPareanalogoustotheğ‘„ğ¾headdimensionandğ‘‰ headdimensionofattention,respec-
tively. JustasinmodernTransformerarchitectures(Chowdheryetal.2023;Touvron,Lavril,etal.2023),inMamba-2we
generallychoosethesetobeconstantsaround64or128;whenthemodeldimensionDincreases,weincreasethenumberof
headswhilekeepingtheheaddimensionsNandPfixed.Inordertodescribehowtodothis,wecantransferandgeneralize
ideasfrommultiheadattentiontodefinesimilarpatternsforSSMs,oranygeneralsequencetransformation.
Multi-headSSM Multi-contractSSM Multi-expandSSM Multi-inputSSM
(Multi-headAttn.) (Multi-queryAttn.) (Multi-keyAttn.) (Multi-valueAttn.)
ğ‘‹ (T,H,P) ğ‘‹ (T,1,P) ğ‘‹ (T,1,P) ğ‘‹ (T,H,P)
(17) (18) (19) (20)
ğ´ (T,H) ğ´ (T,H) ğ´ (T,H) ğ´ (T,H)
ğµ (T,H,N) ğµ (T,1,N) ğµ (T,H,N) ğµ (T,1,N)
ğ¶ (T,H,N) ğ¶ (T,H,N) ğ¶ (T,1,N) ğ¶ (T,1,N)
Multihead SSM (MHS) / Multihead Attention (MHA) Pattern. The classic MHA pattern assumes that the head
dimensionPdividesthemodeldimensionD.ThenumberofheadsisdefinedasH=D/P.Then,Hcopiesofthecoresequence
transformationare createdbycreating H independentcopies ofeachparameter. Notethat whiletheMHApattern was
firstdescribedfortheattentionsequencetransformation,itcanbeappliedtoanythingcompatiblewithDefinition2.1.For
example,amulti-headSSDlayerwouldacceptinputswithshapesaccordingtoequation(17)wheretheSSDalgorithmis
broadcastedovertheH=n_headsdimension.
Multi-contractSSM(MCS)/Multi-queryAttention(MQA)Pattern. Multi-queryattention(Shazeer2019)isaclever
optimizationforattentionthatcandramaticallyimprovethespeedofautoregressiveinference,whichreliesoncaching
theğ¾ andğ‘‰ tensors.Thistechniquesimplyavoidsgivingğ¾ andğ‘‰ theextraheaddimension,orinotherwordsbroadcasts
asingleheadof(ğ¾,ğ‘‰)acrossalltheheadsofğ‘„.
Usingthestatespaceduality,wecandefineanequivalentSSMversionofMQAasequation(18).Here,ğ‘‹ andğµ(theSSM
analogsofattentionâ€™sğ‘‰ andğ¾)aresharedacrosstheHheads.Wealsocallthisthemulti-contractSSM(MCS)headpattern,
becausetheğ¶ parameterwhichcontrolstheSSMstatecontractionhasindependentcopiesperhead.
Wecansimilarlydefineamulti-keyattention(MKA)ormulti-expandSSM(MES)headpattern,whereğµ (whichcontrols
theSSMexpansion)isindependentperheadwhileğ¶ andğ‘‹ aresharedacrossheads.
Multi-inputSSM(MIS)/Multi-valueAttention(MVA)Pattern. WhileMQAmakessenseforattentionbecauseof
its KV cache, it is not the natural choice for SSMs. In Mamba, instead,ğ‘‹ is viewed as the main input to the SSM, and
thereforeğµandğ¶areparametersthataresharedacrosstheinputchannels.Wedefineanewmulti-valueattention(MVA)
of multi-input SSM (MIS) pattern in equation (20), which can again be applied to any sequence transformation such as
SSD.
Armedwiththisvocabulary,wecancharacterizetheoriginalMambaarchitecturemoreprecisely.
Proposition7.2. TheselectiveSSM(S6)layeroftheMambaarchitecture(GuandDao2023)canbeviewedashaving
24â€¢ Headdimensionğ‘ƒ =1:everychannelhasindependentSSMdynamicsğ´.
â€¢ Multi-inputSSM(MIS)or multi-valueattention(MVA)headstructure:theğµ,ğ¶ matrices(correspondingtoğ¾,ğ‘„ inthe
attentionduality)aresharedacrossallchannelsoftheinputğ‘‹ (correspondingtoğ‘‰ inattention).
WecanalsoablatetheseheadpatternvariantswhenappliedtoSSD(Section9.4.3).Interestingly,despitebeingcontrolled
inparametercountsandtotalstatedimension,thereisanoticeabledifferenceindownstreamperformance.Weempirically
findthattheMVApatternasoriginallyusedinMambaperformsbest.
Grouped Head Patterns. The ideas of multi-query attention can be extended to grouped-query attention (Ainslie et
al. 2023): instead of 1 K and V head, one can create G independent K and V heads, where 1 < G and G divides H. This
is motivated both by bridging the performance difference between multi-query and multi-head attention, and enabling
moreefficienttensorparallelismbysettingGtobeamultipleofthenumberofshards(Section8).
Similarly, the multi-input SSM head pattern used in Mamba-2 can be easily extended to grouped-input SSM (GIS),
or synonymously grouped-value attention (GVA). The generalization is straightforward and we omit the details for
simplicity.
7.3 OtherSSDExtensionsfromLinearAttention
We describe here an example of architectural modifications to SSD motivated by linear attention. We ablate these in
Section 9.4.3 as a form of negative result, finding that they do not significantly improve performance enough to adopt
themasdefaultsettings. Nonetheless,theseillustratehowthevastliteratureonattentioncanbeincorporatedtodefine
variantsofSSD.WetreatthechoiceofkernelfeaturemapasahyperparameterintheMamba-2architecture,andexpect
othersimplemodificationsinspiredbyattentiontobepossibleaswell.
KernelAttentionApproximationstoSoftmaxAttention. Manyvariantsoflinearattentionorkernelattentionare
motivatedbyviewingtheattentionscoressoftmax(ğ‘„ğ¾âŠ¤)ascomposedof
1. An exponential kernelğ‘ = exp(ğ‘„ğ¾âŠ¤), which can be approximated byğ‘ = ğœ“(ğ‘„)ğœ“(ğ¾)âŠ¤ for some kernel feature
map.
2. Normalizingthekernelsothatrowssumto1viağ‘€ =ğº/ğº11âŠ¤,wherethedivisionhappenselementwiseand1is
theall1â€™svector.
ExponentialKernelFeatureMaps. InMamba-2,weincorporateaflexiblekernelfeaturemap,andapplyittotheğµ
andğ¶ branches(correspondingtotheğ¾ andğ‘‰ branchesinattention). Thefeaturemapcanalsobeoptionallyappliedto
theğ‘‹ (ğ‘‰)branch,forsimplicityandsymmetry. ThisisrepresentedinFigure6byanarbitrarynonlinearity. Bydefault,
wesimplychooseğœ“ tobeanelementwiseSwish/SiLUfunction(HendrycksandGimpel2016;Ramachandran,Zoph,and
Le 2017). We explore other options in the ablations in Section 9.4.3, including feature maps used by Linear Attention,
Performer,RandomFeatureAttention,andcosFormer(Section4.1.3).
IncorporatingaNormalization(Denominator)Term. Tofindthedenominatorterm, wesimplyhavetocompute
ğ‘€1.Butrecallthatthefinaloutputofthemodelisjustğ‘Œ =ğ‘€ğ‘‹ (equation(16)).Sothenormalizationtermscanbefound
simplybyaugmentingğ‘‹ withanextracolumn1,resultinginatensorofshape(T,P+1).
Notethatinthiscase,thekernelfeaturemapğœ“ mustbepositivesothatthesumispositive.
8 Systems Optimization for SSMs
We describe several systems optimizations for SSMs, in particular the Mamba-2 architecture, for large-scale efficient
trainingandinference. Inparticular,wefocusontensorparallelandsequenceparallelforlarge-scaletraining,asawell
variable-lengthsequencesforefficientfinetuningandinference.
258.1 TensorParallel
Tensorparallelism(TP)(Shoeybietal.2019)isamodelparallelismtechniquethatsplitseachlayer(e.g.,attention,MLP)
to run on multiple accelerators such as GPUs. This technique is widely used to train most large models (Brown et al.
2020; Chowdhery et al. 2023; Touvron, Lavril, et al. 2023; Touvron, L. Martin, et al. 2023) on GPU clusters where each
node typically has 4-8 GPUs with fast networking such as NVLink. TP was originally developed for the Transformer
architecture,anditisnotstraight-forwardtoadaptitotherarchitecture.WefirstshowthechallengeofusingTPwiththe
Mambaarchitecture,andtheshowhowtheMamba-2architectureisdesignedtomakeTPefficient.
Recall the Mamba architecture, with a single input ğ‘¢ âˆˆ Rğ¿Ã—ğ‘‘ (no batching for simplicity), input projection matrices
ğ‘Š(ğ‘¥),ğ‘Š(ğ‘§) âˆˆRğ‘‘Ã—ğ‘’ğ‘‘ whereğ‘’ istheexpansionfactor(typically2),andoutputprojectionmatrixğ‘Š(ğ‘œ) âˆˆRğ‘’ğ‘‘Ã—ğ‘‘:
ğ‘¥ =ğ‘¢ğ‘Š(ğ‘¥)âŠ¤ âˆˆRğ¿Ã—ğ‘’ğ‘‘
ğ‘§ =ğ‘¢ğ‘Š(ğ‘§)âŠ¤ âˆˆRğ¿Ã—ğ‘’ğ‘‘
ğ‘¥
ğ‘
=conv1d(ğ‘¥) âˆˆRğ¿Ã—ğ‘’ğ‘‘ (depthwise,independentalongğ‘‘)
Î”,ğµ,ğ¶ =low-rankprojection(ğ‘¥ ğ‘)
ğ‘¦ =ğ‘†ğ‘†ğ‘€ ğ´,ğµ,ğ¶,Î”(ğ‘¥ ğ‘) âˆˆRğ¿Ã—ğ‘’ğ‘‘ (independentalongğ‘‘)
ğ‘¦
ğ‘”
=ğ‘¦Â·ğœ™(ğ‘§) (gating,e.g.,withğœ™ beingSiLU)
out=ğ‘¦ ğ‘”ğ‘Š(ğ‘œ)âŠ¤ âˆˆRğ¿Ã—ğ‘‘.
WithTP,supposethatwewanttosplitthecomputationalong2GPUs. Itiseasytosplittheinputprojectionmatrices
ğ‘Š(ğ‘¥) andğ‘Š(ğ‘§) intotwopartitionseachofsizeğ‘‘ Ã— ğ‘’ 2ğ‘‘. TheneachGPUwouldholdhalfofğ‘¥ ğ‘ ofsizeğ¿Ã— ğ‘’ 2ğ‘‘. However,
weseethatsinceÎ”,ğµ,ğ¶ arefunctionsareğ‘¥ ,sowewouldneedanextraall-reducebetweentheGPUstogetthewhole
ğ‘
of ğ‘¥ before computing Î”,ğµ,ğ¶. After that the two GPUs can compute the SSM in parallel since they are independent
ğ‘
alongğ‘‘. Attheend,wecansplittheoutputprojectionmatricesğ‘Š(ğ‘œ) intotwopartitionseachofsize ğ‘’ğ‘‘ Ã—ğ‘‘,anddoan
2
all-reduceattheend.ComparedtoTransformers,wewouldincurtwoall-reducesinsteadofone,doublingthetimespent
incommunication.Forlarge-scaleTransformerstraining,communicationmightalreadytakeasignificantfractionoftime
(e.g.10-20%),anddoublingcommunicationwouldmakeMambanotasefficientforlarge-scaletraining.
With Mamba-2, our goal is to have only one all-reduce per block, similar to attention or MLP blocks in Transformers.
Asaresult,wehavetheprojectiontogetÎ”,ğµ,ğ¶ directlyfromğ‘¢ insteadoffromğ‘¥ ,allowingustosplittheseprojection
ğ‘
matrices. This implies that we have different sets of Î”,ğµ,ğ¶ on different GPUs, which is equivalent to having several
â€œgroupsâ€of Î”,ğµ,ğ¶ onalargerâ€œlogicalGPUâ€.Moreover, weuseGroupNormwithineachblock, withnumberofgroups
divisiblebytheTPdegree,sothattheGPUsinaTPgroupdonothaveacommunicatewithintheblock:
ğ‘¥ =ğ‘¢ğ‘Š(ğ‘¥)âŠ¤ âˆˆRğ¿Ã—ğ‘’ğ‘‘
ğ‘§ =ğ‘¢ğ‘Š(ğ‘§)âŠ¤ âˆˆRğ¿Ã—ğ‘’ğ‘‘
Î”,ğµ,ğ¶ =projection(ğ‘¢) (oneormoregroupsofÎ”,ğµ,ğ¶ perGPU)
ğ‘¥
ğ‘
=conv1d(ğ‘¥) âˆˆRğ¿Ã—ğ‘’ğ‘‘ (depthwise,independentalongğ‘‘)
ğ‘¦ =ğ‘†ğ‘†ğ‘€ ğ´,ğµ,ğ¶,Î”(ğ‘¥ ğ‘) âˆˆRğ¿Ã—ğ‘’ğ‘‘ (independentalongğ‘‘)
ğ‘¦
ğ‘”
=ğ‘¦Â·ğœ™(ğ‘§) (gating,e.g.,withğœ™ beingSiLU)
ğ‘¦ ğ‘› =groupnorm(ğ‘¦ ğ‘”) (numberofgroupsdivisiblebydegreeoftensorparallel)
out=ğ‘¦ ğ‘”ğ‘Š(ğ‘œ)âŠ¤ âˆˆRğ¿Ã—ğ‘‘.
Weseethatweonlyneedtosplittheinputprojectionmatrices,andtheoutputprojectionmatrices,andonlyneedtodo
all-reduce at the end of the block. This is similar to the design of TP for attention and MLP layers. In particular, if we
haveTPdegree2,wewouldsplitğ‘Š(ğ‘¥) = [ğ‘Š(ğ‘¥),ğ‘Š(ğ‘¥)] withğ‘Š(ğ‘¥) âˆˆ Rğ‘‘Ã—ğ‘’ğ‘‘/2,ğ‘Š(ğ‘§) = [ğ‘Š(ğ‘§),ğ‘Š(ğ‘§)] withğ‘Š(ğ‘§) âˆˆ Rğ‘‘Ã—ğ‘’ğ‘‘/2,
1 2 ğ‘– 1 2 ğ‘–
26All-reduce
&!(') &&(')
Outputs "
GN GN
Y Y States #
A XBC A XBC
Inputs !
GPU1 GPU2 GPU3
&!(#) &!(%) &&(#) &&(%)
Layer Input
Figure7: (ParallelismwiththeMamba-2Block.) (Left: TensorParallelism)Wesplittheinputprojectionmatrices
ğ‘Š(ğ‘¥),ğ‘Š(ğ‘§) andtheoutputprojectionmatrixğ‘Š(ğ‘œ). EachSSMhead (ğ´,ğµ,ğ¶,ğ‘‹) â†¦â†’ ğ‘Œ livesonasingledevice. Choosing
GroupNormforthefinalnormalizationlayeravoidsextracommunication.Weneedoneall-reduceperlayer,justlikethe
MLP or attention blocks in a Transformer. (Right: Sequence/Context Parallelism) Analogous to the SSD algorithm,
withmultipledevices,wecansplitalongthesequencedimension. Eachdevicecomputesthestateofitssequence,then
passthatstatetothenextGPU.
(cid:34) ğ‘Š(ğ‘œ)(cid:35)
andğ‘Š(ğ‘œ) = 1 withğ‘Š(ğ‘œ) âˆˆRğ‘’ğ‘‘/2Ã—ğ‘‘.Forğ‘– =1,2,theTPMamba-2layercanbewrittenas:
ğ‘Š(ğ‘œ) ğ‘–
2
ğ‘¥(ğ‘–) =ğ‘¢ğ‘Š(ğ‘¥)âŠ¤ âˆˆRğ¿Ã—ğ‘’ğ‘‘/2
ğ‘–
ğ‘§(ğ‘–) =ğ‘¢ğ‘Š(ğ‘§)âŠ¤ âˆˆRğ¿Ã—ğ‘’ğ‘‘/2
ğ‘–
Î”(ğ‘–),ğµ(ğ‘–),ğ¶(ğ‘–) =projection(ğ‘¢) (oneormoregroupsofÎ”,ğµ,ğ¶ perGPU)
ğ‘¥ ğ‘(ğ‘–) =conv1d(ğ‘¥(ğ‘–)) âˆˆRğ¿Ã—ğ‘’ğ‘‘/2
ğ‘¦(ğ‘–) =ğ‘†ğ‘†ğ‘€ ğ´,ğµ,ğ¶,Î”(ğ‘¥ ğ‘(ğ‘–)) âˆˆRğ¿Ã—ğ‘’ğ‘‘/2
ğ‘¦ ğ‘”(ğ‘–) =ğ‘¦(ğ‘–) Â·ğœ™(ğ‘§(ğ‘–))
ğ‘¦ ğ‘›(ğ‘–) =groupnorm(ğ‘¦ ğ‘”(ğ‘–)) (numberofgroupsdivisiblebydegreeoftensorparallel)
out(ğ‘–) =ğ‘¦ ğ‘”(ğ‘–)ğ‘Š ğ‘–(ğ‘œ)âŠ¤ âˆˆRğ¿Ã—ğ‘‘/2
out=âˆ‘ï¸ out(ğ‘–). (summingoutputsfromallGPUswithanall-reduce)
ğ‘–
WeillustratetensorparallelwithMamba-2inFigure7(Left).
8.2 SequenceParallelism
For very long sequences, we might need to split the input and activation to different GPUs along the sequence length
dimension.Therearetwomaintechniques:
1. Sequenceparallelism(SP)fortheresidualandnormalizationoperations:firstproposedbyKorthikantietal.(2023),
this technique decomposes the all-reduce in TP as reduce-scatter and all-gather. Noticing that the residual and
normalizationoperationsarerepeatedonthesameinputforallGPUsinthesameTPgroup,SPsplitstheactivations
alongthesequencelengthdimensionbyperforming:reduce-scatter,residualandnormalization,thenall-gather.
SincetheMamba-2architectureusesthesameresidualandnormalizationstructure,SPapplieswithoutmodification.
2. Sequenceparallelismforthetoken-mixingoperations(attentionorSSM),alsoknownasâ€œcontextparallelismâ€(CP).
Severaltechniqueshavebeendevelopedforattentionlayer(e.g.,Ringattention(Liu,Yan,etal.2024;Liu,Zaharia,
27Sequence Length: 256 Sequence Length: 512 Sequence Length: 1024
1.00
0.75 Attention
Based
0.50 Mamba (N=16)
Mamba-2 (N=16)
0.25 Mamba-2 (N=64)
Mamba-2 (N=256)
0.00
32 64 128 256 32 64 128 256 32 64 128 256
Model dimension Model dimension Model dimension
Figure 8: (Multi-Query Associative Recall (MQAR)). Associative recall tasks are challenging for SSMs, which must
memorizeallrelevantinformationintotheirrecurrentstate.TheSSDlayercombinedwithimprovedarchitectureallows
formuchlargerstatesizesinMamba-2,whichperformssignificantlybetterthanMamba-1andevenvanillaattention.
andAbbeel2023)),withsophisticatedload-balancingtechnique(Brandonetal.2023).Thedifficultywithsequence
parallelisminattentionisthatwecansplitqueriesandkeysintoblock,buteachqueryblockneedstointeractwith
keyblocks,leadingtocommunicationbandwidthquadraticinthenumberofworkers.
With SSMs, we can split the sequence in a simple manner: each worker takes an initial state, compute the SSM
withrespecttotheirinputs,returnthefinalstate,andpassthatfinalstatetothenextworker.Thecommunication
bandwidthislinearinthenumberofworkers. Thisdecompositionisexactlythesameastheblock-decomposition
intheSSDalgorithm(Figure5)tosplitintoblocks/chunks.WeillustratethiscontextparallelisminFigure7(Right).
8.3 VariableLength
Whilepretrainingoftenusesthesamesequencelengthsforthebatch, duringfinetuningorinference, themodelmight
need to process different input sequences of different lengths. One naive way to handle this case is to right-pad all
sequencesinthebatchtothemaximumlength, butthiscanbeinefficientifsequencesarewildlydifferentlengths. For
transformers,sophisticatedtechniqueshavebeendeveloptoavoidpaddinganddoload-balancingbetweenGPUs(Zeng
et al. 2022; Y. Zhai et al. 2023), or packing multiple sequences in the same batch and adjust the attention mask (Ding
et al. 2024; Pouransari et al. 2024). With SSMs and Mamba in particular, we can handle variable sequence lengths by
simplytreatingthewholebatchasonelongsequence,andavoidpassingthestatesbetweenindividualsequences.Thisis
equivalenttosimplysettingğ´
ğ‘¡
=0fortokensğ‘¡ attheendofonesequencetopreventitfrompassinginformationtothe
tokenğ‘¡ +1,whichbelongstoadifferentsequence.
9 Empirical Validation
WeempiricallyevaluateMamba-2onsyntheticrecalltasksthathavebeenchallengingforrecurrentmodels(Section9.1),
andstandardlanguagemodelingpre-traininganddownstreamevaluations(Section9.2). WevalidatethatourSSDalgo-
rithmismuchmoreefficientthanMamba-1(Section9.3)andcomparabletooptimizedattentionformoderatesequence
lengths. Finally,weablatevariousdesignchoicesintheMamba-2architecture(Section9.4).
9.1 Synthetics: AssociativeRecall
Syntheticassociativerecalltaskshavebeenpopularfortestingtheabilityoflanguagemodelstolookupinformationin
theircontext.Broadly,theyinvolvefeedingautoregressivemodelspairsofkey-valueassociations,andthenpromptingthe
modeltoproducethecorrectcompletionuponbeingshownapreviously-seenkey.Themulti-queryassociativerecall
(MQAR)taskisaparticularformulationofthistaskthatrequiresthemodeltomemorizemultipleassociations(Arora,
Eyuboglu, Timalsina, et al. 2024). The original Mamba paper reported results on related synthetic tasks, in particular
SelectiveCopying(GuandDao2023)andInductionHeads(Olssonetal.2022), whichcanbeseenaseasierassociative
recalltasks.TheMQARtaskisalsocloselyrelatedtoâ€œphonebooklook-upâ€taskswhichhasbeenshowntobechallenging
forrecurrentmodelssuchasSSMs,duetotheirfinitestatecapacity(Deetal.2024;Jelassietal.2024).
28
ycaruccA 7 G E P M R K  0 E [ W  S R  8 L I  4 M P I   7 I U Y I R G I  0 I R K X L      
 8 V E R W J S V Q I V  
 1 E Q F E
     1 E Q F E  
  Â‚   
  Â‚   
  Â‚   
  Â‚   
         
 * 0 3 4 W   P S K  W G E P I 
Figure9:(ScalingLaws.)Modelsofsizeâ‰ˆ125ğ‘€ toâ‰ˆ1.3ğµparameters,trainedonthePile.Mamba-2matchesorexceeds
theperformanceofMambaaswellasastrongâ€œTransformer++â€recipe.ComparedtoourTransformerbaseline,Mamba-2
isParetodominantonperformance(perplexity),theoreticalFLOPs,andactualwall-clocktime.
Table1:(Zero-shotEvaluations.)Bestresultsforeachsizeinbold,secondbestunlined.WecompareagainstopensourceLMswith
varioustokenizers,trainedforupto300Btokens.Pilereferstothevalidationsplit,comparingonlyagainstmodelstrainedonthesame
datasetandtokenizer(GPT-NeoX-20B).Foreachmodelsize,Mamba-2outperformsMamba,andgenerallymatchesPythiaattwicethe
modelsize.FullresultsinTable10.
Model Token. Pile LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande OpenbookQA Average
pplâ†“ pplâ†“ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘
Pythia-1B NeoX 7.82 7.92 56.1 47.2 70.7 57.0 27.1 53.5 31.4 49.0
Mamba-790M NeoX 7.33 6.02 62.7 55.1 72.1 61.2 29.5 56.1 34.2 53.0
Mamba-2-780M NeoX 7.26 5.86 61.7 54.9 72.0 61.0 28.5 60.2 36.2 53.5
HybridH3-1.3B GPT2 â€” 11.25 49.6 52.6 71.3 59.2 28.1 56.9 34.4 50.3
Pythia-1.4B NeoX 7.51 6.08 61.7 52.1 71.0 60.5 28.5 57.2 30.8 51.7
RWKV4-1.5B NeoX 7.70 7.04 56.4 52.5 72.4 60.5 29.4 54.6 34.0 51.4
Mamba-1.4B NeoX 6.80 5.04 65.0 59.1 74.2 65.5 32.8 61.5 36.4 56.4
Mamba-2-1.3B NeoX 6.66 5.02 65.7 59.9 73.2 64.3 33.3 60.9 37.8 56.4
HybridH3-2.7B GPT2 â€” 7.92 55.7 59.7 73.3 65.6 32.3 61.4 33.6 54.5
Pythia-2.8B NeoX 6.73 5.04 64.7 59.3 74.0 64.1 32.9 59.7 35.2 55.7
RWKV4-3B NeoX 7.00 5.24 63.9 59.6 73.7 67.8 33.1 59.6 37.0 56.4
Mamba-2.8B NeoX 6.22 4.23 69.2 66.1 75.2 69.7 36.3 63.5 39.6 59.9
Mamba-2-2.7B NeoX 6.09 4.10 69.7 66.6 76.4 69.6 36.4 64.0 38.8 60.2
SSD, Scan, Convolution vs Attention time (A100 80GB PCIe) SSD vs Scan time (A100 80GB PCIe)
FlashAttention-2 FlashAttention-2
1000 Convolution 7 Scan (Mamba)
Scan (PyTorch), state dim 64 SSD (ours)
Scan (Mamba), state dim 64 6
100 SSD (ours), state dim 64 5
10 4
3
1
2
0.1 1
0
512 1k 2k 4k 8k 16k 32k 64k 128k 256k 512k 4816 32 64 128 256
Sequence length State dim
Figure10:(EfficiencyBenchmarks.) (Left)OurSSDis2âˆ’8Ã—fasterthanaMambafusedscanforlargestateexpansion
(ğ‘ =64)andfasterthanFlashAttention-2forsequencelength2kandabove.(Right)Sequencelength4K:Increasingstate
expansionslowsdowntheMambaoptimizedscanimplementationlinearly.SSDcanhandlemuchlargerstateexpansion
factorswithoutmuchslowdown.
29
)sm(
emiT
  I P E G W  K S P   ] X M \ I P T V I 4
)sm(
emiTWecompareonachallengingversionoftheMQARsetupfrom(Arora,Eyuboglu,Zhang,etal.2024),usingahardertask,
longersequences,andsmallermodels. Ourbaselinesincludestandardmulti-headsoftmaxattentionaswellastheBased
architecturewhichcombinesconvolutions,localattention,andalinearattentionvariant.
ResultsareshowninFigure8. WhileMamba-1strugglesonthistask,Mamba-2performswellacrossallsettings. Sur-
prisingly,itissignificantlybetterthanMamba-1evenwhenthestatesizesarecontrolled(N=16).(Wearenotsurewhich
aspectofthearchitectureisthepredominantfactor,whichremainsaquestiontoexploreinfuturework.) Additionally,
this task validates the importance of state size: increasing from N = 16 to N = 64 and N = 256 consistently improves
performanceonMQAR,asthelargerstateallowsmoreinformation(key-valuepairs)tobememorized.
9.2 LanguageModeling
FollowingstandardprotocolsinLLMs,wetrainandevaluatetheMamba-2architectureonstandardautoregressivelan-
guagemodelingagainstotherarchitectures.Wecomparebothpretrainingmetrics(perplexity)andzero-shotevaluations.
Themodelsizes(depthandwidth)followGPT3specifications,from125mto2.7B.WeusethePiledataset(L.Gao,Bider-
man,etal.2020),andfollowthetrainingrecipedescribedinBrownetal.(2020). Thisfollowsthesamesetupasreported
inMamba(GuandDao2023);trainingdetailsareinAppendixD.
9.2.1 ScalingLaws
Forbaselines,wecompareagainstbothMambaanditsTransformer++recipe(GuandDao2023),whichisbasedonthe
PaLMandLLaMaarchitectures(e.g.rotaryembedding,SwiGLUMLP,RMSNorminsteadofLayerNorm,nolinearbias,and
higherlearningrates). AsMambahasalreadydemonstratedthatitoutperformsthestandardTransformerarchitecture
(GPT3architecture)aswellasrecentsubquadraticarchitectures(H3(Dao,D.Y.Fu,etal.2023),Hyena(Polietal.2023),
RWKV-4(B.Peng,Alcaide,etal.2023),RetNet(Y.Sunetal.2023)),weomitthoseintheplotforclarity(seeGuandDao
(2023)forcomparisons).
Figure9showsscalinglawsunderthestandardChinchilla(Hoffmannetal.2022)protocol, onmodelsfromâ‰ˆ 125ğ‘€ to
â‰ˆ1.3ğµparameters.
9.2.2 DownstreamEvaluations
Table 1 shows the performance of Mamba-2 on a range of popular downstream zero-shot evaluation tasks, compared
tothemostwell-knownopensourcemodelsatthesesizes, mostimportantlyPythia(Bidermanetal.2023)whichwere
trainedwiththesametokenizer,dataset,andtraininglength(300Btokens)asourmodels.
9.2.3 HybridModels:CombiningSSDLayerwithMLPandAttention
Recentandconcurrentwork(Dao,D.Y.Fu,etal.2023;Deetal.2024;Gloriosoetal.2024;Lieberetal.2024)suggeststhata
hybridarchitecturewithbothSSMlayersandattentionlayerscouldimprovethemodelqualityoverthatofaTransformer,
orapureSSM(e.g.,Mamba)model,especiallyforin-contextlearning.WeexplorethedifferentwaysthatSSDlayerscan
becombinedwithattentionandMLPtounderstandthebenefitsofeach. Empiricallywefindthathavingaround10%of
thetotalnumberoflayersbeingattentionperformsbest. CombiningSSDlayers, attentionlayers, andMLPalsoworks
betterthaneitherpureTransformer++orMamba-2.
SSD and Attention We find that SSD and attention layers are complementary: by themselves (e.g. in the Mamba-2
architecture vs. Transformer++) their performance (measured by perplexity) is nearly the same, but a mixture of SSD
and attention layers outperforms the pure Mamba-2 or Transformer++ architecture. We show some results (Table 2)
forthe350Mmodel(48layers)trainedto7BtokensonthePilewiththeGPT-2tokenizer(samenumberofparameters,
same hyperparameters, same training and validation set). Adding in just a few attention layers already yields notable
improvementandstrikesthebestbalancebetweenqualityandefficiency. WehypothesizethattheSSMlayersfunction
well as a general sequence-to-sequence mapping, and attention layers act as a retrieval mechanism to quickly refer to
previoustokensinthesequenceinsteadofforcingthemodeltocompressallthecontexttoitsmemory(SSMstates).
30Table2:(CombiningSSDandAttentionBlocks.)Perplexityofa350Mmodelwith48layers,withdifferentnumberof
attentionlayers.Havingarounda10%ratioofattentionlayersperformsbest.
Num.AttnBlocks 0(Mamba-2) 1 2 3 4 5 6 7 9 11 15 24 Transformer++
Perplexityâ†“ 8.60 8.38 8.32 8.29 8.29 8.28 8.26 8.27 8.28 8.30 8.34 8.50 8.68
HybridModelswithSSD,MLP,andAttention WecomparedifferentwaysthatSSDcanbecombinedwiththe(gated)
MLPandattentionlayers,andevaluateatthe2.7Bscale(64layers),trainedto300BtokensonthePile(samenumberof
parameters,samehyperparameters,sametrainingandvalidationset,samedataorder):
1. Transformer++:32attentionlayersand32gatedMLP,interleaving.
2. Mamba-2:64SSDlayers.
3. Mamba-2-MLP:32SSDand32gatedMLPlayers,interleaving.
4. Mamba-2-Attention:58SSDlayersand6attentionlayers(atindices9,18,27,36,45,56)6.
5. Mamba-2-MLP-Attention:28SSDlayersand4attentionlayers,interleavingwith32gatedMLPlayers.
We report the validation perplexity on the Pile, as well as zero-shot evaluation, in Table 3. In general, the quality of
Transformer++andMamba-2modelsarearoundthesame.Weseethataddingjust6attentionlayersnoticeablyimproves
overthepureMamba-2model(andoverTransformer++).AddingMLPlayersreducesmodelquality,butcan(i)speedup
training and inference due to the simplicity and hardware-efficiency of the MLP layer (ii) be easier to up-cycle to MoE
modelsbyreplacingMLPlayerswithmixture-of-experts.
Table3: (Zero-shotEvaluations.) Bestresultsforeachsizeinbold. WecomparedifferentwaysSSD,MLP,andattentionlayerscan
becombined,evaluatedat2.7Bscaletrainedto300BtokensonthePile.
Model Token. Pile LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande OpenbookQA Average
pplâ†“ pplâ†“ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘
Transformer++ NeoX 6.13 3.99 70.3 66.4 75.2 67.7 37.8 63.9 40.4 60.2
Mamba-2 NeoX 6.09 4.10 69.7 66.6 76.4 69.6 36.4 64.0 38.8 60.2
Mamba-2-MLP NeoX 6.13 4.18 69.3 65.0 76.4 68.1 37.0 63.1 38.2 59.6
Mamba-2-Attention NeoX 5.95 3.85 71.1 67.8 75.8 69.9 37.8 65.3 39.0 61.0
Mamba-2-MLP-Attention NeoX 6.00 3.95 70.0 66.6 75.4 70.6 38.6 64.6 39.2 60.7
9.3 SpeedBenchmarks
WebenchmarkthespeedoftheSSDalgorithmagainstMambaâ€™sscanimplementationandFlashAttention-2(Figure10).
SSD,thankstoitsreformulationtousematrixmultiplicationasasubroutine,canexploitspecializedmatrixmultiplication
(matmul)unitsonGPUs,alsoknownastensorcores. Asaresult,itis2-8Ã—fasterthanMambaâ€™sfusedassociativescan,
whichdoesnotleveragematmulunits. Duetoitslinearscalinginsequencelength,SSDisfasterthanFlashAttention-2
startingatsequencelength2ğ¾.
However,wenotethattheMamba-2modelasawholemightnotbeasefficienttotrainasTransformeratshortsequence
length(e.g.at2ğ¾),sinceaTransformerwithğ¿layerswouldhave ğ¿ MLPlayersand ğ¿ attentionlayers,whileaMamba-2
2 2
modelwouldhaveğ¿SSDlayersforthesamenumberofparameters.GenerallytheMLPlayersareveryhardwareefficient
sincetheyconsistofsimplematrixmultiplicationandpointwiselinearity.AsshowninSection9.2.3,onecanalsocombine
ğ¿ SSDlayersand ğ¿ MLPlayerstospeeduptrainingatshortsequencelength.
2 2
6Insmall-scaleexperiments,wefindthataslongastheattentionlayersarespacedout,notattheverybeginningorattheveryend,themodel
qualitydoesnotdependverymuchontheexactlocationoftheattentionlayers.
31Table 4: (Ablations: Mamba-2 block.) We ablate the major differences between the Mamba-2 and Mamba-1 neural
networkblocks(Figure6,Section7.1). Notethatthesecomponentsareindependentoftheinnersequencemixinglayer;
intheseablations,weuseSSDfortheinnerSSMlayer(differingfromtheS6layerofMamba-1).
Block ğ´ğµğ¶ğ‘‹ Projections ExtraNormalization Parameters Perplexity
Mamba-1 Sequential âœ— 129.3M 11.76
Sequential âœ“ 129.3M 11.54
Parallel âœ— 126.5M 11.66
Mamba-2 Parallel âœ“ 126.5M 11.49
9.4 ArchitectureAblations
9.4.1 BlockDesign
Section7.1introducestheMamba-2block,whichhassmallmodificationstotheMamba-1blockwhicharepartlymotivated
bytheconnectiontoattentionandalsotoimprovethescalabilityofMamba-2.Table4ablatesthesearchitecturechanges
totheblock,whichoccuroutsideofthecoreSSMlayer.
Theablationsvalidatethatparallelprojectionstocreate (ğ´,ğµ,ğ¶,ğ‘‹) savesparametersandperformsslightlybetterthan
Mambaâ€™s sequential projections. More importantly, this modification is amenable to tensor parallelism at larger model
sizes(Section8). Additionally,theextranormalizationlayeralsoslightlyimprovesperformance. Moreimportantly,pre-
liminaryexperimentsatlargerscalesobservedthatitalsohelpswithtrainingstability.
9.4.2 HeadStructure
Section 7.2 describes how the dimensions of the ğµ,ğ¶,ğ‘‹ projections can be viewed as a hyperparameter analogous to
notions of multi-head attention and multi-query attention. We also showed how the original Mamba architecture is
analogous to multi-value attention (Proposition 7.2), which was a choice that naturally developed from the state-space
modelpointofviewandwasnotpreviouslyablated.
Table 5 ablates choices of the multi-head structure for the Mamba-2 architecture. Strikingly, we find a large difference
between multi-value and multi-query or multi-key head patterns, despite seeming very similar. Note that this is not
explainedbythetotalstatesize,whichisthesameforallofthem(equaltoHPNortheproductofthenumberofheads,
headdimension,andstatedimension).
Wealsocomparetomulti-headpatternswherethenumberofğ¶,ğµ,ğ‘‹ (analogoustoğ‘„,ğ¾,ğ‘‰)headsisequal. Wecompare
against the standard multi-head pattern, as well as one with aggressive sharing where they all have only 1 head. Note
thatinthelattercase,themodelstillhasHdifferentsequencemixersğ‘€,becauseeachheadstillhasadifferentğ´. When
parameter matched, these multi-head patterns perform similarly to each other, in between the MVA and MQA/MKA
patterns.
9.4.3 AttentionKernelApproximations
Section 7.3 noted how SSD can be combined with ideas from the linear attention literature, such as various forms of
kernel approximations. We ablate several variants of these suggested by previous works in Table 6. These include the
cosFormer(Qin,WeixuanSun,etal.2022),RandomFeatureAttentionH.Pengetal.2021,andPositiveRandomFeatures
(Performer)(Choromanskietal.2021).
Wealsoablateaddinganormalizationterm,akintothedenominatorofthesoftmaxfunctioninstandardattention. We
foundthatthisintroducedinstabilitiestomostvariants,butslightlyimprovedperformancefortheReLUactivationfunc-
tionğœ“.
Table7alsotestsmorerecentproposalstoimprovelinearattentionthatinvolveexpandingthefeaturedimension(Based(Arora,
Eyuboglu, Zhang, etal.2024)andReBased(Aksenovetal.2024)). Theselinearattentionextensionsaimtoappropriate
theexpkernelwithaquadraticapproximation.ReBasedalsoproposestoreplacetheQKactivationfunctionwithalayer
normalization; from an SSM-centric view we apply a normalization on top of (ğµ,ğ¶) before applying the SSM function.
32Table5: (Ablations: Multi-headstructure.) Allmodelshavestateexpansionfactorğ‘ = 64andheadsizeğ‘ƒ = 64and
aretrainedtoChinchillascalinglawtokencounts. Thenumberofğ´headsisalwaysequaltothetotalheadsH,i.e. each
headhasaseparateinput-dependentğ´decayfactor.(Top)125Mmodels,2.5Btokens(Bottom)360Mmodels,7Btokens
SSMHeadPattern Attn.Analog ğ´heads ğµheads ğ¶heads ğ‘‹ heads Layers Params Ppl.
Multi-input(MIS) Multi-value(MVA) 24 1 1 24 24 126.5M 11.66
Multi-contract(MCS) Multi-query(MQA) 24 1 24 1 24 126.5M 12.62
Multi-expand(MES) Multi-key(MKA) 24 24 1 1 24 126.5M 12.59
Multi-head(MHS) Multi-head(MHA) 24 24 24 24 15 127.6M 12.06
Multi-state(MSS) - 24 1 1 1 36 129.6M 12.00
Multi-input(MIS) Multi-value(MVA) 32 1 1 32 48 361.8M 8.73
Multi-contract(MCS) Multi-query(MQA) 32 1 32 1 48 361.8M 9.33
Multi-expand(MES) Multi-key(MKA) 32 32 1 1 48 361.8M 9.36
Multi-head(MHS) Multi-head(MHA) 32 1 1 1 70 361.3M 9.01
Multi-state(MSS) - 32 32 32 32 29 357.3M 9.04
Table6:(Ablations:Kernelapproximations.)Wetestvar- Table 7: (Ablations: Kernel approximations.) We
iousproposalsforthekernelactivationfunctionğœ“,including testthe(Re)Basedmethodsforlinearattentionapprox-
linearattentionvariantsaimingtoapproximatetheexpker- imations,whichinvolveexpandedfeaturemaps. (Top)
nelfromstandardsoftmaxattention. 130Mmodels.(Top)380Mmodelswithğ‘ =256.
Kernelactivationğœ‘ Perplexity Kernelactivationğœ‘ Perplexity
none 11.58 Swish 11.67
Swish 11.66 Swish+Taylor(Based) 12.19
Exp 11.62 LayerNorm 11.50
ReLU 11.73 LayerNorm+Square(ReBased) 11.84
ReLU+normalization 11.64
Swish 8.58
cosFormer 11.97 Swish+Taylor(Based) 8.71
RandomFeatureAttention 11.57 LayerNorm 8.61
PositiveRandomFeatures(Performer) 12.21 LayerNorm+Square(ReBased) 8.63
Wenotethatthistechniquehasbeenindependentlyproposedastheâ€œQK-Normâ€forsoftmaxattention(Team2024)and
anâ€œinternalnormalizationâ€forMamba(Lieberetal.2024).
Overall,Table6andTable7foundthatthekernelapproximationmethodswetrieddidnotseemtoimproveoversimple
pointwisenon-linearactivationfunctionsforğœ“. ThusourdefaultsettingsforMamba-2usedğœ“(ğ‘¥) = Swish(ğ‘¥) tofollow
Mamba-1, but we suggest that removing this activation entirely may be a simpler choice that we did not extensively
test.
WeemphasizehoweverthatSSDandvanillalinearattentiondifferintheinclusionofthe1-semiseparablemaskğ¿,while
thevariouslinearattentionmethodsintheliteraturewerederivedtoapproximatesoftmaxattentionwithoutthisterm;
thus,ournegativeresultsmaybenotunexpected.
10 Related Work and Discussion
ThestatespacedualityframeworkbridgesconnectionsbetweenSSMs,structuredmatrices,andattention. Wediscussin
moredepththerelationsbetweenSSDandtheseconceptsmorebroadly.Usingideasfromeachoftheviewpoints,wealso
suggestsomedirectionsthattheSSDframeworkcanbeextendedinfuturework.
10.1 StateSpaceModels
Structuredstatespacemodelscanbecharacterizedalongtheaxes
33(i) whetheritistime-invariantortime-varying.
(ii) thedimensionalityofthesystem.
(iii) thestructureontherecurrenttransitionsğ´.
SSDcanbedescribedasaselectiveSSMwithSISOdimensionsandscalar-identitystructure.
TimeVariance(Selectivity). TheoriginalstructuredSSMs(S4)werelineartime-invariant(LTI)systems(Gu2023;Gu,
Goel,andRÃ©2022)motivatedbycontinuous-timeonlinememorization(Gu,Dao,etal.2020;Gu,Johnson,Goel,etal.2021;
Gu,Johnson,Timalsina,etal.2023).ManyvariantsofstructuredSSMshavebeenproposed(Dao,D.Y.Fu,etal.2023;Gu,
Gupta,etal.2022; Gupta,Gu,andBerant2022; Maetal.2023; J.T.Smith,Warrington,andLinderman2023),including
severalthatdroptherecurrenceandfocusontheconvolutionalrepresentationofLTISSMs(D.Y.Fuetal.2023;Y.Lietal.
2023;Polietal.2023;Qin,Han,WeixuanSun,B.He,etal.2023).
SSDisatime-varyingstructuredSSM,alsoknownasaselectiveSSMintroducedinMamba(GuandDao2023).Selective
SSMs are closely related to gating mechanisms of RNNs, including classical RNNs such as the LSTM (Hochreiter and
Schmidhuber1997)andGRU(J.Chungetal.2014)aswellasmoremodernvariantssuchastheQRNN(Bradburyetal.
2016), SRU (Lei 2021; Lei et al. 2017), RWKV (B. Peng, Alcaide, et al. 2023), HGRN (Qin, Yang, and Zhong 2023), and
Griffin(Botevetal.2024;Deetal.2024).TheseRNNsdifferintheirparameterizationsinvariousways,mostimportantly
inthelackofastateexpansion.
DimensionalityandStateExpansion. AnimportantcharacteristicofSSD,sharedbypreviousSSMsinitslineage(S4,
H3,Mamba),isthatitisasingle-inputsingle-output(SISO)systemwhereinputchannelsareprocessedindependently.
ThisleadstoamuchlargereffectivestatesizeofNDwhereNistheSSMstatesize(alsocalledstateexpansionfactor)and
D is the standard model dimension. Traditional RNNs either have N = 1 or are multi-input multi-output (MIMO) with
denseğµ,ğ¶ matrices,eitherofwhichleadstoasmallerstate. WhileMIMOSSMshavebeenshowntoworkwellinsome
domains (Lu et al. 2023; Orvieto et al. 2023; J. T. Smith, Warrington, and Linderman 2023), Mamba showed that state
expansioniscrucialforinformation-densedomainssuchaslanguage.OneofthemainadvantagesofSSDisallowingfor
evenlargerstateexpansionfactorswithoutslowingdownthemodel. Manysubsequentworkshavesinceadoptedstate
expansion(Section10.4).
Structure. ComparedtopreviousstructuredSSMs,themainrestrictionofSSDisontheexpressivityofthestatetran-
sitionsğ´ .WenotethatmoregeneralSSMs,suchasthecaseofdiagonalğ´ ,havethesametheoreticalefficiencyasSSD,
ğ‘¡ ğ‘¡
butarelesshardware-friendly.Thisisbecausethedualquadraticformlosesitsattention-likeinterpretationandbecomes
moredifficulttocompute. ThuscomparedtoMamba,SSDdiffersonlyinaslightlymorerestrictiveformofdiagonalğ´ ,
ğ‘¡
andtradesoffthisexpressivityforimprovedhardwareefficiency(andeaseofimplementation).
WehypothesizethatitmaybepossibletorefineourstructuredmatrixalgorithmstoimprovetothegeneraldiagonalSSM
caseaswell.
10.2 StructuredMatrices
Thefirstviewpointofthestatespacedualityadoptstheviewpointofthesemodelsasmatrixsequencetransformations
orâ€œmatrixmixersâ€:sequencetransformations(Definition2.1)thatcanberepresentedasmatrixmultiplication(byaTÃ—T
matrix)alongthesequencedimensionT.
Severalsuchmatrixmixershavebeenproposedbefore,wheretheprimaryaxisofvariationistherepresentationofthe
matrix. These include MLP-Mixer (Tolstikhin et al. 2021) (unstructured matrix), FNet (Lee-Thorp et al. 2021) (Fourier
Transformmatrix),M2(Dao,B.Chen,etal.2022;Dao,Gu,etal.2019;Dao,Sohoni,etal.2020;D.Fuetal.2024)(butter-
fly/monarchmatrix),Toeplitzmatrices(Polietal.2023;Qin,Han,WeixuanSun,B.He,etal.2023),andevenmoreexotic
structures(DeSaetal.2018;Thomasetal.2018).
Animportantcharacterizationisthatefficient(sub-quadratic)matrixsequencetransformationsareexactlythosewhich
have structured matrix mixers. A core result of the SSD framework is viewing SSMs as matrix mixers with a particular
structureâ€“semiseparablematrices(Section3). Thelinearvs. quadraticdualitythentakestheformofstructuredmatrix
multiplicationvs.naivematrixmultiplication.
34ThestructurematrixrepresentationledtoourefficientSSDalgorithmthroughblockdecompositionsofparticularsemisep-
arablematrices(Section6). Wenotethatsemiseparablematricesarewell-studiedinthescientificcomputingliterature,
andincorporatingthoseideasmaybeapromisingavenueformoreimprovementstostatespacemodels.Wealsosuggest
thatfocusingonthematrixmixerviewpointcanleadtomorefruitfuldirectionsforsequencemodels,suchasdesigning
principlednon-causalvariantsofMamba,orfindingwaystocharacterizeandbridgethegapbetweensoftmaxattention
andsub-quadraticmodelsthroughanalyzingtheirmatrixtransformationstructure.
10.3 (Linear)Attention
Comparedtostandard(causal)attention,SSDhasonlytwomaindifferences.
First,SSDdoesnotusethesoftmaxactivationofstandardattention(Bahdanau,Cho,andBengio2015;Vaswanietal.2017),
whichiswhatgivesattentionitsquadraticcomplexity.Whenthesoftmaxisdropped,thesequencecanbecomputedwith
linearscalingthroughthelinearattentionframework(Katharopoulosetal.2020).
Second,SSDmultipliesthelogitsmatrixbyaninput-dependent1-semiseparablemask. Thusthismaskcanbeviewedas
replacingthesoftmaxinstandardattention.
Thissemiseparablemaskcanalsobeviewedasprovidingpositionalinformation.Theelementsğ‘ actasâ€œgatesâ€intheRNN
ğ‘¡
sense,oraâ€œselectionâ€mechanism(seediscussioninMambapaper),andtheircumulativeproductsğ‘ controlhowmuch
ğ‘—:ğ‘–
interactionisallowedbetweenpositionsğ‘–and ğ‘—.Positionalembeddings(e.g.sinusoidal(Vaswanietal.2017),AliBi(Press,
N.Smith,andLewis2022),andRoPE(Suetal.2021))areanimportantcomponentofTransformersthatareoftenviewed
asheuristics,andthe1-SSmaskofSSDcanbeseenasamoreprincipledformofrelativepositionalembeddings.Wenote
thatthisviewwasalsopositedconcurrentlybyGateLoop(Katsch2023).
The second viewpoint of state space duality is a special case of our more general structured masked attention (SMA)
framework,wherethedualityisrevealedasdifferentcontractionorderingsonasimple4-waytensorcontraction. SMA
isastronggeneralizationoflinearattentionthatismuchmoregeneralthanSSDaswell;otherformsofstructuredmasks
mayleadtomorevariantsofefficientattentionwithdifferentpropertiesthanSSD.
Besideleadingtonewmodels, theseconnectionstoattentioncanleadtootherdirectionsforunderstandingSSMs. For
example,wearecuriouswhetherthephenomenonofattentionsinks(Darcetetal.2024;Xiaoetal.2024)existforMamba
models, and more broadly whether interpretability techniques can be transferred to SSMs (Ali, Zimerman, and Wolf
2024).
Finally, many other variants of linear attention have been proposed (Arora, Eyuboglu, Timalsina, et al. 2024; Arora,
Eyuboglu,Zhang,etal.2024;Choromanskietal.2021;H.Pengetal.2021;Qin,Han,WeixuanSun,DongxuLi,etal.2022;
Qin, WeixuanSun, etal.2022; Schlag, Irie, andSchmidhuber2021; Zhangetal.2024; Zheng, C.Wang, andKong2022)
(see Section 4.1.3 for descriptions of several of these), and we expect that many techniques can be transferred to SSMs
(e.g.Section7.3).
WeemphasizethatSSDdoesnotgeneralizestandardsoftmaxattention,oranyothertransformationontheattention
kernel matrix that does not have a finite feature mapğœ“. Compared to general attention, SSDâ€™s advantage is having a
controllablestateexpansionfactorNthatcompressesthehistory, comparedtoquadraticattentionâ€™scacheoftheentire
historyscalingwithsequencelengthTâ‰«N.Concurrentworkhasstartingstudyingthetradeoffsoftheserepresentations,
forexampleoncopyingandin-contextlearningtasks(AkyÃ¼reketal.2024;Grazzietal.2024;Jelassietal.2024;Parketal.
2024). We note that Mamba-2 significantly improves on Mamba on some of these capabilities (e.g. as demonstrated by
MQARresultsinSection9.1),butmoreremainstobeunderstood.
10.4 RelatedModels
Wefinallyhighlightagrowingbodyofrecentandconcurrentworkthathavedevelopedsequencemodelsverysimilarto
MambaandMamba-2.
â€¢ RetNet (Y. Sun et al. 2023) and TransNormerLLM (Qin, Dong Li, et al. 2023) generalize Linear Attention using decay
terms instead of a cumulative sum, and propose dual parallel/recurrent algorithms as well as a hybrid â€œchunkwiseâ€
mode. These algorithms can be seen as an instantiation of SSD whereğ´ is time-invariant (constant for allğ‘¡); in the
ğ‘¡
SMAinterpretation,themaskmatrixğ¿wouldbeadecaymatrixğ¿
ğ‘–,ğ‘—
=ğ›¾ğ‘–âˆ’ğ‘—. Thesemodelsalsodifferarchitecturallyin
35variousways.Forexample,sincetheywerederivedfromanattention-centricperspectivetheypreservethemulti-head
attention(MHA)pattern;sinceMamba-2wasderivedfromanSSM-centricpatternitpreservesthemulti-valueattention
(MVA)ormulti-expandSSM(MES)pattern,whichweshowtobebetter(Section9.4).
â€¢ GateLoop(Katsch2023)concurrentlyproposedusinginput-dependentdecayfactorsğ´ ,anddevelopedthesamedual
ğ‘¡
quadraticformasinSSDwhichtheycallaâ€œsurrogateattentionâ€form.
â€¢ GatedLinearAttention(GLA)(Yangetal.2024)proposedavariantoflinearattentionwithdata-dependentgates,along
withefficientalgorithmstocomputeachunkwisemodeandhardware-awareimplementations.
â€¢ HGRN(Qin,Yang,andZhong2023)introducedanRNNwithinput-dependentgates,whichwasimprovedtoincorporate
stateexpansioninHGRN2(Qin,Yang,WeixuanSun,etal.2024).
â€¢ Griffin (De et al. 2024) and RecurrentGemma (Botev et al. 2024) showed that an RNN with input-dependent gating,
combined with local attention, can be very competitive with strong modern Transformers. Jamba also showed that
combiningMambawithafewlayersofattentionperformsverywellonlanguagemodeling(Lieberetal.2024).
â€¢ xLSTM(Becketal.2024)improvesthexLSTMbyadoptingtheideaofstateexpansionandothergating,normalization,
andstabilizationtechniques.
â€¢ RWKV(-4)(B.Peng,Alcaide,etal.2023)isanRNNbasedonadifferentlinearattentionapproximation(theattention-free
Transformer(S.Zhaietal.2021)). IthasrecentlybeenimprovedtotheRWKV-5/6(EagleandFinch)architectures(B.
Peng,Goldstein,etal.2024)byadoptingtheideasofselectivityandstateexpansion.
11 Conclusion
Weproposedatheoreticalframeworkbasedonwell-studiedclassesofstructuredmatricesthatbridgestheconceptualgap
betweenSSMsandattentionvariants.ThisframeworkyieldsinsightsonhowrecentSSMs(e.g.Mamba)performaswellas
Transformersonlanguagemodeling.Moreover,ourtheoreticaltoolsprovidenewideastoimproveSSMs(andpotentially
Transformers)byconnectingthe algorithmicandsystemsadvancesonbothsides. As ademonstration, theframework
guidesourdesignofanewarchitecture(Mamba-2)attheintersectionofSSMsandstructuredattention.
Acknowledgments
WethankAngelaWuforthesuggestiononhowtoefficientlycomputethegradientofÎ”inanumericallystablemanner.
WethankSukjunHwangandAakashLahotiforassistancewiththeMQARexperiments.
References
[1] JoshuaAinslie,JamesLee-Thorp,MichieldeJong,YuryZemlyanskiy,FedericoLebrÃ³n,andSumitSanghai.â€œGQA:
TrainingGeneralizedMulti-QueryTransformerModelsfromMulti-HeadCheckpointsâ€.In:arXivpreprintarXiv:2305.13245
(2023).
[2] YaroslavAksenov,NikitaBalagansky,SofiaMariaLoCiceroVaina,BorisShaposhnikov,AlexeyGorbatovski,and
DaniilGavrilov.â€œLinearTransformerswithLearnableKernelFunctionsareBetterIn-ContextModelsâ€.In:arXiv
preprintarXiv:2402.10644(2024).
[3] Ekin AkyÃ¼rek, Bailin Wang, Yoon Kim, and Jacob Andreas. â€œIn-Context Language Learning: Architectures and
Algorithmsâ€.In:TheInternationalConferenceonMachineLearning(ICML).2024.
[4] Ameen Ali, Itamar Zimerman, and Lior Wolf. The Hidden Attention of Mamba Models. 2024. arXiv: 2403.01590
[cs.LG].
[5] SimranArora,SabriEyuboglu,AmanTimalsina,IsysJohnson,MichaelPoli,JamesZou,AtriRudra,andChristo-
pherRÃ©.â€œZoology:MeasuringandImprovingRecallinEfficientLanguageModelsâ€.In:TheInternationalConference
onLearningRepresentations(ICLR).2024.
[6] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri
Rudra,andChristopherRÃ©.â€œSimpleLinearAttentionLanguageModelsBalancetheRecall-ThroughputTradeoffâ€.
In:TheInternationalConferenceonMachineLearning(ICML).2024.
[7] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. â€œNeural Machine Translation by Jointly Learning to
AlignandTranslateâ€.In:TheInternationalConferenceonLearningRepresentations(ICLR).2015.
36[8] GeorgeABaker,GeorgeABakerJr,PeterGraves-Morris,andSusanSBaker.PadeApproximants:Encyclopediaof
MathematicsandItâ€™sApplications,Vol.59GeorgeA.Baker,Jr.,PeterGraves-Morris.Vol.59.CambridgeUniversity
Press,1996.
[9] MaximilianBeck,KorbinianPÃ¶ppel,MarkusSpanring,AndreasAuer,OleksandraPrudnikova,MichaelKopp,GÃ¼n-
ter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. â€œxLSTM: Extended Long Short-Term Memoryâ€. In:
arXivpreprintarXiv:2405.04517 (2024).
[10] StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleOâ€™Brien,EricHallahan,Mo-
hammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal.â€œPythia:ASuiteforAnalyzing
LargeLanguageModelsacrossTrainingandScalingâ€.In:TheInternationalConferenceonMachineLearning(ICML).
PMLR.2023,pp.2397â€“2430.
[11] YonatanBisk,RowanZellers,JianfengGao,YejinChoi,etal.â€œPIQA:ReasoningaboutPhysicalCommonsensein
NaturalLanguageâ€.In:ProceedingsoftheAAAIconferenceonArtificialIntelligence.Vol.34.2020.
[12] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor
Leahy,KyleMcDonell,JasonPhang,etal.â€œGpt-NeoX-20B:AnOpen-sourceAutoregressiveLanguageModelâ€.In:
arXivpreprintarXiv:2204.06745(2022).
[13] GuyEBlelloch.â€œPrefixSumsandTheirApplicationsâ€.In:(1990).
[14] AleksandarBotev,SohamDe,SamuelLSmith,AnushanFernando,George-CristianMuraru,RubaHaroun,Leonard
Berrada,RazvanPascanu,PierGiuseppeSessa,RobertDadashi,etal.â€œRecurrentGemma:MovingPastTransform-
ersforEfficientOpenLanguageModelsâ€.In:arXivpreprintarXiv:2404.07839(2024).
[15] GeorgeEPBox,GwilymMJenkins,GregoryCReinsel,andGretaMLjung.TimeSeriesAnalysis:Forecastingand
Control.JohnWiley&Sons,2015.
[16] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. â€œQuasi-recurrent Neural Networksâ€. In:
arXivpreprintarXiv:1611.01576(2016).
[17] WilliamBrandon,AniruddhaNrusimha,KevinQian,ZacharyAnkner,TianJin,ZhiyeSong,andJonathanRagan-
Kelley.â€œStripedattention:Fasterringattentionforcausaltransformersâ€.In:arXivpreprintarXiv:2311.09431(2023).
[18] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
tan,PranavShyam,GirishSastry,AmandaAskell,etal.â€œLanguageModelsareFew-shotLearnersâ€.In:Advances
inNeuralInformationProcessingSystems(NeurIPS)33(2020),pp.1877â€“1901.
[19] KrzysztofChoromanski,ValeriiLikhosherstov,DavidDohan,XingyouSong,AndreeaGane,TamasSarlos,Peter
Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. â€œRethinking Attention with Performersâ€. In: The
InternationalConferenceonLearningRepresentations(ICLR).2021.
[20] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. â€œPaLM: Scaling Language Modeling with Path-
waysâ€.In:JournalofMachineLearningResearch24.240(2023),pp.1â€“113.url:http://jmlr.org/papers/v24/22-
1144.html.
[21] JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio.â€œEmpiricalEvaluationofGatedRecur-
rentNeuralNetworksonSequenceModelingâ€.In:arXivpreprintarXiv:1412.3555(2014).
[22] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,andOyvindTafjord.
â€œThinkyouhaveSolvedQuestionAnswering?TryARC,theAI2ReasoningChallengeâ€.In:arXivpreprintarXiv:1803.05457
(2018).
[23] TriDao.â€œFlashAttention-2:FasterAttentionwithBetterParallelismandWorkPartitioningâ€.In:TheInternational
ConferenceonLearningRepresentations(ICLR).2024.
[24] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao,
AtriRudra,andChristopherRÃ©.â€œMonarch:Expressivestructuredmatricesforefficientandaccuratetrainingâ€.In:
InternationalConferenceonMachineLearning.PMLR.2022,pp.4690â€“4721.
[25] TriDao,DanielYFu,KhaledKSaab,ArminWThomas,AtriRudra,andChristopherRÃ©.â€œHungryHungryHippos:
TowardsLanguageModelingwithStateSpaceModelsâ€.In:TheInternationalConferenceonLearningRepresentations
(ICLR).2023.
[26] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher RÃ©. â€œLearning Fast Algorithms for Linear
TransformsUsingButterflyFactorizationsâ€.In:TheInternationalConferenceonMachineLearning(ICML).2019.
[27] TriDao,NimitSohoni,AlbertGu,MatthewEichhorn,AmitBlonder,MeganLeszczynski,AtriRudra,andChristo-
pherRÃ©.â€œKaleidoscope:AnEfficient,LearnableRepresentationforAllStructuredLinearMapsâ€.In:TheInterna-
tionalConferenceonLearningRepresentations(ICLR).2020.
37[28] TimothÃ©eDarcet,MaximeOquab,JulienMairal,andPiotrBojanowski.â€œVisionTransformersNeedRegistersâ€.In:
TheInternationalConferenceonLearningRepresentations(ICLR).2024.
[29] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba
Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. â€œGriffin: Mixing Gated Linear Recurrences
withLocalAttentionforEfficientLanguageModelsâ€.In:arXivpreprintarXiv:2402.19427 (2024).
[30] ChristopherDeSa,AlbertGu,RohanPuttagunta,ChristopherRÃ©,andAtriRudra.â€œATwo-ProngedProgressin
StructuredDenseMatrixVectorMultiplicationâ€.In:ProceedingsoftheTwenty-NinthAnnualACM-SIAMSymposium
onDiscreteAlgorithms.SIAM.2018,pp.1060â€“1079.
[31] HantianDing,ZijianWang,GiovanniPaolini,VarunKumar,AnoopDeoras,DanRoth,andStefanoSoatto.â€œFewer
truncationsimprovelanguagemodelingâ€.In:arXivpreprintarXiv:2404.10830(2024).
[32] Yuli Eidelman and Israel Gohberg. â€œOn a new class of structured matricesâ€. In: Integral Equations and Operator
Theory34.3(1999),pp.293â€“324.
[33] Dan Fu, Simran Arora, Jessica Grogan, Isys Johnson, Evan Sabri Eyuboglu, Armin Thomas, Benjamin Spector,
MichaelPoli,AtriRudra,andChristopherRÃ©.â€œMonarchmixer:Asimplesub-quadraticgemm-basedarchitectureâ€.
In:AdvancesinNeuralInformationProcessingSystems36(2024).
[34] DanielYFu,ElliotLEpstein,EricNguyen,ArminWThomas,MichaelZhang,TriDao,AtriRudra,andChristopher
RÃ©. â€œSimple Hardware-efficient Long Convolutions for Sequence Modelingâ€. In: The International Conference on
MachineLearning(ICML)(2023).
[35] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,
AnishThite,NoaNabeshima,ShawnPresser,andConnorLeahy.â€œThePile:An800GBDatasetofDiverseTextfor
LanguageModelingâ€.In:arXivpreprintarXiv:2101.00027 (2020).
[36] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey
Hsu,KyleMcDonell,NiklasMuennighoff,JasonPhang,LariaReynolds,EricTang,AnishThite,BenWang,Kevin
Wang,andAndyZou.AFrameworkforFew-shotLanguageModelEvaluation.Versionv0.0.1.Sept.2021.doi:10.
5281/zenodo.5371628.url:https://doi.org/10.5281/zenodo.5371628.
[37] PaoloGlorioso,QuentinAnthony,YuryTokpanov,JamesWhittington,JonathanPilault,AdamIbrahim,andBeren
Millidge.â€œZamba:ACompact7BSSMHybridModelâ€.In:arXivpreprintarXiv:2405.16712(2024).
[38] RiccardoGrazzi,JulienSiems,SimonSchrodi,ThomasBrox,andFrankHutter.â€œIsMambaCapableofIn-Context
Learning?â€In:arXivpreprintarXiv:2402.03170(2024).
[39] AlbertGu.â€œModelingSequenceswithStructuredStateSpacesâ€.PhDthesis.StanfordUniversity,2023.
[40] AlbertGuandTriDao.â€œMamba:Linear-TimeSequenceModelingwithSelectiveStateSpacesâ€.In:arXivpreprint
arXiv:2312.00752(2023).
[41] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher RÃ©. â€œHIPPO: Recurrent Memory with Optimal
PolynomialProjectionsâ€.In:AdvancesinNeuralInformationProcessingSystems(NeurIPS).2020.
[42] AlbertGu,KaranGoel,andChristopherRÃ©.â€œEfficientlyModelingLongSequenceswithStructuredStateSpacesâ€.
In:TheInternationalConferenceonLearningRepresentations(ICLR).2022.
[43] AlbertGu,AnkitGupta,KaranGoel,andChristopherRÃ©.â€œOntheParameterizationandInitializationofDiagonal
StateSpaceModelsâ€.In:AdvancesinNeuralInformationProcessingSystems(NeurIPS).2022.
[44] AlbertGu,IsysJohnson,KaranGoel,KhaledSaab,TriDao,AtriRudra,andChristopherRÃ©.â€œCombiningRecurrent,
Convolutional,andContinuous-timeModelswiththeLinearStateSpaceLayerâ€.In:AdvancesinNeuralInformation
ProcessingSystems(NeurIPS).2021.
[45] AlbertGu,IsysJohnson,AmanTimalsina,AtriRudra,andChristopherRÃ©.â€œHowtoTrainYourHIPPO:StateSpace
ModelswithGeneralizedBasisProjectionsâ€.In:TheInternationalConferenceonLearningRepresentations(ICLR).
2023.
[46] AnkitGupta,AlbertGu,andJonathanBerant.â€œDiagonalStateSpacesareasEffectiveasStructuredStateSpacesâ€.
In:AdvancesinNeuralInformationProcessingSystems35(2022),pp.22982â€“22994.
[47] Dan Hendrycks and Kevin Gimpel. â€œGaussian Error Linear Units (GELUs)â€. In: arXiv preprint arXiv:1606.08415
(2016).
[48] W Daniel Hillis and Guy L Steele Jr. â€œData Parallel Algorithmsâ€. In: Communications of the ACM 29.12 (1986),
pp.1170â€“1183.
[49] SeppHochreiterandJÃ¼rgenSchmidhuber.â€œLongShort-TermMemoryâ€.In:NeuralComputation9.8(1997),pp.1735â€“
1780.
[50] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego
de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. â€œAn Empirical Analysis of Compute-
38Optimal Large Language Model Trainingâ€. In: Advances in Neural Information Processing Systems (NeurIPS) 35
(2022),pp.30016â€“30030.
[51] Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. â€œRepeat After Me: Transformers Are
BetterThanStateSpaceModelsatCopyingâ€.In:TheInternationalConferenceonMachineLearning(ICML).2024.
[52] AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFranÃ§oisFleuret.â€œTransformersareRNNs:FastAu-
toregressiveTransformerswithLinearAttentionâ€.In:InternationalConferenceonMachineLearning.PMLR.2020,
pp.5156â€“5165.
[53] Tobias Katsch. â€œGateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modelingâ€. In: arXiv preprint
arXiv:2311.01927 (2023).
[54] ShivaKaul.â€œLinearDynamicalSystemsasaCoreComputationalPrimitiveâ€.In:AdvancesinNeuralInformation
ProcessingSystems33(2020),pp.16808â€“16820.
[55] VijayAnandKorthikanti,JaredCasper,SangkugLym,LawrenceMcAfee,MichaelAndersch,MohammadShoeybi,
andBryanCatanzaro.â€œReducingactivationrecomputationinlargetransformermodelsâ€.In:ProceedingsofMachine
LearningandSystems5(2023).
[56] JamesLee-Thorp,JoshuaAinslie,IlyaEckstein,andSantiagoOntanon.â€œFnet:Mixingtokenswithfouriertrans-
formsâ€.In:arXivpreprintarXiv:2105.03824(2021).
[57] Tao Lei. â€œWhen Attention Meets Fast Recurrence: Training Language Models with Reduced Computeâ€. In: Pro-
ceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.2021,pp.7633â€“7648.
[58] TaoLei,YuZhang,SidaIWang,HuiDai,andYoavArtzi.â€œSimpleRecurrentUnitsforHighlyParallelizableRecur-
renceâ€.In:arXivpreprintarXiv:1709.02755(2017).
[59] YuhongLi,TianleCai,YiZhang,DemingChen,andDebadeeptaDey.â€œWhatMakesConvolutionalModelsGreat
onLongSequenceModeling?â€In:TheInternationalConferenceonLearningRepresentations(ICLR).2023.
[60] OpherLieber,BarakLenz,HofitBata,GalCohen,JhonathanOsin,ItayDalmedigos,ErezSafahi,ShakedMeirom,
YonatanBelinkov,ShaiShalev-Shwartz,etal.â€œJamba:AHybridTransformer-MambaLanguageModelâ€.In:arXiv
preprintarXiv:2403.19887 (2024).
[61] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. â€œWorld Model on Million-Length Video And Language
WithRingAttentionâ€.In:arXivpreprintarXiv:2402.08268(2024).
[62] HaoLiu,MateiZaharia,andPieterAbbeel.â€œRingattentionwithblockwisetransformersfornear-infinitecontextâ€.
In:arXivpreprintarXiv:2310.01889(2023).
[63] ChrisLu,YannickSchroecker,AlbertGu,EmilioParisotto,JakobFoerster,SatinderSingh,andFeryalBehbahani.
â€œStructuredStateSpaceModelsforIn-ContextReinforcementLearningâ€.In:AdvancesinNeuralInformationPro-
cessingSystems(NeurIPS).2023.
[64] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke
Zettlemoyer. â€œMega: Moving Average Equipped Gated Attentionâ€. In: The International Conference on Learning
Representations(ICLR).2023.
[65] EricMartinandChrisCundy.â€œParallelizingLinearRecurrentNeuralNetsOverSequenceLengthâ€.In:TheInter-
nationalConferenceonLearningRepresentations(ICLR).2018.
[66] TodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal.â€œCanaSuitofArmorConductElectricity?A
NewDatasetforOpenBookQuestionAnsweringâ€.In:arXivpreprintarXiv:1809.02789(2018).
[67] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
AmandaAskell,YuntaoBai,AnnaChen,TomConerly,DawnDrain,DeepGanguli,ZacHatfield-Dodds,Danny
Hernandez,ScottJohnston,AndyJones,JacksonKernion,LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,
JackClark,JaredKaplan,SamMcCandlish,andChrisOlah.â€œIn-contextLearningandInductionHeadsâ€.In:Trans-
formerCircuitsThread(2022).https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
[68] AntonioOrvieto,SamuelLSmith,AlbertGu,AnushanFernando,CaglarGulcehre,RazvanPascanu,andSoham
De.â€œResurrectingRecurrentNeuralNetworksforLongSequencesâ€.In:TheInternationalConferenceonMachine
Learning(ICML).2023.
[69] DenisPaperno,GermÃ¡nKruszewski,AngelikiLazaridou,Ngoc-QuanPham,RaffaellaBernardi,SandroPezzelle,
Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez. â€œThe LAMBADA Dataset: Word Prediction Requiring a
BroadDiscourseContextâ€.In:Proceedingsofthe54thAnnualMeetingoftheAssociationforComputationalLinguis-
tics.2016,pp.1525â€“1534.
[70] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and
DimitrisPapailiopoulos.â€œCanMambaLearnHowtoLearn?AComparativeStudyonIn-ContextLearningTasksâ€.
In:TheInternationalConferenceonMachineLearning(ICML).2024.
39[71] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael
Chung, Matteo Grella, Kranthi Kiran GV, et al. â€œRWKV: Reinventing RNNs for the Transformer Eraâ€. In: arXiv
preprintarXiv:2305.13048(2023).
[72] BoPeng,DanielGoldstein,QuentinAnthony,AlonAlbalak,EricAlcaide,StellaBiderman,EugeneCheah,Teddy
Ferdinan,HaowenHou,PrzemysÅ‚awKazienko,etal.â€œEagleandFinch:RWKVwithmatrix-valuedstatesanddy-
namicrecurrenceâ€.In:arXivpreprintarXiv:2404.05892(2024).
[73] HaoPeng,NikolaosPappas,DaniYogatama,RoySchwartz,NoahASmith,andLingpengKong.â€œRandomFeature
Attentionâ€.In:TheInternationalConferenceonLearningRepresentations(ICLR).2021.
[74] ClÃ©mentPernet.â€œComputingwithQuasiseparableMatricesâ€.In:ProceedingsoftheACMonInternationalSympo-
siumonSymbolicandAlgebraicComputation.2016,pp.389â€“396.
[75] ClÃ©mentPernet,HippolyteSignargout,andGillesVillard.â€œExactcomputationswithquasiseparablematricesâ€.In:
arXivpreprintarXiv:2302.04515(2023).
[76] ClÃ©mentPernetandArneStorjohann.â€œTimeandspaceefficientgeneratorsforquasiseparablematricesâ€.In:Journal
ofSymbolicComputation85(2018),pp.224â€“246.
[77] MichaelPoli,StefanoMassaroli,EricNguyen,DanielYFu,TriDao,StephenBaccus,YoshuaBengio,StefanoErmon,
and Christopher RÃ©. â€œHyena Hierarchy: Towards Larger Convolutional Language Modelsâ€. In: The International
ConferenceonMachineLearning(ICML).2023.
[78] HadiPouransari,Chun-LiangLi,Jen-HaoRickChang,PavanKumarAnasosaluVasu,CemKoc,VaishaalShankar,
andOncelTuzel.â€œDatasetDecomposition:FasterLLMTrainingwithVariableSequenceLengthCurriculumâ€.In:
arXivpreprintarXiv:2405.13226(2024).
[79] Ofir Press, Noah Smith, and Mike Lewis. â€œTrain Short, Test Long: Attention with Linear Biases Enables Input
LengthExtrapolationâ€.In:InternationalConferenceonLearningRepresentations.2022.
[80] ZhenQin,XiaodongHan,WeixuanSun,BowenHe,DongLi,DongxuLi,YuchaoDai,LingpengKong,andYiran
Zhong.â€œToeplitzNeuralNetworkforSequenceModelingâ€.In:TheInternationalConferenceonLearningRepresen-
tations(ICLR).2023.
[81] ZhenQin,XiaodongHan,WeixuanSun,DongxuLi,LingpengKong,NickBarnes,andYiranZhong.â€œThedevilin
lineartransformerâ€.In:arXivpreprintarXiv:2210.10340(2022).
[82] ZhenQin,DongLi,WeigaoSun,WeixuanSun,XuyangShen,XiaodongHan,YunshenWei,BaohongLv,XiaoLuo,
YuQiao,etal.â€œTransNormerLLM:AFasterandBetterLargeLanguageModelwithImprovedTransNormerâ€.In:
arXivpreprintarXiv:2307.14995(2023).
[83] ZhenQin,WeixuanSun,HuiDeng,DongxuLi,YunshenWei,BaohongLv,JunjieYan,LingpengKong,andYiran
Zhong.â€œCosFormer:RethinkingSoftmaxinAttentionâ€.In:TheInternationalConferenceonLearningRepresentations
(ICLR).2022.
[84] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. â€œHGRN2: Gated
LinearRNNswithStateExpansionâ€.In:arXivpreprintarXiv:2404.07904(2024).
[85] ZhenQin,SonglinYang,andYiranZhong.â€œHierarchicallyGatedRecurrentNeuralNetworkforSequenceModel-
ingâ€.In:AdvancesinNeuralInformationProcessingSystems36(2023).
[86] AliRahimiandBenjaminRecht.â€œRandomFeaturesforLarge-ScaleKernelMachinesâ€.In:AdvancesinNeuralIn-
formationProcessingSystems(NeurIPS)20(2007).
[87] PrajitRamachandran,BarretZoph,andQuocVLe.â€œSwish:ASelf-gatedActivationFunctionâ€.In:arXivpreprint
arXiv:1710.059417.1(2017),p.5.
[88] KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi.â€œWinogrande:AnAdversarialWinograd
SchemaChallengeatScaleâ€.In:CommunicationsoftheACM 64.9(2021),pp.99â€“106.
[89] ImanolSchlag,KazukiIrie,andJÃ¼rgenSchmidhuber.â€œLinearTransformersareSecretlyFastWeightProgrammersâ€.
In:TheInternationalConferenceonMachineLearning(ICML).PMLR.2021,pp.9355â€“9366.
[90] NoamShazeer.â€œFastTransformerDecoding:OneWrite-headisAllYouNeedâ€.In:arXivpreprintarXiv:1911.02150
(2019).
[91] SamShleifer,JasonWeston,andMyleOtt.â€œNormFormer:ImprovedTransformerPretrainingwithExtraNormal-
izationâ€.In:arXivpreprintarXiv:2110.09456(2021).
[92] MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,JaredCasper,andBryanCatanzaro.â€œMegatron-
LM:TrainingMulti-BillionParameterLanguageModelsUsingModelParallelismâ€.In:arXivpreprintarXiv:1909.08053
(2019).
[93] JimmyTHSmith,AndrewWarrington,andScottWLinderman.â€œSimplifiedStateSpaceLayersforSequenceMod-
elingâ€.In:TheInternationalConferenceonLearningRepresentations(ICLR).2023.
40[94] JianlinSu,YuLu,ShengfengPan,AhmedMurtadha,BoWen,andYunfengLiu.â€œRoformer:EnhancedTransformer
withRotaryPositionEmbeddingâ€.In:arXivpreprintarXiv:2104.09864(2021).
[95] YutaoSun,LiDong,ShaohanHuang,ShumingMa,YuqingXia,JilongXue,JianyongWang,andFuruWei.â€œReten-
tivenetwork:Asuccessortotransformerforlargelanguagemodelsâ€.In:arXivpreprintarXiv:2307.08621(2023).
[96] YiTay,MostafaDehghani,DaraBahri,andDonaldMetzler.â€œEfficientTransformers:ASurveyâ€.In:ACMComput-
ingSurveys55.6(2022),pp.1â€“28.
[97] ChameleonTeam.â€œChameleon:Mixed-ModalEarly-FusionFoundationModelsâ€.In:arXivpreprintarXiv:2405.09818
(2024).
[98] AnnaThomas,AlbertGu,TriDao,AtriRudra,andChristopherRÃ©.â€œLearningCompressedTransformswithLow
DisplacementRankâ€.In:AdvancesinNeuralInformationProcessingSystems(NeurIPS).2018,pp.9052â€“9060.
[99] IlyaOTolstikhin,NeilHoulsby,AlexanderKolesnikov,LucasBeyer,XiaohuaZhai,ThomasUnterthiner,Jessica
Yung,AndreasSteiner,DanielKeysers,JakobUszkoreit,etal.â€œMLP-Mixer:AnAll-MLPArchitectureforVisionâ€.
In:AdvancesinNeuralInformationProcessingSystems(NeurIPS)34(2021),pp.24261â€“24272.
[100] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothÃ©eLacroix,Baptiste
RoziÃ¨re,NamanGoyal,EricHambro,FaisalAzhar,etal.â€œLlama:OpenandEfficientFoundationLanguageModelsâ€.
In:arXivpreprintarXiv:2302.13971(2023).
[101] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi, YasmineBabaei,NikolayBashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal.â€œLlama2:Openfoundationandfine-tunedchatmodelsâ€.
In:arXivpreprintarXiv:2307.09288(2023).
[102] RafVandebril,MVanBarel,GeneGolub,andNicolaMastronardi.â€œAbibliographyonsemiseparablematricesâ€.In:
Calcolo42(2005),pp.249â€“270.
[103] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,LukaszKaiser,and
IlliaPolosukhin.â€œAttentionIsAllYouNeedâ€.In:AdvancesinNeuralInformationProcessingSystems(NeurIPS).2017.
[104] Shida Wang and Beichen Xue. â€œState-space Models with Layer-wise Nonlinearity are Universal Approximators
withExponentialDecayingMemoryâ€.In:arXivpreprintarXiv:2309.13414(2023).
[105] SinongWang,BelindaZLi,MadianKhabsa,HanFang,andHaoMa.â€œLinformer:Self-attentionwithLinearCom-
plexityâ€.In:arXivpreprintarXiv:2006.04768(2020).
[106] GuangxuanXiao,YuandongTian,BeidiChen,SongHan,andMikeLewis.â€œEfficientStreamingLanguageModels
withAttentionSinksâ€.In:TheInternationalConferenceonLearningRepresentations(ICLR).2024.
[107] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh.
â€œNystrÃ¶mformer: A NystrÃ¶m-Based Algorithm for Approximating Self-Attentionâ€. In: Proceedings of the AAAI
ConferenceonArtificialIntelligence.Vol.35.2021.
[108] SonglinYang,BailinWang,YikangShen,RameswarPanda,andYoonKim.â€œGatedLinearAttentionTransformers
withHardware-EfficientTrainingâ€.In:TheInternationalConferenceonMachineLearning(ICML).2024.
[109] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi.â€œHellaSwag:CanaMachineReallyFinish
YourSentence?â€In:Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics.2019.
[110] JinleZeng,MinLi,ZhihuaWu,JiaqiLiu,YuangLiu,DianhaiYu,andYanjunMa.â€œBoostingdistributedtraining
performanceoftheunpaddedbertmodelâ€.In:arXivpreprintarXiv:2208.08124(2022).
[111] ShuangfeiZhai,WalterTalbott,NitishSrivastava,ChenHuang,HanlinGoh,RuixiangZhang,andJoshSusskind.
â€œAnAttentionFreeTransformerâ€.In:arXivpreprintarXiv:2105.14103(2021).
[112] YujiaZhai, ChengquanJiang, LeyuanWang, XiaoyingJia,Shang Zhang,Zizhong Chen,Xin Liu,andYibo Zhu.
â€œBytetransformer:Ahigh-performancetransformerboostedforvariable-lengthinputsâ€.In:2023IEEEInternational
ParallelandDistributedProcessingSymposium(IPDPS).IEEE.2023,pp.344â€“355.
[113] MichaelZhang,KushBhatia,HermannKumbong,andChristopherRÃ©.â€œTheHedgehog&thePorcupine:Expres-
siveLinearAttentionswithSoftmaxMimicryâ€.In:TheInternationalConferenceonLearningRepresentations(ICLR).
2024.
[114] LinZheng,ChongWang,andLingpengKong.â€œLinearcomplexityrandomizedself-attentionmechanismâ€.In:In-
ternationalConferenceonMachineLearning.PMLR.2022,pp.27011â€“27041.
41A Glossary
Table 8: Glossary of notation and terminology; mnemonics bolded. (Top) Frequently used tensor dimensions. (Bottom)
Matricesandtensorsusedinstatespacemodelsorstructuredmaskedattention.
Notation Description Definition
T Timeaxisortargetsequenceaxis Definition2.1
S Sourcesequenceaxis(inattention) Equation(9)
D Modeldimensionord_model Definition7.1
N State/featuredimensionord_state Equations(2)and(9)
P Headdimensionord_head Definition2.1
H Numberof headsorn_head Definition7.1
ğ‘€ Sequencetransformationmatrix Definition2.3
ğ´ DiscreteSSMrecurrent(state)matrix Equation(2)
ğµ Statespacemodelinputprojection(expansion)matrix Equation(2)
ğ¶ Statespacemodeloutputprojection(contraction)matrix Equation(2)
ğ‘‹ Inputmatrix(shape(T,P)) Equations(2)and(9)
ğ‘Œ Outputmatrix(shape(T,P)) Equations(2)and(9)
ğ‘„ Attentionquerymatrix Equation(9)
ğ¾ Attentionkeymatrix Equation(9)
ğ‘‰ Attentionvaluematrix Equation(9)
ğº AttentionGrammatrix ğ‘„ğ¾âŠ¤(orğ¶ğµâŠ¤)
ğ¿ (Structured)maskmatrix(lower-triangularinthecausalsetting) Definition4.2
B Efficient Algorithms for the Scalar SSM Scan (1-SS Multiplication)
InthissectionwefleshoutvariousalgorithmsforcomputingthescalarSSMscan,throughthelensofstructuredmatrix
decompositions.ThescalarSSMscanisdefinedascomputingtherecurrentpartofthediscreteSSM(7),inthecasewhen
ğ‘ = 1(i.e.ğ´isascalar). ThisiscommonlyusedtocomputeSSMsrecurrently;inparticular,thecaseofstructuredSSMs
whereğ´isdiagonallystructuredreducesdowntothisoperation,suchasintheS5(J.T.Smith,Warrington,andLinderman
2023)andS6(GuandDao2023)models.
The goal of this section is to support a central theme of this paper that efficient algorithms for sequence models can be
viewedasstructuredmatrixmultiplicationalgorithms. Thevariousmatrixdecompositionideasweshowherearerelated
toideasusedtoderivefastSSMalgorithms(Section6),aswellasdirectlyusedasasubroutine.
B.1 ProblemDefinition
Letğ‘ : (D,)andğ‘ : (D,)besequencesofscalars.ThescalarSSMscanisdefinedas
â„
ğ‘¡
=ğ‘ ğ‘¡â„ ğ‘¡âˆ’1+ğ‘ ğ‘¡. (21)
Hereâ„ âˆ’1canbeanarbitraryvaluerepresentingtheprevioushiddenstatetotheSSMrecurrence;unlessotherwisespeci-
fied,weassumeâ„
âˆ’1
=0.
Wealsocallequation(21)thecumprodsum(cumulativeproductsum). Notethatthecumprodsumreducestothecumprod
(cumulativeproduct)whenğ‘ =0istheadditiveidentityanditreducestothecumsum(cumulativesum)whenğ‘ =1isthe
multiplicativeidentity.
42Finally,notethatinvectorizedformwecanwrite
â„ =ğ‘€ğ‘
1
ï£® ï£¹
ï£¯ ğ‘ 1 ï£º
ï£¯ 1 ï£º
ï£¯ ğ‘ ğ‘ ğ‘ 1 ï£º
ğ‘€ = ï£¯ 2 1 2 ï£º
ï£¯ ï£¯ . . . . . . ... ... ï£º ï£º
ï£¯ ï£º
ï£¯ğ‘ ...ğ‘ ğ‘ ...ğ‘ ... ğ‘ 1ï£º
ï£¯ ğ‘‡âˆ’1 1 ğ‘‡âˆ’1 2 ğ‘‡âˆ’1 ï£º
ï£° ï£»
Inotherwords,thisissimplythematrix-vectorproductbya1-SSmatrixğ‘€.
Thereforewehavethreewaysofviewingthisfundamentalprimitiveoperationthatareallequivalent:
â€¢ A(scalar)SSMscan.
â€¢ Acumprodsum.
â€¢ A1-SSmatrix-vectormultiplication.
B.2 ClassicalAlgorithms
WefirstdescribethetwoclassicalwaysofcomputingtheSSMscan(21),previouslyusedbypriorwork.
B.2.1 SequentialRecurrence
Therecurrentmodesimplycomputes(21)onetimestepğ‘¡ atatime. Fromtheperspectiveof1-SSmultiplication,thiswas
alsodescribedinSection3.4.1.
B.2.2 ParallelAssociativeScan
Second,animportantobservationisthatthisrecurrencecanbeturnedintoanassociativescan(E.MartinandCundy2018;
J.T.Smith, Warrington, andLinderman2023). Thisfactisnotcompletelyobvious. Forexample, S5definedthecorrect
associativescanoperatorandthenshowedassociativityoftheoperatorthroughrotecalculation.
Aslightlycleanerwaytoseethatthisiscomputablewithanassociativescanistoturnthemulti-termrecurrenceintoa
single-termrecurrenceonahiddenstateofsize2insteadof1:
â„
ğ‘¡
=ğ‘ ğ‘¡â„ ğ‘¡âˆ’1+ğ‘
ğ‘¡
(cid:20)â„ (cid:21) (cid:20)ğ‘ ğ‘ (cid:21) (cid:20)â„ (cid:21)
ğ‘¡ = ğ‘¡ ğ‘¡ ğ‘¡âˆ’1 .
1 0 1 1
Then computing all theâ„
ğ‘¡
is the same as taking the cumulative products of these 2Ã—2 matrices. Since matrix multi-
plicationisassociative,thiscanbecomputedwithanassociativescan. Theassociativebinaryoperatorissimplymatrix
multiplicationontheseparticularmatrices:
(cid:20)ğ‘
ğ‘¡
ğ‘ ğ‘¡(cid:21) (cid:20)ğ‘
ğ‘ 
ğ‘ ğ‘ (cid:21)
=
(cid:20)ğ‘ ğ‘¡ğ‘
ğ‘ 
ğ‘ ğ‘¡ğ‘
ğ‘ 
+ğ‘ ğ‘¡(cid:21)
.
0 1 0 1 0 1
EquatingthetoprowyieldsthesameassociativescanoperatorasdefinedbyS5:
(ğ‘ ğ‘¡,ğ‘ ğ‘¡) âŠ— (ğ‘ ğ‘ ,ğ‘ ğ‘ ) = (ğ‘ ğ‘¡ğ‘ ğ‘ ,ğ‘ ğ‘¡ğ‘ ğ‘  +ğ‘ ğ‘¡). (22)
Thereasonwhyassociativescansareimportantisthattheycanbeparallelizedusingadivide-and-conqueralgorithm(Blel-
loch1990).Weomitthedetailsofthisalgorithm,andinsteadshowthattheentireassociativeSSMscanalgorithmcanbe
derivedfromscratchthroughmatrixdecompositions(AppendixB.3.5).
43B.3 EfficientAlgorithmsviaStructuredMatrixDecompositions
WediscussseveralalgorithmsforcomputingtheSSMscan,allthroughthelensoffindingstructuredmatrixdecomposi-
tionsofthe1-SSmatrixğ‘€.Thesealgorithmsorcomputationmodesinclude
â€¢ Adilated modewhereinformationispropagated1,2,4,8,... stepsatatime.
â€¢ Astate-passingmodewhereinformationispropagatedforwardinchunks.
â€¢ Afullyrecurrent modethatincrementsonestepatatime,whichisaspecialcaseofthestate-passingmode.
â€¢ Ablockdecompositionparallelmodewhereğ‘€ isdividedintohierarchicalblocks.
â€¢ Ascanmodewhereğ‘€ isdivideintoequalsizeblocksandreducedrecursively.
B.3.1 DilatedMode
This mode factors the 1-SS matrix in a particular way involving increasing â€œstridesâ€. This is best illustrated through a
concreteexample:
ï£®ğ‘0:0
ï£¹
ï£¯ ï£¯ğ‘1:0 ğ‘1:1 ï£º
ï£º
ï£¯ ï£¯ğ‘2:0 ğ‘2:1 ğ‘2:2 ï£º
ï£º
ğ‘€=ï£¯ ï£¯ğ‘3:0 ğ‘3:1 ğ‘3:2 ğ‘3:3 ï£º
ï£º
ï£¯ ï£¯ğ‘4:0 ğ‘4:1 ğ‘4:2 ğ‘4:3 ğ‘4:4 ï£º
ï£º
ï£¯ ï£¯ğ‘5:0 ğ‘5:1 ğ‘5:2 ğ‘5:3 ğ‘5:4 ğ‘5:5 ï£º
ï£º
ï£¯ ï£¯ğ‘6:0 ğ‘6:1 ğ‘6:2 ğ‘6:3 ğ‘6:4 ğ‘6:5 ğ‘6:6 ï£º
ï£º
ï£¯ ï£°ğ‘7:0 ğ‘7:1 ğ‘7:2 ğ‘7:3 ğ‘7:4 ğ‘7:5 ğ‘7:6 ğ‘7:7ï£º
ï£»
ï£®ğ‘0:0 ï£¹ï£®ğ‘0:0 ï£¹ï£®ğ‘0:0
ï£¹
ï£¯
ï£¯
ğ‘1:1 ï£º ï£ºï£¯
ï£¯
ğ‘1:1 ï£º ï£ºï£¯ ï£¯ğ‘1:0 ğ‘1:1 ï£º
ï£º
ï£¯
ï£¯
ğ‘2:2 ï£º ï£ºï£¯ ï£¯ğ‘2:0 ğ‘2:2 ï£º ï£ºï£¯
ï£¯
ğ‘2:1 ğ‘2:2 ï£º
ï£º
=ï£¯
ï£¯
ğ‘3:3 ï£º ï£ºï£¯
ï£¯
ğ‘3:1 ğ‘3:3 ï£º ï£ºï£¯
ï£¯
ğ‘3:2 ğ‘3:3 ï£º
ï£º
ï£¯ ï£¯ğ‘4:0 ğ‘4:4 ï£º ï£ºï£¯
ï£¯
ğ‘4:2 ğ‘4:4 ï£º ï£ºï£¯
ï£¯
ğ‘4:3 ğ‘4:4 ï£º
ï£º
ï£¯
ï£¯
ğ‘5:1 ğ‘5:5 ï£º ï£ºï£¯
ï£¯
ğ‘5:3 ğ‘5:5 ï£º ï£ºï£¯
ï£¯
ğ‘5:4 ğ‘5:5 ï£º
ï£º
ï£¯
ï£¯
ğ‘6:2 ğ‘6:6 ï£º ï£ºï£¯
ï£¯
ğ‘6:4 ğ‘6:6 ï£º ï£ºï£¯
ï£¯
ğ‘6:5 ğ‘6:6 ï£º
ï£º
ï£¯
ï£°
ğ‘7:3 ğ‘7:7ï£º ï£»ï£¯
ï£°
ğ‘7:5 ğ‘7:7ï£º ï£»ï£¯
ï£°
ğ‘7:6 ğ‘7:7ï£º
ï£»
Notethatthiscloselyresemblesthecomputationofdilatedconvolutions.
Wealsonotethatthisfactorizationshowsthat1-SSmatricesareaspecialcaseofbutterflymatrices,anotherbroadand
fundamentaltypeofstructuredmatrix(Dao,Gu,etal.2019;Dao,Sohoni,etal.2020).
Remark8. Thisalgoritihmissometimesdescribedasaâ€œwork-inefficientbutmoreparallelizableâ€prefixsumalgorithm(Hillis
andSteeleJr1986),becausesitusesğ‘‚(ğ‘‡ log(ğ‘‡))operationsbuthashalfthedepth/spanasthework-efficientassociativescan
algorithm.
B.3.2 State-Passing(Chunkwise)Mode
Thismodecanbeviewedasageneralizationofthestandardrecurrentmodewhereinsteadofpassingforwardtherecurrent
stateâ„onestepatatime,wecomputetheansweronchunksofarbitrarylengthğ‘˜ andpassthestatethroughthechunk.
Thiscanalsobederivedfromasimpleblockdecompositionofthe1-SSmatrix.
Remark 9. While we call this â€œstate-passingâ€ to refer to how states are passed from one local segment to another, this is
relatedtotheâ€œchunkwiseâ€algorithmsproposedbyrelatedmodels(Y.Sunetal.2023;Yangetal.2024).
Considercomputingâ„ = ğ‘€ğ‘ inâ€œchunksâ€: forsomeindexğ‘˜ âˆˆ [ğ‘‡],wewanttocomputeâ„ 0:ğ‘˜ ortheoutputuptoindexğ‘˜,
andhaveawaytoreducetheproblemtoasmallerproblemonindices [ğ‘˜ :ğ‘‡].
44Wewriteğ‘€ as
ğ‘
ï£® 0:0 ï£¹
ï£¯ ğ‘ ğ‘ ï£º
ï£¯ 1:0 1:1 ï£º
ï£¯ ï£¯ . . . ... ï£º ï£º
ï£¯ ï£º
ğ‘€ = ï£¯ ï£¯ğ‘ ğ‘˜âˆ’1:0 ... ... ğ‘ ğ‘˜âˆ’1:ğ‘˜âˆ’1 ï£º ï£º
ï£¯ ğ‘ ... ... ğ‘ ğ‘ ï£º
ï£¯ ğ‘˜:0 ğ‘˜:ğ‘˜âˆ’1 ğ‘˜:ğ‘˜ ï£º
ï£¯ ï£¯ . . . . . . . . . ... ï£º ï£º
ï£¯ ï£º
ï£¯ğ‘ ... ... ğ‘ ğ‘ ... ğ‘ ï£º
ï£¯ ğ‘‡âˆ’1:0 ğ‘‡âˆ’1:ğ‘˜âˆ’1 ğ‘‡âˆ’1:ğ‘˜ ğ‘‡âˆ’1:ğ‘‡âˆ’1ï£º
ï£° ï£»
Lettheupper-lefttrianglebeğ‘€ ,lower-rightbeğ‘€ (leftandrightsubproblems),andlower-leftbeğ‘€ . Divideupğ‘ into
ğ¿ ğ‘… ğ¶
ğ‘
ğ¿
=ğ‘
0:ğ‘˜
andğ‘
ğ‘…
=ğ‘
ğ‘˜:ğ‘‡
inthesameway.Notethat
(cid:20) ğ‘€ ğ‘ (cid:21)
ğ‘€ğ‘ = ğ¿ ğ¿
ğ‘€ ğ‘…ğ‘
ğ‘…
+ğ‘€ ğ¶ğ‘
ğ¿
Also,ğ‘€ hastherank-1factorization(thisisessentiallythedefiningpropertyofsemiseparablematrices)
ğ¶
ğ‘
ï£® ğ‘˜:ğ‘˜ ï£¹
ğ‘€ ğ¶ = ï£¯ ï£¯ ï£¯ . . . ï£º ï£º ï£ºğ‘ ğ‘˜ (cid:2)ğ‘ ğ‘˜âˆ’1:0 Â·Â·Â· ğ‘ ğ‘˜âˆ’1:ğ‘˜âˆ’1(cid:3)
ï£¯ğ‘ ï£º
ï£¯ ğ‘‡âˆ’1:ğ‘˜ï£º
ï£° ï£»
Thus
ğ‘
ï£® ğ‘˜:ğ‘˜ ï£¹
ï£¯ . ï£º
ğ‘€ ğ¶ğ‘ ğ¿ = ï£¯ ï£¯ . . ï£º ï£ºğ‘ ğ‘˜ Â·(ğ‘€ğ‘)ğ‘˜âˆ’1.
ï£¯ğ‘ ï£º
ï£¯ ğ‘‡âˆ’1:ğ‘˜ï£º
ï£° ï£»
Here we think of (ğ‘€ğ‘)ğ‘˜âˆ’1 = â„ ğ‘˜âˆ’1 as the â€œfinal stateâ€ of the left chunk, because the row vector inğ‘€ ğ¶â€™s factorization is
thesameasthefinalrowofğ‘€ . Furthermore,notethatthecolumnvectorinğ‘€ â€™sfactorizationisthesameasthefinal
ğ¿ ğ¶
columnofğ‘€ .7 Thus
ğ‘…
ï£®ğ‘ ğ‘˜â„ ğ‘˜âˆ’1+ğ‘
ğ‘˜ï£¹
ï£¯ ğ‘ ï£º
ï£¯ ğ‘˜+1 ï£º
ğ‘€ ğ‘…ğ‘
ğ‘…
+ğ‘€ ğ¶ğ‘
ğ¿
=ğ‘€
ğ‘…
ï£¯
ï£¯
.
. .
ï£º
ï£º
ï£¯ ï£º
ï£¯ ğ‘ ï£º
ï£¯ ğ‘‡âˆ’1 ï£º
ï£° ï£»
Finally, we use the observation that ğ‘€ and ğ‘€ are self-similar to the original matrix ğ‘€; the answers for these two
ğ¿ ğ‘…
smaller1-SSmatrixmultiplicationscanbeperformedarbitrarilyusinganyalgorithm.Intotal,thealgorithmproceedsas
follows:
1. Computethelefthalfoftheanswerâ„ usinganydesiredmethod(i.e.anyofthemethodsfor1-SSmultiplication
0:ğ‘˜
fromthissection).
2. Computethefinalstateâ„ .
ğ‘˜âˆ’1
3. Incrementthestatebyonesteptomodifyğ‘ .
ğ‘˜
4. Computetherighthalfoftheanswerâ„ usinganydesiredmethod.
ğ‘˜:ğ‘‡
Inotherwords,wecomputetheleftsubproblemasablackbox,passitsfinalstateontotherightproblem,andcompute
therightsubproblemasablackbox.
Theutilityofthismethodcomesfrommorecomplicatedsettings,suchasinthegeneralğ‘-semiseparablecase,andwhen
theinputğ‘ hasanadditionalâ€œbatchâ€dimension(orinotherwordsthisisamatrix-matrixinsteadofmatrix-vectormulti-
plication).Inthiscase,wecanuseanalternatealgorithmforthechunks(correspondingtoMMbyğ‘€ andğ‘€ )thatdoes
ğ¿ ğ‘…
notmaterializethefullhiddenstatesâ„. Instead,weskipthehiddenstatesanddirectlycomputethefinalstateâ„ inan
ğ‘˜âˆ’1
alternateway,thenâ€œpassâ€thestatetothenextchunk.
7BoththesefactscanbeseenfromtheWoodburyinverse...
45Complexity. Thismethodcanbeverywork-efficientbecausesteps2-3takesonlyconstanttime. Thereforeassuming
thetwosubproblems(steps1and4)arelineartime,thewholemethodtakeslineartime.
Thedownsideisthatthisisalsosequential.
B.3.3 FullyRecurrentMode
Notethatthefullyrecurrentmode,wheretherecurrenceisevolvedonestepatatime(21),issimplyaninstantiationof
thestate-passingmodewithchunksizeğ‘˜ =1.
B.3.4 (Parallel)BlockDecompositionMode
Thisusesthesamematrixdecompositionasthestate-passingmode,butcomputessubproblemsinadifferentorderthat
tradesoffcomputationforparallelization.
Asusual,wewriteğ‘€ as
1 1 âˆ’1
ï£® ï£¹ ï£® ï£¹
ï£¯
ï£¯
ğ‘
1
1 ï£º
ï£º
ï£¯ ï£¯âˆ’ğ‘
1
1 ï£º
ï£º
ğ‘€ =
ï£¯
ï£¯
ğ‘ 2ğ‘
1
ğ‘
2
1 ï£º
ï£º =
ï£¯
ï£¯
0 âˆ’ğ‘
2
1 ï£º
ï£º
ï£¯ ï£¯ . . . . . . ... ... ï£º ï£º ï£¯ ï£¯ . . . . . . ... ... ï£º ï£º
ï£¯ ï£º ï£¯ ï£º
ï£¯ ï£¯ğ‘ ğ‘‡âˆ’1...ğ‘
1
ğ‘ ğ‘‡âˆ’1...ğ‘
2
... ğ‘
ğ‘‡âˆ’1
1ï£º
ï£º
ï£¯
ï£¯
0 0 ... âˆ’ğ‘
ğ‘‡âˆ’1
1ï£º
ï£º
ï£° ï£» ï£° ï£»
Thekeyobservationisagainthatthebottom-leftquadrantofğ‘€ isrank-1.Asidefrominspection,anotherwaytoseethis
isbyusingtheRHS,observingthatthebottom-leftquadrantofitisatrivialrank-1matrix(itisall0exceptthetop-right
cornerisâˆ’ğ‘ ğ‘‡/2),andusingtheWoodburyinversionformulatoseethatthebottom-leftcorneroftheLHSmustalsobe
rank1.Thisalsoprovidesawaytodeducetherank-1factorization,whichcanbeverifiedthroughinspection:
ï£® (ğ‘ ğ‘‡/2...ğ‘ 1) ... ğ‘ ğ‘‡/2 ï£¹
ğ‘€ lower-left-quadrant = ï£¯ ï£¯ ï£¯ . . . ... . . . ï£º ï£º ï£º
ï£¯ ï£¯(ğ‘ ğ‘‡âˆ’1...ğ‘ ğ‘‡/2ğ‘ ğ‘‡/2âˆ’1...ğ‘ 1) ... (ğ‘ ğ‘‡âˆ’1...ğ‘ ğ‘‡/2)ï£º ï£º
ï£° ï£»
ğ‘
ï£® ğ‘‡/2 ï£¹
= ï£¯ ï£¯ ï£¯ . . . ï£º ï£º ï£º (cid:2) (ğ‘ ğ‘‡/2âˆ’1...ğ‘ 1) ... ğ‘ ğ‘‡/2âˆ’1 1(cid:3).
ï£¯ğ‘ ...ğ‘ ï£º
ï£¯ ğ‘‡âˆ’1 ğ‘‡/2ï£º
ï£° ï£»
A second observation is that this matrix is self-similar: any principle submatrix has the same form. In particular, the
top-leftandbottom-rightquadrantsareboth1-SSmatrices.
Thisprovidesaneasywaytoperformthematrixmultiplicationbyğ‘€:recurseonthetwohalves(i.e.top-leftandbottom-
right) in parallel, and then account for the bottom-left submatrix. This â€œcombinationâ€ step in the divide-and-conquer
algorithmiseasysincethesubmatrixisrank1.Thisleadstoaparallelalgorithm.
Complexity. Likethestate-passingalgorithm,thismethodusesthesameblockdecompositionsoftherank-structured
semiseparablematrices. Thedifferenceisthatwerecurseonbothsubproblemsinparallel,whilethestate-passingalgo-
rithmhandlestheleftandthenrightsubproblems.Thislowersthedepth/spanofthealgorithmfromlineartolog(ğ‘‡).The
tradeoffisthatthecombinationstep(accountingfortherank-1bottom-leftsubmatrix)requireslinearinsteadofconstant
work,sothetotalworkisğ‘‚(ğ‘‡ log(ğ‘‡))insteadoflinear.
Note also that in the recursion, we can stop at any time and compute the subproblems in any other way. This is a
main idea behind the SSD algorithm (Section 6), where we switch to the dual quadratic attention formulation on small
subproblems.
B.3.5 AssociativeScanMode
Thestatepassing(chunkwise)algorithmhaslinearwork,butalsoinvolvessequentialoperations.
46Theblockmatrixreductionanddilatedmodesareparallelizable: theyhavelog(ğ‘‡) depth/span. However, theydoextra
work(ğ‘‚(ğ‘‡ log(ğ‘‡)).
As noted in Appendix B.2.2, there is an algorithm that achieves bothğ‘‚(logğ‘‡) depth andğ‘‚(ğ‘‡) work by leveraging the
associativescan(alsocalledprefixscan)algorithm(Bakeretal.1996). ThisalgorithmismosteasilyseenfromtheSSM
scanorcumprodsumview,andeventhenisnotobvious: itrequiresseparatelyderivinganassociativeoperator(22),and
thenleveragingtheparallel/associative/prefixscanalgorithmasablackbox(Blelloch1990).
Here we show that it is actually possible to derive this parallel scan from leveraging a different matrix decomposi-
tion:
ğ‘
ï£® 0:0 ï£¹
ï£¯ğ‘ ğ‘ ï£º
ï£¯ 1:0 1:1 ï£º
ï£¯ğ‘ ğ‘ ğ‘ ï£º
ï£¯ 2:0 2:1 2:2 ï£º
ï£¯ğ‘ ğ‘ ğ‘ ğ‘ ï£º
ğ‘€ = ï£¯ ï£¯ğ‘3:0 ğ‘3:1 ğ‘3:2 ğ‘3:3 ğ‘ ï£º ï£º
ï£¯ 4:0 4:1 4:2 4:3 4:4 ï£º
ï£¯ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ï£º
ï£¯ 5:0 5:1 5:2 5:3 5:4 5:5 ï£º
ï£¯ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ï£º
ï£¯ 6:0 6:1 6:2 6:3 6:4 6:5 6:6 ï£º
ï£¯ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ï£º
ï£¯ 7:0 7:1 7:2 7:3 7:4 7:5 7:6 7:7ï£º
ï£° ï£»
ï£® ğ‘ 0:0 ï£¹
ï£¯ ï£º
ï£¯ ï£º
ï£¯ ğ‘ 1:0 ğ‘ 1:1 ï£º
ï£¯ ï£º
ï£¯ ï£º
ï£¯ (cid:20)ğ‘ (cid:21) (cid:20)ğ‘ (cid:21)âŠ¤ ğ‘ 2:2 ï£º
ï£¯ 2:2 ğ‘ 1:0 ï£º
ï£¯ ï£¯ ğ‘ 3:2 2:1 ğ‘ 1:1 ğ‘ 3:2 ğ‘ 3:3 ï£º ï£º
ï£¯ ï£º
= ï£¯ ï£º
ï£¯ (cid:20)ğ‘ (cid:21) (cid:20)ğ‘ (cid:21)âŠ¤ (cid:20)ğ‘ (cid:21) (cid:20)ğ‘ (cid:21)âŠ¤ ğ‘ 4:4 ï£º
ï£¯ 4:4 ğ‘ 1:0 4:4 ğ‘ 3:2 ï£º
ï£¯ ï£¯ ğ‘ 5:4 4:1 ğ‘ 1:1 ğ‘ 5:4 4:3 ğ‘ 3:3 ğ‘ 5:4 ğ‘ 5:5 ï£º ï£º
ï£¯ ï£º
ï£¯ ï£º
ï£¯ (cid:20)ğ‘ (cid:21) (cid:20)ğ‘ (cid:21)âŠ¤ (cid:20)ğ‘ (cid:21) (cid:20)ğ‘ (cid:21)âŠ¤ (cid:20)ğ‘ (cid:21) (cid:20)ğ‘ (cid:21)âŠ¤ ğ‘ 6:6 ï£º
ï£¯ ï£¯
ï£¯
ğ‘6 7: :6
6
ğ‘ 6:1 ğ‘1 1: :0
1
ğ‘6 7: :6
6
ğ‘ 6:3 ğ‘3 3: :2
3
ğ‘6 7: :6
6
ğ‘ 6:1 ğ‘5 5: :4
5 ğ‘ 7:6 ğ‘ 7:7
ï£º ï£º
ï£º
ï£¯ ï£º
ï£° ï£»
Nowweproceedinthreestages.
Stage 1. First we compute the answers for each of the diagonal blocks in the multiplication ğ‘€ğ‘. This produces two
numbers,butthefirstelementisunchanged.Forexample,thesecondblockisgoingtocomputeğ‘ 2andğ‘ 3ğ‘ 2+ğ‘
3
Stage2. Nowconsidereachofthe2Ã—2blocksfactoredasarank-1matrixinthestrictlylowertriangularpartofthe
matrix. Note that each of the right side row vectors is the same as the bottom row vector in the diagonal block in its
column:inparticularthe [ğ‘ 1:0ğ‘ 1:1], [ğ‘ 3:2ğ‘ 3:3],and [ğ‘ 5:4ğ‘ 5:5] rows.
ThereforewealreadyhavetheanswerstothesefromStage1,whichisthesecondelementofallğ‘‡/2subproblemsinStage
1. Ifwecallthisarrayofelementsğ‘â€² (ofhalfthesizeofğ‘),thenweneedtomultiplyğ‘â€² bythe1-SSmatrixgeneratedby
ğ‘ ,ğ‘ ,ğ‘ ,ğ‘ .
3:âˆ’1 3:1 5:3 7:5
Stage3. Finally,eachoftheanswerstoStage2canbebroadcastintotwofinalanswersbymultiplyingbytheleft-side
columnvectors:inparticularthe [ğ‘ 2:2ğ‘ 3:2]âŠ¤, [ğ‘ 4:4ğ‘ 5:4]âŠ¤,and [ğ‘ 6:6ğ‘ 7:6]âŠ¤vectors.
Note that this can be slightly modified with some off-by-one shifting of the indices. An equivalent way to view this
algorithmisasthethree-stepmatrixfactorization
47ï£®ğ‘0:0
ï£¹
ï£¯ ï£¯ğ‘1:0 ğ‘1:1 ï£º
ï£º
ï£¯ ï£¯ğ‘2:0 ğ‘2:1 ğ‘2:2 ï£º
ï£º
ğ‘€=ï£¯ ï£¯ğ‘3:0 ğ‘3:1 ğ‘3:2 ğ‘3:3 ï£º
ï£º
ï£¯ ï£¯ğ‘4:0 ğ‘4:1 ğ‘4:2 ğ‘4:3 ğ‘4:4 ï£º
ï£º
ï£¯ ï£¯ğ‘5:0 ğ‘5:1 ğ‘5:2 ğ‘5:3 ğ‘5:4 ğ‘5:5 ï£º
ï£º
ï£¯ ï£¯ğ‘6:0 ğ‘6:1 ğ‘6:2 ğ‘6:3 ğ‘6:4 ğ‘6:5 ğ‘6:6 ï£º
ï£º
ï£¯ ï£°ğ‘7:0 ğ‘7:1 ğ‘7:2 ğ‘7:3 ğ‘7:4 ğ‘7:5 ğ‘7:6 ğ‘7:7ï£º
ï£»
ï£®ğ‘0:0 ï£¹ï£®ğ‘0:0 ï£¹ï£®ğ‘0:0
ï£¹
ï£¯
ï£¯
ğ‘1:1 ï£º ï£ºï£¯
ï£¯
ğ‘1:1 ï£º ï£ºï£¯ ï£¯ğ‘1:0 ğ‘1:1 ï£º
ï£º
ï£¯
ï£¯
ğ‘2:1 ğ‘2:2 ï£º ï£ºï£¯
ï£¯
ğ‘2:2 ï£º ï£ºï£¯
ï£¯
ğ‘2:2 ï£º
ï£º
=ï£¯
ï£¯
ğ‘3:3 ï£º ï£ºï£¯
ï£¯
ğ‘3:1 ğ‘3:3 ï£º ï£ºï£¯
ï£¯
ğ‘3:2 ğ‘3:3 ï£º
ï£º
ï£¯
ï£¯
ğ‘4:3 ğ‘4:4 ï£º ï£ºï£¯
ï£¯
ğ‘4:4 ï£º ï£ºï£¯
ï£¯
ğ‘4:4 ï£º
ï£º
ï£¯
ï£¯
ğ‘5:5 ï£º ï£ºï£¯
ï£¯
ğ‘5:1 ğ‘5:3 ğ‘5:5 ï£º ï£ºï£¯
ï£¯
ğ‘5:4 ğ‘5:5 ï£º
ï£º
ï£¯
ï£¯
ğ‘6:5 ğ‘6:6 ï£º ï£ºï£¯
ï£¯
ğ‘6:6 ï£º ï£ºï£¯
ï£¯
ğ‘6:6 ï£º
ï£º
ï£¯
ï£°
ğ‘7:7ï£º ï£»ï£¯
ï£°
ğ‘7:1 ğ‘7:3 ğ‘7:5 ğ‘7:7ï£º ï£»ï£¯
ï£°
ğ‘7:6 ğ‘7:7ï£º
ï£»
NotethatStage1andStage3requireğ‘‚(ğ‘‡)work,whileStage2reducestoaself-similarproblemofhalfthesize.Itiseasy
tocheckthatthisrequiresğ‘‚(ğ‘‡)totalworkandğ‘‚(logğ‘‡)depth/span.
Remark10. Infact,itispossibletoseethatthecomputationgraphofthisalgorithmisidenticaltothatoftheassociative
scanalgorithmdescribedinAppendixB.2.2. Thekeytakeawayisthatinsteadofthestepsof(1)recognizingthatğ‘€ definesa
recurrence(2)observingthattherecurrencecanbedefinedwithanassociativebinaryoperator;thereisacompletelydifferent
perspectiveofsimplyfindingastructuredmatrixdecompositionalgorithmforğ‘€.
C Theory Details
C.1 Extras: ClosurePropertiesofSSMs
Wepresentheresomeadditionalpropertiesofsemiseparablematricestoillustratetheirflexibilityandutility.Thissection
isnotnecessarytounderstandourcoreresults.
PropositionC.1(SemiseparableClosureProperties). Semiseparablematricesareclosedunderseveralprimitiveoperations.
â€¢ Addition:Thesumofanğ‘-SSandğ‘ƒ-SSmatrixisatmost(ğ‘ +ğ‘ƒ)-SS.
â€¢ Multiplication:Theproductofanğ‘-SSandğ‘ƒ-SSmatrixis(ğ‘ +ğ‘ƒ)-SS.
â€¢ Inverse:Theinverseofanğ‘-SSmatrixisatmost(ğ‘ +1)-SS.
Theadditionandmultiplicationpropertiesareeasilyseen. Theinversepropertyhasmanyproofs;oneapproachfollows
immediately from the Woodbury inversion identity, which has also featured prominently in the structured SSM litera-
ture(Gu,Goel,andRÃ©2022).
Inturn,theseimplyclosurepropertiesofstatespacemodels.
Forexample,theadditionpropertysaysthatsummingtwoparallelSSMmodelsisstillanSSM.Themultiplicationproperty
saysthatsequentiallycomposingorchainingtwoSSMscanstillbeviewedasanSSM,whosetotalstatesizeisadditiveâ€“a
somewhatnontrivialfact.
Finally, the inverse property can let us relate SSMs to other types of models. For example, one can notice that banded
matrices are semiseparable, so their inverses are semiseparable. (In fact, the semiseparable family of structure is often
motivated by taking inverses of banded matrices (Vandebril et al. 2005)). Moreover, the fast recurrence properties of
semiseparablematricescanbeviewedasaconsequenceoftheirinversebeingbanded.
Remark11. Thefactthat1-SSmatricesaresimplerecurrences(7)areequivalenttothefactthattheinverseofa1-SSmatrix
48isa2-bandedmatrix:
1 1 âˆ’1
ï£® ï£¹ ï£® ï£¹
ï£¯
ï£¯
ğ‘
1
1 ï£º
ï£º
ï£¯ ï£¯âˆ’ğ‘
1
1 ï£º
ï£º
ğ‘€ =
ï£¯
ï£¯
ğ‘ 2ğ‘
1
ğ‘
2
1 ï£º
ï£º =
ï£¯
ï£¯
0 âˆ’ğ‘
2
1 ï£º
ï£º
ï£¯ ï£¯ . . . . . . ... ... ï£º ï£º ï£¯ ï£¯ . . . . . . ... ... ï£º ï£º
ï£¯ ï£º ï£¯ ï£º
ï£¯ ï£¯ğ‘ ğ‘‡âˆ’1...ğ‘
1
ğ‘ ğ‘‡âˆ’1...ğ‘
2
... ğ‘
ğ‘‡âˆ’1
1ï£º
ï£º
ï£¯
ï£¯
0 0 ... âˆ’ğ‘
ğ‘‡âˆ’1
1ï£º
ï£º
ï£° ï£» ï£° ï£»
Thusğ‘¦ =ğ‘€ğ‘¥ â†”ğ‘€âˆ’1ğ‘¦ =ğ‘¥,or
1
ï£® ï£¹
ï£¯ ï£¯âˆ’ğ‘
1
1 ï£º
ï£º
ï£¯
ï£¯
0 âˆ’ğ‘
2
1 ï£º
ï£ºğ‘¦ =ğ‘¥.
ï£¯ ï£¯ . . . . . . ... ... ï£º ï£º
ï£¯ ï£º
ï£¯
ï£¯
0 0 ... âˆ’ğ‘
ğ‘‡âˆ’1
1ï£º
ï£º
ï£° ï£»
Orelementwise,
ğ‘¦
ğ‘¡
âˆ’ğ‘ ğ‘¡ğ‘¦
ğ‘¡âˆ’1
=ğ‘¥
ğ‘¡
ğ‘¦
ğ‘¡
=ğ‘ ğ‘¡ğ‘¦ ğ‘¡âˆ’1+ğ‘¥ ğ‘¡.
Conversely,wealsousetheseclosureresultstoprovethatautoregressivestructuredattention(undercertainassumptions)
mustbeSSMs,allowingustoshowthatmoregeneralfamiliesofefficientsequencemodelsincludingattentionvariants
canbereducedtostatespacemodels(AppendixC.2).
C.2 AutoregressiveMaskedAttentionisSemiseparable-StructuredAttention
WeproveTheorem5.2fromSection5.2.InSection4.3wedefinedstructuredattentionasabroadgeneralizationofmasked
attention,wherethepropertyofefficiency(i.e.alinear-timeformforthekernelattention)isabstractedintotheefficiency
ofstructuredmatrixmultiplication.However,beyondcomputationalefficiency,standardlinearattention(Katharopoulos
et al. 2020) also has two important properties. First, it is causal, which is required for settings such as autoregressive
modeling. Moreover,ithasefficientautoregressivegeneration. Inotherwords,thecostofanautoregressivestepâ€“i.e.the
incrementalcostofcomputingtheoutputğ‘¦ uponseeingğ‘¥ ,giventhatğ‘¥ hasalreadybeenseenandpreprocessedâ€“
ğ‘‡ ğ‘‡ 0:ğ‘‡
requiresonlyconstanttime.
HerewecharacterizewhichinstancesofSMAhaveefficientautoregression.
IntheframeworkofSMA,causalityisequivalenttotheconstraintthatthemaskğ¿isalower-triangular matrix.
Characterizingthespaceofğ¿matricesthathaveefficientautoregressionismoredifficult.Wewilluseanarrowtechnical
definition of autoregressive processes, in the spirit of classical definitions from the time series literature (e.g. ARIMA
processes(Boxetal.2015)).
Definition C.2. We define an autoregressive transformationğ‘¥ âˆˆ Rğ‘‡ â†¦â†’ ğ‘¦ âˆˆ Rğ‘‡ of orderğ‘˜ as one where each outputğ‘¦ ğ‘¡
dependsonlyonthecurrentinputandlastğ‘˜ outputs:
ğ‘¦
ğ‘¡
=ğœ‡ ğ‘¡ğ‘¥
ğ‘¡
+â„“ ğ‘¡1ğ‘¦ ğ‘¡âˆ’1+Â·Â·Â·+â„“ ğ‘¡ğ‘˜ğ‘¦ ğ‘¡âˆ’ğ‘˜. (23)
Notethatthe casewhereğ¿ isthecumsum matrixisaspecial casewithğ‘˜ = 1 andthusğ‘¦ ğ‘¡ = ğ‘¥ ğ‘¡ +ğ‘¦ ğ‘¡1. With thisdefini-
tion, characterizingthespaceofefficientautoregressivelineartransformsfollowsfromthepropertiesofsemiseparable
matrices.TheoremC.3formalizesandprovesTheorem5.2.
TheoremC.3. Letğ¿ âˆˆRğ‘‡Ã—ğ‘‡ beanefficientautoregressivetransformationoforderğ‘˜.Thenğ¿isastatespacemodeloforder
ğ‘˜+1.
Proof. Let(ğ‘¥,ğ‘¦)beinputandoutputsequences,sothatğ‘¦ =ğ¿ğ‘¥.Rearrangingthedefinition(23),
ğ‘¦
ğ‘¡
âˆ’â„“ ğ‘¡1ğ‘¦ ğ‘¡âˆ’1âˆ’Â·Â·Â·âˆ’â„“ ğ‘¡ğ‘˜ğ‘¦
ğ‘¡âˆ’ğ‘˜
=ğœ‡ ğ‘¡ğ‘¥ ğ‘¡.
49Vectorizingoverğ‘¡,thiscanbeexpressedasamatrixtransformation
1 ğ‘¦ ğœ‡ ğ‘¥
ï£® ï£¹ ï£® 0 ï£¹ ï£® 0 ï£¹ ï£® 0 ï£¹
ï£¯ ï£¯âˆ’â„“
ğ‘¡1
1 ï£º
ï£º
ï£¯
ï£¯
ğ‘¦
1
ï£º
ï£º
ï£¯
ï£¯
ğœ‡
1
ï£º
ï£º
ï£¯
ï£¯
ğ‘¥
1
ï£º
ï£º
ï£¯ ï£¯ . . . ... ... ï£º ï£º ï£¯ ï£¯ . . . ï£º ï£º ï£¯ ï£¯ ... ï£º ï£º ï£¯ ï£¯ . . . ï£º ï£º
ï£¯ ï£¯ ï£¯âˆ’â„“
ğ‘¡ğ‘˜
... âˆ’â„“
ğ‘¡1
1 ï£º ï£º
ï£º
ï£¯ ï£¯
ï£¯
ğ‘¦
ğ‘˜
ï£º ï£º
ï£º
= ï£¯ ï£¯
ï£¯
ğœ‡
ğ‘˜
ï£º ï£º
ï£º
ï£¯ ï£¯
ï£¯
ğ‘¥
ğ‘˜
ï£º ï£º ï£º.
ï£¯ ï£¯ . . . ... . . . ... ... ï£º ï£º ï£¯ ï£¯ . . . ï£º ï£º ï£¯ ï£¯ ... ï£º ï£º ï£¯ ï£¯ . . . ï£º ï£º
ï£¯ ï£º ï£¯ ï£º ï£¯ ï£º ï£¯ ï£º
ï£¯
ï£¯
0 ... âˆ’â„“
ğ‘‡âˆ’1,ğ‘˜
... âˆ’â„“
ğ‘‡âˆ’1,1
1ï£º
ï£º
ï£¯ ï£¯ğ‘¦ ğ‘‡âˆ’1ï£º
ï£º
ï£¯
ï£¯
ğœ‡ ğ‘‡âˆ’1ï£º
ï£º
ï£¯ ï£¯ğ‘¥ ğ‘‡âˆ’1ï£º
ï£º
ï£° ï£» ï£° ï£» ï£° ï£» ï£° ï£»
Theğœ‡ diagonalmatrixcanbemovedtotheleftandfoldedintothematrixofâ„“ coefficients,whichremainsağ‘˜ +1-band
lower-triangularmatrix.Butwealsohaveğ¿âˆ’1ğ‘¦ =ğ‘¥,soğ¿istheinverseofthismatrix.
Next,notethatğ‘˜+1-bandmatricesareğ‘˜+1-semiseparablebytherankcharacterizationofsemiseparability(Definition3.1).
ByPropositionC.1,theinverseğ¿isthereforeatmostğ‘˜+2-semiseparable.Aslightlystrongerboundofğ‘˜+1canbeobtained
becauseoftheadditionalstructureofbandedmatrices.Finally,thecharacterizationofğ¿asanorder-ğ‘˜+1statespacemodel
followsfromTheorem3.5. â–¡
Inotherwords,efficientautoregressiveattentionissemiseparableSMA.
D Experimental Details
D.1 MQARDetails
WeuseaharderversionofthetaskintroducedinBased(Arora,Eyuboglu,Zhang,etal.2024)wheretokensthatarenot
query/key/values are replaced with random tokens. We also use more key-value pairs, longer sequences, and smaller
modelsizesthantheusualvariantofMQARusedbypriorwork,allofwhichmakethetaskharder.
Foreachsequencelengthğ‘‡ âˆˆ {256,512,1024},weuseğ‘‡/4key-valuepairs.Thetotalvocabsizeis8192.
Weuseaformofcurriculumtrainingwheretrainingcyclesthroughdatasetsusing(ğ‘‡/32,ğ‘‡/16,ğ‘‡/8,ğ‘‡/4)key-valuepairs,
whereeachdatasethas218 â‰ˆ250000examples,foratotalof8epochsthrougheachdataset(totalof228 â‰ˆ270ğ‘€examples).
Thetotalbatchsizeis218 â‰ˆ0.25ğ‘€ tokens(e.g.forğ‘‡ =1024,thebatchsizeis256).
Allmethodsuse2layerswithdefaultsettings;theattentionbaselineadditionallyreceivespositionalembeddings.Foreach
method,wesweepovermodeldimensionsD = {32,64,128,256}andlearningrates{10âˆ’3.5,10âˆ’2,10âˆ’2.5}. Weusealinear
decayschedulethatdropsoneveryepoch(e.g. thelastepochwouldhavealearningrate1/8ofthemaximum/starting
learningrate).
D.2 ScalingLawDetails
AllmodelsweretrainedonthePile.Forthescalinglawexperiments,weusetheGPT2tokenizer.
Model Sizes. Table 9 specifies the model sizes we use for scaling laws following GPT3 (Brown et al. 2020), First, we
changedthebatchsizeofthe1.3Bmodelfrom1Mtokensto0.5Mtokensforuniformity.Second,wechangedthenumber
of training steps and total tokens to roughly match Chinchilla scaling laws (Hoffmann et al. 2022), which specify that
trainingtokensshouldincreaseproportionallytomodelsize.
TrainingRecipes. AllmodelsusedtheAdamWoptimizerwith
â€¢ gradientclipvalue1.0
â€¢ weightdecay0.1
â€¢ nodropout
â€¢ linearlearningratewarmupwithcosinedecay
50Table9: (ScalingLawModelSizes.) Ourmodelsizesandhyperparametersforscalingexperiments. (Modeldimension
andnumberofheadsappliesonlytoTransformermodels.)
Params n_layers d_model n_heads/d_head Trainingsteps LearningRate BatchSize Tokens
125M 12 768 12/64 4800 6e-4 0.5Mtokens 2.5B
350M 24 1024 16/64 13500 3e-4 0.5Mtokens 7B
760M 24 1536 16/96 29000 2.5e-4 0.5Mtokens 15B
1.3B 24 2048 32/64 50000 2e-4 0.5Mtokens 26B
Bydefault,thepeaklearningrateistheGPT3specification.
ComparedtoGPT3recipe,weuseanâ€œimprovedrecipeâ€,inspiredbychangesadoptedbypopularlargelanguagemodels
suchasPaLM(Chowdheryetal.2023)andLLaMa(Touvron,Lavril,etal.2023).Theseinclude:
â€¢ linearlearningratewarmupwithcosinedecayto1ğ‘’âˆ’5,withapeakvalueof5Ã—theGPT3value
â€¢ nolinearbiasterms
â€¢ RMSNorminsteadofLayerNorm
â€¢ AdamWhyperparameterğ›½ = (.9,.95)(theGPT3value)insteadofthePyTorchdefaultofğ›½ = (.9,.999)
D.3 DownstreamEvaluationDetails
To evaluate downstream performance of fully trained, we train Mamba-2 on 300B tokens on the Pile, using the GPT-
NeoX(Blacketal.2022)tokenizer.
Weusethesamehyperparametersasthescalingexperiments,exceptwithbatchsize1Mforthe1.3Band2.7Bmodel.For
the2.7Bmodel,wealsofollowGPT3specification(32layers,dimension2560).
Forallmodels,weuse5xthelearningrateofthecorrespondingGPT3model.
For downstreamevaluation, we use theLM evaluation harnessfrom EleutherAI (L. Gao, Tow, et al. 2021), on the same
tasksasMamba(GuandDao2023)withoneadditionalone:
â€¢ LAMBADA(Papernoetal.2016)
â€¢ HellaSwag(Zellersetal.2019)
â€¢ PIQA(Bisketal.2020)
â€¢ ARC-challenge(P.Clarketal.2018)
â€¢ ARC-easy:aneasysubsetofARC-challenge
â€¢ WinoGrande(Sakaguchietal.2021)
â€¢ OpenBookQA(Mihaylovetal.2018)
D.4 AblationDetails
(Re)BasedDetails. OurablationsinSection9.4.3consideredtheBased(Arora,Eyuboglu,Zhang,etal.2024)andRe-
Based(Aksenovetal.2024)models.
BasedapproximatestheexpkernelwithaquadraticTaylorexpansionexp(ğ‘¥) â‰ˆ1+ğ‘¥+ğ‘¥2/2,whichcanbeaccomplished
bythefeaturemap
âˆš
ğœ“ Taylor(ğ‘¥) =concatenate(1,ğ‘¥,1/ 2ğ‘¥ âŠ—ğ‘¥).
ReBasedproposestousethesimplerfeaturemapğœ“ Quadratic(ğ‘¥) =ğ‘¥âŠ—ğ‘¥ correspondingtothekerneltransformationğ‘¥2,but
alsoappliesalayernormalizationbeforehand.Weviewthelayernormalizationasanalternativenon-linearactivationto
ourdefaultSwishactivation,andablatecombinationsofthese.
51Table10:(Zero-shotEvaluations.)Bestresultsforeachsizeinbold,secondbestunlined.WecompareagainstopensourceLMswith
varioustokenizers,trainedforupto300Btokens.Pilereferstothevalidationsplit,comparingonlyagainstmodelstrainedonthesame
datasetandtokenizer(GPT-NeoX-20B).Foreachmodelsize,Mamba-2outperformsMamba,andgenerallymatchesPythiaattwicethe
modelsize.
Model Token. Pile LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande OpenbookQA Average
pplâ†“ pplâ†“ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘ accâ†‘
HybridH3-130M GPT2 â€” 89.48 25.8 31.7 64.2 44.4 24.2 50.6 27.0 38.2
Pythia-160M NeoX 29.64 38.10 33.0 30.2 61.4 43.2 24.1 51.9 29.2 39.0
Mamba-130M NeoX 10.56 16.07 44.3 35.2 64.5 48.0 24.2 51.9 28.8 42.4
Mamba-2-130M NeoX 10.48 16.86 43.9 35.3 64.9 47.4 24.2 52.1 30.6 42.6
HybridH3-360M GPT2 â€” 12.58 48.0 41.5 68.1 51.4 24.7 54.1 31.6 45.6
Pythia-410M NeoX 9.95 10.84 51.4 40.6 66.9 52.1 24.6 53.8 30.0 45.6
Mamba-370M NeoX 8.28 8.14 55.6 46.5 69.5 55.1 28.0 55.3 30.8 48.7
Mamba-2-370M NeoX 8.21 8.02 55.8 46.9 70.5 54.9 26.9 55.7 32.4 49.0
Pythia-1B NeoX 7.82 7.92 56.1 47.2 70.7 57.0 27.1 53.5 31.4 49.0
Mamba-790M NeoX 7.33 6.02 62.7 55.1 72.1 61.2 29.5 56.1 34.2 53.0
Mamba-2-780M NeoX 7.26 5.86 61.7 54.9 72.0 61.0 28.5 60.2 36.2 53.5
GPT-Neo1.3B GPT2 â€” 7.50 57.2 48.9 71.1 56.2 25.9 54.9 33.6 49.7
HybridH3-1.3B GPT2 â€” 11.25 49.6 52.6 71.3 59.2 28.1 56.9 34.4 50.3
OPT-1.3B OPT â€” 6.64 58.0 53.7 72.4 56.7 29.6 59.5 33.2 51.9
Pythia-1.4B NeoX 7.51 6.08 61.7 52.1 71.0 60.5 28.5 57.2 30.8 51.7
RWKV4-1.5B NeoX 7.70 7.04 56.4 52.5 72.4 60.5 29.4 54.6 34.0 51.4
Mamba-1.4B NeoX 6.80 5.04 65.0 59.1 74.2 65.5 32.8 61.5 36.4 56.4
Mamba-2-1.3B NeoX 6.66 5.02 65.7 59.9 73.2 64.3 33.3 60.9 37.8 56.4
GPT-Neo2.7B GPT2 â€” 5.63 62.2 55.8 72.1 61.1 30.2 57.6 33.2 53.2
HybridH3-2.7B GPT2 â€” 7.92 55.7 59.7 73.3 65.6 32.3 61.4 33.6 54.5
OPT-2.7B OPT â€” 5.12 63.6 60.6 74.8 60.8 31.3 61.0 35.2 55.3
Pythia-2.8B NeoX 6.73 5.04 64.7 59.3 74.0 64.1 32.9 59.7 35.2 55.7
RWKV4-3B NeoX 7.00 5.24 63.9 59.6 73.7 67.8 33.1 59.6 37.0 56.4
Mamba-2.8B NeoX 6.22 4.23 69.2 66.1 75.2 69.7 36.3 63.5 39.6 59.9
Mamba-2-2.7B NeoX 6.09 4.10 69.7 66.6 76.4 69.6 36.4 64.0 38.8 60.2
GPT-J-6B GPT2 â€“ 4.10 68.3 66.3 75.4 67.0 36.6 64.1 38.2 59.4
OPT-6.7B OPT â€“ 4.25 67.7 67.2 76.3 65.6 34.9 65.5 37.4 59.2
Pythia-6.9B NeoX 6.51 4.45 67.1 64.0 75.2 67.3 35.5 61.3 38.0 58.3
RWKV4-7.4B NeoX 6.31 4.38 67.2 65.5 76.1 67.8 37.5 61.0 40.2 59.3
Notethatbecausetheseexpandthefeaturedimension,wemustprojecttosmallerğµ,ğ¶ dimensions;inTable7,usestate
sizeğ‘ =64for130Mmodelsandğ‘ =256for380Mmodels.Forthe(Re)Basedmethods,weprojectto8and16dimensions
respectivelybeforeapplyingtheirfeaturemaps;thisresultsinatotalstatesizeof82 =64forReBasedand1+8+82 =73
forBasedinthe130Mmodelcase.Becausetheğµandğ¶ projectionsaresmaller,thesemethodsusefewerparameters,and
weadjustthelayercountappropriately.
52