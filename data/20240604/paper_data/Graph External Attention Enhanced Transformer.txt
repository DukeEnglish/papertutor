Graph External Attention Enhanced Transformer
JianqingLiang1 MinChen1 JiyeLiang1
Abstract focusesonGraphNeuralNetworks(GNNs). Amilestone
exampleisGCN(Defferrardetal.,2016;Kipf&Welling,
TheTransformerarchitecturehasrecentlygained
2017). It performs convolution operations on the graph.
considerable attention in the field of graph rep-
Basedontheframeworkofmessage-passingGNNs(Gilmer
resentation learning, as it naturally overcomes
et al., 2017), GraphSage (Hamilton et al., 2017), Gat-
several limitations of Graph Neural Networks
edGCN (Bresson & Laurent, 2017) and GIN (Xu et al.,
(GNNs)withcustomizedattentionmechanisms
2019)adapttocomplexgraphdatabyemployingdifferent
or positional and structural encodings. Despite
message-passingstrategies. Whilemessage-passingGNNs
making some progress, existing works tend to
haverecentlyemergedasprominentmethodsforgraphrep-
overlookexternalinformationofgraphs,specifi-
resentation learning, there still exist some critical limita-
callythecorrelationbetweengraphs. Intuitively,
tions,includingthelimitedexpressiveness(Xuetal.,2019;
graphswithsimilarstructuresshouldhavesimilar
Morrisetal.,2019),over-smoothing(Lietal.,2018;Chen
representations. Therefore,weproposeGraphEx-
etal.,2020;Oono&Suzuki,2020),over-squashing(Alon
ternalAttention(GEA)â€”anovelattentionmech-
&Yahav,2021)andpoorlong-rangedependencies.
anismthatleveragesmultipleexternalnode/edge
key-value units to capture inter-graph correla- Instead of aggregating local neighborhood, Graph Trans-
tions implicitly. On this basis, we design an formers(GTs)captureinteractioninformationbetweenany
effective architecture called Graph External At- pair of nodes through a single self-attention layer. Some
tentionEnhancedTransformer(GEAET),which oftheexistingworksfocusoncustomizingspecificatten-
integrates local structure and global interaction tionmechanismsorpositionalencodings(Dwivedi&Bres-
informationformorecomprehensivegraphrepre- son, 2020; Ying et al., 2021; Kreuzer et al., 2021; Hus-
sentations. Extensiveexperimentsonbenchmark sain et al., 2022; Ma et al., 2023), while others combine
datasetsdemonstratethatGEAETachievesstate- message-passingGNNstodesignhybridarchitectures(Wu
of-the-art empirical performance. The source et al., 2021; Chen et al., 2022; RampaÂ´sË‡ek et al., 2022).
codeisavailableforreproducibilityat: https: Thesemethodsenablenodestointeractwithallothernodes
//github.com/icm1018/GEAET. within a graph, facilitating the direct modeling of long-
range relations. This may address typical issues such as
over-smoothinginGNNs.
1.Introduction While the above-mentioned methods have achieved im-
pressiveresults,theyareconfinedtointernalinformation
Graphrepresentationlearninghasattractedwidespreadat-
withinthegraph,neglectingpotentialcorrelationswithother
tention in the past few years. It plays a crucial role in
graphs. Infact,strongcorrelationsbetweendifferentgraphs
variousapplications,suchassocialnetworkanalysis(Pal
generally exist in numerous practical scenarios, such as
etal.,2020),drugdiscovery(Gaudeletetal.,2021b),protein
moleculargraphdata. Figure1shows3moleculargraphs
design(Ingrahametal.,2019),medicaldiagnosis(Lietal.,
withabenzeneringstructure. Intuitively,exploitinginter-
2020b)andsoon.
graphcorrelationscanimprovetheeffectivenessofgraph
Early research in graph representation learning primarily representationlearning.
1Key Laboratory of Computational Intelligence and Chi- In this work, we address the critical question of how to
nese Information Processing of Ministry of Education, School incorporateexternalinformationintographrepresentation
of Computer and Information Technology, Shanxi University, learning. Ourprincipalcontributionistointroduceanovel
Taiyuan030006,Shanxi,China. Correspondenceto: JiyeLiang
GraphExternalAttention(GEA)mechanism,whichimplic-
<ljy@sxu.edu.cn>.
itly learns inter-graph correlations with the external key-
Proceedings of the 41st International Conference on Machine valueunits. Moreover,wedesignGraphExternalAttention
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by Enhanced Transformer (GEAET), combining inter-graph
theauthor(s).
1
4202
nuJ
3
]GL.sc[
2v16012.5042:viXraGraphExternalAttentionEnhancedTransformer
receptive field. Over-smoothing happens when all node
O representations converge to a constant after deep layers,
O
N whereasover-squashingoccurswhenmessagesfromdistant
NH
nodesfailtopropagateeffectively.Itiscrucialtodesignnew
O
O
architecturesbeyondneighborhoodaggregationtoaddress
theseissues.
HO
OH OH
Graph Transformers. The Transformer with self-
O O
O attention, a dominant approach in natural language pro-
cessing (Vaswani et al., 2017; Devlin et al., 2018), has
shown competitiveness in computer vision (Dosovitskiy
Figure1.ThreemoleculargraphsfromtheZINCdatasetarecorre-
etal.,2021). GiventheremarkableachievementsofTrans-
latedtothebenzeneringstructure.
formersandtheircapabilitytoaddresscrucialchallengesof
GNNs,GTshavebeenproposed,attractingincreasingatten-
correlationswithbothlocalstructureandglobalinteraction tion. Existingworksprimarilyfocusondesigningtailored
information. Thisenablestheacquisitionofmorecompre- attentionmechanismsorpositionalandstructuralencodings,
hensivegraphrepresentations,incontrasttomostexisting orcombiningmessage-passingGNNs,enablingmodelsto
methods. Ourcontributionsarelistedasfollows. capturecomplexstructures.
Anumberofworksembedtopologyinformationintograph
â€¢ WeintroduceGEAtoimplicitlylearncorrelationsbe-
nodes by designing tailored attention mechanisms or po-
tweenallgraphs. Thecomplexityscaleslinearlywith
sitionalandstructuralencodingswithoutmessage-passing
thenumberofnodesandedges.
GNNs. GT(Dwivedi&Bresson,2020)isthepioneering
â€¢ WeproposeGEAET,whichusesGEAtolearnexternal work of GTs by integrating Laplacian positional encod-
informationandintegrateslocalstructureandglobalin- ing. In the following years, a series of works spring up.
teractioninformation,resultinginmorecomprehensive SAN (Kreuzer et al., 2021) incorporates both sparse and
graphrepresentations. globalattentionmechanismsineachlayer,utilizingLapla-
cianpositionalencodingsforthenodes. Graphormer(Ying
â€¢ WedemonstratethatGEAETachievesstate-of-the-art
etal.,2021)attainsstate-of-the-artperformanceingraph-
performance. Furthermore, we highlight the signifi-
levelpredictiontaskswithcentralityencoding,spatialen-
canceofGEA,emphasizingitssuperiorinterpretabil-
coding and edge encoding. EGT (Hussain et al., 2022)
ity and reduced dependence on positional encoding
introducestheedgechannelintoattentionmechanismsand
comparedtoself-attention.
adoptsanSVD-basedpositionalencodinginsteadofLapla-
cianpositionalencoding. Whileself-attentioniscommonly
2.RelatedWork constrained by quadratic complexity, our proposed GEA
demonstrates a linear computational complexity with re-
Message-Passing Graph Neural Networks. Early de- specttoboththenumberofnodesandedges.
velopments include GCN (Defferrard et al., 2016; Kipf
Furthermore, some works introduce hybrid architectures
& Welling, 2017), GraphSage (Hamilton et al., 2017),
incorporatingmessage-passingGNNs. Forinstance,Graph-
GIN(Xuetal.,2019),GAT(VelicË‡kovicÂ´ etal.,2018),Gat-
Trans(Wuetal.,2021)utilizesastackofGNNlayersbe-
edGCN(Bresson&Laurent,2017)andothers. Thesemeth-
foreestablishingfullconnectivityattentionwithinthegraph.
ods are based on a message-passing architecture (Gilmer
Focusingonkernelmethods,SAT(Chenetal.,2022)intro-
et al., 2017) that generally faces the challenges of lim-
ducesastructure-awareattentionmechanismusingGNNsto
ited expressivity. Recent advancements include various
extractasubgraphrepresentationrootedateachnodebefore
worksattemptingtoenhanceGNNstoimproveexpressivity.
computingtheattention. Arecentbreakthroughemerged
Examplesincludesomeworksthataddfeaturestodistin-
withtheintroductionofGraphGPS(RampaÂ´sË‡eketal.,2022),
guish nodes (Murphy et al., 2019; Sato et al., 2021; Qiu
thefirstparallelframeworkthatcombineslocalmessage-
etal.,2018;Bouritsasetal.,2023;Dwivedietal.,2022a).
passingandaglobalattentionmechanismwithvariousposi-
Othersfocusonalteringthemessage-passingrule(Beaini
tionalandstructuralencodings. Whilethesemethodshave
etal.,2021)ormodifyingtheunderlyinggraphstructurefor
achievedcompetitiveperformance,theyoverlookexternal
message-passing(Morrisetal.,2019;Bodnaretal.,2021)
informationinthegraph. Therefore,toalleviatethisissue,
tofurtherexploitthegraphstructure.
wedesignGEAET,whichinheritsthemeritsoftheGEA
Despiteachievingstate-of-the-artperformance,GNNsface network,message-passingGNNandTransformer,leverages
over-smoothingandover-squashingduetotheirconstrained inter-graphcorrelations,localstructureandglobalinterac-
2GraphExternalAttentionEnhancedTransformer
Ã— ğ» Ã— ğ»
Q
U U
nk nv
K U ğ‘ 
U U
ek ev
V
(a)Transformerwithself-attention (b)Graphexternalattentionnetwork
Figure2. Transformerversusgraphexternalattentionnetwork.Forsimplicity,weomitskipconnectionsandFFNs.
tioninformation. Thus,inspiredby(Guoetal.,2022),weintroduceanovel
methodcalledGEA,asshowninFigure2b. Itcalculates
attentionbetweenthenodefeaturesoftheinputgraphand
3.Method
theexternalunits,using:
Inthefollowing,wedenoteagraphasG=(V,E),whereV
representsthesetofnodesandE representstheedges. The A GE =norm(XUT)âˆˆRnÃ—S,
(2)
graphhasn = |V|nodesandm = |E|edges. Wedenote GE-Attn(X)=A UâˆˆRnÃ—d,
GE
thenodefeaturesforanodei âˆˆ V asx andthefeatures
i
foranedgebetweennodesiandj ase . Allnodefeatures whereUâˆˆRSÃ—disalearnableparameterindependentof
i,j
and edge features are stored in matrices X âˆˆ RnÃ—d and theinputgraph,itcanbeviewedasanexternalunitwith
EâˆˆRmÃ—d,respectively. S nodes,servingassharedmemoryforallinputgraphs. In
self-attention,A representsthesimilaritiesbetweenthe
Self
3.1.GraphExternalAttention nodesoftheinputgraph,whileinGEA,A GE denotesthe
similaritiesbetweenthenodesoftheinputgraphandthe
WefirstrevisittheTransformer,asillustratedinFigure2a.
external unit. Considering the sensitivity of the attention
Transformerconsistsoftwoblocks: aself-attentionmod-
matrix to the scale of input features, we apply a double-
ule and a feed-forward network (FFN). Specifically, self-
normalizationtechnique(Guoetal.,2021)onA . The
GE
attention regards the graph as a fully connected graph
double-normalizationprocessnormalizesbothcolumnsand
and computes the attention of each node to every other
rowsseparately,itcanbeexpressedas:
node. WiththeinputnodefeaturesX,self-attentionlinearly
projects the input into 3 matrices: a query matrix (Q), a Î±Ëœ =(XUT) ,
i,j i,j
keymatrix(K)andavaluematrix(V),whereQ=XW Q, (cid:88)n
Î±Ë† =exp(Î±Ëœ )/ exp(Î±Ëœ ),
K = XW andV = XW . Thenself-attentioncanbe i,j i,j k,j (3)
K V k=0
formulatedas: (cid:88)S
Î± =Î±Ë† / Î±Ë† .
QKT i,j i,j k=0 i,k
A =softmax(âˆš )âˆˆRnÃ—n,
Self
d out (1) Inpracticalapplications,forboostingthecapabilityofthe
Self-Attn(X)=A VâˆˆRnÃ—dout, network,weutilizetwodistinctexternalunitsforthekey
Self
andvalue. Furthermore,toleveragetheedgeinformation
where W ,W ,W are trainable parameters and d
Q K V out withintheinputgraph,weemployadditionalexternalunits
denotesthedimensionofQ.Theoutputoftheself-attention
foredgefeaturesandasharedunittostoretheconnections
isfollowedbybothaskipconnectionandaFFN.
betweenedgesandnodes:
Self-attention on a graph can be viewed as employing a
X =norm(XU UT )U ,
linearcombinationofnodefeatureswithinasinglegraphto out s nk nv
(4)
refinenodefeatures. However,itexclusivelyfocusesonthe E =norm(EU UT )U ,
out s ek ev
correlationsamongnodeswithinasinglegraph,overlook-
ingimplicitconnectionsbetweennodesindifferentgraphs, whereU âˆˆRdÃ—disasharedunittostoretheconnections
s
whichmaypotentiallylimititscapacityandadaptability. betweenedgesandnodes;U ,U âˆˆRSÃ—dareexternal
nk nv
3
X
Softmax
Xğ‘œğ‘¢ğ‘¡
X
E
Norm
Norm
Xğ‘œğ‘¢ğ‘¡
Eğ‘œğ‘¢ğ‘¡GraphExternalAttentionEnhancedTransformer
Ã— ğ¿
Transformer
Node embedding
Message-passingGNN
Graphexternal
Edge embedding
attentionnetwork
Figure3.OverallarchitectureofGEAET.ItconsistsofagraphembeddinglayerandLfeatureextractionlayers.Thegraphembedding
layertransformsgraphdataintonodeembeddingsXandedgeembeddingsE.Itcomputespositionalencodings,whichareaddedtothe
nodeembeddingsasinputstothefeatureextractionlayers.Eachfeatureextractionlayerconsistsofagraphexternalattentionnetwork,a
message-passingGNNandaTransformertoextractinter-graphcorrelations,localstructuresandglobalinteractioninformation.Finally,
thisinformationisintegratedusingafeed-forwardnetwork(FFN)andthenemployedontheoutputembeddingsforvariousgraphtasks.
key-value units for nodes, while U ek,U
ev
âˆˆ RSÃ—d are whereW x0 âˆˆ RdÃ—dÎ±,W e0 âˆˆ RdÃ—dÎ² andu0,v0 âˆˆ Rd are
externalkey-valueunitsforedges. learnableparameters. Then,weusepositionalencodingto
enhancetheinputnodefeatures:
WithintheTransformerarchitecture,self-attentioniscom-
putedacrossvariousinputchannelsinmultipleinstances, x0 =T0p +xËœ0, (7)
i i i
atechniquereferredtoasmulti-headattention. Multi-head
attentioncancapturediversenoderelations,enhancingthe where T0 âˆˆ RdÃ—k is a learnable matrix and p i âˆˆ Rk is
abilityoftheattentionmechanism. Similarly,takenodesas positionalencoding. Itisnoteworthythattheadvantagesof
anexample,therelationsbetweennodeswithinthegraph differentpositionalencodingsaredependentonthedataset.
andexternalunitsarevarious. Therefore,weadoptananal-
ogousapproach,itcanbewrittenas: FeatureExtractionLayer. Ateachlayer,externalfeature
informationiscapturedbytheGEANetandthenaggregated
h i =GE-Attn(X i,U nk,U nv), withintra-graphinformationtoupdatenodefeatures. The
X =MultiHeadGEA(X,U ,U ) (5) intra-graphinformationisobtainedthroughacombination
out nk nv
ofmessage-passingGNNandTransformer. Thisprocess
=Concat(h ,...,h )W ,
1 H o
canbeformulatedas:
whereh irepresentsthei-thhead,H isthetotalnumberof Xl+1,El+1 =MPNNl(Xl,El,A),
heads,W isalineartransformationmatrix,U ,U âˆˆ M M
o nk nv
RSÃ—dserveassharedmemoryunitsfordifferentheads. Fi- Xl+1 =TLayerl(Xl), (8)
T
nally,theoutputoftheGEAisfollowedbyaskipconnection Xl+1,El+1 =GEANetl(Xl,El+1),
G G M
formingaGraphExternalAttentionNetwork(GEANet).
whereGEANetreferstothegraphexternalattentionnet-
work introduced in Section 3.1, TLayer represents the
3.2.GraphExternalAttentionEnhancedTransformer
Transformerlayerwithself-attention,AâˆˆRnÃ—nisthead-
Figure 3 illustrates an overview of the proposed GEAET jacencymatrix,MPNNisaninstanceofamessage-passing
framework. GEAET consists of two components: graph GNNtoupdatenodeandedgerepresentationsasfollows:
embeddingandfeatureextractionlayers.
xl+1 =f (xl,{xl |j âˆˆN(i)},el ),
i node i j i,j
(9)
GraphEmbedding. Foreachinputgraph,weinitiallyper- el+1 =f (xl,xl,el ),
formalinearprojectionoftheinputnodefeaturesÎ±
i
âˆˆRdÎ± i,j edge i j i,j
andedgefeaturesÎ²
i,j
âˆˆ RdÎ², resultingind-dimensional where xl i+1,xl i,el i+ ,j1,el
i,j
âˆˆ Rd, l is the layer index, i,j
hiddenfeatures: denotes the node index, N(i) is the neighborhood of the
i-thnodeandthefunctionsf andf withlearnable
xËœ0 =W0Î± +u0 âˆˆRd, node edge
i x i (6) parametersdefineanyarbitrarymessage-passingGNNar-
e0 =W0Î² +v0 âˆˆRd, chitecture(Kipf&Welling,2017;Bresson&Laurent,2017;
ij e i,j
4
Graph
embedding
Feed-forward
networkGraphExternalAttentionEnhancedTransformer
Table1. ComparisonofGEAETwithbaselineson6datasets.Bestresultsarecolored:first,second,third.
Model CIFAR10 MNIST PATTERN Peptides-Struct PascalVOC-SP COCO-SP
Accuracy(%)â†‘ Accuracy(%)â†‘ Accuracy(%)â†‘ MAEâ†“ F1scoreâ†‘ F1scoreâ†‘
GCN(Kipf&Welling,2017) 55.710Â±0.381 90.705Â±0.218 71.892Â±0.334 0.3496Â±0.0013 0.1268Â±0.0060 0.0841Â±0.0010
GINE(Xuetal.,2019) â€“ â€“ â€“ 0.3547Â±0.0045 0.1265Â±0.0076 0.1339Â±0.0044
GIN(Xuetal.,2019) 55.255Â±1.527 96.485Â±0.252 85.387Â±0.136 â€“ â€“ â€“
GAT(VelicË‡kovicÂ´etal.,2018) 64.223Â±0.455 95.535Â±0.205 78.271Â±0.186 â€“ â€“ â€“
GatedGCN(Bresson&Laurent,2017) 67.312Â±0.311 97.340Â±0.143 85.568Â±0.088 0.3357Â±0.0006 0.2873Â±0.0219 0.2641Â±0.0045
PNA(Corsoetal.,2020) 70.350Â±0.630 97.940Â±0.120 â€“ â€“ â€“ â€“
DGN(Beainietal.,2021) 72.838Â±0.417 â€“ 86.680Â±0.034 â€“ â€“ â€“
DRew(Gutteridgeetal.,2023) â€“ â€“ â€“ 0.2536Â±0.0015 0.3314Â±0.0024 â€“
CRaWl(Toenshoffetal.,2021) 69.013Â±0.259 97.944Â±0.050 â€“ â€“ â€“ â€“
GIN-AK+(Zhaoetal.,2022) 72.190Â±0.130 â€“ 86.850Â±0.057 â€“ â€“ â€“
SAN(Kreuzeretal.,2021) â€“ â€“ 86.581Â±0.037 0.2545Â±0.0012 0.3230Â±0.0039 0.2592Â±0.0158
K-SubgraphSAT(Chenetal.,2022) â€“ â€“ 86.848Â±0.037 â€“ â€“ â€“
EGT(Hussainetal.,2022) 68.702Â±0.409 98.173Â±0.087 86.821Â±0.020 â€“ â€“ â€“
GraphGPS(RampaÂ´sË‡eketal.,2022) 72.298Â±0.356 98.051Â±0.126 86.685Â±0.059 0.2500Â±0.0005 0.3748Â±0.0109 0.3412Â±0.0044
LGI-GT(Yin&Zhong,2023) â€“ â€“ 86.930Â±0.040 â€“ â€“ â€“
GPTrans-Nano(Gutteridgeetal.,2023) â€“ â€“ 86.731Â±0.085 â€“ â€“ â€“
Graph-ViT/MLPMixer(Heetal.,2023) 73.960Â±0.330 98.460Â±0.090 â€“ 0.2449Â±0.0016 â€“ â€“
GRIT(Maetal.,2023) 76.468Â±0.881 98.108Â±0.111 87.196Â±0.076 0.2460Â±0.0012 â€“ â€“
Exphormer(Shirzadetal.,2023) 74.754Â±0.194 98.414Â±0.038 86.734Â±0.008 0.2481Â±0.0007 0.3966Â±0.0027 0.3430Â±0.0008
GEAET(ours) 76.634Â±0.427 98.513Â±0.086 86.993Â±0.026 0.2445Â±0.0013 0.4585Â±0.0087 0.3895Â±0.0050
Hamilton et al., 2017; VelicË‡kovicÂ´ et al., 2018; Xu et al., ing experiments. Finally, we conduct ablation studies on
2019;Huetal.,2020). eachcomponentofGEANet, includingtheexternalnode
unit,theexternaledgeunitandthesharedunit,toconfirm
Finally,weemployanFFNblocktoaggregatenodeinforma-
theeffectivenessofeachcomponent. Moredetailsonthe
tiontoobtainthenoderepresentationsXl+1. Additionally,
experimentalsetupandhyperparametersareprovidedinthe
weemployEl+1astheedgefeaturesforthel+1-thlayer:
G AppendixB,additionalresultsaregivenintheAppendixC.
Xl+1 =FFNl(Xl+1+Xl+1+Xl+1),
G T M (10) Insummary,ourexperimentsrevealthat(a)GEAETarchi-
El+1 =El+1, tectureoutperformsexistingstate-of-the-artmethodsonvar-
G
iousdatasets,(b)GEANetcanbeseamlesslyintegratedwith
whereXl+1,Xl+1 âˆˆRnÃ—daretheoutputsofl-layerTrans- somebasicGNNs,significantlyenhancingtheperformance,
T M
former and message-passing GNN, Xl+1,El+1 âˆˆ RnÃ—d (c)GEANetshowsbetterinterpretabilitythanTransformer
G G
aretheoutputsofl-layerGEANet. and(d)GEANetislessdependentonpositionalencoding.
SeeAppendixDforthecomplexityanalysisofGEAET.
4.1.ComparisonwithSOTAs
4.Experiments We compare our methods with several recent SOTA
graphTransformers, includingExphormer, GRIT,Graph-
Inthissection, weevaluatetheempiricalperformanceof ViT/MLPMixer, and numerous popular graph representa-
GEANetandGEAETonavarietyofgraphdatasetswith tionlearningmodels,suchaswell-knownmessage-passing
graphpredictionandnodepredictiontasks, includingCI- GNNs (GCN, GIN, GINE, GAT, GatedGCN, PNA), and
FAR10, MNIST, PATTERN, CLUSTER and ZINC from graphTransformers(SAN,SAT,EGT,GraphGPS,LGI-GT,
Benchmarking GNNs (Dwivedi et al., 2020), as well as GPTrans-Nano). Additionally, we consider other recent
PascalVOC-SP, COCO-SP, Petides-Struct, Petides-Func methods with SOTA performance, such as DGN, DRew,
andPCQM-ContactfromLongRangeGraphBenchmark CRaW1andGIN-AK+.
(LRGB; Dwivedi et al., 2022b), and the TreeNeighbour-
As shown in Table 1, for the 3 tasks from Benchmark-
Matchdataset(Alon&Yahav,2021). Detailedinformation
ing GNNs (Dwivedi et al., 2020), we observe that our
isprovidedintheAppendixA.
GEAETachievesSOTAresultsonCIFAR10andMNIST
Wefirstcompareourmainarchitecture,GEAET,withthe andrankssecondonthePATTERNdataset. Forthe3tasks
latest state-of-the-art models. In addition, we integrate onLRGB(Dwivedietal.,2022b),GEAETachievesthebest
GEANetwiththemessage-passingGNNsandcompareit results. ItisnoteworthythattheGEAETachievesF1scores
withthecorrespondingnetworktodemonstratetheroleof of0.4585onPascalVOC-SPand0.3895onCOCO-SP,sur-
GEANet. Furthermore, we conduct a series of compara- passingothermodelsbyasignificantgap.
tiveexperimentswithTransformer,includingvisualization
experimentsonattentioninmoleculargraphs,experiments
varyingthenumberofattentionheadsandpositionalencod-
5GraphExternalAttentionEnhancedTransformer
Table2.Comparisonoftheclassicmessage-passingGNNbaselineswiththeirvariantsaugmentedbyGEANeton5benchmarks.
Model PascalVOC-SP COCO-SP Peptides-Struct Peptides-Func PCQM-Contact
F1scoreâ†‘ F1scoreâ†‘ MAEâ†“ APâ†‘ MRRâ†‘
GCN 0.1268Â±0.0060 0.0841Â±0.0010 0.3496Â±0.0013 0.5930Â±0.0023 0.3234Â±0.0006
+GEANet 0.2250Â±0.0103 0.2096Â±0.0041 0.2512Â±0.0003 0.6722Â±0.0065 0.3244Â±0.0007
GINE 0.1265Â±0.0076 0.1339Â±0.0044 0.3547Â±0.0045 0.5498Â±0.0079 0.3180Â±0.0027
+GEANet 0.2742Â±0.0032 0.2410Â±0.0028 0.2544Â±0.0012 0.6509Â±0.0021 0.3276Â±0.0012
GatedGCN 0.2873Â±0.0219 0.2641Â±0.0045 0.3420Â±0.0013 0.5864Â±0.0077 0.3242Â±0.0008
+GEANet 0.3933Â±0.0027 0.3219Â±0.0052 0.2547Â±0.0009 0.6485Â±0.0035 0.3321Â±0.0008
Table3.Comparisonoftheclassicmessage-passingGNNbaselineswiththeirvariantsaugmentedbyGEANeton5benchmarks.
Model PATTERN CLUSTER MNIST CIFAR10 ZINC
Accuracy(%)â†‘ Accuracy(%)â†‘ Accuracy(%)â†‘ Accuracy(%)â†‘ MAEâ†“
GCN 71.892Â±0.334 68.498Â±0.976 90.705Â±0.218 55.710Â±0.381 0.367Â±0.011
+GEANet 85.323Â±0.128 74.015Â±0.124 96.465Â±0.054 61.925Â±0.271 0.240Â±0.008
GIN 85.387Â±0.136 64.716Â±1.553 96.485Â±0.252 55.255Â±1.527 0.526Â±0.051
+GEANet 85.527Â±0.015 66.370Â±2.145 96.845Â±0.097 62.320Â±0.221 0.193Â±0.001
GatedGCN 85.568Â±0.088 73.840Â±0.326 97.340Â±0.143 67.312Â±0.311 0.282Â±0.015
+GEANet 85.607Â±0.038 77.013Â±0.224 98.315Â±0.097 73.857Â±0.306 0.218Â±0.011
4.2.ComparisonwithGNNs turaldistributionoftheoriginalmoleculargraphs, which
promotestopredicttherestrictedsolubilitymoreaccurately.
To clearly demonstrate the performance improvement of
Incontrast,Transformerdoesnotutilizeinter-graphcorre-
GEANetongraphrepresentationlearningmodels,weinte-
lations,resultinginpoorerpredictiveperformance. More
grateGEANetwithsomecommonlyusedmessage-passing
resultsareprovidedintheAppendixE.
GNNs,providingthemodelswiththeabilitytolearngraph
externalinformation.Inourcomparison,weevaluateourap-
C O N S
proachagainstthecorrespondingGNNs,withGNNresults
sourcedfromDwivedietal.(2020)orDwivedietal.(2022b).
Tomaintainafaircomparison,ourtrainedmodelsstrictlyad- molecule GEANet Transformer
0.07
heretotheparameterconstraintswithoutincorporatingany 0.030
0.06
positionalencoding,consistentwithDwivedietal.(2020) 0.05 0.025
and Dwivedi et al. (2022b). As depicted in Table 2 and 0.04 0.020
0.03 0.015
Table3,GEANetsignificantlyimprovestheperformance
0.02 0.010
ofallbasemessage-passingGNNs,includingGCN(Kipf
0.01 0.005
&Welling,2017),GatedGCN(Bresson&Laurent,2017), molecule GE-Attn 0.00 Self-Attn 0.000
GIN(Xuetal.,2019)andGINE(Huetal.,2020),onvarious 0.10
0.04
datasetssimplybycombiningtheoutputofGEANetwith 0.08
theoutputoftheGNN.Thisisachievedwithoutanyaddi- 0.03
0.06
tionalmodifications,validatingthatGEANetcaneffectively 0.02
0.04
alleviateissuesinmessage-passingGNNs.
0.02 0.01
0.00 0.00
4.3.ComparisonwithSelf-Attention
Figure4.AttentionvisualizationofGEANetandTransformeron
AttentionInterpretation. Tobetterexplaintheattention
ZINC molecular graphs. The left column shows two original
mechanism,werespectivelytrainaGEANetandaTrans-
moleculargraphs,whilethemiddleandrightcolumnsshowthe
former on the ZINC dataset and visualize the attention visualizationresultsofattentionscoreswithGEANetandTrans-
scoresinFigure4. Thesalientdifferencebetweenthetwo former,respectively.
modelsisthatGEANetcancapturethecorrelationbetween
graphs,andthuswecanattributethefollowinginterpretabil-
ity gains to that. While both models manage to identify ImpactofAttentionHeads. Weinvestigatetheimpact
somehydrophilicstructuresorfunctionalgroups,theatten- onthenumberofattentionheadswithtwoattentionmech-
tionscoreslearnedbyGEANetaresparserandmoreinfor- anisms. Figure5showstheMAEvalueswitheitherGCN
mative. GEANet focuses more on important atoms such +TransformerorGCN+GEANetonthePeptides-Struct
asNandO,aswellasatomsthatconnectdifferentmotifs. dataset. For a fair comparison, both models use Lapla-
TheattentiondistributionofGEANetissimilartothestruc- cianpositionalencoding,withthesamenumberoflayers
6GraphExternalAttentionEnhancedTransformer
andapproximatelytotalnumberofparameters(about500k). 0.387
GCN+Transformer
Heads=0correspondstoapureGCNnetworkwithoutatten- GCN+GEANet
tion. Theintroductionofattentionmechanismsignificantly
0.35
improvesperformance. GEANetachievesthebestperfor-
mancewith8heads. Incontrast,Transformerperformsbest
withonehead,suggestingthatmultipleself-attentionheads 0.3
0.286
donotenhanceperformance. Notably,GEANetconsistently
outperformsself-attentionacrossvariousnumbersofheads.
0.251 0.252 0.255
0.25 0.245
GCN+GEANet
GCN+Transformer
0.2
0.265
None LapPE RWPE
Typesofpositionalencoding
0.26
Figure6. TestMAEwithdifferentpositionalencodings.
0.255
0.25
7, effectively alleviating the issue of over-squashing. In
contrast,the5GNNmethodsfailtogeneralizeeffectively
0.245
fromr =4. ThisisconsistentwiththeperspectiveofAlon
0 2 4 6 8 &Yahav(2021)thatGNNsuffersfromover-squashingdue
Numberofattentionheads to a fixed-length embedding of the graph. Our method
addressesthisproblembyutilizinggraphexternalattention
Figure5.TestMAEwithdifferentnumberofattentionheads. mechanismstotransmitlong-rangeinformation.
ImpactofPositionalEncoding. Weconductanablation
1.0
studyonthePeptides-Structdatasettoassesstheimpactof
positional encoding on GEANet and Transformer. Sim- 0.9 GCN
ilar to the experiments with attention heads, we utilize 0.8 GINE
GAT
a parallel architecture consisting of an GCN block and 0.7
GIN
an attention block. Figure 6 shows the MAE values ob-
0.6 GatedGCN
tainedwithandwithoutpositionalencoding,includingRan-
0.5 GEAET
dom Walk Positional Encoding (RWPE; Li et al., 2020a;
0.4
Dwivedietal.,2022a)andLaplacianPositionalEncoding
(LapPE;Dwivedi&Bresson,2020;Kreuzeretal.,2021). 0.3
TheTransformerwithself-attentionperformspoorlywith- 0.2
out positional encoding. The utilization of LapPE and
0.1
RWPEimprovesperformancetosomeextent. Incontrast,
0.0
GEANetachievesangoodperformancewithoutpositional 2 3 4 5 6 7
encoding.ForGEANet,weobservethatLapPEcanenhance r
performance,whileRWPEdecreasesperformance. Onthe
whole,GEANetislessdependentonpositionalencoding Figure7.Test accuracy with different problem radius r on the
comparedtoTransformer. TreeNeighbourMatchdataset.
4.4.GEAETMitigatesOver-Squashing
4.5.AblationExperiments
The TreeNeighbourMatch dataset (Alon & Yahav, 2021)
isusedtoprovideanintuitionofover-squashing. Onthis Toassessthepracticalityofourmodeldesignchoices,we
dataset,eachexampleisrepresentedwithabinarytreeof conductmultipleablationexperimentsonMNISTandPAT-
depthr. Figure7showstheperformancecomparisonofthe TERN.TheresultsareshowninTable4. Wenoticethatthe
standardmessage-passingGNNswithourproposedGEAET removalofeitherexternalnodeunits,externaledgeunits,or
ontheTreeNeighbourMatchdataset. Theresultsindicate externalsharedunitsallleadstopoorerperformance,which
thatourGEAETgeneralizeswellonthedatasetuptor = demonstratesthesoundnessofourarchitecturaldecisions.
7
EAM
EAM
ycaruccAGraphExternalAttentionEnhancedTransformer
Bodnar,C.,Frasca,F.,Otter,N.,Wang,Y.,Lio`,P.,Montu-
Table4.Ablation study on MNIST and PATTERN datasets for
far,G.F.,andBronstein,M. Weisfeilerandlehmango
GEAcomponents.ThemetricistheAccuracy(%).
cellular: Cwnetworks. AdvancesinNeuralInformation
Node Edge Share MNIST PATTERN
ProcessingSystems,34:2625â€“2640,2021.
â€“ â€“ â€“ 98.274Â±0.011 86.882Â±0.028
âœ“ â€“ â€“ 98.474Â±0.013 86.956Â±0.038 Bordes,A.,Usunier,N.,Garcia-Duran,A.,Weston,J.,and
âœ“ âœ“ â€“ 98.488Â±0.082 86.959Â±0.024
âœ“ âœ“ âœ“ 98.513Â±0.086 86.993Â±0.026 Yakhnenko, O. Translating embeddings for modeling
multi-relational data. Advances in Neural Information
ProcessingSystems,26,2013.
5.Conclusion
Bouritsas,G.,Frasca,F.,Zafeiriou,S.,andBronstein,M.M.
Observingthatexistinggraphrepresentationlearningmeth- Improving graph neural network expressivity via sub-
odsareconfinedtointernalinformation,weargueforthe graphisomorphismcounting. IEEETransactionsonPat-
importance of inter-graph correlations. Drawing inspira- ternAnalysisandMachineIntelligence,45(1):657â€“668,
tionfromtheideathatgraphswithsimilarstructuresought 2023.
to have analogous representations, we propose GEA, a
Bresson,X.andLaurent,T. Residualgatedgraphconvnets.
novellightweightyeteffectiveattentionmechanismtoex-
arXivpreprintarXiv:1711.07553,2017.
ploitinter-graphcorrelations. Onthisbasis,weintroduce
GEAETtoexploitlocalstructureandglobalinteractionin- Chen, D., Lin, Y., Li, W., Li, P., Zhou, J., and Sun, X.
formation. GEAETachievesstate-of-the-artperformance Measuringandrelievingtheover-smoothingproblemfor
onvariousgraphdatasets,highlightingthesignificanceof graphneuralnetworksfromthetopologicalview. Pro-
inter-graphcorrelations. Nevertheless,GEAETisnotthe ceedings of the AAAI Conference on Artificial Intelli-
finalchapterofourwork;futureeffortswillfocusonreduc- gence,34(04):3438â€“3445,2020.
ingthehighmemorycostandtimecomplexity,aswellas
addressingthelackofupperboundsonexpressivepower. Chen,D.,Oâ€™Bray,L.,andBorgwardt,K. Structure-aware
transformerforgraphrepresentationlearning. InInterna-
tionalConferenceonMachineLearning,pp.3469â€“3489,
Acknowledgements
2022.
This work is supported by the National Science and
Corso,G.,Cavalleri,L.,Beaini,D.,Lio`,P.,andVelicË‡kovicÂ´,
Technology Major Project (2020AAA0106102) and Na-
P. Principalneighbourhoodaggregationforgraphnets.
tionalNaturalScienceFoundationofChina(No.62376142,
AdvancesinNeuralInformationProcessingSystems,33:
U21A20473, 62272285). The authors would also like to
13260â€“13271,2020.
thankDr. JunbiaoCuiforhisinsightfulopinionsduringthe
rebuttalperiod. Defferrard, M., Bresson, X., and Vandergheynst, P. Con-
volutionalneuralnetworksongraphswithfastlocalized
spectralfiltering. AdvancesinNeuralInformationPro-
ImpactStatement
cessingSystems,29,2016.
Thispaperpresentsworkwhosegoalistoadvancethefield
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert:
of Machine Learning. There are many potential societal
Pre-training of deep bidirectional transformers for lan-
consequences of our work, none which we feel must be
guageunderstanding. arXivpreprintarXiv:1810.04805,
specificallyhighlightedhere.
2018.
References Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
Abbe,E. Communitydetectionandstochasticblockmod- M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,
els: recentdevelopments. JournalofMachineLearning N. An image is worth 16x16 words: Transformers for
Research,18(177):1â€“86,2018. imagerecognitionatscale. InInternationalConference
onLearningRepresentations,2021.
Alon,U.andYahav,E. Onthebottleneckofgraphneural
networksanditspracticalimplications. InInternational Dwivedi, V. P. and Bresson, X. A generalization
ConferenceonLearningRepresentations,2021. of transformer networks to graphs. arXiv preprint
arXiv:2012.09699,2020.
Beaini,D.,Passaro,S.,LeÂ´tourneau,V.,Hamilton,W.,Corso,
G.,andLioÂ´,P. Directionalgraphnetworks. InInterna- Dwivedi,V.P.,Joshi,C.K.,Luu,A.T.,Laurent,T.,Bengio,
tional Conference on Machine Learning, pp. 748â€“758, Y.,andBresson,X.Benchmarkinggraphneuralnetworks.
2021. arXivpreprintarXiv:2003.00982,2020.
8GraphExternalAttentionEnhancedTransformer
Dwivedi,V.P.,Luu,A.T.,Laurent,T.,Bengio,Y.,andBres- Ingraham,J.,Garg,V.,Barzilay,R.,andJaakkola,T. Gener-
son,X. Graphneuralnetworkswithlearnablestructural ativemodelsforgraph-basedproteindesign. Advances
andpositionalrepresentations. InInternationalConfer- inNeuralInformationProcessingSystems,32,2019.
enceonLearningRepresentations,2022a.
Irwin,J.J.,Sterling,T.,Mysinger,M.M.,Bolstad,E.S.,and
Dwivedi,V.P.,RampaÂ´sË‡ek,L.,Galkin,M.,Parviz,A.,Wolf, Coleman,R.G.Zinc:afreetooltodiscoverchemistryfor
G.,Luu,A.T.,andBeaini,D. Longrangegraphbench- biology. JournalofChemicalInformationandModeling,
mark. AdvancesinNeuralInformationProcessingSys- 52(7):1757â€“1768,2012.
tems,35:22326â€“22340,2022b.
Kipf, T. N. and Welling, M. Semi-supervised classifica-
Everingham,M.,VanGool,L.,Williams,C.K.,Winn,J., tionwithgraphconvolutionalnetworks. InInternational
andZisserman,A. Thepascalvisualobjectclasses(voc) ConferenceonLearningRepresentations,2017.
challenge. InternationalJournalofComputerVision,88:
303â€“338,2010. Kreuzer,D.,Beaini,D.,Hamilton,W.,LeÂ´tourneau,V.,and
Tossou,P. Rethinkinggraphtransformerswithspectral
Gaudelet, T., Day, B., Jamasb, A. R., Soman, J., Regep,
attention. Advances in Neural Information Processing
C.,Liu,G.,Hayter,J.B.,Vickers,R.,Roberts,C.,Tang,
Systems,pp.21618â€“21629,2021.
J., et al. Utilizinggraph machinelearning withindrug
discoveryanddevelopment. BriefingsinBioinformatics, Li,P.,Wang,Y.,Wang,H.,andLeskovec,J. Distanceen-
22(6),2021. coding: Designprovablymorepowerfulneuralnetworks
for graph representation learning. Advances in Neural
Gilmer,J.,Schoenholz,S.S.,Riley,P.F.,Vinyals,O.,and
InformationProcessingSystems,33:4465â€“4478,2020a.
Dahl,G.E. Neuralmessagepassingforquantumchem-
istry. InInternationalConferenceonMachineLearning, Li,Q.,Han,Z.,andWu,X.-M. Deeperinsightsintograph
pp.1263â€“1272,2017. convolutionalnetworksforsemi-supervisedlearning. In
ProceedingsoftheAAAIConferenceonArtificialIntelli-
Guo,M.-H.,Cai,J.-X.,Liu,Z.-N.,Mu,T.-J.,Martin,R.R.,
gence,2018.
andHu,S.-M. Pct: Pointcloudtransformer. Computa-
tionalVisualMedia,7:187â€“199,2021.
Li, Y., Qian, B., Zhang, X., and Liu, H. Graph neural
network-baseddiagnosisprediction. BigData,8(5):379â€“
Guo,M.-H.,Liu,Z.-N.,Mu,T.-J.,andHu,S.-M. Beyond
390,2020b.
self-attention: Externalattentionusingtwolinearlayers
forvisualtasks. IEEETransactionsonPatternAnalysis
Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient
andMachineIntelligence,45(5):5436â€“5447,2022.
descentwithwarmrestarts. InInternationalConference
Gutteridge, B., Dong, X., Bronstein, M. M., and Di Gio- onLearningRepresentations,2017.
vanni, F. Drew: Dynamically rewired message pass-
Loshchilov,I.andHutter,F. Decoupledweightdecayreg-
ingwithdelay. InInternationalConferenceonMachine
ularization. In International Conference on Learning
Learning,pp.12252â€“12267,2023.
Representations,2019.
Hamilton,W.,Ying,Z.,andLeskovec,J. Inductiverepre-
Ma, L., Lin, C., Lim, D., Romero-Soriano, A., Dokania,
sentationlearningonlargegraphs. AdvancesinNeural
P. K., Coates, M., Torr, P., and Lim, S.-N. Graph in-
InformationProcessingSystems,30,2017.
ductivebiasesintransformerswithoutmessagepassing.
He, X., Hooi, B., Laurent, T., Perold, A., Lecun, Y., and In International Conference on Machine Learning, pp.
Bresson, X. A generalization of ViT/MLP-mixer to 23321â€“23337,2023.
graphs. InInternationalConferenceonMachineLearn-
ing,pp.12724â€“12745,2023. Morris,C.,Ritzert,M.,Fey,M.,Hamilton,W.L.,Lenssen,
J.E.,Rattan,G.,andGrohe,M. Weisfeilerandlemango
Hu,W.,Liu,B.,Gomes,J.,Zitnik,M.,Liang,P.,Pande,V., neural: Higher-ordergraphneuralnetworks. InProceed-
andLeskovec, J. Strategiesforpre-traininggraphneu- ingsoftheAAAIConferenceonArtificialIntelligence,pp.
ralnetworks. InInternationalConferenceonLearning 4602â€“4609,2019.
Representations,2020.
Murphy,R.,Srinivasan,B.,Rao,V.,andRibeiro,B. Rela-
Hussain,M.S.,Zaki,M.J.,andSubramanian,D. Global
tionalpoolingforgraphrepresentations. InInternational
self-attentionasareplacementforgraphconvolution. In
ConferenceonMachineLearning,pp.4663â€“4673,2019.
Proceedings of the 28th ACM SIGKDD International
ConferenceonKnowledgeDiscoveryandDataMining, Oono,K.andSuzuki,T. Graphneuralnetworksexponen-
pp.655â€“665,2022. tially lose expressive power for node classification. In
9GraphExternalAttentionEnhancedTransformer
InternationalConferenceonLearningRepresentations, Xu,K.,Hu,W.,Leskovec,J.,andJegelka,S. Howpowerful
2020. aregraphneuralnetworks? InInternationalConference
onLearningRepresentations,2019.
Pal,A.,Eksombatchai,C.,Zhou,Y.,Zhao,B.,Rosenberg,
C.,andLeskovec,J. Pinnersage: Multi-modaluserem- Yin, S. and Zhong, G. Lgi-gt: graph transformers with
beddingframeworkforrecommendationsatpinterest. In localandglobaloperatorsinterleaving. InProceedingsof
Proceedings of the 26th ACM SIGKDD International InternationalJointConferenceonArtificialIntelligence,
ConferenceonKnowledgeDiscoveryandDataMining, pp.4504â€“4512,2023.
pp.2311â€“2320,2020.
Ying,C.,Cai,T.,Luo,S.,Zheng,S.,Ke,G.,He,D.,Shen,Y.,
andLiu,T.-Y. Dotransformersreallyperformbadlyfor
Qiu, J., Dong, Y., Ma, H., Li, J., Wang, K., and Tang, J.
graphrepresentation? AdvancesinNeuralInformation
Network embedding as matrix factorization: Unifying
ProcessingSystems,34:28877â€“28888,2021.
deepwalk,line,pte,andnode2vec. InProceedingsofthe
11thACMInternationalConferenceonWebSearchand
Zhao, L., Jin, W., Akoglu, L., and Shah, N. From stars
DataMining,pp.459â€“467,2018.
to subgraphs: Uplifting any GNN with local structure
awareness. In International Conference on Learning
RampaÂ´sË‡ek,L.,Galkin,M.,Dwivedi,V.P.,Luu,A.T.,Wolf,
Representations,2022.
G.,andBeaini,D. Recipeforageneral,powerful,scal-
ablegraphtransformer. AdvancesinNeuralInformation
ProcessingSystems,35:14501â€“14515,2022.
Sato,R.,Yamada,M.,andKashima,H. Randomfeatures
strengthengraphneuralnetworks. InProceedingsofthe
2021SIAMInternationalConferenceonDataMining,pp.
333â€“341,2021.
Shirzad,H.,Velingker,A.,Venkatachalam,B.,Sutherland,
D.J., andSinop, A. K. Exphormer: Sparsetransform-
ersforgraphs. InInternationalConferenceonMachine
Learning,pp.31613â€“31632,2023.
Singh,S.,Chaudhary,K.,Dhanda,S.K.,Bhalla,S.,Usmani,
S.S.,Gautam,A.,Tuknait,A.,Agrawal,P.,Mathur,D.,
and Raghava, G. P. Satpdb: a database of structurally
annotatedtherapeuticpeptides. NucleicAcidsResearch,
44,2016.
Toenshoff, J., Ritzert, M., Wolf, H., and Grohe, M.
Graphlearningwith1dconvolutionsonrandomwalks.
arXiv:2102.08786,2021.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
L.,Gomez,A.N.,Kaiser,L.u.,andPolosukhin,I. At-
tentionisallyouneed. AdvancesinNeuralInformation
ProcessingSystems,30,2017.
VelicË‡kovicÂ´, P., Cucurull, G., Casanova, A., Romero, A.,
Lio`, P., and Bengio, Y. Graph attention networks. In
InternationalConferenceonLearningRepresentations,
2018.
Wu, Z., Jain, P., Wright, M., Mirhoseini, A., Gonzalez,
J.E.,andStoica,I. Representinglong-rangecontextfor
graphneuralnetworkswithglobalattention. Advances
in Neural Information Processing Systems, 34:13266â€“
13279,2021.
10GraphExternalAttentionEnhancedTransformer
A.DatasetDescriptions
Weevaluateourmethodondiversedatasets,including5graphbenchmarkdatasetsfromBenchmarkingGNNs,5long-range
dependencygraphdatasetsfromLRGBandtheTreeNeighbourMatchdataset. Below,weprovidedescriptionsofthedatasets
andpresentsummarystatisticsinTable5.
MNIST and CIFAR10. MNIST and CIFAR10 (Dwivedi et al., 2020) represent the graphical counterparts of their
respective image classification datasets of the same name. A graph is formed by creating an 8-nearest neighbor graph
of the SLIC superpixels of the images. These datasets pose 10-class graph classification challenges. For MNIST, the
resultinggraphshavesizesrangingfrom40to75nodes,whileforCIFAR10,thegraphsvarybetween85and150nodes.
The classification tasks involve the standard dataset splits of 55K/5K/10K for MNIST and 45K/5K/10K for CIFAR10,
correspondingtotrain/validation/testgraphs. Thesedatasetsserveassanitychecks,withanexpectationthatmostGNNs
wouldachievecloseto100%accuracyforMNISTandsatisfactoryperformanceforCIFAR10.
PATTERNandCLUSTER. PATTERNandCLUSTER(Dwivedietal.,2020)aresyntheticdatasetsderivedfromthe
StochasticBlockModel(SBM;Abbe,2018)onnodeclassificationtasks. PATTERNistodeterminewhetheranodebelongs
tooneofthe100predefinedsubgraphpatterns,whileCLUSTERistoclassifynodesinto6distinctclusterswithidentical
distributions. TheuniquefeatureofPATTERNinvolvesrecognizingnodesbelongingtorandomlygeneratedsub-graph
patterns,whileCLUSTERentailsinferringtheclusterIDforeachnodeingraphscomposedof6SBM-generatedclusters.
Weusethesplitsasisusedin(Dwivedietal.,2020).
ZINC. ZINC(Dwivedietal.,2020)isagraphregressiondatasetderivedfromasubsetofmoleculargraphs(12Koutof
250K)sourcedfromafreelyavailabledatabaseofcommerciallyaccessiblecompounds(Irwinetal.,2012). Themolecular
graphsinZINCrangefrom9to37nodes,whereeachnoderepresentsaheavyatom(with28possibleatomtypes)andeach
edgesignifiesabond(with3possibletypes). Theprimarytaskistoregressamolecularpropertyknownasconstrained
solubility. Thedatasetincludesapredefinedtrain/validation/testsplitof10K/1K/1Kinstances.
PascalVOC-SPandCOCO-SP. PascalVOC-SPandCOCO-SP(Dwivedietal.,2022b)aregraph-basedversionsofimage
datasetswithlargerimagesandinvolvethetaskofnodeclassification,specificallythesemanticsegmentationofsuperpixels.
ThesedatasetsrespectivelyderivedfromthePascalVOC2011imagedataset(Everinghametal.,2010)andtheMSCOCO
imagedatasetthroughSLICsuperpixelization,presentamoreintricatenodeclassificationchallengecomparedtoCIFAR10
andMNIST.Eachsuperpixelnodeisassociatedwithaspecificobjectclass,makingthemnodeclassificationdatasetswitha
focusonregionsofimagesbelongingtoparticularclasses.
Peptides-FuncandPeptides-Struct. Peptides-FuncandPeptides-Struct(Dwivedietal.,2022b)arederivedfrom15,535
peptideswithatotalof2.3millionnodessourcedfromSAT-Pdb(Singhetal.,2016).Thegraphsexhibitlargesizes,averaging
150.94nodespergraphandameangraphdiameterof56.99. SpecificallysuitedforbenchmarkinggraphTransformers
orexpressiveGNNscapableofcapturinglong-rangeinteractions. Peptides-funcinvolvesmulti-labelgraphclassification
into10nonexclusivepeptidefunctionalclasses,whilePeptides-structfocusesongraph-levelregressionpredicting113D
structuralpropertiesofthepeptides.
PCQM-Contact. PCQM-Contact(Dwivedietal.,2022b)isderivedfromPCQM4Mv2andcorresponding3Dmolecular
structures,wherethetaskisabinarylinkpredictiontask. Thisdatasetcontains529,434graphswithatotalof15million
nodes,whereeachgraphrepresentsamoleculargraphwithexplicithydrogens. AllgraphsinPCQM-Contactareextracted
fromthePCQM4Mtrainingset,specificallythosewithavailable3Dstructureandfilteredtoretainonlythosewithatleast
onecontact.
TreeNeighbourMatch. TreeNeighbourMatchisasyntheticdatasetintroducedbyAlon&Yahav(2021)toillustratethe
challengeofover-squashinginGNNs. Itfeaturesbinarytreesofcontrolleddepththatsimulateanexponentially-growing
receptivefieldwithaproblemradiusr. Thetaskistopredictalabelforthetargetnode,situatedinoneoftheleafnodes,
necessitatinginformationpropagationfromallleavestothetargetnode. Thissetupexposestheissueofover-squashingat
thetargetnodeduetotheneedforcomprehensivelong-rangesignalincorporationbeforelabelprediction.
11GraphExternalAttentionEnhancedTransformer
Table5. Summarystatisticsofdatasetsusedinthisstudy.
Dataset Graphs Avg.nodes Avg.edges PredictionLevel Task Metric
MNIST 70,000 70.6 564.5 graph 10-classclassif. Accuracy
CIFAR10 60,000 117.6 941.1 graph 10-classclassif. Accuracy
PATTERN 14,000 118.9 3,039.3 inductivenode binaryclassif. Accuracy
CLUSTER 12,000 117.2 2,150.9 inductivenode 6-classclassif. Accuracy
ZINC 12,000 23.2 24.9 graph regression MAE
PascalVOC-SP 11,355 479.4 2,710.5 inductivenode 21-classclassif. F1
COCO-SP 123,286 476.9 2,693.7 inductivenode 81-classclassif. F1
PCQM-Contact 529,434 30.1 61.0 inductivelink linkranking MRR
Peptides-Func 15,535 150.9 307.3 graph 10-classclassif. Avg.Precision
Peptides-Struct 15,535 150.9 307.3 graph regression MAE
TreeNeighbourMatch(r=2) 96 7 6 inductivenode 4-classclassif. Accuracy
TreeNeighbourMatch(r=3) 32,000 15 14 inductivenode 8-classclassif. Accuracy
TreeNeighbourMatch(r=4) 64,000 31 30 inductivenode 16-classclassif. Accuracy
TreeNeighbourMatch(r=5) 128,000 63 62 inductivenode 32-classclassif. Accuracy
TreeNeighbourMatch(r=6) 256,000 127 126 inductivenode 64-classclassif. Accuracy
TreeNeighbourMatch(r=7) 512,000 255 254 inductivenode 128-classclassif. Accuracy
B.HyperparameterChoicesandReproducibility
HyperparameterChoice. Inourhyperparametersearch,weattempttoadjustthenumberofheadsinGEANet,aswell
ashyperparametersrelatedtopositionalencoding,message-passingGNNtypeandTransformer. Consideringthelarge
numberofhyperparametersanddatasets,wedonotconductanexhaustivesearchorgridsearch. Forafaircomparison,we
followcommonlyusedparameterbudgets: forbenchmarkingdatasetsfromBenchmarkingGNNs(Dwivedietal.,2020),a
maximumof500kparametersforPATTERNandapproximately100kparametersforMNISTandCIFAR10;fordatasets
fromLRGB(Dwivedietal.,2022b),weadheretoaparameterbudgetof500k. SeeTable6fordetailedinformation.
Optimization. WeusetheAdamW(Loshchilov&Hutter,2019)optimizerinallourexperiments,withthedefaultsettings
of Î² = 0.9, Î² = 0.999 and Ïµ = 10âˆ’8, and use a cosine scheduler (Loshchilov & Hutter, 2017). The choice of loss
1 2
function,lengthofthewarm-upperiod,baselearningrateandtotalnumberofepochsareadjustedbasedonthedataset.
Table6. HyperparametersusedforGEAETon6datasets.
Hyperparameter CIFAR10 MNIST PATTERN Peptides-Struct PascalVOC-SP COCO-SP
Layers 5 5 7 6 8 8
HiddenDimd 40 40 64 224 68 68
MPNN GatedGCN GatedGCN GatedGCN GCN GatedGCN GatedGCN
SelfAttention Transformer Transformer Transformer None Transformer Transformer
ExternalNetwork GEANet GEANet GEANet GEANet GEANet GEANet
SelfHeads 4 4 4 None 4 4
ExternalHeads 4 4 4 8 4 4
UnitSizeS 10 10 16 28 17 17
PE ESLapPE-8 ESLapPE-8 RWPE-16 LapPE-10 None None
PEDim 8 8 7 16 None None
BatchSize 16 16 32 200 50 50
LearningRate 0.001 0.001 0.0005 0.001 0.001 0.001
NumEpochs 150 150 100 250 200 200
WarmupEpochs 5 5 5 5 10 10
WeightDecay 1e-5 1e-5 1e-5 0 0 0
NumParameters 113,235 113,155 429,052 463,211 506,213 505,661
12GraphExternalAttentionEnhancedTransformer
C.AdditionalResults
Weprovideadditionalresultshere,includingdetailedresultsfromtheattentionheadsandpositionencodingexperiments,
theresultsofGEAETonthelinkpredictiontaskofthePCQM-Contactdatasetandasubstantialnumberofadditional
ablationstudies.
ImpactofAttentionHeads. WeconductexperimentswithdifferentnumbersofattentionheadsonthePeptides-Struct
andPeptides-Funcdatasets. WeadoptaframeworkwherethebaseGNNandtheattentionblock(TransformerorGEANet)
operateinparallel,withtheoutputofthemodelateachlayerbeingthesumoftheoutputsfromtheGNNandtheattention
block. Toensurefairness,weusethesamenumberoflayers,applyLaplacianpositionalencoding(LapPE),useGCNasthe
baseGNNandkeepthetotalnumberofparameterstoabout500k. ThedetailedresultsareshowninTable7,whereall
resultsareaveragedover4differentrandomseeds. Weobservethathavingmultipleattentionheadsdoesnotsignificantly
improveTransformerforbothdatasets. However,thereisanotableimprovementinGEANet,withthelowestMAEachieved
with8headsonPeptides-StructandthebestAPachievedwith8headsonPeptides-Func.
Table7. TheresultsofTransformerandGEANetwithdifferentnumberofattentionheads.
Model #Layers Positional #Heads #Parameters Peptides-Struct Peptides-Func
Encoding MAEâ†“ APâ†‘
GCN+Transformer 6 LapPE 2 490,571 0.2516Â±0.0031 0.6644Â±0.0052
GCN+Transformer 6 LapPE 3 490,571 0.2529Â±0.0012 0.6688Â±0.0072
GCN+Transformer 6 LapPE 4 490,571 0.2524Â±0.0017 0.6634Â±0.0033
GCN+Transformer 6 LapPE 6 490,571 0.2557Â±0.0032 0.6630Â±0.0085
GCN+Transformer 6 LapPE 8 490,571 0.2544Â±0.0037 0.6593Â±0.0060
GCN+GEANet 6 LapPE 2 626,219 0.2474Â±0.0006 0.6828Â±0.0059
GCN+GEANet 6 LapPE 3 539,123 0.2461Â±0.0006 0.6890Â±0.0060
GCN+GEANet 6 LapPE 4 508,571 0.2455Â±0.0009 0.6892Â±0.0042
GCN+GEANet 6 LapPE 6 486,683 0.2470Â±0.0025 0.6880Â±0.0025
GCN+GEANet 6 LapPE 8 463,211 0.2445Â±0.0013 0.6912Â±0.0012
ImpactofPositionalEncoding. Similartotheexperimentsonattentionheads,westudytheimpactofdifferentpositional
encodingsonattention. Table8showstheresultsaveragedover4differentrandomseeds. WefindthatGEANethasalower
dependencyonpositionalencodingcomparedtoTransformerwithself-attention.
Table8. TheresultsofTransformerandGEANetwithdifferentpositionalencodings.
Model #Layers Positional #Heads #Parameters Peptides-Struct Peptides-Func
Encoding MAEâ†“ APâ†‘
GCN+Transformer 6 None 4 492,731 0.3871Â±0.0094 0.6404Â±0.0095
GCN+Transformer 6 RWPE 4 490,143 0.2858Â±0.0044 0.6564Â±0.0122
GCN+Transformer 6 LapPE 4 490,571 0.2524Â±0.0017 0.6589Â±0.0069
GCN+GEANet 6 None 4 510,731 0.2512Â±0.0003 0.6722Â±0.0065
GCN+GEANet 6 RWPE 4 508,143 0.2546Â±0.0018 0.6794Â±0.0089
GCN+GEANet 6 LapPE 4 508,571 0.2445Â±0.0013 0.6892Â±0.0042
GEAETinLinkPredictionTask. Inthelinkpredictiontask,weevaluatecommonrankingmetricsfromtheknowledge
graphlinkpredictionliterature(Bordesetal.,2013)asshowninTable9: Hits@1,Hits@3,Hits@10andMeanReciprocal
Rank(MRR),whereHits@kindicateswhethertheactualanswerisamongthetop-kpredictionsprovidedbythemodel.
ImprovingGNNwithGEANet. TodemonstratetheimportanceofGEANet,weconductadditionalexperimentsonthe
Peptides-Struct,Peptides-Func,andPascalVOC-SPdatasets. AsshowninTable10,comparewithpositionalencoding,
GEANetsignificantlyimprovestheperformanceofallbasemessage-passingGNNs.
D.ComplexityAnalysis
WefirstanalyzethecomplexityofGEANet.AsthemodeldimensionsdandthesizeofexternalunitsSarehyper-parameters,
GEANetscaleslinearlywiththenumberofnodesandedges,resultinginacomplexityofO(|V|+|E|). ForGEAET,the
complexityisprimarilydeterminedbyGEANet,Transformerandmessage-passingGNN.TheGEANet,asdescribedabove,
haslinearcomplexity. Themessage-passingGNNhasacomplexityofO(|E|). Intypicalcases,theTransformerusesthe
13GraphExternalAttentionEnhancedTransformer
Table9.PerformanceofGEAETonthelinkpredictiontaskofthePCQM-Contactdataset.Weselectbaselinemodelsthatalsoreportthe
Hits@1,Hits@3,Hits@10,andMRRmetrics.Ourresultsareaveragedover4runswith4differentseeds,whiletheresultsofthebaseline
modelsareeitherfrom(Dwivedietal.,2022b)ortheoriginalpapers.
Model TestHits@1â†‘ TestHits@3â†‘ TestHits@10â†‘ TestMRRâ†‘
SAN 0.1312Â±0.0016 0.4030Â±0.0008 0.8550Â±0.0024 0.3341Â±0.0006
GatedGCN 0.1288Â±0.0013 0.3808Â±0.0006 0.8517Â±0.0005 0.3242Â±0.0008
Transformer 0.1221Â±0.0011 0.3679Â±0.0033 0.8517Â±0.0039 0.3174Â±0.0020
GCN 0.1321Â±0.0007 0.3791Â±0.0004 0.8256Â±0.0006 0.3234Â±0.0006
GINE 0.1337Â±0.0013 0.3642Â±0.0043 0.8147Â±0.0062 0.3180Â±0.0027
GraphDiffuser 0.1369Â±0.0012 0.4053Â±0.0011 0.8592Â±0.0007 0.3388Â±0.0011
GEAET(ours) 0.1566Â±0.0014 0.4227Â±0.0022 0.8626Â±0.0032 0.3518Â±0.0011
Table10.ImprovingGNNperformancewithGEANet.Weruntheexperimentswith4differentseedsandaveragetheresults.
Model Positional PascalVOC-SP Peptides-Struct Peptides-Func
Encoding F1scoreâ†‘ MAEâ†“ APâ†‘
GCN None 0.1268Â±0.0060 0.3496Â±0.0013 0.5930Â±0.0023
GCN+GEANet None 0.2250Â±0.0103 0.2512Â±0.0003 0.6722Â±0.0065
GCN+GEANet LapPE 0.2353Â±0.0070 0.2445Â±0.0013 0.6892Â±0.0042
GCN+GEANet RWPE 0.2325Â±0.0165 0.2546Â±0.0018 0.6794Â±0.0089
GINE None 0.1265Â±0.0076 0.3547Â±0.0045 0.5498Â±0.0079
GINE+GEANet None 0.2742Â±0.0032 0.2544Â±0.0012 0.6509Â±0.0021
GINE+GEANet LapPE 0.2746Â±0.0071 0.2480Â±0.0023 0.6654Â±0.0055
GINE+GEANet RWPE 0.2762Â±0.0022 0.2546Â±0.0011 0.6618Â±0.0059
GatedGCN None 0.2873Â±0.0219 0.3420Â±0.0013 0.5864Â±0.0077
GatedGCN+GEANet None 0.3933Â±0.0027 0.2547Â±0.0009 0.6485Â±0.0035
GatedGCN+GEANet LapPE 0.3944Â±0.0044 0.2468Â±0.0014 0.6715Â±0.0034
GatedGCN+GEANet RWPE 0.3899Â±0.0017 0.2577Â±0.0006 0.6734Â±0.0028
self-attentionmechanismwithacomplexityofO(|V|2),resultingincomplexityofO(|V|2). Inpractice,weobservethaton
certaindatasetssuchasPeptides-StructandPeptides-Func,notusingTransformeryieldsbetterresults,achievinglinear
complexityinsuchcases. Additionally,wecanuselinearTransformerstoreducethecomplexityofGEAETtolinearity.
E.ModelInterpretation
Inadditiontotheexamplespresentedinthemainpaper,weprovideadditionalvisualizationresultsinFigure8. GEANet
andTransformerusethesamepositionalencoding,andotherhyperparametersettingsaregenerallyconsistent. Thefirst
columnshowstheoriginalmoleculesfromZINC,themiddleandrightcolumnsshowthevisualizationresultsofGEANet
and Transformer, respectively. We observe that GEANet focuses more on the important nodes or connected nodes of
specificstructures,whichimprovestheabilitytodistinguishdifferentgraphsormotifs. ThisindicatesthatGEANetexcels
inhandlingstructuralinformationandconcentratesondiscriminativenodes.
14GraphExternalAttentionEnhancedTransformer
C O N S F Cl
molecule GEANet Transformer molecule GEANet Transformer
0.07 0.030 0.04
0.06 0.08
0.025
0.05 0.03
0.04 0.020 0.06
0.03 0.015 0.04 0.02
0.02 0.010
0.02 0.01
0.01 0.005
molecule GE-Attn 0.00 Self-Attn 0.000 molecule GE-Attn 0.00 Self-Attn 0.00
0.10 0.10 0.04
0.04
0.08 0.08
0.03
0.03
0.06 0.06
0.02 0.02
0.04 0.04
0.02 0.01 0.02 0.01
molecule GE-Attn 0.00 Self-Attn 0.00 molecule GE-Attn 0.00 Self-Attn 0.00
0.10
0.10 0.05 0.04
0.08
0.08 0.04 0.03
0.06
0.06 0.03
0.04 0.02
0.04 0.02
0.02 0.01 0.02 0.01
molecule GE-Attn 0.00 Self-Attn 0.00 molecule GE-Attn 0.00 Self-Attn 0.00
0.035
0.12 0.030 0.10
0.030
0.10 0.025 0.08 0.025
0.08 0.020 0.06 0.020
0.06 0.015 0.015
0.04
0.04 0.010 0.010
0.02 0.005 0.02 0.005
0.00 0.000 0.00 0.000
Figure8.AttentionvisualizationofGEANetandTransformeronZINCmoleculargraphs.Thecentercolumnshowstheattentionweights
ofGEANetandtherightcolumnshowstheattentionweightslearnedbytheclassicTransformer.
15