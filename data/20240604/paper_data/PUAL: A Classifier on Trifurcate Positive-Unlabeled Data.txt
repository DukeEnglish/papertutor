PUAL: A Classifier on Trifurcate
Positive-Unlabeled Data
Xiaoke Wang1, Xiaochen Yang2, Rui Zhu3*, Jing-Hao Xue1
1Department of Statistical Science, University College London, Gower
Street, London, WC1E 6BT, England, UK.
2School of Mathematics & Statistics, University of Glasgow, University
Place, Glasgow, G12 8QQ, Scotland, UK.
3*Bayes Business School, City, University of London, 106 Bunhill Row,
London, EC1Y 8TZ, England, UK.
*Corresponding author(s). E-mail(s): rui.zhu@city.ac.uk;
Contributing authors: xiaoke.wang.18@ucl.ac.uk;
xiaochen.yang@glasgow.ac.uk; jinghao.xue@ucl.ac.uk;
Abstract: Positive-unlabeled (PU) learning aims to train a classifier using the data
containing only labeled-positive instances and unlabeled instances. However, existing
PU learning methods are generally hard to achieve satisfactory performance on tri-
furcate data, where the positive instances distribute on both sides of the negative
instances. To address this issue, firstly we propose a PU classifier with asymmetric
loss (PUAL), by introducing a structure of asymmetric loss on positive instances into
the objective function of the global and local learning classifier. Then we develop a
kernel-based algorithm to enable PUAL to obtain non-linear decision boundary. We
show that, through experiments on both simulated and real-world datasets, PUAL
can achieve satisfactory classification on trifurcate data.
1 Introduction
PU learning is to train a classifier from PU data, which only contain labeled-positive
instances and unlabeled instances, i.e., the PU data lack labeled-negative instances
for training. Recently, there are more and more PU data occurring in practice,
such as deceptive review detection [1], text categorization [2] and remote sensing
classification [3, 4].
1
4202
yaM
13
]LM.tats[
1v07902.5042:viXraThe main difficulty in PU learning is the lack of labeled-negative instances. A
natural way to deal with this issue is topick the instances highly likely to be negative
from the unlabeled set and treat them as negative; then the obtained dataset will
containlabeled-positive,labeled-negativeandunlabeledinstances;andthusfinallyPU
learningcanbeconvertedtoclassicalsemi-supervisedlearning.Themethodsfollowing
thisideaareoftenreferredtoastwo-stepmethods[5â€“12],whichcanalsobegeneralized
to have more steps of further iterative training [13â€“17].
However, the accuracy of such a multi-step method relies heavily on the accuracy
of the algorithm applied in the first step to pick reliable negative instances [18]. This
drawback motivated studies to train a PU classifier in a single step, termed one-step
methods,whichcanbefurthercategorizedintoinconsistentPUlearningmethodsand
consistentPUlearningmethods,dependingoniftheobjectivefunctionisaconsistent
estimator of an expected loss to classify an unknown instance frorm the population.
A pioneer consistent PU learning method is the unbiased PU learning (uPU) [19].
Subsequently, the non-negative PU learning (nnPU) [20] was proposed by taking the
absolute value of the estimated average loss on the negative set in the objective func-
tion of uPU for better convergence of the classifier training. Furthermore, imbalanced
nnPU (imbalancednnPU) [21] was proposed to address imbalanced PU training set.
Nevertheless, the training of the consistent methods often required more information
out of the PU dataset, e.g., the class prior or the distribution of population.
Early attempt of the inconsistent PU learning methods was the biased support
vector machine (BSVM) [22] based on the classic supervised support vector machine
(SVM)[23],assigningahighweighttotheaveragelossofthelabeled-positiveinstances
andalowweighttotheaveragelossoftheunlabeledinstancesintheobjectivefunction.
Subsequently, weighted unlabeled samples SVM (WUS-SVM) [24] was proposed to
assign a distinct weight to each of the unlabeled instances according to the likelihood
of this unlabeled instance to be negative. Then, the biased least squares SVM (BLS-
SVM) was proposed in [25]. It replaces the hinge loss with the squared loss in the
objectiveofBSVMincasethattoomuchimportanceisgiventotheunlabeled-positive
instancestreatedasnegativeduringthetrainingofclassifier.Thenthelocalconstraint
wasintroducedtotheobjectivefunctionofBLS-SVMin[26],encouragingtheinstances
to be classified into the same class as its neighborhoods and the proposed method
is called the global and local learning classifier (GLLC). Moreover, the large-margin
label-calibrated SVM (LLSVM) [27] was proposed to further alleviate the bias by
introducingthehatlosstotheobjectivefunction,wherethehatlossmeasuresthegap
between the inverse tangent of the predictive score function (without intercept) and
a certain threshold.
However, for trifurcate data where the positive set is roughly constituted by two
subsetsdistributingonbothsidesofnegativeinstances,existingPUlearningmethods
areusuallyhardtoachievesatisfactoryperformance.AnexampleofthetrifurcatePU
datasets is illustrated in Fig. 1 by the 2-dimensional projection with the t-distributed
stochastic neighbor embedding (t-SNE) [28] of dataset Wireless Indoor Localization
(wifi) [29].
A classifier with non-linear decision boundary is needed for the classification of
trifurcate datasets. However, when we apply kernel trick to obtain the non-linear
2negative
unlabeledâˆ’positive
labledâˆ’positive
âˆ’0.5 0.0 0.5 1.0
tâˆ’SNE_x1
Fig.1:The2-dimensionalprojectionwitht-SNEofdatasetwifi,wherethepositiveset
is roughly constituted by two subsets distributing on both sides of negative instances;
the perplexity for the training of t-SNE on wifi was set to 750.
decision boundary, the original trifurcate datasets may be converted to follow the
pattern illustrated in Fig. 2, where the distances from the two positive subsets to the
idealdecisionboundary,asindicatedbyasolidblueline,areverydifferent.Inthiscase,
usingthesquaredloss(e.g.thatusedinGLLC)willimposequadraticpenaltynotonly
on the instances wrongly classified but also on all the instances correctly classified.
Therefore,thelabeled-positiveinstancescorrectlyclassifiedby,butfarawayfrom,the
ideal decision boundary, as circled by the dashed lines in Fig. 2, can unfortunately
generatelargepenaltyviathesquaredlossandhencedragtheidealdecisionboundary
towards the labeled-positive instances far away. This leads to the estimated decision
boundary indicated in green solid line, which misclassifies many more instances than
the ideal decision boundary.
The aim of applying the squared loss in the objective function of GLLC was to
ensuretheimportancegiventotheunlabeled-positiveinstancestobenegativesupport
vectors to be lower than the importance given to unlabeled-negative instances to be
negativesupportvectors.Hencewenotethatitisnotnecessarytoalsousethesquared
loss on the labeled-positive set. Furthermore, we note that, as the hinge loss does
not penalise the instances lying on the correct side of the margin, if the hinge loss is
appliedtothelabeled-positiveset,theinstancescorrectlyclassifiedbutfarawayfrom
the ideal decision boundary will not generate any wrong penalty.
Therefore,motivatedbytheaboveanalysis,weproposedaPUclassifierwithasym-
metricloss(PUAL)onpositiveinstancesforbetterclassificationonthetrifurcatePU
datasets. The contribution of this paper can be summarized as follows.
3
2x_ENSâˆ’t
5.0
0.0
5.0âˆ’Labeled-positive instance
Unlabeled-positive instance
Unlabeled-negative instance
Ideal Decision boundary
Decision boundary of GLLC
ð‘¥
2
ð‘¥
1
Fig. 2: A pattern of the linearly separable space constructed from the original trifur-
catePUdatasetsviathekerneltrick;x andx representthemappingsofthefeatures.
1 2
Firstly,inSection3.1,weproposethemethodologyandalgorithmofPUAL,which
cangeneratealineardecisionboundarymuchclosertotheidealdecisionboundaryfor
thedatasetsfollowingthepatterninFig.2butfirstintheoriginalfeaturespace.This
is achieved by using the hinge loss for the labeled-positive instances and the squared
loss for the unlabeled-instances (and thus the unlabeled-positive instances).
Secondly,wedevelopakernel-basedalgorithmtoenablePUALtoobtainnon-linear
decision boundary in the original feature space, as detailed in Section 3.2.
Thirdly, we conduct experiments on both simulated and real-world datasets to
clearly verify the motivation and effectiveness of our proposed methods, as presented
in Section 4.
2 Related Work
Firstly we provide some general definitions involved in the following related works.
Suppose there are n labeled-positive instances and n unlabeled instances with m
p u
features.LetfeaturematrixX
[pu]
=(x 1,...,x np,...,x np+nu)T âˆˆR(np+nu)Ã—m,where
the column vector x âˆˆ RmÃ—1 denotes the vector of the features of the ith instance.
i
Similarly, matrix X
[p]
= (x 1,...,x np)T âˆˆ RnpÃ—m denotes the feature matrix of the
labeled-positive set while matrix X
[u]
= (x np+1,...,x np+nu)T âˆˆ RnuÃ—m denotes the
feature matrix of the unlabeled set. The methods listed in this section are all aimed
to train the following predictive score function for classification:
f =xTÎ²+Î² , (1)
0
4where Î² =(Î² ,Î² ,...,Î² )T âˆˆRmÃ—1 and Î² are the model parameters.
1 2 m 0
2.1 BSVM
InordertoenableSVMtohandlePUclassification,BSVM[22]treatsalltheunlabeled
instances as negative and assigns the loss of labeled-positive instances and the loss
of unlabeled instances with different weights in the objective function. BSVM trains
classifiers by solving the following objective function:
Î»
min Î²TÎ²+C 1T[1 âˆ’(X Î²+1 Î² )]
Î²,Î²02 p p p [p] p 0 + (2)
+C 1T[1 +(X Î²+1 Î² )] ,
u u u [u] u 0 +
whereC ,C ,andÎ»arepositivehyper-parameters,[g(Â·)] indicatesthecolumnvector
p u +
of the maximum between each element of g(Â·) and 0, and 1 = (1,1,Â·Â·Â· ,1)T,k =
p,u
(cid:124) (cid:123)(cid:122) (cid:125)
k
n ,n .
p u
2.2 BLS-SVM
One weakness of BSVM is that sometimes the hinge loss in the objective function
of BSVM in Equation (2) selects more unlabeled-positive instances than unlabeled-
negative instances tobe the support vectors for thenegativeclass, which constructs a
decision boundary tending to misclassify the unlabeled-positive instances as negative.
This is more likely to happen when there are many unlabeled-positive instances close
to the unlabeled negative instances.
To deal with this issue, BLS-SVM was proposed by [25] to force all training
instances to contribute to the construction of the decision boundary of the trained
SVM by solving the following optimization problem:
Î»
min Î²TÎ²+C [1 âˆ’(X Î²+1 Î² )]T[1 âˆ’(X Î²+1 Î² )]
Î²,Î²02 p p [p] p 0 p [p] p 0 (3)
+C [1 +(X Î²+1 Î² )]T[1 +(X Î²+1 Î² )],
u u [u] u 0 u [u] u 0
where the squared loss replaces the hinge loss in BSVM on both labeled-positive
set and unlabeled set. The objective function of BLS-SVM makes all the instances
contribute to the construction of the decision boundary hence the importance given
to the unlabeled-positive instances treated as negative is restricted [30].
2.3 GLLC
The similarities between a training instance and its neighbors can also be treated as
a factor for classification, the idea of which is named local learning [31]. It is noted
that the gap between PU learning and classical supervised learning on accuracy can
be mitigated via GLLC [26], which is a combination of BLS-SVM and local learning.
5The objective function of GLLC is given as
Î»
min Î²TÎ²+C [1 âˆ’(X Î²+1 Î² )]T[1 âˆ’(X Î²+1 Î² )]
Î²,Î²02 p p [p] p 0 p [p] p 0
(4)
+C [1 +(X Î²+1 Î² )]T[1 +(X Î²+1 Î² )]
u u [u] u 0 u [u] u 0
+(X Î²+1 Î² )TR(X Î²+1 Î² ),
[pu] pu 0 [pu] pu 0
where R is the similarity matrix for the instances and their neighbors, which can be
calculated as follows.
Firstly we need to calculate matrix W by
(cid:40) exp(cid:0) âˆ’Ïƒâˆ’1(x âˆ’x )T(x âˆ’x )(cid:1) if the ith and jth instances are KNN of each other,
w = i j i j
ij
0 otherwise,
(5)
where Ïƒ is a hyper-parameter to be selected. Then defining 1 = (1,1,Â·Â·Â· ,1)T,k =
pu
(cid:124) (cid:123)(cid:122) (cid:125)
k
n +n andlettingw denotetheithcolumnofmatrixW andWâˆ— denoteadiagonal
p u Â·i
matrix with the ith diagonal element equal to 1T w , one can obtain
[pu] Â·i
1
R= (Wâˆ—âˆ’W). (6)
(n +n )
p u
3 Methodology
3.1 PUAL with Linear Decision Boundary
3.1.1 Objective Function
As discussed in Section 1, we propose to apply the hinge loss to the labeled-positive
instances,soastoimposenopenaltytotheinstancescorrectlyclassifiedbutfaraway
from the ideal decision boundary. Meanwhile, the squared loss needs to be applied to
the unlabeled instances in case that too much importance is given to the unlabeled-
positive instances treated as negative during the training of the classifier. Hence we
propose a structure of asymmetric loss on positive instances to revise GLLC, so that
the new method PUAL is able to address the issue of classification of trifurcate PU
data. Moreover, a local constraint term of similarity is helpful for PU classification.
Therefore, the unconstrained optimization problem of PUAL can be formulated as
Î»
min Î²TÎ²+C 1T[1 âˆ’(X Î²+1 Î² )]
Î²,Î²02 p p p [p] p 0 +
(7)
+C [1 +(X Î²+1 Î² )]T[1 +(X Î²+1 Î² )]
u u [u] u 0 u [u] u 0
+(X Î²+1 Î² )TR(X Î²+1 Î² ).
[pu] pu 0 [pu] pu 0
However, the hinge loss term 1T[1 âˆ’(X Î²+1 Î² )] in the objective function
p p [p] p 0 +
of PUAL in Equation (7) is not always differentiable in the feasible region of the
6optimization, bringing difficulty to applying the gradient descent directly. To find an
alternative way for solving PUAL, the following reformulation of Equation (7) can be
considered:
min C 1T[h] +C (1 +X Î²+1 Î² )T(1 +X Î²+1 Î² )
p p + u u [u] u 0 u [u] u 0
Î²,Î²0,h
+(X Î²+1 Î² )TR(X Î²+1 Î² )+ Î» Î²TÎ² (8)
[pu] pu 0 [pu] pu 0 2
s.t. h=1 âˆ’(X Î²+1 Î² ).
p [p] p 0
ThepredictivescorefunctionofPUALforinstancexisthesameasthepredictive
score function of SVM in Equation (1).
3.1.2 Parameter Estimation
The convex objective function in Equation (8) can be regarded as the sum of the
functions of (Î²,Î² ) and the function of h while the constraints in Equation (8) can
0
be regarded as a linear combination of (Î²,Î² ) and h; this meets the requirement of
0
ADMM[32,33],whichcandecomposealarge-scaleconvexoptimizationproblemwith
affineconstraintsintoseveralsimplersub-problemsandupdatethesolutioniteratively
until convergence. Moreover, ADMM is able to converge to modest accuracy within
fewer iterations than the gradient descent. Hence, we adopt it here.
The Lagrangian function of problem in Equation (8) is
L(Î¸)=C 1T[h] +C (1 +X Î²+1 Î² )T(1 +X Î²+1 Î² )
p p + u u [u] u 0 u [u] u 0
Î»
+(X Î²+1 Î² )TR(X Î²+1 Î² )+ Î²TÎ² (9)
[pu] pu 0 [pu] pu 0 2
+uT[1 âˆ’(X Î²+1 Î² )âˆ’h],
h p [p] p 0
where Î¸ = {Î²,Î² ,h,u } and u is dual variable. Then the augmented Lagrangian
0 h h
function is given as
L a(Î¸)=L(Î¸)+ Âµ 21 (cid:13) (cid:13)1 pâˆ’(X [p]Î²+1 pÎ² 0)âˆ’h(cid:13) (cid:13)2 2, (10)
where Âµ is the step-size coefficient of the augmented Lagrangian function.
1
It follows that the update of Î² and Î² is
0
Î»
(Î²(k+1),Î²(k+1))=argmin Î²TÎ²
0 Î²,Î²0 2
+C (1 +X Î²+1 Î² )T(1 +X Î²+1 Î² )
u u [u] u 0 u [u] u 0
+(X Î²+1 Î² )TR(X Î²+1 Î² ) (11)
[pu] pu 0 [pu] pu 0
+u (k)T [1 âˆ’(X Î²+1 Î² )âˆ’h(k)]
h p [p] p 0
Âµ (cid:13) (cid:13)2
+ 1 (cid:13)1 âˆ’(X Î²+1 Î² )âˆ’h(k)(cid:13) ,
2 (cid:13) p [p] p 0 (cid:13) 2
7whichisaquadraticoptimizationwitheverytermdifferentiable.LetI ,âˆ€k âˆˆZ,denote
k
a kÃ—k identity matrix. By defining
M =Î»I +2C XT X +2XT RX +Âµ XT X ,
11 m u [u] [u] [pu] [pu] 1 [p] [p]
M =2C XT 1 +2XT R1 +Âµ XT 1 ,
12 u [u] u [pu] pu 1 [p] p
M =2C 1TX +21T RX +Âµ 1TX ,
21 u u [u] pu [pu] 1 p [p]
(12)
M =2C n +21T R1 +Âµ n ,
22 u u pu pu 1 p
m =âˆ’2C XT 1 +XT u +Âµ XT (1 âˆ’h),
1 u [u] u [p] h 1 [p] p
m =âˆ’2C n +uT1 +Âµ (1 âˆ’h)T1 ,
2 u u h p 1 p p
the solution of problem in Equation (11) can be obtained by solving the following
linear equation w.r.t. Î² and Î² :
0
(cid:20)
M M
(cid:21)(cid:20) Î²(k+1)(cid:21) (cid:20)
m
(cid:21)
11 12 = 1 . (13)
M 21 M 22 Î² 0(k+1) m 2
Then the update of h becomes
h(k+1) =argminC 1T[h] +u(k)T [1 âˆ’(X Î²(k+1)+1 Î²(k+1))âˆ’h]
p p + h p [p] p 0
h
(14)
Âµ (cid:13) (cid:13)2
+ 1 (cid:13)1 âˆ’(X Î²(k+1)+1 Î²(k+1))âˆ’h(cid:13) ,
2 (cid:13) p [p] p 0 (cid:13) 2
which is equivalent to solving the problem
min(cid:88)np (cid:40) C
p[h ] +
1
[1+
u( hk i)
âˆ’(xTÎ²(k+1)+Î²(k+1))âˆ’h
]2(cid:41)
. (15)
h Âµ 1 i + 2 Âµ 1 i 0 i
i=1
According to [34], for constant c>0, we can obtain
ï£±
dâˆ’c d>c,
1
ï£´ï£²
argminc[x] + âˆ¥xâˆ’dâˆ¥2 = 0 0â‰¤dâ‰¤c, (16)
x + 2 2 ï£´ï£³d
d<0.
Thus, by defining s (d) = argminc[x] + 1âˆ¥xâˆ’dâˆ¥2, the ith element of h(k+1) in
c x + 2 2
problem in Equation (14) is solved as
(cid:32) (cid:33)
u(k)
h(k+1) =s 1+ hi âˆ’(xTÎ²(k+1)+Î²(k+1)) ,i=1,...,n . (17)
i Cp Âµ i 0 p
Âµ1 1
8According to [33], the update of the dual parameter u can be
h
u(k+1) =u(k)+Âµ [1 âˆ’(X Î²(k+1)+1 Î²(k+1))âˆ’h(k+1)]. (18)
h h 1 p [p] p 0
3.1.3 Algorithm
The algorithm of PUAL can be summarized in Algorithm 1.
Algorithm 1 PUAL with linear decision boundary
Input: PU dataset, C , C , Î», Ïƒ and Âµ
p u 1
Output: Î² and Î²
0
1: Initialize Î², Î² 0, h, u h
2: while not converged do
3: Update (Î²(k+1),Î² 0(k+1))=argminL a(Î²,Î² 0,h(k),u( hk))
Î²,Î²0
4: Update h(k+1) =argminL a(Î²(k+1),Î² 0(k+1),h,u( hk))
h
5: Update u( hk+1) =u( hk)+Âµ 1[1 pâˆ’(X [p]Î²(k+1)+1 pÎ² 0(k+1))âˆ’h(k+1)]
6: end while
3.2 PUAL with Non-Linear Decision Boundary
In this section, we develop a kernel-based algorithm to enable PUAL to have non-
linear decision boundary, so that PUAL can be applied on the non-linear separable
datasets including trifurcate PU datasets. The techniques applied in this section are
similar to many previous methods [26, 35â€“37].
3.2.1 Objective Function
Suppose Ï•(x) âˆˆ RrÃ—1 be a mapping of the instance vector x. Then let Ï•(X ) âˆˆ
[k]
RnkÃ—r,k = p,u,pu be the mapping of the original data matrix X [k]. The ith row of
Ï•(X ) is Ï•(x )T. According to Equations (12) and (13), using Ï•(X ) as features
[k] i [k]
matrix instead of X for the training of PUAL, we can find the following necessary
[k]
condition for the optimal solution of Î²:
BÎ² =Ï•(X )Tâ„¦, (19)
[pu]
where
M M
B =M âˆ’ 12 21, (20)
11 M
22
and
â„¦ =(cid:20) u h âˆ’âˆ’ 2Âµ C1 uMm 12 u2 21 âˆ’p 2+ MmÂµ
22
21 C(1 up 1âˆ’
u
h)(cid:21) âˆ’2 Mm 22 2R1 pu. (21)
9Equation (19) can be regarded as a condition when the objective function reaches
its minimum. For n ,n > m, B is symmetric and invertible. In this case, we can
p u
obtain
Î² =Bâˆ’1Ï•(X )Tâ„¦. (22)
[pu]
Substituting Equation (22) into the objective function in Equation (7), we have the
new objective function as
Î»
min â„¦TÏ•(X )Bâˆ’1Bâˆ’1Ï•(X )Tâ„¦+C 1T[1 âˆ’(Ï•(X )Bâˆ’1Ï•(X )Tâ„¦+1 Î² )]
â„¦,Î²02 [pu] [pu] p p p [p] [pu] p 0 +
+C [1 +Ï•(X )Bâˆ’1Ï•(X )Tâ„¦+1 Î² ][1 +Ï•(X )Bâˆ’1Ï•(X )Tâ„¦+1 Î² ]T
u u [u] [pu] u 0 u [u] [pu] u 0
+(Ï•(X )Bâˆ’1Ï•(X )Tâ„¦+1 Î² )TR(Ï•(X )Bâˆ’1Ï•(X )Tâ„¦+1 Î² ).
[pu] [pu] pu 0 [pu] [pu] pu 0
(23)
To prove Ï•(X )Bâˆ’1Ï•(X )T and Ï•(X )Bâˆ’1Bâˆ’1Ï•(X )T in Equation (23)
[k] [pu] [k] [pu]
arekernelmatricesforX andX ,weneedtointroducethetwoclosureproperties
[k] [pu]
for the construction of kernel functions proved in [38] as follows.
Theorem 1. Let Ï•(X),Ï•(Z) be a mapping of matrices of X,Z and Îº (Ï•(X),Ï•(Z))
1
be a kernel matrix of Ï•(X) and Ï•(Z). Then the following two matrices Îº (X,Z) and
2
Îº (X,Z) can be regarded as the kernel matrix w.r.t. X,Z:
3
â€¢ Îº (X,Z)=Îº (Ï•(X),Ï•(Z)),
2 1
â€¢ Îº (X,Z)=XFZT, with F to be a symmetric matrix.
3
Then, according to the closure properties in Theorem 1, we can obtain
Ï•(X )Bâˆ’1Ï•(X )T =Î¦â€²(Ï•(X ),Ï•(X ))
[k] [pu] [k] [pu]
(24)
=Î¦(X ,X )
[k] [pu]
and
Ï•(X )Bâˆ’1Bâˆ’1Ï•(X )T =Î¦â€²â€²(Ï•(X ),Ï•(X ))
[k] [pu] [k] [pu]
(25)
=Î¦ (X ,X ),
2 [k] [pu]
where Î¦â€²(Ï•(X ),Ï•(X )), Î¦â€²â€²(Ï•(X ),Ï•(X )) are the kernel matrices for
[k] [pu] [k] [pu]
Ï•(X ) and Ï•(X ), and Î¦(X ,X ),Î¦ (X ,X ) are the kernel matrices for
[k] [pu] [k] [pu] 2 [k] [pu]
X and X .
[k] [pu]
Hence, the objective function of PUAL can be reformulated as
Î»
min â„¦TÎ¦ (X ,X )â„¦+C 1T[1 âˆ’(Î¦(X ,X )â„¦+1 Î² )]
â„¦,Î²02 2 [pu] [pu] p p p [p] [pu] p 0 +
(26)
+C [1 +Î¦(X ,X )â„¦+1 Î² ]T[1 +Î¦(X ,X )â„¦+1 Î² ]
u u [u] [pu] u 0 u [u] [pu] u 0
+(Î¦(X ,X )â„¦+1 Î² )TR(Î¦(X ,X )â„¦+1 Î² ),
[pu] [pu] pu 0 [pu] [pu] pu 0
whose solution is not related to Xk],k = p.u.pu once the kernel matrices are
[
determined.
The predictive score function for instance xâˆ— of PUAL can be now transformed to
f =Î¦(xâˆ—,X )â„¦+Î² . (27)
[pu] 0
103.2.2 Parameter Estimation
In this case, we can update Î² via
0
m
Î²(k+1) = 2 âˆ’Q(k+1)/M , (28)
0 M b 22
22
where m , M are not related to X ,X ,X and
2 22 [p] [u] [pu]
Q(k+1) =2C 1TÎ¦(X ,X )â„¦(k+1)
b u u [u] [pu]
+21T RÎ¦(X ,X )â„¦(k+1) (29)
pu [pu] [pu]
+Âµ 1TÎ¦(X ,X )â„¦(k+1).
1 p [p] [pu]
The update of h,u can be reformulated as
h
(cid:32) (cid:33)
u(k)
h(k+1) =s 1+ hi âˆ’(Î¦(x ,X )â„¦(k+1)+Î²(k+1)) ,i=1,...,n ,
i C Âµ1p Âµ 1 i [pu] 0 p (30)
u(k+1) =u(k)+Âµ [1 âˆ’(Î¦(X ,X )â„¦(k+1)+1 Î²(k+1))âˆ’h(k+1)].
h h 1 p [p] [pu] p 0
As Î¦ (X ,X ) does not directly appear in the update process for the opti-
2 [k] [pu]
mization,weonlyneedtodeterminetheformofÎ¦(X ,X )inpractice.Moreover,
[k] [pu]
Î» either does not appear directly in the above update process or it is contained in the
matrix B as a part of Î¦(X ,X ). Therefore, for convenience, in the case of using
[k] [pu]
the kernel trick in Section 4.1, we use Î» to represent the hyper-parameter(s) of the
kernel matrix Î¦(X ,X ).
[k] [pu]
3.2.3 Algorithm
The algorithm of PUAL with non-linear decision boundary can be summarized in
Algorithm 2.
Algorithm 2 PUAL with non-linear decision boundary
Input: PU dataset, Î¦, C , C , Î», Ïƒ and Âµ
p u 1
Output: â„¦ and Î²
0
1: Initialize â„¦, Î² 0, h, u h.
2: while not converged do
3: Update â„¦ via Equation (21) w.r.t. h(k) and u(k)
h
4: Update Î² 0 via Equation (28)
5: Update h, u h via Equation (30)
6: end while
114 Experiments
4.1 Experiments on Synthetic Data
In this section, we conduct experiments on synthetic datasets following the pattern in
Fig. 2 to verify the superior performance of PUAL over GLLC.
4.1.1 Generation of Synthetic Positive-Negative (PN) Datasets
The synthetic datasets following the pattern in Fig. 2 were obtained by the following
steps:
1. Togeneratethefirstsubsetofthe2-dimensionalsyntheticpositiveset,200instances
were sampled from the multivariate normal distribution with mean vector (15,15)
(cid:20) (cid:21)
50 0
and the covariance matrix .
0 50
2. To generate the second subset of the 2-dimensional synthetic positive set, 200
instances were sampled from the multivariate normal distribution with the mean
(cid:20) (cid:21)
50 0
vector (mean ,mean ) and the covariance matrix .
p2 p2 0 50
3. To generate the 2-dimensional synthetic negative set, 400 instances were sampled
from the multivariate normal distribution of mean vector (0,0) and the covariance
(cid:20) (cid:21)
50 0.2
matrix .
0.2 50
4. Mixing the first subset of the synthetic positive set obtained in Step 1, the second
subset of the synthetic positive set obtained in Step 2 and the synthetic negative
set obtained in Step 3, a simple 2-dimensional synthetic dataset can be eventually
obtained as shown in Fig. 2.
In Step 2, mean took value from (50,100,200,500,1000). For each value of
p2
mean , the above steps were repeated 5 times so that we have 5 synthetic datasets
p2
for each value of mean .
p2
4.1.2 Training-Test Split for the Synthetic PU Datasets
It should be noted that both GLLC and PUAL can be applied on the datasets sam-
pled from either single-training-set scenario [39] or case-control scenario [19] as we
set the suitable metric for hyper-parameter tuning in practice. More specifically, the
case-control scenario indicates that the unlabeled training set can be regarded to be
i.i.d.sampledfromthepopulation,whilethesingle-training-setscenarioindicatesthat
the whole training set can be regarded to be i.i.d. sampled from the population. In
this case, for more intuitive comparison, we split each of the synthetic dataset gener-
ated in Section 4.1.1 to construct the PU training and test sets consistent with the
single-training-set scenario by the following two steps:
1. Firstly, to split the dataset into a training set and a test set, 70% of the instances
inthesimplesyntheticdatasetobtainedinSection4.1.1wererandomlyselectedas
the training set while the test set was constituted by the rest 30% instances.
122. Secondly, to construct the labeled-positive set and unlabeled-set for training, 25%
of the positive instances in the above obtained training set were randomly selected
toformthelabeled-positivesetfortraining.Therestofthepositiveinstanceswere
mixed with the negative set, contributing to the unlabeled set for training.
Then 25 pairs of PU training set and test set were obtained.
4.1.3 Model Setting
We note that the real value of F1-score on the training dataset is not accessible if we
do not use the label information during model training. Therefore, by fixing C to 1
p
and the number K of the nearest neighbors to 5, C , Î», Ïƒ in the objective functions
u
of PUAL and GLLC were determined by 4-fold cross-validation (CV), which reached
the highest average PUF-score proposed in [40] on the validation sets. PUF-score is
similar to F1-score and can be directly obtained from PU data:
recall2
PUF-score= , (31)
P[sgn(f(x))=1]
where â€˜recallâ€™ can be estimated by computing 1 (cid:80) â„¶(sgn(f(x )) = 1) with
np xiâˆˆX[p] i
the indicator function denoted by â„¶(Â·), and at the single-training-set scenario
P[sgn(f(x))=1]canbeestimatedbycomputing 1 (cid:80) â„¶(sgn(f(x ))=1).
np+nu xiâˆˆX[pu] i
Furthermore, Î», Ïƒ were tuned from the set {1,2,3,4,5} â—¦ {0.1,1,10,100}; C was
u
selected from the set {0.01,0.02,...,0.5} based on the setting in [26].
4.1.4 Results and Analysis
Results of the experiments on the constructed synthetic PU datasets are summarized
inTable1.TheperformancearemeasuredbytheaverageF1-score,whichisawidely-
usedmetricfortheevaluationofPUlearningmethods[41].Thepatternsofthedecision
boundary obtained by PUAL and GLLC on the synthetic datasets are illustrated in
Fig. 3. For each value of mean , we use the first generated synthetic dataset as
p2
example.
According to the experimental results in Table 1, PUAL always has better per-
formance than GLLC on the synthetic PU datasets with all the 5 values of mean .
p2
Furthermore,withthevalueofmean increasing,thegapbetweenPUALandGLLC
p2
becomes increasingly large. This is more clearly in the six plots in Fig. 3, as one of
the positive subset becomes increasingly far away, the decision boundary of GLLC
is dragged to a strange position that more and more instances are misclassified by
the decision boundary of GLLC, while the decision boundary of PUAL is unaffected.
Therefore,itisverifiedthatPUALcangeneratemuchbetterlineardecisionboundary
than GLLC on the datasets following the pattern in Fig. 2.
4.2 Experiments on Real-World Data
Inthissection,wefurtherassesstheclassificationperformanceofPUALonreal-world
datasets.
13Table 1: Summary of the average
F1-score(%)withthestandarddevi-
ation, from the experiments on the
synthetic dataset; the result high-
lighted in blue is the better one
between PUAL and GLLC.
meanp2 PUAL GLLC
50 95.07Â±0.78 91.17Â±1.52
100 94.89Â±0.61 86.27Â±1.83
200 93.56Â±0.75 81.04Â±2.93
500 92.83Â±3.22 73.58Â±2.58
1000 93.47Â±2.08 71.28Â±2.37
0 20 40 60 0 20 40 60
x1 x1
âˆ’20 0 20 40 60 80 100 âˆ’20 0 20 40 60 80 100
x1 x1
0 100 200 300 400 500 0 100 200 300 400 500
x1 x1
Fig. 3: The decision boundaries trained by (left) PUAL and (right) GLLC on the
synthetic data with mean = 50,100,500. Pink area: the negative region of PUAL;
p2
orange area: the negative region of GLLC; red points: positive instances; blue points:
negative instances; the instances in the plots are from the test sets.
14
2x
2x
2x
06
04
02
0
001
06
02
02âˆ’
005
003
001
0
2x
2x
2x
06
04
02
0
001
06
02
02âˆ’
005
003
001
04.2.1 Real-World Datasets
Firstly, 16 datasets from the UCI Machine Learning Repository
(https://archive.ics.uci.edu/ml/index.php) were selected to assess the performance of
PUAL and verify our motivation: Accelerometer (Acc), Ecoli, Pen-Based Recogni-
tion of Handwritten Digits (Pen), Online Retail (OR1), Online Retail II (OR2),
Parking Birmingham (PB), wifi, Sepsis survival minimal clinical records (SSMCR),
Avila, Raisin Dataset (RD), Occupancy Detection (OD), User Knowledge Modeling
Data Set (UMD), Seeds, Energy efficiency Data Set (ENB), Heart Disease (HD)
and Liver Disorders (LD). The details of these real-world datasets are summarized in
Table 2.
Table 2: Summary of the real-world datasets.
Dataset Positiveinstances Negativeinstances #Features
Acc 100red 100blue 4
Ecoli 116im&52pp 143cp&25om 6
Pen 200one&200eight 400four 16
OR1 301UK 301Germany 4
OR2 500UK 500Germany 4
SSMCR 391alive 109dead 3
PB 500BullRing 500BHMBCCMKT01 3
OD 100occupied 300notoccupied 5
UMD 83Low 63high 5
Seeds 70Kama 70Rosa 7
ENB 144TypeII 144TypeIII 7
wifi 100Location2&100Location4 499Location1&100Location3 7
Avila 300E 900A 10
RD 450Kecimen 450Besni 7
LD 144class1 200class2 6
HD 150absence 119presence 13
4.2.2 Compared Methods
GLLC, uPU and nnPU were also trained on the 16 real datasets as the compared
methods with PUAL. GLLC serves as the baseline of PUAL. uPU and nnPU are two
consistent PU learning methods.
4.2.3 Training-Test Split for the Real PU Datasets
Different from the steps in Section 4.1.2, PU training and test sets for the 16 real
datasetsareconstructedunderthecase-controlscenariosincewewouldliketoseethe
performanceofPUALonthedatasetswithvariouslabelingmechanism.Furthermore,
underthecase-controlscenario,itispossibletodofaircomparisonbetweenGLLCand
the two convincing methods for PU classification, i.e., uPU [19] and nnPU [20], since
theywereproposedunderthecase-controlscenario.Thestepstoperformtraining-test
split for the 16 PU real datasets are summarized as follows:
151. To obtain the binary positive-negative (PN) datasets from the original multi-class
dataset,certainclassesineachoftheoriginalrealdatasetsweretreatedaspositive
whilesomeotherclassesweretreatedasnegativewiththerestofclassesabandoned.
2. Toconstructthelabeled-positivesetandunlabeled-set,Î³â€² ofthepositiveinstances
in each binary PN dataset obtained in Step 1 were randomly selected to form
the labeled-positive set, and the rest of the positive instances were mixed into the
unlabeled set, contributing to the unlabeled set of the PU dataset.
3. TogeneratethetrainingsetandthetestsetfromtheconstructedPUdatasets,the
labeled-positive set and 70% of the instances in the unlabeled set were selected as
the training set while the test set was constituted by the rest 30% of the instances
in the unlabeled set; this corresponds to the setting of case-control scenario since
the unlabeled training set and the test set can be regarded to be sampled from the
same population.
During preliminary experiments, we found that the label frequency Î³ needs to be
greater than 20% for the hyper-parameter tuning strategy introduced in Section 4.2.4
to achieve adequate results. Therefore, the value of Î³â€² is set to 7 , 7 so that we
17 37
have the label frequency Î³ = Î³â€²/(0.3Î³â€² +0.7) = 0.5,0.25, which is the fraction of
positiveinstancesthatarelabeled,inthecorrespondingconstructedPUtrainingsets,
respectively. Then Step 2 and Step 3 were repeated for 10 times on each of the 16
binary PN datasets and obtained 10 pairs of PU training and test sets for each of the
16 binary PN datasets with a certain value of Î³â€².
4.2.4 Model Setting
At the case-control scenario, we also use PUF-score in Equation (31) for hyper-
parameter selection. The numerator â€˜recallâ€™ can still be estimated by computing
1 (cid:80) â„¶(sgn(f(x )) = 1), while the denominator P[sgn(f(x)) = 1] needs to be
np xiâˆˆX[p] i
estimated by computing 1 (cid:80) â„¶(sgn(f(x )) = 1) at the case-control scenario.
nu xiâˆˆX[u] i
Therefore,byfixingC to1andthenumberK ofthenearestneighborsto5,C ,Î»,Ïƒ
p u
in the objective functions of PUAL and GLLC were firstly tuned by 4-fold CV, which
reached the highest average PUF-score.
Furthermore, considering the higher complexity of the real datasets com-
pared with the synthetic datasets in Section 4.1, we modified our strategy
for hyper-parameter selection, enabling efficient selection of hyper-parameters
across a broader range. More specifically, Î», Ïƒ were tuned from the set
{10âˆ’4,10âˆ’3,10âˆ’2,10âˆ’1,100,101,102,103,104} and C was selected to from the set
u
{0.5,0.3,0.1,0.05,0.01} based on the setting in [26]. Then Î», Ïƒ and C were contin-
u
ually tuned following the greedy algorithm based on the average PUF-score on the
validation sets as follows:
1. Set Î», Ïƒ and C to the best combination from the grid search.
u
2. Sequentiallyupdateoneofhyper-parametersÎ»,Ïƒ andC byincreasing/decreasing
u
10% of its current value with the rest of the hyper-parameters unchanged. The
optimal scenario on 4-fold CV is set to be the final update of this step.
3. Repeat Step 2 until there is no better scenario appearing.
16In addition, the hyper-parameters of uPU and nnPU were fixed as the recom-
mended setting in the open source provided by [20] at https://github.com/kiryor/
nnPUlearning. Radial Basis Function (RBF) kernel was applied to both PUAL and
GLLC. More specifically, we computed
exp(cid:0)
âˆ’âˆ¥x âˆ’x
âˆ¥2/2Î»2(cid:1)
as the (i,j) element of
i j
Î¦â€²(Ï•(X ),Ï•(X )).
[pu] [pu]
4.2.5 Summary of Comparison between PUAL and GLLC
OD LD PB Avila Acc Seeds RD ENB UMD SSMCR Ecoli HD OR2 Pen OR1 wifi
Dataset
(a)Î³=0.5
Avila RD OD LD HD PB Seeds Ecoli SSMCR UMD ENB Acc Pen wifi OR2 OR1
Dataset
(b)Î³=0.25
Fig. 4: Boxplots for the difference between F1-scores of PUAL and GLLC on each
dataset increasingly ranked by medians; label frequencies Î³ = 0.5 (top) and 0.25
(bottom);x-axis:thedatasets;y-axis:thedifferencebetweenPUALandGLLCinF1-
score.
17
erocs
1F
no
ecnereffiD
erocs
1F
no
ecnereffiD
2.0
1.0
0.0
1.0âˆ’
2.0âˆ’
2.0
1.0
0.0
1.0âˆ’
2.0âˆ’The results of the difference between the F1-scores of PUAL and GLLC on each
pairwise experiments are shown in the boxplots in Fig. 4 with Î³ = 0.5,0.25, respec-
tively. In both figures, the best four datasets for PUAL over GLLC are wifi, OR1,
OR2, and Pen. Furthermore, with Î³ =0.5,0.25, three of the worst four datasets for
PUAL compared with GLLC are OD, LD, Avila. When Î³ =0.5, the rest one of the
worst four datasets is PB; when Î³ = 0.25, it is RD. According to the t-tests with
p-value lower than 0.05 between the pairwise results of these four methods, there are
totally 8 cases where PUAL is the optimal choice among the four methods.
4.2.6 Pattern Analysis for the Real Datasets Preferring PUAL to
GLLC
âˆ’0.5 0.0 0.5 1.0 âˆ’6 âˆ’4 âˆ’2 0 2 4 6
tâˆ’SNE_x1 tâˆ’SNE_x1
2âˆ’dimensional projection with tâˆ’SNE of wifi 2âˆ’dimensional projection with tâˆ’SNE of OR1
âˆ’10 âˆ’5 0 5 10 âˆ’0.5 0.0 0.5
tâˆ’SNE_x1 tâˆ’SNE_x1
2âˆ’dimensional projection with tâˆ’SNE of OR2 2âˆ’dimensional projection with tâˆ’SNE of Pen
Fig. 5: The t-SNE plots of the best four datasets wifi, OR1, OR2 and Pen; the
perplexityforthetrainingoft-SNEonthesefourdatasetswassetto750,40,250,750,
respectively; label frequency Î³=0.25; red: positive instances; blue: negative instances;
triangle: labeled instances; circle: unlabeled instances.
The t-SNE plots of the best four datasets, i.e., wifi, OR1, OR2, and Pen, for
PUAL compared with GLLC are shown in Fig. 5. According to the four t-SNE plots,
the following observations can be made:
1. The trifurcate pattern of the best four datasets in Fig. 5 is clear that the positive
set is constituted by two subsets distributing on both sides of the negative set as
discussed in the motivation of PUAL in Section 3.1.
2. Inthet-SNEplotofdatasetOR1,therearemanymoreinstancesintheright-hand
labeled-positivesubsetthantheinstancesintheleft-handlabeled-positiveset.This
18
2x_ENSâˆ’t
2x_ENSâˆ’t
5.0
0.0
5.0âˆ’
01
5
0
5âˆ’
2x_ENSâˆ’t
2x_ENSâˆ’t
6
4
2
0
2âˆ’
4âˆ’
6âˆ’
5.0
0.0
5.0âˆ’indicates that the trifurcate pattern does not have to be balanced for PUAL to
outperform GLLC.
4.2.7 Pattern Analysis for the Real Datasets Preferring GLLC to
PUAL
nulaenbgllaeabdtei âˆ’vleepdoâˆ’siptiovseitive
âˆ’4 âˆ’2 0 2 4 âˆ’4 âˆ’2 0 2 4
tâˆ’SNE_x1 tâˆ’SNE_x1
2âˆ’dimensional projection with tâˆ’SNE of OD 2âˆ’dimensional projection with tâˆ’SNE of Avila
âˆ’2 âˆ’1 0 1 2 âˆ’5 0 5
tâˆ’SNE_x1 tâˆ’SNE_x1
2âˆ’dimensional projection with tâˆ’SNE of LD 2âˆ’dimensional projection with tâˆ’SNE of PB
âˆ’6 âˆ’4 âˆ’2 0 2 4 6
tâˆ’SNE_x1
2âˆ’dimensional projection with tâˆ’SNE of RD
Fig. 6: The t-SNE plots of the worst five datasets OD, Avila, LD, PB, RD; cross
entropy loss for training with perplexity = 200, 550, 250, 300, 300; label frequency
Î³=0.25, 0.25, 0.25, 0.5, 0.25. The rest of the caption is as in Fig. 5.
There are overall five datasets to be the worst four datasets for PUAL compared
with GLLC under two cases of label frequency, i.e., Î³ =0.5,0.25. The t-SNE plots for
them are illustrated in Fig. 6 and their patterns can be summarized as follows:
1. According to the t-SNE plots of the two datasets LD and Avila, the positive set
and the negative set are mixed together, making the dataset challenging to be
separated. In this case, the labeled-positive instances selected as support vectors
19
2x_ENSâˆ’t
2x_ENSâˆ’t
4
2
0
2âˆ’
2
1
0
1âˆ’
2x_ENSâˆ’t
6
4
2
0
2âˆ’
4âˆ’
6âˆ’
2x_ENSâˆ’t
2x_ENSâˆ’t
4
2
0
2âˆ’
4âˆ’
6
4
2
0
2âˆ’
4âˆ’by the hinge loss of PUAL are not sufficient to adequately represent the pattern of
the positive set, while the squared loss of GLLC on the labeled-positive set, which
selects all positive instances as the support vectors, can somewhat alleviate this
issue. As a result, GLLC on these two datasets outperforms PUAL.
2. Thet-SNEplotsofthethreedatasetsOD,PB,andRDrepresentthetypicaltwo-
classpatterns.Inthistypeofdatasets,theproblemofGLLCmentionedinSection
3.1 does not exist. Therefore, the optimal combination(s) of the hyper-parameters
for GLLC to outperform PUAL was (were) found in at least one case of label
frequency Î³.
4.2.8 Comparison between PUAL, uPU and nnPU
The performance of PUAL, GLLC, uPU and nnPU on the 16 real datasets are sum-
marized in Table 3, from which we can obtain the following findings. First, there are
in total 22 cases of the 32 cases where PUAL outperforms uPU and nnPU. Secondly,
uPU and nnPU sometimes have much larger standard deviations than PUAL since
their algorithms based on Adam for the optimization of their non-convex objective
functions cannot always converge to the optimal solution, although nnPU alleviates
this issue to some extent.
Table 3: The average F1-score (%) of the classifiers. For each of the 16 original
datasets,theaverageF1-scoresandstandarddeviationsinthetworowswereobtained
under label frequencies Î³ = 0.5 (top) and 0.25 (bottom), respectively. In each row,
theresulthighlightedinblueindicatesthestatisticallyoptimalchoiceamongthefour
methods according to the t-tests with p-value lower than 0.05 (Sometimes an optimal
choice was missed in a row since there was no such a method holding statistically
significantly higher F1-score than other methods).
Dataset PUAL GLLC uPU nnPU
ENB 42.82 Â± 4.76 42.69 Â± 4.62 29.58 Â± 22.14 30.20 Â± 23.67
45.82 Â± 7.50 44.16 Â± 6.56 26.12 Â± 30.53 26.88 Â± 31.28
HD 82.72 Â± 2.35 81.97 Â± 5.45 71.38 Â± 4.23 74.38 Â± 2.19
81.92 Â± 4.03 84.46 Â± 4.11 71.01 Â± 3.97 75.06 Â± 2.40
Pen 92.47 Â± 8.13 88.92 Â± 10.15 77.76 Â± 31.00 87.50 Â± 14.94
91.73 Â± 9.04 87.02 Â± 11.38 72.55 Â± 31.03 84.06 Â± 16.85
LD 44.24 Â± 5.72 50.79 Â± 6.86 11.88 Â± 25.75 31.54 Â± 27.79
36.85 Â± 9.97 40.05 Â± 8.85 10.15 Â± 22.39 20.09 Â± 26.27
OR1 90.05 Â± 2.25 85.62 Â± 3.77 16.64 Â± 33.33 84.08 Â± 6.88
83.88 Â± 5.78 72.95 Â± 5.50 20.90 Â± 33.12 72.06 Â± 6.99
RD 82.45 Â± 2.18 83.01 Â± 2.38 70.61 Â± 12.89 71.29 Â± 13.63
77.63 Â± 3.77 81.99 Â± 2.84 72.92 Â± 14.53 73.12 Â± 12.83
Seeds 92.31 Â± 4.86 94.63 Â± 2.81 92.37 Â± 1.51 97.25 Â± 3.65
89.05 Â± 5.53 91.18 Â± 4.48 86.85 Â± 3.12 93.08 Â± 3.89
wifi 95.10 Â± 1.96 90.43 Â± 4.65 91.16 Â± 4.29 92.17 Â± 3.17
96.69 Â± 1.83 89.09 Â± 4.77 87.69 Â± 2.88 89.27 Â± 2.61
Avila 55.82 Â± 2.90 59.56 Â± 4.10 62.74 Â± 8.82 63.75 Â± 9.07
50.05 Â± 4.22 54.99 Â± 6.17 61.30 Â± 9.23 61.00 Â± 9.04
20Table 3 â€“ continued from previous page
Dataset PUAL GLLC uPU nnPU
OD 89.00 Â± 8.44 100.00 Â± 0.00 80.00 Â± 42.16 100.00 Â± 0.00
95.69 Â± 6.74 100.00 Â± 0.00 80.00 Â± 42.16 100.00 Â± 0.00
OR2 88.93 Â± 1.22 86.49 Â± 1.38 76.92 Â± 4.90 81.60 Â± 4.23
85.50 Â± 3.42 77.10 Â± 5.65 74.41 Â± 5.45 77.28 Â± 3.76
PB 95.90 Â± 1.12 100.00 Â± 0.00 69.77 Â± 2.62 67.19 Â± 3.17
97.86 Â± 0.67 100.00 Â± 0.00 68.75 Â± 2.63 66.63 Â± 4.09
Acc 65.02 Â± 4.79 68.10 Â± 2.18 20.05 Â± 27.57 20.46 Â± 28.56
66.36 Â± 4.42 64.08 Â± 6.31 21.95 Â± 29.61 23.43 Â± 31.40
Ecoli 90.80 Â± 2.59 90.15 Â± 6.28 84.41 Â± 6.13 85.92 Â± 6.69
88.04 Â± 4.44 88.03 Â± 3.96 84.92 Â± 6.82 86.05 Â± 6.55
SSMCR 87.63 Â± 1.29 87.35 Â± 2.14 85.71 Â± 1.98 87.42 Â± 1.37
87.63 Â± 1.29 87.35 Â± 2.14 84.97 Â± 2.03 86.79 Â± 1.50
UMD 100.00 Â± 0.00 99.80 Â± 0.62 100.00 Â± 0.00 100.00 Â± 0.00
99.58 Â± 0.88 98.41 Â± 1.80 100.00 Â± 0.00 100.00 Â± 0.00
5 Conclusion
In this paper, we propose PUAL for better classification on trifurcate PU datasets,
where the positive set is constituted by two subsets distributing on both sides of the
negative set. The key novelty of PUAL is an asymmetric structure composed of hinge
loss and squared loss for training the PU classifier. Experimental results demonstrate
the superiority of PUAL in performing PU classification on trifurcate data. As an
SVM-basedmethod,PUALisstillnegativelyaffectedbytheirrelevantfeaturesinthe
datasets, partly due to the L2 regularization term of model parameters, which cannot
compress the coefficients of the irrelevant features to be exactly zero thoroughly [42].
To address this issue, a future work is to explore the L1 regularization term of the
model parameters for PUAL.
References
[1] Ren,Y.,Ji,D.,Zhang,H.:Positiveunlabeledlearningfordeceptivereviewsdetec-
tion. In: Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 488â€“498 (2014)
[2] Li, H., Chen, Z., Liu, B., Wei, X., Shao, J.: Spotting fake reviews via collec-
tivepositive-unlabeledlearning.In:2014IEEEInternationalConferenceonData
Mining, pp. 899â€“904 (2014). IEEE
[3] Li, W., Guo, Q., Elkan, C.: A positive and unlabeled learning algorithm for one-
class classification of remote-sensing data. IEEE transactions on geoscience and
remote sensing 49(2), 717â€“725 (2010)
[4] Dai, S., Li, X., Zhou, Y., Ye, X., Liu, T.: GradPU: positive-unlabeled learn-
ing via gradient penalty and positive upweighting. In: Proceedings of the AAAI
Conference on Artificial Intelligence, vol. 37, pp. 7296â€“7303 (2023)
21[5] Liu, B., Lee, W.S., Yu, P.S., Li, X.: Partially supervised classification of text
documents. In: ICML, vol. 2, pp. 387â€“394 (2002). Sydney, NSW
[6] Yu, H., Han, J., Chang, K.-C.: PEBL: Web page classification without negative
examples. IEEE Transactions on Knowledge and Data Engineering 16(1), 70â€“81
(2004)
[7] Li, X., Liu, B.: Learning to classify texts using positive and unlabeled data. In:
IJCAI, vol. 3, pp. 587â€“592 (2003)
[8] He,F.,Liu,T.,Webb,G.I.,Tao,D.:Instance-dependentPUlearningbyBayesian
optimal relabeling. arXiv preprint arXiv:1808.02180 (2018)
[9] Liu,B.,Liu,Z.,Xiao,Y.:Anewdictionary-basedpositiveandunlabeledlearning
method. Applied Intelligence, 1â€“15 (2021)
[10] Liu,B.,Liu,Q.,Xiao,Y.:Anewmethodforpositiveandunlabeledlearningwith
privileged information. Applied Intelligence 52(3), 2465â€“2479 (2022)
[11] Ienco,D.,Pensa,R.G.,Meo,R.:Fromcontexttodistance:Learningdissimilarity
for categorical data clustering. ACM Transactions on Knowledge Discovery from
Data (TKDD) 6(1), 1â€“25 (2012)
[12] Liu,L.,Peng,T.:Clustering-basedMethodforPositiveandUnlabeledTextCat-
egorization Enhanced by Improved TFIDF. Journal of Information Science &
Engineering 30(5) (2014)
[13] Chaudhari,S.,Shevade,S.:Learningfrompositiveandunlabelledexamplesusing
maximum margin clustering. In: International Conference on Neural Information
Processing, pp. 465â€“473 (2012). Springer
[14] Ke, T., Yang, B., Zhen, L., Tan, J., Li, Y., Jing, L.: Building high-performance
classifiers using positive and unlabeled examples for text classification. In:
International Symposium on Neural Networks, pp. 187â€“195 (2012). Springer
[15] He, Y., Li, X., Zhang, M., Fournier-Viger, P., Huang, J.Z., Salloum, S.: A
novel observation points-based positive-unlabeled learning algorithm. CAAI
Transactions on Intelligence Technology 8(4), 1425â€“1443 (2023)
[16] Xu,C.,Liu,C.,Yang,S.,Wang,Y.,Zhang,S.,Jia,L.,Fu,Y.:Split-PU:Hardness-
aware Training Strategy for Positive-Unlabeled Learning. In: Proceedings of the
30th ACM International Conference on Multimedia, pp. 2719â€“2729 (2022)
[17] Dorigatti, E., Goschenhofer, J., Schubert, B., Rezaei, M., Bischl, B.: Positive-
Unlabeled Learning with Uncertainty-aware Pseudo-label Selection. arXiv
preprint arXiv:2201.13192 (2022)
[18] Liang, Q., Zhu, M., Wang, Y., Wang, X., Zhao, W., Yang, M., Wei, H., Han, B.,
22Zheng, X.: Positive distribution pollution: rethinking positive unlabeled learning
from a unified perspective. In: Proceedings of the AAAI Conference on Artificial
Intelligence, vol. 37, pp. 8737â€“8745 (2023)
[19] Du Plessis, M.C., Niu, G., Sugiyama, M.: Analysis of learning from positive and
unlabeled data. Advances in neural information processing systems 27 (2014)
[20] Kiryo, R., Niu, G., Plessis, M.C.d., Sugiyama, M.: Positive-unlabeled learning
with non-negative risk estimator. arXiv preprint arXiv:1703.00593 (2017)
[21] Su, G., Chen, W., Xu, M.: Positive-unlabeled learning from imbalanced data. In:
Proceedings of the 30th International Joint Conference on Artificial Intelligence,
Virtual Event (2021)
[22] Liu, B., Dai, Y., Li, X., Lee, W.S., Yu, P.S.: Building text classifiers using posi-
tive and unlabeled examples. In: Third IEEE International Conference on Data
Mining, pp. 179â€“186 (2003). IEEE
[23] Cortes,C.,Vapnik,V.:Supportvectormachine.Machinelearning20(3),273â€“297
(1995)
[24] Liu, Z., Shi, W., Li, D., Qin, Q.: Partially supervised classification: based on
weighted unlabeled samples support vector machine. In: Data Warehousing and
Mining:Concepts,Methodologies,Tools,andApplications,pp.1216â€“1230(2008)
[25] Ke,T.,Lv,H.,Sun,M.,Zhang,L.:Abiasedleastsquaressupportvectormachine
basedonMahalanobisdistanceforPUlearning.PhysicaA:StatisticalMechanics
and its Applications 509, 422â€“438 (2018)
[26] Ke,T.,Jing,L.,Lv,H.,Zhang,L.,Hu,Y.:Globalandlocallearningfrompositive
and unlabeled examples. Applied Intelligence 48(8), 2373â€“2392 (2018)
[27] Gong, C., Liu, T., Yang, J., Tao, D.: Large-margin label-calibrated support vec-
tor machines for positive and unlabeled learning. IEEE transactions on neural
networks and learning systems 30(11), 3471â€“3483 (2019)
[28] Maaten, L.: Visualizing data using t-SNE. Journal of machine learning research
9(Nov), 2579 (2008)
[29] Bhatt,R.:WirelessIndoorLocalization.UCIMachineLearningRepository.DOI:
https://doi.org/10.24432/C51880 (2017)
[30] Scott, C., Blanchard, G.: Novelty detection: Unlabeled data definitely help. In:
Artificial Intelligence and Statistics, pp. 464â€“471 (2009). PMLR
[31] Chapelle,O.,Scholkopf,B.,Zien,A.:Semi-supervisedlearning(chapelle,o.etal.,
eds.;2006)[bookreviews].IEEETransactionsonNeuralNetworks20(3),542â€“542
(2009)
23[32] Gabay, D., Mercier, B.: A dual algorithm for the solution of nonlinear varia-
tionalproblemsviafiniteelementapproximation.Computers&mathematicswith
applications 2(1), 17â€“40 (1976)
[33] Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J.: Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers.
Foundations and TrendsÂ® in Machine learning 3(1), 1â€“122 (2011)
[34] Ye, G.-B., Xie, X.: Split Bregman method for large scale fused Lasso. Computa-
tional Statistics & Data Analysis 55(4), 1552â€“1569 (2011)
[35] Jain,S.,White,M.,Trosset,M.W.,Radivojac,P.:Nonparametricsemi-supervised
learning of class proportions. arXiv preprint arXiv:1601.01944 (2016)
[36] Christoffel, M., Niu, G., Sugiyama, M.: Class-prior estimation for learning from
positiveandunlabeleddata.In:AsianConferenceonMachineLearning,pp.221â€“
236 (2016). PMLR
[37] Bekker, J., Davis, J.: Estimating the class prior in positive and unlabeled data
through decision tree induction. In: Proceedings of the AAAI Conference on
Artificial Intelligence, vol. 32 (2018)
[38] Genton, M.G.: Classes of kernels for machine learning: a statistics perspective.
Journal of machine learning research 2(Dec), 299â€“312 (2001)
[39] Bekker,J.,Davis,J.:Learningfrompositiveandunlabeleddata:asurvey.Mach.
Learn. 109(4), 719â€“760 (2020)
[40] Lee,W.S.,Liu,B.:Learningwithpositiveandunlabeledexamplesusingweighted
logistic regression. In: ICML, vol. 3, pp. 448â€“455 (2003)
[41] Liu, Y., Zhao, J., Xu, Y.: Robust and unbiased positive and unlabeled learning.
Knowledge-Based Systems 277, 110819 (2023)
[42] Nguyen,H.T.,Franke,K.,Petroviâ€™c,S.:OngeneraldefinitionofL1-normsupport
vector machines for feature selection. International Journal of Machine Learning
and Computing 1(3), 279 (2011)
24