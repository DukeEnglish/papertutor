VARIATIONAL AUTO-ENCODER BASED SOLUTIONS TO
INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS
APREPRINT
YinghuiPan BiyangMa
NationalEngineeringLaboratoryforBigDataSystemComputingTechnology SchoolofComputerScience
ShenzhenUniversity MinnanNormalUniversity
Shenzhen,China Zhangzhou,China
panyinghui@szu.edu.cn mby@mnnu.edu.cn
HanyiZhang
NationalEngineeringLaboratoryforBigDataSystemComputingTechnology
ShenzhenUniversity
Shenzhen,China
YifengZeng
DepartmentofComputerandInformationSciences
NorthumbriaUniversity,Newcastle,UK
yifeng.zeng@northumbria.ac.uk
October1,2024
ABSTRACT
Addressing multiagent decision problems in AI, especially those involving collaborative or com-
petitive agents acting concurrently in a partially observable and stochastic environment, remains
a formidable challenge. While Interactive Dynamic Influence Diagrams (I-DIDs) have offered a
promisingdecisionframeworkforsuchproblems,theyencounterlimitationswhenthesubjectagent
encountersunknownbehaviorsexhibitedbyotheragentsthatarenotexplicitlymodeledwithinthe
I-DID.Thiscanleadtosub-optimalresponsesfromthesubjectagent. Inthispaper,weproposea
noveldata-drivenapproachthatutilizesanencoder-decoderarchitecture,particularlyavariational
autoencoder,toenhanceI-DIDsolutions. Byintegratingaperplexity-basedtreelossfunctioninto
theoptimizationalgorithmofthevariationalautoencoder,coupledwiththeadvantagesofZig-Zag
One-Hotencodinganddecoding,wegeneratepotentialbehaviorsofotheragentswithintheI-DID
thataremorelikelytocontaintheirtruebehaviors,evenfromlimitedinteractions. Thisnewapproach
enablesthesubjectagenttorespondmoreappropriatelytounknownbehaviors,thusimprovingits
decisionquality. Weempiricallydemonstratetheeffectivenessoftheproposedapproachintwowell-
establishedproblemdomains,highlightingitspotentialforhandlingmulti-agentdecisionproblems
withunknownbehaviors. Thisworkisthefirsttimeofusingneuralnetworksbasedapproachesto
dealwiththeI-DIDchallengeinagentplanningandlearningproblems.
Keywords Decision-making Multi-agentSysterm VariationalAuto-encoder
Â· Â·
1 Introduction
Interactionsbetweenintelligentagentsoperatinginasharedanduncertainenvironmentamplifytheoverallsystemâ€™s
uncertainty,posingformidablechallengesforefficientmodelinganddecision-makinginmulti-agentsystems. Conse-
quently,theactionsoftheseagentsmutuallyinfluenceeachother,necessitatingthecomprehensiveconsiderationof
bothenvironmentaldynamicsandpotentialactionsequencesofotheragentsinordertomakeoptimaldecisions[39].
Thisissueiscommonlyreferredtoasmulti-agentsequentialdecisionmakingunderuncertainty[28].
4202
peS
03
]AM.sc[
1v56991.9042:viXraVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Overtheyears,variousdecisionmodelshavebeenproposedtotacklemulti-agentsequentialdecisionmaking(MSDM)
problems,includingdecentralizedpartiallyobservableMarkovdecisionprocesses(POMDPs)[36],interactivePOMDPs
[35],andinteractivedynamicinfluencediagrams(I-DIDs)[34]. Amongthese,I-DIDsstandoutfortheirabilityto
modelbothcollaborativeandcompetitiveagents,aswellastheircomputationaladvantages[32,33]. Furthermore,their
probabilisticgraphicalmodelstructuremakestheminherentlyexplainableinreasoningaboutthebehaviorsofother
agents.
TosolveI-DIDs,knowledge-basedapproacheshavebeenexploredtoprovidethesubjectagentwithmoreinformation
aboutotheragents[25,26,24]. However,theseapproachesmaybelimitedbyenvironmentalcomplexity,incomplete
data,anduncertainty,resultingininaccurateorlimitedinformation. Oneapproachtoaddressthisisbyincreasing
the number of candidate models for other agents, but this can lead to a prohibitively large model set, increasing
computationalcomplexity[37,29]. Asanalternative,data-drivenmethodshavebeenproposedthatleveragehistorical
behavioraldataofotheragentstolearntheirpotentialdecisionmodels,enhancingtheadaptabilityandinterpret-ability
ofthedecisionmodels[7].
However,data-drivendecisionmodelingconfrontstwosignificantchallenges: scarcityorconstraintsindataavailability
andinadequacyorbiasindataquality[43].Limitedorbiasedhistoricaldataoriginatingfromotheragentsfrequentlyfails
toaccuratelycapturetheirgenuinemodels.Moreover,inadequateorrestricteddatasamplinghindersthedevelopmentof
comprehensiveintentmodels,ultimatelyleadingtoalossofinformationfromtheoriginalhistoricaldecisionsequences.
Consequently,thesefactorscancontributetosub-optimalornon-generalizabledecisionmodelsforthesubjectagent.
Inthispaper,weproposeanoveldata-drivenframeworktolearnthetruepolicymodelofotheragentsfromlimited
historicalinteractiondata. Weextracthistoricalbehaviorcharacteristicstolearnanewsetofincompletebehavior
models from interaction sequences. To enhance modeling accuracy, we adapt the Variational Autoencoder (VAE)
[44,18]togenerateacollectionofmodelsthatcanencompassthetruemodelsofotheragentsfromanincomplete
setofpolicytrees. Thisnewapproachleveragesinformationfromincompletepolicytreesthatareoftendiscardedor
approximatedbytraditionalmethods. Wedevelopaperplexity-basedmetrictoquantifythelikelihoodofincludingtrue
behaviormodelsandselectoptimizedtop-K behaviorsfromthelargecandidateset. Ourcontributionsaresummarized
asfollows:
â€¢ WeproposetheVAE-basedalgorithmtoaddresschallengesindata-drivenI-DIDs,resultinginVAE-enabled
behaviors. ThispavesthewayforfutureresearchonneuralnetworksinMSDMproblems.
â€¢ WedeveloptheZig-ZagOne-Hot(ZZOH)encodinganddecodingtechnique,tailoredspecificallyforpolicy
trees,whichempowerstheVAEtoefficientlyhandlebothcompleteandincompletepolicytreesasinputdata,
thusenhancingitsversatility.
â€¢ We analyze the quality of I-DID solutions using a novel perplexity-based metric, providing insights for
algorithmfine-tuningandconfidenceintheperformance.
â€¢ Weconductempiricalcomparisonstostate-of-the-artI-DIDsolutionsintwoproblemdomainsandinvestigate
thepotentialofthenovelalgorithms.
Theremainderofthispaperisorganizedasfollows. Section2reviewsrelatedworksonsolvingI-DIDs. Section3
providesbackgroundknowledgeonI-DIDsandVAE.Section4presentsourapproachtogeneratingbehaviorsofother
agentsinI-DIDs. Section5showstheexperimentalresultsbycomparingvariousI-DIDsolutions. Finally,weconclude
theresearchanddiscussfutureworkinSection6.
2 RelatedWorks
Thissectionreviewsthecurrentresearchandemergingtrendsinthreekeyareas: neuralcomputing-baseddecision-
makingmodeling,modelingofunknownagentbehaviors,andI-DIDmodels,alongwithrelateddata-drivendecision-
makingapproaches.
Neuralcomputingtechniques,particularlydeepreinforcementlearning(DRL),haveattainedsignificantprogressin
addressingmulti-agentdecision-makingchallenges[42,14,15].DRL,whichcombinesdeeplearningandreinforcement
learning,facilitatesend-to-endlearningcontrolbutencounterslimitationsinsparserewards,limitedsamples,andmulti-
agentenvironments. Recentadvancementsinhierarchical,multi-agent,andimitationlearning,alongwithmaximum
entropy-basedmethods,offerpromisingresearchdirections[17]. InvestigationssuchasKononovandMaslennikovâ€™s
recurrentneuralnetworkstrainedthroughreinforcementlearning[16]andZhangetal.â€™smethodforgeneratingnatural
languageexplanationsforintelligentagentsâ€™behavior[11],demonstratethepotentialofDRL.Additionally,MO-MIX
andotherDRLapproachesenablemulti-agentcooperationacrossdiversedomains[19,20,21,22,40].
2VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
To emulate human-level intelligence and handle unexpected agent behaviors, neuro-symbolic multi-agent systems
areemerging. Thesesystemsleverageneuralnetworksâ€™abilitytoextractsymbolicfeaturesfromrawdata,combined
with sophisticated symbolic reasoning mechanisms. Techniques like agent-based models (ABMs) [13, 12], data-
drivendecision-making[4,3], andlargelanguagemodels(LLMs)[10]arebeingexplored. Anetal.[5]proposed
areinforcementlearningandCNN-basedapproachforagentstoself-learnbehaviorrulesfromdata,promotingthe
integrationofABMswithdatascienceandAI.Wangetal.[41]presentedoptimaldecision-makingandpathplanning
formulti-agentsystemsincomplexenvironmentsusingmean-fieldmodelingandreinforcementlearningforunmanned
aerialvehicles. Wasonetal.[9]andNascimentoetal.[8]exploredintegratinglargelanguagemodelsintomulti-agent
systems,highlightingtheirhuman-likecapabilitiesbutalsotheirdistinctiveness,agentautonomy,anddecision-making
indynamicenvironments.
Similarly, several I-DID solutions have relied solely on behavioral and value equivalences to constrain the model
space for other agents, presuming that the true behaviors of those agents fall within the subject agentâ€™s modeling
capabilities[32,33]. However,thisapproachfallsshortinfullyaccountingforunexpectedornovelbehaviors. Pan
etal.[6]proposedageneticalgorithm-basedframeworkthatincorporatesrandomnessintoopponentmodeling,thus
generatingnovelbehaviorsforagents,therebydemonstratingthesignificantpotentialofevolutionaryapproaches.
In the context of data-driven I-DIDs [7], the objective is to optimize multi-agent decision-making in uncertain
environments,asignificantchallengeinAIresearch. Ourworkinthisarticlepioneersanovelapproachtomodeling
otheragentsinI-DIDs,exploringneuralcomputing-baseddata-drivenmethodstoapproximaterealagentbehavior
modelsfromhistoricaldatainagentplanningresearch.
3 PreliminaryKnowledge
AswewilladapttheVAE-baseddatagenerationmethodstoaugmenttheI-DIDmodelwithnovelmetricsforother
agentsandgeneratenovelbehaviors,weprovidebackgroundknowledgeonI-DIDandVAE.
3.1 BackgroundknowledgeonI-DIDs
Thetraditionalinfluencediagram,designedforsingle-agentdecisions,hasevolvedintotheI-DIDframework. This
frameworkisaprobabilisticgraphtailoredforinteractivemulti-agentdecision-makingunderpartialobservability. From
theagentiâ€™sperspective,theI-DIDpredictsagentjâ€™sactions,aidingagentiinoptimizingitsdecisions. Wefocuson
agentiandintegrateagentjâ€™spotentialbehaviorsintothedecisionframework.
Weintroducethedynamicinfluencediagram(DID)forasingleagentinFig.1(a). ThisDIDrepresentstheagentâ€™s
decisionprocessoverthreetimesteps,withsolutionsshowninFig.1(b). IntheDID,ovalsrepresenteitherchance
nodes for environmental states (S) or observations (O); rectangles are decision nodes for the agentâ€™s actions (A);
diamondsdenoteutilitynodescapturingrewards(R). Attimet,theagentâ€™sdecision(At)isinfluencedbythecurrent
observation(Ot)andthepreviousdecision(Atâˆ’1). Theobservation(Ot)dependsonthepreviousdecision(Atâˆ’1)and
thecurrentstate(St). Thestates(St)arepartiallyobservableandinfluencedbythepreviousstates(Stâˆ’1)anddecision
(Atâˆ’1). Rewards(Rtâˆ’1)aredeterminedbyautilityfunctionconsideringboththestatesanddecisions. Arcsmodel
conditionalprobabilitiesamongtheconnectednodes. Forexample,ifOtisinfluencedbyAtâˆ’1andSt,thearrowsfrom
Atâˆ’1andSttoOtindicatethisdependency. Similarly,ifStisaffectedbyAtâˆ’1andtransitionsfromStâˆ’1,thearrows
fromAtâˆ’1andStâˆ’1toStshowthestatetransition. TheDIDdemonstrateshowtheagentoptimizesitsdecisionsin
theresponsetothechangingenvironment. Afterdefiningthetransition, observation, andutilityfunctions, weuse
traditionalinferencealgorithmstosolvethemodelandgettheagentâ€™soptimalpolicy. Thispolicyisrepresentedbya
policytree(seeFig.1(b)). Attimet=1,theagenttakesactiona andthesubsequentactionsbasedonobservations.
1
Thepathsinthetreecorrespondtotheobservations,whilenodesrepresenttheagentâ€™schosenactions. Indifferent
domains,thenumberofthepathsandnodetypesmayvary. Thepolicytree,asaDIDsolution,encapsulatesthemodelâ€™s
optimalpolicy(behavioralmodel). Itâ€™safullk-arytree,wherekisthenumberofpossibleobservations.
IntheI-DIDmodel, ahexagonalnode, knownasthemodelnodeM , dynamicallyextendsthebehavioralmodels
j
ofotheragentsintheinfluencediagram. Thisnodecontainspotentialbehavioralmodelsofagentj,whicharethen
providedtothesubjectagenti,transformingthecomplexI-DIDproblemintoaconventionalDID.However,asthe
numberoftimeslicesincreases,managinganextensivesetofcandidatemodelsbecomesintractable. Compression
andpruningofmodelnodesareoftennecessarybeforeintegratingthemintotheDID,butthiscanresultinnodeswith
similarbehavioralmodels,leadingtoalimitedsetofoptimaldecisionsforagenti.
Inthisarticle,toaddressthechallengesposedbyalargenumberoftimeslicesandselectingvastcandidatemodels,
weuseavariationalautoencoder(VAE)tolearnfromagentjâ€™shistoricaltrajectory. Thenewmethodgeneratesaset
3VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
ğ’•=ğŸ ğ’•=ğŸ ğ’•=ğŸ‘ ğ’•=ğŸ
ğ‘¹ ğ‘ºğŸ,ğ‘¨ğŸ ğ‘¹ğŸ ğ‘¹ ğ‘ºğŸ,ğ‘¨ğŸ ğ‘¹ğŸ ğ‘¹ ğ‘ºğŸ‘,ğ‘¨ğŸ‘ ğ‘¹ğŸ‘ ğ’‚
ğŸ
ğ’ ğŸ ğ’ ğŸ
ğ‘¨ğŸ ğ‘¨ğŸ ğ‘¨ğŸ‘
ğ’•=ğŸ
ğ’‚ ğ’‚
ğŸ‘ ğŸ
ğ’ ğ’ ğ’ ğ’
ğ‘ºğŸ ğ‘ºğŸ ğ‘ºğŸ‘ ğŸ ğŸ ğŸ ğŸ
ğ‘·ğ’“ ğ‘ºğŸ Pr ğ‘ºğŸğ‘ºğŸ,ğ‘¨ğŸ Pr ğ‘ºğŸ‘ğ‘ºğŸ,ğ‘¨ğŸ
Pr ğ‘¶ğŸğ‘ºğŸ Pr ğ‘¶ğŸğ‘ºğŸ,ğ‘¨ğŸ Pr ğ‘¶ğŸ‘ğ‘ºğŸ‘,ğ‘¨ğŸ ğ’‚ ğŸ ğ’‚ ğŸ‘ ğ’‚ ğŸ
ğ‘¶ğŸ ğ‘¶ğŸ ğ‘¶ğŸ‘
ğ’•=ğŸ‘
ğ’‚ ğ‘
Figure1: Adynamicinfluencediagramanditssolutions:(a)theleftisthedynamicinfluencediagramwiththreetime
stepsand(b)therightisitssolutionrepresentedasapolicytree. Theblockswiththesamecolorin(a)and(b)belongto
thesametimeslice.
ğ‘…(cid:3047)(cid:2879)(cid:2869) ğ‘…(cid:3047) ğ‘…(cid:3047)(cid:2878)(cid:2869)
(cid:3036) (cid:3036) (cid:3036)
ğ´(cid:3047)(cid:2879)(cid:2869) ğ´(cid:3047) ğ´(cid:3047)(cid:2878)(cid:2869)
(cid:3036) (cid:3036) (cid:3036)
ğ‘†(cid:3047)(cid:2879)(cid:2869) ğ‘†(cid:3047) ğ‘†(cid:3047)(cid:2878)(cid:2869)
ğ‘‚(cid:3047)(cid:2879)(cid:2869) ğ‘‚(cid:3047) ğ‘‚(cid:3047)(cid:2878)(cid:2869)
(cid:3036) (cid:3036) (cid:3036)
ğ´(cid:3047)(cid:2879)(cid:2869) ğ´(cid:3047) ğ´(cid:3047)(cid:2878)(cid:2869)
(cid:3037) (cid:3037) (cid:3037)
Figure2: ByextendingDIDmodel(thebluepart),theagentioptimisesitsdecisionintheI-DIDmodelswiththeblue
partwhichmodelsagentjâ€™sdecisionmakingprocess.
ofhighlyreliablepolicytrees,capturingkeyfeaturesofthetrajectory. Byembeddingthepotentiallytruebehavioral
modelofagentj intotheI-DID,weenableagentiâ€™soptimalpolicytoconsiderthemostprobablebehaviorsofagentj.
3.2 BackgroundknowledgeonVAE
VariationalAutoencoders(VAEs)[31,38]areatypeofunsupervisedlearningmodelthatcombinesthecapabilitiesof
autoencoderswithprobabilisticgenerativemodels. Unliketraditionalautoencoders[30],whichencodeinputsintoa
fixedlatentrepresentation,VAEsmaptheinputstoaprobabilitydistributioninthelatentspace. Thekeyideabehind
VAEsistoencodeinputdataintotheparametersofalatentdistribution,typicallyaGaussiandistribution. Theencoder
networkpredictsthemean(Âµ )andvariance(Ïƒ2)ofthisdistribution, whilethedecodernetworkreconstructsthe
Ï• Ï•
originalinputfromlatentsamples. Toenabledifferentiablesamplingfromthelatentdistribution,VAEsemploythe
reparameterization trick. This involves sampling random noise from a standard normal distribution and scaling it
accordingtothepredictedmeanandvariancetoobtainalatentsample(z Rl).
âˆˆ
TheVAEâ€™sobjectivefunctionconsistsoftwoparts:areconstructionlossthatmeasuresthesimilaritybetweentheoriginal
inputandthereconstructedoutput,andaKullbackâ€“Leibler(KL)divergencethatregularizesthelatentdistributiontobe
closetoapriordistribution(e.g.,astandardnormaldistribution). Optimizingthisobjectivefunctionleadstoencoder
anddecodernetworksthatcanencodemeaningfulrepresentationsoftheinputdataintothelatentspace. Formally,fora
giveninputxfromdataset ,theVAElossfunctionisdefinedas:
D
vae(x)= KL(P (z x),P (z))+ recon(x) (1)
LÏ•,Î¸ LÏ•,Î¸ Ï• | Î¸ LÏ•,Î¸
where KL istheKLdivergencebetweentheposteriordistributionP (z x)andthepriordistributionP (z),and recon
LÏ•,Î¸ Ï• | Î¸ LÏ•,Î¸
isthereconstructionloss. Overtheentiredataset ,theVAElossfunctionisgivenby:
D
(cid:88)
vae( )= vae(x) (2)
LÏ•,Î¸ D LÏ•,Î¸
xâˆˆD
Byminimizingthelossfunctionthroughthetechniqueslikeastochasticgradientdescent,theVAElearnstoencode
inputsintoalatentspacethatcapturestheiressentialfeatureswhileenablingthegenerationofnewdatasamplesthat
followthedistributionoftheoriginalinputdata,wherewecanapplystochasticgradientdescent(SGD)[38]tofindthe
optimizedparametersÏ•âˆ—andÎ¸âˆ—.
4VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
4 PolicyTreeGenerationBasedonVAE
Policytreegeneration,oftenreferredtoaslearningagentsâ€™behavioralmodels,involvestheautomatedconstructionofa
behavioralrepresentationintheformofapolicytreefromextensivemulti-agentinteractiondata. Thebehavioralmodel
aimstopredicttheactionsanagentwilltakegivenobservationsfromtheenvironment,whichistypicallyrepresentedas
acompletemulti-forktree,asdepictedinFig.1.
Fromtheperspectiveofagenti,itisnotnecessarytounderstandhowagentjoptimizesitsbehaviorbutrathertopredict
howagentjwillact. IntheI-DIDmodel,agentiisprovidedwithmpolicytreesrepresentingagentjâ€™sbehavior,where
thesetreesshallbecomplete. However,twochallengesarise:
â€¢ Manypolicytreesgenerateddirectlyfromhistoricalbehaviorsequencesarenotfullydeveloped-theyare
incomplete.
â€¢ Thegeneratedpolicytreesmaynotaccuratelyreflectthetruebehavioralmodelofagentj.
Addressing these issues is crucial for the effective utilization of the policy trees in predicting and understanding
multi-agentinteractions. Toaddressthefirstissue,therearetwotraditionalmethods,butbothhavetheirlimitations[2].
Discardingpolicytreeswithmissingnodesisproblematicbecauseitrisksexcludingsignificantpolicysequencesthat
maycontainvaluableinformationfromtheoriginalhistoricaldata. Thisapproachisonlyfeasiblewhendataisabundant,
butinmostcases,dataislimited. Thealternativemethod,basedonstatisticalanalysis,estimatestheprobabilityofeach
actioninthecurrenttimesliceusingtheactionsfromtheprevioustimesliceandthecurrentobservations. Then,a
roulettewheelselectionprocessisemployedtorandomlyfillinthemissingactionsinthepolicytree. However,this
approachhasitsshortcomingsaswell. Theactionsatdifferentlevelsinthepolicytreerepresentdistinctcontexts. For
instance,theactiona4attimeslicet=4isinfluencednotonlybytheactiona3anditscorrespondingobservation,but
alsobytheactionsandobservationsatearliertimeslicessuchast=1andt=2. Consequently,thehighconfidencein
takingactiona2att=2doesnotnecessarilytranslatetothehighprobabilityoftakingthesameactionatt=4.
Totacklethesecondissueofpotentialincompletenessinmodelingagentjâ€™sbehaviors,weendeavortogatherabroader
andmorediversecollectionofbehavioralmodels. Thegreaterthediversitywithinthisset,thehighertheprobability
thatitwillcapturethefullspectrumofagentjâ€™sactualbehaviorpatterns. Thisapproachensuresthatourmodelsare
comprehensiveandrepresentative,increasingtheirreliabilityandadaptability.
Tothatend,wedevelopanewframeworkcapableofgeneratingacomprehensiveensembleofpolicytreesfromagentsâ€™
interactiondata. Thisframeworknotonlyreconstructspolicytreesaccuratelybutalsoguaranteesthediversitywithin
theresultingset. Totacklethesechallenges,wedevelopapolicygenerationmethodleveragingvariationalautoencoder
techniques.
4.1 ReconstructinganIncompletePolicyTree
Giventhelimitationthatagentsoftencannotstorealargenumberofpolicytreestonavigatecomplexmulti-agent
interactiveenvironments,theagentmustrepeatedlytraversethepolicyfromtherootdownduringinteractions. Ina
simpleterm,fromthesubjectagentâ€™sperspective,wecanconstructpotentiallyincompletepolicytreesforotheragents
solelybasedonasinglesequenceofinteractiondata.
To obtain a long sequence of agent jâ€™s action-observations, we control the interaction between agent i and the
environment. ThissequenceisdenotedashL =(a1o1,a2o2,...,atot,...,aLoL),whereat Aandot â„¦represent
âˆˆ âˆˆ
analternatingseriesofactionsandobservationsattimeslicet. TheparameterListhetotalnumberoftimeslicesin
thisinteraction.
GiventheinteractiondatahL,thedepthofthepolicytree(denotedasT),andthenumberofagentjâ€™smodels(denoted
asm),wecanreconstructasetofpossibleincompletepolicytrees(cid:83) T ofthefixeddepthT (T L)usingfour
H â‰ª
operators: split,union,roulette,andgraphing. AsillustratedinFig.3,theseoperatorsenableustogeneratethe
desiredsetofthepolicytrees,andtheimplementationoftheseoperatorsisdescribedinAlg.3oftheAppendix.
Utilizingthesplitoperator(denotedas inFig.3 1),webeginatthestartingpointandextractsequencesofactions
S âƒ
withafixedlengthT,therebyformingpolicypathsthatcommencewithaspecificactiona. Werefertosuchapathas
hT. Ifthispolicypathisalignedwithasequenceofobservationso,itcanbedenotedashT . Concurrently,wecompute
a ao
theprobabilityofeachpolicypath,denotedasP(hT) #(hT) T/L. Here,#(hT)representingthenumberoftimes
hT appearsintheentiresequencehL. Werepeattha isâ† operatioa nuÂ· ntiltheendofthea sequence,ultimatelyformingaset
a
ofpolicypathsHT denotedasHT (hL).
T
â†S
ToobtainasetofpolicytreesfromthesetofpolicypathsHT,weusetheunionoperator(denotedas inFig.3
2). First, we define a subset of policy paths that begin with the same action a and observations oU as HT =
âƒ ao
5VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
(cid:83) (hT ,P(hT )). Thissubsetcontainsallthepolicypathsthatstartwiththesameactionaandfollowa
(hT,P(hT))âˆˆHT ao ao
ao ao
specificsequenceofobservationso. Wethencomputetheprobabilityofthissubsetbysumminguptheprobabilities
ofallthepolicypathswithinit: P(HT) = (cid:80) P(hT ). AftercollectingallthesubsetsHT forevery
ao (hT,P(hT))âˆˆHT ao ao
ao ao
possiblesequenceoftheobservationso â„¦Tâˆ’1followingactiona,weconstructalargersetHTthatcontainsallthe
policypathsbeginningwithactiona. Thâˆˆ issetisdefinedasHT =(cid:83) (HT,P(HT))andia tsoverallprobability
a oâˆˆâ„¦Tâˆ’1 ao ao
iscomputedasP(HT) = (cid:80) P(HT). Finally, foreveryactiona A, wecombinethesetsHT toformthe
completesetofpolica ypathsHo Tâˆˆâ„¦ ,wTâˆ’ h1 ichisa do enotedasHT =(cid:83) (HT,P(Hâˆˆ T)). Thissetrepresentsallpa ossiblepolicy
aâˆˆA a a
treesofdepthT thatcanbeconstructedfromtheoriginalinteractionsequencehL.
Togenerateasetofincompletepolicytrees T containingmsamples,wefirstapplytherouletteoperator(denotedas
,Fig.3 3)onthesetofpolicytreesetsHDT. ThisrandomselectionprocessyieldsasinglepolicytreesetHTfora
R particularâƒ actiona,representedasHT HT. Next,foreachpossiblesequenceofobservationso â„¦Tâˆ’1fola lowing
a â†R âˆˆ
actiona,weapplytherouletteoperatoragaintoselectasubsetofpolicypathsfromthecorrespondingsubsetHT .This
ao
resultsinacollectionofsubsets,whichisapossiblepolicytree(denotedas T),definedas T (cid:83) HT .
Ha Ha â† oâˆˆâ„¦Tâˆ’1R ao
Byrepeatedlyapplyingtherouletteoperatorinthismanner,wegeneratemincompletepolicytreecollections, T .
m
Thesetisdenotedas T andrepresentsadiversesetofpartiallyconstructedpolicytrees,eachofwhichstart{ sH wit} ha
D
randomlyselectedactionaandcontainsrandomlysampledpolicypathsforeachpossiblesequenceofobservations.
When applying the graphing operator (denoted as , Fig. 3 4) to the set of policy paths T for the purpose of
visualizingthecorrespondingpolicytree,werepresentG thistransâƒ formationas T T. NotH ea that,althoughweuse
thesamenotation T torefertoboththepolicytreeandthesetofpolicypatH hsa roâ† oteG dH ata awithalengthoftime-slice
Ha
T, thevisualizationprocessactuallycreatesarepresentationofthepolicytreeinagraphicalform. Toclarifythis
distinction,wecanexplicitlystatethat T representsboththeabstractpolicytreestructureandtheconcretesetof
policypaths,while T representsthesH ama epolicytreebutinagraphicalrepresentationgeneratedbythegraphing
GHa
operator .
G
AsdepictedinFig.3,theincompletepolicytree T isreconstructedfromthehistoricalbehaviorsequencehLthrough
Ha1
the application of the four operators: split, union, roulette, and graphing (Fig. 3 1-4). Additionally, Fig. 1
âƒ âƒ
illustratesascenariowhere,afteragentj observeso inthefirsttimesliceandtakesactiona ,itisunabletoselect
2 1
asubsequentactionifitencounterstheobservationo . Notably,someincompletepolicytreesareinevitablycreated
1
duringthegenerationofagentjâ€™spolicytreesfromhistoricaldata. Theseincompletepolicytrees,thoughaspecial
typeofbehaviormodel,differfromcompletepolicytreesinthattheylackspecifiedactionsforcertainobservations
incertaintimeslices. However,theI-DIDmodelrequiresacompletepolicytreemodelforitssolutionmethodology.
Therefore,anincompletepolicytreecannotbedirectlyutilizedinI-DID.Thus,weproposeaVAE-basedmethodto
convertincompletepolicytreesintotheircompletecounterparts,therebyenablingtheirutilizationwithintheI-DID.
4.2 StandardPolicyTreeGeneralization
Asmentionedearlier, discardingorrandomlyprocessingincompletepolicytreescanresultinasignificantlossof
crucialinformationfromtheoriginalhistoricaldecisionsequences. Toaddressthis,wedevelopaVAE-basedapproach
forgeneratingpolicytrees. Thismethodeffectivelyleveragesbothcompleteandincompletepolicytrees,enablingthe
productionofnumerousnewpolicytreesfromjustafewexamples. Bydoingso,itmaximizesthecoverageofagentjâ€™s
truebehaviors.
Inthismanner,ourapproachsolvestwokeyproblemsfacedbytraditionalmethodsinmodelingagentjâ€™sbehaviors.
AsillustratedinFig.4, aftertrainingtheVAEwithaselectionofincompletepolicytrees, themodeliscapableof
generatingpolicytreeswithvaryingdegreesofdeviation,adheringcloselytothedistributionofthehistoricaldata. This
approachsignificantlyenhancesthediversityoftheoverallpolicytreecollection,leadingtoamorecomprehensive
representationofagentjâ€™sbehaviors.
Here,weintroducetheZig-ZagOne-Hot(ZZOH)encodinganddecodingtechniquespecificallydesignedforpolicy
trees. ThistechniqueenablestheVAEtoeffectivelyhandleboththecompleteandincompletepolicytreesasinput
andoutputdata. SinceZZOHencodingprioritizesserializingthepolicytreestructure,nodesclosertotheroothavea
greaterimpactontheoveralldecision-makingeffectivenessoftheentirepolicytree.
Therefore,weadaptthelossfunctionoftheVAEnetworktoprioritizelearningbehaviorsfromnodesassociatedwith
earliertimeslices. Thisapproachhelpscapturethestructuralinformationembeddedintheoriginalpolicytree. After
reconstructingandfilteringalargenumberofpolicytreesusingtheVAE,weutilizetheMeasurementofdiversitywith
frames(MDF)andinformationconfusiondegree(IDF)toselectthetop-K policytreesfromthegeneratedset.
6VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
â„(cid:3013):ğ‘(cid:2869)ğ‘œ(cid:2869),ğ‘(cid:2871)ğ‘œ(cid:2869),ğ‘(cid:2871)ğ‘œ(cid:2870),ğ‘(cid:2870)ğ‘œ(cid:2869),ğ‘(cid:2871)ğ‘œ(cid:2869),ğ‘(cid:2871)ğ‘œ(cid:2870),â‹¯,ğ‘(cid:2871)ğ‘œ(cid:2869),ğ‘(cid:2871)ğ‘œ(cid:2869),ğ‘(cid:2869)ğ‘œ
(cid:2869)
1 splitğ’®
ğ»(cid:3021)â†ğ’®(cid:3021)â„(cid:3013): (cid:4666)ğ‘(cid:2869)ğ‘œ(cid:2869),ğ‘(cid:2871)ğ‘œ(cid:2869),ğ‘(cid:2870)ğ‘œ(cid:2870),0.03(cid:4667) (cid:4666)ğ‘(cid:2870)ğ‘œ(cid:2869),ğ‘(cid:2871)ğ‘œ(cid:2869),ğ‘(cid:2871)ğ‘œ(cid:2870),0.03(cid:4667) â‹¯(cid:4667)
2 Union ğ’°
H(cid:3021)â†ğ’°ğ»(cid:3021) ğ»(cid:3021): (cid:4666)ğ‘(cid:2869)ğ‘œ(cid:2869),ğ‘(cid:2871)ğ‘œ(cid:2869),ğ‘(cid:2870)ğ‘œ(cid:2870),0.03(cid:4667) â‹¯ ARoule ğ‘tte â†â„›
â„›A
ğ‘ğ’â†ğ‘(cid:2869)ğ‘œ(cid:2869)ğ‘œ(cid:2869) ğ‘0.2
ğ»(cid:3028)(cid:3021) ğ’â† (cid:3032)(cid:3276)ğ’â†(cid:3035)(cid:3276)(cid:3269)ğ’(cid:4651)
,â„™(cid:3035)(cid:3276)(cid:3269)ğ’
âˆˆ(cid:3009)(cid:3269)ğ‘’(cid:3028)ğ’ ğ‘œ ğ‘œ(cid:2869) (cid:2869)ğ‘œ ğ‘œ(cid:2869) (cid:2869): :ğ‘ ğ‘(cid:2869) (cid:2869)ğ‘ ğ‘(cid:2871) (cid:2871)ğ‘ ğ‘(cid:2870)
(cid:2871)
0 0. .0 03
2
ğ‘ ğ‘0 0. .2 10 ğ‘.2
ğ‘
ğ‘â„›
ğ‘
ğ‘
Hğ’ (cid:3028)(cid:3021)âˆˆ â†Î©(cid:3021) (cid:4651)(cid:2879)(cid:2869) ğ»â„™ (cid:3028)(cid:3021)ğ» ğ’(cid:3028) ,(cid:3021) â„™ğ’ â† ğ»(cid:3028)(cid:3021)(cid:3533)
ğ’
(cid:3035)(cid:3276)(cid:3269)ğ’,â„™(cid:3035)(cid:3276)(cid:3269)ğ’ âˆˆ(cid:3009)(cid:3276)(cid:3269)ğ’â„™â„(cid:3028)(cid:3021) ğ’
ğ»ğ»(cid:3028) (cid:3028)(cid:3021) (cid:3021)(cid:3117) (cid:3117)(cid:3042) (cid:3042)(cid:3117) (cid:3117)(cid:3042) (cid:3042)(cid:3117)
(cid:3118)
0 0. .0 05
1
â„‹(cid:3028)(cid:3021)0 (cid:3117).4
â†
ğ‘0 (cid:2869). ğ’¢8 â„‹1
(cid:3028)(cid:3021) (cid:3117)
ğ’âˆˆ(cid:2960)(cid:3269)(cid:3127)(cid:3117) ğ»(cid:3028)(cid:3021) (cid:3117)(cid:3042)(cid:3118)(cid:3042)(cid:3118) 0.04 ğ‘œ(cid:2869) ğ‘œ(cid:2870)
ğ‘ Hâˆˆ (cid:3021)ğ´
â†(cid:4651)
Hâ„™ (cid:3028)(cid:3021),H â„™(cid:3028)(cid:3021) Hâ† (cid:3028)(cid:3021)(cid:3533) (cid:3009)(cid:3276)(cid:3269)ğ’,â„™(cid:3009)(cid:3276)(cid:3269)ğ’ âˆˆ(cid:2892)(cid:3276)(cid:3269)â„™ğ»(cid:3028)(cid:3021) ğ’ HH(cid:3028) (cid:3028)(cid:3021)
(cid:3021)(cid:3117)
(cid:3118)
0 0. .5
3
ğ‘œ(cid:2869)ğ‘(cid:2871) ğ‘œ(cid:2870)ğ‘œ(cid:2869)ğ‘(cid:2869)
ğ‘œ(cid:2870)
(cid:3028)âˆˆ(cid:3002) H(cid:3028)(cid:3021)
(cid:3119)
0.2 ğ‘(cid:2870) ğ‘(cid:2871) ğ‘(cid:2869)
3 Rouletteâ„› 4 graphing ğ’¢
H HH(cid:3028) (cid:3028) (cid:3028)(cid:3021) (cid:3021) (cid:3021)(cid:3117) (cid:3118)
(cid:3119)
0 0 0. . .5 3
2
0 â„›.2 ğ» ğ»ğ»(cid:3028) (cid:3028) (cid:3028)(cid:3021) (cid:3021) (cid:3021)(cid:3117) (cid:3117) (cid:3117)(cid:3042) (cid:3042) (cid:3042)(cid:3117) (cid:3117) (cid:3118)(cid:3042) (cid:3042) (cid:3042)(cid:3117) (cid:3118)
(cid:3118)
0 0 0. . .0 0 05 1
4
ğ‘œ ğ‘œ ğ‘œ ğ‘œ(cid:2869) (cid:2869) (cid:2869) (cid:2870)ğ‘œ ğ‘œ ğ‘œ ğ‘œ(cid:2869) (cid:2869) (cid:2870) (cid:2870): : : :ğ‘ ğ‘ ğ‘ ğ‘(cid:2869) (cid:2869) (cid:2869) (cid:2869)ğ‘ ğ‘ ğ‘ ğ‘(cid:2871) (cid:2871) (cid:2871)
(cid:2869)
â‹®ğ‘ ğ‘ ğ‘ ğ‘(cid:2870) (cid:2871) (cid:2871)
(cid:2869)
0 0 0 0. . . .0 0 0 03 2 1 2ğ’âˆˆ 0 0â„›. .4 1Î© 0(cid:3021) .3(cid:2879) (cid:2869) ğ‘ğ‘ ğ‘(cid:2869) (cid:2869) (cid:2869)â†’ â†’(cid:3042) (cid:3042) â†’(cid:3042)(cid:3117) (cid:3117) (cid:3118)ğ‘ ğ‘ ğ‘(cid:2871) (cid:2871) (cid:2869)â†’ â†’ â†’(cid:3042) (cid:3042) (cid:3042)(cid:3117) (cid:3118) (cid:3118)ğ‘ ğ‘ ğ‘(cid:2869)(cid:2870) (cid:2871)
H(cid:3021) H(cid:3028)(cid:3021) (cid:3117)â†â„›H(cid:3021) ğ’ğ»(cid:3028)(cid:3021) (cid:3117)ğ’ â„‹(cid:3028)(cid:3021) (cid:3117)â†âˆª ğ’âˆˆ(cid:2960)(cid:3269)(cid:3127)(cid:3117)â„›ğ»(cid:3028)(cid:3021) (cid:3117)ğ’
ğ’Ÿ(cid:3021) (cid:3404) â„‹(cid:3021) (cid:3040)
Figure3: Reconstructingmincompletepolicytreesviafouroperators(split,union,rouletteandgraphying)from
behaviorsequences.
ğ‘ ğ‘
ğ‘œ ! ğ‘œ ğ‘œ ! ğ‘œ
! " ! "
ğ‘ ğ‘ ğ‘ ğ‘
# ! # "
ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ
! " ! " ! " ! "
ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘
" # " " # # " !
ğ‘ ğ‘
ğ‘œ ! ğ‘œ ğ‘œ ! ğ‘œ
! " ! "
ğ‘ ğ‘ ğ‘ ğ‘
# ! # "
ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ
! " ! " ! " ! "
ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘
! # ! ! " # " #
Decoder
noise
ğœº~ğ’©(ğŸ,ğˆ)
compressed space
Encoder
ğ‘ ğ‘
ğ‘œ ! ğ‘œ ğ‘œ ! ğ‘œ
! " ! "
ğ‘ ğ‘ ğ‘ ğ‘
# ! # "
ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ ğ‘œ
! " ! " ! " ! "
ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘
! # " " # !
Incomplete policy trees
Figure4: TheprincipleofaVAE-basedapproachthatleveragesincompletepolicytreestogeneratediversenewtrees,
maximizingcoverageofagentjâ€™struebehaviorsandenhancingthediversityoftheoverallpolicytreecollection.
7VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Empirically,weverifythatthebehaviormodelofagentj generatedusingtheVAEcloselyalignswiththedistribution
ofagentjâ€™shistoricaldata. Furthermore,wecomparethediversityandcredibilityofthetop-K policytrees. Wehave
describedthedetailedprocessofgeneratingmorediversepolicytreesusingtheVAEmodelinFig.5.
ğ’Ÿ(cid:3021) (cid:3404) â„‹(cid:2904)
(cid:3040)
â„’(cid:3087),(cid:3109)x(cid:3404)â„’(cid:3109)(cid:3012)(cid:3013)x(cid:3397)â„’(cid:3087)(cid:3045)(cid:3032) ,(cid:3109)(cid:3030)(cid:3042)(cid:3041)x
ğ‘ğ‘œ (cid:2871)ğ‘ ğ´(cid:2869) ğ‘ğ‘œ (cid:2870)ğ‘œ(cid:2869) (cid:2869) (cid:3404)(cid:2869)ğ‘ (cid:2870)ğ‘ğ‘(cid:2871)ğ‘œ ğ‘ğ‘œ(cid:2871)(cid:2869)ğ‘œ ğ‘ (cid:2869)(cid:2870)ğ‘œğ‘œ(cid:2869) ğ‘œ ,ğ‘(cid:2871)(cid:2870)(cid:2869)ğ‘ ğ‘(cid:2870) ğ‘(cid:2871)(cid:2869)ğ‘ (cid:2871)ğ‘ (cid:2870)ğ‘(cid:2869) ğ‘œ ,(cid:2869) (cid:2870)ğ‘œ ğ‘ ğ‘(cid:2869)ğ‘œ(cid:2870)ğ‘œ ğ‘œğ‘œ (cid:2870)(cid:2869) (cid:2871)(cid:2870) (cid:2869)ğ‘(cid:2870) (cid:2870)ğ‘ğ‘(cid:2870)ğ‘œ(cid:2869)(cid:2870)ğ‘œğ‘œ(cid:2870) ğ‘(cid:2870) (cid:2869) â„¤ 1â„‹(cid:2904) ğ± 1 0 0 0 0 0 1 0 1 0 0 0 0. . . ğœ™â† ğ‘”E (cid:3109)ğœ™ n :(cid:3397) c â„™ğ›¼ o (cid:3109)âˆ‡ d (cid:4666)(cid:3109) e ğ³â„’(cid:3087) |r ğ±,(cid:3109) (cid:4667)x ğ³~ğ’©(cid:4666)2 ğ (cid:3109),T V ğˆra A (cid:3109)in (cid:4667)i Eng via ğ³ SGD ğœƒâ† ğ‘“ğœƒ D (cid:3087)(cid:3397) e :â„™ğ›¼ câˆ‡ (cid:3087)o(cid:3087) â„’ d ğ±(cid:3087)(cid:3045) e(cid:3032) ,(cid:3109)(cid:3030) ğ³r(cid:3042)(cid:3041)x .. .. .. 0 0 . . 0ğ± 0(cid:3556) . . .81 19 41 5 1. . . I ğ±(cid:3556) ğ± 1 0 0 0 0 0 1 0 0 1 0 0(cid:3540) . . .
Î©(cid:3404) ğ‘œ ,ğ‘œ 0 ğ³(cid:3404)ğ (cid:3397)ğˆ â¨€ğœº .1 0
(cid:2869) (cid:2870) 0 (cid:3109) (cid:3109) .8 1
ğ‘‡(cid:3404)3 1 ğ›˜ ğœº~ğ’©(cid:4666)ğŸ,ğˆ(cid:4667) 0 .1. 0 0
Zigâ€ZagOneâ€Hot (ZZOH)encoding & decoding ğ±
â„¤ğ±(cid:3540) (cid:2897)3 Generating
1 Topâ€K argmax ğ‘‘(cid:4666) â„‹(cid:3553)(cid:2904) (cid:4667) ğ‘‘(cid:3014)(cid:3005)(cid:3007)
â„‹(cid:2904) â„‹(cid:2904)â‡”â„¤ ğ± 1 0 0 0 0 â„‹(cid:3553)(cid:3152) (cid:3143)âŠ‚â„¤ğ±(cid:3540)(cid:3145) (cid:2895) ğ‘‘(cid:3010)(cid:3004)(cid:3005)
â„=3 ğ‘œ (cid:2869) ğ‘ (cid:2869) ğ‘œ (cid:2870) en dco ecd oin dg in g 0 0 1 ğ‘œ0 0 (cid:2869) ğ‘ (cid:2869) ğ‘œ (cid:2870) 1 0 0 0 0 1 0 1 0 ğ’Ÿ(cid:3561)(cid:3021) (cid:3404)4 â„‹(cid:3553)(cid:2904)â„‹(cid:3553) (cid:2895)(cid:2904) (cid:2895) ğ‘œ (cid:2869)ğ‘œğ‘œ (cid:2869)(cid:2869)ğ‘ğ‘(cid:2869)ğ‘ (cid:2869)(cid:2869)ğ‘œğ‘œ(cid:2870)ğ‘œ (cid:2870)(cid:2870)
â„=2 ğ‘œ (cid:2869) ğ‘ (cid:2871)ğ‘œ (cid:2870) ğ‘œ (cid:2869) ğ‘ (cid:2869) ğ‘œ (cid:2870) ğ‘ 1 0(cid:2869)ğ‘ 0 1(cid:2870)ğ‘ 0 0(cid:2871) 0 0 1 0 0 ğ‘œ0 (cid:2869) ğ‘ (cid:2871) 0 0 1ğ‘œ (cid:2870) ğ‘œ (cid:2869)0 1 0ğ‘ (cid:2869) ğ‘œ0 (cid:2870) 0 0 0 0 0 . . . ğ‘œ (cid:2869)ğ‘œğ‘œ (cid:2869)(cid:2869)ğ‘ğ‘(cid:2870)ğ‘ (cid:2871)(cid:2871)ğ‘œğ‘œ(cid:2870)ğ‘œ (cid:2870)(cid:2870) ğ‘œ (cid:2869)ğ‘œğ‘œ (cid:2869)(cid:2869)ğ‘ğ‘(cid:2870)ğ‘ (cid:2870)(cid:2869)ğ‘œğ‘œ(cid:2870)ğ‘œ (cid:2870)(cid:2870)
â„=1 ğ‘ (cid:2869) ğ‘ (cid:2871) ğ‘ (cid:2870) 0 0 0 0 1 0 0 1 0 ğ‘ (cid:2869) 0 ğ‘ (cid:2871) ğ‘ (cid:2870) 0 1 0 0
0 1 ğ‘ğ‘(cid:2871)ğ‘ (cid:2870)(cid:2870) ğ‘ğ‘(cid:2871)ğ‘ (cid:2871)(cid:2871)ğ‘ (cid:2870)ğ‘ (cid:2870) ğ‘ğ‘ (cid:2869)(cid:2870)
Figure5: TheVAEnetworkcreatesnewpolicytrees. WiththeZig-ZagOne-Hotencoding-decodingmethoddesigned
forpolicytrees,VAEcanhandlebothcompleteandincompletetrees,emphasizinglearningfromearliernodes. By
usingMDFandIDF,wepickthemostdiverseandreliabletop-K trees,matchinghistoricalagentpatternsclosely.
4.2.1 Zig-ZagOne-Hotencoding&decoding
ToserializethepolicytreeandmakeitcompatiblewiththeVAEnetwork,weintroducetheZig-ZagOne-Hot(ZZOH)
encodinganddecodingtechnique. Thisapproacheffectivelytransformsbothcompleteandincompletepolicytreesinto
columnvectorsthatcanbeutilizedbytheVAEnetwork.
Considering some empty nodes in incomplete policy tree, we enlarge the action space A = a ,a ,...a of
j
{
1 2 |Aj|
}
agentj withemptyactiona ,denotedasAËœ = a ,a ,a ,...a . Then,weuseone-hotencodingtoencodeeach
0 j
{
0 1 2 |Aj|
}
actionoftheenlargedactionspaceAËœ intoabinarycodewiththelength AËœ andgenerateanencodingsetofaction
j j
space, denoted as AËœc. For example, given the action space A = a ,a| ,a| , we have enlarged the action space
j j { 1 2 3 }
AËœ = a ,a ,a ,a ,andtheone-hotencodingsetofenlargedactionspace
j 0 1 2 3
{ }
a a a a
0 1 2 3
1 0 0 0
AËœc = 0 1 0 0
j
0 0 1 0
0 0 0 1
, andwehaveAËœc[a ] = [1;0;0;0]. Byencodingtheactionspace, wedefineanoperator forencoding/decoding
j 0 Z
policytreesto/frombinarycodes. Thislosslesstransformationintoavectorrepresentationpreservesthetreeâ€™sstructural
information,enablingefficientprocessingandanalysis.
Thus,forgivenapolicytree T,weencodeeachactionnodeintoabinarycodeusingtheencodingsetofenlarged
actionspace,andthenconcateH natethebinarycodesintoacolumnvectorxinazigzagorder,denotedas T. Wherein
ZH
the numerical information indicates the node action information, and the node position information indicates the
sequenceinformationinfrontofthenodeandthecurrentobservationresult. Werepeatthisoperationtoencodethe
policytreeinthegivenpolicytreeset T andformthedatasetX fortrainingandtestingtheVAEnetwork,whichis
denotedas T,asshowninFig.5 1D . WealsopresenttheimplementationoftheoperatorsinAlg.5oftheAppendix.
ZD âƒ
8VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
4.2.2 TreeLossFunctioninVAE
Theintricatestructureofpolicytreesencapsulatescriticalinformation,includingactionvalues,sequentialorder,and
theirrelativeimportance. Actionnodesclosertotherootexercisegreaterinfluenceontheoveralldecision-making
effectivenessofthetree,shapingthetrajectorytoalltheleafnodes. However,thisstructuralinformationisnotfully
capturedinserializedtreerepresentations.
ToleverageserializedpolicytreedataforVAEtrainingandbettercapturestructuralnuances,weproposeamodified
VAElossfunction. Thisrefinementprioritizesthebehaviorlearningfromnodeswithsmallertimeslices,emphasizing
thestructuralinformationembeddedintheoriginalpolicytree. Effectively,thisapproachassignshigherweightsto
actionnodesclosertotheroot,ensuringthattheirsignificanceisreflectedintheVAElearning.
GiventhepriordistributionoverlatentvariablesasacenteredisotropicmultivariateGaussian: P (z)= (z;0,I),we
Î¸
definethelikelihoodfunctionP (x z)asamultivariateBernoullidistribution(forbinarydata)wherethN edistribution
Î¸
|
parametersaredeterminedfromzusingafully-connectedneuralnetworkwithasinglehiddenlayer. Sincethetrue
posterior P (z x) is often intractable, we employ a variational approximate posterior P (z x) to approximate it.
Î¸ Ï•
| |
Specifically,weletthevariationalapproximateposteriorbeamultivariateGaussianwithadiagonalcovariancestructure:
P (z x) = (z;Âµ ,Ïƒ2I). whereÂµ andÏƒ2 arethemeanandvariancevectorspredictedbytheencodernetwork
Ï• | N Ï• Ï• Ï• Ï•
parameterizedbyÏ•. Toenabledifferentiablesamplingfromthisvariationalposterior,weutilizethereparameterization
trick. Specifically,wesamplearandomnoisevectorÎµfromastandardnormaldistribution (0,I)andthentransform
N
itaccordingtothepredictedmeanandvariance: z=Âµ +Ïƒ Îµ. Thisre-parameterizationallowsgradientstoflow
Ï• Ï• âŠ™
throughthesamplingprocess,enablingtheuseofgradient-basedoptimizationtechniquestotraintheVAE.
Âµ ,Ïƒ =Encoder (x)
Ï• Ï• Ï•
Îµ P(Îµ)= (Îµ;0,I) (3)
âˆ¼ N
z=Âµ +Ïƒ Îµ
Ï• Ï• âŠ™
WehaveathedecoderofVAEforBernoullidata:
Ëœx=Decoder (z)
Î¸
|x|
(cid:88)
logP (x z)= logP (x z)
Î¸ Î¸ k
| |
k=1
|x|
(cid:88)
= Bernoulli(x ;Ëœx ) (4)
k k
k=1
|x|
(cid:88)
= x logËœx +(1 x )log(1 Ëœx )
k k k k
âˆ’ âˆ’
k=1
=â„“(x,Ëœx)
whereâ„“(x,Ëœx)denotestheBinaryCross-EntropyLoss(BCELoss)[38]duetotheVAEdecoderforBernoullidata,aka
thebasicconstructionerror.
ToutilizeserializeddatafromthepolicytreetotrainaVAEnetworkandbetterrepresentthestructuralinformationand
datadistributioncharacteristicsoftheoriginalpolicytree,weproposethedecoderofVAEforBernoullidatawithtree
weights.
|x|
(cid:88)
â„“tree(x,Ëœx)= w(k)(x logËœx +(1 x )log(1 Ëœx )) (5)
k k k k
âˆ’ âˆ’
k=1
wherew(k)=log(1+h( kâˆ’1 +1)),N = x/(A +1), Ëœx Ëœx:0 Ëœx 1. h(n)istheheightofanodeinthe
âŒŠ N âŒ‹ | | | j | âˆ€ i âˆˆ â‰¤ i â‰¤
policytree,whichcanbecalculatedas:
â„¦câˆ’1 1 â„¦c 1
h(n)=T c+1,if n [| | âˆ’ +1,| | âˆ’ ],c R+ (6)
âˆ’ âˆˆ â„¦ 1 â„¦ 1 âˆˆ
| |âˆ’ | |âˆ’
ThenwehavethelossfunctionofVAE:
Lr Î¸e ,c Ï•on(x)= âˆ’E zâˆ¼P Ï•(z|x)log(P Î¸(x |z))
= E log(P (x Âµ +Ïƒ Îµ))
âˆ’ Îµâˆ¼N(Îµ;0,I) Î¸ | Ï• Ï• âŠ™ (7)
1
(cid:88)ns
= â„“tree(x,Ëœx(l))
âˆ’2n
s
l=1
9VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
KL(x;Ï•)= (P (z x),P (z))
LÎ¸,Ï• LKL Ï• | Î¸
=D ( (z;Âµ ,Ïƒ2I) (z;0,I))
KL N Ï• Ï• ||N
=
1| (cid:88)Âµ Ï•|
(cid:0) 1+logÏƒ2 Âµ2 Ïƒ2 (cid:1)
(8)
2 Ï•,kâˆ’ Ï•,kâˆ’ Ï•,k
k=1
= KL(x;Ï•)
LÏ•
(x)= recon(x)+ KL(x) (9)
LÎ¸,Ï• LÎ¸,Ï• LÏ•
wheren isthebatchlearningsamplingsize,Ëœx=f (g (x))istheoutputvectorofVAEnetwork,theoperator isa
s Î¸ Ï•
âŠ™
Algorithm1:Learningatree-lossbasedVAEnetworkthroughSGD(@VAE)
Data: Dataset = x ,learningrateÎ±,batchsizen ,samplingsizen .
m b s
Result: LearneX dnetw{ o} rkNetvae()withparametersÎ¸andÏ•
Î¸,Ï• Â·
1 Initialize: VariationalnetworkparametersÎ¸andÏ•randomly
2 repeat
3 Sampleabatchofn bdata XËœ = {x }nb from
X
4 forx Ëœdo
âˆˆX
5 Âµ,Ïƒ Encoder Ï•(x)
â†
6 forl 1,2,...,n s do
âˆˆ{ }
7 GeneratenoiseÎµ (0,I)
âˆ¼N
8 z Âµ+Ïƒ Îµ
â† âŠ™
9 Ëœx(l) Decoder Î¸(z)
â†
10 end
11 Computethelossfunction:
12 Lr Î¸e ,c Ï•on(x) â†âˆ’2n1
s
(cid:80) (cid:16)n l=s 1â„“tree(x,Ëœx(l))
(cid:17)
13 LK Ï•L(x) â† 1 2(cid:80)| kÂµ =Ï• 1| 1+logÏƒ2 Ï•,kâˆ’Âµ2 Ï•,kâˆ’Ïƒ2 Ï•,k
14 end
15 Comput (cid:16)ethegradientsofthe (cid:17)lossfunctionwithrespecttothenetworkparameters: âˆ‡Î¸(cid:80) xLr Î¸e ,c Ï•on(x)
16
âˆ‡Ï•(cid:80)
x
Lr Î¸e ,c Ï•on(x)+ LK Ï•L(x)
17 Updateparametersusinggradientdescent:
18 Î¸ â†Î¸ âˆ’Î± âˆ‡Î¸(cid:80) xL(cid:16)r Î¸e ,c Ï•on(x)
(cid:17)
19 Ï• â†Ï• âˆ’Î± âˆ‡Ï•(cid:80) x Lr Î¸e ,c Ï•on(x)+ LK Ï•L(x)
20 untilConvergence;
element-wiseproductoftwovectorandremovesallthezero-elements.
Subsequentlyweapplystochasticgradientdescentalgorithm(SGD)withthebatchlearningtotraintheVAEnetwork,
minimizingthemodifiedreconstructionloss. First,wecomputethegradientsofthelossfunctionwithrespecttothe
(cid:16) (cid:17)
networkparameters: (cid:80) recon(x)and (cid:80) recon(x)+ KL(x) . Then,weupdateparametersusinggradient
âˆ‡Î¸ xLÎ¸,Ï• âˆ‡Ï• x LÎ¸,Ï• LÏ•
(cid:16) (cid:17)
descent:Î¸ Î¸ Î± (cid:80) recon(x) and Ï• Ï• Î± (cid:80) recon(x)+ KL(x) . After the training process is
â† âˆ’ âˆ‡Î¸ xLÎ¸,Ï• â† âˆ’ âˆ‡Ï• x LÎ¸,Ï• LÏ•
completed,theoriginalpolicytreeisreconstructedtoexpandandobtainthecompletepolicytree,asshowninFig. 5.
DuringthetrainingphaseofVAE,wewilltraintheVAEnetworkparameterstolearnthedistributionofthebehaviors
fromhistoricalpolicytreesthatmayhavesomeincompletepolicytrees. Inthetesting,wereconstructtheinputpolicy
treetoobtainacompletepolicytree,includingtheselectionprobabilitiesforeachnode.
We outline the VAE training in Alg. 1. We initialize the VAE parameters Î¸ and Ï• and choose a data batch of size
n (lines 1-2). Then we encode each datum to get the compressed variable statistics and decode with the noise to
b
reconstruct(lines5-10). SubsequentlyweadjustandcomputetheVAEloss(lines11-13),calculatethegradients,update
theparameters,andrepeattheproceduresuntiltheparametervaluesbecomestable(lines15-20). Afterthenetwork
parametersarefinalized,weinputthegivenpolicytrees T intothetrainedVAEnetwork. Thisgeneratesanewvector
H
xË‡ representingapolicytreeasoutputs. ApplyingtheOne-hotencodingoperator (xËœ)transformsaprobabilityvector
I
toone-hotbinaryvectorforeachactionnode,whichmeansselectingtheactionwiththehighestprobabilityinthe
10VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
correspondingchosenvectorforthenode,presentedinAlg.4. Then,weapplytheZOOHdecodingoperatortoproduce
apolicytree xË‡,ensuringthattheresultingpolicytreeiscompleteandvalid. WerepeatthisprocessuntilweobtainM
completepolZ icytrees(Fig.5 3),denotedas Ë‡T = xË‡ .
M
âƒ D {Z }
4.3 EvaluationIndex
Afterreconstructingasignificantnumberofpolicytrees,weneedtofilteroutthosethatdonotaccuratelyrepresent
agentjâ€™struebehavior. Toachievethis,weleveragethemeasurementofdiversitywithframes(MDF)[6]andintroduce
anovelmetriccalledinformationconfusiondegree(ICD).Thesemetricsaretoselectthetop-K policytreesfromthe
generatedset.
MDFevaluatesthediversityacrossboththeverticalandhorizontaldimensions: theverticalrefersthevariationsin
behaviorsequenceswithineachpolicytree,andthehorizontalexploresthesequencedifferencesacrossallpotential
observationsatatimestep.
T
d(cid:16)
Ë†T
(cid:17)MDF =(cid:88)Diff(h t)+Diff(H t)
(10)
{H }K â„¦ tâˆ’1
t=1 | j |
whereDiff(h )andDiff(H )countthedifferentsequencesh andsub-trees(frames)H respectivelyin T =
t t t t DK
Ë†T . â„¦ denotesthenumberofagentjâ€™sobservations.
K j
{H } | |
Additionally, we introduce ICD to evaluate the reliability of the policy tree data. This metric guarantees a high
confidenceforeachnodeinthereconstructedtree. IntheVAE-generatedtree,nodesactasclassifiers,choosingthemost
probableaction. Asmallmarginbetweenthetopactionâ€™sprobabilityandtherestleadstohighinformationconfusion,
indicatingunclearorunreliabledecisions. WewanttheICDvalueofthepolicytreetoreachthemaximumvalue,and
selectthetop-K treeastheoutput.
N
d(cid:16) Ë†T (cid:17)ICD = (cid:88) (cid:88) log(1+h(n)) p log(p ) (11)
K n n
{H } âˆ’ âˆ—
HË†Tâˆˆ{HË†T}Kn=1
wherep istheprobabilityofactionundercorrespondingobservationandN isthenumberofactionnodes.
n
Byselectingthetop-K policytrees,weaimforthemaximumdiversityorminimalinformationconfusionamongthe
generatedtrees. SinceMDFcapturesthediversityoftheoverallbehaviorsandtheICDofthesetreesreflectstheirtrue
distributioninthehistoricaldata,weconductthetop-K selectionusingbothMDFandICD.
(cid:16) (cid:17)
Ë†T = argmax d Ë†T (12)
K K
{H } {H }
{HË†T}KâŠ‚DË‡T
whered()iscalculatebyeitherEq.11orEq.10.
Â·
4.3.1 TheFrameworkoftheVAE-basedPolicyTreeGeneration
WeelaboratethegenerationofthediversepolicytreesusingtheVAEinAlg.2. AsdepictedinFig.3andFig.5,thekey
stepsareasfollows.
Initially, the process begins by reconstructing a set of incomplete policy trees, denoted as T, from an agent jâ€™s
interactivehistoryhL. ThisreconstructionisachievedthroughaseriesofoperationsincludingD split,union,roulette,
andgraphying,whichservetoextractandorganizepolicytreesfromthehistoricaldata(lines1-9,Fig.3 1-4).
âƒ âƒ
Subsequently,aZZOHencodingoperatorisappliedtoeachpolicytreein T,convertingthemintoabinaryvector
D
representation. Thisencodeddataset,denotedasX,isthenusedtotrainandtesttheVAEnetwork(line10,Fig.5 1).
âƒ
Utilizingstochasticgradientdescent(SGD)andatree-specificlossfunction,theVAEnetworklearnstocapturethe
latentrepresentationsofthepolicytrees(line11,Fig.5 2).
âƒ
OncetheVAEnetworkhasbeentrained,itisreadytogeneratemorecompletepolicytrees(lines12-18,Fig.5 3).
âƒ
Thisisachievedbyrandomlyselectingabinaryvectorfromtheoriginaldataset andfeedingitintotheVAEnetwork.
X
Thenetworkthengeneratesanewbinaryvector,whichisdecodedusingOne-hotencodingandtheZZOHdecoding
operatortoreconstructacompletepolicytree(lines14-17). Thisprocessisrepeateduntiladesirednumberofcomplete
policytreesareobtained(lines13-18).
Finally,fromthegeneratedsetofpolicytrees Ë‡T,themostdiverseorreliablebehaviorsareselectedusingmetricssuch
D
astheMDFandICD.ThesemetricsallowustoidentifytheK policytreesthatexhibitthehighestdegreeofdiversity
orreliability,enablingtheselectionofbehaviorsthatarethebestsuitedforagiventaskorscenario(line19,Fig.5 4).
âƒ
11VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Algorithm2:VAE-Enabledbehaviors(@VEB)
Data: agentjâ€™sinteractivehistoryhL,learningrateÎ±,actionsetAandobservationsetâ„¦,thenumberofpolicy
treesm,thenumberofgeneratedpolicytreesM,theevaluationindexfunctiond(),theparameteroftop-K
Â·
selectionfunctionK
Result: NewK policytrees Ë†T
K
1 HT ThL.â—Applyingspl{ itH op} erator;
â†S
2
HT HT.â—Applyingunionoperator;
â†U
T
3
D â†âˆ…
4 fork 1,2, m do
5 HT aâˆˆ â†{ RHÂ·TÂ· .Â· â—A} pplyingrouletteoperator;
6 HaT â†âˆªoâˆˆâ„¦Tâˆ’1 RHT ao.â—Applyingrouletteoperator;
7
HaT â†GHaT.â—Applyinggraphingoperator;
T T T
8 D â†D âˆªHa
9 end
10 HTâˆˆDT
T.â—ApplyingZZOHoperator;
11 NX eâ† tv Î¸,a Ï•eâˆª ( Â·) â†VAZ EH ( X).â—LearningVAEwithtreeloss;
Ë‡T
12
D â†âˆ…
13 fork 1,2, M do
âˆˆ{ Â·Â·Â· }
14 x .â—Applyingrouletteoperator;
15
Ëœxâ† â†NR eX tv Î¸,a Ï•e(x).â—GeneratedataviaVAE;
16 Ë‡x (Ëœx).â—ApplyingOne-hotencoding;
17 Ë‡â† T I Ë‡T Ë‡x â—ApplyingZZOHoperator;
D â†D âˆªZ
18 end
(cid:16) (cid:17)
19 {HË†T }K â†argmax {HË†T}KâŠ‚DË‡T d {HË†T }K â—Applyingtop-K operator;
5 ExperimentalResults
WeimprovetheI-DIDbyincorporatingtheVAE-basedmethodtogenerateandselectthediverseandrepresentative
behaviorsforagentj. Weconducttheexperimentsintwowell-studiedmulti-agentproblemdomains: themulti-agent
tigerproblem(Tiger)andthemulti-agentunmannedaerialvehicle(UAV)problem[46,45,47,29]. Alltheexperiments
areperformedonaWindows10systemwithan11-thGenIntelCorei7-6700CPU@3.40GHz(4cores)and24GB
RAM.
Inthemulti-agentcontext, wefocusonageneralscenariowithtwoagents, consideringagentiasthesubjectand
constructinganI-DIDmodelfortheproblemdomains. TheM nodesinthismodelrepresentthebehavioralmodelsof
j
agentj,asshowninFig.2. OurproposedapproachoptimizestheI-DIDmodelbyprovidingagentiwithasetofagent
jâ€™srepresentativebehaviors. WecomparesevenalgorithmsforsolvingtheI-DIDmodelintheexperiments:
â€¢ TheclassicI-DIDalgorithm,whichreliessolelyonknownmodelsM toexpandthemodelnodes,assuming
thetruebehaviorofagentj lieswithinthesenodes[47].
â€¢ Thegeneticalgorithmbasedalgorithm(IDID-GA)generatesagentjâ€™sbehaviorsthroughgeneticoperations[48,
6].
â€¢ TheIDID-MDF[6]andIDID-VAE-MDFalgorithms,whichutilizeVAEmethodstogeneratenewdatafrom
agentjâ€™shistoricaldata,andthenemploytheMDFmetrictoselectthetop-kbehaviorsofagentj.
â€¢ TheIDID-Random,IDID-VAE-MDF,andIDID-VAE-ICDalgorithms,whichdifferinhowtheyselectthe
top-K behaviorsfromthepolicytreesgeneratedbyVAE.IDID-Randomselectsthebehaviorsrandomly,while
IDID-VAE-MDFandIDID-VAE-ICDutilizeMDFandICDrespectivelyandIDID-VAE-BCELossreplaces
theproposedtreelosswiththetraditionalBCELoss.
Allsevenalgorithms(IDID,IDID-MDF,IDID-VAE-MDF,IDID-GA,IDID-Random,IDID-VAE-ICDandIDID-VAE-
BCELoss)usethesameunderlyingalgorithmtosolvetheI-DIDmodel,differingonlyinhowtheyexpandthemodel
nodeswithcandidatemodelsforagentj.
Toevaluatethealgorithms,weconsidertheaveragerewardsreceivedbyagentiduringinteractionswithagentj. We
randomlyselectonebehaviormodelofagentj asitstruemodelfromthesetofM nodes. Agentithenexecutesits
j
12VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
GR(Growls from Right) GR(Growls from Right)
L(Listen) OL(Open Left)
Figure6: Amulti-agenttigerproblem,beingsimplifiedtothetwo-agentversion,wheremodelingagentjâ€™sbehavioris
tooptimizeagentiâ€™sdecision. Theproblemspecificationfollows: S =2,A = A =3,â„¦ =6,and â„¦ =2.
i j i j
| | | | | | | | | |
optimalI-DIDpolicy,whileagentj followstheselectedbehavior. Ineachinteraction,agentiaccumulatesrewards
basedontheactionresultsateachtimestepovertheentireplanninghorizonT. Werepeattheinteraction50timesand
computetheaveragerewardforagenti.
5.1 Multi-agenttigerproblems
Thetwo-agenttigerproblemservesasacanonicalbenchmarkforassessingtheperformanceofmulti-agentplanning
models. InFig.6,agentiandagentj mustmakeachoicebetweenopeningtheright/leftdoor(OR/OL)orlistening(L)
todeterminethetigerâ€™sposition,givenlimitedvisibility. Ifanagentopensthedoorwithgoldbehindit,itwilltakethe
gold;however,ifthetigerisbehindthedoor,theagentwillbeeaten. Meanwhile,ifbothagentssimultaneouslyopen
thedoorwiththegold,theywillsharetherewardequally. Theagentsâ€™decisionsareinfluencedbytheirobservations,
whichmaynotalwaysbeaccurate. Forinstance,asqueakemanatingfromadoorcouldbemistakenforanotheragentâ€™s
voice,thetigerâ€™sgrowl,oramisinterpretationofthesound.
5.1.1 DiversityandMeasurements
We first explore how the top-K selection algorithm impacts the diversity of the policy tree set and investigate the
relationshipbetweentheselectioncriteriaofthepolicytree. Toevaluatethediversityofthetop-K policytreesetfor
agentj generatedbyIDID-VAE-MDFandIDID-VAE-ICDintheTigerproblem,weconductedtheexperimentsonthe
correlationbetweentheK valuesandthediversity(MDFevaluationmetric). AsshownintheFig.7,thereisasmall
diversitygapbetweenthetop-K policytreesselectedusingtheICDindexandthoseselectedusingthediversityindex
MDF.ThisverifiesthatthepolicytreesgeneratedbyVAEgenerallyhavegooddiversity,whichmakesitdifficultfor
MDFtodistinguishbetweenpolicytreesandselectpossiblytruebehaviors.
IDID-VAE-MDF 18 IDID-VAE-MDF
22.5 IDID-VAE-ICD IDID-VAE-ICD
20.0 16
17.5 14
15.0 12
12.5 10
10.0 8
7.5 6
5.0 4
2.5
1 2 3 4 5 6 7 8 9 10 11 1 2 3 4 5 6 7 8 9 10 11
K K
(a) T=3 (b) T=4
Figure7: For(a)T =3and(b)T =4,givendifferentK values,thediversityofthetop-K policytreesgeneratedby
IDID-VAE-MDFandIDID-VAE-ICDrespectively.
To explore the correlation between the average reward obtained by the subject agent i and the corresponding met-
rics(MDFandICD),weselecttheobtainedpolicytreesetusingvariousevaluationmetricstodecidethetop-K set. The
13
ytisreviD ytisreviDVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
selectedmetricsarethennormalized. Finally,wemeasuretheaveragerewardthatagenticanachieveusingthispolicy
treeset. AsshowninFig.8,therewardsobtainedbyagentiincreaseasthecorrespondingmetricsrise. Thex-axiswith
thelabeldÂ¯representsthenormalizedvaluesofICDandMDFmetrics,processedusingmin-maxnormalization. Under
thepolicytreesselectedbyMDF,theaveragerewardsofagentiincreasewithMDF,butthereseemstobeanupper
limit. ThepolicytreesselectedbyICDappeartobelessstable. AttimesliceT =4,thereisanoverallupwardtrend.
IDID-VAE-MDF IDID-VAE-MDF
2 IDID-VAE-ICD 2 IDID-VAE-ICD
0 0
-2
-2
-4
-4
-6
-6
-8
-8
-10
-10
-12
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
d d
(a) T=3 (b) T=4
Figure 8: For (a) T = 3 and (b) T = 4, the correlation between the average rewards obtained by agent i and the
correspondingmetrics(MDFandICD)referstotherelationshipbetweentherewardsobtainedusingdifferentmodels,
namelyIDID-VAE-MDFandIDID-VAE-ICD.
5.1.2 ComparisonResultsofMultipleI-DIDAlgorithms
Weinvestigatethequalityofthemodel,specificallytheaveragerewards,andcomparemultipleI-DIDalgorithms. To
optimizeitsdecision-making,agentimustanticipateagentjâ€™sactionsconcurrently. WeconstructanI-DIDmodel
tailoredforagentianddevisethemodelspaceM foragentj throughmultipleI-DIDalgorithms.
j
100
IDID 100 IDID
IDID-MDF IDID-MDF
50 IDID-VAE-MDF 50 IDID-VAE-MDF
IDID-RANDOM IDID-RANDOM
0
0
50
50
100
100
150
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Agent j's Behaviours Agent j's Behaviours
(a) T=3 (b) T=4
Figure9: Theaveragerewardsarereceivedbythesubjectagentiwithagentjâ€™sbehaviorsmodels(generatedbyIDID,
IDID-MDF,IDID-VAE-MDFandIDID-Random)for(a)T =3and(b)T =4.
InFigs.9and 10, theresultsshowtheaveragerewardsforagentiusingvariousI-DIDmethodsforT =3andT
= 4. The original I-DID gives agent j six historical models (M = 6). We compared IDID-MDF, IDID-Random,
IDID-VAE-MDF,IDID-GA,IDID-VAE-ICD,andIDID-VAE-BCELoss,pickingthetop10modelsforagentj. Wefind
thatIDID-MDF,IDID-VAE-MDF,IDID-GA,andIDID-VAE-ICDallbeattheoriginalI-DID.Notably,IDID-VAE-ICD
14
sdraweR
egarevA
sdraweR
egarevA
sdraweR
egarevA
sdraweR
egarevAVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
60
IDID-VAE-MDF 40 IDID-VAE-MDF
IDID-VAE-ICD IDID-VAE-ICD
40 IDID-GA 30 IDID-GA
IDID-VAE-BCELoss IDID-VAE-BCELoss
20 20
10 0
0
20
10
40
20
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Agent j's Behaviours Agent j's Behaviours
(a) T=3 (b) T=4
Figure10: Theaveragerewardsarereceivedbythesubjectagentiwithagentjâ€™sbehaviorsmodels(generatedby
IDID-VAE-MDF,IDID-VAE-ICD,IDID-GAandIDID-VAE-BCELoss)for(a)T =3and(b)T =4.
andIDID-MDFdosimilarlywell,probablybecausethetigerproblemisnâ€™ttoocomplex. Theseimprovedmethods
arebetteratmodelingandpredictingagentjâ€™struebehavior,aidingagentiâ€™sdecisions. Inthetests,IDID-VAE-ICD
outperformedIDID-VAE-ICD-BCELoss. ThisisbecauseIDID-VAE-ICDusesaspecializedlossbasedonpolicytree
nodesâ€™importance,whereasIDID-VAE-ICD-BCELossreliesonthestandardbinarycross-entropyloss. Inaddition,the
VAE-basedI-DIDsolutionsoutperformtheIDID-GAandshowtheirpotentialingeneratingagentjâ€™struebehaviors
fromtheknownmodels.
5.2 Multi-agentUAVproblems
agent j
agent i
Figure11: Inatwo-agentUAVproblem,agentiaimstocaptureagentj beforeagentj reachesthesafehouse. The
problemspecificationfollows: S =81, A = A =5,and â„¦ = â„¦ =4.
i j i j
| | | | | | | | | |
Themulti-agentUnmannedAerialVehicle(UAV)problemposesasignificantchallengeintherealmofmulti-agent
planning.AsdepictedinFig.11,bothUAVs,referredtoasagents,havetheoptiontomoveinfourdifferentdirectionsor
remainstationary. Inlinewithrealisticscenarios,theUAVsareunabletoascertaintheprecisepositionsofotheragents
andcanonlyreceivesignalsrelativetoeachother. Here,wedesignateagentiasthechaser,taskedwithintercepting
thefleeingagentj,whileagentj aimstoreachasafehouse. Sincebothagentsoperateconcurrently,agentirequires
anaccurateestimationofagentjâ€™sbehaviorinordertosuccessfullyachieveitsgoal. Agentiwillberewardedifit
successfullyinterceptsagentj beforeitreachesthesafehouse. WeletchaseriuseanI-DIDmodel,providingitwith
potentialtruebehaviormodelsofagentj.
5.2.1 DiversityandMeasurements
Initially, weexplorehowthetop-K selectionalgorithmaffectsthediversityofthepolicytreeset, delvingintothe
correlationbetweenpolicytreeselectioncriteria. AsillustratedinFig.12,bothcurvesdisplaynotablefluctuationsat
smallvaluesofK,buttheygraduallyriseandstabilizeasK increases,ultimatelyresultinginaslightdivergence. By
examiningthetop-K selectionfunctionalongsideexperimentaldatafromtwodistinctproblemdomains,itbecomes
evidentthatasmallK valuecorrespondstoawiderangeofpotentialpolicytreesetcombinations,therebyaugmenting
15
sdraweR
egarevA
sdraweR
egarevAVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
the diversity within these sets. This effect may not be immediately obvious in less complex problem domains.
Nonetheless,asthecomplexityofthedomainincreases,sodoesthepotentialdecisiontreespace. Hence,whenselecting
onlyafewpolicytreesfromthisexpandedspace,thereisasubstantialchanceofencounteringsignificantdisparities
among them. According to the diversity function, smaller K values are associated with a limited number of sets,
restrictingthepotentialpolicytreepathsandsub-treesthatcanbeuncovered. Thislimitationsuggeststhatthediversity
valuelinkedtosmallerK willnotsurpassthatoflargerK values. AsK increases,thefinalconvergencevalueof
IDID-VAE-ICD is lower than that of IDID-VAE-MDF, indicating its superiority in facilitating the selection of an
appropriatepolicytreeset. Inessence,continuouslyaugmentingthenumberofpotentialpolicytreesdoesnotinherently
leadtogreaterdiversity,nordoesitensureahigherlikelihoodofdiscoveringthetruebehaviorofagentj.
IDID-VAE-MDF IDID-VAE-MDF
9 IDID-VAE-ICD IDID-VAE-ICD
12
8
7 10
6 8
5
6
4
4
3
2 2
1 2 3 4 5 6 7 8 9 10 11 1 2 3 4 5 6 7 8 9 10 11
K K
(a) T=3 (b) T=4
Figure12: For(a)T =3and(b)T =4,givendifferentK values,thediversityofthetop-K policytreesgeneratedby
IDID-VAE-MDFandIDID-VAE-ICDrespectively.
Westudythecorrelationbetweenagentiâ€™saveragerewardandtwometrics: MDFandICD.Aftercarefullyselecting
thetop-K policytreesfromtheavailablesetusingvariousmetricsandnormalizingthemviamin-maxnormalization
(dÂ¯),wediscoveradirectcorrelationbetweenthesemetricsandagentiâ€™saveragereward,asillustratedinFig.13. Inthe
Tigerproblem(refertoFig.8),whiletheICDmetricdoesnotstronglycorrelatewiththeaveragerewardcompared
totheMDFmetric,bothstillensurerespectableaveragerewardsforthechosenpolicytrees. However,intheUAV
problem,theICDmetricdemonstratesastrongercorrelationwithaveragerewardsthantheMDFmetric. Thissuggests
thattheMDFmetricmaynotconsistentlyidentifythetruepolicytree,particularlygiventhevarietyoftreesgenerated
byVAE.Incontrast,theICDmetricensuresbothdiversityandacloseralignmentwiththeactualbehaviormodel. This
reinforcesourobservationthatVAE-generatedpolicytreesexhibitdiversity,posingachallengeforMDFinrecognizing
andselectingpotentialrealbehaviormodels. Whenpresentedwithabroadarrayofpolicytrees,theMDFmetricfinds
itdifficulttodiscernwhichonescloselyalignwiththetruedistributionofagentjâ€™sbehaviors.
5.2.2 ComparisonResultsofMultipleI-DIDAlgorithms
Weassessmodelperformancebycomparingtheaveragerewardsacrossmultiplemodels. Foragentitoimproveits
decisions,itmustpredictagentjâ€™sactions. WearetailoringanI-DIDmodelforagentiandbuildingamodelspaceM
j
foragentj usingvariousI-DIDtechniques. Theobjectiveistomeasuretheaccuracyofthesemethodsinmodelingand
predictingagentjâ€™sbehavior,therebyinfluencingagentiâ€™stargetinterceptionsuccess.
Figs. 14 and 15 compare the average rewards in a multi-agent UAV setting. Among the various methods tested,
includingmultipleIDIDtechniques,IDID-VAE-ICDstandsout,outperformingtheoriginalI-DIDandotherapproaches.
InFigs.14(T=3)and 15(T=3),IDID-G,IDID-VAE-BCELoss,andIDID-VAE-ICDsurpassothermodelsdueto
theircapabilitytogeneratediversepolicytrees,comprehensivelycoveringthetruemodel. Notably,inFigs.14(T=
4)and 15(T=4),unlikeIDID-GAandIDID-VAE-BCELoss,IDID-VAE-ICDshinesinpolicytreegenerationand
selection. Thissuccessispartlyattributedtothelossfunctionweproposed,whichassignsgreaterweighttotheearly
nodesofthepolicytreeinlong-termdecision-making,andpartlyattributedtotheICD,theconstructionindexofthe
policytreeset. TheVAEmodel,combinedwiththeICDindexandourpolicytreelossfunction,effectivelyconstructs
theauthenticsubspacewithinthevastpolicytreespace,demonstratingitsabilitytoaccuratelygenerateagentjâ€™strue
behaviorsfromhistoricalinteractivedata.
16
ytisreviD ytisreviDVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
70 IDID-VAE-MDF IDID-VAE-MDF
IDID-VAE-ICD 60 IDID-VAE-ICD
60
50
50
40
40
30
30
20
20
10
10
0
0
-10
-10
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
d d
(a) T=3 (b) T=4
Figure 13: For (a) T = 3 and (b) T = 4, the correlation between the average rewards obtained by agent i and the
correspondingmetrics(MDFandICD)referstotherelationshipbetweentherewardsobtainedusingdifferentmodels,
namelyIDID-VAE-MDFandIDID-VAE-ICD.
30
IDID IDID
IDID-MDF 60 IDID-MDF
20
IDID-VAE-MDF IDID-VAE-MDF
IDID-RANDOM IDID-RANDOM
10 40
0
20
10
0
20
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Agent j's Behaviours Agent j's Behaviours
(a) T=3 (b) T=4
Figure14: Theaveragerewardsarereceivedbythesubjectagentiwithagentjâ€™sbehaviorsmodels(generatedbyIDID,
IDID-MDF,IDID-VAE-MDFandIDID-Random)for(a)T =3and(b)T =4.
5.3 ExperimentalSummaryandDiscussions
AftercomparingIDID-VAE-ICDandIDID-VAE-MDF,wefindthatwithintheVAEframework,evaluatingpolicy
tree diversity using MDF is less effective. This is because VAE can generate more novel paths, maintaining high
diversityamongnewtrees,whichisafeatureabsentinthepreviousmethods. ICDleveragestheinformationfrom
theVAE-basedpolicytreegenerationprocess,allowingforanintuitivecomparisonofdifferentpolicytreesâ€™abilityto
representhistoricalbehaviorcharacteristics.
OurexperimentsrevealthattheVAEframeworkoffersalinearrelationshipbetweentreegenerationspeedandplanning
range,providingthespeedincreaseoverthepreviousmethods,asdepictedinTable1. AlthoughIDID-MDF,IDID-VAE-
ICD,andIDID-GAtakelongerthanthebasicIDIDduetotheiterationsforthenewmodelgeneration,IDID-VAE-ICD
provesmoreefficientthanIDID-GA.However,sinceallthesealgorithmsoperateoffline,theirefficiencydoesnâ€™thinder
theoverallperformance. Additionally,IDID-VAE-ICDoutperformsIDID-VAE-ICD-BCELossthankstoitscustomized
lossfunctiondesignedforpolicytreenodeimportancedistribution.
ComprehensiveexperimentsshowthatbothIDID-MDFandIDID-VAE-ICDoutperformIDID-GAandIDIDinmost
cases. IDID-VAE-ICDconsistentlyperformswellduetotheVAEâ€™sabilitytobalancecoherenceandindividualityin
17
sdraweR
egarevA
sdraweR
egarevA
sdraweR
egarevA
sdraweR
egarevAVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
250
IDID-VAE-MDF IDID-VAE-MDF
80
IDID-VAE-ICD IDID-VAE-ICD
IDID-GA 200 IDID-GA 60
IDID-VAE-BCELoss IDID-VAE-BCELoss
150
40
100
20
50
0
0
20
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Agent j's Behaviours Agent j's Behaviours
(a) T=3 (b) T=4
Figure15: Theaveragerewardsarereceivedbythesubjectagentiwithagentjâ€™sbehaviorsmodels(generatedby
IDID-VAE-MDF,IDID-VAE-ICD,IDID-GAandIDID-VAE-BCELoss)for(a)T =3and(b)T =4.
behaviors,whileIDID-MDFtendstoproducemonotonicbehaviors. Althoughfindingoptimalparametervaluesremains
crucial,theVAE-basedI-DIDsolutionshavedemonstratedpotentialinmodelingunexpectedbehaviorsthatclassicalAI
approachescanâ€™tachieve. DespitetherandomnessintheVAEnetwork,experimentalresultsshowstableperformance
intermsofaveragerewards,supportedbyouranalysis. Overall,IDID-VAE-ICDexhibitsstrongadaptabilitytocomplex
behavioralpatterns,maintainingstableperformanceinuncertainenvironments,pavingthewayforfutureintelligent
decision-makingsystems.
Table1: Runningtimes (sec)forallthefourmethodsinthetwoproblemdomainsgenerating,specificallyfocusingon
âˆ¼
generatingM =10policytrees.
Domain Tiger UAV
T 3 4 3 4
IDID 0.26 32.41 8.16 406.57
IDID-GA 8.8 45.1 27 637.18
IDID-MDF 442.94 573.85 81.96 97.40
IDID-VAE-ICD 2.09 5.36 9.23 41.84
6 Conclusion
Weexploredneuralcomputing-basedI-DIDmethodstoaddressunpredictableagentbehaviorsinplanningresearch.
Using a VAE approach, we overcame the challenge of reusing incomplete policy trees from interactive data. The
VAEmodelnotonlygeneratesnumerousnewpolicytrees,bothcompleteandincomplete,fromalimitedsetofinitial
examples,butalsooffersflexibilityinproducingvaryingdegreesofdeviation. Thisallowsforthecreationofabroader
rangeofpolicytrees. Furthermore,theintegrationofanovelperplexity-basedmetricwithinourVAE-basedapproach
hassignificantlyenhancedthediversityoftheoverallpolicytreeensemble. Bymaximizingthecoverageofagentjâ€™s
truepolicymodels,thenewmethoddemonstratespromisingpotentialforaccuratelyapproximatingrealagentbehavior
modelsfromhistoricaldata. FollowingtheVAEtrainingwithaselectionofincompletepolicytrees,thisdata-driven
approachhasthecapabilitytobeappliedtoincreasinglydiverseandcomplexdomains,highlightingitsadaptabilityand
robustness.
Thisworkrepresentsasignificantcontributiontothefieldofagentplanningresearch,providingavaluableframework
forhandlingtheunpredictablebehaviorsofotheragentsinmulti-agentsystems. Italsoopensthedoortoexploitneural
networksbasedapproachesforaddressingtheI-DIDchallenge,whichhasbeenaddressedthroughtraditionalBayesian
approachinthepastdecade. Inthefuture,wewillimprovetheefficiency,accuracy,andinterpret-abilityofourmodels,
thereby increasing their adaptability to the uncertainties of online interactive environments. This will be achieved
throughadvancementsindeeplearningarchitecturesandtheincorporationofexplainableAItechniques.
18
sdraweR
egarevA
sdraweR
egarevAVariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
7 Acknowledgments
ThisworkissupportedinpartbytheNationalNaturalScienceFoundationofChina(GrantsNo.62176225,62276168
and61836005)andtheNaturalScienceFoundationofFujianProvince,China(GrantNo. 2022J05176)andGuangdong
Province,China(GrantNo. 2023A1515010869).
Appendix
A:[TheFourOperatorsinReconstructinganIncompletePolicyTree]Wepresentthepseudo-codetoimplement
thefouroperatorsinreconstructinganincompletepolicytree. TheoperatorsaredescribedinSection4.1andinAlg.2.
AsdepictedinAlg.3,thekeystepsofeachoperatorareasfollows:
â€¢ SplitOperator: Thesplitoperator dividesasequenceintomultiplesub-sequencesbasedonafixedlength,
S
andstoresthemalongwiththeirprobabilitiesintheformofaset.
â€“ Initially,anemptysetHT isinitializedtostorepolicypathsandtheirassociatedprobabilities(line4).
â€“ Thegivenaction-observationsequenceisthensegmentedintomultiplesub-sequencesoffixedlength(line
5).
â€“ Thesesub-sequences, alongwiththeirprobabilities, arestoredinthesetHT (lines6-9), providinga
structuredrepresentationofpolicypaths.
â€¢ UnionOperator: Theunionoperator combinestheelementsofthegivensetofpolicypathsintoasetof
U
setsofpolicytreesbasedonthegroupingoftheirrootnodeactionsandobservationpairs.
â€“ PolicypathsubsetsHT aredefinedandtheirprobabilitiescomputed,whereeachsubsetbeginswitha
ao
specificactionaandobservationsequenceo(lines15-16).
â€“ For every possible observation sequence o that follows action a, the corresponding subsets HT are
ao
collected(lines14-16).
â€“ SetsHTarethenconstructed,encompassingallpolicypathsthatstartwithaparticularactiona(line17).
a
â€“ TheoverallprobabilityP(HT)ofeachHTisdeterminedbysummingtheprobabilitiesofitsconstituent
a a
subsetsHT (line18).
ao
â€“ CombiningthesetsHTandtheirprobabilitiesforallactionsa Ayieldsthecomprehensivesetofpolicy
pathsHT(lines13-19a
).
âˆˆ
â€“ ThecomprehensivesetHTrepresentsallpotentialpolicytreesofdepthT thatcanbederivedfromthe
originalinteractionsequencehL(line20).
â€¢ RouletteOperator: Therouletteoperator randomlyselectsanelementfromagivenset.
R
â€“ Given a set A of elements (a,p) where p is the probability associated with a, a random number p
r
between0and1isgenerated(line24).
â€“ Arunningsump ismaintainedtotrackthecumulativeprobabilitiesencounteredwhileiteratingthrough
c
thesetA(line25).
â€“ Assoonasp exceedsorequalsp ,thecorrespondingelementaisreturned,effectivelyselectingan
c r
elementbasedonitsassociatedprobability(lines26-31).
â€¢ GraphingOperator:Thegraphingoperator convertsthesetofpathsfromthepolicytreeintoatreestructure
G
withinagraphmodel.
â€“ AnedgesetE isconstructedtostorethesub-pathswithinthepolicytree(lines35-41).
â€“ DuplicatesareremovedfromE,ensuringthatedgeswiththesametimeslice,nodevalue(action),and
edgeweight(observation)arenotrepresentedmultipletimes(line42).
â€“ UniquenodesfromthededuplicatededgesetE arethenextractedandcompiledintoanodesetV (line
44).
â€“ Finally,adirectedgraphG(E,V)isgeneratedandplotted,representingthepolicytreeconstructedfrom
theedgeandnodesets(line45).
B:[TheOperatorsinencodinganddecodinganIncompletePolicyTree]TheZZOHencoderanddecoderoperators
aredescribedinSection4.2.1andinAlg.2. Theoperatorsareusedtoperformazigzagtransformationontheactionsof
agivenpolicytree,encodingthemintoabinaryformatusingone-hotencodingtoformacolumnvector. Conversely,it
canalsodecodeagivencolumnvectorfromone-hotencodingbackintoactions,subsequentlyreconstructingthepolicy
tree. AsdepictedinAlg.5,thekeystepsofeachoperatorareasfollows:
19VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Algorithm3:TheFourOperatorsinReconstructinganIncompletePolicyTree
1 â—splitoperator
2 Function (hL,T) â—HT ThL:
3
a1o1,aS 2o2...at,ot,...aLoâ† L S hL
â†
4
HT
â†âˆ…
5 {hT }â†{a(lâˆ’1)T+1o(lâˆ’1)T+1,...,alTolT }lâŒŠ =L T 1âŒ‹
6 forhT hT do
7 P(hâˆˆT){ â†} #(hT a)T/L
8
â—#(hT a)indicatesthenumberoftimeshTappearsinthesequencecountthetimesofhL
HT HT(cid:83) (hT,P )
hT
â†
9 end
10
returnHT
11 â—unionoperator
12 Function (HT) â—HT HT:
U â†U
13 fora Ado
14 foâˆˆ ro â„¦Tâˆ’1do
15 Hâˆˆ aT o â†(cid:83) (hT ao,P(hT ao))âˆˆHT(hT ao,P(hT ao))P(H aT o) â†(cid:80) (hT ao,P(hT ao))âˆˆHTP(hT ao)
16 end
1 17
8
H P(T a Hâ†
T
a)(cid:83) â†oâˆˆ (cid:80)â„¦T oâˆ’ âˆˆ1 â„¦( TH âˆ’T a 1o P,P (H(H
T
aoT a )o))
19 end
20 HT â†(cid:83) aâˆˆA(HT a,P(HT a))
21
returnHT
22 â—rouletteoperator
23 Function (A) â—a A:
R â†R
24 p r random()
â†
25 p c 0
â†
26 for(a,p) Ado
âˆˆ
27 p c p c+p
â†
28 ifp r <=p cthen
29 return a
30 end
31 end
32 returna
33 â—Graphingoperator
34 Function G( HaT) â—Tree â†GHaT:
35 E
â†âˆ…
36 for(hT,P(hT)) T do
37
a1o1,a2o2..âˆˆ .atH ,ot,...aToT hT
â†
38 fort 1,2, T 1 do
39
Eâˆˆ{ E(cid:83)Â· (Â· oÂ·t :aâˆ’t,a}t+1)
â†
40 end
41 end
42 E Deduplicate(E)
â†
43 â—deduplicatetheedgewithsametimeslice,samenodevalue(action)andedgeweight(observation)
44 V Node(E))
â†
45 Tree G(E,V)
â†
46 returnTree
20VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Algorithm4:One-HotEncodingOperator
1 â—One-hotencoding
2 Function (Ëœx) â—x (Ëœx):
I â†I
3 x
R(|Aj|+1)|â„¦ |â„¦|T |âˆ’âˆ’ 11
âˆˆ
4 forl 1,2,
|â„¦|Tâˆ’1
do
âˆˆ{ Â·Â·Â· |â„¦|âˆ’1 }
5 v Ëœx[(l 1)(A j +1)+1:(A j +1)l]
â† âˆ’ | | | |
6 k â†argmax kâˆˆ{1,2,Â·Â·Â·|Aj|+1}v[k]
7 x[(l 1)(A j +1)+1:(A j +1)l] AËœ j[k]
âˆ’ | | | | â†
8 end
9 returnx
â€¢ ZZOHEncoderOperator:
â€“ Initially,weextracttheactionsequencefromthegivenpolicytree T (lines3-12).
H
â€“ Subsequently,foreachactioninthesequence,wegenerateacorrespondingone-hotbinarycoderepre-
sentationp(lines13-16). Thisone-hotencodingisdesignedspecificallyforpolicytrees,utilizingthe
Zig-Zagencodingtechnique.
â€¢ ZZOHDecoderOperator:
â€“ Initially,givenaone-hotbinarycoderepresentationx,wedecodeitbackintoanactionsequencep(lines
20-26).ThedecodingprocessreversestheZig-Zagencoding,accuratelyreconstructingtheoriginalaction
sequence.
â€“ Subsequently,usingthedecodedactionsequencep,wereconstructthepolicytree T (lines27-40). This
H
reconstructionensuresthatthegeneratedtreecloselymatchestheoriginaltree,capturingitsstructureand
behavior.
TheOne-hotencodingoperatorisdescribedinSection4.3.1andinAlg.2. Theone-hotoperator usedtoconvertthe
I
summaryvectoroutputbytheVAEnetworkintoabinaryencodingformatthatmatchestheone-hotencodingofthe
policytree. AsdepictedinAlg.4,thekeystepsofeachoperatorareasfollows: Initializeavectorxtorepresentthe
one-hotencodingofthepolicytree(line3). Then,foreachnodeinthepolicytree(lines4-8).Locatetheindexwiththe
maximumvalueinthecorrespondingsub-binaryrepresentation(line6). Representthatsub-binaryencodingusingthe
enlargedactionspaceAËœ (line7).
j
21VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
Algorithm5:ZZOHEncoder&DecoderOperators
1 â—ZZOHencoderoperator
2 Function ( T) â—x T:
3 â—genZ eratH eactionseâ† quZ enH ceoftree T
H
4 x
R(|Aj|+1)|â„¦ |â„¦|T |âˆ’âˆ’ 11
âˆˆ
5 p â†{a l |a l â†0, âˆ€l âˆˆ{1,2, Â·Â·Â·|â„¦ |â„¦|T |âˆ’âˆ’ 11
}}
6 for(hT,i) T do
7
a1o1,a2âˆˆ o2H ...at,ot,...aToT hT
â†
8 fort 1,2, T do
âˆˆ{ Â·Â·Â· }
9 l |â„¦|(tâˆ’1)âˆ’1 + iâˆ’1 +1
â† |â„¦|âˆ’1 âŒŠ|â„¦|(tâˆ’1)âŒ‹
10 p[l] at
â†
11 end
12 end
13 â—generateone-hotbinarycodeofactionsequencep
14 forl 1,2,
|â„¦|Tâˆ’1
do
âˆˆ{ Â·Â·Â· |â„¦|âˆ’1 }
15 x[(l âˆ’1)( |A j |+1)+1:( |A j |+1)l] â†AËœc j[p[l]]
16 end
17 returnx
18 â—ZZOHdecoderoperator
19 Function (x) â— T x:
Z H â†Z
20 â—generateactionsequencepofone-hotbinarycodex
21 p â†{a l |a l â†0, âˆ€l âˆˆ{1,2, Â·Â·Â·|â„¦ |â„¦|T |âˆ’âˆ’ 11
}}
22 forl 1,2,
|â„¦|Tâˆ’1
do
âˆˆ{ Â·Â·Â· |â„¦|âˆ’1 }
23 v â†AËœc j Â·x[(l âˆ’1)( |A j |+1)+1:( |A j |+1)l]
24 k â†argmax kâˆˆ{1,2,Â·Â·Â·|Aj|+1}v[k]
25 p[l] AËœ j[k]
â†
26 end
27 â—generatepolicytreefromactionsequencep
28
T (cid:83) hT
H â†
29 for(hT,i) T do
30
a1o1,a2âˆˆ o2H ...at,ot,...aToT hT
â†
31 fort 1,2, T do
âˆˆ{ Â·Â·Â· }
32 l |â„¦|(tâˆ’1)âˆ’1 + iâˆ’1 +1
â† |â„¦|âˆ’1 âŒŠ|â„¦|(tâˆ’1)âŒ‹
33 at p[l]
â†
34 ift=T then
35
Ì¸
k
(iâˆ’1)|â„¦|t
+1
â†âŒŠ|â„¦|(Tâˆ’1) âŒ‹
36 ot+1 o k
â†
37 end
38 end
39
hT a1o1,a2o2...at,ot,...aToT
â†
40 end
41 return T
H
22VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
References
[1] Pan,Yinghui,Zhang,Hanyi,Zeng,Yifeng,Ma,Biyang,Tang,Jing,andMing,Zhong."DiversifyingAgentâ€™s
BehaviorsinInteractiveDecisionModels."Int.J.Intell.Syst.37,no.12(2022): 12035-12056.
[2] Ross Conroy, Yifeng Zeng, Marc Cavazza, and Yingke Chen. "Learning Behaviors in Agents Systems with
InteractiveDynamicInfluenceDiagrams."InIJCAI,2015,pp.39â€“45.
[3] Wang, Xindi, Liu, Hao, andGao, Qing."Data-Driven DecisionMakingand Near-Optimal PathPlanning for
MultiagentSysteminGames."IEEE-J-MASS4,no.3(2023): 320-328.
[4] RonaldBuijsse,MartijnWillemsen,andChrisSnijders."Data-DrivenDecision-Making."InDataSciencefor
Entrepreneurship: PrinciplesandMethodsforDataEngineering,Analytics,Entrepreneurship,andtheSociety.
SpringerInternationalPublishing,Cham,2023,pp.239â€“277.
[5] LiAn,VolkerGrimm,YuBai,AbigailSullivan,B.L.Turner,NicolasMalleson,AlisonHeppenstall,Christian
Vincenot,DerekRobinson,XinyueYe,JianguoLiu,EmilieLindkvist,andWenwuTang."Modelingagentdecision
andbehaviorinthelightofdatascienceandartificialintelligence."Environ.Modell.Softw.166(2023): 105713.
[6] Pan,Yinghui,Ma,Biyang,Zeng,Yifeng,Tang,Jing,Zeng,Buxin,andMing,Zhong."AnEvolutionaryFramework
forModellingUnknownBehavioursofOtherAgents."IEEETrans.EmergingTop.Comput.Intell.7,no.4(2023):
1276-1289.
[7] YinghuiPan,JingTang,BiyangMa,Yi-fengZeng,andZhongMing."Towarddata-drivensolutionstointeractive
dynamicinfluencediagrams."Knowl.Inf.Syst.63,no.9(2021): 2431-2453.
[8] Nascimento,Nathalia,PauloAlencar,andDonaldCowan."Self-AdaptiveLargeLanguageModel(LLM)-Based
MultiagentSystems."In2023IEEEACSOS-C,2023,pp.104-109.
[9] Wason, Ritika, ParulArora, DevanshArora, JasleenKaur, SunilPratapSingh, andM.N.Hoda."Appraising
SuccessofLLM-basedDialogueAgents."In2024IEEEINDIACom,2024,pp.1570-1573.
[10] Lu, Jiaying, Bo Pan, Jieyi Chen, Yingchaojie Feng, Jingyuan Hu, Yuchen Peng, and Wei Chen. "AgentLens:
VisualAnalysisforAgentBehaviorsinLLM-basedAutonomousSystems."IEEETrans.VisualComput.Graphics,
2024,pp.1-17.
[11] Zhang,Xijia,YueGuo,SimonStepputtis,KatiaSycara,andJosephCampbell."ExplainingAgentBehaviorwith
LargeLanguageModels."arXiv,2023,eprint2309.10346.
[12] Berger,Uta,AndrewBell,C.MichaelBarton,EmileChappin,GunnarDreÃŸler,TatianaFilatova,ThibaultFronville,
AllenLee,EmielvanLoon,IrisLorscheid,MatthiasMeyer,BirgitMÃ¼ller,CyrilPiou,ViktoriiaRadchuk,Nicholas
Roxburgh,LennartSchÃ¼ler,ChristianTroost,NandaWijermans,TimG.Williams,Marie-ChristinWimmler,and
VolkerGrimm."Towardsreusablebuildingblocksforagent-basedmodellingandtheorydevelopment."Environ.
Modell.Softw.,vol.175,2024,p.106003.
[13] Zhang, Wei, Andrea Valencia, and Ni-Bin Chang. "Synergistic Integration Between Machine Learning and
Agent-BasedModeling: AMultidisciplinaryReview."IEEETrans.NeuralNetworksLearn.Syst.,vol.34,no.5,
2023,pp.2170-2190.
[14] Hazra, Rishi, andLucDeRaedt."DeepExplainableRelationalReinforcementLearning: ANeuro-Symbolic
Approach."InMachineLearningandKnowledgeDiscoveryinDatabases: ResearchTrack,2023,pp.213â€“229.
Cham: SpringerInternationalPublishing.
[15] Belle, Vaishak, Michael Fisher, Alessandra Russo, Ekaterina Komendantskaya, and Alistair Nottle. "Neuro-
Symbolic AI + Agent Systems: A First Reflection on Trends, Opportunities and Challenges." In AAMAS
Workshops,2024,pp.180â€“200.Cham: SpringerNatureSwitzerland.
[16] Kononov, Roman, andOlegMaslennikov."Performingdecision-makingtasksthroughdynamicsofrecurrent
neuralnetworkstrainedwithreinforcementlearning."InProc.2023DCNA,2023,pp.144-147.
[17] Wang, Xu, Sen Wang, Xingxing Liang, Dawei Zhao, Jincai Huang, Xin Xu, Bin Dai, and Qiguang Miao.
"DeepReinforcementLearning: ASurvey."IEEETrans.NeuralNetworksLearn.Syst.,vol.35,no.4,2024,pp.
5064-5078.
[18] Almaeen,Manal,YasirAlanazi,NobuoSato,W.Melnitchouk,MichelleP.Kuchera,andYaohangLi."Variational
AutoencoderInverseMapper: AnEnd-to-EndDeepLearningFrameworkforInverseProblems."InProc.2021
IJCNN,2021,pp.1-8.
[19] Hu,Tianmeng,BiaoLuo,ChunhuaYang,andTingwenHuang."MO-MIX:Multi-ObjectiveMulti-AgentCooper-
ativeDecision-MakingWithDeepReinforcementLearning."IEEETrans.PatternAnal.Mach.Intell.,vol.45,no.
10,Oct.2023,pp.12098â€“12112.
23VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
[20] SenYang,YiZhang,XinzhengLu,WeiGuo,andHuiquanMiao."Multi-agentdeepreinforcementlearningbased
decisionsupportmodelforresilientcommunitypost-hazardrecovery."Reliab.Eng.Syst.Safe.,vol.242,2024,p.
109754.
[21] Le,Ngan,VidhiwarSinghRathour,KashuYamazaki,KhoaLuu,andMariosSavvides."Deepreinforcement
learningincomputervision: acomprehensivesurvey."Artif.Intell.Rev.,2022,pp.1â€“87.
[22] Zhu,Kai,andTaoZhang."Deepreinforcementlearningbasedmobilerobotnavigation: Areview."Tsinghua
ScienceandTechnology,vol.26,no.5,2021,pp.674â€“691.
[23] Wang,Xiangjun,JunxiaoSong,PenghuiQi,PengPeng,ZhenkunTang,WeiZhang,WeiminLi,XiongjunPi,
JujieHe,ChaoGao,andothers."SCC:Anefficientdeepreinforcementlearningagentmasteringthegameof
StarCraftII."InProc.2021ICML,2021,pp.10905â€“10915.
[24] Zeng,Yifeng,andPrashantDoshi."ExploitingModelEquivalencesforSolvingInteractiveDynamicInfluence
Diagrams."J.Artif.Int.Res.,vol.43,no.1,Jan.2012,pp.211â€“255.
[25] Pan, Yinghui, YifengZeng, YanpingXiang, LeSun, andXuefengChen."Time-CriticalInteractiveDynamic
InfluenceDiagram."Int.J.Approx.Reasoning,vol.57,no.C,Feb.2015,pp.44â€“63.
[26] Pan, Yinghui, Yifeng Zeng, and Hua Mao. "Learning Agentsâ€™ Relations in Interactive Multiagent Dynamic
InfluenceDiagrams."InADMI2014.LNCS,2015,pp.1â€“11.Cham: SpringerInternationalPublishing.
[27] StefanoV.AlbrechtandPeterStone."Autonomousagentsmodellingotheragents: Acomprehensivesurveyand
openproblems."arXiv,2017,vol.abs/1709.08071.
[28] MasoumehTabaehIzadi."SequentialDecisionMakingUnderUncertainty."InAbstraction,Reformulationand
Approximation,2005,pp.360â€“361.Berlin,Heidelberg: SpringerBerlinHeidelberg.
[29] Zeng, Yifeng, Prashant Doshi, Yingke Chen, Yinghui Pan, Hua Mao, and Muthukumaran Chandrasekaran.
"ApproximatingBehavioralEquivalenceforScalingSolutionsofI-DIDs."Knowl.Inf.Syst.,vol.49,no.2,Nov.
2016,pp.511â€“552.
[30] GeoffreyE.HintonandRuslanR.Salakhutdinov."ReducingtheDimensionalityofDatawithNeuralNetworks."
Science,vol.313,no.5786,2006,pp.504â€“507.
[31] DiederikP.KingmaandMaxWelling."Auto-EncodingVariationalBayes."CoRR,abs/1312.6114,2013.
[32] Zeng,Yifeng,andPrashantDoshi."ExploitingModelEquivalencesforSolvingInteractiveDynamicInfluence
Diagrams."J.Artif.Intell.Res.,vol.43,2012,pp.211-255.
[33] PrashantDoshi,PiotrJ.Gmytrasiewicz,andEdmundH.Durfee."Recursivelymodelingotheragentsfordecision
making: Aresearchperspective."Artif.Intell.,vol.279,2020.
[34] PrashantDoshi,YifengZeng,andQiongyuChen."GraphicalModelsforInteractivePOMDPs: Representations
andSolutions."Auton.AgentMulti-ag.,vol.18,no.3,2009,pp.376-416.
[35] GmytrasiewiczP.J.andDoshiP."AFrameworkforSequentialPlanninginMultiagentSettings."J.Artif.INntell.
Res.,vol.24,2005,pp.49-79.
[36] Seuken,S.,andS.Zilberstein."Formalmodelsandalgorithmsfordecentralizeddecisionmakingunderuncer-
tainty."Auton.AgentMulti-ag.,vol.17,no.2,2008,pp.190â€“250.
[37] Chen,Yingke,PrashantDoshi,andYifengZeng."IterativeOnlinePlanninginMultiagentSettingswithLimited
ModelSpacesandPACGuarantees."InAAMASâ€™15,2015,pp.1161â€“1169.Richland,SC:AAAIPress.
[38] MingDing."TheroadfromMLEtoEMtoVAE:Abrieftutorial."AIOpen,vol.3,2021,pp.29-34.
[39] Rizk,Yara,MarietteAwad,andEdwardW.Tunstel."DecisionMakinginMultiagentSystems: ASurvey."IEEE
Trans.Cognit.Dev.Syst.,vol.10,no.3,2018,pp.514-529.
[40] Chen,Wenbin,GuoXie,WenjiangJi,RongFei,XinhongHei,SiyuLi,andJialinMa."DecisionMakingfor
OvertakingofUnmannedVehicleBasedonDeepQ-learning."In2021IEEEDDCLS,2021,pp.350-353.
[41] Wang, Xindi, Hao Liu, and Qing Gao. "Data-Driven Decision Making and Near-Optimal Path Planning for
MultiagentSysteminGames."IEEEJ.MiniaturizationAirSpaceSyst.,vol.4,no.3,2023,pp.320-328.
[42] Sun,Changyin,WenzhangLiu,andLuDong."ReinforcementLearningWithTaskDecompositionforCooperative
MultiagentSystems."IEEETrans.NeuralNetworksLearn.Syst.,vol.32,no.5,2021,pp.2054-2065.
[43] Habib, Maki K., Samuel A. Ayankoso, and Fusaomi Nagata. "Data-Driven Modeling: Concept, Techniques,
ChallengesandaCaseStudy."In2021IEEEICMA,2021,pp.1000-1007.
24VariationalAuto-encoderBasedSolutionstoInteractiveDynamicInfluenceDiagrams APREPRINT
[44] Abroshan,Mahed,KaiHouYip,CemTekin,andMihaelavanderSchaar."ConservativePolicyConstruction
UsingVariationalAutoencodersforLoggedDataWithMissingValues."IEEETrans.NeuralNetworksLearn.
Syst.,vol.34,no.9,2023,pp.6368-6378.
[45] Conroy, Ross, Yifeng Zeng, Marc Cavazza, and Yingke Chen. "Learning Behaviors in Agents Systems with
InteractiveDynamicInfluenceDiagrams."InProc.2015IJCAI,2015,pp.39â€“45.BuenosAires,Argentina: AAAI
Press.
[46] Zeng,Yifeng,HuaMao,YinghuiPan,andJianLuo."ImprovedUseofPartialPoliciesforIdentifyingBehavioral
Equivalence."InProc.2012AAMAS,vol.2,2012,pp.1015â€“1022.Richland,SC:AAAIPress.
[47] Doshi,Prashant,YifengZeng,andQiongyuChen."GraphicalmodelsforinteractivePOMDPs: representations
andsolutions."AutonomousAgentsandMulti-AgentSystems,vol.18,no.3,June2009,pp.376â€“416.
[48] Zeng, Yifeng, Ran, Qiang, Ma, Biyang, and Pan, Yinghui. "Modelling other agents through evolutionary be-
haviours."Memet.Comput.14,no.1(2022): 19â€“30.
25