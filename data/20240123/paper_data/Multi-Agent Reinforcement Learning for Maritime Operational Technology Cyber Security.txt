Multi-Agent Reinforcement Learning for Maritime
Operational Technology Cyber Security
AlecWilson1, RyanMenzies1, NeelaMorarji1, DavidFoster2, MarcoCasassaMont1,
EsinTurkbeyler1 and LisaGralewski1
1BMT,London,UK
2ADSP,London,UK
Abstract
Thispaperdemonstratesthepotentialforautonomouscyberdefencetobeappliedonindustrialcontrol
systemsandprovidesabaselineenvironmenttofurtherexploreMulti-AgentReinforcementLearning‚Äôs
(MARL)applicationtothisproblemdomain. Itintroducesasimulationenvironment, IPMSRL,ofa
genericIntegratedPlatformManagementSystem(IPMS)andexplorestheuseofMARLforautonomous
cyberdefencedecision-makingongenericmaritimebasedIPMSOperationalTechnology(OT).
OTcyberdefensiveactionsarelessmaturethantheyareforEnterpriseIT.Thisisduetotherelatively
‚Äòbrittle‚ÄônatureofOTinfrastructureoriginatingfromtheuseoflegacysystems,design-timeengineering
assumptions,andlackoffull-scalemodernsecuritycontrols.Therearemanyobstaclestobetackled
acrossthecyberlandscapeduetocontinuallyincreasingcyber-attacksophisticationandthelimitations
oftraditionalIT-centriccyberdefencesolutions. TraditionalITcontrolsarerarelydeployedonOT
infrastructure,andwheretheyare,somethreatsaren‚Äôtfullyaddressed.
Inourexperiments,asharedcriticimplementationofMultiAgentProximalPolicyOptimisation
(MAPPO)outperformedIndependentProximalPolicyOptimisation(IPPO).MAPPOreachedanoptimal
policy(episodeoutcomemeanof1)after800Ktimesteps, whereasIPPOwasonlyabletoreachan
episodeoutcomemeanof0.966afteronemilliontimesteps.Hyperparametertuninggreatlyimproved
trainingperformance. Acrossonemilliontimestepsthetunedhyperparametersreachedanoptimal
policywhereasthedefaulthyperparametersonlymanagedtowinsporadically,withmostsimulations
resultinginadraw. Wetestedareal-worldconstraint,attackdetectionalertsuccess,andfoundthat
whenalertsuccessprobabilityisreducedto0.75or0.9,theMARLdefenderswerestillabletowinin
over97.5%or99.5%ofepisodes,respectively.
Keywords
Multi-AgentReinforcementLearning,CyberSecurity,OperationalTechnology
1. Introduction
OperationalTechnology(OT)cyberdefensiveactionsarelessmaturethantheyareforEnterprise
IT.Thisisduetotherelative‚Äòbrittle‚ÄônatureofOTinfrastructureoriginatingfromtheuseof
legacysystems,design-timeengineeringassumptions,andlackoffull-scalemodernsecurity
controls. Traditional IT controls are rarely deployed on OT infrastructure, and where they
are, some threats aren‚Äôt fully addressed. Additionally, there may be a lack of trained cyber
CAMLIS‚Äô23:ConferenceonAppliedMachineLearningforInformationSecurity,October19‚Äì20,2023,Arlington,VA
$Alec.Wilson@uk.bmt.org(A.Wilson);Ryan.Menzies@uk.bmt.org(R.Menzies)
(cid:26)0009-0003-9181-9766(A.Wilson);0009-0003-9646-196X(R.Menzies);0000-0001-9002-1187(N.Morarji);
0009-0004-7611-6947(M.CasassaMont)
¬©2023Copyrightforthispaperbyitsauthors.UsepermittedunderCreativeCommonsLicenseAttribution4.0International(CCBY4.0).
CEURWorkshopProceedings(CEUR-WS.org)
CWPrEooUrckR esehdoinpgshISttSpN:// c1e6u1r3-w-0s0.o7r3g
4202
naJ
81
]GL.sc[
1v94101.1042:viXrapersonnelavailableindeployedoperationalOTenvironmentse.g., at-seavessels, hencethe
opportunitytodevelopautonomousdefensivesystems. Reinforcementlearning(RL)[1][2]isa
subsetofmachinelearningthatallowsanAI-drivensystem(sometimesreferredtoasanagent)
tolearnthroughtrialanderrorusingfeedbacksignals(throughrewards)fromitsactions. An
agentexecutesautonomousactionsandisgivenareward(orpunishment)contingentwiththe
consequencesoftheseactionsinanenvironment. Theagentadaptsitsstrategy,referredtoasa
policy,tomaximizecumulativereward.
RLhasseenkeyrecentdevelopmentsinvariousdomainssuchasgameenvironmentse.g.,
Go[3]andrealworldproblemse.g.,videocompression[4]. Manydomains,includingcyber-
security,arenowusingRLtotrainwithinsimulatedenvironments. Thereareafewexamples
ofcyberfocusedRLenvironmentsintheliterature,notably: YawningTitan[5],CyberBattleSim
[6],andtheCybORGenvironment[7]. TheseenvironmentsarefocusedonITnetworksand
currentlylacktheabilitytotrainmultipledefenders. Theexistingenvironmentsdonotreflect
the challenges of cyber defence on an OT Industrial Control System (ICS). IPMSRL aims to
reflectthenuancesofOTmoreaccuratelyinanabstractsimulatorandprovideaplatformfor
defensiveagentstobetrainedtorecoveranIPMSfromacyber-attack.
WeappliedMulti-agentReinforcementLearning(MARL)tothecyberdefenceofICSsbecause
ofthecollaborativenatureoftheproblem.Agentsworkingtogetherdistributedacrossanetwork
takingcoordinatedremedialactionsislikelytobemoresuccessfulthanasingle,ormultiple
uncoordinated,agent(s)workingindependently.
2. Operational Technology Scenario
ThispaperfocusedonagenericIPMS,aformofIndustrialControlSystemusedonboardships
andsubmarines.
An IPMS provides the capability for remote monitoring, control and management of the
ship‚Äôsmachinerysystemsanddamagecontrolfromkeycomponents. Thisreal-timemonitoring
andcontrolfacilitatescontinuoussituationalawarenessofthemachinerystateanddamage
conditionoftheship.
IPMScontrolsandmonitorsmanyshipsystemsacrossPropulsion,Power,Steering,Stability,
AuxiliaryandAncillarysystemsaswellasthoseprovidingDamageControlandFireFighting
(DCFF).Toachievethis,IPMSutilisesadistributedcontrolsystemarchitecturethatfacilitates
interfaceswithsensors,equipment,plants,software-basedcontrolsystemsandnetwork-based
data.
An IPMS produces alerts in event of failures, anomalies, or issues. These alerts are not
necessarilycybersecurityalerts;however,theyarelikelytobeinstrumentalinthecontextof
cyber-attackdetectionandresponse. ThesealertswereassumedtobefedintoadditionalCyber
ThreatDetectionandSecurityInformationandEventManagement(SIEM)tools,leveraging
IPMSinputstodetectandgeneraterelevantattackalerts. TheIPMSarchitectureisbasedupon
traditionalIndustrialControlSystem(ICS)design,inclusiveofRemoteTerminalUnits(RTUs),
ProgrammableLogicControllers(PLCs)andmultifunctioncontrolconsolesprovidingHuman
MachineInterfaces(HMIs)combinedbyadualredundantnetworkbackbone. Specifically,HMI
referstoIPMSconsolesaswellaspanelsontheequipment. Thearchitectureworkhasbeendeliberatelyabstractedandgeneralisedusingopen-sourceinformation.
3. IPMSRL - Simulation Environment
In this paper we introduce IPMSRL, a highly configurable network based multi-agent RL
environment. IPMSRLsimulatesanabstractedgenericmaritimeIPMSwheredefensiveagents
attempttorestoreaninfectednetwork. Meanwhileanattackerattemptstopropagatethrough
thenetworkanddisruptIPMSandcontrolledsystemstonegativelyimpactoperations.
TheIPMSRLnetworkcomprisestwotypesofnode: criticalnodesandnon-criticalinfectable
nodes. Critical nodes represent components of core-controlled systems critical to effective
operation of the vessel. Two controlled systems are modelled, Chilled Water Plants (CWPs)
andthePropulsionSystem. Thisselectionrepresentsadiversesubsetofavailablecontrolled
systems. InfectablenodesrepresentHMIs, RTUs, LocalOperatingPanels(LOPs), PLCs, and
networkswitches.
The properties of the links between nodes, shown in Figure 1, differ depending on their
type. In Figure 1, the star representation of the network is for visualisation purposes only.
Any HMI/RTU/LOP node which is connected to a network switch is adjacent to any other
nodedirectlyconnectedtoanetworkswitch,i.e.,theyareinterconnected. Thenetworkholds
redundancyinitsstructurethroughitsdualringnetworkbackbone.
Figure1:ExampleofanIPMSRLNetworkTopology.3.1. MITREATT&CK¬ÆFramework
MITRE ATT&CK1¬Æ ICS [8], provides detailed analysis of typical Tactics, Techniques, and
Procedures(TTPs)adoptedbycyberattackers,whichisrelevanttoindustrialcontrolsystems.
Fine-grainedattacksteps,basedonMITREATT&CK¬ÆICS,wereimplementedtoenablemore
realism,contextandcomplexitytothecyberdefensiveremedialactions. AtdifferentMITRE
ATT&CK¬Æ ICS stages the attacker will have different capabilities e.g., lateral movement by
leveraginganHMI‚Äôsnetworkcardandconnections. Differentdefensiveremedialactionsare
requireddependingontheMITREATT&CK¬ÆICSstagethattheattackerhasexploited.
The twelve attack stages, represented in IPMSRL, correspond to the associated MITRE
ATT&CK¬ÆICSTacticsareshowninFigure2:
Figure2:MITREATT&CK¬ÆICSTacticswiththenumberoftechniquesforeachtactic[8].
3.2. Attacker
The goal of the attacker is to compromise the vessel‚Äôs operational capacity, for example, by
disruptingIPMSsystemsandcontrolledsystems(CWPs,PropulsionSystem)whichnegatively
impactsthevessel. Anattackerpropagatesthroughthenetworkbyinitiallyinfectinganon-
criticalinfectablenode,increasingthenode‚ÄôsinfectionlevelusinganabstractionoftheMITRE
ATT&CK¬Æ ICS [8], framework, and then infecting an adjacent node. Any given node after
infectionactsindependently. Itisthereforepossiblethatifmultiplenodesareinfectedtheneach
infectednodecanprogresstheinfectionorlaterallymoveinthesametimestep. Thismakesit
verydifficultforthedefender(s)torecoverthenetworktoacleanstateonceaninfectionhas
spread.
Attackerscanbecreatedonaslidingscalefromfullytargetedtofullyviral. Fullytargeted
attackersmovedirectlytowardscriticalnodesdisplaying‚Äòknowledge‚Äôofthenetwork. Fully
viralattackersmoverandomlytoanyadjacentnode. Partiallyviralattackersmovetowards
criticalnodesbutmayalsomoverandomlywithagivenconfiguredprobability.
Attackersalsohaveotherconfigurableattributessuchastheprobabilityaninfectionwill
progressthroughtheMITREATT&CK¬ÆICSstagesortheprobabilityofasuccessfullateral
move.
3.3. Alerts
Each of the infectable nodes has an alert system which can be triggered when an infection
is present or when an attacker initially tries to infect a non-infected node. The probability
thatanalertissuccessfullyparsedtothedefender(s)allowstheusertoconfigurethescaleof
asymmetricinformationwithintheenvironment.Inthecontextofthispaper,anassumptionwas
1¬©2023TheMITRECorporation. ThisworkisreproducedanddistributedwiththepermissionofTheMITRE
Corporation.madethatIPMS-generatedalertswerefedintoaSIEMforfurtherprocessingandcyberthreat
detection. Thealertsuccessprobability,therefore,directlyimpactsthepartialobservabilityof
thedefender‚Äôsobservationspace.
Itisworthnotingthatthealertsarestaticinnature. Thismeansthatifanalertissetoffata
certaintimestep,thedefenderwillreceiveinformationabouttheprogressofthenodeinfection
atagiventimestep. Nofurtherinformationabouttheprogressionoftheinfectionwillbegiven
tothedefenderunlessanotheralertissetofforthedefendertakesaremedialactiononanode.
3.4. NISTSP-800-61
NISTSP-800-61[9],describesastandardprocesstohandlecyberincidentsandresponses. The
three key cyber defensive steps relevant to the problem space are Contain, Eradicate, and
Recover,showninFigure3. Eachofthesetypesofactionneedtobetakeninalogicalorder
basedoncyberdefensivebestpractice. Ingeneral, thiswillbetocontainanattacktoavoid
it spreading across systems, eradicate the attack to remove it from the infected system and
recoveranodetoreturnthesystembacktoanoperationalstate. Theorderoftheseactionsis
importantandotherfactorssuchastheseverityofinfectionimpacttheeffectivenessandease
oftakingtheseactions.
Figure3:SummaryofNISTSP-800-61RemedialActions,adaptedtotargetedMaritime/OTscenario.3.5. DefensiveRemedialActions
Eachdefensiveagentcantakefourtypesofaction: Contain,Eradicate,RecoverandWait. These
arealignedwiththereal-worldcyberincidentresponseprocessindicatedinNISTSP-800-61
discussedabove.
Containingmakesagivennodeun-operationalbutpreventstheinfectionfrompropagating
from that node. Eradicating cleans a node of infection. Recovering returns the node to an
operationalstate. Forcriticalnodes,onlytherecoverandwaitactionsareavailablewhereas,for
allothernodes,allactionsareavailable. Ateachtimestep,eachdefenderinsuccessiontakesan
action.Eachoftheseactionshaveaconfigurabledelaywhereatimesteplagisapplieddepending
ontheinfectionstateofthenodeandtypeofaction. Thisdelaytriestoreflectthereal-world
differencesbetweentakingsimpleactionsagainstearlyinfectionse.g.,modifyingcredentials,
andtakingmoreextremeactionstopreventunrecoverabledamagee.g.,quarantiningsystems.
Additionaldialogueonvaluingthecostsofremedialactionsisconsideredinlaterdiscussion
abouttherewardfunction.
IPMSRLsupportspartialobservabilityinwhicheachdefensiveagenthasitsownviewof
the network state. In addition to the alerts, defensive agents can investigate a node‚Äôs true
infectioninformationbyinteractingwiththatnode. Thisknowledgeisnotsharedamongst
otherdefensiveagents.
3.6. RewardFunction
IPMSRLfeaturesahighlycustomisablerewardfunctionforbothglobalandintrinsicrewards.
Globalrewardsareawardedtotheagentsattheendoftheepisode,whereasintrinsicrewards
areawardedwithinanepisode2. Globalrewardshavebeensubdividedintomissionobjective
and state rewards. The implemented reward function aims to tackle the problem of sparse
globalrewards,whereagentsstruggletotrainsincetheyonlyreceiveasmallnumberofreward
signals, often at the end of an episode. This is a known problem within RL, and specifically
withinappliedcontrolproblemssuchasrobotics[10]. TherewardfunctionusedinIPMSRL
uses a combination of intrinsic rewards and reward shaping [11] [12] to allow for further
experimentationintohowtheseadditionspositively/negativelyaffecttraining. Themission
objectiveisdefinedasfollowsforwin/loss/draw. Forepisodestepùë°andmaxstepsinanepisode
ùëá,awin,draworlossisdefinedas:
‚Ä¢ Win(+1)‚ÄìTherearenoinfectednodesorcontainednodes,andùë° < ùëá.
‚Ä¢ Loss(-1)‚ÄìAnyofthecriticalinfrastructurehasbeeninfected,andùë° < ùëá.
‚Ä¢ Draw(+0)‚Äìùë° = ùëá andneitherthewinnorlosscriteriahasbeenmetfortsteps.
Forourexperiments,ùëá = 50.
Thestaterewardreflectstheimpacttonon-criticalandcriticalnodesduringtheepisodeand
isgradedaslow,mediumorhigh. Criticalnodes,suchasPLCs,RTUsandPCSsthatdirectly
controlmachineryorswitches,aremorestronglypenalisedthanothernodes,suchasHMIs.
2Theglobalandintrinsicrewardsaredividedandsharedequallybetweeneachagent.Theweightingofpenaltieswasinformedbythefailureeffectsandimpactanalysisofageneric
IPMS.Staterewardisproducedbya50/50splitof‚Äòstategeneric‚Äôand‚Äòstatespecific‚Äôrewards:
StateGeneric‚Äìthisreferstopenaltiesthatareprovidedwhennodeswithintheenvironment
meetcertainconditionse.g.,anodeiscontainedbutnotinfectedoranodeisuninfectedand
containedwithoutbeingrecovered.
StateSpecific‚Äìthisisdefined,inlevelsofseverity,asthenumberofdifferenttypesofnodes
whichhadbeeninfectedinanepisode. Forexample,iftwoHMIswereinfectedthenthesmallest
negativerewardvalue(definedintheconfig)isapplied.
IPMSRLhastheabilityforagentstoreceiveintrinsicrewardscalledactionscorerewards.
Thedefender(s)receiveanegativerewarddependingontheactionchosenandthestatusofthe
nodethatisbeinginteractedwith(gradedaslow,mediumorhigh). Nodeswithamoresevere
infection status receive a higher penalty. The agents are therefore incentivised to deal with
infectionsearlywhentheyarefirstalerted. Theweightingoftherewardfunctioncomponents
areuserconfigurableandwillbediscussedinfurtherdetailwithintheexperimentalresults
section.
4. Multi-Agent Reinforcement Learning (MARL)
In this paper, two MARL algorithms, IPPO [13], and MAPPO [14], were tested3. Our imple-
mentationofIPPOisequivalenttotheIPPOalgorithmreferencedintheoriginalpaper[13].
InIPPO,eachagenthasanactorandcriticnetworkwiththeactornetworkrepresentingthe
policy,andthecriticnetworkrepresentingthevaluefunction. Theseactorandcriticnetworks
areindependenttoeachagent. OurMAPPOimplementationisslightlydifferenttotheMAPPO
algorithmusedintheoriginalpaper[14]. TheoriginalpaperstatesthattheirMAPPOalgorithm
usessharedparametersforboththepolicyandcriticnetworks. TheMAPPOimplementation
usedinthispaperusesindependentpolicynetworksforeachagent,butusesasharedcentralised
criticalsoreferredtoasacentralisedvaluefunction. Animportantnoteisthatinotherpapers,
some MAPPO implementations use shared information between agents by using the joint
observationsandactionspaces4. BothProximalPolicyOptimisation(PPO)basedalgorithms
werestableduringtraininganddemonstratedtheabilitytorecoveraninfectednetworkwithin
IPMSRL.Twodefensiveagentswereusedforalltheexperimentsinthispaper. Thehardware
usedfortrainingwasa6coreCPU,56GBRAMandaNVIDIATeslaK80with12GBsofvRAM.
Ray Tune [16] and RLlib were utilised for training. Training the PPO based algorithms for
one million timesteps took approximately one hour. The performance metrics used for the
experimentsweremeanepisodereward,meanepisodelengthandmeanepisodeoutcomewhich
isthemeanoftheepisodeoutcomes(win(+1)/draw(+0.5)/loss(0)).
4.1. HyperparameterTuningwithMAPPO
An Initial hyperparameter tuning stage to create a baseline for the environment parameter
experimentswascompleted.Abriefgrid-searchovercommonlysuccessfulhyperparameterswas
3Multi-AgentDeepDeterministicPolicyGradient(MADDPG)andQ-MIXwerealsoexploredbuttheimplementations
withinRayandRLlibwerefoundtobeunstableduringtrainingonIPMSRL.
4ThisisthesameastheimplementationwithinMARLlib[15],usingtheglobalstatewithintraining.usedforelevenPPOspecificandgeneralmachinelearninghyperparameters. InFigure4,aclear
differencebetweenthetrainingperformancebetweenthetunedanddefaulthyperparameters
canbeseen. Thehyperparameterswhichweremostinfluentialintheimprovedperformance
weretheLearningRate,Gamma,LambdaandtheClipParameter. WhenMAPPOwastrained
using the default hyperparameters, it converged to an optimal policy more slowly than the
tuned model, but it was still able to reach an optimal policy. MAPPO was also used during
theExperimentalResultssection. Throughouttheexperiments,itwasfoundthatingeneral,
PPObasedalgorithmswerestillabletoimprovetheirperformancesduringtrainingwithmost
configurationsofhyperparameters.
Figure4:ComparisonbetweendefaultandtunedhyperparametersonMAPPOwith90%CI.
4.2. IPPOvsMAPPO
MAPPOoutperformedIPPO,convergingtotheoptimalpolicyquicker. Thisstatementisbased
onthehigherwinrate,higherrewardtotalandtheabilitytowinanepisodeinfewerepisode
steps,suggestingamoreefficientstrategyhasdeveloped. Thepointatwhichthetwoalgorithms
divergeinFigure5underpinssomeofthedifferencesinperformance. At200KtimestepsIPPO
andMAPPOhavesimilarperformanceandaredrawingmostepisodes. Afterthispoint,both
IPPOandMAPPOstarttodevelopawinningpolicybutwithMAPPOoptimisingitspolicyata
fasterrate. Inourexperimentation,MAPPOclearlygainsanadvantageusingthesamecritic
network. Thecentralisedcriticallowsfortheagentstofindabetterpolicywhereagentsare
collaborativefasterthanwithoutthesharedcriticnetwork. Thisistheconversetowhathas
beenfoundelsewhereintheliterature,wheretheadditionofasharedvaluefunctionreduced
theperformancesignificantly[13].
MAPPO‚Äôsconfidenceinterval(90%)bandsweremuchnarrowerthanIPPO.Thenarrower
areaoftheconfidenceintervalsuggeststhattheindividualtrialsofMAPPOalgorithmwere
morestablethanIPPO,implyingthetrainingismoreconsistent. Thisexperimentdemonstrates
thatthecentralisedcriticcanhelpimprovethepolicyupdatesateachtrainingepoch.Figure5:IPPOvsMAPPOExperimentwithaCIof90%
5. Experimental Results
5.1. AlertSuccessProbability
Aconfigurableattackerparameteristhealertsuccessprobability: thisistheprobabilityofan
alertbeingsetoffonanyinfectednodeafteraninfectiononanodeprogressessuccessfully
laterallytoanothernode.
Figure6:AlertSuccessProbabilityExperiment.
Figure 6.1 shows that the episode outcome mean for probabilities 0.00 (blue plot) and 0.1
(orange plot) converges to 0.5. This suggests that almost every episode is ending in a draw.
Figure6.3showstheyallconvergetoepisodelengthmeanof50. Thisimpliesthedefenderis
notreceivingenoughinformationtowinbutismanagingtodrawnearlyallepisodes.
In Table 1, the impact of the defenders receiving more accurate information about theenvironment is clear. The more accurate the information they receive, the better they can
perform. Withoutanalertsuccessprobabilityof1,withinthetrainingtimeset,noneofthe
parameterswereabletoproduceanoptimalstrategy.However,anagentwithanalertprobability
of0.75or0.9wasstillabletowininover97.5%or99.5%ofepisodes,respectively. Theseresults
suggestthattheagentscanlearneffectivestrategiesinpartiallyobservableenvironments,but
thattheSIEMsalertdetectionandprocessingisimportanttothesuccessofautonomouscyber
defence.
Table1
AlertSuccessProbabilityExperimentResults.
Parameter EpisodeOutcomeMean EpisodeRewardMean EpisodeLengthMean
0 0.4963 -4.8858 49.9803
0.1 0.4927 -4.0741 49.7891
0.25 0.5463 -1.5508 43.5021
0.5 0.7913 0.0672 22.9676
0.75 0.9772 0.845 5.8354
0.9 0.9962 0.9454 3.9113
1 0.9995 0.9801 3.2588
5.2. RewardExperiments
InFigure7,aninvestigationintothedifferenceintrainingperformancebetweenstate-based
rewardsandbalancedrewardswasconducted. Statebasedrewardreferstoarewardfunction
solelymadeupofthestaterewardsdiscussedintherewardfunctionsection. Whereasbalanced
rewardalsoincludesintrinsicrewards,giventotheagentsduringanepisodebasedontheir
actions,andanoverallscorebasedontheepisodeoutcome. Usingsolelystaterewards,con-
vergencetowardsawinningstrategywasfasterthanforbalancedrewards,butperformedless
optimallyoverallthanbalancedrewards. Figure7showsthetwoparametersetsplottedwith
90%confidenceintervals,validatingtheirsignificantdifferencesinbehaviour.
Figure7:RewardShapingExperiment-StateRewardvsBalancedRewardwith90%CI.Figure7.1showsthatoncethestate-basedrewardparameterreachesanepisodeoutcome
meanofalmost1,indicatingithaswonalmosteveryepisode,theepisodelengthmean,shown
inFigure7.3reachesavaluearound10stepsperepisode. Itthenplateausforthedurationofthe
training. Thisisafarhighermeanstepsperepisodethanthebalancedrewardagents. Alower
episode length mean suggests that the defender(s), if the win rate is near 100%, find a more
efficientdefensivestrategy. Thebalancedrewardapproach,whichinitiallyconvergestoward
a winning strategy more slowly, reaches an episode length mean of 3.18. This is a dramatic
improvementandrepresentsafarmoreeffectiveandperformantstrategy. Thestatereward
agentshavenoincentivetoimproveorfine-tunetheirbehaviourfurtherthanthecondition
required to reliably win. The agents trained with just state reward are therefore ‚Äôlazy‚Äô with
theiractionchoices,takingmoreactionsthanisnecessarytowinanepisode. Thisexperiment
showsthevalueofrewardshapingandhowimportantthedynamicsofagentincentivesareto
aperformantoutcome.
6. Conclusion and Future Work
ThispaperintroducedIPMSRL,ahighlyconfigurablenetworkbasedmulti-agentRLenvironment
anddemonstratedthecapabilitiesofMARLagentstosuccessfullyrecoveranabstractIPMStoan
operationalstatefollowingacyber-attack. Findingsdemonstratethathyperparametertuning,
rewardshapingandthequalityofalertsareallimportantaspectsindevelopingperformant
agents.
AkeyissuewithusingasimulatortotrainanRLagentisthesim-to-real-gap. Thisdescribes
the differences present between real systems and the simulated environments that are used
to replicate them and the problems that even small differences can cause. The issues are
compoundedfurtherwhenenvironmentsareintentionallyabstractedawayfromtherealsystem
for reasons of dimensionality, security or complexity. RL simulators such as IPMSRL are
importantfordevelopingthecoreconceptsneededtobuildconfidenceandunderstandingin
thetechniquesbeingemployed,suchasMARL.Butmorerealisticsimulatorsoremulatorswill
berequired to facilitatemovementtowards applying theseconcepts onrealsystems, which
shouldbetheeventualgoal.
Another avenue of future research is the generalisability of the agents. Agents will need
to be able to adapt to different types of attack, scenario (where components may be valued
differently)andnetworktopology. Withoutdemonstratingthisflexibility,agentsareunlikely
tobestableenoughtobetrustedinrealworldcontrolsystems.
Acknowledgments
Research funded by Frazer-Nash Consultancy Ltd. on behalf of the Defence Science and
Technology Laboratory (Dstl) which is an executive agency of the UK Ministry of Defence
providing world class expertise and delivering cutting-edge science and technology for the
benefitofthenationandallies. TheresearchsupportstheAutonomousResilientCyberDefence
(ARCD)projectwithintheDstlCyberDefenceEnhancementprogramme.The authors would also like to thank Clare Jubb, Andy Pollard, Christos Giachritsis, Jake
Rigby,BenGolding,JulieKimbreyandBrianBassilfortheirwidercontributiontotheproject
andpaper.
References
[1] A.Barto,S.Bradtke,S.Singh,LearningtoActusingReal-TimeDynamicProgramming‚Äô,
1995.
[2] R.Sutton,A.Barto, Reinforcementlearning: anintroduction,Secondedition, in: Adaptive
computationandmachinelearningseries,TheMITPress,Cambridge,Massachusetts,2018.
[3] D.Silver,A.Huang,C.J.Maddison,A.Guez,L.Sifre,G.vandenDriessche,J.Schrittwieser,
I.Antonoglou,V.Panneershelvam,M.Lanctot,S.Dieleman,D.Grewe,J.Nham,N.Kalch-
brenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, D. Hassabis,
MasteringthegameofGowithdeepneuralnetworksandtreesearch, Nature529(2016)
484‚Äì489.URL:https://doi.org/10.1038/nature16961.doi: .
10.1038/nature16961
[4] A.Mandhane,A.Zhernov,M.Rauh,C.Gu,M.Wang,F.Xue,W.Shang,D.Pang,R.Claus,
C.-H. Chiang, C. Chen, J. Han, A. Chen, D. J. Mankowitz, J. Broshear, J. Schrittwieser,
T.Hubert,O.Vinyals,T.Mann,MuZerowithSelf-competitionforRateControlinVP9
VideoCompression,2022.URL:http://arxiv.org/abs/2202.06626,arXiv:2202.06626[cs,eess].
[5] A. Andrew, S. Spillard, J. Collyer, N. Dhir, Developing Optimal Causal Cyber-Defence
AgentsviaCyberSecuritySimulation‚Äô,2022.URL:http://arxiv.org/abs/2207.12355,volume:
26.
[6] M.D.Research,CyberBattleSim‚Äô,2021.URL:https://github.com/microsoft/cyberbattlesim.
[7] M. Standen, M. Lucas, D. Bowman, T. Richer, J. Kim, D. Marriott, CybORG: A Gym
for the Development of Autonomous Cyber Agents‚Äô, arXiv 26 (2021) 2023. URL: http:
//arxiv.org/abs/2108.09118.
[8] TheMITRECorporation,‚ÄòMITREATT&CKICSMATRIX‚Äô,2023.URL:https://attack.mitre.
org/matrices/ics/.
[9] P.Cichonski,T.Millar,T.Grance,K.Scarfone,ComputerSecurityIncidentHandlingGuide:
RecommendationsoftheNationalInstituteofStandardsandTechnology,TechnicalReport
NIST SP 800-61r2, National Institute of Standards and Technology, 2012. URL: https://
nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-61r2.pdf.doi:
10.6028/NIST.
.
SP.800-61r2
[10] A.Qureshi,Y.Nakamura,Y.Yoshikawa,H.Ishiguro, Intrinsicallymotivatedreinforcement
learningforhuman-robotinteractioninthereal-world‚Äô, NeuralNetw107(2018)23‚Äì33,.
doi:
10.1016/j.neunet.2018.03.014.
[11] I.Popov,N.Heess,T.Lillicrap,R.Hafner,G.Barth-Maron,M.Vecerik,T.Lampe,Y.Tassa,
T.Erez,M.Riedmiller, Data-efficientDeepReinforcementLearningforDexterousManipu-
lation(2017).
[12] D.H.Mguni,T.Jafferjee,J.Wang,O.Slumbers,N.Perez-Nieves,F.Tong,L.Yang,J.Zhu,
Y.Yang,J.Wang,LIGS:LearnableIntrinsic-RewardGenerationSelectionforMulti-Agent
Learning,2022.URL:http://arxiv.org/abs/2112.02618,arXiv:2112.02618[cs].
[13] C.S.deWitt,T.Gupta,D.Makoviichuk,V.Makoviychuk,P.H.S.Torr,M.Sun,S.Whiteson,IsIndependentLearningAllYouNeedintheStarCraftMulti-AgentChallenge?,2020.URL:
http://arxiv.org/abs/2011.09533,arXiv:2011.09533[cs].
[14] C.Yu,A.Velu,E.Vinitsky,J.Gao,Y.Wang,A.Bayen,Y.Wu,TheSurprisingEffectiveness
ofPPOinCooperative,Multi-AgentGames,2022.URL:http://arxiv.org/abs/2103.01955,
arXiv:2103.01955[cs].
[15] S.Hu,Y.Zhong,M.Gao,W.Wang,H.Dong,Z.Li,X.Liang,X.Chang,Y.Yang,MARLlib:
AScalableMulti-agentReinforcementLearningLibrary,2023.URL:http://arxiv.org/abs/
2210.13708,arXiv:2210.13708[cs].
[16] R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. E. Gonzalez, I. Stoica, Tune: A research
platformfordistributedmodelselectionandtraining, arXivpreprintarXiv:1807.05118
(2018).