ğ‘˜
KD A Defense Mechanism Against Label Inference Attacks in
Vertical Federated Learning
MarcoArazzi SerenaNicolazzo AntoninoNocera
UniversityofPavia UniversityofMilan UniversityofPavia
Pavia,Italy Milan,Italy Pavia,Italy
marco.arazzi01@universitadipavia.it serena.nicolazzo@unimi.it antonino.nocera@unipv.it
ABSTRACT Finally,TransferLearningisapplicableforscenariosinwhichthere
VerticalFederatedLearning(VFL)isacategoryofFederatedLearn- islittleoverlappingindatasamplesandfeatures,andmultiplesub-
inginwhichmodelsaretrainedcollaborativelyamongpartieswith jectswithheterogeneousdistributionsbuildmodelscollaboratively.
verticallypartitioneddata.Typically,inaVFLscenario,thelabels Hence,HFLischaracterizedbyindependenttrainingandnoraw
ofthesamplesarekeptprivatefromallthepartiesexceptforthe dataissharedamongparticipants,onlymodelupdates,reducing
aggregatingserver,thatisthelabelowner.Nevertheless,recent theriskofprivacyconcernsandtheoverheadoftransmitteddata.
worksdiscoveredthatbyexploitinggradientinformationreturned VFL,instead,exploitsdeeperattributedimensionsandleadstomore
bytheservertobottommodels,withtheknowledgeofonlyasmall accuratemodelsbecausedatafeaturesarecomplementaryacross
set of auxiliary labels on a very limited subset of training data differentsources.Thisresultsinahighercomplexity,alsodueto
points,anadversarycaninfertheprivatelabels.Theseattacksare feature-levelcoordination.
knownaslabelinferenceattacksinVFL.Inourwork,weproposea However,evenifrawdataisnotsharedduetothecalculation
novelframeworkcalledKDğ‘˜,thatcombinesKnowledgeDistillation andexchangeoffeatures,combininginformationacrossfeatures
andğ‘˜-anonymitytoprovideadefensemechanismagainstpotential andthepossiblepresenceofacompromisedparticipantmayraise
labelinferenceattacksinaVFLscenario.Throughanexhaustive privacyleakage[24,25].Possibleattacksthatrepresentasignificant
experimentalcampaignwedemonstratethatbyapplyingourap- concerninthiscontextaretheLabelInferenceAttacks,becauseof
proach,theperformanceoftheanalyzedlabelinferenceattacks thehighsensitivityofthelabelsthatmayrevealcrucialclientsâ€™
decreasesconsistently,evenbymorethan60%,maintainingthe information(e.g.,diseases,financialdetails,etc.).Inourwork,we
accuracyofthewholeVFLalmostunaltered. startconsideringthelabelinferenceattackstoVFLdescribedinthe
recentpaperofFuetal.[8].
KEYWORDS Inourstudy,weprovideadefenseagainsttheabove-citedattacks.
OurapplicationscenarioistheclassicalVFLscenarioinwhichtwo
FederatedLearning,VerticalFederatedLearning,VFL,LabelInfer-
enceAttack,KnowledgeDistillation,ğ‘˜anonymity. typologiesofparticipants,aserverandasetofclients,collabora-
tivelytrainanMLholdingdifferentfeaturespaces.Theserveror
1 INTRODUCTION activeparticipantalsostoresthelabelsthatarekeptprivatefrom
theotherpassiveparticipants.Theadversarycontrolssomepas-
FederatedLearning(FL,forshort)hasemergedinthelastyears
siveparticipantsandaimsatdiscoveringtheprivatelabels.The
asakeytechnologyenablingcollaborativemodeltrainingacross
authorsof[8]demonstratethat,inthisscenario,differenttypesof
differententitieswithouttheneedtogatherdatainacentralloca-
labelinferenceattackssucceedinmostcasesreachinggoodaccu-
tion[17].Theapplicationofthisparadigmisadvantageouswhen
racyresults.Specifically,in[8],Fuetal.discoverthatthanksto
organizationsorindividualshavetocooperateonmodeldevelop-
(i)thetrainedlocalmodelheldbythemaliciousparticipant,and
mentwithoutrevealingtheirsensitivedata.Sincemodelupdates
(ii)thereceivedgradientsofthelossthatcontainshiddeninforma-
areperformedlocallyalsocommunicationcostsarereduced,more-
tionaboutlabels,theattackercansucceedinconductingalabel
over,heterogeneousdatacanbeintegratedmoreeasilymaintaining
inferenceattack.Theydescribethreepossiblecategoriesoflabel
thepeculiaritiesofthedifferentparticipantsandbuildingjointand
inferenceattacks.Thefirstattackisapassiveattack,inwhich,with
richerdatapools.Nevertheless,althoughthisapproachisdesigned
thehelpofsomeauxiliarylabeleddata,themaliciousparticipant
tokeeprawdataonlocaldevices,thereisstillariskofindirect
canfine-tunehis/hertrainedbottommodeltoinferthelabelsin
informationleakage,particularlyduringthemodelaggregation
asemi-supervisedmanner.Thesecondtypeisanactiveattack,in
stage.
whichtheattackertriestoscaleupthelearningrateofher/hisbot-
Accordingtothedifferentdatapartitionstrategiesadopted,three
tommodelduringthetrainingphasetoforcethetopmodeltorely
maincategoriesofFLhavebeenformulated,i.e.,Horizontal,Ver-
moreonher/hismodelthusboostingthelabelinferenceaccuracy.
tical,andTransferLearning[29].HorizontalFL(HFL,forshort)
Thethirdtypeisadirectattackthroughwhichtheadversarycan
requiresallpartiestoholdthesameattributespacebutdifferent
inferlabelsbyanalyzingthesignsofgradientsfromtheserver.
samplespaceanditissuitableforscenarioswherevariousregional OurproposalconsistsofdesigninganovelframeworkcalledKDğ‘˜
branchesofthesamebusinesswanttobuildaricherdataset.Ver- (KnowledgeDiscoveryandğ‘˜-anonymity)asadefensemechanism
ticalFL(VFL,hereafter),instead,isbasedonthecollaborations
relyingonanadditionalcomponentfortheserver(oractive)partic-
amongnon-competingentitieswithverticallypartitioneddatathat
ipant.ThisincludesaKnowledgeDistillationstepandanobfuscation
shareoverlappingdatasamplesbutdifferinthefeaturespace(i.e.,
algorithm.Specifically,KnowledgeDistillation(KD,hereafter)[9]
amobilephonecompanyandaTVstreamingserviceprovider).
4202
rpA
81
]GL.sc[
1v96321.4042:viXraisanMLcompressiontechniqueabletotransferknowledgefroma iscrucialtheyarestillanopenchallengeandonlyafewefforts
largerteachermodeltoasmallerstudentone.Theteachernetwork havebeenmade.
producessofterprobabilitydistributionsinsteadofhardlabelsthat Forexample,thearticles[13,14]studylabelinferenceattacksin
canbettercaptureessentialfeaturesandrelationshipsinthedata. VFL,buttheyspecificallyfocusonsplitlearningscenarios.Inpar-
Weincludeintheactiveparticipantateachernetworkwhose ticular,[13]formalizesathreatmodelforlabelleakageintwo-party
outputsaresoftlabels.Thesearethenprocessedbyanalgorithm splitlearninginthecontextofbinaryclassificationandproposes
basedontheconceptofğ‘˜-anonymity[22]toaddafurtherlevel acountermeasurebasedonrandomperturbationtechniquesthat
of uncertainty. This step groups together theğ‘˜ labels with the minimizetheamountoflabelleakageofaworst-caseadversary.
higherprobabilitiesmakingithardfortheattackertoinferthe Whereas,theproposalin[14]presentsapassiveclusteringlabel
mostprobableone.Thenthetopmodeloftheservercanbefed inferenceattackforsplitlearning,inwhichtheadversary(thatcan
withthesenewsoftandpartlyanonymizedlabelsandtheVFLtasks beanyclientsortheserver)retrievestheprivatelabelsbycollect-
canbeexecutedcollaboratively. ingtheexchangedgradientsandsmasheddatabothduringand
Our experimental campaign demonstrates that using our ap- afterthetrainingphase.[15]designtheinversionandreplacement
proachtheaccuracyofthethreetypesoflabelinferenceattacks attackstodiscloseprivatelabelsfrombatch-levelmessagesinaVFL
decreasessignificantly. whosecommunicationisprotectedbyaHomomorphicencryption
Insummary,themaincontributionsofthispaperare: mechanismandaconfusionalautoencoder(CoAE)methodasapos-
â€¢ wedesignacountermeasureforthedifferenttypesoflabel siblecountermeasure.Theproposalin[23]dealswiththedesignof
inferenceattacksproposedby[8]. alabelleakageattackfromtheforwardembeddingintwo-party
â€¢ Weconductanexperimentalcampaigntodemonstratethat splitlearningandacorrespondingdefensethatreducesthedis-
the accuracy of all the analyzed types of label inference tancecorrelationbetweencutlayerembeddingandprivatelabels.
attacksconsistentlydecreasesifourcompleteapproachis Kholodetal.,[11]proposeaparallelizationmethodtodecreasedata
applied. transmission,and,consequently,boththelearningcostandprivacy
â€¢ Weprovideacomparisonwithexistingdefensestrategies leakagerisk.AframeworkcalledLabelGuardhasbeendesigned
andshowthehighereffectivenessofoursolution. in[27]todefendagainstlabelinferenceattacksviaacascadeVFL
algorithmthroughaminimizationoftheVFLtasktrainingloss.
Theorganizationofthispaperisoutlinedasfollows.Section2
Inthiswork,westartfromtheproposals[8].Theauthorsof[8]
describesthemainworksrelatedtoourapproach.Section3delves
intothedetailsaboutFL,ğ‘˜-anonimity,andKnowledgeDistillation describethreekindsoflabelinferenceattack,i.e.,passivelabelinfer-
enceattack,activelabelinferenceattack,anddirectlabelinference
that are essential to the understanding of our solution. Section
attack,forVFL.Adversariescouldinferthelabelsoftheactiveparty
4presentsthetypesoflabelinferenceattacksagainstwhichwe
fromboththereceivedplaintextgradientsandobtainedplaintext
provideadefense.Section6discussestheexperimentalcampaign,
finalmodelweights.Althoughtheseattacksareveryeffective,they
includingthesetupandresultsofourdefensemechanisms.Ulti-
makeastrongassumptiononauxiliarylabelsthathavetobeheld
mately,Section7concludestheworkandpresentspossiblefuture
fortheadversary.
directions.
3 BACKGROUND
2 RELATEDWORK
Inthissection,wedescribesomeconceptsusefultounderstand
RecentworkshaveshownthatFLisvulnerabletomultipletypes
ourapproach.Inparticular,weexaminethekeyaspectsanddif-
ofinferenceattacks,suchasmembershipinference,propertyin-
ferentcategoriesofFederatedLearning,werecalltheconceptof
ference,andfeatureinference[18,19,32].Theobjectiveofamem-
ğ‘˜-anonimityandwedelveintotheanalysisofthemainfeaturesof
bershipinferenceistodiscriminatewhetheraspecificrecordisin
KnowledgeDistillation.
apartyâ€™strainingdatasetornot.Nevertheless,thistypeofattack
Table1summarizestheacronymsusedinthispaper.
hasnoreasontoexistinVFLaseveryparticipantknowsallthe
trainingsampleIDs.Propertyinferenceaimstoextractsomeprop-
3.1 FederatedLearning
ertiesaboutapartyâ€™strainingdataset,whichareuncorrelatedto
thetrainingtask.Inafeatureinferenceattack,instead,apartytries FL is a Machine Learning method designed to train a model in
torecoverthesamplesusedinanotherpartyâ€™strainingdataset.For adistributedmanneracrossdifferentdevicesholdinglocaldata
instance,Luoetal.proposeafeatureinferenceattackforVFL[16], samples.Thefactthatdataisnottransferredandcentralizedis
inwhichtheactivepartytriestoinferthefeaturesownedbythe advantageousforprivacypreservationreasonsandnetworktraffic
passiveparty.However,theauthorsstronglyassumethattheactive reductionduetolargedatavolumes.
partyknowsthemodelparametersofthepassiveparty,whichis TheactorsofthisprotocolareCdevices(orâ€œclientsâ€orâ€œwork-
difficulttoachieveinreal-worldscenarios.Differentlyfromthe ersâ€),runninglocaltrainingandholdingprivatedata;andacentral
above-citedworks,ourproposaldealswithadifferenttypeofinfer- servercalledâ€œaggregatorâ€,thatcoordinatesthewholeFLprocess
enceattackinVFL,knownasalabelinferenceattack,conducted aggregatingthelocalupdates.Specifically,FLaimstotrainaglobal
bythepassivepartyandaimingatleakingthelabelsownedby modelwbyuploadingtheweightsoflocalmodels{wğ‘–|ğ‘– âˆˆC}toa
theactiveparticipant.Sincelabelsoftencontainhighlysensitive parametricserveroptimizingalossfunction:
informationthistypeofattackdeservesmoreandmoreattention. ğ‘›
Althoughfindingpossibledefensestrategiesagainsttheseattacks
minğ‘™(w)=âˆ‘ï¸ğ‘ 
ğ‘– ğ¿ ğ‘–(wğ‘– ) (1)
w C
ğ‘–=1
2Table1:Summaryoftheacronymsusedinthepaper.
Symbol Description
DL DeepLearning
FCNN FullyConnectedNeuralNetwork
FL FederatedLearning
FTL FederatedTransferLearning
GC GradientCompression
HFL HorizontalFederatedLearning
KD KnowledgeDistillation
ML MachineLearning
NG NoiseGradient
OA OriginalArchitecture
PPDL Privacy-PreservingDeepLearning
VFL VerticalFederatedLearning
Figure1:TheFederatedLearningworkflow
whereğ¿ ğ‘–(wi) = ğ‘ 1
ğ‘–
(cid:205) ğ‘—âˆˆğ¼ğ‘–ğ‘™ ğ‘—(wğ‘–,ğ‘¥ ğ‘–) isthelossfunction,ğ‘  ğ‘– isthe
localdatasizeofthei-thworker,andğ¼ ğ‘– identifiesthesetofdata
indiceswith|ğ¼ ğ‘–|=ğ‘  ğ‘–,andğ‘¥ ğ‘— isadatapoint.
ThebasicFLworkflow,showninFigure1,canbedividedinto
thefollowingsteps[30]:
(1) Modelinitialization,inwhichthecentralserverinitializesall
thenecessaryparametersfortheglobalMLmodelw.This
phasealsoincludestheworkersâ€™selectionprocess.
(2) Localmodeltrainingandupload,inwhichtheworkersdown-
loadthecurrentglobalmodelandperformlocaltraining
(a)HorizontalFL(HFL) usingtheirprivatedata.Afterthat,eachclientcomputesthe
modelparameterupdatesandtransmitsthemtothecentral
server.Thelocaltrainingtypicallyinvolvesmultipleitera-
tionsofgradientdescent,back-propagation,orotheropti-
mizationmethodstoimprovethelocalmodelâ€™sperformance.
Specifically,atthet-iteration,eachclientupdatestheglobal
modelbytrainingwiththeirdatasets:wğ‘– ğ‘¡ â†wğ‘– ğ‘¡âˆ’ğœ‚ğœ•ğ¿ ğœ•(w wğ‘¡ ğ‘– ğ‘¡,ğ‘) ,
whereğœ‚ andğ‘ identify the learning rate and local batch,
respectively.
(3) Globalmodelaggregationandupdate,inwhichthecentral
servercollectsandaggregatesthemodelparameterupdates
(b)VerticalFL(VFL) fromalltheworkers,{wğ‘–|ğ‘– âˆˆC}.Thecentralservercanem-
ployvariousaggregationmethodslikeaveraging,weighted
averaging,orsecuremulti-partycomputationtoincorporate
thereceivedupdatesfromeachclient,thusimprovingthe
performanceoftheglobalmodel.
FLcanbeclassifiedintothreescenariosaccordingtothediffer-
entdatapartitionstrategiesadopted,i.e.,Horizontal,Vertical,and
TransferLearningtypes[29],asshowninFigure2.
As visible in Figure 2a, Horizontal FL (HFL, hereafter), or
sample-basedFL,isintroducedinthescenariosinwhichthedataset
ofthepartiessharethesamefeaturespace,buthavedifferentspaces
(c)FederatedTransferLearning(FTL) insamples.Forinstance,twobranchesofthesamecompany(two
regionalbanksortwohospitals)holddatareferringtousersof
Figure2:ThethreecategoriesofFLdividedforfeatureand distinctareas,buttheysharethesamefeaturespaces(i.e.,thesame
samplespaces characteristics).Ifthetwopartiesaggregatetheirsamples,they
couldbuildalargerdatasetandthentrainamoreaccuratemodel.
Yet,privacylawforbidsthedirectsharingofsensitiveuserdata.In
suchcases,HFLcanhelptosolvethischallengeandprovidearich
privacy-preservingdataset.
3 VerticalFL(VFL,forshort)orfeature-basedFL,instead,applies
tothecasewherethedatasetsshareoverlappingdatasamplesbut3.2 k-Anonimity
Theconceptofğ‘˜-anonymity,firstdescribedin[22],representsone
foundationalprincipleindatabasetheoryforprivacy-preserving
datapublishing.Itaimstosafeguardtheanonymityoftheindivid-
ualsâ€™databyensuringthateachrecordinthedatasetisindistin-
guishablefromatleastğ‘˜âˆ’1otherrecords.Severalprocedurescan
beappliedtoattributestoobtainğ‘˜-anonymity,suchas:
â€¢ Suppression,whichimpliesremovingorcleansingcertain
information.
â€¢ Generalizationisreplacingdistinctivevalueswithmoregen-
eralones(e.g.,substitutingexactageswithageranges).
Figure3:Genericarchitectureofknowledgedistillationusing
3.3 KnowledgeDistillation
ateacher-studentmodel
KnowledgeDistillation(KD,forshort)isanMLmodelcompres-
siontechnique,inwhichtheknowledgefromacomplexmodel,
differinthefeaturespace(asshownin2b.Considerthecaseof
orâ€œteacherâ€model,istransferredtoasmallerandmoreefficient
twodifferentbusinessessharinginformationaboutoverlapping
model,knownastheâ€œstudentâ€modelwithoutasignificantdrop
setsofcustomers.Forexample,amobilephonecompanyanda
inaccuracy[9].ThegeneralideawasfirstpresentedbyBucilua
TVstreamingserviceprovidercanhavecommonclients,butthe
etal.in2006[2]andmodeledinitscurrentknownformin2014
typesofdatarelatedtothemareverydifferent.Nevertheless,arich
byHintonetal.[10]whofounditeasiertotrainaclassifierusing
predictionmodelforservicepurchasescanbebuiltleveragingboth
theoutputsofanotherclassifierastargetvaluesthanusingactual
datasetscollaboratively,butdirectlyrevealingpersonalcustomer
ground-truthlabels.Theteachernetworkoutputsarerepresented
informationtothirdpartiesisnotalwayspossiblebecauseofGDPR.
bytheso-calledsoftprobabilitiesthatcontainmoreinformation
Hence,VFLcanrepresentasolutionforthissettingallowingthe
aboutadatapointthanjusttheclasslabel(orhardpredictions)and
companytocollaborativelytrainajoinmodelwitharichdataset.
aretheinputofthestudentnetwork.
VFLcanbewithorwithoutmodelsplitting: Inpractice,givenaninputğ‘¥ theteachernetworkproducesa
â€¢ Inthepresenceofmodelsplitting,everyparticipantrunsa vectorofscoresğ‘  ğ‘¥ğ‘¡ = [ğ‘  1ğ‘¡,ğ‘  2ğ‘¡,...,ğ‘  ğ¾ğ‘¡ ]thatareconvertedintoproba-
bottom(orlocal)modelwithoutsharingtheentiremodel bilities:
withotherparticipantsandrelyingonfeatureslocallyavail-
ğ‘ğ‘¡ (ğ‘¥)=
ğ‘’ğ‘  ğ‘˜ğ‘¡
(2)
a stb rl ue ca tet dea bc yh ap sa er rt vy e. rT th he atfi cn oa ml bto inp e( so tr hg elo lob ca al l) lym to rd aie nl ei dsr me oco dn el- ğ‘˜ (cid:205) ğ‘—ğ‘’ğ‘ ğ‘¡ ğ‘—
portionstocomputeafinaloutput.Forinstance,oneparty Hintonetal.[10]proposedtomodifytheseprobabilitiesinsoft
may focus on training the modelâ€™s parameters related to probabilitiesasfollowing:
d pe am rao mg er ta ep rh si rc elf ae ta et dur toes d, aw tah ail be oa un to pt uh re cr hp asa er .tymayworkon
ğ‘ğ‘¡ (ğ‘¥)=
ğ‘’ğ‘  ğ‘˜ğ‘¡ /ğœ
(3)
â€¢ Intheabsenceofmodelsplitting,themodelremainscentral- ğ‘˜ (cid:205) ğ‘—ğ‘’ğ‘ ğ‘¡ ğ‘—/ğœ
ized,andeachpartycalculatesgradientsofthelossrelying whereğœisahyperparameter.Astudentnetworkwillproduceasoft-
onitslocaldata,thenitsharesthesegradientswithacentral enedclassprobabilitydistribution,pËœğ‘ (ğ‘¥).Thelossforthestudent
server,whichaggregatesthemtoupdatetheglobalmodel. networkisalinearcombinationofthecrossentropyloss,namely
Afterthetrainingprocessisfinished,attheinferencetime,VFL Lğ‘ğ‘™ andaknowledgedistillationlossLğ¾ğ·:
requiresallparticipantstogetinvolved,insteadforHFL,thetrained
globalmodelissharedwitheveryparticipantwhichperformsin- L=ğ›¼Lğ‘ğ‘™ âˆ’(1âˆ’ğ›¼)Lğ¾ğ· (4)
ferenceindividually. whereLğ¾ğ· =âˆ’ğœ2(cid:205) ğ‘˜ğ‘Ëœğ‘¡(ğ‘¥)logğ‘Ëœğ‘ (ğ‘¥)andğ›¼andğœarehyperparam-
ThelastcategoryofFLisFederatedTransferLearning(FTL, eters.
hereafter)whichisacombinationofthetwoprevioustypes[26]. Figure 3 shows the generic architecture of the KD using the
Indeed,FTLisapplicableforscenariosinwhichthereislittleover- teacher-studentmodel.Thankstothedistillationalgorithmthestu-
lappinginbothdatasamplesandfeaturesasvisibleinFigure2c.For dentmimicstheteachernetworklearningtherelationshipbetween
instance,thecaseinwhichmultiplesubjectswithheterogeneous different classes discovered by the teacher model that contains
distributionsbuildmodelscollaboratively.Considerthecasewhere informationbeyondthegroundtruthlabels.
anAmericancompanyproducinghealthIoTsensorswantstojoin
itsdatasamplewithaprivatehealthclinicinCanada.Thesetwo 4 LABELINFERENCEATTACKS
entitieshavetofollowlawrestrictions.Moreover,boththeirsets
Inthissection,wedescribethemostcommontypesoflabelinfer-
ofclientsandfeatureshavesmallintersections.Inthiscase,FTL
enceattacksagainstVFL,whichwefocusontodesignourdefense
techniquescanbeappliedtoprovideasolutionandmakethetwo
strategy.Astypicallydoneintherelatedliterature,wemakeexplicit
partiescooperateinthebuildingofonemodel.
referencetothemorecomplexscenarioinwhichVFLiscombined
withmodelsplitting[7](seeSection3).
44.2 ActiveLabelInferenceAttack
Thisattackisclassifiedasactivebecausethemaliciousparticipant
performssomeactionsinthetrainingstage,inparticular,she/he
tries to scale up the learning rate during the training phase of
her/hisbottommodel.Inthisway,she/heaimstoacceleratethe
gradientdescentonher/hisbottommodeltosubmitbetterfeatures
totheserverineachiteration.Consequently,she/hecanforcethe
topmodeltorelymoreonher/hisbottommodelthantheother
participants.
Sinceincreasingthelearningratedoesnotalwaysresultina
moreefficientgradientdescent,theauthorsof[8]performthis
attackbydesigningandexecutingamaliciouslocaloptimizer.This
componentadaptivelyscalesupthegradientofeachparameterin
theadversaryâ€™sbottommodeltoavoidtheoscillationphenomenon
aroundthelocalminimumpoint,thatistypicaloftheuseofan
overlylargelearningrateforgradientdescent.
Usingthemaliciouslocaloptimizer,theattackercangetatrained
bottommodelwithmorehiddeninformationaboutlabels.Inaddi-
tion,she/hecanperformthemodelcompletionstepofthepassive
Figure4:LabelinferenceattackscenarioagainstVFL
attack(seeSection4.1)tofine-tunethebottommodelwithanaddi-
tionalclassificationlayerandobtainthefinallabelinferencemodel.
In this setting, as originally proposed by [8], label inference
4.3 DirectLabelInferenceAttack
attacksarecarriedoutbyadversaries,controllingoneormoreof
thebottommodels,whichaimtoinfertheprivatelabelsforany Tocarryoutthisattacktheadversarydirectlyexploitsthegradients
samplesinthedataset.Recallthat,accordingtothemodelsplitting she/hereceivesfromthetopmodeltoinferthelabelsofthetraining
paradigm,onlytheactiveparty,i.e.,theserver,hastheclassification examples.Thisisbasedontheanalysisofthesignsofthegradients
layer,whoseobjectiveisthepredictionofthecorrectlabelforeach ofthelosses.Theauthorsof[8]demonstratethroughmathematical
datapointininput.Therefore,labelsareavailableonlytothisactive proofthatthismethodworksforlabelinferenceinVFLwithout
party of the FL system and, therefore, are considered sensitive modelsplitting(seeSection3.1fordetailsaboutmodelsplitting).
information.Tocarryoutalabelinferenceattack,adversariescan Sincenogradientsareavailableattheinferencetime,withthis
mainlyexploittwomainaspectsofVFLthat,accordingto[8],may attack,themaliciousparticipantcanonlyinferthelabelsoftraining
generatelabelleakage,namely: examples.Nevertheless,thesediscoveredlabelscanbeusedasthe
â€¢ thetrainedlocalmodelthatisunderthefullcontrolofthe auxiliarydatanecessarytoperformapassivelabelinferenceattack.
maliciousparticipant; Inthisway,theattackercaninferthelabelofanarbitrarysample.
â€¢ thereceivedgradientsofthelossthatcontainhiddeninfor-
5 APPROACHDESCRIPTION
mationaboutlabels.
Inthefollowing,wedescribefourmaintypesoflabelinference Ourapproachaimstoprovideacountermeasureforallthetypes
attacks,namely:(i)PassiveLabelInferenceattack[8];(ii)Active oflabelinferenceattacksdescribedinSection4.
LabelInferenceattack[8];and(iii)DirectLabelInferenceAttack Tobetterpresentourdefensestrategy,asdoneonceagainin
[8]. [8],wewillfocusonabasicVFLattackscenarioshowninFigure
4.Here,twoparticipantsholdingthesamesetofsamplesbutwith
4.1 PassiveLabelInferenceAttack featuresfromdifferentspaceswanttotrainamodelcollaboratively
throughVFL.
Adversaries can perform this attack by exploiting their locally
ownedbottommodel.Itisreferredtoaspassivebecausethema- Thefirstparticipant,thatistheserver,runsboththetopğ‘‡ ğ‘€ and
liciousparticipantdoesnotperformanyactiveactionduringthe thebottomğµ ğ´ models,henceshe/heisthelabelownerğ»ğ¿ and
trainingorinferencephase,butshe/heremainshonestbutcurious.
holdspartoftheverticallypartitioneddatağ‘‹ ğ´.Forthisreason,
she/heisalsoreferredtoasanactiveclient.Her/hisobjectiveis
Thistypeofattackassumesthattheadversarialcanrelyonafew
auxiliarylabeleddata(in[8]onlythe0.08%ofthelabeledtraining toenhancethemodelperformancebycombiningher/hisfeatures
withtheonesofotherparticipantscomingfromdifferentbusiness
samples have been used as auxiliary labels in the experimental
domains.
campaign).Iftheattackercangetthisadditionalknowledge,she/he
Thesecondparticipantinourexampleistheadversarialorpas-
caninferthelabelsbyfine-tuningher/hisbottommodelthrougha
siveclientwhoaimsatinferringthelabelsfromthetrainingprocess
furtherclassificationlayerinasemi-supervisedmanner.Thisstepis
referredasmodelcompletionattack.Oncethetrainingiscompleted, andhasaccessonlytoitsbottommodelğµ ğ‘ƒ anditspartvertically
themodelcanpredictalabelforeveryitemofthesampleofthe
partitioneddatağ‘‹ ğ‘ƒ.Ateachtraininground,thebottommodelout-
adversary.
putsğ» = {ğ» ğ´,ğ» ğ‘ƒ}aresenttotheserverrunningthetopmodel
ğ‘‡ ğ‘€,which,hence,returnsthecorrespondentpartialgradientsâˆ‡ğ»
ğ´
5andâˆ‡ğ»
ğ‘ƒ
ofthelossğ‘™.Theseareusedtoupdatetheclientsâ€™bottom theoriginaltrainingset.Thefunctionmax(ğ¾ğ·(ğ»ğ¿))returnsthe
modelparametersğ‘Š ğ´ (ğ‘Š ğ‘ƒ).Thelocalmodelsupdatesâˆ‡ğ‘Š ğ´ and labelswiththehighestprobabilityforeachdatapoint.Whereas,
âˆ‡ğ‘Š ğ‘ƒ arecalculatedasfollows(ğ¶ğ¸=cross-entropy,ğ‘†ğ‘€ =softmax): thefunctionğ‘¡ğ‘œğ‘ ğ‘˜(ğ¾ğ·(ğ»ğ¿))returnsthesetoftheğ‘˜âˆ’1labelshav-
ğ» =ğ¶ğ‘‚ğ‘ğ¶ğ´ğ‘‡(ğ» ğ´,ğ» ğ‘ƒ),ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘  =ğ‘‡ ğ‘€(ğ») (5) ingthehighestvaluesfollowingthemaximum(i.e.,onceagain,
thehighestprobabilitiesofbeingthecorrectlabelofthetarget
ğ‘™ =ğ¶ğ¸(ğ‘†ğ‘€(ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘ ), ğ‘†ğ‘€(ğ»ğ¿))
datapointasestimatedbytheKDmodel)foreachdatapoint.
âˆ‡ğ‘Š ğ´ =âˆ‘ï¸ ğœ•ğ»ğœ•ğ‘™ Â· ğœ• ğœ•ğ» ğµğ´. (6)
ğ´ ğ´ Algorithm1SoftLabelAlgorithm.
âˆ‡ğ‘Š ğ‘ƒ =âˆ‘ï¸ ğœ•ğ»ğœ•ğ‘™
ğ‘ƒ
Â· ğœ• ğœ•ğ» ğµ ğ‘ƒğ‘ƒ. (7) R 1e :q ğ»ui ğ¿r :e s:
etofHardLabels
Althoughtheprivatelabelsğ»ğ¿neverleavethefirstparticipantâ€™s 2: ğ¾:setoftop-klabelswithhigherconfidence
storage,theadversarycanexploitthereceivedpartialgradients 3: ğ‘˜:|ğ¾|cardinatilyofğ¾
andthetrainedbottommodeltoconductalabelinferenceattack. 4: ğœ–:smoothingparameter
5: ğ‘›:numberofclasses
In particular, to perform the first attack, or Passive Label In-
6: ğ‘‡ğ‘œğ‘ğ¾ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ğ‘’ğ‘ ,ğ‘€ğ‘ğ‘¥ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’â†ğ‘”ğ‘’ğ‘¡ğ‘‡ğ‘œğ‘ğ¾ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ğ‘’ğ‘ (ğ»ğ¿,K)
ference Attack, the adversary relies on a small set of auxiliary
7: ğ‘†ğ¿â†ğ‘§ğ‘’ğ‘Ÿğ‘œğ‘ (ğ‘›)
labels.Ifshe/hemanagestoobtainthissetshe/hecanfine-tune 8: forğ‘–inğ‘‡ğ‘œğ‘ğ¾ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ğ‘’ğ‘ do
her/hisbottommodelthroughafurtherclassificationlayerina 9: ifğ»ğ¿[ğ‘–]==ğ‘€ğ‘ğ‘¥ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’then
semi-supervisedmannertoinferthetraininglabels.Toconduct 10: ğ‘†ğ¿[ğ‘–]â†1âˆ’ğœ–
theotherattacks(i.e.,theActiveandtheDirectLabelInference 11: else
Attacks),instead,themaliciousparticipantexploitsthefactthat, 12: ğ‘†ğ¿[ğ‘–]â†ğœ–/(ğ‘˜âˆ’1)
eventhoughshe/hedoesnothavedirectaccesstothelabel,her/his 13: endif
bottommodelimplicitlyholdsinformationaboutthem,becauseof 14: endfor
thetrainingstep.Withtheselaststrategies,theadversarycannot
obtainalltheprivatelabels,buthe/shecan,then,runasubsequent
passiveattacktoimprovetheattackperformance. 6 EXPERIMENTALRESULTS
Atthispoint,wearereadytopresentourdefensemechanism Inthissection,weillustratetheexperimentscarriedouttoassessthe
againsttheabove-citedtypesoflabelinferenceattacks.Inparticular, performanceofourdefensemechanism.Specifically,inSection6.1,
weincludeintheactiveparticipantarchitectureanadditionalcom- wedescribethedataset,theevaluationmetrics,andtheenvironment
ponentcomprisedofafine-tunedteachernetworkthatperformsa usedforourexperiments.Theremainingsectionsaredevotedto
KnowledgeDistillationğ¾ğ·steptooutputsoftlabelsğ‘†ğ¿insteadof
analyzingtheresultsandtheperformanceofourdefenseapproach
hardonesğ»ğ¿.Theoutputvectorofagivendatapointcontainsthe
againstthedifferenttypesofanalyzedlabelinferenceattacksand
probabilitiesthatitbelongstoeachclassrepresentedbytheprivate thecomparisonwithotherdefensemechanisms.
labels.Theoutputfromthislayeristhenprocessedbyanalgorithm
basedontheconceptofğ‘˜-anonymity(see3.2fordetail)toadda 6.1 Testbedsdescription
secondlevelofuncertainty.Throughthisfurtherstep,insteadof
Toevaluatetherobustnessofourapproachagainstlabelinference
selectingasinglelabelforeachsample,weselectasetofğ‘˜labels
attacksweadoptsomeofthedatasetsusedby[8],namely:
inğ‘†ğ¿withthehighestprobability.Hence,asshowninAlgorithm
â€¢ CIFAR-10dataset[12]consistingof60,00032ğ‘¥32colorim-
1,ifthelabelistheoneassociatedwiththehighestconfidence
itisscaledbyğœ–(whereğœ–isasmoothingparameter),otherwise,if ages divided into 10 classes with 6,000 images per class.
thelabelbelongstotheğ‘˜âˆ’1highestprobabilitylabels(excluding
Thereare50,000intrainingimagesand10,000intestim-
themaximum)ağœ–/(ğ‘˜âˆ’1)factorisappliedtoscaleuptheirfinal ages.
â€¢ CIFAR-100[12]datasetthatissimilartoCIFAR-10,butithas
probabilityvalue.Atthispoint,sincethecorrectlabelforeach
itemisobfuscatedinagroupofğ‘˜labels,aswewilldemonstratein 100classescontaining600imageseachwith500training
imagesand100testingimagesperclass.
theexperiments,theattackercannolongereasilyinferthemost
â€¢ CINIC-10[5],whichisalargedatasetandanextendedal-
probableoneperforminganyoftheabove-citedattacks.TheVFL
ternativeforCIFAR-10with270,000images,(i.e.,4.5times
processchangesasfollows:
morethatofCIFAR-10).
ğ‘†ğ¿â†ğ¾ğ·ğ‘˜(ğ»ğ¿,ğ‘˜,ğœ–) (8) â€¢ Yahoo!Answerstopicclassificationdataset[31]isformedby
ğ¾ğ·ğ‘˜(ğ»ğ¿,ğ‘˜,ğœ–)=ï£±ï£´ï£´ï£´ï£²1
ğ‘˜ğœ–
âˆ’âˆ’ 1ğœ– ii ff ğ¿ğ¿ ğ‘–ğ‘– âˆˆâˆˆ ğ‘¡m ğ‘œa ğ‘x ğ‘˜( (ğ¾ ğ¾ğ· ğ·( (ğ» ğ»ğ¿ ğ¿) ))
) (9)
â€¢
1
s
Ca0
rm
im
tp
ea oli en
s
[4c
a
]a nt ide sg
6
ao ,r
0
ri e0e a0s l-ta wen sd
oti
rne lda gc dsh
a
amc tl aa
p
ss
l
es
e
tsc r.o en lata tein ds t1 o4 c0 o,0 m0 m0t er ra ci eni fn og
r
ï£´ï£´ï£´0
otherwise
predictingadclick-throughrates.Inthisdataset,composed
ï£³
ğ‘™ ğ‘˜ğ‘‘ğ‘˜ =ğ¶ğ¸(ğ‘†ğ‘€(ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘ ),ğ‘†ğ‘€(ğ‘†ğ¿)) (10) ofonly2classes,bothcategoricalandcontinuousfeatures
âˆ‡ğ‘Š ğ‘ƒ
=âˆ‘ï¸ğœ• ğœ•ğ‘™
ğ»ğ‘˜ğ‘‘ğ‘˜ Â·
ğœ• ğœ•ğ»
ğµğ‘ƒ. (11)
Toaa sr se ese sm tp hl eoy ee ffd e.
ctivenessofourdefenseapproach,weadopt
ğ‘ƒ ğ‘ƒ
Here ğ¾ğ·(ğ»ğ¿) contains the soft labels (a probability vector) re- thefollowingevaluationmetrics:
turnedbytheknowledgedistillationmodelforeachdatapointin
6Table3:TeacherNetworkArchitecturesforKDğ‘˜.
architecturesthathavebeenimplementedforeachdatasetisvisible
inTable3.
TeacherNetwork Allexperimentshavebeenperformedonaworkstationequipped
Dataset
Architecture withaAMD(R)Ryzen(R)7CPU5800x@3.80GHz,32GBRAM,and
CIFAR-10 ResNet-50+FCNN-1 anNVIDIARTX3070TiGPUcard.
CIFAR-100 ResNet-50+FCNN-1
CINIC-10 ResNet-50+FCNN-1 6.2 LabelInferenceAttacksPerformance
Yahoo!Answers Bert+FCNN-1 Comparison
Criteo FCNN-4
Inthissection,wereporttheperformanceresultsofourdefense
mechanismagainstthefourtypesofLabelInferenceattacksde-
â€¢ Top-1Accuracy,thatistheconventionalaccuracyorthe scribedinSection4.
ratioofcorrectlypredictedsamplestothetotalnumberof Foreachanalyzedattack,wechosetheappropriateconfigura-
samples in the dataset. It measures how many times the tionofthetwoanonymizationparametersğœ–andğ‘˜,whereğœ–isthe
network has predicted the correct label with the highest smoothingparameterandğ‘˜ isthenumberofthehighestlabels
probability. consideredforeachdatapointtobuildoursoftlabelsolution.
â€¢ Top-5Accuracy,isametricthatindicateshowmanytimes
6.2.1 PassiveLabelInferenceAttack. WecarriedoutthePassive
thecorrectlabelappearsinthenetworkâ€™stopfivepredicted LabelInferenceAttackwitha0.08%ofauxiliarylabeleddata(as
classes.Itisusefulforlarge-scaledatasetswithnumerous proposedin[8])andwetesteditagainstourKDğ‘˜frameworkwith
classesandforcasesinwhichadegreeofflexibilityisac- thetwoanonymizationparametersğœ–andğ‘˜setforeachdatasetas
ceptableandtheexactclasscanbenotpredictedwithhigh
arereportedinTable4.Thistablereportsalsotheaccuracyresults
confidence[20].
oftheattackagainsttheoriginalmodelproposedby[8]andagainst
â€¢ Top-1AttackSuccessRate(Top-1ASR)thatisthepercent-
ourdefensemechanism.Thesevaluesshowthattheperformance
ageoflabelscorrectlyextractedbyattacks. oftheattackagainstKDğ‘˜isdrasticallyreducedand,inmostcases,
â€¢ Top-5AttackSuccessRate(Top-5ASR)measureshow
halvedcomparedtotheperformanceagainsttheoriginalmodel
manytimesthelabelcorrectlyextractedbyattacksappears
of[8].Itisworthobservingthat,intheresultsobtainedonthe
inthenetworkâ€™stopfivepredictedclasses.
Yahoo!Answerdatasetwecanseeasmallerreductionintheattack
Forourexperimentalcampaign,werefertoanOriginalArchi- performance,whichiscausedbythefactthatthebottommodel
tecture(OA,hereafter)thatrepresentsaVFLscenariowithoutany isanalreadypre-trainedBertmodel.Theknowledgeincludedin
defensemechanismaspresentedby[8].Thisarchitecture,shownin theBertmodelisalreadyenoughtoobtainabasicclassificationof
Figure4,employsdifferenttypesofnetworksforeachoftheabove- thetext(i.e.,informationonthelabels),eveniftheattackerdoes
describeddatasets.Inparticular,asvisibleinTable2,thetopmodel notinferadditionalinformationfromthetopmodel.Thereforethe
oftheVFLisimplementedthroughapre-trainedResNet-18(i.e.,an performanceoftheattackdoesnotdecreaseasmuchasintheother
18-layerconvolutionalneuralnetworkpre-trainedongeneraldata cases.
andfine-tunedontheactiveparticipantdata)fortheCIFAR-10,
CIFAR-100,andCINIC-10datasets;afine-tunedBERTmodel[6] 6.2.2 ActiveLabelInferenceAttack. ToperformtheActiveLabel
forYahoo!Answer(thatincludestextualdata);anda3-layerFully Inference Attack, we executed the malicious local optimizer in
ConnectedNeuralNetwork(FCNN-3)toprocesssamplesinthe thetrainingstageofourKDğ‘˜ modelandthenweperformedthe
Criteodataset. completion step of the passive inference attack to get the final
label inference model as done in [8]. The configurations of the
Table2:OriginalModelArchitectures. twoanonymizationparametersğœ–andğ‘˜chosenforeachdatasetare
reportedinTable5.Similarlytothepreviousexperiment,when
Top Bottom we performed an active label inference attack against our KDğ‘˜
Dataset
ModelArchitecture ModelArchitecture frameworktheASRconsistentlydecreasedcomparedtotheASRof
CIFAR-10 FCNN-4 ResNet-18 theattackedOA.Observethat,theattackstrategyofincreasingthe
CIFAR-100 FCNN-4 ResNet-18 learningrateonthecontrolledclienttopromotemoreinformative
CINIC-10 FCNN-4 ResNet-18 feedbackfromtheserverdoesnotresultinanadvantage,because
Yahoo!Answers FCNN-4 Bert thankstoourdefensethereceivedsignalisheavilyobfuscated.Also
Criteo FCNN-3 FCNN-3 inthiscase,theresultsfromtheYahoo!Answerdatasetpresenta
smallerreductionintheattackperformance,whichis,onceagain,
Moreover,weimplementedourKDğ‘˜solutionwhosecomponents causedbythefactthatthebottommodelisanalreadypre-trained
Bertmodel.
areillustratedinFigure5.ComparedtotheOriginalArchitecture,
KDğ‘˜includesapreliminaryprocessingstepexecutedonlybythe 6.2.3 DirectLabelInferenceAttack. WecarriedouttheDirectLabel
activeparticipantfortheanonymizationofthelabels.Thisstepis InferenceAttackdescribedin[8]andwetesteditagainstourKDğ‘˜
realizedthrough:(i)ateachernetworkthatimplementstheKnowl- frameworkwiththetwoanonymizationparametersğœ–andğ‘˜setfor
edgeDistillationand(ii)analgorithmthatobfuscatestheğ‘˜labels eachdatasetasarereportedinTable6.Inthistable,wereportthe
withhigherconfidencebasedonğ‘˜-anonymity.Theteachernetwork ASRresultsoftheDirectLabelInferenceAttackagainstOAandour
7Figure5:KDğ‘˜maincomponents
Table4:PassiveLabelInferenceAttackperformanceagainstOAandKDğ‘˜
AttackSuccessRate(ASR)
OA KDğ‘˜
Dataset TypeofASR ğ ğ’Œ
TrainingSet TestSet TrainingSet TestSet
CIFAR-10 Top-1 0.45 3 80.6% 61.7% 43.9% 35.8%
CIFAR-100 Top-1 0.50 3 31.3% 18.0% 15.9% 10.5%
CIFAR-100 Top-5 0.50 3 62.2% 41.0% 40.3% 29.7%
CINIC-10 Top-1 0.45 3 65.2% 49.0% 32.0% 26.0%
Yahoo!Answers Top-1 0.35 3 63.3% 63.7% 47.5% 47.4%
Criteo Top-1 0.40 2 71.2% 71.9% 50.6% 50.3%
Table5:ActiveLabelInferenceAttackperformanceagainstOAandKDğ‘˜
AttackSuccessRate(ASR)
OA KDğ‘˜
Dataset TypeofASR ğ ğ’Œ
TrainingSet TestSet TrainingSet TestSet
CIFAR-10 Top-1 0.50 3 84.8% 63.4% 40.5% 35.1%
CIFAR-100 Top-1 0.60 3 39.3% 21.4% 17.6% 12.2%
CIFAR-100 Top-5 0.60 3 72% 47.4% 43.3% 32.7%
CINIC-10 Top-1 0.50 3 73.5% 50.5% 34.5% 29.3%
Yahoo!Answers Top-1 0.40 3 64.2% 64.1% 52.2% 52.1%
Criteo Top-1 0.40 3 71.2% 71.9% 50.0% 50.0%
Table6:DirectLabelInferenceAttackperformanceagainst KDğ‘˜framework.Sincenogradientsareavailableattheinference
OAandKDğ‘˜
time,thisattackcanbeconductedonlyatthetrainingstephence
wereporttheASRvaluesreferredtothetrainingset.Aswecan
AttackSuccessRate(ASR) observe,ingeneral,ourdefensemechanismcanreducetheASRof
OA KDğ‘˜ theattackbymorethan60%exceptfortheCriteodatasetbecause
Dataset TypeofASR ğ ğ’Œ
TrainingSet
CIFAR-10 Top-1 0.45 3 100% 38.5%
CIFAR-100âˆ— Top-1 0.5 3 100% 32.6%
CINIC-10 Top-1 0.45 3 100% 38.3%
Yahoo!Answers Top-1 0.35 3 100% 39.6%
Criteo Top-1 0.4 2 100% 80%
8
*Inthiscase,wedonotconsidertheTop-5accuracybecause
theTop-1isalready100%.thenumberofclassesisequalto2andthereforearandomchoice [8,15],forCIFAR-100weconsideronlytheTop-5accuracythat
ofthetargetlabelwouldleadtoanASRresulthigherthan0.5. providesamorenuancedevaluationbecauseofthelargenumber
ofclasses.
6.3 ModelsPerformanceComparison AsvisibleinFigure6,theuseofhigherğœ–valuesresultsinbal-
Themainideabehindourapproach,aspresentedinSection5,is ancingtheprobabilitiesoftheclasses,andthisaffectstheoverall
toobfuscatetheinformationofthereallabeltoadduncertainty performanceofboththeattackandtheKDğ‘˜model.Usingdifferent
inthebottommodeloftheattackertoinhibittheeffectivenessof ğ‘˜values,instead,doesnotaffectourdefensemechanism.Interest-
theattackspresentedinSection4.Inevitably,thisapproachwill ingly,settingğ‘˜ = 10andusingadatasetcomposedof10classes
affecttheperformanceofthemodelontheoriginaltask,though makethedefenseineffective.Thisresultconfirmsourintuition
wetrytominimizeitbyobfuscatingtherealinformationinaset behindthelogicthatmakesourproposalwork.Thereasonwhy
ofhighlyprobablealternatives(and,therefore,possiblyavoiding ourapproachiseffectivereliesontheuncertaintyinstilledinthe
toheavilyimpacttheperformanceofthetopmodel).Theresultsin bottommodelsobtainedbyanonymizingthereallabelbetween
theprevioussectionhavebeenobtainedbysettingtheanonymiza- ğ‘˜additionalandrelatedones(asindicatedbyourknowledgedis-
tionparameterssotoguaranteethepreservationoftheoriginal tillationcomponent).Inthecaseofğ‘˜ =10,wearesettingallthe
globalmodelperformance.TheaccuracyperformanceofourKDğ‘˜ secondarylabelstothesameprobability.Thisbreaksthemainlogic
modelcomparedtoOA(theoriginalarchitectureproposedin[8]), behindourapproach.Settingallthesecondarylabelstothesame
isshowninTable7foreachanalyzeddataset.Aswecanseethe valueproducessimilar(withtheadditionofanoffset)lossvalues
performanceoftheoriginalmodelaremostlypreservedwithsmall comparedtothescenariousinghardlabels,directly.Inthiscase,
dropsof4%atmaximum.ObservethatfortheCIFAR-100dataset, theoffsetaddedtothecross-entropylossisnotsufficienttoprop-
theaccuracyresultisevenhigher,becauseoftheeffectofKD.In- erlyobfuscatethelabels,thusresultinginasmalldecreaseinthe
deed,forlargedatasets,ourmodelbenefitsfromthegeneralization accuracyoftheattackinthecaseoftheCINI10datasetorcaneven
capabilitiesoftheteachernetwork[3,28].Aswesaidourapproach beineffectiveinthecaseofCIFAR-10.Tobeeffectivewithğ‘˜ =10,
canimpactthemodelaccuracyaccordingtothestrengthlevelof ourapproachmustpushtheğœ–valuetoextremevalues.Thissetting
theparametersğ‘˜ andğœ–.InthefollowingSection6.4,wepresent iseffectiveagainsttheattackbutalsopreventsthemodelfrom
adetailedanalysiscombiningparameterswithdifferentlevelsof trainingusingtheprobabilitydistributionbalancedacrossallthe
strengthandrecordingthemodelaccuracyinmodelandtheattack labels,thusresultinginanaccuracyclosetorandomguessing.
successrate.
6.5 ComparisonwithotherDefense
Table7:PerformanceonKDğ‘˜comparedtoOAfortheused
Mechanisms
datasets
Intheproposalof[8]severaldefensivestrategiesareappliedtothe
ModelAccuracy gradientstopreventinformationleakagefromtheserverandtry
Dataset Accuracy OA KDğ‘˜ tomitigatethedifferentlabelinferenceattacks.Inthissectionwe
CIFAR-10 Top-1 81% 79% compareourdefensemechanismswiththefollowingapproaches
CIFAR-100 Top-1 49.1% 49.4% in[8]:
CIFAR-100 Top-5 78.5% 79.9% â€¢ NoisyGradients(NG).ToperformthisdefenseinVFL,the
CINIC-10 Top-1 66.7% 64.3% serveraddsalaplaciannoisetogradientsbeforesending
Yahoo!Answers Top-1 71.1% 67.5% themtopassiveparticipants.Themetricweanalyzetocom-
Criteo Top-1 71.3% 69.9% parethisapproachwithourKDğ‘˜ isthenoisescale,which
representsseveralscalesoftheusedlaplaciannoise.
â€¢ GradientCompression(GC).Thisstrategyusedforcom-
6.4 Performancewithdifferentvaluesofthe munication efficiency and privacy protection consists of
anonymizationparameters sharing fewer gradients with the largest absolute values.
Themetricweconsidertocomparethisapproachwithour
Inthisexperiment,weanalyzehowboththeASRandtheperfor-
KDğ‘˜isthecompressionrate,whichistheratiobetweenthe
manceofthemodel(intermsofaccuracy)changeinrelationto
uncompressedsizeandcompressedsizeofthegradientval-
highervaluesoftheanonymizationparametersğœ– andğ‘˜.Forthis
ues.
study,weconsideredonlythreedatasets,namelyCIFAR-10,CIFAR-
â€¢ Privacy-PreservingDeepLearning(PPDL).Ineachit-
100,andCINIC-10becausetheyhaveatleast10.Criteohasnot
eration,theserver(i)randomlyselectsonegradientvalue
been considered since it is a dataset with binary labels making
andaddsnoisetothisgradient;(ii)setstozerothegradient
itimpossibletotestoursolutionwithğ‘˜ higherthan2.Yahooin-
valuessmallerthanathresholdvalueğœ;(iii)repeatsthefirst
steadisnotincludedsincereliesonapre-trainedBertmodeland,
twostepsuntilÎ˜ ğ‘¢ fractionofgradientvaluesarecollected.
aswealreadystatedinSection6.2.1,itsaccuracyisintrinsecally
Bothğœ andÎ˜ ğ‘¢ arehyperparameterstobalancethetrade-off
guarateedbytheperformanceofsuchunderlingmodel,henceit
betweenmodelperformanceanddefenseperformance.We
cannotdecreaselowerthanthevaluespresentedinTable4with
evaluatetheperformanceofthistypeofdefensebyanalyzing
anyparametercombination.
Thatsaid,westudiedtheperformancefordifferentvaluesofğ‘˜ differentsettingsofthehyperparameterÎ˜ ğ‘¢.
(i.e.,ğ‘˜ =3,ğ‘˜ =5,andğ‘˜ =10).Astypicallydoneintheliterature
9OA ASR_Train ASR_Test
CIFAR10, k=3 CIFAR10, k=5 CIFAR10, k=10
100 100 100
80 80 80
60 60 60
40 40 40
20 20 20
0 0 0
0.2 0.3 0.4 0.5 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8
CIFAR100, k=3 CIFAR100, k=5 CIFAR100, k=10
100 100 100
80 80 80
60 60 60
40 40 40
20 20 20
0 0 0
0.2 0.3 0.4 0.5 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8
CINIC10, k=3 CINIC10, k=5 CINIC10, k=10
100 100 100
80 80 80
60 60 60
40 40 40
20 20 20
0 0 0
0.2 0.3 0.4 0.5 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8
Figure6:AnalysisoftheperformanceoftheattackandtheperformanceofKDğ‘˜fordifferentğœ–andğ‘˜values
Table8:ComparisonwithotherDefenseMechanismsagainstdirectlabelinferenceattacksusingCIFARdatasets
CIFAR-10 CIFAR-100
NoiseScale ModelAccuracy AttackAccuracy ModelAccuracy AttackAccuracy
1e-4 81.4% 80.6% 82.4% 12.2%
Noisy 1e-3 81.1% 49.1% 83.1% 2.0%
Gradients 1e-2 71.9% 24.5% 5.1% 2.0%
1e-1 10.0% 12.7% 5.0% 0.6%
CompressionRate ModelAccuracy AttackAccuracy ModelAccuracy AttackAccuracy
75% 80.4% 99.9% 82.4% 100%
Gradient 50% 80.5% 99.3% 83.1% 100%
Compression 25% 78.4% 92.4% 82.4% 99.9%
10% 10.0% 0.1% 73.8% 99.9%
Î˜ğ‘¢ ModelAccuracy AttackAccuracy ModelAccuracy AttackAccuracy
0.75 79.8% 39.0% 81.9% 4.6%
Privacy- 0.50 80.5% 38.9% 81.7% 4.5%
preservingDL 0.25 19.9% 0.1% 5.2% 1.1%
0.10 10.0% 0.1% 5.0% 0.9%
N ModelAccuracy AttackAccuracy ModelAccuracy AttackAccuracy
24 81.0% 96.7% 11.2% 99.9%
Discrete 18 80.8% 94.3% 8.8% 99.9%
SGD 12 78.7% 94.7% 7.1% 99.9%
6 74.3% 91.5% 7.3% 99.7%
k ğ ModelAccuracy AttackAccuracy ModelAccuracy AttackAccuracy
3 0.45 79.0% 38.5% 80.6% 32.6%
KDğ‘˜ 5 0.70 71.1% 23.0% 80.5% 19.2%
5 0.75 66.7% 21.7% 79.7% 19.1%
5 0.85 37.5% 14.2% 76.7% 18.8%
â€¢ DiscreteSGDacustomizedversionofsignSGD[1]thought sharedgradients.Followingthethree-sigmarule[21],the
forVFL.Thedefensemechanismproceedsasfollows.(i)In serversetsanintervalas[ğœ‡âˆ’2ğœ,ğœ‡+2ğœ](whereğœ‡isthemean
thefirstepoch,theserverobservesthedistributionofthe
10
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poTOA ASR_Train ASR_Test
CIFAR10, NG CIFAR10, GC CIFAR10, PPDL CIFAR10, DiscreteSGD CIFAR10, KDk
100 100 100 100 100
80 80 80 80 80
60 60 60 60 60
40 40 40 40 40
20 20 20 20 20
0 0 0 0 0
0 1e-4 1e-3 1e-2 1e-1 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 24 18 12 6 0.2 0.3 0.4 0.5 0.6
Noise Scale 1 - Compression Rate 1 - u N
CIFAR100, NG CIFAR100, GC CIFAR100, PPDL CIFAR100, DiscreteSGD CIFAR100, KDk
100 100 100 100 100
80 80 80 80 80
60 60 60 60 60
40 40 40 40 40
20 20 20 20 20
0 0 0 0 0
0 1e-4 1e-3 1e-2 1e-1 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 24 18 12 6 0.2 0.3 0.4 0.5 0.6
Noise Scale 1 - Compression Rate 1 - u N
CINIC10, NG CINIC10, GC CINIC10, PPDL CINIC10, DiscreteSGD CINIC10, KDk
100 100 100 100 100
80 80 80 80 80
60 60 60 60 60
40 40 40 40 40
20 20 20 20 20
0 0 0 0 0
0 1e-4 1e-3 1e-2 1e-1 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 24 18 12 6 0.2 0.3 0.4 0.5 0.6
Noise Scale 1 - Compression Rate 1 - u N
Figure7:ComparisonwithotherDefenseMechanismsagainstpassiveandactivelabelinferenceattacks
Table9:ComparisonwithotherDefenseMechanismsagainst Forthisexperiment,weperformedbothpassiveandactivela-
directlabelinferenceattacksusingCINIC-10dataset belinferenceattacksonthreedatasets:CIFAR-10,CIFAR-100,and
CINIC-10.Observethat,onceagain,astypicallydoneinthere-
CINIC-10 lated literature, for CIFAR-100 we considered only Top-5 accu-
NoiseScale ModelAccuracy AttackAccuracy
1e-4 70.5% 84.3% racytocopewiththelargenumberofclasses.Asforthesetting
GrN ao di is ey nts 1 1e e- -3
2
6 59 5. .9 5%
%
4 29 4. .7 3%
%
of the different defense mechanisms, we considered the follow-
1e-1 10.3% 12.6% ingparameters:Laplaciannoiselevelâˆˆ {10âˆ’1,10âˆ’2,10âˆ’3,10âˆ’4},
Compre 7s 5s %ionRate Mode 7l 0A .9c %curacy Attack 99A .8c %curacy gradient compression percentage âˆˆ {75%,50%,25%,10%}, PPDL
CoG mra pd reie sn sit on 5 2 10 5 0% %
%
6 5 19 4 0. . .1 7 0% %
%
9 9 09 2 .0. .3 5 1% %
%
Î˜ teğ‘¢ rvf ar la sct Nion âˆˆâˆˆ {6{ ,1 10 2% ,1,2 8,5 2% 4, }5 .0 T% h,7 e5% pa} r, aD mis ec tetr re ste oS fG oD urn au pm pb roer aco hf ii nn --
Î˜ğ‘¢ ModelAccuracy AttackAccuracy steadaresetasfollows:ğ‘˜ = 3andğœ– varyingbetweenthevalues
Privacy- 0 0. .7 55
0
6 69 8. .4 4%
%
3 38 8. .9 6%
%
âˆˆ{0.25,0.3,0.45,0.5,0.66}.
preservingDL 0.25 20.8% 0.10%
0.10 12.9% 0.04% 6.5.1 PassiveandActiveAttacks. Theresultsofthedefensesagainst
N ModelAccuracy AttackAccuracy
24 63.1% 97.9% themodelcompletionattackarereportedinFigure7.Asvisiblein
Discrete 18 59.6% 95.6% Figures7,fornoisygradients(NG),weexperimentedusingseveral
SGD 12 45.8% 94.3%
6 43.6% 90.3% scalesoflaplaciannoisetoevaluateitsdefenseperformanceagainst
k ğ ModelAccuracy AttackAccuracy modelcompletioninferenceattack.Obviouslythegreaterthevalue
3 0.45 67.7% 38.3%
KDğ‘˜ 5 0.70 62.2% 24.1% ofthenoisescalethemoresuccessfulareallthemitigationtech-
5 0.75 56.7% 22.2% niques.Tobeeffectivethisdefensemustapplytothegradientsan
5 0.85 34.5% 15.3%
extremelyhighlevelofnoisethatdisruptstheperformanceofthe
andğœisthestandarddeviation).Thegradientsoutsideofthe modelontheoriginaltask.Withlowerlevelsofnoise,itisinterest-
ingtoseehowthisapproachcanevenhelpobtaininghigherattack
intervalareregardedasoutliersandnotconsidered.(ii)The
performance.
serverslicestheintervalintoNsub-intervals.(iii)Before
Inthesecondcolumnofsub-figuresinFigure7,weevaluate
transmittingthegradientstoalltheparticipants,theserver
GradientCompression(GC)techniquesfordifferentcompression
firstroundseachgradientvaluetothenearestendpointof
thesub-intervals.Thehyperparameterğ‘controlshowmuch rates.Alsofromthesefigures,wecannoticethatforgreatercom-
pressionratesboththemodelandtheattackperformancedecrease.
magnitudeinformationofthesharedgradientsispreserved.
Wecanseehowbetweentheselecteddefensescomparedtoours,
Weevaluatethefourdefenseapproachesintroducedaboveand
wecomparethemwithourKDğ‘˜approach. gradientcompressionisthebestpreservingtheoriginalaccuracy
11
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poT
ycarucca
1-poT
ycarucca
5-poT
ycarucca
1-poTofthemodelbutonlyslightlyaffectingtheperformanceoftheat- them,whereastheyarekeptsecretfromalltheotherparties(pas-
tack,especiallywithlowervaluesofcompression.AsforthePPDL siveactors).Nevertheless,recentworkshavestartedtodescribe
mechanism, from the sub-figures in the third column in Figure labelleakageissuesinthiscontextproposingstrategiesforlabel
7wecannoticethatforallthreeanalyzeddatasets,thedefense inferenceattacks,namelypassive,active,anddirectattacks.Inthis
canmitigatelabelinferenceattackswiththehyperparameterÎ˜ ğ‘¢ paper, we analyzed such existing attacks and proposed a novel
setto0.25orlower(i.e.,theaccuracyresultislowerthan40%for defensemechanism,calledKDğ‘˜,abletoprotectVFLfromallthe
1âˆ’Î˜ ğ‘¢ =0.75).Eveninthiscase,wecannoticehowthedefense knowntypesoflabelinferenceattackswithveryhighperformance.
iseffectiveonlywithahighlevelofmanipulationofthegradient Ourapproachmodifiestheactiveparticipantmodel,integrating
resultinginaheavylossintermsofperformanceontheoriginal bothaKnowledgeDistillationteachernetworkandağ‘˜-anonymity
taskforbothCIFAR-10andCINIC-10.AsforCIFAR-100,instead, processingsteptoobtainagroupofğ‘˜ mostprobablesoftlabels
PPDLrepresentsthebest-performingdefensewearecomparing foreachiteminsteadofasinglehardlabel.Thisaddsalevelof
with. uncertaintythatpreventstheattackerfromperforminglabelin-
Finally,similarlytothepreviousdefenses,wecannoticehow ferencesuccessfully.Wetestedtheperformanceofoursolution
DiscreteSGDisnotcapableofaffectingtheattackpreservingthe withathoroughexperimentalcampaing,whoseobjectivewastode-
functionalityoftheoriginalmodel.Thisdefensecanachieveslightly mostratethatourapproachcaneffectivelyinhibittheattackerfrom
highperformanceonlyforCIFAR-10. beingabletoperformlabelinference(attacksuccessratereduced,
Lookingatoursolutioncomparedtotheotherswecanseehow insomecases,evenmorethan60%withrespecttoitsperformance
wecanpreventtheattackdecreasingitssuccessratetoalmostthe intheabsenceofourdefense),stillmaitaininganalmostunaltered
sameasarandomguessvalueonCIFAR-10andCINIC-10using accuracyofthefederatedglobalmodel(lessthan2%performance
extreme values forğœ–, while preserving most of the accuracy of decreaseonaverage).Finally,wedemonstratedthesuperiorityof
themainmodel.Itisalsointerestingtoseehow,evenwithlower ourproposalwithrespecttothemostrecentandstate-of-the-art
defenseintensityvalues,ourapproachaffectstheattackstillmore existingdefenses,whichprovedtobeeitheruneffectiveagaistthe
thantheothersolutions. attack,or,insomecases,effectiveagainstonlysomeattackvariants
andoftenatthecostofanextremelyhigh,andhencenotacceptable,
6.5.2 DirectLabelInferenceAttack. InTables8and9,weanalyze
impactonthefederatedglobalmodelperformance.
theperformanceofthethreeanalyzeddefensemechanismsand
Theproposalandresultsdescribedinthispapermustnotbe
KDğ‘˜againstthedirectlabelinferenceattack.Theemployeddatasets
seenasthefinalconclusionofthisreasearch.Asamatteroffact,
are,onceagain,CIFAR-10,CIFAR-100(seeTable8),andCINIC-10
inthefuture,weplantofurtherdevelopourproposedKDğ‘˜ de-
(seeTable9).Aswecansee,thebehaviorofthedefenseswearecom-
fensemethodtoprovideenhancedprotectionforotherkindsof
paringwithissimilartotheonewitnessedforthepassiveandactive
FLandattacks,designingamorecompleteprotectionframework.
attacks.Indeed,especiallyforCIFAR-10andCINIC-10,thedefenses
Forinstance,weintendtofocusalsoonHorizontalFL.Duetothe
areeffectiveonlywhenthealterationissuchthattheimpacton
peculiaritiesofthisvariant,athoroughexaminationmustbecon-
themaintaskaccuracyisnotnegligible.Theonlycountermeasure
ductedtocomprehendhowourdefensemechanismcanbeadjusted
capableofmatchingoursolutionintermsofpreservationofthe
accordingtoit.
originalmodelaccuracyanddetrimentoftheattackperformance
istheNoisyGradientsdefense.LookingatCIFAR-100,instead,we
REFERENCES
canseehowalsothePPDLdefenseiscapableofachievinggood
[1] JeremyBernstein,Yu-XiangWang,KamyarAzizzadenesheli,andAnimashree
results.Ourdefensecomparedtotheothersisequallyeffectiveon
Anandkumar.2018.signSGD:Compressedoptimisationfornon-convexproblems.
thethreeconsidereddatasets.Inthiscase,though,amorepowerful InInternationalConferenceonMachineLearning.PMLR,Vienna,Austria,560â€“569.
settingisrequiredtocounterthemorepowerfulDirectAttack. [2] CristianBuciluaË‡,RichCaruana,andAlexandruNiculescu-Mizil.2006. Model
compression.InProceedingsofthe12thACMSIGKDDinternationalconference
Insummary,fromtheaboveexperimentswecanconcludethat, onKnowledgediscoveryanddatamining.AssociationforComputingMachinery
ourdefensestrategyistheonlyoneobtaininggoodperformance (ACM),Beijing,China,535â€“541.
[3] JangHyunChoandBharathHariharan.2019. Ontheefficacyofknowledge
againstallthedifferentattacksandforalltheanalyzeddatasets.
distillation.InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
Generally,mostoftheotherdefesesfailedprotectingagainstlabel vision.IEEE,Seoul,SouthKorea,4794â€“4802.
inferenceattacks.OnlythePPDLandNoisyGradientssucceeded [4] Criteo.2024.CriteoAILab.https://ailab.criteo.com/ressources.
[5] LukeN.Darlow,ElliotJ.Crowley,AntreasAntoniou,andAmosJ.Storkey.2018.
insomeoftheconsideredattackscenariosbut,asvisibleinour
CINIC-10isnotImageNetorCIFAR-10. arXiv:1810.03505[cs.CV]
results,theycannotbeusedasageneraldefensebecausetheydo [6] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:
notprovideanadequateprotectionagainstallthepossibleattack Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.
[7] KaiFan,JingtaoHong,WenjieLi,XingwenZhao,HuiLi,andYintangYang.2023.
settings. FLSG:ANovelDefenseStrategyAgainstInferenceAttacksinVerticalFederated
Learning.IEEEInternetofThingsJournal11(2023),1816â€“1826.
7 CONCLUSION [8] ChongFu,XuhongZhang,ShoulingJi,JinyinChen,JingzhengWu,Shanqing
Guo,JunZhou,AlexXLiu,andTingWang.2022.Labelinferenceattacksagainst
FederatedLearning(FL)isanovelparadigmaimingattrainingML verticalfederatedlearning.In31stUSENIXSecuritySymposium(USENIXSecurity
22).USENIXAssociation,Boston,MA,USA,1397â€“1414.
modelsinaprivacy-preservingandcollaborativeway.Differently
[9] JianpingGou,BaoshengYu,StephenJMaybank,andDachengTao.2021.Knowl-
fromHorizontalFL,inVerticalFL(VFL)participantssharethesame edgedistillation:Asurvey.InternationalJournalofComputerVision129(2021),
samplespace,buttheirlocalprivatedatadifferinthefeaturespace. 1789â€“1819.
[10] GeoffreyHinton,OriolVinyals,andJeffDean.2015.DistillingtheKnowledgein
Moreover,instandardVFL,thelabelsofthesamplesaresensitive
aNeuralNetwork. arXiv:1503.02531[stat.ML]
informationandshouldbeprotectedfromhonest-but-curiouspar-
ties.Hence,onlytheaggregatingserver(oractiveactor)knows
12[11] IvanKholod,AndreyRukavitsyn,AlexeyPaznikov,andSergeiGorlatch.2021. [21] FriedrichPukelsheim.1994.Thethreesigmarule.TheAmericanStatistician48,2
Parallelizationoftheself-organizedmapsalgorithmforfederatedlearningon (1994),88â€“91.
distributedsources.TheJournalofSupercomputing77(2021),6197â€“6213. [22] PierangelaSamaratiandLatanyaSweeney.1998.Protectingprivacywhendis-
[12] AlexKrizhevsky,GeoffreyHinton,etal.2009.Learningmultiplelayersoffeatures closinginformation:k-anonymityanditsenforcementthroughgeneralization
fromtinyimages.UniversityofToronto,Toronto,ON,Canada. andsuppression.
[13] OscarLi,JiankaiSun,XinYang,WeihaoGao,HongyiZhang,JunyuanXie,Vir- [23] JiankaiSun,XinYang,YuanshunYao,andChongWang.2022. LabelLeak-
giniaSmith,andChongWang.2022.LabelLeakageandProtectioninTwo-party ageandProtectionfromForwardEmbeddinginVerticalFederatedLearning.
SplitLearning. arXiv:2102.08504[cs.LG] arXiv:2203.01451[cs.LG]
[14] JunlinLiuandXinchenLyu.2022. Clusteringlabelinferenceattackagainst [24] SeanVucinichandQiangZhu.2023.TheCurrentStateandChallengesofFairness
practicalsplitlearning. arXiv:2203.05222[cs.LG] inFederatedLearning.IEEEAccess11(2023),80903â€“80914.
[15] YangLiu,TianyuanZou,YanKang,WenhanLiu,YuanqinHe,ZhihaoYi,and [25] KangWei,JunLi,ChuanMa,MingDing,ShaWei,FanWu,GuihaiChen,and
QiangYang.2022.BatchLabelInferenceandReplacementAttacksinBlack-Boxed ThilinaRanbaduge.2022.VerticalFederatedLearning:Challenges,Methodologies
VerticalFederatedLearning. arXiv:2112.05409[cs.LG] andExperiments. arXiv:2202.04309[cs.LG]
[16] XinjianLuo,YunchengWu,XiaokuiXiao,andBengChinOoi.2021. Feature [26] KarlWeiss,TaghiMKhoshgoftaar,andDingDingWang.2016. Asurveyof
inferenceattackonmodelpredictionsinverticalfederatedlearning.In2021IEEE transferlearning.JournalofBigdata3,1(2016),1â€“40.
37thInternationalConferenceonDataEngineering(ICDE).IEEE,Chania,Greece, [27] WenshengXia,YingLi,LanZhang,ZhonghaiWu,andXiaoyongYuan.2023.
181â€“192. CascadeVerticalFederatedLearningTowardsStragglerMitigationandLabel
[17] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and PrivacyoverDistributedLabels.IEEETransactionsonBigData1(2023),1â€“14.
BlaiseAguerayArcas.2017. Communication-efficientlearningofdeepnet- [28] ChenglinYang,LingxiXie,SiyuanQiao,andAlanYuille.2018. Knowledge
worksfromdecentralizeddata.InArtificialintelligenceandstatistics.PMLR,Ft. DistillationinGenerations:MoreTolerantTeachersEducateBetterStudents.
Lauderdale,FL,USA,1273â€“1282. arXiv:1805.05551[cs.CV]
[18] LucaMelis,CongzhengSong,EmilianoDeCristofaro,andVitalyShmatikov.2019. [29] QiangYang,YangLiu,TianjianChen,andYongxinTong.2019.Federatedmachine
Exploitingunintendedfeatureleakageincollaborativelearning.In2019IEEE learning:Conceptandapplications.ACMTransactionsonIntelligentSystemsand
symposiumonsecurityandprivacy(SP).IEEE,SanFrancisco,CA,USA,691â€“706. Technology(TIST)10,2(2019),1â€“19.
[19] MiladNasr,RezaShokri,andAmirHoumansadr.2018.Comprehensiveprivacy [30] ChenZhang,YuXie,HangBai,BinYu,WeihongLi,andYuanGao.2021. A
analysisofdeeplearning.InProceedingsofthe2019IEEESymposiumonSecurity surveyonfederatedlearning.Knowledge-BasedSystems216(2021),106775.
andPrivacy(SP).IEEE,SanFrancisco,CA,USA,1â€“15. [31] XiangZhang,JunboZhao,andYannLeCun.2015.Character-levelconvolutional
[20] FelixPetersen,HildeKuehne,ChristianBorgelt,andOliverDeussen.2022.Dif- networksfortextclassification.Advancesinneuralinformationprocessingsystems
ferentiabletop-kclassificationlearning.InInternationalConferenceonMachine 28(2015),649â€”-657.
Learning.PMLR,Baltimore,MD,17656â€“17668. [32] LigengZhu,ZhijianLiu,andSongHan.2019. Deepleakagefromgradients.
Advancesinneuralinformationprocessingsystems32(2019),14747â€“14756.
13