1
Multi-Modal Data-Efficient 3D Scene
Understanding for Autonomous Driving
Lingdong Kong, Xiang Xu, Jiawei Ren, Wenwei Zhang, Liang Pan, Kai Chen, Wei Tsang Ooi, Ziwei Liu
Abstractâ€”Efficientdatautilizationiscrucialforadvancing3Dsceneunderstandinginautonomousdriving,whererelianceonheavily
human-annotatedLiDARpointcloudschallengesfullysupervisedmethods.Addressingthis,ourstudyextendsintosemi-supervised
learningforLiDARsemanticsegmentation,leveragingtheintrinsicspatialpriorsofdrivingscenesandmulti-sensorcomplementsto
augmenttheefficacyofunlabeleddatasets.WeintroduceLaserMix++,anevolvedframeworkthatintegrateslaserbeammanipulations
fromdisparateLiDARscansandincorporatesLiDAR-cameracorrespondencestofurtherassistdata-efficientlearning.Ourframeworkis
tailoredtoenhance3Dsceneconsistencyregularizationbyincorporatingmulti-modality,including1)multi-modalLaserMixoperationfor
fine-grainedcross-sensorinteractions;2)camera-to-LiDARfeaturedistillationthatenhancesLiDARfeaturelearning;and3)language-
drivenknowledgeguidancegeneratingauxiliarysupervisionsusingopen-vocabularymodels.TheversatilityofLaserMix++enables
applicationsacrossLiDARrepresentations,establishingitasauniversallyapplicablesolution.Ourframeworkisrigorouslyvalidated
throughtheoreticalanalysisandextensiveexperimentsonpopulardrivingperceptiondatasets.ResultsdemonstratethatLaserMix++
markedlyoutperformsfullysupervisedalternatives,achievingcomparableaccuracywithfivetimesfewerannotationsandsignificantly
improvingthesupervised-onlybaselines.Thissubstantialadvancementunderscoresthepotentialofsemi-supervisedapproachesin
reducingtherelianceonextensivelabeleddatainLiDAR-based3Dsceneunderstandingsystems.Codeandbenchmarktoolkitsare
publiclyavailableathttps://github.com/ldkong1205/LaserMix.
IndexTermsâ€”Semi-SupervisedLearning;LiDARSemanticSegmentation;3DSceneUnderstanding;AutonomousDriving;Robustness
âœ¦
1 INTRODUCTION supervisedlearning.Forinstance,thespatialdistributionof
pointsinaLiDARscandirectlycorrespondstothephysical
LiDAR segmentation stands as a cornerstone task in the
layout of the environment, offering robust cues that are
domain of autonomous driving perception, essential for
absentintraditional2Dimages[10].
vehicles to effectively understand the dense 3D structure
Despiterecenteffortsinadaptingsemi-supervisedlearn-
of their surrounding environment [1], [2]. This capability
ingfor3Ddata,mostexistingmethodologiesfailtofullyex-
is fundamental for safe navigation and interaction with
ploitthesynergisticpotentialofcombiningLiDARwithother
complexanddynamicenvironments[3],[4],[5],[6].
sensormodalities[22],[23],[24],[25].Thisunderutilization
However,therequirementforextensivemanualannota-
representsamissedopportunity,particularlyinautonomous
tion of LiDAR point clouds imposes significant costs and
driving systems equipped with multiple types of sensors,
logisticalchallenges,whichseverelylimitsthescalabilityof
includingcamerasandradar,alongsideLiDAR[26],[27],[28].
fullysupervisedlearningmethodsinreal-worldapplications
Eachsensortypeprovidescomplementaryinformationthat
[7], [8], [9], [10], [11], [12]. Given these constraints, semi-
canenhancethemodelâ€™sunderstandingofitsenvironment,
supervised learning emerges as a promising solution that
particularly under varying operational conditions such as
leveragestheabundanceofreadilyavailableunlabeleddata
lowlightoradverseweather[29],[30],[31],[32].
to reduce dependence on costly human annotations while
Building on this premise, this work introduces Laser-
stillmaintainingsatisfactoryperceptionaccuracy[13],[14].
Mix++, an advanced data-efficient 3D scene understand-
Traditional semi-supervised learning approaches have
ing framework that expands the semi-supervised learning
predominantlyfocusedonimage-basedtasks,whereasmeth-
paradigmtoincorporatemulti-modaldataintegration.The
odslikeMixMatch[15],FixMatch[16],andothers[17],[18],
baselineLaserMixframework[10]leveragedspatialpriors
[19],[20],[21]haveshownconsiderablesuccess.However,
inherentinLiDARdata(asshowninFigure1a)bymixing
these methods often underperform when directly applied
laserbeamsfromdifferentscanstoenhancetheconsistency
toLiDARdataduetotheinherentdifferencesbetweenthe
and confidence of predictions across unlabeled datasets.
RGBimagedataandLiDARpointclouds[10],[11].LiDAR
ThisapproachutilizedthegeometricdistributionofLiDAR-
dataencapsulatesrichgeometricandtopologicalinformation
acquired driving scenes to infer the scene semantics with
whichpresentsuniquechallengesandopportunitiesforsemi-
minimalsupervision,settingaseminaryetstrongbenchmark
insemi-supervised3Dsceneunderstanding[10].
â€¢ L. Kong and W. T. Ooi are with the School of Computing, National Expandinguponthis,LaserMix++integratesadditional
UniversityofSingapore.L.KongisalsowithCNRS@CREATE,Singapore. sensor modalities, specifically camera data, to address the
â€¢ X.XuiswiththeCollegeofComputerScienceandTechnology,Nanjing
complexitiesofautonomousdrivingenvironmentsmorecom-
UniversityofAeronauticsandAstronautics.
â€¢ W.Zhang,L.Pan,andK.ChenarewithShanghaiAILaboratory. prehensively.ByharnessingbothLiDARandcamerainputs,
â€¢ J.RenandZ.LiuarewithS-Lab,NanyangTechnologicalUniversity. LaserMix++ aims to exploit the complementary nature of
â€¢ ThecorrespondingauthorisZiweiLiu:ziwei.liu@ntu.edu.sg.
spatialpriorsfromLiDARandtexturaldetailsfromcamera
4202
yaM
8
]VC.sc[
1v85250.5042:viXra2
vvveeeggg mmmIIIoooUUU(((%%%))) FFFuuullllll
777666
555000%%% CCCyyyllliiinnndddeeerrr333DDD
lllooowwweeerrr mmmiiiddd uuuppppppeeerrr
[[[CCCVVVPPPRRRâ€™â€™â€™222111]]]
222000%%%
777444
PPPooolllaaarrrSSStttrrreeeaaammm
[[[NNNeeeuuurrrIIIPPPSSSâ€™â€™â€™222111]]]
111000%%% SSSaaalllsssaaaNNNeeexxxttt
777222
cccaaarrr LLLiiiDDDAAARRR RRReeeppprrreeessseeennntttaaatttiiiooonnn [[[IIISSSVVVCCCâ€™â€™â€™222000]]]
FFFIIIDDDNNNeeettt
777000 [[[IIIRRROOOSSSâ€™â€™â€™222111]]]
lllooowwweeerrr mmmiiiddd uuuppppppeeerrr OOOppptttiiiooonnn 111::: RRRaaannngggeee VVViiieeewww OOOppptttiiiooonnn 222::: VVVoooxxxeeelll PPPooolllaaarrrNNNeeettt
[[[CCCVVVPPPRRRâ€™â€™â€™222000]]]
666888
LLLaaassseeerrrMMMiiixxx++++++,,, RRRaaannngggeeeVVViiieeewww
666666
LLLaaassseeerrrMMMiiixxx++++++,,, VVVoooxxxeeelll
rrroooaaaddd
LLLaaassseeerrrMMMiiixxx LLLaaassseeerrrMMMiiixxx LLLaaassseeerrrMMMiiixxx,,, RRRaaannngggeeeVVViiieeewww
666444 LLLaaassseeerrrMMMiiixxx,,, VVVoooxxxeeelll
lllooowwweeerrr mmmiiiddd uuuppppppeeerrr SSSuuuppp...---ooonnnlllyyy,,, RRRaaannngggeeeVVViiieeewww
(((555777...555)))
SSSuuuppp...---ooonnnlllyyy,,, VVVoooxxxeeelll
666222
(((aaa))) (((bbb))) (((ccc)))
(a)SpatialPrior (b)LiDARModality (c)PerformanceOverview
Fig.1:Motivation.(a)WeobserveastrongspatialpriorfromLiDAR-acquireddrivingscenes,whereobjectsandbackgrounds
aroundtheego-vehiclehaveapatterneddistributionondifferent(lower,middle,andupper)laserbeams.(b)Theproposed
laserbeammixingtechniqueisagnostictodifferentLiDARmodalitiesandcanbeuniversallyappliedtoexistingLiDAR
segmentationbackbones.(c)Ourapproachesachievedsuperiorperformancethanstate-of-the-artmethods[33],[34],[35],
[36],[37]underbothlow-data(10%,20%,and50%labels)andhigh-data(fullsupervision)regimesonnuScenes[38].
images. This multi-modal approach enhances the modelâ€™s theoveralleffectivenessofthemulti-modaldata-efficient3D
abilitytogeneralizeacrossdifferentscenarios,particularly sceneunderstandingsystem.
inconditionswheresupervisionsignalsarenotsufficiently OurextensivevalidationsofLaserMix++onprominent
available[7],[10],[11].LaserMix++introducesthreenovel multi-modaldrivingperceptiondatasets,suchasnuScenes
componentstoachievebettermulti-modalintegration: [38], SemanticKITTI [28], and ScribbleKITTI [7], confirms
its effectiveness and superiority. Despite the simplicity of
â€¢ Multi-ModalLaserMixOperation:Weextendtheorig-
thepipeline,LaserMix++notonlymeetsbutoftenexceeds
inalLaserMix[10]toincludecameraimages,allowing
the performance of fully supervised methods while re-
themodeltoprocess,mix,andmanipulateinformation
quiring significantly fewer human annotations. Moreover,
frombothLiDARpointcloudsandtheircorresponding
LaserMix++ directly operates on LiDAR point clouds so
cameraimages.Thisfusionnotonlyenrichesthefeature
as to be agnostic to different LiDAR representations (see
setbutalsoalignsspatialandtexturaldata,enhancing
Figure1b),e.g.,rangeview[40],birdâ€™seyeview[37],sparse
thedescriptivepowerofthespatialpriors.
voxel[33],andmulti-viewfusion[41].Thespecialproperty
â€¢ Camera-to-LiDAR Feature Distillation: Leveraging
marks LaserMix++ as a universally applicable solution.
recentendeavorsinimagesegmentation,weproposeto
Besides, the substantial reduction in the need for labeled
extractsemanticallyrichfeaturesfromcameraimages
data,coupledwiththeabilitytointegrateandleveragemulti-
and integrate them into the LiDAR data processing
modal inputs, underscores the potential of more scalable
stream.Thisprocesshelpsinenhancingthedata-efficient
semi-supervisedapproachesinthecontextofLiDAR-based
learningoftheLiDARpointclouddata,particularlyby
3Dsceneunderstanding.
improvingthefeaturerepresentationinenvironments
whereLiDARdataannotationaloneisinsufficient. By providing a robust solution to the challenges of
â€¢ Language-DrivenKnowledgeGuidance:Drawingon dataannotationandsensordatautilizationinautonomous
recentadvancementsinvision-languagemodels[39],we vehicles,LaserMix++setsanewstandardfordata-efficient
aimtoutilizeopen-vocabularydescriptionstoprovide learninginthefield.AsshowninFigure1c,ourapproaches
contextualcuesthatassistinthedata-efficientlearning exemplifyhowtheintegrationofmulti-modaldatacanlead
process. By generating auxiliary labels through these topromisingimprovementsinthereliabilityandefficiency
models,LaserMix++canprovideadditionalsupervisory ofautonomousdrivingperceptiontechnologies.Tosumup,
signals to the semi-supervised learning framework, thisworkconsistsofkeycontributionsasfollows:
furtherrefiningandimprovingthemodelâ€™spredictions.
â€¢ WepresentLaserMix++,anoveldata-efficient3Dscene
Thecombinationoftheabovecomponentsnotonlyad- understanding framework that integrates LiDAR and
dressesthelimitationsofusingsingle-modalitydatabutalso camera data to enhance feature learning through tex-
booststherobustnessandaccuracyofthesemi-supervised turalandspatialsynergies,tailoredtoimprovemodel
LiDAR segmentation model. Each step in the expansion interpretationundervariouslow-dataregimes.
of our LaserMix++ framework builds logically on the last, â€¢ Buildingoncross-sensordataintegration,weintroduce
ensuringthattheenhancementscontributemeaningfullyto two pivotal enhancements: camera-to-LiDAR feature3
distillationandlanguage-drivenknowledgeguidance. facilitateself-training,thoughtheymayintroduceconsider-
These components work together to generate robust ablestoragedemandswhenscaledtolargeLiDARdatasets
auxiliary signals that enrich the training data without [26],[27],[28],[38].OurproposedLaserMix++framework
theneedforadditionalannotations. buildsupontheseprinciples,adaptingandenhancingthem
â€¢ Ourapproachesarerigorouslyformulatedtoleverage to maintain scalability and efficiency in 3D environments
spatialcuesinLiDARdataeffectively,facilitatingsemi- withouttheneedforsignificantcomputationalresources.
supervisedlearningandensuringthatourmethodology
isbothpracticalandtheoreticallysound. 2.3 Data-EfficientLearningin3D
â€¢ Extensivelyvalidatedagainststate-of-the-artmethods,
While semi-supervised learning has been extensively ex-
LaserMix++demonstratessignificantperformanceim-
ploredin2Dcontexts,itsapplicationto3Ddata,especially
provements across both low- and high-data regimes,
outdoorLiDARpointclouds,islessmature.Mostexisting
underscoringthepotentialtorevolutionizedata-efficient
studies focus on semi-supervised learning techniques for
3Dsceneunderstandinginamoreunifiedmanner.
object-centric point clouds [69], [70] or indoor scenes [3],
[71], [72], [73], which typically do not encounter the scale
2 RELATED WORK andvariabilitypresentedinoutdoorenvironments[26],[28],
[74]. Some efforts [23], [24], [25], [75] have been made to
This section provides a literature review of works that are
applysemi-supervisedstrategiesto3Dobjectdetectionusing
closelyrelatedtodata-efficient3Dsceneunderstanding.
LiDARdata.For3Dsceneunderstanding,GPC[22]explores
semi-supervisedpointcloudsemanticsegmentationthrough
2.1 3DSceneUnderstanding contrastive learning but remains focused on indoor point
clouds, thus not fully addressing the unique properties of
The task of 3D scene understanding via LiDAR data is
outdoor LiDAR data. LaserMix [10] establishes the first
fundamentalforvariousapplications,especiallyautonomous
benchmarkforsemi-supervisedLiDARsegmentationbased
driving[42],[43],[44],robotics[45],andmixedreality[46].
on large-scale driving datasets [7], [28], [38]. It employs a
Severalmethodologieshavebeenemployedtoaddressthe
dual-branchframeworktoencourageconsistencyinpredic-
challengesofLiDARscenesegmentation,categorizedmainly
tionsfromLiDARscansbeforeandafterlaser-wisemixing.
by the type of data representation: range view [35], [36],
Thesubsequentwork,LiM3D[11],proposestoreducethe
[40], [47], [48], [49], [50], birdâ€™s eye view [37], [51], sparse
spatiotemporalredundancyandsplitthemostinformative
voxel [33], [41], [52], [53], and multi-view fusion [54], [55],
dataasthelabeledset,resultinginimprovedperformance.
[56].Whilethesefully-supervisedapproacheshaveachieved
In this work, we address the limitations of these previous
significantmilestones,theirrelianceonextensiveandmeticu-
single-modality methods by integrating cross-sensor data.
louslyannotateddatasetsposesachallenge.Thisdependency
WepresentLaserMix++toenhancefeaturelearningthrough
onlarge-scaleannotationsresultsindiminishedperformance
fine-grainedLiDARandcameradatasynergies,exhibitinga
whendataisscarce[13].Toaddressthisproblem,innovations
strongerperformanceinbothhigh-andlow-dataregimes.
inweak[8],[57],scribble[7],andbox[58]supervisions,along
withactivelearningtechniques[12],[59],havebeenproposed
2.4 Multi-ModalDrivingPerception
to mitigate the high costs of LiDAR data annotation. Our
approachextendstheseeffortsbyleveragingsemi-supervised Advanceddrivingperceptionsystemsareequippedwitha
learningtoeffectivelyutilizeunlabeledLiDARscans,thus combinationofversatilesensorsofdifferenttypes[26],[27].
enhancing the robustness and reducing the reliance on Prevailingsensorconfigurationsoftenincludeoneormore
extensivelabeleddatasetsfortrainingeffectivemodels. LiDARs,multipleRGBcamerascoveringsurroundingviews,
aswellasradar,IMU,GPS,etc.Thedataacquiredbydifferent
sensorstendtocomplementeachother,furtherenhancing
2.2 Data-EfficientLearningin2D
theresilienceoftheperceptionsystem[4],[29],[30].Recently,
Thedomainof2Dimageprocessinghasseenconsiderable severalworkshaveexploredtheintegrationofLiDARand
successinapplyingsemi-supervisedlearningtechniquesto cameras for driving perception. SLidR [76], Seal [77], and
reducetheneedforlabeleddata.Foundationalalgorithms ScaLR[78]establishpretrainingobjectivesusingtheimage-
suchasPi-Model[60],MeanTeacher[18],andvariousmix- to-LiDARcorrespondence.xMUDA[79]andthesubsequent
based methods like MixMatch [15], ReMixMatch [17], and works[80],[81]proposetoleverageimagedataforunsuper-
FixMatch [16] have set benchmarks in image recognition viseddomainadaptationofLiDARsegmentationmodelsin
tasks. Notably, in semantic segmentation, methods like cross-domain scenarios. CLIP2Scene [82], OpenScene [83],
CutMix-Seg [61] and PseudoSeg [62] alter input data to andCNS[84]utilizeCLIP[39]togenerateopen-vocabulary
strategicallypositiondecisionboundariesinlessdenseareas predictions on LiDAR point clouds. Most recently, M3Net
ofthelabelspace.Consistency-basedmethodssuchasCPS [85]introducesaunifiedmulti-datasettrainingframework
[21]andGCT[20]enforcemodelstabilitybetweenmodified usingtheimagemodalitytobridgeheterogeneousLiDAR
networkoutputs[63].Whiletheseperturbationsandconsis- data acquired from different datasets. Motivated by these
tencyenforcementsshowpromisein2Dtasks,theirefficacy endeavors,inthiswork,wepursuetheintegrationofLiDAR
diminisheswhendirectlyappliedtothe3Ddomainduetoits andcameradatafordata-efficient3Dsceneunderstanding.
inherentcomplexityanddatarepresentationchallenges[64], TheproposedLaserMix++frameworkconsistsofcamera-to-
[65], [66]. Techniques that focus on entropy minimization, LiDARfeaturedistillationandlanguage-drivenknowledge
like CBST [67] and ST++ [68], generate pseudo-labels to guidancemodulestailoredtogenerateauxiliarysupervisions4
forunlabeleddata.Thesecomponentssynergizetoachieve takesfull-sizeddataasinputsduringtheinference.Therefore,
state-of-the-artperformanceacrossvariousbenchmarks. tocomputeP(Y in|X in,A)inEquation(3),wefirstpadthe
dataoutsidetheareatoobtainthefull-sizeddata.Here,we
3 DATA-EFFICIENT 3D SCENE UNDERSTANDING denotethedataoutsidetheareaasX out;wethenletthemodel
inferP(Y in|X in,X out,A),andfinallymarginalizeX out as:
Inthissection,wefirstintroducethespatialpriorofLiDAR-
based driving scenes (Sec. 3.1). We then present LaserMix, P(Y in|X in,A)=EË† Xout[P(Y in|X in,X out,A)]. (4)
which strives to efficiently encourage confident and con-
It is worth noting that the generative distribution of the
sistentLiDARpredictions(Sec.3.2).Finally,weestablisha
strong3Dsceneconsistencyregularizationbaseline(Sec.3.3).
paddingP(X out)canbedirectlyobtainedfromthedataset.
Training Objectives. Finally, we train the segmentation
modelG Î¸ usingthestandardmaximum-a-posteriori(MAP)
3.1 SpatialPriorin3DSceneUnderstanding
estimation.Wemaximizetheposteriorthatcanbecomputed
SpatialPriorObservation.Understandingandutilizingthe by Equation (2), Equation (3), and Equation (4), which is
spatialdistributioninherentinLiDARscenesispivotalfor formulatedasfollows:
enhancing semi-supervised learning. LiDAR point clouds C(Î¸)=L(Î¸)âˆ’Î»HË†(Y |X ,A)=L(Î¸)
in in
offeruniquespatialpriorsthatarenotprevalentin2Dimages. (5)
âˆ’Î»EË† [P(Y |X ,A)logP(Y |X ,A)].
As shown in Tab. 1, each LiDAR-acquired semantic class Xin,Yin,A in in in in
posesuniquedistributionpatternsthatdirectlyreflectreal- Here,L(Î¸)isthelikelihoodfunctionwhichcanbecomputed
world driving scenes. Our methodology leverages these usinglabeleddata,i.e.,theconventionalsupervisedlearning.
priors by encouraging the model to maintain consistent Minimizing HË†(Y in|X in,A) requires the marginal probabil-
predictionsacrossvaryingLiDARdatamanipulations. ity P(Y in|X in,A) to be confident, which further requires
SpatialPriorFormulation.Thespatialdistributionofobjects P(Y in|X in,X out,A)tobebothconfidentandconsistentwith
andbackgroundswithinaLiDARscenesignificantlyinflu- respecttodifferentoutsidedataX out.Tosumup,ourpro-
encestheirrepresentationsinthepointclouddata.Objects
posedsemi-supervisedlearningframeworkinEquation(5)
and backgrounds at different distances and orientations
encouragesthesegmentationmodeltomakeconfidentand
fromthesensorexhibitdistinctspatialpatternsandcanbe
consistentpredictionsatapredefinedarea,regardlessofthe
leveragedtoreducethepredictionuncertaintyinunlabeled dataoutsidethearea.ThepredefinedareasetAdetermines
scenarios. Specifically, we define a spatial area a âˆˆ A theâ€œstrengthâ€oftheprior.WhensettingAtothefullarea
where LiDAR points and their semantic labels inside this (i.e.,thewholepointcloud),ourframeworkdegradestothe
area (denoted as X in and Y in, respectively) exhibit lower classicentropyminimizationframework[86].
variation.Thisisquantifiedbyasmallerconditionalentropy
Practical Implementations. Implementing our proposed
H(X in,Y in|A),whereArepresentsdifferentspatialregions
prior-basedsemi-supervisedlearningframeworkeffectively
withinthedata.
involvesthreecriticalsteps,whichare:
EntropyMinimization.Giventhespatialprior,ourobjective
â€¢ Step1):IdentifyandselectanappropriatepartitionsetA
istominimizetheentropyofpredictionswithinpredefined
thatencapsulatesastrongspatialprior,whichisessential
areas.Formally,weexpresstheentropyconditionasfollows:
forguidingthelearningprocess;
E Î¸[H(X in,Y in|A)]=c, (1) â€¢ Step2):Efficientlycomputethemarginalprobability,i.e.,
wherecisasmallconstantandÎ¸representsthemodelparam-
P(Y in|X in,A),whichisfundamentalforunderstanding
thedistributionoflabelswithinspecifiedspatialregions;
eters.Similartotheclassicentropyminimizationframework
â€¢ Step3):Efficientlyminimizethemarginalentropy,rep-
[86], the constraint in Equation (1) is transformed into a resented as HË†(Y in|X in,A), to enhance the consistency
probabilisticmodelwherethemodelparameterdistribution
andconfidenceofthepredictionsacrossunlabeleddata.
isguidedbytheprincipleofmaximumentropy:
A detailed and effective implementation of these steps
P(Î¸)âˆexp(âˆ’Î»H(X ,Y |A))
in in is proposed in the subsequent section, showcasing their
(2)
âˆexp(âˆ’Î»H(Y |X ,A)), practicalapplicabilityandimpactontheoverallframework.
in in
where Î» > 0 acts as the Lagrange multiplier associated
withtheentropyconstraint,whichcorrespondstoconstant 3.2 LaserMix
c.H(X in|A)hasbeenignoredforbeingindependentofthe
LiDAR Scene Partitions. The prevailing rotating LiDAR
model parameter Î¸. Here, we consider Equation (2) as the
sensorsdeployafixednumber(e.g.,32,64,and128)oflaser
formalformulationofthespatialprioranddiscusshowto
beamswhichareemittedisotropicallyaroundtheego-vehicle
empiricallycomputeitinthefollowingsections.
withpredefinedinclinationanglesasshowninFigure2.To
Marginalization.ToutilizethespatialpriordefinedinEqua-
delineatedistinctandproperspatialareasA,weproposeto
tion(2),weempiricallycomputetheentropyH(Y in|X in,A)
partitiontheLiDARpointcloudbasedontheselaserbeams.
oftheLiDARpointsinsideareaAasfollows:
Specifically,eachpointcapturedbyaparticularlaserbeam
HË†(Y |X ,A)= alignsataconsistentinclinationanglerelativetothesensor
in in
EË† [P(Y |X ,A)logP(Y |X ,A)], (3) plane.Forpointi,itsinclinationÏ• i iscalculatedasfollows:
Xin,Yin,A in in in in
pz
w Lih De Are RË†. sd ege mno et ne ts att ih oe ne mm op di eri lc Gal Î¸e (wsti im tha pti ao rn a. mT eh tee rsen Î¸d )- ut so u-e an lld
y
Ï• i =arctan( (cid:112) (px i)2i +(py i)2), (6)class distribution heatmap
road
class 1 2 d3istr4ibu5tio6n 7 8 heatmap
car
road 1 2 3 4 5 6 7 8
1 2 3 4 5 6 7 8
traffic-sign
car 1 2 3 4 5 6 7 8
class distribution heatmap
1 2 3 4 5 6 7 8
building
traffic-sign 1 2 3 4 5 6 7 8
road
1 2 3 4 5 6 7 8
vegetation
1 2 3 4 5 6 7 8
building 1 2 3 4 5 6 7 8
car
1 2 3 4 5 6 7 8
motorcycle
cc ll aa ss ss 1 2 dd3ii ss tt rr4i ib bu u5t ti io o6n n 7 8 h he ea at tm ma ap p
vegetation 1 2 3 4 5 6 7 8
traffic-sign
1 2 3 4 5 6 7 8
truck
rr oo aa dd 1 2 3 4 5 6 7 8
motorcycle 1 2 3 4 5 6 7 8
1 2 3 4 5 6 7 8
1 2 3 4 5 6 7 8
building
1 2 3 4 5 6 7 8
parking
cc aa rr 1 2 3 4 5 6 7 8
truck 1 2 3 4 5 6 7 8
1 2 3 4 5 6 7 8
1 2 3 4 5 6 7 8
vegetation 5
sidcleawsaslk 1 2 d3istr4ibu5tio6n 7 8 heatmap
TABLE1:Aca tts rre aas fft ffu iid ccy -- sso iin ggt nnhestrong 1spati 2alpr 3ioro 4frep 5rese 6ntati 7vese 8manticclassesfromtheSemanticKITTI[28]dataset.
Foreachclass,wpeashrokwinitgstype(static1ord2ynam3 ic),4prop5ortio6n(v7alid8numberofpointsinpercentage),distributionamong
eightareas(A m= o{ ta o1, ra c2y,. c.. l, ea 8},i.e.,ei1 1ghtl2 2aser3 3beam4 4 gro5 5ups)6 6,and7 7the8 8heatmapinrangeview(lightercolorscorrespondto
areasthathaveatherirgorhaaedrinlikelihoodto1 appe2 aran3 dvi4 ceve5 rsa).6 Bes7 tview8
edincolorsandzoom-edinforadditionaldetails.
bb uu ii ll dd ii nn gg 1 2 3 4 5 6 7 8
Class sTiydpeewaPlrkoportion 11 22 33 Dis44 tribu55 tion 66 77 88 Heatmap
1 2 3 4 5 6 7 8
1 2 3 4 5 6 7 8
truck
pecrasron 1 2 3 4 5 6 7 8
vegetation vv eestggac tieecla tt aas tts ii o2o4 nn.825% 1 2 d 3istr 4ibu 5tio 6n 7 8 heatmap
terrain 11 22 33 44 55 66 77 88
1 2 3 4 5 6 7 8
1 2 3 4 5 6 7 8
parking
road mmtr sa ootaff tttreif oocoinc rrac- ccdse yyi2 ccg2 ll.n ee545% 1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
person 11 22 33 44 55 66 77 88
class 111 222 d333 istr444 ibu5 55 tio66 6n 77 7 88 8 heatmap
sidewalk
building bstautt riic uldnikn1 g2.118% 1 2 3 4 5 6 7 8
tt crrc luu aa cc sr kk
s
1 2 d3 istr4 ibu5 tio6
n
7 8
heatmap
terrain sttaef tre ircon ra ac de in8.122% 11 111 22 222 33 333 4 4444 5 5 555 6 66 66 77 7 77 888 88
tv re ppag aaf rp fe rr oiot kkc aal - ii det s nni i go ggn n 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 6 6 6 7 7 7 8 8 8
car dynt ar mcu iacn rk 4.657% 1 11 1 2 22 2 3 33 3 4 44 4 55 55 66 66 77 77 8 88 8
person 11 22 33 44 55 66 77 88
pole
m trssb
s
aob ii tadud ft ti
pi
fo cc i ee
c
il oay cwr wd
l
-rcc ei saayl ne illc gg 0kk .l
2
ne
96%
1
1
111 1
1
2
2
22 22
2
3
3
33 33
3
4
4
44 44
4
5 5
55
55
5
6 6
66
66
6
7 7
77
77
7
8 8
8
888
8
fence 11 22 33 44 55 66 77 88
traffic-sign o tt v rh e satt te ag eet ftr fir e rr c- iu t rr cv a aac -e t sk iih i nn io g0i .c n 0 n6le 1% 1 11 2 22 3 33 4 44 5 55 6 66 7 77 8 88
bb ui ic ly dc il ne g 1 11 1 2 22 2 3 33 3 4 44 4 55 55 66 66 7 77 7 88 88
11 22 33 44 55 66 77 88
person mdybp opni aa tec mory rik crsc ci olni ynsg c0t.l0e36% 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8
bpueirldsoinng
ot vh ee gr e- tv ae th ioic nle 1 11 1 2 22 2 3 33 3 4 44 4 55 55 66 66 7 77 7 88 88
11 22 33 44 55 66 77 88
motorcyclist mdsoynitda ftom erer unicwc ccya kecl0 lk. i0 s0 t4% 1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
vegfeentacteion
mb oi tc oy rc cl yis ct le 1 11 1 2 22 2 3 33 3 4 44 4 55 55 66 66 7 77 7 88 88
11 22 33 44 55 66 77 88
where(px,poy,tphz pt)ee arrer r-prg krareisnioen guntnthdeCartes1 1ianc2 2oord3 3inate4 4sof5 5the 6 6 the7 7afor8 8ementionedlaser-partitionedareasAfromtwoscans
trunk
L griD ouA pR ap llo pin ot is m n.m tF sooo ftr rtt o oa ro mn r ury c cc eyt kayw c cco l hilL se si ctD anAR bysc ta h1 11 1n es ir,x in2 22 21 ca lin nd 3 33 3atx io2 n, 4 44 4w ae ngfi 55 55lr es st .66 66 i An 1a 7 77 7n =in {88 88te ar 1t ,w ai 3n ,i .n ..g }w anay d,i t.e h. e,o on te ht ea rke tas kf ero sm fro od md- ein vd enex -ie nd da er xe ea ds
1 2 3 4 5 6 7 8
More concretely,pteoressotanblish m non-1over2lapp3ing4area5s, a 6 are7asA82 ={a 2,a 4,...},sothateachareaâ€™sneighborwillbe
setofm+1incs lii nd tpa re t uoiow l cne ka al nk glesÎ¦={Ï•1 0,Ï• 12,Ï• 2,3...,Ï•4 m}w5 ill 6 fro7 mth8 eotherscan.Moreformally,wedefinetheLaserMix
be evenly saomtphleedr-wgirthoiunnthde range1 1of th2 2e m3 3inim4 4um a5 5nd 6 6 ope7 7ratio88 nasfollows:
parking 1 2 3 4 5 6 7 8
maximuminclinationanglesintheda1taset2,and3the4area5set 6 7 8
A a i= in{ tha 1 e, ia n2 c, li. n.. a,a tpibtm of e ai ne } cr rn rcr y kaaac c inn ne i l gn et eghe [Ï•n iâˆ’be 1,c Ï•o in ).fi In1 11 te id sib2 2 2my pb oo3 3 3ru tan nd t4 4 4in tog na5 5 5ore ta e 6 6 6 7 7 7 8 8 8 xËœxËœ 11 =,xËœ 2 xa 1= 1 âˆªLa xs a 2e 2rM âˆªi xx a 1( 3x 1 âˆª,x Â·Â·2 Â·), , (7)
thattherangeosfiidnceliwnaatiloknanglesva 111 riesd 222 epe 33n3 din 444 gon 555 the 666 77 7 8 88 xËœ 2 =xa 21 âˆªxa 12 âˆªxa 23 âˆªÂ·Â·Â· ,
specificconfi og tu hra p eti reo -rn vss eoo hf nd icif lf eerentLiD 1AR 2senso 3rs. 4 5 6 wh7ere x8a ij is the data crop of x i confined within area area
R aio mle si tn oo eu ffr ef cr ta im vees lw ytiod eâ€œr erek x: rw cT aitha iee nâ€lkp ar so tp roo ns ged spla a11s tie ar l-b p22a ris oe rd i33npa thrt e44it Lio iDn 55i An Rg
66
a j.
7
7Cor
8
8respondingly, the semantic labels are mixed in the
1 2 3 4 5 6 sam7e w8ay as Equation (7). It is worth highlighting that
pointcloud,asdescribedbyStep1inoursemi-supervised
LaserMixisdirectlyappliedtotheLiDARpointcloudsand
fence 1 2 3 4 5 6 7 8
learning framewboicrky.cAlisstshown in Figure 1a and Tab. 1,
terrain isthusagnostictothevariousLiDARrepresentations,such
this partitioning pr eev re sa ols nclear distrib 11ution 22 pa 3t3tern 44s in 55the 66 as7 7rang8 8e view [40], birdâ€™s eye view [37], raw points [89],
semanticclassesdetectedbyeachlase1rbea2m.D3espi4tebe5ing 6 7 8
sparsevoxel[33],andmulti-viewfusion[41]representations.
anempiricalchoice,wewillshowin1later2secti3ons4that5our 6 7 8
motorcyclist ThisversatilityallowsLaserMixtobeeffectivelyintegrated
laser-based partiptieonrisnognmethod significantly outperforms
across a broad spectrum of existing LiDAR segmentation
otherpartitionchofiecens,ciencludingrando1mp2oints3(Mi4xUp-5like 6 7 8
1 2 3 4 5 6 fra7mew8orks,withoutnecessitatinganymodificationstothe
partition[87]),randomareas(CutMix-likepartition[88]),and
1 2 3 4 5 6 un7derly8ingdataornetworkstructures.
otherheurisotictshleikre-agzrimouutnhdÎ±(sensorhorizontaldirection)
fence
orradiusr(sensorrangedirection)pa 1rtitio 2ns. 3 4 5 6 Rol7eino8urframework:Centraltooursemi-supervisedlearn-
1 2 3 4 5 6 7 8
LiDARSceneMixing.Inthepursuitofrefiningthecontrol ing framework, LaserMix streamlines the computation of
overspatialprior ts, rw ue ni kntroduceLaserMix,asophisticated marginal probability P(Y in|X in,A) â€“ a process otherwise
LiDARmixingstrategytailoredtooptimizethemanipulation computationallyintensiveintypicalscenariosâ€“asdescribed
ofspatialdatafrommultipleLiDARs1cans2.Las3erMi4xmi5xes 6 by7Step82inourframework.Thecostfordirectlycomputing
pole
1 2 3 4 5 6 7 8
bicycle
1 2 3 4 5 6 7 8
other-vehicle
1 2 3 4 5 6 7 8
bicyclist
1 2 3 4 5 6 7 8
motorcyclist
1 2 3 4 5 6 7 8
other-ground
1 2 3 4 5 6 7 86
Beam 1 Beam 2 Beam 3 Beam 4 theircorrespondingpseudo-labelsandgroundtruth.Next,
road
we use the Student network Gs to predict the mixed data
Î¸
andcomputethecross-entropylossL (w/ mixedlabels).
mix
OptimizatAiroean 1ObjeAcretaiv 2es.TAhreea p3oint-Awreias 4ecross-entropyloss
ğœ™ ğ’
" ğ’€ for a labeled or unlabeled LiDAR point cloud x and its
ğœ™ # ğœ™ ! correspondinggroundtruthorpseudo-labelsyontheLiDAR
ğ‘¿ segmentationnetworkG Î¸ iscalculatedasfollows:
|x|
LAcreea= 1 |x1 |A(cid:88) rea 2CrossAErena t3ropyA(yre( ai ) 4,G Î¸(i)(x)), (8)
ğœ™ $ i=1
where(i)denotesthei-thpointinthepointcloud.Moreover,
ourframeworkiscompatiblewiththemeanteacherconsis-
ğ‘ $% ğ‘ !$
Area 1 tency regularization [18], where the exponential moving
ğ‘ $& ğœŒ != (ğ‘ !")#+(ğ‘ !$)# Area 2 average (EMA) of the Student network G Î¸s is used to
ğ‘% Area 3
updatetheparametersofTeachernetworkG Î¸t,alongwitha
ğœ™
$
ğœ™ !=arctan( ğœŒ! !)
Area 4
t fe rom mpe twra otu nre etc wo oe rffi kc si ,e in .et .. ,T Lhe ,â„“ i2 sclo as lcs ub le at tw edee an sfth ole lop wre sd :ictions
mt
Fig. 2: Laser partition exa(ma)ple. We group LiDAR points L mt =||G Î¸s(x)âˆ’G Î¸t(x)||2 2, (9)
(px i,py i,pz i)whoseinclinationsÏ• i arewithinthesameincli- where || Â· ||2 denotes the â„“2 norm. The overall loss func-
2
nationrangeintothesamearea,asdepictedincolorregions. tion of this consistency regularization framework is L =
L +Î» L +Î» L ,whereÎ» andÎ» arelossweights.
sup mix mix mt mt mix mt
TheTeachernetworkGt isusedduringinferenceduetoits
themarginalprobabilityinEquation(4)onreal-worldLiDAR Î¸
empirically observed stability. Compared to conventional
data is prohibitive; we need to iterate through all areas in
fully supervised learning, there is no additional computa-
A and all outside data in X out, which requires |A|Â·|X out|
tionalburdenintheinferencephase.
predictions in total. To reduce the training overhead, we
Roleinourframework:Ourconsistencyregularizationpipeline
takeadvantageofthefactthatapredictioninanareawill
is designed to effectively minimize the marginal entropy
be largely affected by its neighboring areas and let X out
for data-efficient 3D scene understanding, as described in
fill only the neighbors instead of all the remaining areas.
Step 3 of our framework. Since the objective for minimiz-
LaserMixmixestwoscansbyintertwiningtheareassothat
ing the entropy has a challenging optimization landscape,
theneighborsofeachareaarefilledwithdatafromtheother
pseudo-labelingemergesasaresortinpractice[90].Unlike
scan. As a result, we obtain the prediction on all areas A
conventionalpseudo-labeloptimizationinsemi-supervised
oftwoscansfromonlytwopredictions,whichonaverage
learning which only aims to encourage the predictions to
reduces the cost from |A| to 1. The scan before and after
beconfident,minimizingthemarginalentropyrequiresall
mixing counts as two data fillings, therefore |X out| = 2.
predictionstobebothconfidentandconsistent.Tothisend,
Overall,thetrainingoverheadisreducedfrom|A|Â·|X out|to
weleveragebothgroundtruthandpseudo-labelsasanchors
2:onlyonepredictiononoriginaldataandoneadditional
toencouragethe3Dsceneunderstandingmodelâ€™spredictions
predictiononmixeddataarerequiredforeachLiDARscan.
tobeconfidentandconsistentwiththesesupervisionsignals.
Duringtraining,thememoryconsumptionforabatchwill
be 2Ã— compared to a standard semi-supervised learning
framework,andthetrainingspeedwillnotbeaffected. 4 LASERMIX++
ToleverageinteractionsbetweenthecameraandLiDAR,we
3.3 3DSceneConsistencyRegularization
firstelaborateonthecorrespondencebetweenLiDARand
Dual-BranchConsistency.Wedesignaconsistencyregular- cameras (Sec. 4.1). We then propose a multi-modal Laser-
ization framework based on our three-step procedures to Mix operation (Sec. 4.2), a cross-sensor feature distillation
enhance the data efficiency in 3D scene understanding, as (Sec.4.3),andlanguage-drivenknowledgeguidance(Sec.4.4).
depicted in Figure 3. The framework incorporates a dual- Finally, we describe the overall LaserMix++ pipeline for
branch architecture, consisting of a Student network Gs enhancingdata-efficient3Dsceneunderstanding(Sec.4.5).
Î¸
and a Teacher network Gt. Gs and Gt take certain LiDAR Prevailingdrivingperceptionsystemsoftenconsistofa
Î¸ Î¸ Î¸
representations (e.g., range images or voxel grids) as the spectrumofsensorstoensuresafeoperations[26],[27],[28].
inputandmakepredictions.Duringthetrainingphase,each Atypicalsensorsetupincorporatedbyexistingperception
batchcontainsanequalmixoflabeledandunlabeleddata. systems contains both LiDAR and surrounding cameras.
WecollectthepredictionsfrombothGs andGt,andgenerate Both2D(trainedw/ RGBimages)and3D(trainedw/ LiDAR
Î¸ Î¸
the pseudo-labels from the Teacher networkâ€™s predictions point clouds) perception models are leveraged which can
with a predefined confidence threshold T. For labeled complementeachother,especiallyinchallengingconditions
data, the cross-entropy loss L is calculated between the suchaslowlight,adverseweather,andmotionperturbations
sup
predictions of the Student network and the ground truth. [29], [30], [31], [32]. In this work, instead of solely relying
Concurrently, LaserMix is applied to mix each unlabeled on LiDAR point clouds to train semi-supervised learning
scan with a randomly selected labeled scan, together with models,weproposetoleveragetheabundantRGBimages7
LabeledScan ğ’™ ğ’ Student ğ“› Ground-Truth ğ’š ğ’
ğ¬ğ®ğ©
Loss
ğ“›
ğ¦ğ¢ğ±
LaserMix EMA Loss LaserMix
ğ’™ ğ’š
ğ¦ğ¢ğ± ğ¦ğ¢ğ±
Thresholdğ‘»
UnlabeledScan ğ’™ ğ’– Teacher Pseudo-Label ğ’š ğ’–
Fig. 3: Overview of our baseline consistency regularization framework. We feed the labeled scan x l into the Student
networktocomputethesupervisedlossL sup (w/ groundtruthy l).Theunlabeledscanx u andthegeneratedpseudo-labely u
aremixedwith(x l,y l)viaLaserMix(Sec.3.2)toproducemixeddatasample(x mix,y mix),whichisthenfedintotheStudent
networktocomputethemixinglossL .Additionally,weencouragetheconsistencybetweentheStudentnetworkandthe
mix
TeachernetworkbycomputingthemeanteacherlossL overtheirpredictions,wheretheTeachernetworkâ€™sparameters
mt
areupdatedbytheexponentialmovingaverageofthatoftheStudentnetwork.Duringinference,onlytheTeachernetwork
isneededwhichmaintainsthesamecomputationalcostastheconventionalLiDARsegmentationpipeline.
from cameras as additional resources for a more holistic information that could be supplementary to the 3D scene
multi-modal data-efficient 3D scene understanding. This understanding task. To leverage such visual guidance for
enhancedframework,dubbedLaserMix++,doesnotrequire data-efficient learning, we associate the image pixels with
any image annotations, thus maintaining the same data theLiDARpointsduringLaserMix.SimilartoEquation(7),
efficiencyasthebaselineframeworkdescribedinSec.3.3. suchamulti-modaldatamixingprocessbetweentheLiDAR
andcameracanbedefinedas:
4.1 2D-3DCorrespondences
ÏƒËœ 1,ÏƒËœ
2
=Multi-ModalLaserMix[{x p,x img} 1,{x p,x img} 2],
ÏƒËœ ={x ,x }a1 âˆª{x ,x }a2 âˆª{x ,x }a3 âˆªÂ·Â·Â· ,
Assumingthatthedrivingperceptionsystemconsistsofone 1 p img 1 p img 2 p img 1
LiDARandonecamerasensor1thathavebeenwellcalibrated ÏƒËœ
2
={x p,x img}a 21 âˆª{x p,x img}a 12 âˆª{x p,x img}a 23 âˆªÂ·Â·Â· .
andsynchronized,wecanobtain,atacertaindataacquisition (11)
frequency,apairofLiDARpointcloudx =(px,py,pz)and
cameraimagex
.Toestablish2D-3Dcp
orrespondencesfor
Theareaset{a 1,a 2,...,a m}canbeobtainedinthesameway
img asSec.3.2.Themulti-modalvariantofLaserMixleverages
a given pair {x ,x }, we first project point cloud x
p img lidar off-the-shelfvisualinformationfromsynchronizedcamera
onto the image plane (pu,pv) based on sensor calibration
images to assist the consistency regularization in Sec. 3.3.
parameters,i.e.,
Such an approach further enhances the effect of spatial
[pu,pv,1]T = p1 z Ã—Î“ K Ã—Î“ câ†lÃ—[px,py,pz]T, (10) â€œp pr aio inrs tei dn â€d wriv iti hng exs tc re an te es x, ts uin rece inth foe rL miD atA ioR np fro oi mnts thh eav imeb agee en s.
AfterobtainingmixedLiDAR-imagepairs,weproposethe
whereÎ“ K denotesthecameraintrinsicmatrixandÎ“ câ†l is
followingtwooperationsintheLaserMix++frameworkto
the transformation matrix from the LiDAR to the camera.
assist the data-efficient 3D scene understanding task with
It is worth mentioning that the correspondences between
suchmulti-modalinputs.
points and pixels do not always hold due to the possible
miss-overlapbetweenthefield-of-viewsoftheLiDARand
thecamera.WecreateacorrespondencemaskMofthesame 4.3 Camera-to-LiDARFeatureDistillation
lengthwithx toqueryvalidpoint-pixelrelationships.For
p Overthepastfewyears,imagesegmentationmodelshave
LiDAR points that do not have a corresponding pixel, we
become both more efficient and highly accurate on large-
padtheirpixelcorrespondenceswithzero.
scalebenchmarks[2],[91],[92],[93],[94].Mostrecentmodels
aretrainedacrossawidespectrumofimagecollectionsand,
4.2 Multi-ModalLaserMix as a return, show promising zero-shot generalizability to
unseen domains [95], [96], [97]. Such a versatile capability
Differentfromthepurepositionalinformationencodedinthe
opensupnewpossibilitiesfordrivingperceptionsfromRGB
LiDARpointcloud,theimagepixelsprovideextratexture
cameras[4].Observingthepromisingimagesegmentation
capability from these foundation models, we propose to
1.Thissimpleconfigurationcanbeeasilyextendedtosingle-LiDAR
multi-cameraandmulti-LiDARmulti-camerascenarios. leverageLiDAR-cameracorrespondencesfordata-efficient8
3Dsceneunderstanding,withoutusinggroundtruthimage Uniform Random Sequential
labels. Specifically, given {x ,x } pairs, we extract the
p img
point cloud and image features from the backbone of the 0
StudentnetworkandanimagesegmentationbackboneGË†img
Î¾ 1
withapretrainedparametersetÎ¾ asfollows:
2
F =Hs(GË†s(x )), F =GË†img(x ), (12)
p Î¶ Î¸Ë† p img Î¾ img
3
whereGË† Î¸Ë†withparametersÎ¸Ë†
isthebackboneoftheStudent
4
networkGt,andHs withparametersÎ¶ isaprojectionlayer
Î¸ Î¶
used to map the dimension of the point cloud feature to 5
that of the image feature. According to the known sensor
6
calibrationparametersinEquation(10),weobtainthepaired
point-imagefeatures{F ,FËœ },whereFËœ hasbeenaligned ... ... ...
p img img
to match the shape with F . To distill the knowledge
p Fig.4:Data splitting strategiesfordata-efficient3Dscene
from
GË† Î¾img
to
GË†
Î¸Ë†, we design the camera-to-LiDAR feature understanding.Thelabeled(color)andunlabeled(gray-scale)
distillation loss L c2l, which aims to minimize the cosine LiDARscanscanbesplitviauniform(left),random(middle),
distancebetweentwofeaturesasfollows: andsequential(right)samplingstrategies,respectively.
L =MâŠ™(1âˆ’âŸ¨F , FËœ âŸ©), (13)
c2l p img
where âŠ™ denotes the element-wise multiplication; âŸ¨,âŸ© cal- the target driving datasets. We first propose multi-modal
culatestheinnerproductoftwofeatures.Thiscross-sensor LaserMixtoenablericherinteractionsacrosssensors.Toalign
optimization objective transfers the semantically more co- pointcloudandimagefeatures,weadaptapretrainedimage
herent features from RGB images to LiDAR point clouds, segmentationmodeltotheLiDARsegmentationbackbone.
which assists the LiDAR segmentation model in learning Toobtainauxiliarysupervisionsignalsforunlabeledscans,
betterrepresentationunderdata-efficientscenarios. weproposetogeneratetext-alignednon-probabilisticoutputs
fromimagesandmatchthemwiththatoftheLiDARpoint
clouds.Aswewillshowinthefollowingsections,thesetwo
4.4 Language-DrivenKnowledgeGuidance
modulescontributetosignificantperformanceimprovements
Themostprevailingvision-languagemodelslikeCLIP[39] fordata-efficient3Dsceneunderstanding.
havebeenwidelyexploitedintheimagedomaintoenable
open-vocabulary predictions. Such a paradigm enables a
moreholisticperceptionbeyondthefixedlabelset,whichis 4.5 OverallFramework
especiallysuitablefordrivingscenes.Recentendeavorson
Incorporating everything together, we now present the
open-vocabularyimagesegmentationattempttotrainlarge-
overall LaserMix++ framework. Based on the 3D scene
scalemulti-taskmodelsthatalignwellwithtextinputs[96],
consistencyregularizationbaselineinSec.3.3,ourapproach
[97],[98].Toenrichthesupervisionsignalsfordata-efficient
seamlesslyintegratesoptimizationofobjectivesasfollows:
3Dsceneunderstanding,weproposetousethesepretrained
â€¢ Theconventionalsupervisionsignalsfromlabeleddata,
modelsalongwithopenvocabulariestogenerateauxiliary
asinEquation(8).
labels on unlabeled LiDAR point clouds. Specifically, given
â€¢ Themixing-basedcross-sensorconsistencyfrommulti-
theclassnamesastextprompts,wefirstusetheCLIPâ€™stext
encoder Gtxt to extract text embedding Fâ€² , with Ï± being modalLaserMix,asinEquation(11).
Ï± txt â€¢ The consistency regularization between Student and
thepretrainedparameters.Next,wefeedtheimagefeatures
Teachernetworks,asinEquation(9).
F extracted from Equation (12) to the pretrained open-
img
vocabularyimagesegmentationheadH Ï‚img andalsothepoint â€¢ T agh ee sc aa nm de Lra iD-t Ao- RLi pD oA inR tcf le oa utu dr se ,ad sis intil Ela qt uio an tiob net (w 13e )e .n im-
cloud to the LiDAR segmentation model Gs. The whole
Î¸ â€¢ Theauxiliarysupervisionsforunlabeleddata,obtained
processcanbedescribedasfollows:
fromtext-drivenknowledgeguidanceinEquation(15).
F pâ€² =G Î¸s(x p), F iâ€²
mg
=âŸ¨H Ï‚img(F img), F tâ€² xtâŸ©, (14) Theoverallobjectiveaimstominimizethefollowinglosses:
where Ï‚ denotes the parameters of H Ï‚img . F pâ€² and F iâ€²
mg
are L=L sup+Î» mixL mix+Î» mtL mt+Î» c2lL c2l+Î» lkgL lkg, (16)
the non-probabilistic outputs from the models. Similar to
where Î» and Î» denote the loss weights of the camera-
c2l lkg
Equation(13),wefirstfindthepairedpoint-imageoutputs
to-LiDARfeaturedistillationlossandthelanguage-driven
{Fâ€²,FËœâ€² }.Toachievelanguage-drivenknowledgeguidance
p img knowledgeguidanceloss,respectively.
fromimagestoLiDARpointclouds,weminimizethecosine
distancebetweentwooutputsasfollows:
L =MâŠ™(1âˆ’âŸ¨Fâ€² , FËœâ€² âŸ©). (15) 5 EXPERIMENTS
lkg p img
Roleinourframework:Ourenhancedframeworkisdesigned Inthissection,weconductthoroughcomparativeandabla-
to leverage both LiDAR and image data for data-efficient tionexperimentstovalidatetheeffectivenessandsuperiority
3Dsceneunderstanding,withoutneedingimagelabelsfrom oftheproposedLaserMix++framework.9
TABLE2:Benchmarkingresultsamongstate-of-the-artapproachesusingtheLiDARrangeview,birdâ€™seyeview(BEV),sparse
voxel,andmulti-viewfusionbackbones,respectively.Aunifiedbackbonesetupisusedacrossrepresentations,exceptforGPC
[22]andLiM3D[11]whichadoptextramodules.AllmIoUscoresaregiveninpercentage(%).Thesup.-only,best,andsecond
bestscoresundereachdatasplitwithineachrepresentationgroupareshadedwithgray,blue,andyellow,respectively.
nuScenes[38] SemanticKITTI[28] ScribbleKITTI[7]
Repr. Method Venue Backbone
1% 10% 20% 50% 1% 10% 20% 50% 1% 10% 20% 50%
Sup.-only - FIDNet 38.3 57.5 62.7 67.6 36.2 52.2 55.9 57.2 33.1 47.7 49.9 52.5
MeanTeacher[18] NeurIPSâ€™17 42.1 60.4 65.4 69.4 37.5 53.1 56.1 57.4 34.2 49.8 51.6 53.3
CBST[67] ECCVâ€™18 40.9 60.5 64.3 69.3 39.9 53.4 56.1 56.9 35.7 50.7 52.7 54.6
CutMix-Seg[61] BMVCâ€™20 FIDNet 43.8 63.9 64.8 69.8 37.4 54.3 56.6 57.6 36.7 50.7 52.9 54.3
CPS[21] CVPRâ€™21 40.7 60.8 64.9 68.0 36.5 52.3 56.3 57.4 33.7 50.0 52.8 54.6
LaserMix[10] CVPRâ€™23 49.5 68.2 70.6 73.0 47.4 60.1 61.0 62.6 45.7 55.5 56.8 58.7
LaserMix++ Ours 51.6 69.8 71.7 73.7 50.1 61.9 62.4 63.7 47.1 57.1 58.0 59.1
FIDNet
Improv.â†‘ - +2.1 +1.6 +1.1 +0.7 +2.2 +1.8 +1.4 +1.1 +1.4 +1.6 +1.2 +0.4
Sup.-only - PolarNet 50.9 67.5 69.5 71.0 45.1 54.6 55.6 56.5 42.6 52.8 53.4 54.4
MeanTeacher[18] NeurIPSâ€™17 51.9 68.1 69.7 71.1 47.4 55.6 56.6 57.1 43.7 53.4 54.4 54.9
CPS[21] CVPRâ€™21 PolarNet 52.1 67.7 69.8 71.2 46.9 54.9 56.0 56.9 44.0 53.5 54.4 55.1
LaserMix[10] CVPRâ€™23 54.0 69.5 70.8 71.9 51.0 57.7 58.6 60.0 45.7 55.5 56.0 56.6
LaserMix++ Ours 56.5 71.5 71.8 72.7 54.0 59.9 60.6 62.3 48.3 57.8 58.6 58.8
PolarNet
Improv.â†‘ - +2.5 +2.0 +1.0 +0.8 +3.0 +2.2 +2.0 +2.3 +2.6 +2.3 +2.6 +2.2
Sup.-only - Cylinder3D 50.9 65.9 66.6 71.2 45.4 56.1 57.8 58.7 39.2 48.0 52.1 53.8
MeanTeacher[18] NeurIPSâ€™17 51.6 66.0 67.1 71.7 45.4 57.1 59.2 60.0 41.0 50.1 52.8 53.9
CBST[67] ECCVâ€™18 53.0 66.5 69.6 71.6 48.8 58.3 59.4 59.7 41.5 50.6 53.3 54.5
CPS[21] CVPRâ€™21 52.9 66.3 70.0 72.5 46.7 58.7 59.6 60.5 41.4 51.8 53.9 54.8
GPC[22] ICCVâ€™21 Cylinder3D - - - - - 49.9 58.8 - - - - -
CRB[7] CVPRâ€™22 - - - - - 58.7 59.1 60.9 - 54.2 56.5 58.9
LaserMix[10] CVPRâ€™23 55.3 69.9 71.8 73.2 50.6 60.0 61.9 62.3 44.2 53.7 55.1 56.8
LiM3D[11] CVPRâ€™23 - - - - - 61.6 62.6 62.8 - 60.3 60.5 60.9
LaserMix++ Ours 58.5 71.1 72.8 74.0 56.2 62.3 62.9 63.4 47.3 56.7 57.6 59.8
Cylinder3D
Improv.â†‘ - +3.2 +1.2 +1.0 +0.8 +5.6 +0.7 +0.3 +0.6 +3.1 âˆ’3.6 âˆ’2.9 âˆ’1.1
Sup.-only - MinkUNet 58.3 71.0 73.0 75.1 53.9 64.0 64.6 65.4 48.6 57.7 58.5 60.0
MeanTeacher[18] NeurIPSâ€™17 60.1 71.7 73.4 75.2 56.1 64.7 65.4 66.0 49.7 59.4 60.0 61.7
CPS[21] CVPRâ€™21 MinkUNet 59.8 71.6 73.4 75.1 54.7 64.1 65.5 66.2 50.1 59.6 60.3 61.6
LaserMix[10] CVPRâ€™23 62.8 73.6 74.8 76.1 60.9 66.6 67.2 68.0 57.2 61.1 61.4 62.4
LaserMix++ Ours 64.7 74.6 75.6 76.6 63.1 67.9 68.2 68.7 61.0 64.2 64.8 65.1
MinkUNet
Improv.â†‘ - +1.9 +1.0 +1.2 +0.5 +2.2 +1.3 +1.0 +0.7 +3.8 +3.1 +3.4 +2.7
Sup.-only - SPVCNN 57.9 71.7 73.0 74.6 52.7 64.1 64.5 65.1 47.2 57.3 58.2 58.8
MeanTeacher[18] NeurIPSâ€™17 59.4 72.5 73.1 74.7 54.4 64.8 65.2 65.7 49.9 58.3 58.6 59.1
CPS[21] CVPRâ€™21 SPVCNN 58.7 72.0 73.2 74.7 54.6 64.6 65.3 65.9 48.7 58.0 58.4 59.0
LaserMix[10] CVPRâ€™23 63.2 74.1 74.6 75.8 60.3 66.6 67.0 67.6 57.1 60.8 60.7 61.0
LaserMix++ Ours 65.3 75.3 75.2 76.3 63.2 67.5 67.7 68.6 60.6 63.6 65.0 66.2
SPVCNN
Improv.â†‘ - +3.1 +1.2 +0.6 +0.5 +2.9 +0.9 +0.7 +1.0 +3.5 +2.8 +4.3 +5.2
5.1 Datasets image modalities. Our approaches can also be applied to
robust3Dperception.WeverifythisontheSemanticKITTI-
Weutilizethreedata-efficient3Dsceneunderstandingbench-
C and WOD-C benchmarks from Robo3D [31], which are
marks for our experiments: nuScenes [38], SemanticKITTI
constructed based on the SemanticKITTI [28] and Waymo
[28],andScribbleKITTI[7].nuScenes[38]andSemanticKITTI
Open[27]datasets,respectively.
[28]arethetwomostpopularmulti-modaldrivingpercep-
tiondatasets,with29,130and19,130trainingscansand6,019
5.2 ExperimentalSetups
and4,071validationscans,respectively.ScribbleKITTI[7],a
derivativeofSemanticKITTI[28],featuresthesamenumber 3DBackboneConfigurations.TovalidatethatLaserMix++
ofscansbutisannotatedwithlinescribbles,asopposedto isuniversallyapplicabletodifferentLiDARrepresentations,
fullannotations.weemployarangeoflabeledtrainingscans we conduct experiments using a total of five backbones,
â€“ 1%, 10%, 20%, and 50% â€“ treating the rest as unlabeled includingFIDNet[36](rangeview),PolarNet[37],MinkUNet
to align with standard semi-supervised settings from the [52] and Cylinder3D [33] (sparse voxel), and SPVCNN
image segmentation community. Additionally, we extend [41] (multi-view fusion). The input resolution of range
ourevaluationstotheCityscapesdataset[94],followingthe images is set to 32Ã—1920 for nuScenes [38] and 64Ã—2048
semi-supervisedsplitscommonlyusedinpreviousstudies forSemanticKITTI[28]andScribbleKITTI[7].Thegridcell
[19], [20], [21] â€“ 1/16, 1/8, 1/4, and 1/2 data splits â€“ to sizeofbirdâ€™seyeviewmethodsissetto[480,360,32].The
assessthegeneralityofourmethodsacrossbothLiDARand voxelsizeofvoxel-basedmethodsisfixedas[240,180,20]
weiVegnaR
VEB
lexoV
noisuF10
car bicy moto truc o.veh ped b.list m.list road park walk o.gro build fenc veg trun terr pole sign
Fig. 5: Qualitative assessments of state-of-the-art data-efficient 3D scene understanding models from the LiDAR birdâ€™s
eyeviewandrangeviewonthevalidationsetofSemanticKITTI[28].Tohighlightthedifferences,thecorrectandincorrect
predictionsarepaintedingrayandred.Bestviewedincolorsandzoomed-inforadditionaldetails.
forallthreedatasets.Thesamevoxelsizeisappliedtothe Î» ,andÎ» inEquation(16)aresetto2.0,250,1.5,and1.0,
c2l lkg
voxelbranchinthemulti-viewfusionbackbone. respectively.AllexperimentsareimplementedusingPyTorch
oneightNVIDIAA100GPUs.Allbaselinemodelsaretrained
Implementation Details. Our LaserMix++ framework is
withabatchsizeof2oneachGPU,alongwiththeAdamW
establishedbasedontheMMDetection3Dcodebase[106].We
optimizer[107],OneCyclelearningrateschedule[108],anda
denotethesupervised-onlybaselineassup.-onlyinourexper-
learningrateof0.008.Wedonotincludeanytypeoftest-time
iments.Forsemi-supervisedlearning,thelabeled/unlabeled
augmentationormodelensembleduringtheevaluation.
data are selected in four ways as shown in Figure 4:
Evaluation Protocol. We follow the Realistic Evaluation
1) random sampling, 2) uniform sampling, 3) sequential
Protocol[109]whenbuildingourbenchmarks.Undereach
sampling, and 4) ST-RFD [11] sampling strategies. Due to
setting,theconfigurationsareunifiedtoensureafaircompar-
the lack of previous works, we also compare consistency
isonamongdifferentsemi-supervisedlearningalgorithms.
regularization[18],[21],[61]andentropyminimization[67]
methods from semi-supervised image segmentation. The EvaluationMetrics.Inthiswork,wereporttheintersection-
number of spatial area sets m in multi-modal LaserMix is over-union(IoU)foreachsemanticclassandthemeanIoU
uniformlysampledfrom2to6.ThelossweightsÎ» ,Î» , (mIoU)scoresacrossallsemanticclassesinsemi-supervised
mix mt
hturT-dnuorG
ylnO-.puS
rehcaeTnaeM
sruO11
TABLE3:Robustnessenhancementeffectanalysisamongdifferentmixing-based3Dsceneaugmentationmethodsonthe
Robo3D[31]benchmarkforLiDARsemanticsegmentation(w/ aMinkUNet[52]backbone)and3Dobjectdetection(w/ a
CenterPoint[99]backbone)tasks,respectively.AllmIoU/mAP/mCE/mRRscoresaregiveninpercentage(%).Thebaseline,
best,andsecondbestscoresundereachevaluationmetricareshadedwithgray,blue,andyellow,respectively.
Set Method Venue Backbone mCEâ†“ mRRâ†‘ Fog Rain Snow Motio Beam Cross Echo Sensor
None - MinkUNet 100.0 81.9 55.9 54.0 53.3 32.9 56.3 58.3 54.4 46.1
Common - 110.6 83.4 38.9 57.1 52.0 41.2 49.4 55.0 51.7 41.3
Mix3D[100] 3DVâ€™21 96.7 88.0 57.3 54.4 56.3 42.9 55.8 59.0 52.9 48.1
MinkUNet
PolarMix[101] NeurIPSâ€™22 86.8 86.9 61.0 63.8 61.0 50.0 61.5 58.9 55.9 53.6
LaserMix[10] CVPRâ€™23 83.3 87.0 62.7 66.3 62.1 48.9 66.3 56.9 58.0 57.8
LaserMix++ Ours 80.9 88.1 63.4 66.4 63.2 52.8 66.5 59.5 58.7 58.3
MinkUNet
Improv.â†‘ - âˆ’2.4 +0.1 +0.7 +0.1 +1.1 +2.8 +0.2 +0.5 +0.7 +0.5
None - CenterPoint 100.0 83.3 43.1 62.8 58.6 43.5 54.4 60.3 57.0 44.0
Common - 110.6 83.4 38.9 57.1 52.0 41.2 49.4 55.0 51.7 41.3
GTSampling - 115.5 80.2 38.5 55.6 51.2 40.0 45.8 51.1 49.7 36.6
CenterPoint
PolarMix[101] NeurIPSâ€™22 101.0 83.7 42.7 62.7 58.7 43.2 53.5 59.2 56.4 43.8
LaserMix[10] CVPRâ€™23 100.5 82.9 43.6 63.2 59.2 43.4 53.8 58.9 56.6 43.5
LaserMix++ Ours 98.2 84.3 45.1 63.6 60.1 45.2 54.3 59.8 57.8 45.1
CenterPoint
Improv.â†‘ - âˆ’2.3 +0.6 +1.5 +0.4 +0.9 +1.8 +0.5 +0.6 +1.2 +1.3
TABLE4:Benchmarking resultsamongexisting3Drepre- 5.3 ComparativeStudy
sentationlearningmethodspre-trained,linear-probed(LP),
andfine-tunedonnuScenes[38].AllmIoUscoresaregiven Improvements over Baselines. In Tab. 2, we compare the
inpercentage(%).Thesup.-only,best,andsecondbestscores proposed LaserMix++ with the LaserMix [10] and sup.-
areshadedwithgray,blue,andyellow,respectively. only baselines on nuScenes [38], SemanticKITTI [28], and
ScribbleKITTI[7].Sinceourapproachescanbeuniversally
Method nuScenes[38] appliedtodifferentLiDARrepresentations,wealsosetup
LP 1% 5% 10% 20% Full
thebenchmarkusingvariousbackbonesfromtheliterature.
Sup.-only 81.0 30.3 47.8 56.2 65.5 74.7 We observe consistent improvements achieved across all
PointContrast[102] 21.9 32.5 - - - - different settings by integrating multi-modal learning for
DepthContrast[103] 22.1 31.7 - - - - data-efficient3Dsceneunderstanding.Onaverage,thereis
PPKT[104] 35.9 37.8 53.7 60.3 67.1 74.5 a2%to3%performancegainbroughtbyLaserMix++over
SLidR[76] 38.8 38.3 52.5 59.8 66.9 74.8
thepreviousframework.Theimprovementsareespecially
ST-SLidR[105] 40.5 40.8 54.7 60.8 67.7 75.1
Seal[77] 45.0 45.8 55.6 63.0 68.4 75.6 predominantwhenthenumberoflabeleddataisextremely
limited (e.g., 1%), which verifies the effectiveness of our
w/LaserMix[10] - 48.4 57.8 65.5 70.8 77.1
Improv.â†‘ - +2.6 +2.2 +2.5 +2.4 +1.5 approaches. As for the sup.-only baselines, the gains vary
under different backbones. LaserMix++ yields up to 13%
w/LaserMix++(Ours) - 49.9 58.5 66.7 71.6 77.9
Improv.â†‘ - +1.5 +0.7 +1.2 +0.8 +0.8 mIoUimprovementsacrossthreedatasetswhenusingthe
rangeviewbackbone[36].Similartrendsholdforthebirdâ€™s
eyeview[37],sparsevoxel[33],[52],andmulti-viewfusion
TABLE 5: Benchmark results of different semi-supervised [41] backbones, where the mIoU gains under the 1% split
learning approaches on the Cityscapes [94] dataset. (a) are around 6%, 8%, and 10%, respectively. The results
MethodsusingMeanTeacher[18]asthebackbone.(b)Meth- strongly verify the effectiveness of our framework and
ods using CPS [21] as the backbone with a CutMix [88] further highlight the importance of leveraging unlabeled
augmentation.AllmIoUscoresaregiveninpercentage(%). dataforLiDARsemanticsegmentation.
Compare with State of the Arts.WecompareLaserMix++
# Method 1/16 1/8 1/4 1/2
with recent arts from the literature, i.e., GPC [22], CRB [7],
MeanTeacher[18] 66.1 71.2 74.4 76.3 and Lim3D [11], and show results (using the Cylinder3D
w/Ours 68.7+2.6 72.3+1.1 75.7+1.3 76.8+0.5 backbone) in Tab. 2. As can be seen, LaserMix++ exhibits
a CCT[19] 66.4 72.5 75.7 76.8 much better results than GPC [22] and [7] across various
GCT[20] 65.8 71.3 75.3 77.1 scenarios.Themostrecentwork,Lim3D[11],achievesthe
CPS[21] 69.8 74.4 76.9 78.6 bestperformanceonScribbleKITTI[7].However,itsresults
CPS-CutMix[21] 74.5 76.6 77.8 78.8 wereobtainedbyvotingfrommultipleaugmentedtest-time
b ensembles, which created a different evaluation protocol
w/Ours 75.5+1.0 77.1+0.5 78.3+0.5 79.1+0.3
from ours. In addition to the above, we also reproduced
several popular algorithms [18], [21], [67] from the semi-
supervisedimagesegmentationdomain.TheresultsinTab.2
learning benchmarks. For out-of-distribution robustness verify that these methods, albeit competitive in 2D tasks,
assessment, we follow Robo3D [31] protocol to report the only yield sub-par performance in the semi-supervised
meancorruptionerror(mCE)andmeanresiliencerate(mRR). LiDARsemanticsegmentationbenchmark.Thishighlights
C-ITTIKcitnameS
C-DOW12
TABLE6:AblationstudyfordifferentcomponentsintheLaserMix++framework(w/ aFIDNet[36]backbone)ontheval
setsofSemanticKITTI[28]andnuScenes[38].(a)Thesup.-onlyresults.(b)Thebaseline[18]results;(c)TheLaserMixresults
w/ Studentnetsupervision(SS);(d)TheLaserMixresultsw/ Teachernetsupervision(TS).(e)TheLaserMix++resultsw/ the
camera-to-LiDARfeaturedistillation(L ).(f)TheLaserMix++resultsw/ thelanguage-drivenknowledgeguidance(L ).
c2l lkg
(g)ThecompleteLaserMix++configurationresults.AllmIoUscoresaregiveninpercentage(%).
SemanticKITTI[28] nuScenes[38]
# L mt L mix SS TS L c2l L lkg 1% 10% 20% 50% 1% 10% 20% 50%
a âœ— âœ— âœ— âœ— âœ— âœ— 36.2âˆ’1.3 52.2âˆ’0.9 55.9âˆ’0.2 57.2âˆ’0.2 38.3âˆ’3.8 57.5âˆ’2.9 62.7âˆ’2.7 67.6âˆ’1.8
b âœ“ âœ— âœ— âœ— âœ— âœ— 37.5+0.0 53.1+0.0 56.1+0.0 57.4+0.0 42.1+0.0 60.4+0.0 65.4+0.0 69.4+0.0
âœ— âœ“ âœ“ âœ— âœ— âœ— 43.2+5.7 57.1+4.0 58.3+2.2 59.8+2.4 45.6+3.5 64.3+3.9 67.8+2.4 71.6+2.2
c âœ“ âœ“ âœ“ âœ— âœ— âœ— 45.3+7.8 58.3+5.2 58.8+2.7 60.2+2.8 47.0+4.9 65.5+5.1 69.5+4.1 72.0+2.6
âœ— âœ“ âœ— âœ“ âœ— âœ— 46.5+9.0 59.3+6.2 60.4+4.3 61.9+4.5 46.0+3.9 64.1+3.7 69.5+4.1 72.3+2.9
d âœ“ âœ“ âœ— âœ“ âœ— âœ— 47.4+9.9 60.1+7.0 61.0+4.9 62.6+5.2 49.5+7.4 68.2+7.8 70.6+5.2 73.0+3.6
âœ“ âœ“ âœ“ âœ— âœ“ âœ— 48.3+0.9 60.7+0.6 61.4+0.4 63.1+0.5 50.1+0.6 68.8+0.6 71.0+0.4 73.1+0.1
e âœ“ âœ“ âœ— âœ“ âœ“ âœ— 48.6+1.2 60.9+0.8 61.6+0.6 63.2+0.6 50.2+0.7 69.0+0.8 71.1+0.5 73.2+0.2
âœ“ âœ“ âœ“ âœ— âœ— âœ“ 49.6+2.2 61.2+1.1 61.9+0.9 63.4+0.8 50.8+1.3 69.4+1.2 71.3+0.7 73.3+0.3
f âœ“ âœ“ âœ— âœ“ âœ— âœ“ 49.7+2.3 61.4+1.3 62.2+1.2 63.5+0.9 51.0+1.5 69.5+1.3 71.5+0.9 73.5+0.5
g âœ“ âœ“ âœ— âœ“ âœ“ âœ“ 50.1+2.7 61.9+1.8 62.4+1.4 63.7+1.1 51.6+2.1 69.8+1.6 71.7+1.1 73.7+0.7
the importance of exploiting the LiDAR data structure for manipulationsofLiDARpointcloudsinLaserMixandLaser-
data-efficient3Dsceneunderstanding. Mix++havethepotentialtoenrichthetrainingdistribution
Compare with Fully Supervised Methods. As shown in and, as a return, achieve better robustness. In this work,
Figure1c,thecomparisonsofLaserMix++overtheprevailing weconductexperimentsontheRobo3Dbenchmark[31]to
LiDAR semantic segmentation methods [33], [34], [35], validatetherobustnessenhancementeffectofourframework.
[37] validate that our approaches are competitive to the AsshowninTab.3,ourapproachesconsistentlyyieldlower
fully supervised counterparts while only requiring 2Ã— to mCEscoresforbothLiDARsemanticsegmentationand3D
5Ã— fewer annotations. Additionally, the results in Tab. 2 objectdetectioncomparedtoothermixing-basedtechniques,
verify the strong augmentation and regularization ability suchasMix3D[100]andPolarMix[101].
of LaserMix++ again. Our approaches have yielded better Generalize to Image Segmentation. The proposed scene
resultsinthehigh-dataregimeandextremelow-dataregime prior-based consistency regularization can be extended to
(i.e.,0.8%humanannotationsonScribbleKITTI[7]). image domains since they also reflect real-world driving
scenarios. To validate this, we conduct experiments on
Qualitative Assessments.Figure5displaysvisualizations
Cityscapes[94]andshowtheresultsinTab.5.Ascanbeseen,
of the 3D scene segmentation results for different semi-
our approaches achieved improved performance over the
supervised learning algorithms on the validation set of
nuScenes [38], where each example covers a 50Ã—50 m2 strongimagesegmentationbaselines[18],[19],[20],[21].Such
ageneralizabilityensuresourapproachestobeuniversally
drivingscenecenteredbytheego-vehicle.Weobservethat
applicabletodifferentsensormodalities.
previousmethodscanonlyimprovepredictionsinlimited
regions, while our approaches holistically eliminate false
predictions in almost every region around the ego-vehicle. 5.4 AblationStudy
The consistency enlightened by our methods has yielded
Inthissection,weconductseveralablationstudiestoverify
better3Dsegmentationaccuracyunderannotationscarcity.
the effectiveness of each component. Without otherwise
EnhancingRepresentationLearningEffects.Recentexplo- mentioned,westickwiththe10%labelbudgetsettingand
rations on self-supervised LiDAR semantic segmentation therangeviewbackboneinourablationexperiments.
exhibit promising representation learning effects during ComponentAnalyses.TheablationresultsinTab.6validate
model pretraining [76], [77], [102], [103], [104], [105]. Such that each of the proposed components contributes signifi-
methods establish suitable self-supervised pretraining ob- cantlytotheoverallimprovementindata-efficient3Dscene
jectivesandprobethequalitativeofrepresentationlearning understanding.Themixing-basedconsistencyregularization
viafew-shotfine-tuning.TheresultsshowninTab.4verify establishes a stable yet effective baseline across different
thatourapproachesareeffectiveinenhancingtheeffectsof datasets (Sec. 3.2). Meanwhile, using the Teacher network
representationlearningduringfine-tuning.combinedwith instead of the Student network to generate pseudo-labels
Seal [77], LaserMix++ also achieves superior performance tendstoyieldbetterresults,astheformaltemporallyensem-
under full supervision (from 74.7% mIoU to 77.9% mIoU). blesandencouragesspatialconsistency(Sec.3.3).Moreover,
Thisstudyfurtherprovestheversatilityofourframeworkin integrating multi-modal interactions between LiDAR and
handlingdifferentLiDARsemanticsegmentationtasks. camerasconsistentlyimprovesperformance,asencouraged
EnhancingOut-of-DistributionRobustness.Theabilityto bythecamera-to-LiDARfeaturedistillation(Sec.4.3)andthe
tackle scenarios that are not observed during the training language-drive knowledge guidance (Sec. 4.4). It is worth
timeiscrucialfora3Dsceneunderstandingmodel,especially notingthatallmodelconfigurationshaveachievedsuperior
underautonomousdrivingcontext[6],[32].Thefine-grained results than the baseline MeanTeacher [18], which further13
707070mmIomIUoI Uo(%U (% )(%)) 707700 mmImoIUoIoU (U% ( (%)%))
ww//LwaLsa/esLreaMrsMiexri+xM++i+x++ 696.98.689.8 616.196.91.9 575.715.71.1
ww//LwaLsa/esLreaMrsMiexrixMix 686.82.628.2 606.016.10.1 555.555.55.5
nunSnucuSecSnecenesneses nunnSuucSSeccneeennseess
656565 656655
ww//MwMix/i3xMD3iDx[13[0D130][31]00] 656.57.675.7 565.625.26.2 525.235.32.3
ww//CwuCtu/MtCMiuxitx[M8[7i8x]7[]88] 646.49.694.9 565.645.46.4 525.215.12.1 606060 606600 SeSSmeemamnaatnnitctiKiccKIKTIITTTTITII
SeSmeSmeamnatnaitnciKtciKIcTKITTITITTII
ww//CwuCtu/OtCOuutut[Ot1[u11]t1[]110] 636.32.623.2 545.425.24.2 505.025.20.2
555555 555555 ww//MwMix/iUxMUpipx[8U[68p]6[]87] 616.16.661.6 525.295.92.9 484.834.38.3
ScSrcSirbcirbbilbbelbKelKIeTKITTITITTII ScSSrcicrbribibblbeblKleeKIKTIITTTTITII
BaBsaesBleianlsiene el[i4 n[24e]2 []18] 606.04.640.4 535.315.13.1 494.984.89.8
EMEEMAMAA TThTrhherrseehsshh
505050 505500
606066036366366666696969 525252555555585858616161 484848515151545454575757 000 00.50.5.500.90.9.900.90.99.99090.90.99.999999 000 00.05.5.5 00.09..9900.09..9999900.09..99999999
(a)MixingStrategy (b)EMA (c)Threshold
Fig.6:Ablationstudyon(a)Differentmixing-basedtechniquesusedinpointpartition&mixing.(b)DifferentEMAdecay
ratesbetweentheTeacherandStudentnetworks.(c)DifferentconfidencethresholdsT usedinthepseudo-labelgeneration.
TABLE7:Ablationstudyonlaserbeampartitionstrategies
area structure and precise spatial positioning, provides
inLaserMix.Horizontalaxis:inclinationdirectionÏ•;Vertical
a more substantial boost in performance, outperforming
axis:azimuthdirectionÎ±.A(i-Î±,j-Ï•)strategydenotesthat
CutMixbyupto3.3%mIoU.CutOutcanbeconsideredas
thereareiazimuthandj inclinationpartitionsintotal. setting X out to a dummy filling instead of sampling from
datasets, and it leads to a considerable performance drop
Baseline (1Î±,2Ï•) (1Î±,3Ï•) (1Î±,4Ï•) (1Î±,5Ï•) (1Î±,6Ï•)
fromCutMix.
Alternative Heuristic Data Mixing.Exploringfurther,we
evaluatedifferentheuristicapproachestoLiDARscanparti-
tioning,includingazimuthandradius-basedsplits.Asshown
60.4 63.5 65.2 66.5 66.2 65.4
(+3.1) (+4.8) (+6.1) (+5.8) (+5.0) inTab.7,azimuthsplitsdonotcorrelatewellwithsemantic
(2Î±,1Ï•) (2Î±,2Ï•) (2Î±,3Ï•) (2Î±,4Ï•) (2Î±,5Ï•) (2Î±,6Ï•) distributions, offering no performance gains. In contrast,
finergranularityinmixinggenerallyimprovesresultsuntil
it reaches a threshold beyond which it disrupts semantic
coherence.Thisobservationsupportsouruseofinclination-
61.5 63.3 65.9 66.1 66.7 65.3 basedpartitioninginLaserMix,whichmaintainssemantic
(+1.1) (+2.9) (+5.5) (+5.7) (+6.3) (+4.9)
integritybetterthanpurelyradialorazimuthalapproaches.
(3Î±,1Ï•) (3Î±,2Ï•) (3Î±,3Ï•) (3Î±,4Ï•) (3Î±,5Ï•) (3Î±,6Ï•)
Mix Unlabeled Data Only. To underscore that LaserMix
extendsbeyondsimpledataaugmentation,weexperiment
withmixingsolelybetweenunlabeledscans.Thismodifica-
tionresultsinaslightperformancedrop(from68.2%to66.9%
60.9 64.2 65.9 66.3 66.0 65.2
(+0.6) (+3.8) (+5.5) (+5.9) (+5.6) (+4.8) mIoU),yetstillsignificantlyoutperformstraditionalmethods,
(4Î±,1Ï•) (4Î±,2Ï•) (4Î±,3Ï•) (4Î±,4Ï•) (4Î±,5Ï•) (4Î±,6Ï•) highlightingthestrongconsistencyregularizationimparted
byLaserMixevenwithoutdirectlabeleddatainteraction.
Data Splitting Strategies. For a semi-supervised learning
problem, the way of partitioning labeled and unlabeled
60.9 64.7 65.3 65.6 65.7 65.2 data plays a crucial. We compare four different splitting
(+0.6) (+4.3) (+4.9) (+5.2) (+5.3) (+4.8)
strategies (the first three are shown in Figure 4) that meet
therequirementsofdrivingdatacollectionandperception,
emphasizestheeffectivenessofourframeworkintackling i.e., random, uniform, sequential sampling strategies, and
LiDARsegmentationunderdata-efficientlearningsetups. therecentvisual-encouragedST-RFD[11]strategy.Asshown
MixingStrategies.Anotherablationexperiment,depictedin inTab.8,methodsundertherandomoruniformsampling
Figure6a,comparetheperformanceofLaserMixandLaser- achieved higher results than sequential sampling. This is
Mix++ against traditional mixing techniques, i.e., MixUp because the former two introduce a more diverse sample
[87], CutOut [110], CutMix [88], and Mix3D [100]. While distributionthansamplingsequentially.ST-RFDwhichpicks
MixUp and CutMix manipulate random points and areas scans using additional image cues provides a even richer
respectively,theydonotinherentlyleveragethestructured labeled data set. Under all four strategies, LaserMix++
natureofLiDARdata,wherespatialrelationssignificantly consistentlyachievedbetterperformancethanthebaseline
influence segmentation accuracy. CutMix shows improve- LaserMix [10] and MeanTeacher [18] frameworks and is
mentbyutilizingthestructuralpriorsinscenesegmentation; competitivewithLim3D[11]onScribbleKITTI[7].
however,LaserMixandLaserMix++,whichconsidersboth Dual-BranchConsistency.AsshowninFigure6b,ourdual-
nuScenes nuScenes nuScenes
SemanticKITTI SemanticKITTI SemanticKITTI ScribbleKITTI ScribbleKITTI ScribbleKITTI14
TABLE8:Ablationstudyonstate-of-the-art3DSSLmethods
Our empirical evaluations demonstrate that LaserMix++
(w/ the same Cylinder3D [33] backbone except for LiM3D
significantlyoutperformsexistingmethods,achievinghigh
[11])underdifferentdatasplittingstrategies,i.e.,(1)random,
accuracywithfarfewerlabels.Thisefficiencyunderscores
(2)uniform,(3)sequential,and(4)spatio-temporalredundant
its potential for reducing the dependency on extensive
framedownsampling(ST-RFD)strategies.AllmIoUscores
annotated data. Looking forward, we aim to refine spatial
aregiveninpercentage(%).Thesup.-only,best,andsecond
partitioningandincorporateadvancedsemi-supervisedtech-
bestscoresundereachdatasplitwithineachdatasplitting
niques to expand our frameworkâ€™s applications to other
groupareshadedwithgray,blue,andyellow,respectively.
criticaltaskssuchas3Dobjectdetectionandtracking.
Acknowledgments. This work is under the programme
SemanticKITTI[28] ScribbleKITTI[7]
Split Method 1% 10% 20% 1% 10% 20% DesCartesandissupportedbytheNationalResearchFoun-
dation, Prime Ministerâ€™s Office, Singapore, under its Cam-
Sup.-only 45.4 56.1 57.8 39.2 48.0 52.1
pus for Research Excellence and Technological Enterprise
MT[18] 45.4 57.1 59.2 41.0 50.1 52.8 (CREATE)programme.Thisworkisalsosupportedbythe
CPS[21] 46.7 58.7 59.6 41.4 51.8 53.9
MinistryofEducation,Singapore,underitsMOEAcRFTier
LaserMix[10] 50.6 60.0 61.9 44.2 53.7 55.1
LiM3D[11] - 61.6 62.6 - 60.3 60.5 2(MOET2EP20221-0012),NTUNAP,andundertheRIE2020
IndustryAlignmentFundâ€“IndustryCollaborationProjects
LaserMix++ 56.2 62.3 62.9 47.3 56.7 57.6
(IAF-ICP) Funding Initiative, as well as cash and in-kind
Sup.-only 44.7 56.2 57.7 39.6 49.9 52.4 contributionfromtheindustrypartner(s).
MT[18] 45.7 58.3 60.1 40.0 51.8 54.3
CPS[21] 45.6 58.5 59.8 41.7 50.1 53.1
LaserMix[10] 50.8 61.4 62.6 44.0 54.0 55.9 REFERENCES
LiM3D[11] - 61.3 62.4 - 60.6 60.3
[1] R.Roriz,J.Cabral,andT.Gomes,â€œAutomotivelidartechnology:
LaserMix++ 56.0 62.6 63.0 47.3 57.3 58.0 Asurvey,â€IEEETrans.Intell.Transport.Syst.,vol.23,no.7,pp.
6282â€“6297,2021. 1
Sup.-only 17.6 41.8 50.3 16.4 38.4 47.9
[2] A.Geiger,P.Lenz,andR.Urtasun,â€œArewereadyforautonomous
MT[18] 17.0 42.4 50.9 15.9 38.6 46.7 driving? the kitti vision benchmark suite,â€ in IEEE/CVF Conf.
CPS[21] 17.6 43.3 50.9 16.5 38.7 48.1 Comput.Vis.PatternRecog.,2012,pp.3354â€“3361. 1,7
LaserMix[10] 18.1 47.7 56.3 16.9 42.4 48.2 [3] L.Nunes,R.Marcuzzi,X.Chen,J.Behley,andC.Stachniss,â€œSeg-
LiM3D[11] - - - - - - contrast:3dpointcloudfeaturerepresentationlearningthrough
self-supervisedsegmentdiscrimination,â€IEEERobot.Autom.Lett.,
LaserMix++ 19.1 49.5 57.3 18.6 44.7 48.6
vol.7,pp.2116â€“2123,2022. 1,3
Sup.-only 47.2 56.9 58.1 40.1 54.1 55.5 [4] X. Yan, H. Zhang, Y. Cai, J. Guo, W. Qiu, B. Gao, K. Zhou,
Y.Zhao,H.Jin,J.Gao,Z.Li,L.Jiang,W.Zhang,H.Zhang,D.Dai,
MT[18] 49.1 59.8 60.2 42.3 55.0 56.8 andB.Liu,â€œForgingvisionfoundationmodelsforautonomous
CPS[21] 48.7 60.5 59.0 42.4 55.2 56.6 driving: Challenges, methodologies, and opportunities,â€ arXiv
LaserMix[10] 53.3 62.6 62.9 44.7 54.1 57.1 preprintarXiv:2401.08045,2024. 1,3,7
LiM3D[11] - 62.2 63.1 - 61.0 61.2 [5] S.D.Pendleton,H.Andersen,X.Du,X.Shen,M.Meghjani,Y.H.
Eng,D.Rus,andM.H.Ang,â€œPerception,planning,control,and
LaserMix++ 57.5 63.1 63.2 48.0 57.6 58.5
coordinationforautonomousvehicles,â€Machines,vol.5,no.1,
p.6,2017. 1
[6] L.Kong,S.Xie,H.Hu,L.X.Ng,B.R.Cottereau,andW.T.Ooi,
branchconsistencysetup,leveragingdifferentexponential â€œRobodepth:Robustout-of-distributiondepthestimationunder
movingaverage(EMA)decayrates,showsthatabalancebe- corruptions,â€inAdv.NeuralInf.Process.Syst.,vol.36,2023. 1,12
[7] O. Unal, D. Dai, and L. V. Gool, â€œScribble-supervised lidar
tween0.9and0.99optimizesperformance,whilehigherrates
semanticsegmentation,â€inIEEE/CVFConf.Comput.Vis.Pattern
disruptnetworkconsistency.Thisconfigurationcorroborates Recog.,2022,pp.2697â€“2707. 1,2,3,9,11,12,13,14
thesynergisticpotentialoftheteacher-studentarchitecture [8] Q.Hu,B.Yang,G.Fang,Y.Guo,A.Leonardis,N.Trigoni,and
withinourapproaches,facilitatingtheintegrationofmodern A.Markham,â€œSqn:Weakly-supervisedsemanticsegmentationof
large-scale3dpointcloudswith1000xfewerlabels,â€inEur.Conf.
semi-supervisedlearningtechniques.
Comput.Vis.,2022,pp.600â€“619. 1,3
ConfidenceThreshold.Theroleofpseudo-labelsiscritical [9] L. Kong, N. Quader, and V. E. Liong, â€œConda: Unsupervised
in our framework. As shown in Figure 6c, adjusting the domainadaptationforlidarsegmentationviaregularizeddomain
concatenation,â€inIEEEInt.Conf.Robot.Autom.,2023,pp.9338â€“
confidencethresholdforpseudo-labelgenerationshowsthat
9345. 1
overly low thresholds degrade performance by enforcing [10] L.Kong,J.Ren,L.Pan,andZ.Liu,â€œLasermixforsemi-supervised
consistencyonunreliablelabels.Conversely,highthresholds lidar semantic segmentation,â€ in IEEE/CVF Conf. Comput. Vis.
maydiminishthemixingbenefits.Optimalthresholdtuning, PatternRecog.,2023,pp.21705â€“21715. 1,2,3,9,11,13,14
[11] L.Li,H.P.H.Shum,andT.P.Breckon,â€œLessismore:Reducing
around0.9,iscrucialandvariesacrossdatasets,underpin-
taskandmodelcomplexityfor3dpointcloudsemanticsegmen-
ningtheadaptabilityofourapproaches. tation,â€inIEEE/CVFConf.Comput.Vis.PatternRecog.,2023,pp.
9361â€“9371. 1,2,3,9,10,11,13,14
[12] M.Liu,Y.Zhou,C.R.Qi,B.Gong,H.Su,andD.Anguelov,â€œLess:
6 CONCLUSION
Label-efficientsemanticsegmentationforlidarpointclouds,â€in
In this study, we have extended semi-supervised LiDAR IEEE/CVFConf.Comput.Vis.PatternRecog.,2022,pp.70â€“89. 1,3
[13] B.Gao,Y.Pan,C.Li,S.Geng,andH.Zhao,â€œArewehungryfor
semanticsegmentationbyintroducingLaserMix++,aframe-
3dlidardataforsemanticsegmentation?asurveyofdatasetsand
work that integrates LiDAR and camera data to exploit methods,â€ IEEE Trans. Intell. Transport.Syst., vol. 23,no. 7, pp.
spatialandtexturalsynergies.BuildingonourinitialLaser- 6063â€“6081,2021. 1,3
Mixtechnique,whichintertwineslaserbeamsfromdifferent [14] A.H.Gebrehiwot,P.Vacek,D.Hurych,K.Zimmermann,P.PeÂ´rez,
andT.Svoboda,â€œTeachersinconcordanceforpseudo-labelingof
scans,LaserMix++enhancesfeaturerichnessandmodelro-
3dsequentialdata,â€IEEERobot.Autom.Lett.,vol.8,pp.536â€“543,
bustness,particularlyundervariedenvironmentalconditions. 2022. 1
modnaR
mrofinU
laitneuqeS
]11[DFR-TS15
[15] D.Berthelot,N.Carlini,I.Goodfellow,N.Papernot,A.Oliver,and [35] T. Cortinhal, G. Tzelepis, and E. E. Aksoy, â€œSalsanext: Fast,
C.A.Raffel,â€œMixmatch:Aholisticapproachtosemi-supervised uncertainty-awaresemanticsegmentationoflidarpointclouds
learning,â€inAdv.NeuralInf.Process.Syst.,vol.32,2019. 1,3 forautonomousdriving,â€arXivpreprintarXiv:2003.03653,2020. 2,
[16] K.Sohn,D.Berthelot,N.Carlini,Z.Zhang,H.Zhang,C.A.Raffel, 3,12
E.D.Cubuk,A.Kurakin,andC.-L.Li,â€œFixmatch:Simplifying [36] Y.Zhao,L.Bai,andX.Huang,â€œFidnet:Lidarpointcloudsemantic
semi-supervisedlearningwithconsistencyandconfidence,â€in segmentationwithfullyinterpolationdecoding,â€inIEEE/RSJInt.
Adv.NeuralInf.Process.Syst.,vol.33,2020. 1,3 Conf.Intell.RobotsSyst.,2021,pp.4453â€“4458. 2,3,9,11,12
[17] D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, [37] Y.Zhang,Z.Zhou,P.David,X.Yue,Z.Xi,B.Gong,andH.Foroosh,
H.Zhang,andC.Raffel,â€œRemixmatch:Semi-supervisedlearning â€œPolarnet:Animprovedgridrepresentationforonlinelidarpoint
withdistributionalignmentandaugmentationanchoring,â€arXiv cloudssemanticsegmentation,â€inIEEE/CVFConf.Comput.Vis.
preprintarXiv:1911.09785,2019. 1,3 PatternRecog.,2020,pp.9601â€“9610. 2,3,5,9,11,12
[18] A.TarvainenandH.Valpola,â€œMeanteachersarebetterrolemod- [38] W. K. Fong, R. Mohan, J. V. Hurtado, L. Zhou, H. Caesar,
els:Weight-averagedconsistencytargetsimprovesemi-supervised O. Beijbom, and A. Valada, â€œPanoptic nuscenes: A large-scale
deeplearningresults,â€inAdv.NeuralInf.Process.Syst.,vol.30, benchmarkforlidarpanopticsegmentationandtracking,â€IEEE
2017. 1,3,6,9,10,11,12,13,14 Robot.Autom.Lett.,vol.7,pp.3795â€“3802,2022. 2,3,9,11,12
[19] Y.Ouali,C.Hudelot,andM.Tami,â€œSemi-supervisedsemantic [39] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,
segmentationwithcross-consistencytraining,â€inIEEE/CVFConf. G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and
Comput.Vis.PatternRecog.,2020,pp.12674â€“12684. 1,9,11,12 I.Sutskever,â€œLearningtransferablevisualmodelsfromnatural
[20] Z.Ke,D.Qiu,K.Li,Q.Yan,andR.W.Lau,â€œGuidedcollaborative languagesupervision,â€inInt.Conf.Mach.Learn.,2021,pp.8748â€“
trainingforpixel-wisesemi-supervisedlearning,â€inEur.Conf. 8763. 2,3,8
Comput.Vis.,2020,pp.429â€“445. 1,3,9,11,12 [40] A.Milioto,I.Vizzo,J.Behley,andC.Stachniss,â€œRangenet++:Fast
[21] X.Chen,Y.Yuan,G.Zeng,andJ.Wang,â€œSemi-supervisedseman- andaccuratelidarsemanticsegmentation,â€inIEEE/RSJInt.Conf.
ticsegmentationwithcrosspseudosupervision,â€inIEEE/CVF Intell.RobotsSyst.,2019,pp.4213â€“4220. 2,3,5
Conf.Comput.Vis.PatternRecog.,2021,pp.2613â€“2622. 1,3,9,10, [41] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, and S. Han,
11,12,14 â€œSearching efficient 3d architectures with sparse point-voxel
[22] L. Jiang, S. Shi, Z. Tian, X. Lai, S. Liu, C.-W. Fu, and J. Jia, convolution,â€ in Eur. Conf. Comput. Vis., 2020, pp. 685â€“702. 2,
â€œGuided point contrastive learning for semi-supervised point 3,5,9,11
cloud semantic segmentation,â€ in IEEE/CVF Int. Conf. Comput. [42] K. Muhammad, T. Hussain, H. Ullah, J. D. Ser, M. Rezaei,
Vis.,2021,pp.6423â€“6432. 1,3,9,11 N.Kumar,M.Hijji,P.Bellavista,andV.H.C.deAlbuquerque,
[23] J.Park,C.Xu,Y.Zhou,M.Tomizuka,andW.Zhan,â€œDetmatch: â€œVision-based semantic segmentation in scene understanding
Two teachers are better than one for joint 2d and 3d semi- forautonomousdriving:Recentachievements,challenges,and
supervised object detection,â€ in Eur. Conf. Comput. Vis., 2022, outlooks,â€IEEETrans.Intell.Transport.Syst.,vol.23,no.12,pp.
pp.370â€“389. 1,3 22694â€“22715,2022. 3
[24] C. R. Qi, Y. Zhou, M. Najibi, P. Sun, K. Vo, B. Deng, and [43] L.Kong,X.Xu,J.Cen,W.Zhang,L.Pan,K.Chen,andZ.Liu,
D. Anguelov, â€œOffboard 3d object detection from point cloud â€œCalib3d: Calibrating model preferences for reliable 3d scene
sequences,â€inIEEE/CVFConf.Comput.Vis.PatternRecog.,2021, understanding,â€arXivpreprintarXiv:2403.17010,2024. 3
pp.6134â€“6144. 1,3
[44] Y.Li,L.Kong,H.Hu,X.Xu,andX.Huang,â€œOptimizinglidar
[25] C.Liu,C.Gao,F.Liu,P.Li,D.Meng,andX.Gao,â€œHierarchical
placementsforrobustdrivingperceptioninadverseconditions,â€
supervisionandshuffledataaugmentationfor3dsemi-supervised
arXivpreprintarXiv:2403.17009,2024. 3
objectdetection,â€inIEEE/CVFConf.Comput.Vis.PatternRecog.,
[45] P.Jiang,P.Osteen,M.Wigness,andS.Saripallig,â€œRellis-3ddataset:
2023,pp.23819â€“23828. 1,3
Data,benchmarksandanalysis,â€inIEEEInt.Conf.Robot.Autom.,
[26] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu,
2021,pp.1110â€“1116. 3
A.Krishnan,Y.Pan,G.Baldan,andO.Beijbom,â€œnuscenes:A
[46] M.Naseer,S.Khan,andF.Porikli,â€œIndoorsceneunderstanding
multimodaldatasetforautonomousdriving,â€inIEEE/CVFConf.
in2.5/3dforautonomousagents:Asurvey,â€IEEEAccess,vol.7,
Comput.Vis.PatternRecog.,2020,pp.11621â€“11631. 1,3,6
pp.1859â€“1887,2018. 3
[27] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik,
[47] C. Xu, B. Wu, Z. Wang, W. Zhan, P. Vajda, K. Keutzer, and
P.Tsui,J.Guo,Y.Zhou,Y.Chai,B.Caine,V.Vasudevan,W.Han,
M.Tomizuka,â€œSqueezesegv3:Spatially-adaptiveconvolutionfor
J.Ngiam,H.Zhao,A.Timofeev,S.Ettinger,M.Krivokon,A.Gao,
efficient point-cloud segmentation,â€ in Eur. Conf. Comput. Vis.,
A.Joshi,Y.Zhang,J.Shlens,Z.Chen,andD.Anguelov,â€œScalability
2020,pp.1â€“19. 3
inperceptionforautonomousdriving:Waymoopendataset,â€in
IEEE/CVFConf.Comput.Vis.PatternRecog.,2020,pp.2446â€“2454.1, [48] L.Kong,Y.Liu,R.Chen,Y.Ma,X.Zhu,Y.Li,Y.Hou,Y.Qiao,
3,6,9 and Z. Liu, â€œRethinking range view representation for lidar
segmentation,â€ in IEEE/CVF Int. Conf. Comput. Vis., 2023, pp.
[28] J.Behley,M.Garbade,A.Milioto,J.Quenzel,S.Behnke,C.Stach-
228â€“240. 3
niss, and J. Gall, â€œSemantickitti: A dataset for semantic scene
understandingoflidarsequences,â€inIEEE/CVFInt.Conf.Comput. [49] A.Ando,S.Gidaris,A.Bursuc,G.Puy,A.Boulch,andR.Marlet,
Vis.,2019,pp.9297â€“9307. 1,2,3,5,6,9,10,11,12,14 â€œRangevit:Towardsvisiontransformersfor3dsemanticsegmen-
tationinautonomousdriving,â€inIEEE/CVFConf.Comput.Vis.
[29] D.J.Yeong,G.Velasco-Hernandez,J.Barry,andJ.Walsh,â€œSensor
PatternRecog.,2023,pp.5240â€“5250. 3
andsensorfusiontechnologyinautonomousvehicles:Areview,â€
Sensors,vol.21,no.6,p.2140,2021. 1,3,6 [50] X. Xu, L. Kong, H. Shuai, and Q. Liu, â€œFrnet: Frustum-
[30] K.Chitta,A.Prakash,B.Jaeger,Z.Yu,K.Renz,andA.Geiger, rangenetworksforscalablelidarsegmentation,â€arXivpreprint
â€œTransfuser:Imitationwithtransformer-basedsensorfusionfor arXiv:2312.04484,2023. 3
autonomous driving,â€ IEEE Trans. Pattern Anal. Mach. Intell., [51] Z.Zhou,Y.Zhang,andH.Foroosh,â€œPanoptic-polarnet:Proposal-
vol.45,no.11,pp.12878â€“12895,2023. 1,3,6 freelidarpointcloudpanopticsegmentation,â€inIEEE/CVFConf.
[31] L.Kong,Y.Liu,X.Li,R.Chen,W.Zhang,J.Ren,L.Pan,K.Chen, Comput.Vis.PatternRecog.,2021,pp.13194â€“13203. 3
andZ.Liu,â€œRobo3d:Towardsrobustandreliable3dperception [52] C.Choy,J.Gwak,andS.Savarese,â€œ4dspatio-temporalconvnets:
againstcorruptions,â€inIEEE/CVFInt.Conf.Comput.Vis.,2023,pp. Minkowskiconvolutionalneuralnetworks,â€inIEEE/CVFConf.
19994â€“20006. 1,6,9,11,12 Comput.Vis.PatternRecog.,2019,pp.3075â€“3084. 3,9,11
[32] S.Xie,L.Kong,W.Zhang,J.Ren,L.Pan,K.Chen,andZ.Liu, [53] F.Hong,L.Kong,H.Zhou,X.Zhu,H.Li,andZ.Liu,â€œUnified
â€œRobobev: Towards robust birdâ€™s eye view perception under 3dand4dpanopticsegmentationviadynamicshiftingnetworks,â€
corruptions,â€arXivpreprintarXiv:2304.06719,2023. 1,6,12 IEEETrans.PatternAnal.Mach.Intell.,vol.46,no.5,pp.3480â€“3495,
[33] X.Zhu,H.Zhou,T.Wang,F.Hong,Y.Ma,W.Li,H.Li,andD.Lin, 2024. 3
â€œCylindricalandasymmetrical3dconvolutionnetworksforlidar [54] V. E. Liong, T. N. T. Nguyen, S. Widjaja, D. Sharma, and Z. J.
segmentation,â€inIEEE/CVFConf.Comput.Vis.PatternRecog.,2021, Chong, â€œAmvnet: Assertion-based multi-view fusion network
pp.9939â€“9948. 2,3,5,9,11,12,14 forlidarsemanticsegmentation,â€arXivpreprintarXiv:2012.04934,
[34] Q.Chen,S.Vora,andO.Beijbom,â€œPolarstream:Streaminglidar 2020. 3
object detection and segmentation with polar pillars,â€ in Adv. [55] J.Xu,R.Zhang,J.Dou,Y.Zhu,J.Sun,andS.Pu,â€œRpvnet:Adeep
NeuralInf.Process.Syst.,vol.34,2021,pp.26871â€“26883. 2,12 and efficient range-point-voxel fusion network for lidar point16
cloudsegmentation,â€inIEEE/CVFInt.Conf.Comput.Vis.,2021, [77] Y.Liu,L.Kong,J.Cen,R.Chen,W.Zhang,L.Pan,K.Chen,and
pp.16024â€“16033. 3 Z.Liu,â€œSegmentanypointcloudsequencesbydistillingvision
[56] Y.Liu,R.Chen,X.Li,L.Kong,Y.Yang,Z.Xia,Y.Bai,X.Zhu,Y.Ma, foundationmodels,â€inAdv.NeuralInf.Process.Syst.,vol.36,2023.
Y.Li,Y.Qiao,andY.Hou,â€œUniseg:Aunifiedmulti-modallidar 3,11,12
segmentationnetworkandtheopenpcsegcodebase,â€inIEEE/CVF [78] G. Puy, S. Gidaris, A. Boulch, O. SimeÂ´oni, C. Sautier, P. PeÂ´rez,
Int.Conf.Comput.Vis.,2023,pp.21662â€“21673. 3 A.Bursuc,andR.Marlet,â€œThreepillarsimprovingvisionfounda-
[57] Y.Zhang,Y.Qu,Y.Xie,Z.Li,S.Zheng,andC.Li,â€œPerturbedself- tionmodeldistillationforlidar,â€inIEEE/CVFConf.Comput.Vis.
distillation:Weaklysupervisedlarge-scalepointcloudsemantic PatternRecog.,2024. 3
segmentation,â€ in IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. [79] M.Jaritz,T.-H.Vu,R.deCharette,E.Wirbel,andP.PeÂ´rez,â€œxmuda:
15520â€“15528. 3 Cross-modalunsuperviseddomainadaptationfor3dsemantic
[58] Y.Liu,Q.Hu,Y.Lei,K.Xu,J.Li,andY.Guo,â€œBox2seg:Learning segmentation,â€inIEEE/CVFConf.Comput.Vis.PatternRecog.,2020,
semanticsof3dpointcloudswithbox-levelsupervision,â€arXiv pp.12605â€“12614. 3
preprintarXiv:2201.02963,2022. 3 [80] J. Xu, W. Yang, L. Kong, Y. Liu, R. Zhang, Q. Zhou, and
[59] J.Mei,B.Gao,D.Xu,W.Yao,X.Zhao,andH.Zhao,â€œSemantic B.Fei,â€œVisualfoundationmodelsboostcross-modalunsupervised
segmentation of 3d lidar data in dynamic scene using semi- domainadaptationfor3dsemanticsegmentation,â€arXivpreprint
supervisedlearning,â€IEEETrans.Intell.Transport.Syst.,vol.21, arXiv:2403.10001,2024. 3
no.6,pp.2496â€“2509,2019. 3 [81] M. Jaritz, T.-H. Vu, R. de Charette, E. Wirbel, and P. PeÂ´rez,
[60] S.LaineandT.Aila,â€œTemporalensemblingforsemi-supervised â€œCross-modal learning for domain adaptation in 3d semantic
learning,â€inInt.Conf.Learn.Represent.,2017. 3 segmentation,â€ IEEE Trans. Pattern Anal. Mach. Intell., vol. 45,
[61] G.French,T.Aila,S.Laine,M.Mackiewicz,andG.Finlayson, no.2,pp.1533â€“1544,2023. 3
â€œSemi-supervised semantic segmentation needs strong, high- [82] R.Chen,Y.Liu,L.Kong,X.Zhu,Y.Ma,Y.Li,Y.Hou,Y.Qiao,
dimensional perturbations,â€ in Brit. Mach. Vis. Conf., 2020. 3, and W. Wang, â€œClip2scene: Towards label-efficient 3d scene
9,10 understandingbyclip,â€inIEEE/CVFConf.Comput.Vis.Pattern
[62] Y.Zou,Z.Zhang,H.Zhang,C.-L.Li,X.Bian,J.-B.Huang,and Recog.,2023,pp.7020â€“7030. 3
T. Pfister, â€œPseudoseg: Designing pseudo labels for semantic [83] S.Peng,K.Genova,C.Jiang,A.Tagliasacchi,M.Pollefeys,and
segmentation,â€inInt.Conf.Learn.Represent.,2020. 3 T.Funkhouser,â€œOpenscene:3dsceneunderstandingwithopen
[63] Y.Liu,Y.Tian,Y.Chen,F.Liu,V.Belagiannis,andG.Carneiro, vocabularies,â€inIEEE/CVFConf.Comput.Vis.PatternRecog.,2023,
â€œPerturbedandstrictmeanteachersforsemi-supervisedsemantic pp.815â€“824. 3
segmentation,â€inIEEE/CVFConf.Comput.Vis.PatternRecog.,2021,
[84] R.Chen,Y.Liu,L.Kong,N.Chen,X.Zhu,Y.Ma,T.Liu,and
pp.4258â€“4267. 3 W. Wang., â€œTowards label-free scene understanding by vision
[64] J.Yuan,Y.Liu,C.Shen,Z.Wang,andH.Li,â€œAsimplebaseline foundationmodels,â€inAdv.NeuralInf.Process.Syst.,vol.36,2023.
for semi-supervised semantic segmentation with strong data 3
augmentation,â€ in IEEE/CVF Int. Conf. Comput. Vis., 2021, pp.
[85] Y.Liu,L.Kong,X.Wu,R.Chen,X.Li,L.Pan,Z.Liu,andY.Ma,
8229â€“8238. 3
â€œMulti-spacealignmentstowardsuniversallidarsegmentation,â€
[65] V.Olsson,W.Tranheden,J.Pinto,andL.Svensson,â€œClassmix: inIEEE/CVFConf.Comput.Vis.PatternRecog.,2024. 3
Segmentation-baseddataaugmentationforsemi-supervisedlearn-
[86] Y. Grandvalet and Y. Bengio, â€œSemi-supervised learning by
ing,â€inIEEE/CVFWinterConf.Appl.Comput.Vis.,2021,pp.1369â€“
entropyminimization,â€inAdv.NeuralInf.Process.Syst.,vol.17,
1378. 3
2004. 4
[66] W.LuoandM.Yang,â€œSemi-supervisedsemanticsegmentation
[87] H.Zhang,M.Cisse,Y.N.Dauphin,andD.Lopez-Paz,â€œmixup:
viastrong-weakdual-branchnetwork,â€inEur.Conf.Comput.Vis.,
Beyondempiricalriskminimization,â€inInt.Conf.Learn.Represent.,
2020,pp.784â€“800. 3
2018. 5,13
[67] Y.Zou,Z.Yu,B.V.K.V.Kumar,andJ.Wang,â€œUnsupervised
[88] S.Yun,D.Han,S.J.Oh,S.Chun,J.Choe,andY.Yoo,â€œCutmix:
domainadaptationforsemanticsegmentationviaclass-balanced
Regularizationstrategytotrainstrongclassifierswithlocalizable
self-training,â€inEur.Conf.Comput.Vis.,2018,pp.289â€“305. 3,9,
features,â€inIEEE/CVFInt.Conf.Comput.Vis.,2019,pp.6023â€“6032.
10,11
5,11,13
[68] L. Yang, W. Zhuo, L. Qi, Y. Shi, and Y. Gao, â€œSt++: Make self-
[89] Q.Hu,B.Yang,L.Xie,S.Rosa,Y.Guo,N.T.ZhihuaWang,and
trainingworkbetterforsemi-supervisedsemanticsegmentation,â€
A. Markham, â€œRandla-net: Efficient semantic segmentation of
inIEEE/CVFConf.Comput.Vis.PatternRecog.,2022,pp.4268â€“4277.
large-scalepointclouds,â€inIEEE/CVFConf.Comput.Vis.Pattern
3
Recog.,2020,pp.11108â€“11117. 5
[69] A.P.S.Kohli,V.Sitzmann,andG.Wetzstein,â€œSemanticimplicit
[90] D.-H. Lee, â€œPseudo-label: The simple and efficient semi-
neuralscenerepresentationswithsemi-supervisedtraining,â€in
supervisedlearningmethodfordeepneuralnetworks,â€inInt.
IEEEInt.Conf.3DVision,2020,pp.423â€“433. 3
Conf.Mach.Learn.Worksh.,vol.3,2013. 6
[70] C.-Y. Sun, Y.-Q. Yang, H.-X. Guo, P.-S. Wang, X. Tong, Y. Liu,
andH.-Y.Shum,â€œSemi-supervised3dshapesegmentationwith [91] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,
multilevelconsistencyandpartsubstitution,â€ComputationalVisual P.DollaÂ´r,andC.L.Zitnick,â€œMicrosoftcoco:Commonobjectsin
Media,vol.9,no.2,pp.229â€“247,2023. 3 context,â€inEur.Conf.Comput.Vis.,2014,pp.740â€“755. 7
[71] S.Deng,Q.Dong,B.Liu,andZ.Hu,â€œSuperpoint-guidedsemi- [92] H. Zhang, F. Li, X. Zou, S. Liu, C. Li, J. Gao, J. Yang, and
supervised semantic segmentation of 3d point clouds,â€ arXiv L. Zhang, â€œObjects365: A large-scale, high-quality dataset for
preprintarXiv:2107.03601,2021. 3 objectdetection,â€inIEEE/CVFInt.Conf.Comput.Vis.,2019,pp.
[72] M.Cheng,L.Hui,J.Xie,andJ.Yang,â€œSspc-net:Semi-supervised 8430â€“8439. 7
semantic3dpointcloudsegmentationnetwork,â€inAAAIConf. [93] B.Zhou,H.Zhao,X.Puig,S.Fidler,A.Barriuso,andA.Torralba,
Artifi.Intell.,2021,pp.1140â€“1147. 3 â€œSceneparsingthroughade20kdataset,â€inIEEE/CVFConf.Comput.
[73] J. Hou, B. Graham, M. NieÃŸner, and S. Xie, â€œExploring data- Vis.PatternRecog.,2017,pp.633â€“641. 7
efficient3dsceneunderstandingwithcontrastivescenecontexts,â€ [94] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
inIEEE/CVFConf.Comput.Vis.PatternRecog.,2021,pp.15587â€“ R.Benenson,U.Franke,S.Roth,andB.Schiele,â€œThecityscapes
15597. 3 datasetforsemanticurbansceneunderstanding,â€inIEEE/CVF
[74] L. Kong, Y. Liu, L. X. Ng, B. R. Cottereau, and W. T. Ooi, Conf.Comput.Vis.PatternRecog.,2016,pp.3213â€“3223. 7,9,11,12
â€œOpeness:Event-basedsemanticsceneunderstandingwithopen [95] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,
vocabularies,â€inIEEE/CVFConf.Comput.Vis.PatternRecog.,2024. T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. DollaÂ´r, and
3 R.Girshick,â€œSegmentanything,â€inIEEE/CVFInt.Conf.Comput.
[75] J. Wang, H. Gang, S. Ancha, Y.-T. Chen, and D. Held, â€œSemi- Vis.,2023,pp.4015â€“4026. 7
supervised 3d object detection via temporal graph neural net- [96] H.Zhang,F.Li,X.Zou,S.Liu,C.Li,J.Gao,J.Yang,andL.Zhang,
works,â€inIEEEInt.Conf.3DVision,2021,pp.413â€“422. 3 â€œA simple framework for open-vocabulary segmentation and
[76] C.Sautier,G.Puy,S.Gidaris,A.Boulch,A.Bursuc,andR.Marlet, detection,â€inIEEE/CVFInt.Conf.Comput.Vis.,2023,pp.1020â€“
â€œImage-to-lidarself-superviseddistillationforautonomousdriving 1031. 7,8
data,â€ in IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2022, pp. [97] X.Zou,Z.-Y.Dou,J.Yang,Z.Gan,L.Li,C.Li,X.Dai,H.Behl,
9891â€“9901. 3,11,12 J.Wang,L.Yuan,N.Peng,L.Wang,Y.J.Lee,andJ.Gao,â€œGen-17
eralizeddecodingforpixel,image,andlanguage,â€inIEEE/CVF
Conf.Comput.Vis.PatternRecog.,2023,pp.15116â€“15127. 7,8
[98] X. Zou, J. Yang, H. Zhang, F. Li, L. Li, J. Gao, and Y. J. Lee,
â€œSegmenteverythingeverywhereallatonce,â€inAdv.NeuralInf.
Process.Syst.,vol.36,2023. 8
[99] T. Yin, X. Zhou, and P. Krahenbuhl, â€œCenter-based 3d object
detectionandtracking,â€inIEEE/CVFConf.Comput.Vis.Pattern
Recog.,2021,pp.11784â€“11793. 11
[100] A. Nekrasov, J. Schult, O. Litany, B. Leibe, and F. Engelmann,
â€œMix3d:Out-of-contextdataaugmentationfor3dscenes,â€inIEEE
Int.Conf.3DVision,2021,pp.116â€“125. 11,12,13
[101] A.Xiao,J.Huang,D.Guan,K.Cui,S.Lu,andL.Shao,â€œPolarmix:
Ageneraldataaugmentationtechniqueforlidarpointclouds,â€in
Adv.NeuralInf.Process.Syst.,vol.35,2022,pp.11035â€“11048. 11,
12
[102] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, and O. Litany,
â€œPointcontrast: Unsupervised pre-training for 3d point cloud
understanding,â€ in Eur. Conf. Comput. Vis., 2020, pp. 574â€“591.
11,12
[103] Z.Zhang,R.Girdhar,A.Joulin,andI.Misra,â€œSelf-supervised
pretrainingof3dfeaturesonanypoint-cloud,â€inIEEE/CVFInt.
Conf.Comput.Vis.,2021,pp.10252â€“10263. 11,12
[104] Y.-C.Liu,Y.-K.Huang,H.-Y.Chiang,H.-T.Su,Z.-Y.Liu,C.-T.Chen,
C.-Y.Tseng,andW.H.Hsu,â€œLearningfrom2d:Contrastivepixel-
to-pointknowledgetransferfor3dpretraining,â€arXivpreprint
arXiv:2104.0468,2021. 11,12
[105] A. Mahmoud, J. S. Hu, T. Kuai, A. Harakeh, L. Paull, and
S.L.Waslander,â€œSelf-supervisedimage-to-pointdistillationvia
semanticallytolerantcontrastiveloss,â€inIEEE/CVFConf.Comput.
Vis.PatternRecog.,2023,pp.7102â€“7110. 11,12
[106] M.Contributors,â€œMMDetection3D:OpenMMLabnext-generation
platformforgeneral3Dobjectdetection,â€https://github.com/
open-mmlab/mmdetection3d,2020. 10
[107] I.LoshchilovandF.Hutter,â€œDecoupledweightdecayregulariza-
tion,â€inInt.Conf.Learn.Represent.,2018. 10
[108] L.N.SmithandN.Topin,â€œSuper-convergence:Veryfasttraining
of neural networks using large learning rates,â€ arXiv preprint
arXiv:1708.07120,2017. 10
[109] A.Oliver,A.Odena,C.A.Raffel,E.D.Cubuk,andI.Goodfellow,
â€œRealisticevaluationofdeepsemi-supervisedlearningalgorithms,â€
inAdv.NeuralInf.Process.Syst.,vol.31,2018. 10
[110] T. DeVries and G. W. Taylor, â€œImproved regularization of
convolutional neural networks with cutout,â€ arXiv preprint
arXiv:1708.04552,2017. 13