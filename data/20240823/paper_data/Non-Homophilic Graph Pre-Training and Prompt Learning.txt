Non-Homophilic Graph Pre-Training and Prompt Learning
XingtongYuâˆ— JieZhangâˆ—
SingaporeManagementUniversity NationalUniversityofSingapore
Singapore Singapore
starlien0905@gmail.com jiezhang_jz@u.nus.edu
YuanFangâ€  RenheJiangâ€ 
SingaporeManagementUniversity TheUniversityofTokyo
Singapore Japan
yfang@smu.edu.sg jiangrh@csis.u-tokyo.ac.jp
Abstract
enterthecorrectconferencetitlefromyourrightsconfirmationemai(Confer-
Graphsareubiquitousformodelingcomplexrelationshipsbetween enceacronymâ€™XX).ACM,NewYork,NY,USA,15pages.https://doi.org/
XXXXXXX.XXXXXXX
objectsacrossvariousfields.Graphneuralnetworks(GNNs)have
becomeamainstreamtechniqueforgraph-basedapplications,but 1 Introduction
theirperformanceheavilyreliesonabundantlabeleddata.Tore-
ducelabelingrequirement,pre-trainingandpromptlearninghas Graphdataarepervasiveinreal-worldapplications,suchascitation
becomeapopularalternative.However,mostexistingpromptmeth- networks[18,56],socialnetworks[16,68],andmoleculargraphs
odsdonotdifferentiatehomophilicandheterophiliccharacteristics [21,51].Traditionalmethodstypicallytraingraphneuralnetworks
ofreal-worldgraphs.Inparticular,manyreal-worldgraphsare (GNNs) [20, 47] or graph transformers [59, 66] in a supervised
non-homophilic, not strictly or uniformly homophilic with mix- manner.However,theyrequirere-trainingandsubstantiallabeled
inghomophilicandheterophilicpatterns,exhibitingvaryingnon- dataforeachspecifictask.
homophiliccharacteristicsacrossgraphsandnodes.Inthispaper, Tomitigatethelimitationsofsupervisedmethods,pre-training
weproposeProNoG,anovelpre-trainingandpromptlearning methods have gained significant traction [15, 36, 48, 60]. They
frameworkforsuchnon-homophilicgraphs.First,weanalyzeex- firstlearnuniversal,task-independentpropertiesfromunlabeled
istinggraphpre-trainingmethods,providingtheoreticalinsights graphs,andthenfine-tunethepre-trainedmodelstovariousdown-
intothechoiceofpre-trainingtasks.Second,recognizingthateach streamtasksusingtask-specificlabels[48,60].However,asignif-
nodeexhibitsuniquenon-homophiliccharacteristics,wepropose icantgapoccursbetweenthepre-trainingobjectivesanddown-
aconditionalnetworktocharacterizethenode-specificpatterns streamtasks,resultinginsuboptimalperformance[44,61].More-
indownstreamtasks.Finally,wethoroughlyevaluateandanalyze over,fine-tuninglargepre-trainedmodelsiscostlyandstillrequires
ProNoGthroughextensiveexperimentsontenpublicdatasets. sufficienttask-specificlabelstopreventoverfitting.Asanalter-
nativetofine-tuning,promptlearninghasemergedasapopular
CCSConcepts parameter-efficienttechniqueforadaptationtodownstreamtasks
[7,26,42,43,46,62].Theyfirstutilizeauniversaltemplatetounify
â€¢Informationsystemsâ†’Webmining;Datamining;â€¢Com-
pre-traininganddownstreamtasks.Then,alearnablepromptis
putingmethodologiesâ†’Learninglatentrepresentations.
employedtomodifytheinputfeaturesorhiddenembeddingsof
Keywords thepre-trainedmodeltoalignwiththedownstreamtaskwithout
updatingthepre-trainedweights.Sinceaprompthasfarfewer
Graphmining,non-homophilicgraph,promptlearning,pre-training, parametersthanthepre-trainedmodel,promptlearningcanbe
few-shotlearning. especiallyeffectiveinlow-resourcesettings[61].
ACMReferenceFormat: However, current graph â€œpre-train, promptâ€ approaches rely
XingtongYuâˆ—,JieZhangâˆ—,YuanFangâ€ ,andRenheJiangâ€ .2018.Non-Homophilic on the homophily assumption or overlook the presence of het-
GraphPre-TrainingandPromptLearning.InProceedingsofMakesureto erophilicedges.Specifically,thehomophilyassumption[29,75]
statesthatneighboringnodesshouldsharethesamelabels,whereas
âˆ—Co-firstauthors.WorkwasdonewhileattheUniversityofTokyo. heterophilyreferstotheoppositescenariowheretwoneighboring
â€ Correspondingauthors.
nodeshavedifferentlabels.Weobservethatreal-worldgraphsare
typicallynon-homophilic,meaningtheyareneitherstrictlyoruni-
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
formlyhomophilicandmixbothhomophilicandheterophilicpatterns
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation [54,55].Inthiswork,weinvestigatethepre-trainingandprompt
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe learningmethodologyfornon-homophilicgraphs.Wefirstrevisit
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
existing graph pre-training methods for such graphs, followed
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. byproposingaPromptlearningframeworkforNon-homophilic
Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Graphs(orProNoGinshort).Thesolutionisnon-trivial,asthe
Â©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
notionofhomophilyencompassestwokeyaspects,eachwithits
ACMISBN978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX ownuniquechallenge.
4202
guA
22
]GL.sc[
1v49521.8042:viXraConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.,XingtongYuâˆ—,JieZhangâˆ—,YuanFangâ€ ,andRenheJiangâ€ 
performance.Forexample,astandardgraphpromptlearningap-
proach[26]generallyperformsworsewhenthehomophilyratios
1  0
ofnodesdecrease,asshowninFig.1(d),evenwithanon-homophily
3  2 pretexttask[60].Thoughsomerecentworks[5,46]haveproposed
node-specificprompts,theyarenotdesignedtoaccountforthe
3  2 variationinnodesâ€™non-homophiliccharactersitics.Inspiredbycon-
ditional prompt learning [71], we propose generating a unique
Lab  e0 l Music Sports M  o1 vies promptfromeachnodewithaconditionalnetwork(condition-net)
Cora Citeseer PROTEINS Cornell Chameleon Wisconsin HomopC hl ia lcss
edge ratio
Gender Hobby Wto ec fiap rst tu cr ae pt th ue refi tn he e-g nr oa ni -n he od m,d oi ps hti in lic ct pc ah tta er ra nc ste or fis et ai cc hso nf odea ec bh yn ro ead de -.
(a) Varying non-homophilic patterns (b) Dependence of 2ho7moph4il7iy ratio
ingoutitsmulti-hopneighborhood.Then,conditionedonthese
across different graphs on the target label
non-homophilicpatterns,thecondition-netproducesaseriesof
prompts,oneforeachnodethatreflectsitsvaryingnon-homophilic
characteristics.Thesepromptscanadjustthenodeembeddingsto
betteralignthemwiththedownstreamtask.
Insummary,thecontributionsofthisworkarethreefold:(1)
Weobservevaryingdegreesofhomophilyacrossgraphs,which
motivatesustorevisitgraphpre-trainingtasks.Weprovidedtheo-
reticalinsightswhichguideustochoosenon-homophilytasksfor
graphpre-training.(2)Wefurtherobservethat,withinthesame
graph,differentnodeshavediversedistributionsofnon-homophilic
characteristics.Toadapttotheuniquenon-homophilicpatternsof
(c) Diverse non-homophilic patterns (d) Performance of standard graph
across nodes in the same graph prompt across nodes eachnode,weproposetheProNoGframeworkfornon-homophilic
Figure1:Non-homophiliccharacteristicsofgraphs. promptlearning,whichisequippedwithacondition-nettogener-
ateaseriesofpromptsconditionedoneachnode.Thenode-specific
promptsenablesfine-grained,node-wiseadaptationforthedown-
First,differentgraphsexhibitvaryingdegreesofnon-homophily.
streamtasks.(3)Weperformextensiveexperimentsontenbench-
AsshowninFig.1(a),theCoracitationnetworkthatisgenerallycon-
markdatasets,demonstratingthesuperiorperformanceofProNoG
sideredlargelyhomophilicwith81%homophilicedges1,whereas
comparedtoasuiteofstate-of-the-artmethods.
theWisconsinwebpagegraphlinksdifferentkindsofwebpages,
whichishighlyheterophilicwithonly21%homophilicedges.More-
over,thenon-homophiliccharacteristicsofagraphalsodependson 2 RelatedWork
thetargetlabel.Forexample,inadatingnetworkshowninFig.1(a),
takinggenderasthenodelabel,thegraphismoreheterophilic Graphrepresentationlearning.GNNs[11,20,47,57,63]are
with2/7homophilicedges.However,takinghobbiesasthenode mainstreamtechniqueforgraphrepresentationlearning.Theytyp-
label,thegraphbecomesmorehomophilicwith4/7homophilic icallyoperateonamessage-passingframework,wherenodesit-
edges.Hence,howdowepre-trainagraphmodelirrespectiveofthe eratively update their representations by aggregating messages
graphâ€™shomophilycharacteristics? Inthiswork,weproposedefi- receivedfromtheirneighboringnodes.However,theeffectiveness
nitionsforhomophilytasksandhomophilysamples.Weshowthat ofGNNsheavilyreliesonabundanttask-specificlabeleddataand
pre-trainingwithnon-homophilysamplesincreasesthelossofany requiresre-trainingforvarioustasks.Inspiredbythesuccessof
homophilytask.Meanwhile,alesshomophilicgraphresultsina pre-trainingmethodsinthelanguage[4,6,9,40]andvision[1,67,
highernumberofnon-homophilysamples,subsequentlyincreas- 71,72]domains,pre-trainingmethods[14,15,19,27,36,45,48,60]
ingthepre-traininglossforhomophilytasks.Thismotivatesusto havebeenwidelyexploredforgraphs.Thesemethodsfirstpre-
moveawayfromhomophilytasksforgraphpre-training[26,42] trainagraphencoderbasedonself-supervisedtasks,thentransfer
andinsteadchooseanon-homophilytask[54,60]. priorknowledgetodownstreamtasks.However,alltheseGNNs
Second,differentnodeswithinthesamegrapharedistributed andpre-trainingmethodsarebasedonthehomophilicassumption,
differently in terms of their non-homophilic characteristics. As overlookingthatreal-worldgraphsaregenerallynon-homophilic.
shown in Fig. 1(c), on both Cora and Cornell, their nodes have Non-homophilicgraphlearning.ManyGNNs[2,28,29,35,73â€“
adiversehomophilyratios2.Hence,howdowecapturethefine-
75]havebeenproposed fornon-homophilic graphs,employing
grained,node-specificnon-homophiliccharacteristics? Duetothe
methodssuchascapturinghigh-frequencysignals[2],discover-
diversecharacteristicsacrossnodes,aone-size-fits-allsolutionfor
ingpotentialneighbors[17,33],andhigh-ordermessagepassing
allnodeswouldbeinadequate.However,existingapproachesgen-
[75].Moreover,recentworkshaveexploredpre-trainingonnon-
erallyapplyasingleprompttoallnodes[7,26,42,43],treatingall
homophilicgraphs[13,54,55]bycapturingneighborhoodinfor-
nodesuniformly.Thus,thesemethodsoverlookthefine-grained
mationtoconstructunsupervisedtasksforpre-trainingthegraph
node-wisenon-homophiliccharacteristics,leadingtosuboptimal
encoderandthentransferringpriornon-homophilicknowledge
1Definedasedgesconnectingtwonodesofthesamelabel;seeEq.(1)inSect.3. todownstreamtasksthroughfine-tuningwithtask-specificsuper-
2Definedasthefractionofanodeâ€™sneighborswiththesamelabel;seeEq.(2)inSect.3. vision.However,asignificantgapexistsbetweentheobjectivesNon-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
ofpre-trainingandfine-tuning[23,44,61].Whilepre-trainingfo- whereğœƒğ‘™ arethelearnableparametersintheğ‘™-thlayer,andAggr(Â·)
cusesonlearninginherentgraphattributeswithoutsupervision, istheaggregationfunction,whichcantakevariousforms[11,20,47,
fine-tuningadaptstheseinsightstodownstreamtasksbasedon 57,63].Inthefirstlayer,theinputnodeembeddingh0 ğ‘£ istypically
task-specificsupervision.Thisdiscrepancyhinderseffectiveknowl- initializedfromthenodefeaturevectorxğ‘£.Thefullsetoflearnable
edgetransferandnegativelyimpactsdownstreamperformance. parametersisdenotedasÎ˜={ğœƒ1,ğœƒ2,...}.Forsimplicity,wedefine
Graphpromptlearning.Originallydevelopedforthelanguage theoutputnoderepresentationsofthefinallayerashğ‘£,whichcan
domain,promptlearningeffectivelyunifiespre-traininganddown- thenbefedintothelossfunctionforaspecifictask.
streamobjectives[4,22,24].Recently,graphpromptlearninghas Problemstatement.Inthiswork,weaimtopre-trainagraphen-
emergedasapopularalternationtofine-tuningmethods[7,26, coderanddevelopapromptlearningframeworkfornon-homophilic
42,43,46,62,64].Thesemethodsfirstproposeaunifiedtemplate, graphs.Morespecifically,boththepre-trainingandpromptlearn-
thendesignpromptsspecificallytailoredtoeachdownstreamtask, ingarenotsensitivetothehomophiliccharacteristicsofthegraph
allowingthemtobetteralignwiththepre-trainedmodelwhile anditsnodes.
keepingthepre-trainedparametersfrozen.However,currentgraph Toevaluateournon-homophilicpre-trainingandpromptlearn-
promptlearningmethodstypicallyassumegraphsarehomophilic ing,wefocusontwocommontasksongraphdata:nodeclassi-
[43,61],neglectingthefactthatreal-worldgraphsaregenerallynon- fication and graph classification, in few-shot settings. For node
homophilic,exhibitingamixtureofhomophilicandheterophilic classificationwithinagraphğº = (ğ‘‰,ğ¸),letğ‘Œ bethesetofnode
patterns.Furthermore,thesemethodsusuallyapplyasingleprompt classes.Eachnodeğ‘£ ğ‘– âˆˆ ğ‘‰ hasaclasslabelğ‘¦ ğ‘– âˆˆ ğ‘Œ.Similarly,for
forallnodes,overlookingtheuniquecharacteristicsofeachnodeâ€™s graphclassificationacrossasetofgraphsG,letY bethesetof
non-homophilicpattern. possiblegraphlabels.Eachgraphğº ğ‘– âˆˆGhasaclasslabelğ‘Œ ğ‘– âˆˆY.
Inthefew-shotsetting,thereareonlyğ‘˜labeledsamplesperclass,
3 Preliminaries whereğ‘˜isasmallnumber(e.g.,ğ‘˜ â‰¤10).Thisscenarioisknownas
ğ‘˜-shotclassification[26,62,65].Notethatthehomophilyratiois
Graph. A graph is defined asğº = (ğ‘‰,ğ¸), whereğ‘‰ represents
definedwithrespecttosomepredefinedsetoflabels,whichmay
thesetofnodesandğ¸representsthesetofedges.Thenodesare
alsoassociatedwithafeaturematrixXâˆˆR|V|Ã—ğ‘‘ ,suchthatxğ‘£ âˆˆ notberelatedtotheclasslabelsindownstreamtasks.
Rğ‘‘ is a row of X representing the feature vector for node ğ‘£ âˆˆ 4 RevisitingGraphPre-training
ğ‘‰.Foracollectionofmultiplegraphs,weusethenotationG =
{ğº 1,ğº 2,...,ğº ğ‘}. Inthissection,werevisitgraphpre-trainingtaskstocopewithnon-
homophilicgraphs.Wefirstproposethedefinitionofhomophily
Homophilyratio.Givenamappingbetweenthenodesofagraph
tasksandrevealitsconnectiontothetrainingloss.Thetheoretical
andapredefinedsetoflabels,letğ‘¦ ğ‘£denotethelabelmappedtonode
insightsfurtherguideusinchoosinggraphpre-trainingtasks.
ğ‘£.ThehomophilyratioH(ğº)evaluatestherelationshipsbetween
thelabelsandthegraphstructure[29,75],measuringthefraction 4.1 TheoreticalInsights
ofhomophilicedgeswhosetwoendnodessharethesamelabel.
Wefocusoncontrastivegraphpre-trainingtasks.Givenamain-
Moreconcretely,
streamcontrastivetask[12,26,36,60,62,76],ğ‘‡ = ({Ağ‘¢ : ğ‘¢ âˆˆ
H(ğº)= |{(ğ‘¢,ğ‘£) âˆˆğ¸:ğ‘¦ ğ‘¢ =ğ‘¦ ğ‘£}| . (1) ğ‘‰},{Bğ‘¢ : ğ‘¢ âˆˆ ğ‘‰}),itslossfunction Lğ‘‡ canbegeneralizedtoa
|ğ¸|
standardformbelow.
âˆ‘ï¸
Additionally,thehomophilyratiocanbedefinedforeachnode Lğ‘‡ =âˆ’ lnğ‘ƒ(ğ‘¢,Ağ‘¢,Bğ‘¢), (4)
basedonitslocalstructure[30,55],measuringthefractionofa ğ‘¢âˆˆğ‘‰
n cao nde bâ€™ esn de ei fig nh eb dor as sthatshar |e {ğ‘¢th âˆˆes Nam (ğ‘£e )l :a ğ‘¦b ğ‘¢el. =T ğ‘¦h ğ‘£is }|node-specificratio ğ‘ƒ(ğ‘¢,Ağ‘¢,Bğ‘¢) â‰œ
(cid:205)
ğ‘âˆˆAğ‘¢sim(cid:205) (hğ‘ ğ‘¢âˆˆ ,A hğ‘¢ ğ‘)si +m (cid:205)(h ğ‘ğ‘¢ âˆˆ, Bh ğ‘¢ğ‘) sim(hğ‘¢,hğ‘), (5)
H(ğ‘£)= , (2) wheresim(Â·,Â·)representsasimilarityfunctionsuchascosinesim-
|N(ğ‘£)|
ilarity[37]inourexperiment,Ağ‘¢ isthesetofpositiveinstances
where|N(ğ‘£)|isthesetofneighboringnodesofğ‘£.Notethatboth fornodeğ‘¢,andBğ‘¢ isthesetofnegativeinstancesforğ‘¢.Theopti-
H(ğº)andH(ğ‘£)arein[0,1].Graphsornodeswithalargerpropor- mizationobjectiveoftaskğ‘‡ inEq.(4)istomaximizethesimilarity
tionofhomophilicedgeshaveahigherhomophilyratio. betweenğ‘¢anditspositiveinstanceswhileminimizingthesimilarity
Graphencoder.Graphencoderslearnlatentrepresentationsof betweenğ‘¢anditsnegativeinstances.Basedonthisloss,wefurther
graphs,embeddingtheirnodesintosomefeaturespace.Awidely proposethedefinitionsofhomophilytasksandhomophilysamples.
usedfamilyofgraphencodersisGNNs,whichtypicallyutilizea
Definition1(HomophilyTask). Onagraphğº =(ğ‘‰,ğ¸),apre-
message-passingmechanism[53,70].Specifically,eachnodeaggre-
trainingtaskğ‘‡ =({Ağ‘¢ :ğ‘¢ âˆˆğ‘‰},{Bğ‘¢ :ğ‘¢ âˆˆğ‘‰})isahomophilytask
gatesmessagesfromitsneighborstogenerateitsownrepresenta-
ifandonlyif,âˆ€ğ‘¢ âˆˆğ‘‰,âˆ€ğ‘ âˆˆAğ‘¢,âˆ€ğ‘ âˆˆBğ‘¢,(ğ‘¢,ğ‘) âˆˆğ¸âˆ§(ğ‘¢,ğ‘) âˆ‰ğ¸.A
tion.Bystackingmultiplelayers,GNNsenablesrecursivemessage
taskthatisnotahomophilytaskiscalledanon-homophilytask. â–¡
passingthroughoutthegraph.Formally,theembeddingofanode
ğ‘£intheğ‘™-thGNNlayer,denotedashğ‘™
ğ‘£,iscomputedasfollows. Inparticular,thewidelyusedlinkpredictiontask[26,32,34,62,
hğ‘™ ğ‘£ =Aggr(hğ‘™ ğ‘£âˆ’1,{hğ‘¢ğ‘™âˆ’1:ğ‘¢ âˆˆN(ğ‘£)};ğœƒğ‘™ ), (3) 6 to4, ğ‘¢6 a5 n] dis Ba ğ‘¢h io sm ao sp uh bi sl ey tt oa fsk n, ow deh se nre otA liğ‘¢ nkis edas toub ğ‘¢s .etofnodeslinkedConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.,XingtongYuâˆ—,JieZhangâˆ—,YuanFangâ€ ,andRenheJiangâ€ 
Definition 2 (Homophily Sample). On a graphğº = (ğ‘‰,ğ¸), non-homophilymethods,sinceAğ‘¢ andBğ‘¢ intheirpre-training
consideratriplet (ğ‘¢,ğ‘,ğ‘) whereğ‘¢ âˆˆ ğ‘‰, (ğ‘¢,ğ‘) âˆˆ ğ¸ and (ğ‘¢,ğ‘) âˆ‰ ğ¸. tasksarenotrelatedtotheconnectivitywithğ‘¢.Furtherdetails
Thetriplet (ğ‘¢,ğ‘,ğ‘) isahomophilysampleifandonlysim(ğ‘¢,ğ‘) > ofthesemethodsareshowninAppendix.B.Inourexperiment,
sim(ğ‘¢,ğ‘),anditisanon-homophilysampleotherwise. â–¡ weusethenon-homophilymethodGraphCLasthepretexttask
toobtainourmainresultsfornon-homophilygraphs,asitisa
Subsequently,wecanestablishthefollowingtheorems.
classicpre-trainingmethodwithcompetitiveperformance.Wealso
Theorem1. Forahomophilytaskğ‘‡,addingahomophilysample experimentwithlinkprediction[26]andGraphACLforfurther
alwaysresultsinasmallerlossthanaddinganon-homophilysample. evaluation,asshowninTable5.
Proof. Considerahomophilysample(ğ‘¢,ğ‘,ğ‘)forsome(ğ‘¢,ğ‘) âˆˆ 5 Non-homophilicPromptLearning
ğ¸and(ğ‘¢,ğ‘)âˆ‰ğ¸,aswellasanon-homophilysample(ğ‘¢,ğ‘â€²,ğ‘â€²)for
some(ğ‘¢,ğ‘â€²) âˆˆğ¸and(ğ‘¢,ğ‘â€²) âˆ‰ğ¸.Lettheoveralllosswith(ğ‘¢,ğ‘,ğ‘) Inthissection,weproposeProNoG,ourpromptlearningframe-
beğ¿ ğ‘‡,andthatwith(ğ‘¢,ğ‘â€²,ğ‘â€²)beğ¿ ğ‘‡â€².Since(ğ‘¢,ğ‘,ğ‘)ishomophily, workfornon-homophilicgraphs.Wefirstintroducetheoverall
wehavesim(ğ‘¢,ğ‘) >sim(ğ‘¢,ğ‘),andthusğ‘(ğ‘¢,ğ‘,ğ‘) >0.5.Moreover, framework,andthendevelopthepromptgenerationandtuning
since(ğ‘¢,ğ‘â€²,ğ‘â€²)isnon-homophily,wehavesim(ğ‘¢,ğ‘â€²) â‰¤sim(ğ‘¢,ğ‘â€²), process.Finally,wepresenttheoverallalgorithmandanalyzeits
andthusğ‘(ğ‘¢,ğ‘â€²,ğ‘â€²) â‰¤0.5.Hence,ğ‘(ğ‘¢,ğ‘,ğ‘) >ğ‘(ğ‘¢,ğ‘â€²,ğ‘â€²),implying complexity.
thatğ¿
ğ‘‡
<ğ¿ ğ‘‡â€². â–¡
5.1 Overallframework
Theorem2. Consideragraphğº =(ğ‘‰,ğ¸)withalabelmapping WeillustratetheoverallframeworkofProNoGinFig.2.Itinvolves
functionğ‘‰ â†’ğ‘Œ,andletğ‘¦
ğ‘£
âˆˆğ‘Œ denotethelabelmappedtoğ‘£ âˆˆğ‘‰.
twostages:(a)graphpre-trainingand(b)downstreamadaptation.
Supposethelabelmappingsatisfiesthat Ingraphpre-training,wepre-trainagraphencoderusinganon-
âˆ€ğ‘¢,ğ‘,ğ‘ âˆˆğ‘‰,ğ‘¦ ğ‘¢ =ğ‘¦ ğ‘âˆ§ğ‘¦ ğ‘¢ â‰ ğ‘¦ ğ‘ â‡’sim(ğ‘¢,ğ‘) >sim(ğ‘¢,ğ‘). homophilicpre-trainingtask,asshowninFig.2(a).Subsequently,
toadaptthepre-trainedmodeltodownstreamtasks,wepropose
LetE ğ‘‡ denotetheexpectednumberofhomophilysamplesforaho-
aconditionalnetwork(condition-net)thatgeneratesaseriesof
mophilytaskğ‘‡ onthegraphğº.Then,E ğ‘‡ increasesmonotonicallyas
prompts,asdepictedinFig.2(b).Asaresult,eachnodeisequipped
thehomophilyratioH(ğº)definedw.r.t.ğ‘Œ increases.
with its own prompt, which can be used to modify its features
Proof. Forahomophilytaskğ‘‡ =({Ağ‘¢ :ğ‘¢ âˆˆğ‘‰},{Bğ‘¢ :ğ‘¢ âˆˆğ‘‰}), toalignwiththedownstreamtask.Morespecifically,theprompt
atriplet(ğ‘¢,ğ‘,ğ‘)forsomeğ‘¢ âˆˆğ‘‰,ğ‘âˆˆAğ‘¢andğ‘ âˆˆBğ‘¢isahomophily generationisconditionedontheuniquepatternsofeachnode,in
samplewithaprobabilityofğ‘ƒ(ğ‘¦ ğ‘¢ =ğ‘¦ ğ‘)(1âˆ’ğ‘ƒ(ğ‘¦ ğ‘¢ =ğ‘¦ ğ‘)),sinceğ‘¦ ğ‘¢ = ordertoachievefine-grainedadaptationcateringtothediverse
ğ‘¦ ğ‘ âˆ§ğ‘¦ ğ‘¢ â‰ ğ‘¦ ğ‘ impliessim(ğ‘¢,ğ‘) > sim(ğ‘¢,ğ‘).Hence,theexpected non-homophiliccharacteristicsofeachnode,asdetailedinFig.2(c).
numberofhomophilysamplesforğ‘‡ is
5.2 PromptGenerationandTuning
âˆ‘ï¸
E ğ‘‡ = |Ağ‘¢||Bğ‘¢|ğ‘ƒ(ğ‘¦ ğ‘¢ =ğ‘¦ ğ‘)(1âˆ’ğ‘ƒ(ğ‘¦ ğ‘¢ =ğ‘¦ ğ‘)). (6)
ğ‘¢âˆˆğ‘‰
Promptgeneration.Innon-homophilicgraphs,differentnodes
Foraconstantnumberofnodeswithlabelğ‘¦ ğ‘¢,asH(ğº)increases, arecharacterizedbyuniquenon-homophilicpatterns.Specifically,
ğ‘ƒ(ğ‘¦ ğ‘¢ =ğ‘¦ ğ‘)alwaysincreaseswhileğ‘ƒ(ğ‘¦ ğ‘¢ =ğ‘¦ ğ‘)alwaysdecreases, differentnodestypicallyhavediversehomophilyratiosH(ğ‘£),indi-
leadingtoalargerE ğ‘‡. â–¡ catingdistincttopologicalstructureslinkingtotheirneighboring
node.Moreover,evennodeswithsimilarhomophilyratiosmay
In the next part, the theorems will guide us in choosing the havedifferentneighborhooddistributionsintermsofthevarying
appropriatepre-trainingtasksfornon-homophilicgraphs. homophilyratiosoftheneighboringnodes.Therefore,insteadof
learningasinglepromptforallnodesasinstandardgraphprompt
4.2 Non-homophilicGraphPre-training
learning[26,42,43,62],wedesignacondition-net[71]togenerate
Considerahomophilytaskğ‘‡.AccordingtoTheorem2,fornon- aseriesofnon-homophilicpattern-conditionedprompts.Conse-
homophilicgraphswithlowerhomophilyratios,onaveragethere quently,eachnodeisequippedwithitsownuniqueprompt,aiming
arefewerhomophilysamplesandmorenon-homophilysamples toadapttoitsdistinctnon-homophiliccharacteristics.
forğ‘‡.Consequently,basedonTheorem1,addinganon-homophily First,thenon-homophilicpatternsofanodecanbecharacterized
samplealwaysresultsinalargerlossthanaddingahomophily byconsideringamulti-hopneighborhoodaroundthenode.Specifi-
sample. Therefore, for non-homophilic graphs, especially those cally,givenanodeğ‘£,wereadouttheirğ›¿-hopego-networkğ‘† ğ‘£,which
withlowhomophilyratio,non-homophilytasksareabetterchoice isaninducedsubgraphcontainingthenodeğ‘£andnodesreachable
comparedtohomophilytaskswhenoptimizingthetrainingloss. fromğ‘£ inatmostğ›¿ steps.InspiredbyGGCN[58],thereadoutis
Werevisitmainstreamgraphpre-trainingmethodsandcatego- weightedbythesimilaritybetweenğ‘£andtheirneighbors,asshown
rizethemintotwocategories:homophilymethodsthatemploy inFig.2(c),obtainingarepresentationofthesubgraphğ‘† ğ‘£ givenby
homophilytasks,andnon-homophilymethodsthatdonot.Specifi- 1 âˆ‘ï¸
cally,GMI[34],GraphPrompt[26],MultiGPrompt[65],HGPrompt
sğ‘£ =
|ğ‘† ğ‘£|
hğ‘¢ Â·sim(hğ‘¢,hğ‘£), (7)
[64]andGraphPrompt+[62]areallhomophilymethods,sincetheir
ğ‘¢âˆˆğ‘†ğ‘£
pre-trainingtasksutilizesaformoflinkprediction,whereAğ‘¢ isa where|ğ‘† ğ‘£|denotesthenumberofnodesinğ‘† ğ‘£.Inourexperiment,
setofnodeslinkedtoğ‘¢,andBğ‘¢ isasetofnodesnotlinkedfrom wesetğ›¿ = 2tobalancebetweenefficiencyandcapturingmore
ğ‘¢.Incontrast,DGI[48],GraphCL[60],andGraphACL[55]are uniquenon-homophilicpatternsintheneighborhoodofğ‘£.Non-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
Prompt generation
Non-homophily Tuned
pre-training loss No md ae t re im xb Readout eS mu bb g mr aa tp rh ix Frozen           (  ,      )                     âˆ™                (( ( ( (            ,, , , ,                )) ) ) )
Graph Pre-trained Downstream
Condition-net
encoder graph encoder


Condition-net


P mro am trip xt Prom âˆ™pting sub  g2  r- ah po hp  o  f v promp  t 
(a) Pre-training (b) Downstream adaptation with conditional prompting (c) Details of prompt generation
Figure2:OverallframeworkofProNoG.
Next,foreachdownstreamtask,ourgoalistoassignaunique ğ‘Œ.Thedownstreamlossfunctionis
p pr ro om mp pt tv ve ec ct to or rt so wea oc uh ldn so id ge n. iH fio caw ne tv lyer i, nd cir re ec at sl ey tp ha era nm ue mte br eiz ri on fg lt eh ae rs ne
- L down(ğœ™ ğ‘¡)=âˆ’ âˆ‘ï¸ ln
exp(cid:16) ğœ1si (cid:16)m(hËœ ğ‘¡,ğ‘¥ğ‘–,hÂ¯ ğ‘¡,ğ‘¦ğ‘–)(cid:17)
(cid:17), (10)
ableparameters,whichmayoverfittothelightweightsupervision (ğ‘¥ğ‘–,ğ‘¦ğ‘–)âˆˆDğ‘¡ (cid:205) ğ‘âˆˆğ‘Œexp ğœ1sim(hËœ ğ‘¡,ğ‘¥ğ‘–,hÂ¯ ğ‘¡,ğ‘)
infew-shotsettings.Tocatertotheuniquenon-homophilicchar-
acteristicsofeachnodewithminimalparameters,weproposeto wherehËœ ğ‘¡,ğ‘¥ğ‘– denotestheoutputembeddingofnodeğ‘£/graphğºfor
employacondition-net[71]togeneratenode-specificpromptvec- taskğ‘¡.Specifically,fornodeclassificationhËœ ğ‘¡,ğ‘£ istheoutputembed-
tors.Specifically,conditionedonthesubgraphreadoutsğ‘£ofanode dinginEq.9;forgraphclassification,hËœ ğ‘¡,ğº =(cid:205) ğ‘¢âˆˆğ‘‰ hËœ ğ‘¡,ğ‘¢ involving
ğ‘£,thecondition-netgeneratesauniquepromptvectorforğ‘£w.r.t.a anadditionalgraphreadout.Theprototypeembeddingforclassğ‘,
taskğ‘¡,denotedbypğ‘¡,ğ‘£,asfollows. hÂ¯ ğ‘¡,ğ‘,istheaverageoftheoutputembeddingofallnodes/graphs
pğ‘¡,ğ‘£ =CondNet(sğ‘£;ğœ™ ğ‘¡), (8)
belongingtoclassğ‘.
Duringprompttuning,weupdateonlythelightweightparame-
whereCondNetisthecondition-netparameterizedbyğœ™ ğ‘¡.Itoutputs tersofthecondition-net(ğœ™ ğ‘¡),whilefreezingthepre-trainedGNN
auniquepromptvectorpğ‘¡,ğ‘£,whichvariesbasedontheinputğ‘  ğ‘£ weights.Thus,ourprompttuningisparameter-efficientandamenable
thatcharacterizesthenon-homophilypatternsofnodeğ‘£.Notethat tofew-shotsettings,whereDğ‘¡ containsonlyasmallnumberof
thisisaformofhypernetworks[10],whichemploysasecondary
trainingexamplesfortaskğ‘¡.
networktogeneratetheparametersforthemainnetworkcondi-
5.3 AlgorithmandComplexityAnalysis
tionedontheinputfeature.Inourcontext,thecondition-netis
thesecondarynetwork,generatingpromptparameterswithoutex- Algorithm.Wedetailthemainstepsforconditionalpromptgen-
pandingthenumberoflearnableparametersinthemainnetwork.
erationandtuninginAlgorithm1,AppendixA.
ThesecondarynetworkCondNetcanbeanylearnablefunction,
Complexityanalysis.Foradownstreamgraphğº,thecomputa-
suchasafully-connectedlayeroramulti-layerperceptron(MLP).
tionalprocessofProNoGinvolvestwomainparts:encodingnodes
WeemployanMLPwithacompactbottleneckarchitecture[52].
viaapre-trainedGNN,andconditionalpromptlearning.Thefirst
Subsequently,weperformfine-grained,node-wiseadaptationto
taskğ‘¡.Concretely,thepromptpğ‘¡,ğ‘£ fornodeğ‘£isemployedtoadjust partâ€™scomplexityisdeterminedbytheGNNâ€™sarchitecture,akinto
ğ‘£â€™sfeaturesoritsembeddingsinthehiddenoroutputlayers[62].In othermethodsemployingapre-trainedGNN.InastandardGNN,
eachnodeaggregatesfeaturesfromuptoğ‘› neighborsperlayer.
ourexperiments,wechooseasimpleyeteffectiveimplementation
Assumingtheaggregationinvolvesatmostğ·neighbors,thecom-
thatmodifiesthenodesâ€™outputembeddingsthroughanelement-
plexityofcalculatingnodeembeddingsoverğ¿layersisğ‘‚(ğ·ğ¿Â·|ğ‘‰|),
wiseproduct,asfollows.
where|ğ‘‰|denotesthenumberofnodes.Thesecondpart,condi-
hËœ ğ‘¡,ğ‘£ =pğ‘¡,ğ‘£ âŠ™hğ‘£, (9) tional prompt learning, has two stages: prompt generation and
prompttuning.Inthepromptgenerationstage,eachsubgraphem-
wherethepromptpğ‘¡,ğ‘£ isgeneratedwithanequaldimensionashğ‘£. beddingisfedintothecondtion-net.Inourexperment,weusea2
Prompttuning.Inthiswork,wefocusontwocommontypes layerMLPascondition-net,resultinginacomplexityofğ‘‚(2Â·|ğ‘‰|).
ofdownstreamtask:nodeclassificationandgraphclassification. Duringprompttuning,eachnodeinğº isadjustedusingaprompt
Theprompttuningprocessdoesnotdirectlyoptimizetheprompt vector,withacomplexityofğ‘‚(|ğ‘‰|).Therefore,thetotalcomplexity
vectors;insteaditoptimizesthecondition-net,whichsubsequently forconditionalpromptlearningisğ‘‚(3Â·|ğ‘‰|).
generatesthepromptvectors,foragivendownstreamtask. Inconclusion,theoverallcomplexityof ProNoGisğ‘‚((ğ·ğ¿ +
Weutilizealossfunctionbasedonnode/graphsimilarityfol-
3)Â·|ğ‘‰|).Thefirstpartdominatestheoverallcomplexity,asğ‘‚(ğ‘›ğ¿Â·
lowingpreviouswork[26,62].Formally,forataskğ‘¡ withalabeled |ğ‘‰|) â‰« ğ‘‚(3Â·|ğ‘‰|),wherewesetğ¿ = 2forexperimentsonlow-
trainingsetDğ‘¡ ={(ğ‘¥ 1,ğ‘¦ 1),(ğ‘¥ 2,ğ‘¦ 2),...},whereğ‘¥ ğ‘– canbeeithera homophilygraphs.Thus,theadditionalcomputationalcostintro-
nodeoragraph,andğ‘¦ ğ‘– âˆˆğ‘Œ isğ‘¥ ğ‘–â€™sclasslabelfromasetofclasses ducedbytheconditionalprompttuningstepisminimal.
Readout MeanConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.,XingtongYuâˆ—,JieZhangâˆ—,YuanFangâ€ ,andRenheJiangâ€ 
Table1:Summaryofdatasets.
[43],arebasedonthemeta-learningparadigm[8],whichrequires
anadditionalsetoflabeledbaseclassesinadditiontothefew-shot
Homophily Graph Avg. Avg. Node Node
Graphs classes.Consequently,thesemethodsarenotdirectlycomparable
ratio classes nodes edges features classes
toourframework.
Wisconsin 1 0.21 - 251 199 1,703 5 Parametersettings.Forallbaselines,weusetheoriginalauthorsâ€™
Squirrel 1 0.22 - 5,201 217,073 2,089 5
codeandfollowtheirrecommendedsettings,whilefurthertuning
Chameleon 1 0.23 - 2,277 36,101 2,325 5
theirhyperparameterstoensureoptimalperformance.Detailed
Cornell 1 0.30 - 183 295 1,703 5
PROTEINS 1,113 0.66 2 39.06 72.82 1 3 descriptionsoftheimplementationsandsettingsforboththebase-
ENZYMES 600 0.67 6 32.63 62.14 18 3 linesandourProNoGareprovidedinAppendixE.
Citeseer 1 0.74 - 3,327 4,732 3,703 6 Setup of downstream tasks. We conduct two types of down-
Cora 1 0.81 - 2,708 5,429 1,433 7
streamtask:nodeclassification,andgraphclassification.Thesetasks
BZR 405 - 2 35.75 38.36 3 - aresetupasğ‘˜-shotclassificationproblems,meaningthatforeach
COX2 467 - 2 41.22 43.45 3 -
class,ğ‘˜instances(nodesorgraphs)arerandomlyselectedforsu-
HomophilyratioiscalculatedbyEq.1.NotethatBZRandCOX2donothaveany
pervision.Giventhatalllow-homophilydatasets,i.e.,Wisconsin,
nodelabel,andthusitisnotabletocalculatetheirhomophilyratios.
Squirrel,ChameleonandCornellonlycompriseasinglegraphand
cannotbedirectlyusedforgraphclassification.Thus,followingpre-
6 Experiments viousresearch[27,64],wegeneratemultiplegraphsbyconstruct-
ingego-networkscenteredonthelabelednodesineachdataset.
Inthissection,weconductexperimentstoevaluateProNoG,and
Wethenperformgraphclassificationontheseego-networks,each
analyzetheempiricalresults.
labeledaccordingtoitscentralnode.Fordatasetswithhighho-
mophilyratios,PROTEINS,ENZYMES,BZRandCOX2haveoriginal
6.1 ExperimentalSetup
graphlabels,sowedirectlyconductgraphclassificationonthese
Datasets. We conduct experiments on ten benchmark datasets. graphs.Sincetheğ‘˜-shottasksarebalancedclassificationproblems,
Wisconsin[33],Cornell[33],Chameleon[39],andSquirrel[39]are weuseaccuracytoevaluateperformance,inlinewithpriorstud-
allwebpagegraphs.Eachdatasetfeaturesasinglegraphwhere ies[25,26,49,62].Wepre-trainthegraphencoderonceforeach
nodescorrespondtowebpagesandedgesrepresenthyperlinks datasetandthenusethesamepre-trainedmodelforalldownstream
connectingthesepages.Cora[31]andCiteseer [41]arecitation
tasks.Wegenerate100ğ‘˜-shottasksforbothnodeclassificationand
networks.Thesedatasetsconsistofasinglegrapheach,withnodes graphclassificationbyrepeatingthesamplingprocess100times.
signifyingscientificpapersandedgesindicatingcitationrelation- Eachtaskisexecutedwithfivedifferentrandomseeds,leadingtoa
ships.PROTEINS[3]consistsofaseriesofproteingraphs.Nodes totalof500resultspertasktype.Wereportthemeanandstandard
inthesegraphsdenotesecondarystructures,whileedgesdepict deviationofthese500outcomes.
neighboringrelationshipseitherwithintheaminoacidsequence
orinthree-dimensionalspace.ENZYMES[50],BZR[38],andCOX2 6.2 Few-shotPerformanceEvaluation
[38]arecollectionsofmoleculargraphs.Thesedatasetsdescribe
enzymestructuresfromtheBRENDAenzymedatabase,ligandsre- Wefirstevaluateone-shotclassificationtasks.Then,wevarythe
latedtobenzodiazepinereceptors,andcyclooxygenase-2inhibitors, numberofshotstoinvestigatetheirimpactonperformance.
respectively.WesummarizethesedatasetsinTable1,andpresent One-shotperformance.Wepresenttheresultsofone-shotnode
furtherdetailsinAppendixC. and graph classification tasks on non-homophilic graphs in Ta-
Baselines.WeevaluateProNoGagainstaseriesofstate-of-the-art bles2and3,respectively.Wemakethefollowingobservations:(1)
methods,categorizedintothreeprimarygroups: ProNoGsurpassesallbaselinemethodsacrossallsettings,outper-
(1)End-to-endgraphneuralnetworks:GCN[20],GAT[47],H2GCN formingthebestcompetitorbyupto21.49%onnodeclassification
[75],andFAGCN[2]aretrainedinasupervisedmannerdirectly and6.50%ongraphclassification.Theseresultsdemonstrateits
usingdownstreamlabels.Specifically,GCNandGATareoriginally effectivenessinlearningpriorknowledgefromnon-homophilic
designedforhomophilicgraphs,H2GCNforheterophilicgraphs, graphs and capturing nodesâ€™ specific patterns. (2) Other graph
andFAGCNfornon-homophilicgraphs. promptlearningmethods,i.e.,GPPT,GraphPrompt,andGraph-
(2)Graphpre-trainingmodels:DGI[48],GraphCL[60],DSSL Prompt+,significantlylagbehindProNoG.Theirsuboptimalper-
[54],GraphACL[55]followtheâ€œpre-train,fine-tuneâ€paradigm. formancecanbeattributedtotheirinabilitytoaccountforavariety
(3)Graphpromptlearningmodels:GPPT[42],GraphPrompt[26], ofnode-specificpatterns.Theseresultsunderscoretheimportance
andGraphPrompt+[62]useself-supervisedpre-trainingtaskswith ofourconditionalpromptingincharacterizingnodeembeddings
asingletypeofpromptfordownstreamadaptation.NotethatGPPT tocapturenodesâ€™specificpatterns.(3)GPPTisatbestcomparable
isspecificallydesignedfornodeclassificationandcannotbedirectly to,andoftenperformsworsethanotherbaselinesbecauseitisnot
usedforgraphclassification.Therefore,inourexperiments,weuse specificallydesignedforfew-shotlearning.
GPPTexclusivelyfornodeclassificationtasks. Few-shotperformance.ToassesstheperformanceofProNoG
WeprovidefurtherdetailsonthesebaselinesinAppendixD.Itâ€™s with different amounts of labeled data, we vary the number of
worthnotingthatsomegraphfew-shotlearningmethods,suchas shotsinthedownstreamtasksandpresenttheresultsinFig.3
Meta-GNN[69],AMM-GNN[49],RALE[25],VNT[46],andProG andAppendixF.NotethatgiventhelimitednumberofnodesinNon-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
Table2:Accuracyevaluationonfew-shotnodeclassification.
Methods Wisconsin Squirrel Chameleon Cornell PROTEINS ENZYMES Citeseer Cora
GCN 21.39Â± 6.56 20.00Â± 0.29 25.11Â± 4.19 21.81Â± 4.71 43.32Â± 9.35 48.08Â± 4.71 31.27Â± 4.53 28.57Â± 5.07
GAT 28.01Â± 5.40 21.55Â± 2.30 24.82Â± 4.35 23.03Â±13.19 31.79Â±20.11 35.32Â±18.72 30.76Â± 5.40 28.40Â± 6.25
H2GCN 23.60Â± 4.64 21.90Â± 2.15 25.89Â± 4.96 32.77Â±14.88 29.60Â± 6.99 37.27Â± 8.73 26.98Â± 6.25 34.58Â± 9.43
FAGCN 35.03Â±17.92 20.91Â± 1.79 22.71Â± 3.74 28.67Â±17.64 32.63Â± 9.94 35.87Â±13.47 26.46Â± 6.34 28.28Â± 9.57
DGI 28.04Â± 6.47 20.00Â± 1.86 19.33Â± 4.57 32.54Â±15.66 45.22Â±11.09 48.05Â±14.83 45.00Â± 9.19 54.11Â± 9.60
GraphCL 29.85Â± 8.46 21.42Â± 2.22 27.16Â± 4.31 24.69Â±14.06 46.15Â±10.94 48.88Â±15.98 43.12Â± 9.61 51.96Â± 9.43
DSSL 28.46Â±10.31 20.94Â± 1.88 27.92Â± 3.93 20.36Â± 5.38 40.42Â±10.08 66.59Â±19.28 39.86Â± 8.60 40.79Â± 7.31
GraphACL 34.57Â±10.46 24.44Â± 3.94 26.72Â± 4.67 33.17Â±16.06 42.16Â±13.50 47.57Â±14.36 35.91Â± 7.87 46.65Â± 9.54
GPPT 27.39Â± 6.67 20.09Â± 0.91 24.53Â± 2.55 25.09Â± 2.92 35.15Â±11.40 35.37Â± 9.37 21.45Â± 3.45 15.37Â± 4.51
GraphPrompt 31.48Â± 5.18 21.22Â± 1.80 25.36Â± 3.99 31.00Â±13.88 47.22Â±11.05 53.54Â±15.46 45.34Â±10.53 54.25Â± 9.38
GraphPrompt+ 31.54Â± 4.54 21.24Â± 1.82 25.73Â± 4.50 31.65Â±14.48 46.08Â± 9.96 57.68Â±13.12 45.23Â±10.01 52.51Â± 9.73
ProNoG 44.72Â±11.93 24.59Â± 3.41 30.67Â± 3.73 37.90Â± 9.31 48.95Â±10.85 72.94Â±20.23 49.02Â±10.66 57.92Â±11.50
Resultsarereportedinpercent.Thebestmethodisboldedandtherunner-upisunderlined.
Table3:Accuracyevaluationonfew-shotgraphclassification.
Methods Wisconsin Squirrel Chameleon Cornell PROTEINS ENZYMES BZR COX2
GCN 21.39Â± 6.56 11.77Â± 3.10 17.21Â± 4.80 26.36Â± 4.35 51.66Â±10.87 19.30Â± 6.36 45.06Â±16.30 43.84Â±13.94
GAT 24.93Â± 7.59 20.70Â± 1.51 25.71Â± 3.32 22.66Â±12.46 51.33Â±11.02 20.24Â± 6.39 46.28Â±15.26 51.72Â±13.70
H2GCN 22.23Â± 6.38 20.69Â± 1.42 26.76Â± 3.98 23.11Â±11.78 53.81Â± 8.85 19.40Â± 5.57 50.28Â±12.13 53.70Â±11.73
FAGCN 23.81Â± 9.50 20.83Â± 1.43 25.93Â± 4.03 25.71Â±13.12 55.45Â±11.57 19.95Â± 5.94 50.93Â±12.41 50.22Â±11.50
DGI 29.77Â± 6.22 20.50Â± 1.52 24.29Â± 4.33 18.60Â±12.79 50.32Â±13.47 21.57Â± 5.37 49.97Â±12.63 54.84Â±14.76
GraphCL 27.93Â± 5.27 21.01Â± 1.86 26.45Â± 4.30 20.03Â±10.05 54.81Â±11.44 19.93Â± 5.65 50.50Â±18.62 47.64Â±22.42
DSSL 22.05Â± 3.90 20.74Â± 1.61 26.19Â± 3.72 18.38Â±10.63 52.73Â±10.98 23.14Â± 6.71 49.04Â± 8.75 54.23Â±14.17
GraphACL 22.98Â± 5.89 20.80Â± 1.28 26.28Â± 3.93 26.50Â±17.18 56.11Â±13.95 20.28Â± 5.60 49.24Â±17.87 49.59Â±23.93
GraphPrompt 28.34Â± 3.89 21.22Â± 1.80 26.51Â± 4.67 24.06Â±13.71 53.61Â± 8.90 21.85Â± 6.17 50.46Â±11.46 55.01Â±15.23
GraphPrompt+ 26.95Â± 7.42 20.80Â± 1.45 26.03Â± 4.17 25.31Â± 7.65 54.55Â±12.61 21.85Â± 5.15 53.26Â±14.99 54.73Â±14.58
ProNoG 31.54Â± 5.30 20.92Â± 1.37 28.50Â± 5.30 27.17Â± 9.58 56.11Â±10.19 22.55Â± 6.70 51.62Â±14.27 56.46Â±14.57
Table4:Ablationstudyontheeffectsofkeycomponents.
Nodeclassification Graphclassification
Methods
Wisconsin Squirrel Chameleon PROTEINS ENZYMES Citeseer Wisconsin Squirrel Chameleon PROTEINS ENZYMES COX2
NoPrompt 25.41Â±3.13 20.60Â±1.30 22.71Â±3.54 47.22Â±11.05 66.59Â±19.28 43.12Â±9.61 20.85Â±6.74 20.18Â±1.30 22.34Â±4.15 53.61Â±8.90 21.85Â±6.17 54.29Â±17.31
SinglePrompt 32.76Â±5.21 20.85Â±1.32 22.78Â±3.35 30.33Â±19.59 65.32Â±21.67 48.64Â±10.09 25.77Â±6.24 20.68Â±0.91 27.03Â±3.98 56.35 Â±10.59 19.38Â±7.12 47.24Â±15.53
NodeCond 35.56Â±4.65 21.26Â±3.95 21.13Â±2.23 36.01Â±19.70 68.54Â±19.31 48.30Â±10.22 25.30Â±4.62 20.98Â±1.56 27.24Â±5.24 56.61Â±10.03 20.70Â±6.67 55.92Â±14.66
ProNoG\sim 30.65Â±4.05 20.05Â±0.59 20.96Â±4.21 33.73Â±17.82 36.02Â±20.64 18.74Â±2.66 22.05Â±5.86 19.93Â±0.42 20.20Â±1.11 52.30Â±10.94 16.70Â±1.28 50.05Â±17.67
ProNoG 44.72 Â±11.93 24.59 Â±3.41 30.67 Â±3.73 48.95 Â±10.85 72.94 Â±20.23 49.02 Â±10.66 31.54 Â±5.30 20.92 Â±1.37 28.50 Â±5.30 56.11Â±10.19 22.55 Â±6.70 56.46 Â±14.57
6.3 AblationStudy
Wisconsin and Cornell, we only conduct tasks up to 3-shot. We
observethat:(1)ProNoGsignificantlyoutperformsallbaselines Tocomprehensivelyunderstandtheinfluenceofconditionalprompt-
inlow-shotscenarioswithverylimitedlabeleddata(e.g.,ğ‘˜ â‰¤5),
inginProNoG,weperformanablationstudycomparingProNoG
showcasingtheeffectivenessofourapproachinthesesituations. withfourofitsvariants:NoPromptreplacesconditionalprompting
(2)Asthenumberofshotsincreases,allmethodsgenerallyshow withaclassifierfordownstreamadaptation.SinglePromptusesa
improvedperformanceasexpected.However,ProNoGremains singlepromptinsteadofconditionalpromptingtomodifyallnodes.
competitiveandoftensurpassestheothermethods,demonstrating NodeConddirectlyusestheoutputembeddingofthepre-trained
therobustnessof ProNoG. graphencoderasinputtothecondition-nettogeneratetheprompt
withoutreadingoutthesubgraphinEq.7.ProNoG\simreadoutConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.,XingtongYuâˆ—,JieZhangâˆ—,YuanFangâ€ ,andRenheJiangâ€ 
Table5:Comparisonbetweenhomophilyandnon-homophilypre-training.
Nodeclassification Graphclassification
Pre-trainingtask Wisconsin Cornell PROTEINS ENZYMES Wisconsin Cornell PROTEINS ENZYMES
0.21 0.30 0.66 0.67 0.21 0.30 0.66 0.67
LinkPrediction[42] 23.01Â±11.40 26.27Â± 7.61 35.88Â± 5.41 36.74Â± 2.61 20.96Â± 4.21 25.38Â± 2.50 51.50Â± 6.02 17.47Â± 4.04
LinkPrediction[26] 28.93Â±11.74 16.29Â± 7.93 48.95Â±10.85 52.87Â±14.73 23.15Â± 5.67 22.05Â±13.80 55.83Â±10.87 22.23Â± 5.51
GraphACL[55] 33.91Â± 9.04 29.55Â±12.30 44.08Â±10.03 50.57Â±13.11 26.42Â± 7.25 26.15Â± 3.87 54.15Â±10.58 21.64Â± 5.88
GraphCL[60] 44.72Â±11.93 37.90Â± 9.31 48.28Â±11.09 51.46Â±13.93 31.54Â± 1.37 27.17Â± 5.30 53.91Â± 5.51 21.78Â±12.12
Wisconsin PROTEINS Wisconsin Cornell
Cora Citeseer
Squirrel ENZYMES
Figure4:Resultsondifferentnodepatterns.
linkpredictionusedinGraphPrompt[26],andnon-homophilytasks
GraphCL[60]andDSSL[54],respectively.Notethatlinkpredicition
inGPPT[42]isinagenerativeformat,thusfallingbeyondthe
scopeofhomophilytask,butitâ€™salsoaffectedbynon-homophily
ingraphs.Wecomparethesepretexttasksandshowtheresultsin
Table5.Weobservethatforgraphswithalowhomophilyratio,the
Chamelon Cora non-homophilytasksignificantlyoutperformsthehomophilytasks.
Conversely,forgraphswithahighhomophilyratio,theresultsof
Figure3:Impactsofdifferentshotsonnodeclassification. thesetwomethodsaremixed,witheachhavingtheirownstrengths
andweaknesses.
6.5 AnalysisonVaryingNodePatterns
thesubgraphviamean-poolingwithoutsimilaritybetweencen-
tralnodesandtheirneighborsasinEq.7.AsshowninTable4, Toevaluatetheabilityof ProNoGincapturingnode-specificpat-
ProNoGconsistentlyoutperformsorisatleastcompetitivewith terns,wecalculatetheaccuracyondifferentnodegroupswithvary-
thesevariants.Thishighlightsthenecessityofreadoutsubgraphs inghomophilyratios,i.e.,[0.0,0.2),[0.2,0.4),[0.4,0.6),[0.6,0.8),
weightedbysimilaritytocapturenodesâ€™non-homophilicpatterns, [0.8,1.0].WecompareProNoGwithseveralcompetitivebaselines
andtheadvantagesofusingconditionalpromptingtospecifically andpresenttheresultsinFig.4.WeobservethatProNoGcon-
characterizenodes. sistentlyoutperformsorisatleastcompetitivewithallbaselines
acrossallnodepatterns,regardlessoftheirhomophilyratio.These
6.4 AnalysisonPre-TrainingMethods
resultsfurtherdemonstratetheeffectivenessof ProNoGincap-
Tofurtherevaluatehomophilyandnon-homophilytasks,using turingnode-specificpatternsandhighlighttheadvantagesofour
ProNoGfordownstreamadaptation,weemployhomophilytasks proposedconditionalprompting.Non-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
7 Conclusions
Inthispaper,weexploredpre-trainingandpromptlearningon
non-homophilicgraphs.Theobjectivesaretwofold:learningcom-
prehensiveknowledgeirrespectiveofthevaryingnon-homophily
characteristicsofgraphs,andadaptingthenodeswithdiversedistri-
butionsofnon-homophilypatternstodownstreamapplicationsina
fine-grained,node-wisemanner.Wefirstrevisitgraphpre-training
onnon-homophilicgraphs,providingtheoreticalinsightsintothe
choiceofpre-trainingtasks.Then,fordownstreamadaptation,we
proposedcondition-nettogenerateaseriesofpromptsconditioned
onvariousnon-homophilicpatternsacrossnodes.Finally,wecon-
ductedextensiveexperimentsontenpublicdatasets,demonstrat-
ingthatProNoGsignificantlyoutperformsdiversestate-of-the-art
baselines.Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.,XingtongYuâˆ—,JieZhangâˆ—,YuanFangâ€ ,andRenheJiangâ€ 
References
[30] HaitaoMao,ZhikaiChen,WeiJin,HaoyuHan,YaoMa,TongZhao,NeilShah,and
[1] HangboBao,LiDong,SonghaoPiao,andFuruWei.2022. BEiT:BERTPre- JiliangTang.2023.Demystifyingstructuraldisparityingraphneuralnetworks:
TrainingofImageTransformers.InICLR. Canonesizefitall?.InNeurIPS.
[2] DeyuBo,XiaoWang,ChuanShi,andHuaweiShen.2021.Beyondlow-frequency [31] AndrewKachitesMcCallum,KamalNigam,JasonRennie,andKristieSeymore.
informationingraphconvolutionalnetworks.InAAAI.3950â€“3957. 2000.Automatingtheconstructionofinternetportalswithmachinelearning.
[3] KarstenMBorgwardt,ChengSoonOng,StefanSchÃ¶nauer,SVNVishwanathan, InformationRetrieval(2000).
AlexJSmola,andHans-PeterKriegel.2005.Proteinfunctionpredictionviagraph [32] Trung-KienNguyenandYuanFang.2024.Diffusion-basedNegativeSampling
kernels.Bioinformatics21,suppl_1(2005),i47â€“i56. onGraphsforLinkPrediction.InWWW.948â€“958.
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, [33] HongbinPei,BingzheWei,KevinChen-ChuanChang,YuLei,andBoYang.
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda 2020. Geom-gcn:Geometricgraphconvolutionalnetworks. arXivpreprint
Askell,etal.2020.Languagemodelsarefew-shotlearners.NeurIPS33(2020), arXiv:2002.05287(2020).
1877â€“1901. [34] ZhenPeng,WenbingHuang,MinnanLuo,QinghuaZheng,YuRong,Tingyang
[5] MouxiangChen,ZeminLiu,ChenghaoLiu,JundongLi,QihengMao,andJianling Xu,andJunzhouHuang.2020. Graphrepresentationlearningviagraphical
Sun.2023. Ultra-dp:Unifyinggraphpre-trainingwithmulti-taskgraphdual mutualinformationmaximization.InWWW.259â€“270.
prompt.arXivpreprintarXiv:2310.14845(2023). [35] OlegPlatonov,DenisKuznedelev,MichaelDiskin,ArtemBabenko,andLiudmila
[6] LiDong,NanYang,WenhuiWang,FuruWei,XiaodongLiu,YuWang,Jianfeng Prokhorenkova.2023.AcriticallookattheevaluationofGNNsunderheterophily:
Gao,MingZhou,andHsiao-WuenHon.2019. Unifiedlanguagemodelpre- Arewereallymakingprogress?.InICLR.
trainingfornaturallanguageunderstandingandgeneration.NeurIPS32(2019). [36] JiezhongQiu,QibinChen,YuxiaoDong,JingZhang,HongxiaYang,MingDing,
[7] TaoranFang,YunchaoZhang,YangYang,ChunpingWang,andLeiChen.2024. KuansanWang,andJieTang.2020.GCC:Graphcontrastivecodingforgraph
Universalprompttuningforgraphneuralnetworks.NeurIPS(2024). neuralnetworkpre-training.InSIGKDD.1150â€“1160.
[8] ChelseaFinn,PieterAbbeel,andSergeyLevine.2017. Model-agnosticmeta- [37] FaisalRahutomo,TeruakiKitasuka,MasayoshiAritsugi,etal.2012.Semantic
learningforfastadaptationofdeepnetworks.InICML.1126â€“1135. cosinesimilarity.InICAST.
[9] TianyuGao,AdamFisch,andDanqiChen.2021.MakingPre-trainedLanguage [38] RyanA.RossiandNesreenK.Ahmed.2015.TheNetworkDataRepositorywith
ModelsBetterFew-shotLearners.InACL.3816â€“3830. InteractiveGraphAnalyticsandVisualization.InAAAI.4292â€“4293.
[10] DavidHa,AndrewDai,andQuocVLe.2016.Hypernetworks.arXivpreprint [39] BenedekRozemberczki,CarlAllen,andRikSarkar.2021.Multi-scaleattributed
arXiv:1609.09106(2016). nodeembedding.JournalofComplexNetworks(2021),cnab014.
[11] WillHamilton,ZhitaoYing,andJureLeskovec.2017.Inductiverepresentation [40] TimoSchickandHinrichSchÃ¼tze.2021.Itâ€™sNotJustSizeThatMatters:Small
learningonlargegraphs.NeurIPS(2017),1025â€“1035. LanguageModelsAreAlsoFew-ShotLearners.InNAACL.2339â€“2352.
[12] KavehHassaniandAmirHoseinKhasahmadi.2020. Contrastivemulti-view [41] PrithvirajSen,GalileoNamata,MustafaBilgic,LiseGetoor,BrianGalligher,and
representationlearningongraphs.InICML.4116â€“4126. TinaEliassi-Rad.2008.Collectiveclassificationinnetworkdata.AImagazine
[13] DongxiaoHe,JitaoZhao,RuiGuo,ZhiyongFeng,DiJin,YuxiaoHuang,Zhen (2008).
Wang,andWeixiongZhang.2023.Contrastivelearningmeetshomophily:two [42] MingchenSun,KaixiongZhou,XinHe,YingWang,andXinWang.2022.GPPT:
birdswithonestone.InInternationalConferenceonMachineLearning.12775â€“ GraphPre-trainingandPromptTuningtoGeneralizeGraphNeuralNetworks.
12789. InSIGKDD.1717â€“1727.
[14] WeihuaHu,BowenLiu,JosephGomes,MarinkaZitnik,PercyLiang,VijayPande, [43] XiangguoSun,HongCheng,JiaLi,BoLiu,andJihongGuan.2023.AllinOne:
andJureLeskovec.2020.StrategiesforPre-trainingGraphNeuralNetworks.In Multi-TaskPromptingforGraphNeuralNetworks.InSIGKDD.
ICLR. [44] XiangguoSun,JiawenZhang,XixiWu,HongCheng,YunXiong,andJiaLi.2023.
[15] ZiniuHu,YuxiaoDong,KuansanWang,Kai-WeiChang,andYizhouSun.2020. Graphpromptlearning:Acomprehensivesurveyandbeyond. arXivpreprint
GPT-GNN:Generativepre-trainingofgraphneuralnetworks.InSIGKDD.1857â€“ arXiv:2311.16534(2023).
1867. [45] ShiyinTan,DongyuanLi,RenheJiang,YingZhang,andManabuOkumura.2024.
[16] ShuoJi,XiaodongLu,MingzheLiu,LeileiSun,ChuanrenLiu,BowenDu,and Community-InvariantGraphContrastiveLearning.InICML.
HuiXiong.2023. Community-baseddynamicgraphlearningforpopularity [46] ZhenTan,RuochengGuo,KaizeDing,andHuanLiu.2023.VirtualNodeTuning
prediction.InSIGKDD.930â€“940. forFew-shotNodeClassification.arXivpreprintarXiv:2306.06063(2023).
[17] WeiJin,TylerDerr,YiqiWang,YaoMa,ZitaoLiu,andJiliangTang.2021.Node [47] PetarVeliÄkoviÄ‡,GuillemCucurull,ArantxaCasanova,AdrianaRomero,Pietro
similaritypreservinggraphconvolutionalnetworks.InWSDM.148â€“156. Lio,andYoshuaBengio.2018.Graphattentionnetworks.InICLR.
[18] AnshulKanakia,ZhihongShen,DarrinEide,andKuansanWang.2019.Ascalable [48] PetarVelickovic,WilliamFedus,WilliamLHamilton,PietroLiÃ²,YoshuaBengio,
hybridresearchpaperrecommendersystemformicrosoftacademic.InWWW. andRDevonHjelm.2019.DeepGraphInfomax.InICLR.
2893â€“2899. [49] NingWang,MinnanLuo,KaizeDing,LinglingZhang,JundongLi,andQinghua
[19] ThomasNKipfandMaxWelling.2016. Variationalgraphauto-encoders.In Zheng.2020.Graphfew-shotlearningwithattributematching.InCIKM.1545â€“
BayesianDeepLearningWorkshop. 1554.
[20] ThomasNKipfandMaxWelling.2017.Semi-supervisedclassificationwithgraph [50] SongWang,YushunDong,XiaoHuang,ChenChen,andJundongLi.2022.FAITH:
convolutionalnetworks.InICLR. Few-ShotGraphClassificationwithHierarchicalTaskGraphs.InIJCAI.
[21] NamkyeongLee,KanghoonYoon,GyoungSNa,SeinKim,andChanyoungPark. [51] XuWang,HuanZhao,Wei-weiTu,andQuanmingYao.2023. Automated3D
2023. Shift-robustmolecularrelationallearningwithcausalsubstructure.In pre-trainingformolecularpropertyprediction.InSIGKDD.2419â€“2430.
SIGKDD.1200â€“1212. [52] YuzhongWuandTanLee.2018. ReducingmodelcomplexityforDNNbased
[22] BrianLester,RamiAl-Rfou,andNoahConstant.2021.ThePowerofScalefor large-scaleaudioclassification.InICASSP.331â€“335.
Parameter-EfficientPromptTuning.InEMNLP.3045â€“3059. [53] ZonghanWu,ShiruiPan,FengwenChen,GuodongLong,ChengqiZhang,and
[23] PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,and SYuPhilip.2020.Acomprehensivesurveyongraphneuralnetworks.TNNLS
GrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyof 32,1(2020),4â€“24.
promptingmethodsinnaturallanguageprocessing. Comput.Surveys(2023), [54] TengXiao,ZhengyuChen,ZhimengGuo,ZeyangZhuang,andSuhangWang.
1â€“35. 2022.Decoupledself-supervisedlearningforgraphs.NeurIPS(2022),620â€“634.
[24] XiaoLiu,YananZheng,ZhengxiaoDu,MingDing,YujieQian,ZhilinYang,and [55] TengXiao,HuaishengZhu,ZhengyuChen,andSuhangWang.2023. Simple
JieTang.2021.GPTunderstands,too.arXivpreprintarXiv:2103.10385(2021). andasymmetricgraphcontrastivelearningwithoutaugmentations.Advancesin
[25] ZeminLiu,YuanFang,ChenghaoLiu,andStevenCHHoi.2021.Relativeand NeuralInformationProcessingSystems(2023).
absolutelocationembeddingforfew-shotnodeclassificationongraph.InAAAI. [56] ChenyanXiong,RussellPower,andJamieCallan.2017.Explicitsemanticranking
4267â€“4275. foracademicsearchviaknowledgegraphembedding.InWWW.1271â€“1279.
[26] ZeminLiu,XingtongYu,YuanFang,andXinmingZhang.2023.GraphPrompt: [57] KeyuluXu,WeihuaHu,JureLeskovec,andStefanieJegelka.2019.Howpowerful
Unifyingpre-traininganddownstreamtasksforgraphneuralnetworks.InWWW. aregraphneuralnetworks?.InICLR.
417â€“428. [58] YujunYan,MiladHashemi,KevinSwersky,YaoqingYang,andDanaiKoutra.
[27] YuanfuLu,XunqiangJiang,YuanFang,andChuanShi.2021.Learningtopre-train 2022. Twosidesofthesamecoin:Heterophilyandoversmoothingingraph
graphneuralnetworks.InAAAI.4276â€“4284. convolutionalneuralnetworks.InICDM.1287â€“1292.
[28] SitaoLuan,ChenqingHua,QinchengLu,JiaqiZhu,MingdeZhao,Shuyuan [59] ChengxuanYing,TianleCai,ShengjieLuo,ShuxinZheng,GuolinKe,DiHe,
Zhang,Xiao-WenChang,andDoinaPrecup.2022. Revisitingheterophilyfor YanmingShen,andTie-YanLiu.2021.Dotransformersreallyperformbadlyfor
graphneuralnetworks.Advancesinneuralinformationprocessingsystems(2022), graphrepresentation?.InNeurIPS.28877â€“28888.
1362â€“1375. [60] YuningYou,TianlongChen,YongduoSui,TingChen,ZhangyangWang,and
[29] YaoMa,XiaoruiLiu,NeilShah,andJiliangTang.2022.Ishomophilyanecessity YangShen.2020.Graphcontrastivelearningwithaugmentations.NeurIPS33
forgraphneuralnetworks?.InICLR. (2020),5812â€“5823.Non-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
[61] XingtongYu,YuanFang,ZeminLiu,YuxiaWu,ZhihaoWen,JianyuanBo,Xin-
mingZhang,andStevenCHHoi.2024. Few-ShotLearningonGraphs:from
Meta-learningtoPre-trainingandPrompting.arXivpreprintarXiv:2402.01440
(2024).
[62] XingtongYu,ZhenghaoLiu,YuanFang,ZeminLiu,SihongChen,andXinming
Zhang.2024.Generalizedgraphprompt:Towardaunificationofpre-training
anddownstreamtasksongraphs. IEEETransactionsonKnowledgeandData
Engineering(2024).
[63] XingtongYu,ZeminLiu,YuanFang,andXinmingZhang.2023. Learningto
countisomorphismswithgraphneuralnetworks.InAAAI.
[64] XingtongYu,ZeminLiu,YuanFang,andXinmingZhang.2024.HGPROMPT:
BridgingHomogeneousandHeterogeneousGraphsforFew-shotPromptLearn-
ing.InAAAI.
[65] XingtongYu,ChangZhou,YuanFang,andXinmingZhang.2024.MultiGPrompt
forMulti-TaskPre-TrainingandPromptingonGraphs.InWWW.
[66] SeongjunYun,MinbyulJeong,RaehyunKim,JaewooKang,andHyunwooJKim.
2019.Graphtransformernetworks.NeurIPS32(2019).
[67] YuhangZang,WeiLi,KaiyangZhou,ChenHuang,andChenChangeLoy.2022.
Unifiedvisionandlanguagepromptlearning.arXivpreprintarXiv:2210.07225
(2022).
[68] ShiqiZhang,YiqianHuang,JiachenSun,WenqingLin,XiaokuiXiao,andBo
Tang.2023.Capacityconstrainedinfluencemaximizationinsocialnetworks.In
SIGKDD.3376â€“3385.
[69] FanZhou,ChengtaiCao,KunpengZhang,GoceTrajcevski,TingZhong,andJi
Geng.2019.Meta-GNN:Onfew-shotnodeclassificationingraphmeta-learning.
InCIKM.2357â€“2360.
[70] JieZhou,GanquCui,ShengdingHu,ZhengyanZhang,ChengYang,ZhiyuanLiu,
LifengWang,ChangchengLi,andMaosongSun.2020.Graphneuralnetworks:
Areviewofmethodsandapplications.AIopen(2020),57â€“81.
[71] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiweiLiu.2022.Conditional
promptlearningforvision-languagemodels.InCVPR.16816â€“16825.
[72] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiweiLiu.2022.Learning
topromptforvision-languagemodels.IJCV(2022),2337â€“2348.
[73] JiongZhu,JunchenJin,DonaldLoveland,MichaelTSchaub,andDanaiKoutra.
2022.Howdoesheterophilyimpacttherobustnessofgraphneuralnetworks?
theoreticalconnectionsandpracticalimplications.InSIGKDD.2637â€“2647.
[74] JiongZhu,RyanARossi,AnupRao,TungMai,NedimLipka,NesreenKAhmed,
andDanaiKoutra.2021. Graphneuralnetworkswithheterophily.InAAAI.
11168â€“11176.
[75] JiongZhu,YujunYan,LingxiaoZhao,MarkHeimann,LemanAkoglu,andDanai
Koutra.2020.Beyondhomophilyingraphneuralnetworks:Currentlimitations
andeffectivedesigns.NeurIPS(2020),7793â€“7804.
[76] YanqiaoZhu,YichenXu,FengYu,QiangLiu,ShuWu,andLiangWang.2020.
Deepgraphcontrastiverepresentationlearning.arXivpreprintarXiv:2006.04131
(2020).Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.,XingtongYuâˆ—,JieZhangâˆ—,YuanFangâ€ ,andRenheJiangâ€ 
Appendices
sites.Inthisdataset,eachnoderepresentsasecondarystruc-
A Algorithm ture,andeachedgesignifiesaneighboringrelationshipei-
therwithintheaminoacidsequenceorinthree-dimensional
Wedetailthemainstepsforconditionalpromptgenerationandtun-
space.Nodesareclassifiedintothreecategories,whilethe
inginAlgorithm1.Inbrief,weiteratethrougheachdownstream
graphsthemselvesaredividedintotwoclasses.Theedge
task to learn the corresponding prompt vectors individually. In
homophilyratiois0.66.
lines3â€“5,wecomputetheembeddingforeachnodeusingthepre- â€¢ ENZYMES8[50]isacollectionof600enzymes,sourcedfrom
trainedgraphencoder,withthepre-trainedweightsÎ˜ 0remaining
theBRENDAenzymedatabase.Theenzymesaredivided
fixedthroughouttheadaptationprocess.Inlines8â€“22,weoptimize
into6differentclasses,followingtheirtop-levelECenzyme
thecondition-net.Specifically,weperformtsimilarity-weighted
classification.Theedgehomophilyratiois0.67.
readout(lines9â€“11),generateprompts(lines12â€“13),modifynodesâ€™ â€¢ Citeseer9[41]contains3,312scientificpapers,dividedintosix
embeddingsusingtheseprompts(lines12â€“15),andupdatetheem-
differentcategories.Thedatasetincludesacitationnetwork
beddingsfortheprototypicalnodes/graphsbasedonthefew-shot
with 4,732 edges. Each paper is represented by a binary
labeleddataprovidedinthetask(lines18â€“19).Notethatupdating
wordvector,indicatingthepresenceorabsenceofeachword
prototypicalnodes/graphsisnecessaryonlyforclassificationtasks.
fromadictionarycomprising3,703uniqueterms.Theedge
homophilyratiois0.74.
B HomophilyandNon-HomophilyMethods
â€¢ Cora10 [31] includes 2,708 scientific papers, divided into
WeprovidefurtherdetailsaboutthesetofpositivesamplesAand sevendistinctcategories.Thedatasetfeaturesacitationnet-
negativesamplesBofhomophilyandnon-homophilymethodsin workwith5,429edges.Eachpaperisrepresentedbyabinary
Table5. wordvector,indicatingwhethereachofthe1,433unique
wordsfromthedictionaryispresentorabsent.Theedge
C FurtherDescriptionsofDatasets
homophilyratiois0.81.
Weconductexperimentsontenbenchmarkdatasets. â€¢ BZR11[38]comprisesadatasetof405ligandslinkedtothe
â€¢ Wisconsin3[33]isanetworkof251nodes,whereeachnode benzodiazepinereceptor,representedasindividualgraph
structures. These ligands are divided into 2 distinct cate-
standsforawebpage,and199edgessignifythehyperlinks
gories.
connectingthesepages.Thefeaturesofthenodesarederived
â€¢ COX212[38]consistsofadatasetof467molecularstructures
fromabag-of-wordsrepresentationofthewebpages.These
representing cyclooxygenase-2 inhibitors. In this dataset,
pagesaremanuallyclassifiedintofivecategories:student,
eachnodecorrespondstoanatomandeachedgedenotesa
project,course,staff,andfaculty.Theedgehomophilyratio
chemicalbondâ€”single,double,triple,oraromaticâ€”between
is0.21.
â€¢ Cornell4 [33]isalsoawebpagenetwork.Itcomprises183 atoms.Themoleculesareclassifiedintotwocategories.
nodes,eachsymbolizingawebpage,and295edges,which
D FurtherDescriptionsofBaselines
represent the hyperlinks between these pages. The node
featuresareobtainedfromabag-of-wordsrepresentation Inthissection,wepresentmoredetailsforthebaselinesusedin
ofthewebpages.Thesepagesaremanuallysortedintofive ourexperiments.
categories:student,project,course,staff,andfaculty.The (1)End-to-endGraphNeuralNetworks
edgehomophilyratiois0.22. â€¢ GCN[20]:GCNutilizesamean-poolingstrategyforneigh-
â€¢ Chameleon5[39]isaWikipedianetwork,consistingof2,277 borhoodaggregationtointegrateinformationfromneigh-
Wikipediapages.Thepagesaredividedintofivecategories boringnodes.
accordingtotheiraveragemonthlytraffic.Thisdatasetcre- â€¢ GAT[47]:GATalsoleveragesneighborhoodaggregationfor
atesanetworkofpageswith36,101connections,andthe end-to-endnoderepresentationlearning,uniquelyassigns
nodefeaturesconsistofvariouskeynounsextractedfrom varyingattentionweightstodifferentneighbors,thereby
theWikipediapages.Theedgehomophilyratiois0.23. adjustingtheirimpactontheaggregationprocess.
â€¢ Squirrel6[39]comprises5,201Wikipediawebpagesofdis- â€¢ H2GCN[75]:H2GCNimprovesnodeclassificationbysepa-
cussingthedefinedtopics.Thedatasetisalsodividedinto ratingego-andneighbor-embeddings,usinghigher-order
fivecategoriesaccordingtotheiraveragemonthlytraffic. neighborhoods,andcombiningintermediaterepresentations.
Thisdatasetisapage-pagenetworkwith217,073edges,and Thesedesignshelpitperformwellonbothhomophilousand
thenodefeaturesarebasedonseveralinformativenounsin heterophilousgraphs.
theWikipediapages.Theedgehomophilyratiois0.30. â€¢ FAGCN[2]:FAGCNimprovesnoderepresentationbyadap-
â€¢ PROTEINS7[3]comprisesadatasetofproteingraphs,reflect- tivelycombininglow-andhigh-frequencysignalsusinga
ingvariouscharacteristicssuchasaminoacidsequences, self-gatingmechanism,makingiteffectivefordifferentnet-
conformations,structures,anduniquefeatureslikeactive worktypesandreducingover-smoothing.
3https://github.com/bingzhewei/geom-gcn/tree/master/new_data/wisconsin 8http://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip
4https://github.com/bingzhewei/geom-gcn/tree/master/new_data/cornell 9https://nrvis.com/download/data/labeled/citeseer.zip
5https://github.com/SitaoLuan/ACM-GNN/tree/main/new_data/chameleon 10https://relational.fit.cvut.cz/dataset/CORA
6https://github.com/SitaoLuan/ACM-GNN/tree/main/new_data/squirrel 11https://www.chrsmrrs.com/graphkerneldatasets/BZR.zip
7https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip 12https://www.chrsmrrs.com/graphkerneldatasets/COX2.zipNon-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
Table6:Positiveandnegativesamplesforhomophilyandnon-homophilymethods.
Pre-trainingtask PositiveinstancesAğ‘¢ NegativeinstancesBğ‘¢ Homophilytask
Linkprediction[26,62,64] anodeconnectedtonodeğ‘¢ nodesdisconnectedtonodeğ‘¢ Yes
DGI[48] nodesingraphğº nodesincorruptedgraphğºâ€² No
GraphCL[60] anaugmentedgraphfromgraphğº augmentedgraphsfromğºâ€²â‰ ğº No
GraphACL[55] nodeswithsimilarego-subgraphtonodeğ‘¢ nodeswithdissimilarego-subgraphtonodeğ‘¢ No
Algorithm1ConditionalPromptLearningforProNoG nodebyenforcingidentityrepresentationsfromtwo-hop
Input: Pre-trainedgraphencoderwithparametersÎ˜ 0, neighbors.
1: asetofdownstreamtasksT={ğ‘¡ 1,...,ğ‘¡ğ‘›}. (3)GraphPromptModels
O 2u :tp fou rt: ğ‘–â†Opt 1im toiz ğ‘›ed doparameters{ğœ™ğ‘¡ 1,...,ğœ™ğ‘¡ğ‘›}ofğ‘›condition-nets
â€¢ GPPT[42]:GPPTutilizesaGNNmodelpre-trainedviaa
3: /*Encodinggraphsviapre-trainedgraphencoder*/ linkpredictiontaskwhichisastronghomophilymethod.
4: foreachgraphğº=(ğ‘‰,ğ¸,X)intaskğ‘¡ğ‘–do The downstream prompt module is designed specifically
5: Hâ†GraphEncoder(ğº;Î˜ 0) fornodeclassification,aligningitwiththepre-traininglink
6: hğ‘£ â†H[ğ‘£],whereğ‘£isanodeinğº predictiontask.
7: ğœ™ğ‘– â†initialization â€¢ GraphPrompt[26]:GraphPromptemployssubgraphsim-
8: whilenotconvergeddo ilaritycalculationsasaunifiedtemplatetobridgethegap
9: foreachnodeğ‘£ âˆˆğ‘‰ intaskğ‘¡ğ‘–do
betweenpre-traininganddownstreamtasks,includingnode
10: /*SubgraphsamplingandreadoutEq.(7)*/
andgraphclassification.Alearnablepromptisfine-tuned
11: Sampleğ‘£â€™sğ‘˜-hopsubgraphğ‘†ğ‘£
12: sğ‘£ â†Average({hğ‘¢Â·sim(hğ‘¢,hğ‘£):ğ‘¢ âˆˆğ‘‰(ğ‘†ğ‘£)}) duringdownstreamadaptationtoincorporatetask-specific
13: /*Generatepattern-basedpromptsbyEq.(8)*/ knowledge.
14: pğ‘¡ğ‘–,ğ‘£ â†CondNet(sğ‘£;ğœ™ğ‘¡ğ‘–) â€¢ GraphPrompt+[62]:GraphPrompt+buildsonGraphPrompt
15: /*PromptmodificationbyEq.(9)*/ byintroducingaseriesofpromptvectorswithineachlayer
16: hËœ ğ‘¡ğ‘–,ğ‘£ â†pğ‘¡ğ‘–,ğ‘£âŠ™hğ‘£ ofthepre-trainedgraphencoder.Thistechniqueutilizeshi-
17: hğ‘¡ğ‘–,ğº =Average(hËœ ğ‘¡ğ‘–,ğ‘£ :ğ‘£ âˆˆ V) erarchicalinformationfrommultiplelayers,beyondjustthe
18: /*Updateprototypicalsubgraphs*/ readoutlayer.
19: foreachclassğ‘intaskğ‘¡ğ‘–do
20: hÂ¯ ğ‘¡ğ‘–,ğ‘ â†Average(hËœ ğ‘¡ğ‘–,ğ‘¥:instanceğ‘¥belongstoclassğ‘) E ImplementationDetailsofApproaches
21: /*Optimizingtheparametersincondition-net*/
22: CalculateL down(ğœ™ğ‘–)byEq.(10) GeneralsettingsOptimizer.Forallexperiments,weusetheAdam
23: Updateğœ™ğ‘–bybackpropagatingL down(ğœ™ğ‘¡ğ‘–) optimizer.
24: return{ğœ™ğ‘¡ 1,...,ğœ™ğ‘¡ğ‘›}
Environment.Theenvironmentinwhichwerunexperimentsis:
â€“ Linuxversion:5.15.0-78-generic
â€“ Operatingsystem:Ubuntu18.04.5LTS
(2)GraphPre-trainingModels
â€“ CPUinformation:Intel(R)Xeon(R)Platinum8352V
â€¢ DGI [47]: DGI operates as a self-supervised pre-training â€“ GPUinformation:GeForceRTX4090(24GB)
methodologytailoredforhomogeneousgraphs.Itispredi-
Detailsofbaselines.Weusetheofficialcodeprovidedforallopen-
catedonthemaximizationofmutualinformation(MI),aim-
sourcebaselines.Eachmodelistunedaccordingtothesettings
ingtoenhancetheestimatedMIbetweenlocallyaugmented
recommendedintheirrespectivepublicationstoensureoptimal
instancesandtheirglobalcounterparts.
â€¢ GraphCL[60]:GraphCLleveragesavarietyofgraphaug- performance.Weuseearlystoppingstrategyfortrainingandset
patienceto50steps.Thenumberoftrainingepochsissetto2,000.
mentations for self-supervised learning, tapping into the
intrinsicstructuralpatternsofgraphs.Theoverarchinggoal â€¢ ForthebaselineGCN[20],weemploya3-layerarchitecture
istoamplifytheconcordancebetweendifferentaugmenta- onWisconsin,Squirrel,Chameleon,Cornelldatasetsand2-
tionsthroughoutgraphpre-training. layerarchitectureonCora,Citeseer,ENZYMES,PROTEINS,
â€¢ DSSL[54]:DSLLuseslatentvariablemodelingtodecouple COX2,BZRdatasets.Hiddendimensionsis256.
semanticsinneighborhoods,avoidingaugmentationsand â€¢ ForGAT[47],weemploya2-layerarchitectureandsetthe
optimizingwithvariationalinferencetocapturelocaland hiddendimensionto256.Additionally,weapply8attention
globalinformation,enhancingnoderepresentationlearning. headsinthefirstGATlayer.
â€¢ GraphACL[55]:GraphACLconsiderseachnodefromtwo â€¢ ForH2GCN[75],weemploya2-layerarchitectureandset
perspectives:identityrepresentationandcontextrepresenta- thehiddendimensionto256.
tion.Themodeltrainstheformerbypredictingthecontext â€¢ ForFAGCN[2],weemploya2-layerarchitecture.Thehyper-
representationofone-hopneighborsusinganasymmetric parametersettingis:eps=0.3,dropout=0.5,hidden=256.
predictor,andthenreconstructsthesamelatterofthecentral weusereluasactivationfunction.Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.,XingtongYuâˆ—,JieZhangâˆ—,YuanFangâ€ ,andRenheJiangâ€ 
Node classification Table7:Comparisonofthenumberoftunableparameters
duringthedownstreamadaptationphase.
Methods Wisconsin Chameleon Citeseer Cora
GCN 501,504 660,736 947,968 366,848
FAGCN 440,654 601,130 956,654 370,994
GraphCL 1,280 1,280 1,536 1,792
GraphACL 1,280 1,280 12,288 14,336
GraphPrompt 256 256 256 256
GraphPrompt+ 512 512 512 512
Cornell Citeseer ProNoG 1,024 1,024 1,024 1,024
Graph classification
keeptheotherhyper-parametersthesameasintheoriginal
demonstrationsintheirGithubrepository.
â€¢ ForGPPT[42],weutilizea2-layerGraphSAGEasitsbase
model,settingthehiddendimensionsto256.ForbaseGraph-
SAGE,wealsoemployameanaggregator.
â€¢ For GraphPrompt [26], we employ a 3-layer architecture
onWisconsin,Squirrel,Chameleon,Cornelldatasetsand2-
layerarchitectureonCora,Citeseer,ENZYMES,PROTEINS,
Wisconsin Cornell COX2,BZRdatasets.Hiddendimensionsaresetto256.
â€¢ ForGraphPrompt+[62],weemploya2-layerGCNonCora,
Citeseer,ENZYMES,PROTEINS,COX2,BZRdatasetsand
3-layerGCNontherestdatasets.Hiddendimensionsareset
to256.
DetailsofProNoG.ForourproposedProNoG,weutilizea2-layer
FAGCNarchitectureasbackboneforpre-trainingtaskwithgraph
contrastivemethodsforWisconsin,Squirrel,Chameleon,Cornell.
Especially,weimplementedge-droppingonsub-graphlevelfor
Wisconsin,Squirrel,Chameleon,Cornell.Hiddendimensionsare
BZR COX2 setto256.ForCora,Citeseer,BZR,COX2,weemploy1-layerGCN
asbasemodelforpre-trainingtask.Hiddendimensionsaresetto
256.ForPROTEINS,weemploy1-layerGCNonlinkpredictiontask
Figure5:Impactsofdifferentshotsonnodeandgraphclassi- forpre-training.Hiddendimensionsissetto64.ForENZYMES,
fication. weimplementDSSLforpretraining.Hiddendimensionsissetto
64.Allexperimentsareundertakenwiththeseedof39.Especially,
wefoundthatonChameleon,Squirrel,keepingtheoriginalnode
featuresasinputwithoutnormalizationperformsthebest,whilefor
â€¢ ForDGI[47],weutilizea1-layerGCNasthebasemodeland others,normalizationofnodefeaturesremainsroutine.Exceptfor
setthehiddendimensionsto256.Additionally,weemploy DSSL,weusecosine-similaritylossonnodelevelaslossfunction.
preluastheactivationfunction.
â€¢ ForGraphCL[60],a1-layerGCNisalsoemployedasitsbase F ImpactofShots
model,withthehiddendimensionssetto256.Specifically,
Wevarythenumberofshotsandconductnodeclassificationon
weselectedgedroppingastheaugmentations,withadefault
CornellandCiteseer,andgraphclassificationtasksonWisconsin,
augmentationratioof0.2.
Cornell,BZRandCOX2.TheresultsareillustratedinFig.5,and
â€¢ ForDSSL[54],thehiddendimensionsearchspaceisin{64,
weobservesamepatternsasshowninnodeclassificationtaskson
256,2048}.WereportthebestperformanceonPROTEINS
otherdatasets.
andENZYMESwithhiddensizeof64,CoraandCiteseer
with2048,andtherestdatasetswith256.Wekeeptheother
G Parametersefficiency
hyper-parametersthesameasintheoriginaldemonstrations
intheirGithubrepository. We evaluate the parameter efficiency of ProNoG compared to
â€¢ ForGraphACL[55],thehiddendimensionsearchspaceis other notable methods. Specifically, we evaluate the number of
in{64,256,1024,2048}.Wereportthebestperformanceon parameters that need to be updated or tuned during the down-
PROTEINS and ENZYMES with hidden size of 64 , Cora streamadaptationphase,andpresenttheresultsinTable7.For
andCiteseerwith2048,andtherestdatasetswith256.We GCNandFAGCN,sincethesemodelsaretrainedend-to-end,allNon-HomophilicGraphPre-TrainingandPromptLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
Node classification Graph classification
Figure6:Sensitivitystudyofğ‘š.
model weights must be updated, leading to the lowest parame-
terefficiency.Incontrast,forGraphCLandxxx,onlythedown-
streamclassifierisupdated,whilethepre-trainedmodelweights
remainunchanged,significantlyreducingthenumberofparam-
etersthatrequiretuning.Prompt-basedmethodsGraphPrompt,
GraphPrompt+,andProNoGarethemostparameter-efficient,as
promptsorcondition-netarelightweightandcontainfewerparam-
etersthantypicalclassifierslikefullyconnectedlayers.Notethat
thereportednumberofparametersforProNoGarebasedonğ‘‘ =2,
giventhatProNoGstillperformscompetitivelywithsuchhyperpa-
rametersetting.Althoughourconditionalpromptdesignrequires
toupdatemoreparametersthanGraphPromptandGraphPrompt+
duringdownstreamadaptation,theincreaseisminorcomparedto
updatingtheentireclassifierormodelweights,andthusdoesnot
poseamajorissue.
H HyperparameterAnalysis
Inourexperiment,weusea2-layerMLPwithabottleneckstruc-
tureasthecondition-net.Weevaluatetheimpactofthehidden
dimensionofthecondition-netğ‘š andreportthecorresponding
performanceinFig.6.Weobservethatforbothnodeandgraph
classification,asğ‘šincreasesfrom2,theperformancegenerallyfirst
decreasesbecausealargerğ‘šintroducesmorelearnableparameters,
whichmayleadtoworseperformanceinfew-shotsettings.How-
ever,afterreachingatrough,accuracystartstograduallyincrease
asğ‘šgrowsfurther,sincehigherdimensionsincreasemodelcapac-
ity,untilreachingapeak.Thentheperformancefurtherdeclines
asğ‘š improves,givenmorelearnableparameters.Notethatthe
overallvariationinperformanceisgenerallysmall,andthepeakis
generallyatğ‘š=2orğ‘š=64.Inourexperiment,wesetğ‘š=64in
ourexperiments.