MotionBooth: Motion-Aware Customized
Text-to-Video Generation
JianzongWu1,3,XiangtaiLi2,3‚Ä†,YanhongZeng3,JiangningZhang4,QianyuZhou5,
YiningLi3,YunhaiTong1,KaiChen3
1PKU2S-Lab,NTU3ShanghaiAILaboratory4ZJU5SJTU
ProjectPage: https://jianzongwu.github.io/projects/motionbooth
Email: jzwu@stu.pku.edu.cn,xiangtai94@gmail.com
Subject Subject motion Camera motion Generated Videos
Down Left
A dog jumping down the stairs
A monster toy walking in Times Square
A cat jumping over the wall
Right
A cartoon running in a painting forest
Figure 1: Motion-aware customized video generation results of MotionBooth. Our method animates a
customizedobjectwithcontrollablesubjectandcameramotions.
Abstract
In this work, we present MotionBooth, an innovative framework designed for
animatingcustomizedsubjectswithprecisecontroloverbothobjectandcamera
movements. Byleveragingafewimagesofaspecificobject,weefficientlyfine-
tuneatext-to-videomodeltocapturetheobject‚Äôsshapeandattributesaccurately.
Ourapproachpresentssubjectregionlossandvideopreservationlosstoenhance
thesubject‚Äôslearningperformance,alongwithasubjecttokencross-attentionloss
tointegratethecustomizedsubjectwithmotioncontrolsignals. Additionally,we
proposetraining-freetechniquesformanagingsubjectandcameramotionsduring
inference. In particular, we utilize cross-attention map manipulation to govern
subjectmotionandintroduceanovellatentshiftmoduleforcameramovement
controlaswell. MotionBoothexcelsinpreservingtheappearanceofsubjectswhile
simultaneouslycontrollingthemotionsingeneratedvideos. Extensivequantitative
andqualitativeevaluationsdemonstratethesuperiorityandeffectivenessofour
method. Modelsandcodeswillbemadepubliclyavailable.
WorkdonewhenJianzongisaninternatShanghaiAILaboratory.‚Ä†:ProjectLead.
Preprint.Underreview.
4202
nuJ
52
]VC.sc[
1v85771.6042:viXra1 Introduction
Generatingvideosforcustomizedsubjects,suchasspecificscenariosinvolvingaparticulardog‚Äôstype
orappearance,hasgainedresearchattention[51,25,39]. Thiscustomizedgenerationfieldoriginated
fromtext-to-image(T2I)generationmethods,whichlearnasubject‚Äôsappearancefromafewimages
andgeneratediverseimagesofthatsubject[12,39,29]. Followingthem,subject-driventext-to-video
(T2V) generation has seen increasing interest, which has found a wide range of applications in
personalshortsorfilmproduction[51,25,39,53,13]. Canyouimagineyourtoyridingalongthe
roadfromadistancetothecameraoryourpetdogdancingonthestreetfromthelefttotheright?
However, renderingsuchlovelyimaginaryvideosisachallengingtask. Itofteninvolvessubject
learningandmotioninjectionwhilemaintainingthegenerativecapabilitytogeneratediversescenes.
Notably,VideoBooth[25]trainsanimageencodertoembedthesubject‚Äôsappearanceintothemodel,
generatingashortclipofthesubject.However,thegeneratedvideosoftendisplayminimalormissing
motion,resemblinga"movingimage."Thisapproachunderutilizesthemotiondiversityofpre-trained
T2Vmodels. Anotherlineofworks[53,57,13]fine-tunesthecustomizedmodelonspecificvideos,
requiringmotionlearningforeachspecificcameraorsubjectmotiontype. Theirpipelinesrestrictthe
typeofmotionandrequirefine-tuninganewadapterforeachmotiontype,whichisinconvenientand
computationallyexpensive.
Thekeyliesintheconflictbetweensubjectlearningandvideomotionpreservation. Duringsubject
learning, training on limited images of the specific subject significantly shifts the distribution of
thebaseT2Vmodel,leadingtosignificantdegradation(e.g.,blurredbackgroundsandstaticvideo).
Therefore,existingmethodsoftenneedadditionalmotionlearningforspecificmotioncontrol. Inthis
paper,wearguethatthebaseT2Vmodelalreadyhasdiversemotionprior,andthekeyistopreserve
videocapabilityduringsubjectlearninganddiggingoutthemotioncontrolduringinference.
Toensuresubject-drivenvideogenerationwithuniversalandprecisemotioncontrol,wepresentMo-
tionBooth,whichcanperformmotion-awarecustomizedvideogeneration. Thevideosgenerated
byMotionBoothareillustratedin Fig.1. MotionBoothcantakeanycombinationofsubject,subject
motion,andcameramotionasinputsandgeneratediversevideos,maintainingqualityonparwith
pre-trainedT2Vmodels.
MotionBoothlearnssubjectswithouthurtingvideogenerationcapability,enablingatraining-free
motioninjectionforsubject-drivenvideogeneration. First,duringsubjectlearning,weintroduce
subjectregionlossandvideopreservationloss,whichenhancebothsubjectfidelityandvideoquality.
Inaddition,wepresentasubjecttokencross-attentionlosstoconnectthecustomizedsubjectwith
motioncontrolsignals. Duringinference,weproposetraining-freetechniquestocontrolthecamera
andsubjectmotion. Wedirectlymanipulatethecross-attentionmapstocontrolthesubjectmotion.
Wealsoproposeanovellatentshiftmoduletogovernthecameramovement. Itshiftsthenoised
latenttomovethecamerapose. Throughquantitativeandqualitativeexperiments,wedemonstrate
thesuperiorityandeffectivenessoftheproposedmotioncontrolmethods,andtheycanbeappliedto
differentbaseT2Vmodelswithoutfurthertuning.
Ourcontributionsaresummarizedasfollows: 1)Weproposeaunifiedframework,MotionBooth,
for motion-aware customized video generation. To our knowledge, this is the first framework
capableofgeneratingdiversevideosbycombiningcustomizedsubjects,subjectmotions,andcamera
movementsasinput. 2)Weproposeanovelloss-augmentedtrainingarchitectureforsubjectlearning.
This includes subject region loss, video preservation loss, and subject token cross-attention loss,
significantlyenhancingsubjectfidelityandvideoquality. 3)Wedevelopinnovative,training-free
methods for controlling subject and camera motions. Extensive experiments demonstrate that
MotionBoothoutperformsexistingstate-of-the-artvideogenerationmodels.
2 RelatedWork
Text-to-videogeneration. T2Vgenerationleveragesdeeplearningmodelstointerprettextinput
and generate corresponding video content. It builds upon earlier breakthroughs in text-to-image
generation[41,38,19,21,34,45,56,58]butintroducesmorecomplexdynamicsbyincorporating
motionandtime[43,20,18,2,63,55]. Recentadvancementsparticularlyleveragediffusion-based
architectures. NotablemodelssuchasModelScopeT2V[48]andLaVie[50]integratetemporallayers
withinspatialframeworks. VideoCrafter1[6]andVideoCrafter2[7]addressthescarcityofvideo
2Training Inference
Video Preservation Loss ‚Ñíùë£ùëñùëë
‚ÄúA [V] dog‚Äù Camera Control ‚ÄúA [V] dog running on the beach‚Äù Extra Inputs
Camera Direction
Bounding Box
Sequence
Preservation Video ¬∑ ¬∑
¬∑.nttA fleS.nttA
ssorC¬∑ ¬∑
¬∑.nttA fleS.nttA
ssorC¬∑ ¬∑ ¬∑ Image/Video Outputs ùúé1S ~h ùúéif 2ted Latent ¬∑ ¬∑
¬∑.nttA fleS.nttA
ssorC¬∑ ¬∑
¬∑.nttA fleS.nttA
ssorC¬∑ ¬∑ ¬∑
Latent Shift
Generated Video
Input Image
DC ia rem ce tir oa
n
ùíÑùëêùëéùëö
Extract cross-attention maps ùêÄ at [V] dog Subject Motion Control <start> a [V] dog ¬∑ ¬∑ ¬∑ <end>
Subject Region Loss No No
Cross-ASu ttb ej nec tit
o
T no Lk oe sn
s ‚Ñíùë†ùë°ùëêùëé
‚Ñíùë†ùë¢ùëè Change Change
Trainable amplify suppress
Subject Mask Cross Attn. Map Step t Latent Bounding Box Sequence Edit Cross-Attention Maps
Figure2: TheoverallpipelineofMotionBooth.Wefirstfine-tuneaT2Vmodelonthesubject.Thisprocedure
incorporatessubjectregionloss,videopreservationloss,andsubjecttokencross-attentionloss.Duringinference,
we control the camera movement with a novel latent shift module. At the same time, we manipulate the
cross-attentionmapstogovernthesubjectmotion.
databyutilizinghigh-qualityimagedatasets. Latte[33]andW.A.L.T[14]adoptTransformersas
backbones[46]. VideoPoet[28]exploresgeneratingvideosautoregressivelytoproduceconsistent
longvideos. RecentSora[3]excelsingeneratingvideoswithimpressivequality,stableconsistency,
andvariedmotion. Despitetheseadvancements,controllingvideocontentthroughtextaloneremains
challenging,highlightingacontinuingneedforresearchintomorerefinedcontrolsignals.
Customized generation. Generating images and videos with customized subjects is attracting
growinginterest. Mostworksconcentrateonlearningaspecificsubjectwithafewimagesfromthe
samesubject[23,8,10,40,44,36]orspecificdomains[15,16,47]. TextualInversion[12]proposes
totrainanewwordtocapturethefeatureofanobject. Incontrast,DreamBooth[39]fine-tunesthe
wholeU-Net,resultinginabetterIPpreservationability. Followingthem,manyworksexploremore
challengingtasks,suchascustomizingmultipleobjects[29,51,32,5],developingcommonsubject
adapter[54,25,60,11,62],andsimultaneouslycontrollingtheirpositions[11,32]. However,the
customizationofvideomodelsfromafewimagesoftenresultsinoverfitting. Themodelsfailto
incorporatesignificantmotiondynamics. Arecentwork,DreamVideo[53],addressesthisbylearning
specificmotiontypesfromvideodata. Yet,thismethodisrestrictedtopre-definedmotiontypesand
lackstheflexibilityoftext-driveninput. Incontrast,ourworkintroducesMotionBoothtocontrol
boththesubjectandcameramotionswithoutneedingpre-definedmotionprototypes.
Motion-awarevideogeneration. Recentworksexploreincorporatingexplicitmotioncontrolin
video generation. This includes camera and object motions. To control camera motion, existing
works like AnimateDiff [13], VideoComposer [49], CameraCtrl [17], Direct-A-Video [61], and
MotionCtrl [52] design specific modules to encode the camera movement or trajectory. These
modelsusuallyrelyontrainingonlarge-scaledatasets[1,9],leadingtohighcomputationalcosts.
Incontrast,ourMotionBoothframeworkbuildsatraining-freecameramotionmodulethatcanbe
easilyintegratedwithanyT2Vmodel,eliminatingtheneedforre-training. Forobjectmotioncontrol,
recentworks[59,30,31,24,61,4,64,26,22]proposeeffectivemethodstomanipulateattention
valuesduringtheinferencestage. Inspiredbytheseapproaches,weconnectsubjecttexttokenstothe
subjectpositionusingasubjecttokencross-attentionloss. Thisallowsforstraightforwardcontrol
overthemotionofacustomizedobjectbyadjustingcross-attentionvalues.
3 Method
3.1 Overview
Taskformulation. Wefocusongeneratingmotion-awarevideosfeaturedbyacustomizedsubject.
Tocustomizevideosubjects,wefine-tunetheT2Vmodelonaspecificsubject. Thisprocesscan
beaccomplishedwithjustafew(typically3-5)imagesofthesamesubject. Duringinference,the
fine-tuned model generates motion-aware videos of the subject. The motion encompasses both
cameraandsubjectmovements,whicharefreelydefinedbytheuser. Forcameramotion,theuser
3inputsthehorizontalandverticalcameramovementratios,denotedasc =[c ,c ]. Forsubject
cam x y
motion,theuserprovidesaboundingboxsequence[B ,B ,...,B ]toindicatethedesiredpositions
1 2 L
ofthesubject,whereLrepresentsthevideolength. Eachboundingboxspecifiesthex-ycoordinates
ofthetop-leftandbottom-rightpointsforeachframe. Byincorporatingtheseconditionalinputs,the
modelisexpectedtogeneratevideosthatincludeaspecificsubject,alongwithpredefinedcamera
movementsandsubjectmotions.
Overallpipeline. TheoverallpipelineofMotionBoothisillustratedinFig.2. Duringthetraining
stage,MotionBoothlearnstheappearanceofthegivensubjectbyfine-tuningtheT2Vmodel. To
prevent overfitting, we introduce video preservation loss and subject region loss in Section 3.2.
Additionally, weproposeasubjecttokencross-attention(STCA)lossinSection3.2toexplicitly
connectthesubjecttokenswiththesubject‚Äôspositiononcross-attentionmaps,facilitatingthecontrol
ofsubjectmotion. Cameraandsubjectmotioncontrolareperformedduringtheinferencestage. We
manipulatethecross-attentionmapsbyamplifyingthesubjecttokensandtheircorrespondingregions
whilesuppressingothertokensinSection3.3. Thisensuresthatthegeneratedsubjectsappearin
thedesiredpositions. Bytrainingonthecross-attentionmap,theSTCAlossenhancesthesubjects‚Äô
motioncontrol. Forcameramovement,weintroduceanovellatentshiftmoduletoshiftthenoised
latentdirectly,achievingsmoothcameramovementinthegeneratedvideosinSection3.4.
3.2 SubjectLearning
Givenafewimagesofasubject,previousworks
a dog running on the grass a [V] dog running on the grass
havedemonstratedthatfine-tuningadiffusion
modelontheseimagescaneffectivelylearnthe
appearanceofthesubject[39,23,8,10,40,44].
However, two significant challenges remain.
(a)Pre-trained text-to-video (c) Region Video (e) Region Video
First,duetothelimitedsizeofthedataset,the
modelquicklyoverfitstheinputimages,includ-
ingtheirbackgrounds,withinafewsteps. This
overfittingofthebackgroundimpedesthegen-
erationofvideoswithdiversescenes,aproblem (b) Subject (d) Region Video (f) Region Video
alsonotedinpreviousworks[39,12]. Second, Figure3: Casestudyonsubjectlearning. ‚ÄúRegion‚Äù
fine-tuningT2Vmodelsusingimagescanimpair indicates subject region loss. ‚ÄúVideo‚Äù indicates
themodel‚Äôsinherentabilitytogeneratevideos, videopreservationloss. Theimagesareextracted
leadingtoseverebackgrounddegradationinthe fromgeneratedvideos.
generatedvideos. Toillustratetheseissues,we
conductedatoyexperiment. AsdepictedinFig.3,withoutanymodifications,themodeloverfitsthe
backgroundtothesubjectimage. Toaddressthis,weproposecomputingthediffusionreconstruction
losssolelywithinthesubjectregion. However, evenwiththisadjustment, thebackgroundinthe
generatedvideosremainsover-smoothed. ThisdegradationlikelyresultsfromtuningaT2Vmodel
exclusively with images, which damages the model‚Äôs original weights for video generation. To
mitigatethis,weproposeincorporatingvideodataaspreservationdataduringthetrainingprocess.
Althoughtrainingwithvideodatabutwithoutsubjectregionlossstillsuffersfromoverfitting,our
approach,MotionBooth,cangeneratevideoswithdetailedanddiversebackgrounds. (xiangtai: hard
tounderstand.)
Preliminary. T2V diffusion models learn to generate videos by reconstructing noise in a latent
space[39,29,51,12]. Theinputvideoisfirstencodedintoalatentrepresentationz . Noiseœµis
0
addedtothislatentrepresentation,resultinginanoisedlatentz ,wheretrepresentsthetimestamp.
t
Thisprocesssimulatesthereverseprocessofafixed-lengthMarkovChain[38]. Thediffusionmodel
œµ istrainedtopredictthisnoise. Thetrainingloss,whichisareconstructionloss,isgivenby:
Œ∏
L=E (cid:2) ||œµ‚àíœµ (z ,c,t)||2(cid:3) , (1)
z,œµ‚àºN(0,I),t,c Œ∏ t 2
where c is the conditional input used in classifier-free guidance methods, which can be text or a
referenceimage. Duringinference,apurenoisez isgraduallydenoisedtoacleanlatentz‚Ä≤,where
T 0
T is the length of the Markov Chain. The clean latent is then decoded back into RGB space to
generatethevideoX‚Ä≤.
4Subjectregionloss. Toaddressthechallengeofoverfittingbackgroundsintrainingimages, we
proposeasubjectregionloss.Thecoreideaistocalculatethediffusionreconstructionlossexclusively
withinthesubjectregion,therebypreventingthemodelfromlearningthebackground. Specifically,
wefirstextractthesubjectmaskforeachimage. Thiscanbedonemanuallyorthroughautomatic
methods,suchasasegmentationmodel. Inpractice,weuseSAM[27]tocollectallthemasks. The
subjectregionlossisthencalculatedasfollows:
L =E (cid:2) ||(œµ‚àíœµ (z ,c ,t))¬∑M||2(cid:3) , (2)
sub z,œµ‚àºN(0,I),t,c Œ∏ t i 2
whereMrepresentsthebinarymasksforthetrainingimages. Thesemasksareresizedtothelatent
spacetocomputethedotproduct. c isafixedsentenceintheformat"a[V][classname],"where
i
"[V]"isararetokenand"[classname]"istheclassnameofthesubject[39]. Wehavefoundthat
withthesubjectregionloss,thetrainedmodeleffectivelyavoidsthebackgroundoverfittingproblem.
Videopreservationloss. ImagecustomizationdatasetslikeDreamBooth[39]andCustomDiffu-
sion[29]provideexcellentexamplesofmultipleimagesfromthesamesubject. However,inthe
customizedvideogenerationtask,directlyfine-tuningthevideodiffusionmodelonimagesleads
tosignificantbackgrounddegradation. Intuitively,thisimage-basedtrainingprocessmayharmthe
original knowledge embedded in video diffusion models. To address this, we introduce a video
preservationlossdesignedtomaintainvideogenerationknowledgebyjointtrainingwithvideodata.
Unliketheclass-specificpreservationdatausedinpreviousworks[39,51],weutilizecommonvideos
withcaptionsdenotedasc . OurexperimentsinSection4demonstratethatcommonvideosare
v
moreeffectiveforsubjectlearningandpreservingvideogenerationcapabilities. Thelossfunctionis
formulatedasfollows:
L =E (cid:2) ||œµ‚àíœµ (z ,c ,t)||2(cid:3) . (3)
vid z,œµ‚àºN(0,I),t,c Œ∏ t v 2
Subjecttokencross-attentionloss. Tocontrolthesub-
ject‚Äôsmotion,wedirectlymanipulatethecross-attention
mapsduringinference. Sinceweintroduceauniquetoken,
‚Äú[V]‚Äù,inthetrainingstageandassociateitwiththesubject,
weneedtolinkthisspecialtokentothesubject‚Äôsposition
withinthecross-attentionmaps. AsillustratedinFig.4,
fine-tuning the model does not effectively connect the
uniquetokentothecross-attentionmaps. Therefore,we
proposeaSubjectTokenCross-Attention(STCA)lossto
guidethisprocessexplicitly. First,weextractthecross-
(a) Input Images (b) w/o STCA loss (c) w/ STCA loss
attentionmap,A,atthetokens‚Äú[V][classname]‚Äù. We
then apply a Binary Cross-Entropy Loss to ensure that Figure 4: Case study on subject token
thecorrespondingattentionmapislargeratthesubject‚Äôs cross-attentionmaps. (b)and(c)arevi-
position and smaller outside this region. This process sualization of cross-attention maps on
incorporatesthesubjectmaskandcanbeexpressedas: tokens‚Äú[V]‚Äùand‚Äúdog‚Äù.
L =‚àí[Mlog(A)+(1‚àíM)log(1‚àíA)]. (4)
stca
Duringtraining,theoveralllossfunctionisdefinedas:
L=L +Œª L +Œª L , (5)
sub 1 vid 2 stca
whereŒª andŒª arehyperparametersthatcontroltheweightsofthedifferentlosscomponents.
1 2
3.3 SubjectMotionControl
Wechoseboundingboxesasthemotioncontrolsignalforsubjectsbecausetheyareeasytodraw
andmanipulate. Incontrast,providingobjectmasksforeveryframeislabor-intensive,requiring
consideration of the subject‚Äôs shape transformation between frames. In practice, we find that
bounding boxes are sufficient for precisely controlling the positions of subjects. Previous works
likeGLIGEN[30]attempttocontrolobjectpositionsbytraininganextraconditionmodulewith
5large-scaleimagedata. However,thesetrainingmethodsfixthemodelsandcannoteasilyalignwith
customizedmodelsfine-tunedforspecificsubjects. Therefore,weadoptanalternativeapproachthat
directlyeditsthecross-attentionmapsduringinferenceinatraining-freemanner[61,26,4]. This
cross-attentioneditingmethodisplug-and-playandcanbeusedwithanycustomizedmodel.
Incross-attentionlayers,thequeryfeaturesQareextractedfromthevideolatentandrepresentthe
visionfeatures. ThekeyandvaluefeaturesKandVarederivedfrominputlanguagetokens. The
calculationprocessoftheeditedcross-attentionlayercanbeformulatedasfollows:
(cid:18) QK‚ä§ (cid:19)
EditedCrossAttn(Q,K,V)=Softmax ‚àö +Œ±S V, (6)
d
wheredisthefeaturedimensionofQandservesasanormalizationterm. Q ‚àöK‚ä§ isthenormalized
d
productionbetweenQandK,representingtheattentionscoresbetweenvisionandlanguagefeatures.
WemanipulatetheproductionbyaddinganewtermŒ±S,whereShaspositivevaluesonthesubject
regionprovidedinboundingboxesandlargenegativevaluesoutsidethedesiredpositions. Œ±isa
hyperparametertocontroltheeditingstrength. TheeditingmatrixSissetasfollows:
Ô£±
Ô£¥Ô£≤1‚àí | |B Qk ||, ifi‚ààB
k
andj ‚ààPandt‚â•œÑ
S k[i,j]= 0, ifi‚ààB
k
andj ‚ààPandt<œÑ (7)
Ô£¥Ô£≥‚àí‚àû,
otherwise
where i, j, and k indicate the vision token, language token, and frame indexes, respectively. P
representstheindexesforsubjectlanguagetokensinthetextprompt. Inthiswork,wechoose‚Äú[V]‚Äù
and ‚Äú[class name]‚Äù as subject tokens. The SCTA loss in Section 3.2 binds the two tokens with
cross-attentionmaps. tisthedenoisingtimestampandœÑ isahyperparameterdefiningatimestamp
threshold. Sincediffusionmodelstendtoformtheapproximateobjectlayoutinearlierdenoising
stepsandrefinethedetailsinlatersteps[59],weapplystrongerattentionamplificationinearliersteps
andnoamplificationinlatersteps. Notethatattentionsuppressionoutsidetheboundingboxregions
persiststhroughoutthegeneration. |B |and|Q|aretheareasoftheboxandquery, respectively.
k
Followingpreviousworks[26,61],smallerboxesshouldhavelargeramplifications,andwedonot
applyanyeditingonthe<start>and<end>tokens.
3.4 CameraMovementControl
Simplyeditingthecross-attentionmapcaneffi- ‚ÄúA waterfall in a beautiful forest with fall foliage‚Äù Denoising ùúé1‚àíùúé2steps
cientlycontrolthemotionofthesubject. This
suggests that the latent can be considered a
"shrunkimage,"whichmaintainsthesamevi-
sualgeographicdistributionasthegeneratedim-
ages. Forcameramovementcontrol,anintuitive Noised Latent ùëßùúé1 Step 1: Shift Step 3: Fill in the Missing Part
approach is to directly shift the noised latent
duringinferencebasedonthecameramovement InputCamera
Movement
signalc cam =[c x,c y]. Thelatentshiftpipeline x-axis y-axis
isillustratedinTable2. Thekeychallengewith Shifted Noised Latent ùëßùúé2 Step 2: Sample Tokens
thisideaisfillinginthemissingpartscausedby
Figure 5: Illustration of camera movement control
thelatentshift(thequestionmarkregioninStep throughshiftingthenoisedlatent.
1). Toaddressthisissue,weproposesampling
tokens from the original noised latent and using them to fill the gap. This is based on the prior
knowledgethatwhenacameramovesinavideo,thenewsceneitcapturesissemanticallycloseto
thepreviousone. Forexample,inavideowithforestscenes,whenthecamerapansleft,itishighly
likelytocapturemoretreessimilartothoseintheoriginalscene. Anotherassumptionisthatina
normallyangledvideo,avisualelementismorelikelytobesemanticallyclosetoelementsalongthe
samex-axisory-axisratherthanotherelements. Forinstance,inthewaterfallvideoinFig.5,trees
areatthetopandbottom,spreadinghorizontally,whilethewaterfallspansthemiddlex-axisarea.
Experimentally,weoverthatsamplingtokenshorizontallyandverticallyprovidesbetterinitialization
andresultsinsmoothervideotransitions. Randomlysamplingtokensdegradesthegeneratedvideo
quality. Thelatentshiftprocessfortimestamptcanbeformulatedasfollows:
6h =SampleHorizontal(z ,B,c ),
x t x
h =SampleVertical(z ,B,c ),
y t y
(8)
z =Crop(Shift(z ,c ,c )),
shift t x y
z =Fill(z ,h ,h ,c ,c ),
t shift x y x y
whereh andh aresampledtokensalongthexandyaxes,respectively. Crop(¬∑)removesthetokens
x y
outside the camera view after the shift. B is the subject bounding box. We filter out the tokens
belongingtothesubjectsbecausetheyarenotlikelytooccurinthenewscenes. Inaddition,toavoid
adrasticchangeinlatentinoneshift,wespreadthelatentshiftovermultipletimestamps,witheach
steponlyshiftingasmallnumberoftokens. Notethatthelatentshiftneedstobeappliedafterthe
subject‚Äôsapproximatelayoutisfixedbutbeforethevideodetailsarecompleted. Wesetapairof
hyperparametersœÉ andœÉ . Thelatentshiftonlyappliesinthetimestamprange[œÉ ,œÉ ].
1 2 1 2
4 Experiments
4.1 ExperimentalSetup
Datasets. Forcustomization,wecollectatotalof26objectsfromDreamBooth[39]andCustomDif-
fusion[29]. Theseobjectsincludepets,plushies,toys,cartoons,andvehicles. Toevaluatecameraand
objectmotioncontrol,webuiltadatasetcontaining40text-objectmotionpairsand40text-camera
motionpairs,ensuringthatthecameraandobjectmotionpatternsareconsistentwiththetextprompts.
Thisdatasetevaluatesthevideosgeneratedforeachsubjectinvariousscenariosandmotions.
Implementationdetails. WetrainMotionBoothfor300stepsusingtheAdamWoptimizer,with
alearningrateof5e-2andaweightdecayof1e-2. Wecollect500preservationvideosfromthe
Panda-70M[9]trainingset,chosenrandomly. Eachbatchconsistsofonebatchforimagesandone
forvideos, withbatchsizesequaltothenumberoftrainingimagesand1forimagesandvideos,
respectively. ThelossweightparametersŒª andŒª aresetto1.0and0.01. WeuseZeroscopeand
1 2
LaVieasbasemodels.Duringinference,weperform50-stepdenoisingusingtheDDIMschedulerand
settheclassifier-freeguidancescaleto7.5. Thegeneratedvideosare576x320x24and512x320x16
forZeroscopeandLaVie,respectively. Thetrainingprocessfinishesinaround10minutesinasingle
NVIDIAA10080GGPU.AdditionalimplementationdetailscanbefoundinAppendixA.3.
Baselines.Sincewearepioneeringmotion-awarecustomizedvideogeneration,wecompareourmeth-
odswithcloselyrelatedworks,includingDreamBooth[39],CustomVideo[51],andDreamVideo[53].
Dreambooth customizes subjects for text-to-image generation. We follow its practice with class
preservationimagesandfine-tuneT2Vmodelsforgeneratingvideos. CustomVideoisarecentvideo
customizingmethod. Weadoptitsparameter-efficienttrainingprocedure. DreamVideolearnsmotion
patternsfromvideodata. Toprovidesuchdata,wesamplevideosfromPanda-70M,whicharemost
relevanttotheevaluationmotions. Sincethesemethodscannotcontrolmotionsduringinference,
weapplyourcameraandobjectmotioncontroltechnologiesforafaircomparison. Additionally,
wecompareourcameracontrolmethodwithtraining-basedmethods,AnimateDiff[13]andCamer-
aCtrl[17],focusingoncameramotioncontrolwithoutsubjectcustomization. SinceAnimateDiff
istrainedwithonlybasiccameramovementtypesandcannottakeuser-definedcameramovement
c =[c ,c ]asinput,weusetheclosestbasicmovementtypeforevaluation.
cam x y
Evaluationmetrics. Weevaluatemotion-awarecustomizedvideogenerationfromthreeaspects:
regionsubjectfidelity,temporalconsistency,andcameramotionfidelity. 1)Toensurethesubjectis
well-preservedandaccuratelygeneratedinthespecifiedmotion,weintroduceregionCLIPsimilarity
(R-CLIP)andregionDINOsimilaritymetrics(R-DINO).ThesemetricsutilizetheCLIP[37]and
DINOv2 [35] models to compute the similarities between the subject images and frame regions
indicatedbyboundingboxes. Additionally,weuseCLIPimage-textsimilarity(CLIP-T)tomeasure
the similarity between entire frames and text prompts. 2) We evaluate temporal consistency by
computing CLIP image features between each consecutive frame. 3) We use VideoFlow [42] to
predicttheopticalflowofthegeneratedvideos. Then,wecalculatetheflowerrorbycomparingthe
predictedflowwiththeground-truthcameramotionprovidedintheevaluationdataset.
7Table1: Quantitativecomparisonformotion-awarecustomizedvideogeneration.
T2VModel Method R-CLIP‚Üë R-DINO‚Üë CLIP-T‚Üë T-Cons.‚Üë Flowerror‚Üì
DreamBooth[39] 0.608 0.279 0.231 0.951 0.690
CustomVideo[51] 0.657 0.267 0.245 0.955 0.516
Zeroscope
DreamVideo[53] 0.656 0.238 0.258 0.954 0.349
MotionBooth(Ours) 0.667 0.306 0.258 0.958 0.252
DreamBooth[39] 0.696 0.426 0.238 0.958 1.156
CustomVideo[51] 0.634 0.189 0.248 0.911 1.055
LaVie
DreamVideo[53] 0.649 0.216 0.243 0.925 0.691
MotionBooth(Ours) 0.712 0.472 0.247 0.962 0.332
Table2: Quantitativecomparisonforcameramovementcontrol.
Method ModuleWeightStorage CLIP-T‚Üë T-Cons.‚Üë Flowerror‚Üì
AnimateDiff[13] 74M 0.245 0.925 1.683
CameraCtrl[17] 2.5G 0.237 0.939 0.807
MotionBooth(Zeroscope) [NoTraining] 0.252 0.948 0.190
MotionBooth(LaVie) [NoTraining] 0.241 0.963 0.296
Subject
+ + Base Model + Base Model
Motion Zeroscope LaVie
DreamBooth
CustomVideo
DreamVideo
MotionBooth
(Ours)
a [V] cat jumping off the stairs a [V} toy riding a bike on the road with a view of mountains
Figure6: Qualitativecomparisonofcustomizingobjectsandcontrollingtheirmotions.
4.2 MainResults
Quantitativeresults. Weconductquantitativecomparisonswithbaselinemodelsonbothmotion-
awarecustomizedvideogenerationandcameramovementcontrol. Theresultsformotion-aware
customizedvideogenerationareshowninTable1. TheresultsdemonstratethatMotionBoothoutper-
formsallbaselinesonbothZeroscopeandLaViemodels,indicatingthatourproposedtechnologies
canbeextendedtodifferentT2Vmodels. Thankstothetraining-freearchitectureofsubjectandcam-
eramotioncontrolmethods,MotionBoothisexpectedtobeadaptabletomoreopen-sourcedmodels
inthefuture,suchasSora[3]. Notably,DreamVideo[53]achievesthesecond-bestscoresinT-Cons.
andflowerror,whichalignswithourobservationthatincorporatingvideodataasauxiliarytraining
dataenhancesvideogenerationperformance. Ontheotherhand,CustomVideo[51]showsinferior
performanceinR-DINOscores,indicatingapoorerabilitytogeneratesubjectsingivenpositions.
Thismaybeattributedtoitsapproachofonlyfine-tuningthetextembeddingsandcross-attention
layersofthediffusionmodels,whichisinsufficientforlearningthesubjects.
Forcameramovementcontrol,wecompareourmethodwithtwotraining-basedmethods,AnimateD-
iff[13]andCameraCtrl[17]. TheresultsareshowninTable2. Remarkably,MotionBoothachieves
superiorresultscomparedtothetwobaselineswithourtraining-freelatentshiftmodule. Specifically,
MotionBoothoutperformstherecentmethodCameraCtrlby0.617,0.015,and0.009inflowerror,
CLIP-T, and T-Cons. metrics with Zeroscope, and 0.511, 0.004, and 0.024 for the LaVie model.
Theseresultsdemonstratethatthelatentshiftmethodissimpleyeteffective.
8Up Right Up Down Left Right
A playful puppy frolicking in flowers A squirrel gathering acorns A waterfall in a beautiful forest clown fish swimming in a coral reef
with fall foliage
AnimateDiff CameraCtrl Ours (Zeroscope) AnimateDiff CameraCtrl Ours (Zeroscope) AnimateDiff CameraCtrl Ours (LaVie) AnimateDiff CameraCtrl Ours (LaVie)
Figure7: Qualitativecomparisonofcameramotioncontrol.Linesandpointsareusedtohelpthereaderstrack
thecameramovementmoreeasily.
Table3: Ablationstudyfortrainingtechnologies.‚Äúmask‚Äùmeanssubjectregionloss.‚ÄúSTCA‚Äùmeanssubject
tokencross-attentionloss.‚Äúvideo‚Äùmeansvideopreservationloss.‚Äúw/classvideo‚Äùmeansutilizingclass-specific
videosinvideopreservationloss.TheresultsareevaluatedonLaVie.
Method R-CLIP‚Üë R-DINO‚Üë CLIP-T‚Üë T-Cons.‚Üë Flowerror‚Üì
w/omask 0.673 0.216 0.245 0.943 0.451
w/oSTCA 0.710 0.453 0.244 0.962 0.363
w/ovideo 0.677 0.364 0.244 0.953 0.344
w/classvideo 0.677 0.364 0.244 0.953 0.345
MotionBooth 0.712 0.472 0.247 0.962 0.332
Qualitativeresults. Thequalitativecomparisonresultsforvideogenerationwithcustomizedobjects
andcontrolledsubjectmotionsarepresentedinFig.6. OurobservationsrevealthatMotionBooth
excelsinsubjectmotionalignment,textpromptalignment,andoverallvideoquality. Incontrast,
DreamBoothandCustomVideoproducevideoswithvaguebackgrounds,highlightingthatgenerated
backgroundsdeterioratewhentrainingisconductedwithoutvideodata. Additionally,CustomVideo
andDreamVideostruggletocapturethesubjects‚Äôappearances,likelybecausetheirapproachtunes
onlypartofthediffusionmodel,preventingthelearningprocessfromfullyconverging.
Wealsoconductqualitativeexperimentsfocusedoncameramovementcontrol,withresultsshown
inFig.7. AnimateDiff,limitedtobasicmovements,doesnotsupportuser-definedcameradirections.
AlthoughtheCameraCtrlmethodcanacceptuserinput,itgeneratesvideoswithsubparaesthetics
andobjectsthatexhibitflashmovements. Incontrast,ourMotionBoothmodeloutperformsboththe
ZeroscopeandLaviemodels.Theproposedlatentmethodgeneratesvideosthatadheretouser-defined
cameramovementswhilemaintainingtimeconsistencyandhighvideoquality.
4.3 AblationStudies
Training technologies. We analyze the technologies proposed during the subject learning stage.
TheablationresultsareshowninTable3. Clearly,withouttheproposedmodules,thequantitative
metricsdropaccordingly. Theseresultsdemonstratethattheproposedsubjectregionloss,STCA
loss, andvideopreservationlossarebeneficialforsubjectlearningandgeneratingmotion-aware
customizedvideos. Specifically,theR-DINOmetricdecreasessignificantlyby0.256withoutthe
subject region loss, highlighting its core contribution in filtering out image backgrounds during
training. Additionally, the "w/ class video" experiment, which uses class-specific videos instead
ofrandomlysampledcommonvideos,yieldsworseresults. Thisapproachrestrictsthescenesand
backgroundsinclass-specificvideos,hinderingthemodels‚Äôabilitytogeneralizeeffectively.
5 Conclusion
ThispaperintroducesMotionBooth,anovelframeworkformotion-aware,customizedvideogen-
eration. MotionBoothfine-tunesaT2Vdiffusionmodeltolearnspecificsubjects,utilizingsubject
region loss to focus on the subject area. The training procedure incorporates video preservation
datatopreventbackgrounddegradation. Additionally,anSTCAlossisdesignedtoconnectsubject
tokenswiththecross-attentionmap. Duringinference,training-freetechnologiesareproposedto
9controlbothsubjectandcameramotion. Extensiveexperimentsdemonstratetheeffectivenessand
generalizationabilityofourmethod. Inconclusion,MotionBoothcangeneratevividvideoswith
givensubjectsandcontrollablesubjectandcameramotions.
Acknowledgement. ThisworkissupportedbytheNationalKeyResearchandDevelopmentProgram
ofChina(No. 2023YFC3807600).
References
[1] MaxBain,ArshaNagrani,G√ºlVarol,andAndrewZisserman. Frozenintime:Ajointvideoandimage
encoderforend-to-endretrieval. InICCV,2021. 3
[2] AndreasBlattmann,RobinRombach,HuanLing,TimDockhorn,SeungWookKim,SanjaFidler,and
KarstenKreis. Alignyourlatents:High-resolutionvideosynthesiswithlatentdiffusionmodels. InCVPR,
2023. 2
[3] TimBrooks,BillPeebles,ConnorHolmes,YufeiGuoWillDePue,LiJing,DavidSchnurr,JoeTaylor,
TroyLuhman,EricLuhman,ClarenceNg,RickyWang,andAdityaRamesh. Sora:Creatingvideofrom
text,2024. 3,8
[4] Changgu Chen, Junwei Shu, Lianggangxu Chen, Gaoqi He, Changbo Wang, and Yang Li. Motion-
zero: Zero-shotmovingobjectcontrolframeworkfordiffusion-basedvideogeneration. arXivpreprint
arXiv:2401.10150,2024. 3,6
[5] Hong Chen, Xin Wang, Guanning Zeng, Yipeng Zhang, Yuwei Zhou, Feilin Han, and Wenwu Zhu.
Videodreamer: Customized multi-subject text-to-video generation with disen-mix finetuning. arXiv
preprintarXiv:2311.00990,2023. 3
[6] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing,
YaofangLiu,QifengChen,XintaoWang,etal. Videocrafter1: Opendiffusionmodelsforhigh-quality
videogeneration. arXivpreprintarXiv:2310.19512,2023. 2
[7] HaoxinChen,YongZhang,XiaodongCun,MenghanXia,XintaoWang,ChaoWeng,andYingShan.
Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint
arXiv:2401.09047,2024. 2
[8] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth:
Disentangled parameter-efficient tuning for subject-driven text-to-image generation. arXiv preprint
arXiv:2305.03374,2023. 3,4
[9] Tsai-ShienChen,AliaksandrSiarohin,WilliMenapace,EkaterinaDeyneka,Hsiang-weiChao,ByungEun
Jeon,YuweiFang,Hsin-YingLee,JianRen,Ming-HsuanYang,etal. Panda-70m:Captioning70mvideos
withmultiplecross-modalityteachers. arXivpreprintarXiv:2402.19479,2024. 3,7
[10] WenhuChen,HexiangHu,YandongLi,NatanielRuiz,XuhuiJia,Ming-WeiChang,andWilliamWCohen.
Subject-driventext-to-imagegenerationviaapprenticeshiplearning. NeurIPS,2024. 3,4
[11] XiChen,LianghuaHuang,YuLiu,YujunShen,DeliZhao,andHengshuangZhao. Anydoor:Zero-shot
object-levelimagecustomization. CVPR,2024. 3
[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel
Cohen-Or. Animageisworthoneword:Personalizingtext-to-imagegenerationusingtextualinversion.
arXivpreprintarXiv:2208.01618,2022. 2,3,4
[13] YuweiGuo,CeyuanYang,AnyiRao,ZhengyangLiang,YaohuiWang,YuQiao,ManeeshAgrawala,
DahuaLin,andBoDai. Animatediff:Animateyourpersonalizedtext-to-imagediffusionmodelswithout
specifictuning. InICLR,2024. 2,3,7,8
[14] AgrimGupta,LijunYu,KihyukSohn,XiuyeGu,MeeraHahn,LiFei-Fei,IrfanEssa,LuJiang,andJos√©
Lezama. Photorealisticvideogenerationwithdiffusionmodels. arXivpreprintarXiv:2312.06662,2023. 3
[15] YueHan,JiangningZhang,JunweiZhu,XiangtaiLi,YanhaoGe,WeiLi,ChengjieWang,YongLiu,
XiaomingLiu,andYingTai. Ageneralistfacexvialearningunifiedfacialrepresentation. arXivpreprint
arXiv:2401.00551,2023. 3
[16] YueHan,JunweiZhu,KekeHe,XuChen,YanhaoGe,WeiLi,XiangtaiLi,JiangningZhang,Chengjie
Wang,andYongLiu. Faceadapterforpre-traineddiffusionmodelswithfine-grainedidandattribute
control. arXivpreprintarXiv:2405.12970,2024. 3
[17] HaoHe,YinghaoXu,YuweiGuo,GordonWetzstein,BoDai,HongshengLi,andCeyuanYang.Cameractrl:
Enablingcameracontrolfortext-to-videogeneration. arXivpreprintarXiv:2404.02101,2024. 3,7,8
[18] JonathanHo, WilliamChan, ChitwanSaharia, JayWhang, RuiqiGao, AlexeyGritsenko, DiederikP
Kingma, BenPoole, MohammadNorouzi, DavidJFleet, etal. Imagenvideo: Highdefinitionvideo
generationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022. 2
[19] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InNeurIPS,2020. 2
[20] JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,MohammadNorouzi,andDavidJFleet.
Videodiffusionmodels. InNeurIPS,2022. 2
[21] EmielHoogeboom,JonathanHeek,andTimSalimans. simplediffusion:End-to-enddiffusionforhigh
resolutionimages. InICML,2023. 2
10[22] Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, and
LizhuangMa. Motionmaster:Training-freecameramotiontransferforvideogeneration. arXivpreprint
arXiv:2404.15789,2024. 3
[23] MiaoHua,JiaweiLiu,FeiDing,WeiLiu,JieWu,andQianHe. Dreamtuner:Singleimageisenoughfor
subject-drivengeneration. arXivpreprintarXiv:2312.13691,2023. 3,4
[24] YashJain,AnshulNasery,VibhavVineet,andHarkiratBehl. Peekaboo:Interactivevideogenerationvia
masked-diffusion. arXivpreprintarXiv:2312.07509,2023. 3
[25] YumingJiang,TianxingWu,ShuaiYang,ChenyangSi,DahuaLin,YuQiao,ChenChangeLoy,andZiwei
Liu. Videobooth:Diffusion-basedvideogenerationwithimageprompts. InCVPR,2024. 2,3
[26] YunjiKim,JiyoungLee,Jin-HwaKim,Jung-WooHa,andJun-YanZhu. Densetext-to-imagegeneration
withattentionmodulation. InICCV,2023. 3,6
[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao, SpencerWhitehead, AlexanderCBerg, Wan-YenLo, etal. Segmentanything. arXivpreprint
arXiv:2304.02643,2023. 5
[28] DanKondratyuk,LijunYu,XiuyeGu,Jos√©Lezama,JonathanHuang,RachelHornung,HartwigAdam,
HassanAkbari,YairAlon,VighneshBirodkar,etal. Videopoet: Alargelanguagemodelforzero-shot
videogeneration. arXivpreprintarXiv:2312.14125,2023. 3
[29] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept
customizationoftext-to-imagediffusion. InCVPR,2023. 2,3,4,5,7,14
[30] YuhengLi,HaotianLiu,QingyangWu,FangzhouMu,JianweiYang,JianfengGao,ChunyuanLi,and
YongJaeLee. Gligen:Open-setgroundedtext-to-imagegeneration. InCVPR,2023. 3,5
[31] HanLin,AbhayZala,JaeminCho,andMohitBansal. Videodirectorgpt: Consistentmulti-scenevideo
generationviallm-guidedplanning. arXivpreprintarXiv:2309.15091,2023. 3
[32] ZhihengLiu,YifeiZhang,YujunShen,KechengZheng,KaiZhu,RuiliFeng,YuLiu,DeliZhao,Jingren
Zhou,andYangCao. Cones2: Customizableimagesynthesiswithmultiplesubjects. arXivpreprint
arXiv:2305.19327,2023. 3
[33] XinMa,YaohuiWang,GengyunJia,XinyuanChen,ZiweiLiu,Yuan-FangLi,CunjianChen,andYuQiao.
Latte:Latentdiffusiontransformerforvideogeneration. arXivpreprintarXiv:2401.03048,2024. 3
[34] AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilisticmodels. In
ICML,2021. 2
[35] MaximeOquab,Timoth√©eDarcet,Th√©oMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,Pierre
Fernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2:Learningrobustvisual
featureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023. 7
[36] XuPeng,JunweiZhu,BoyuanJiang,YingTai,DonghaoLuo,JiangningZhang,WeiLin,TaisongJin,
ChengjieWang,andRongrongJi. Portraitbooth: Aversatileportraitmodelforfastidentity-preserved
personalization. arXivpreprintarXiv:2312.06354,2023. 3
[37] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InICCV,2021. 7
[38] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InCVPR,2022. 2,4
[39] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman. Dream-
booth:Finetuningtext-to-imagediffusionmodelsforsubject-drivengeneration. InCVPR,2023. 2,3,4,5,
7,8,14
[40] NatanielRuiz,YuanzhenLi,VarunJampani,WeiWei,TingboHou,YaelPritch,NealWadhwa,Michael
Rubinstein,andKfirAberman.Hyperdreambooth:Hypernetworksforfastpersonalizationoftext-to-image
models. arXivpreprintarXiv:2307.06949,2023. 3,4
[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistictext-to-
imagediffusionmodelswithdeeplanguageunderstanding. NeurIPS,2022. 2
[42] XiaoyuShi,ZhaoyangHuang,WeikangBian,DasongLi,ManyuanZhang,KaChunCheung,SimonSee,
HongweiQin,JifengDai,andHongshengLi. Videoflow:Exploitingtemporalcuesformulti-frameoptical
flowestimation. InICCV,2023. 7
[43] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn,SongyangZhang,QiyuanHu,HarryYang,
OronAshual,OranGafni,etal. Make-a-video:Text-to-videogenerationwithouttext-videodata. arXiv
preprintarXiv:2209.14792,2022. 2
[44] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and Hongxia
Jin. Continualdiffusion:Continualcustomizationoftext-to-imagediffusionwithc-lora. arXivpreprint
arXiv:2304.06027,2023. 3,4
[45] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. arXivpreprint
arXiv:2010.02502,2020. 2
[46] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,≈Åukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. NeurIPS,2017. 3
11[47] ChaoyangWang,XiangtaiLi,LuQi,HenghuiDing,YunhaiTong,andMing-HsuanYang. Semflow:
Bindingsemanticsegmentationandimagesynthesisviarectifiedflow. arXivpreprintarXiv:2405.20282,
2024. 3
[48] JiuniuWang,HangjieYuan,DayouChen,YingyaZhang,XiangWang,andShiweiZhang. Modelscope
text-to-videotechnicalreport. arXivpreprintarXiv:2308.06571,2023. 2
[49] XiangWang,HangjieYuan,ShiweiZhang,DayouChen,JiuniuWang,YingyaZhang,YujunShen,Deli
Zhao,andJingrenZhou. Videocomposer: Compositionalvideosynthesiswithmotioncontrollability.
NeurIPS,2024. 3
[50] YaohuiWang,XinyuanChen,XinMa,ShangchenZhou,ZiqiHuang,YiWang,CeyuanYang,Yinan
He,JiashuoYu,PeiqingYang,etal. Lavie:High-qualityvideogenerationwithcascadedlatentdiffusion
models. arXivpreprintarXiv:2309.15103,2023. 2
[51] ZhaoWang, AoxueLi, EnzeXie,LingtingZhu,YongGuo,QiDou, andZhenguoLi. Customvideo:
Customizingtext-to-videogenerationwithmultiplesubjects. arXivpreprintarXiv:2401.09962,2024. 2,3,
4,5,7,8
[52] ZhouxiaWang,ZiyangYuan,XintaoWang,TianshuiChen,MenghanXia,PingLuo,andYingShan.
Motionctrl:Aunifiedandflexiblemotioncontrollerforvideogeneration.arXivpreprintarXiv:2312.03641,
2023. 3
[53] YujieWei,ShiweiZhang,ZhiwuQing,HangjieYuan,ZhihengLiu,YuLiu,YingyaZhang,JingrenZhou,
andHongmingShan. Dreamvideo:Composingyourdreamvideoswithcustomizedsubjectandmotion.
arXivpreprintarXiv:2312.04433,2023. 2,3,7,8
[54] YuxiangWei,YaboZhang,ZhilongJi,JinfengBai,LeiZhang,andWangmengZuo.Elite:Encodingvisual
conceptsintotextualembeddingsforcustomizedtext-to-imagegeneration. InICCV,2023. 3
[55] JianzongWu,XiangtaiLi,ChenyangSi,ShangchenZhou,JingkangYang,JiangningZhang,YiningLi,
KaiChen,YunhaiTong,ZiweiLiu,etal. Towardslanguage-drivenvideoinpaintingviamultimodallarge
languagemodels. CVPR,2024. 2
[56] JianzongWu,XiangtaiLi,ShilinXu,HaoboYuan,HenghuiDing,YiboYang,XiaLi,JiangningZhang,
YunhaiTong,XudongJiang,BernardGhanem,andDachengTao. Towardsopenvocabularylearning:A
survey. T-PAMI,2024. 2
[57] JayZhangjieWu,YixiaoGe,XintaoWang,StanWeixianLei,YuchaoGu,YufeiShi,WynneHsu,Ying
Shan,XiaohuQie,andMikeZhengShou. Tune-a-video:One-shottuningofimagediffusionmodelsfor
text-to-videogeneration. InICCV,2023. 2
[58] JiahaoXie,WeiLi,XiangtaiLi,ZiweiLiu,YewSoonOng,andChenChangeLoy.Mosaicfusion:Diffusion
modelsasdataaugmentersforlargevocabularyinstancesegmentation. arXivpreprintarXiv:2309.13042,
2023. 2
[59] JinhengXie,YuexiangLi,YawenHuang,HaozheLiu,WentianZhang,YefengZheng,andMikeZheng
Shou. Boxdiff:Text-to-imagesynthesiswithtraining-freebox-constraineddiffusion. InICCV,2023. 3,6
[60] BinxinYang,ShuyangGu,BoZhang,TingZhang,XuejinChen,XiaoyanSun,DongChen,andFangWen.
Paintbyexample:Exemplar-basedimageeditingwithdiffusionmodels. InCVPR,2023. 3
[61] ShiyuanYang,LiangHou,HaibinHuang,ChongyangMa,PengfeiWan,DiZhang,XiaodongChen,and
JingLiao. Direct-a-video:Customizedvideogenerationwithuser-directedcameramovementandobject
motion. arXivpreprintarXiv:2402.03162,2024. 3,6
[62] HaoyuZhao,TianyiLu,JiaxiGu,XingZhang,ZuxuanWu,HangXu,andYu-GangJiang. Videoassem-
bler: Identity-consistentvideogenerationwithreferenceentitiesusingdiffusionmodel. arXivpreprint
arXiv:2311.17338,2023. 3
[63] DaquanZhou,WeiminWang,HanshuYan,WeiweiLv,YizheZhu,andJiashiFeng. Magicvideo:Efficient
videogenerationwithlatentdiffusionmodels. arXivpreprintarXiv:2211.11018,2022. 2
[64] YupengZhou,DaquanZhou,Ming-MingCheng,JiashiFeng,andQibinHou. Storydiffusion:Consistent
self-attentionforlong-rangeimageandvideogeneration. arXivpreprintarXiv:2405.01434,2024. 3
12A Appendix
Overview. Thesupplementaryincludesthefollowingsections:
‚Ä¢ AppendixA.1. Humanpreferencestudy.
‚Ä¢ AppendixA.2. Limitationsandfuturework.
‚Ä¢ AppendixA.3. Implementationdetailsoftheexperiments.
‚Ä¢ AppendixA.4. Ablationstudies.
‚Ä¢ AppendixA.5. Socialimpacts.
‚Ä¢ AppendixA.6. Morequalitativeresults.
VideoDemo. Wealsopresentavideoinaseparatesupplementaryfile,whichshowstheresultsin
videoformat.
A.1 HumanPreferenceStudy
Toevaluateourapproachtounderstandinguser
p m
a
t
bht
ar
e
eee
sd
nnf ee t
db
a.r
y
se oW kn
nM
ec e
d
te
o
hcs
7t
eo,
io
clw fl
n
o
oee
B
l
lc
l
leoc t oe ao
o
wd gn
th
iud 3 neu a0 gsnc g ctdt oe rr io bd tsau eea
s
rp
le
ieu s
al
cs
i
:o tne sf ter uhvs
m
beit jdu
e
b
eed
t
e
cho sy
to
ts mde vgx
s
oie
.
dp tn
e
iWe oe or nri
e
s-- )% ( etaR ecnereferP1 24680 00 0000
16.4
0.87.475.4
3.4 4
1675.6 1.53.613.381.6
9.3 6
9.275.5
alignment,cameramovementalignment,subject Motion Alignment Camera Alignment Subject Alignment Temporal Consistency
DreamBooth CustomVideo DreamVideo MotionBooth (Ours)
appearancealignment,andtemporalconsistency.
Figure8:Humanpreferencestudy.OurMotionBooth
Foreachgroupofvideos,theannotatorsselected
achieves the best human preference scores in all the
only the best one. For the subject appearance
evaluationaspects.
alignment, the annotators were provided with
correspondingsubjectimages. AsshowninFig.8,MotionBoothwasthemostpreferredmethod
acrossallmodelsandevaluationaspects,particularlyinsubjectappearancealignment. Theseresults
furtherdemonstratetheeffectivenessofourmethod.
A.2 LimitationsandFutureWork
InFig.9,weillustrateseveralfailurecasesofMotionBooth. OnesignificantlimitationofMotion-
Boothisitsstrugglewithgeneratingvideosinvolvingmultipleobjects. AsshowninFig.9(a),the
subject‚Äôsappearancecansometimesmergewithotherobjects,resultinginvisuallyconfusingoutputs.
Thisissuemightberesolvedbyincorporatingadvancedtrainingtechnologiesformultiplesubjects.
Anotherlimitationisthemodel‚Äôscapabilityto
depict certain motions indicated by the text
prompt. AsdepictedinFig.9(b),MotionBooth
mayfailtoaccuratelyrepresentmotionsthatare
Subject a [V] dog and a horse walking on the grass
unlikely to be performed by the subject. For (a) Failure case on multiple objects
example,itishardtoimagineascenewherea
wolfplushieisridingabike. Thesefailurecases
highlight the need for further improvement in
themodel‚Äôssubjectseparationandmotionun- Subject a [V] plushie riding abikeontheroad
(b) Failure case on hard motions
derstandingcapabilitiestoenhancetherealism
andaccuracyofthegeneratedvideos. Utilizing Figure9: FailurecasesofMotionBooth.
morepowerfulT2Vmodelsmayeliminatethese
drawbacks. Futureworkcouldfocusonrefiningtheseaspectstoaddressthecurrentlimitationsand
providemorerobustperformanceincomplexscenarios.
A.3 ImplementationDetails
Hyperparameters. FortheLaViemodel,wesetŒ±=10.0,œÑ =0.7T,œÉ =0.8T,andœÉ =0.6T. For
1 2
theZeroscopemodel,wesetŒ±=10.0,œÑ =0.9T,œÉ =0.9T,andœÉ =0.7T.
1 2
13Figure10: Theevaluationdataset.Wepresentonepictureforeachsubject.
(ùúé ,ùúé )
1 2
ùúè 0.95 ùêì 0.90 ùêì 0.85 ùêì 0.80 ùêì Pan Right
ùõº (0.9ùêì, 0.7ùêì)
5.0
Camera Direction
7.5 (0.8ùêì, 0.6ùêì)
Subject
a child blowing
10.0 bubbles in the park
(0.7ùêì, 0.5ùêì)
Position
a [V] cat running 15.0
on the grass First Frame Last Frame
(a)Ablationstudyonsubjectmotioncontrol.Onlythe (b) Ablation study on latent shift. Experiments on
firstframeisshown.ExperimentsonZeroscope. LaVie.AhigherœÉmeansanearlierdenoisingstep.
Figure12: Ablationstudyonmotioncontrolhyperparameters.
Evaluation dataset. We collect a total of 26 subjects from DreamBooth [39] and CustomDiffu-
sion[29]. WeshowoneimageforeachsubjectinFig.10. Thesubjectscontainawidevarietyof
types,includingpets,plushietoys,cartoons,andvehicles,whichcanprovideuswithathorough
analysisofthemodel‚Äôseffectiveness.
Userstudyinterface. Weshowtheapplication
interfaceforhumanpreferencestudyinFig.11.
Duringuserstudy,weasktheannotatorstose-
lectthebestvideobasedonthequestion, e.g.,
‚ÄúWhichvideodoyouthinkhasthebesttemporal
consistency?‚Äù
Pseudo-code of latent shift. To present the Figure11: Theapplicationinterfaceforuserstudy.
latentshiftmodulemoreclearly, weshowthe
pseudo-codeofthealgorithminFig.13. Ourlatentshiftmodulecancontrolthecameramovementin
videosinatraining-freemanneratminimalcosts.
A.4 AblationStudies
Subjectmotioncontrolhyperparameters. Weconductablationstudiesonthehyperparameterfor
subjectmotioncontrol,withtheresultspresentedinFig.12a. Weexaminedtheeffectsofvarying
thefactorŒ±ofSandthemaximumcross-attentionmanipulationtimestampœÑ. Thefindingsindicate
thatincreasingŒ±andextendingthecontrollingstepsleadtostrongercontrol. Withlowercontrol
strengths,thesubjectdoesnotappearinthedesiredposition,oronlypartofitsbodyalignswiththe
intendedspot. Conversely,whenthecontrolstrengthsaretoohigh,thegeneratedsubjectstendto
appearunnaturallysquareinshape.
Latentshifthyperparameters. WeexperimentwiththeinfluenceofœÉ andœÉ inthelatentshift
1 2
module. The results are shown in Fig. 12b. The results indicate that applying latent shift in the
earlierstepsofthedenoisingprocessresultsinincompletecameramovement,asevidencedbythe
treesinthebackground. Conversely,shiftingthelatentinthelaterstepsdegradesvideoqualityand
14Table4: Ablationstudyforthenumberofvideopreservationdata.
#Videos R-CLIP‚Üë R-DINO‚Üë CLIP-T‚Üë T-Cons.‚Üë Flowerror‚Üì
100 0.714 0.488 0.245 0.964 0.324
300 0.711 0.486 0.245 0.962 0.330
500 0.712 0.472 0.247 0.962 0.332
700 0.712 0.473 0.247 0.963 0.340
900 0.712 0.480 0.244 0.963 0.328
introducesartifacts,highlightedbytheredboxesinthelastrow. Empirically,settingœÉ andœÉ to
1 2
middlevaluesprovidesoptimalcontrolovercameramovement.
Numberofpreservationvideos. Weconductanablationstudyonthenumberofpreservationvideos.
AsshowninTable4,rangingthepreservationvideosfrom100to900doesnotbringlargechangesto
thequantitativescores. Weconcludethatthekeyistousevideodatatopreservethevideogeneration
abilityofthepre-trainedT2Vmodels. Thenumberofvideodatacanbeflexible.
A.5 SocialImpacts
Positivesocietalimpacts. MotionBoothallowsforprecisecontrolovercustomizedsubjectsand
cameramovementsinvideogeneration,openingnewavenuesforartists,filmmakers,andcontent
creatorstoproduceuniqueandhigh-qualityvisualcontentwithoutextensiveresourcesorprofessional
equipment.
Potentialnegativesocietalimpacts. Theabilitytogeneraterealisticcustomizedvideoscouldbe
misusedtocreatedeepfakes,leadingtopotentialdisinformationcampaigns,privacyviolations,and
reputationaldamage. Thisriskisparticularlysignificantinthecontextofpoliticalmanipulationand
socialmedia. Iftheunderlyingmodelsaretrainedonbiaseddatasets,thegeneratedcontentmight
reinforceharmfulstereotypesorexcludecertaingroups. Ensuringdiversityandfairnessintraining
dataiscrucialtomitigatethisrisk.
Mitigationstrategies. Developingandadheringtostrictethicalguidelinesfortheuseanddissemina-
tionofvideogenerationtechnologiescanhelpmitigatemisuse. Thisincludesimplementingusage
restrictionsandpromotingtransparencyaboutthegeneratedcontent.
A.6 MoreQualitativeResults
WeshowmorequalitativeresultsinFig.14.
15def shift_latent_one_step(latent, cx, cy, bbox, num_shift_steps):
# latent: noised latent for timestamp t
# cx: x-axis speed for camera movement
# cy: y-axis speed for camera movement
# bbox: the bounding box for the customized subject
# num_shift_steps: ùúé 1‚àíùúé 2, the total steps needed to complete latent shift
# get the latent shape
batch_size, channels, num_frames, height, width = latent.shape
# divide the camera speed by num_shift_steps.
# for each step, we move a part of the total distance
sx = cx / num_shift_steps
sy = cy / num_shift_steps
for f in range(num_frames):
# define latent shift distance for each frame
sfx = int(sx * f)
sfy = int(sy * f)
# define a obj_mask to avoid sampling tokens within the subject region
obj_mask = torch.ones_like(latent[0,0,f,:,:])
obj_mask[bbox] = False
# sampling tokens horizontally
if sfx != 0:
fill_x = torch.zeros_like(latent[:,:,f,:,:abs(sfx)])
for i in range(height):
included_indices = [x for x in range(0, width) if obj_mask[i,x]]
sampled_indices = random_choice(included_indices, size=abs(sfx)
fill_x[:,:,i,:] = latent[:,:,f,i,sampled_indices]
# sampling tokens vertically
if sfy != 0:
fill_y = torch.zeros_like(latent[:,:,f,:abs(sfy),:])
for j in range(width):
included_indices = [y for y in range(0, height) if obj_mask[y,j]]
sampled_indices = random_choice(included_indices, size=abs(sfy)
fill_y[:,:,:,j] = latent[:,:,f,sampled_indices,j]
# shift the original latent and fill in the hole with sampled tokens
if sfx > 0:
temp = latent[:,:,f,:,sfx:]
latent[:,:,f,:,:] = torch.cat([temp, fill_x], dim=-1)
elif sfx < 0:
temp = latent[:,:,f,:,:sfx]
latent[:,:,f,:,:] = torch.cat([fill_x, temp], dim=-1)
if sfy > 0:
temp = latent[:,:,f,sfy:,:]
latent[:,:,f,:,:] = torch.cat([temp, fill_y], dim=-2)
elif sfy < 0:
temp = latent[:,:,f,:sfy,:]
latent[:,:,f,:,:] = torch.cat([fill_y, temp], dim=-2)
return latents
Figure13: Pseudo-codeofthelatentshiftalgorithm.
16Subject Subject motion Camera motion Generated Videos
Up Right
A plushie panda running on the grass
Down Left
A dog jumping down the stairs
A robot toy walking in sunrise
A dog running on the beach
A motorbike moving closer on the beach with
seashore in the background
A car driving on the highway
Right
A plushie swimming around coral reef
Up Left
A wolf plushie running on the grass
Figure14: MorequalitativeresultsofourMotionBooth.
17