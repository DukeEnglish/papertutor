SPHINX-X: Scaling Data and Parameters for a Family of
Multi-modal Large Language Models
PengGao‚Ä†*1 RenruiZhang*12 ChrisLiu*1 LongtianQiu*1 SiyuanHuang*1 WeifengLin*1
ShitianZhao1 ShijieGeng3 ZiyiLin12 PengJin1 KaipengZhang1 WenqiShao1 ChaoXu1 ConghuiHe1
JunjunHe1 HaoShao2 PanLu4 HongshengLi‚Ä†2 YuQiao‚Ä†1
Abstract
ùüêùüîùüéùüé SPHINX-MoE
We propose SPHINX-X, an extensive Multi- SPHINX-Plus
ùüêùüìùüéùüé
modality Large Language Model (MLLM) se-
ries developed upon SPHINX. To improve the ùüêùüíùüéùüé
Data
architectureandtrainingefficiency,wemodifythe SPHINX-Intern2 Scaling-up
ùüêùüëùüéùüé
SPHINXframeworkbyremovingredundantvi-
sualencoders,bypassingfully-paddedsub-images ùüêùüêùüéùüé SPHINX
with skip tokens, and simplifying multi-stage
ùüêùüèùüéùüé SPHINX-X
trainingintoaone-stageall-in-oneparadigm. To SPHINX-Tiny Family
ùüêùüéùüéùüé
fully unleash the potential of MLLMs, we as-
sembleacomprehensivemulti-domainandmulti- ùüèùêÅ ùüïùêÅ ùüèùüëùêÅ ùüñ√óùüïùêÅ
modal dataset covering publicly available re- Parameter Size
sourcesinlanguage,vision,andvision-language
Figure1.Performancecomparisonwithdataandparameter
tasks. Wefurtherenrichthiscollectionwithour
scaling.WeintroduceSPHINX-X,ageneralandwell-performing
curatedOCRintensiveandSet-of-Markdatasets,
MLLMfamilydevelopeduponSPHINX(Linetal.,2023).
extendingthediversityandgenerality.Bytraining
overdifferentbaseLLMsincludingTinyLlama-
to unlock myriad novel applications and further push the
1.1B,InternLM2-7B,LLaMA2-13B,andMixtral-
boundaryofnext-levelartificialgeneralintelligence,span-
8√ó7B,weobtainaspectrumofMLLMsthatvary
ning from embodied intelligence (Geng et al., 2023), au-
in parameter size and multilingual capabilities.
tonomousdriving(Wenetal.,2023;Caoetal.,2023;Yang
Comprehensive benchmarking reveals a strong
etal.,2023d)tographicaluserinterfaces(GUI)agents(He
correlationbetweenthemulti-modalperformance
etal.,2024;Yangetal.,2023f).
withthedataandparameterscales.Codeandmod-
els are released at https://github.com/ Inspired by this, a wide array of open-source MLLMs
Alpha-VLLM/LLaMA2-Accessory. have been developed within merely one year, including
BLIP series (Li et al., 2023b; Dai et al., 2023), LLaMA-
Adapter(Zhangetal.,2024c;Gaoetal.,2023),LLaVA(Liu
1.Introduction etal.,2023b;a),MiniGPT-4(Zhuetal.,2023a),mPLUG-
Owl (Ye et al., 2023b;c), and SPHINX (Lin et al., 2023).
SincethereleaseofOpenAI‚ÄôsGPT-4(V)(OpenAI,2023) Althoughtheseopen-sourceMLLMsdemonstratepromis-
andGoogle‚ÄôsGemini(GeminiTeam,2023),Multi-modal ingmulti-modalcapabilities,theirperformanceisstillcon-
LargeLanguageModels(MLLMs)havebecomeanincreas- strained by the training data from few task domains and
ingly popular research area (Fu et al., 2023c; Yang et al., limitedchoicesofLLMparameters:
2023e). Byaligningmulti-modalencoderswithLargeLan-
LimitedDataCoverageforTasks. Popular open-source
guageModels(LLMs),MLLMsdemonstratethepotential
MLLMs,suchasBLIP-2,LLaVA,andLLaMA-Adapter,are
*Equal contribution 1Shanghai AI Laboratory 2MMLab, typicallytrainedonrawvision-languagedatafromthenat-
CUHK 3Rutgers University 4University of Califor- uralimagedomain(e.g.,LAION(Schuhmannetal.,2021;
nia, Los Angeles. ‚Ä†Equal corresponding authors:
2022), SBU (Ordonez et al., 2011), and Conceptual Cap-
Peng Gao <gaopeng@pjlab.org.cn>, Hongsheng Li
tions(Sharmaetal.,2018)),andvisualinstruction-following
<hsli@ee.cuhk.edu.hk>,YuQiao<qiaoyu@pjlab.org.cn>.
data(Chenetal.,2023b;Wangetal.,2023a)generatedby
GPT-4(V)(OpenAI,2023). Asaresult,theynormallyex-
1
4202
beF
8
]VC.sc[
1v53950.2042:viXra
ecnamrofreP
egarevASPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
High-resolution Detection Text Spotting Bilingual Multi-modal Understanding
User: User: User: SPHINX-X: User:
Detect the tallest building in the image. Please detect all texts and provide ËøôËØ∑ËØ¶ÁªÜÊèèËø∞ËøôÂº†Âõæ„ÄÇ ËøôÂº†ÂõæÁâáÊçïÊçâÂà∞‰∫Ü‰∏ÄÂπÖ‰ª§‰∫∫ÊÉäÂèπÁöÑÊôØË±°‚Ä¶„ÄÇ ËøôÂº†ÂõæË∑üÊó•Êú¨Âì™‰∏™ÁîªÂÆ∂
their bounding box coordinates. ËøôÈÅìÊ≥¢Êµ™ÊòØÂõæÁâáÁöÑ‰∏ªË¶ÅÁÑ¶ÁÇπÔºåÂÆÉË¢´Ê∂ÇÊàê‰∫Ü‰∏Ä ÁöÑÁîªÁ±ª‰ººÔºü
SPHINX-X: ÁßçÈ≤úËâ≥ÁöÑËìùËâ≤Ôºå‰∏éÊ∑±ËìùËâ≤ÁöÑËÉåÊôØÂΩ¢Êàê‰∫ÜÈ≤úÊòé
There is a tallest building [0.47, 0.44, ÁöÑÂØπÊØî„ÄÇ‚Ä¶ Âú®Ê≥¢Êµ™ÁöÑÂè≥‰æßÔºåÂèØ‰ª•ÁúãÂà∞‰∏ÄÂ∫ß SPHINX-X:
0.49, 0.50]in the image. Â±±ÔºåÂÆÉÁöÑËΩÆÂªìÂú®Ê∑±ËìùËâ≤ÁöÑÂ§©Á©∫‰∏≠ÊòæÂæóÊ†ºÂ§ñÁ™Å
BILLGATES Âá∫„ÄÇ‚Ä¶ Ê≥¢Êµ™ÂíåÂ±±ÁöÑÁõ∏ÂØπ‰ΩçÁΩÆË°®ÊòéÊ≥¢Êµ™Ê≠£Âú® ËøôÂº†Âõæ‰∏éÊó•Êú¨ÁîªÂÆ∂ËëõÈ•∞
ÂêëÂ±±ÂÜ≤Êù•ÔºåÊöóÁ§∫ÁùÄ‰∏ÄÁßçËøêÂä®ÂíåËÉΩÈáè„ÄÇ‚Ä¶ ÂåóÊñãÁöÑÁîª‰Ωú„ÄäÁ•ûÂ•àÂ∑ùÂÜ≤
THE Êµ™Èáå„ÄãÈùûÂ∏∏Áõ∏‰ºº„ÄÇËøôÂπÖ
ROAD U ËØ∑s Áî®er Ê°Ü: Ê°ÜÊ£ÄÊµãÂá∫Êù•Â±±ÁöÑ‰ΩçÁΩÆ„ÄÇ Áîª Ëøë‰Ωú ÁöÑÊèè ‰∏ÄÁªò Âú∫‰∫Ü Â§ßÊó• Êµ™Êú¨ ÔºåÊµ∑ ‰∏éÂ≤∏ ËøôÈôÑ Âº†
ÂõæÊâÄÂ±ïÁ§∫ÁöÑÊ≥¢Êµ™ÊúâÁùÄÁõ∏
SPHINX-X: ‰ººÁöÑÂä®ÊÄÅÂíåËßÑÊ®°„ÄÇ
AHEAD Â±±[0.46, 0.49, 0.71, 0.63]
GUI Grounding Multi-modal Programming
User:
Write a python code to generate this image.
User: User: User:
Add a new city. Back to previous page. Adjust the volume. SPHINX-X:
S [0P .0H 5, I 0N .05X , - 0X .1: , 0.12] S [0P .1H 1, I 0N .12X , - 0X .1: 4, 0.15] S [0P .8H 8, I 0N .90X , - 0X .9: 0, 0.92] i # dm ap D to e ar f t i = n m e {a t tp hl eo t dl ai tb a.pyplot as plt # p d l aC t tr . ae b .a a vt r ae ( l d ut a eh t se a ( . )b k )a er y sp (l )o ,t
' ' ' ' ' ' 'H G G R L B Bu e P a L a im m T n a r n-a i d V d g4n n o A ' -V' i m : C1': ' h.: : G 1 a5 9 u 9 t'35 4 e . ':8. 0 s 0 : .7 . s , 27, 7 ' 14,, : 7. .72 3,5 ,.0, # p p ( p M l l % l oA t t ) t dd . . ' . ed x y ) t l l l i sl a a t 'a b b l )b e e ee l l (l ( ( 's ' ' A M A ca o c cn d c ud e u r l r at ' a ci ) c yt y l oe f
' ' 'I m Ln i Ls n at i V-r AGu 'Pc :Tt 4B 6'L .:I 0 P 1' 2: . 71 ,6.7, # p lS th .o sw h ot wh (e ) plot
}
Geometric Reasoning Pose Estimation
User: User:
What's the question? Perform reasoning then answer. User:
Detect the key points of the
SPHINX-X: SPHINX-X: horse and person in the image.
4. If the line m is parallel to the
s a Ui .d s e 1 er 3A :0B ¬∞ o bf . 2?A 5B ¬∞C c, . w 6h 5a ¬∞t d i .s 5 a 0 Sng Pl He a IN? X-X: S A a deni Bn gg.c rlte e eh el Ae sin n . , e S ta him n n eg c nli es e a ap a nna ggir lsa l e e l c l aoe Anl i sg it s ro 5 u 5 0es 0 i nd te to S T h 0oP h .2reH s 4 ed ,I 0 eN k .t 6eeX 8yc- ]ptX ,o e ai: d n n t dk s e h[y 0 u m.p 4o a5i nn , 0t ks .2 e a y6r p,e o‚Ä¶ in t, s
What's the answer? d. degrees. [0.64,0.32, ‚Ä¶, 0.68,0.74]
Figure2.DemonstrationsofSPHINX-X.Withourproposeddataandtrainingstrategies,SPHINX-Xcanachievesuperiormulti-modal
understandingandreasoningcapabilitiesindiversedomains,e.g.,bilingual,servingasamulti-purposevisiongeneralist.
hibitasuperiormulti-modalunderstandingperformancein To resolve the aforementioned limitations of existing
naturalimages. However,theydisplaylimitedordegraded MLLMs, we introduce a family of MLLMs termed
resultsinout-of-domainscenarios,suchasOpticalCharac- SPHINX-X by extending the data coverage of tasks and
terRecognition(OCR),table,chart,andmathematicsfields, parameter scales in SPHINX, as shown in Figure 1. The
where in-depth domain-specific knowledge is critical. In superiormulti-modalgeneralizationcapacityofSPHINX-X
contrast,domain-specificMLLMslikeShikra(Chenetal., foradiversityoftasksisexhibitedinFigure2. Importantly,
2023a),mPLUG-DocOwl(Yeetal.,2023a),andKosmos- we adjust the training process and model architecture of
2.5(Lvetal.,2023)aretailoredtoexcelinspecifictasks, SPHINXtobetteraccommodatetheefficientandlarge-scale
butattheexpenseoftheirgeneralmulti-modalcapabilities. multi-modaltraining:
LimitedChoicesofModelParameters.Mostopen-source ‚ûÄModificationsoverSPHINX. Forthemixedfourvision
MLLMs are developed on top of dense LLMs, e.g., encodersinSPHINX,weonlypreservetwoofthem, i.e.,
LLaMA(Touvronetal.,2023a;b),with7Bor13Bparam- CLIP-ConvNeXt (Liu et al., 2022) and DINOv2 (Oquab
eters. Whilesuchparametercountsareoftenprohibitively etal.,2023). Consideringtheirdistinctmethodologiesand
largefordeploymentonportabledevices,thesamenumber architectures,thetwoencoderscanprovidethemostcom-
ofparametersremainsinadequatetofullyexploretheperfor- plementaryvisualsemantics,denotedasMixtureofVisual
manceboundariesofMLLMs. Therefore,scalingdownthe experts(MoV).Then,forthesub-imagedivisionstrategy
modelscaleofMLLMscouldfacilitatethebroaderadoption ofhigh-resolutionimages,iftheinputimagehasalargeas-
of mobile devices. Meanwhile, scaling up the parameter pectratio,weobserveafrequentoccurrenceoffully-padded
countthroughtheintegrationofsparsely-activatedMixture- sub-images, where all pixels are zeros. To address this,
of-Experts(MoE)architecture(Shazeeretal.,2017)could we adopt a learnable skip token to represent them within
alsounlockthefullpotentialofMLLMsinaddressingcom- LLMs,therebyshorteningthesequencelengthforefficiency,
plexreal-worldmulti-modalchallenges. whilestillpreservingtherelativepositionsofsub-images.
2SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
Furthermore,giventheincreasedtrainingdatavolume,we ‚Ä¢ Wecollectedanextensivemulti-modaldatasetcoveringa
condensethepreviousmulti-stagetrainingpipelineintoa broadspectrumoftasksandmodalities.Ontopofthat,we
morestraightforwardsingle-stageparadigm. Insteadoffine- curatedtwonewdatasetsforenhancingtheOCR-intensive
tuningdifferentpartsofLLMparametersintwostageswith andSet-of-MarkspromptingcapabilitiesofMLLMs.
different datasets, we directly train all the parameters of
LLMsonallourcollecteddatasets.
2.RelatedWork
‚ûÅMulti-DomainandMulti-ModalDatasets. Tofullyun-
Large Language Models (LLMs) Advancements in
leashthepotentialofMLLMs,weassembleanextensive
recent MLLM research are based on the breakthrough
collectionofpublicdatasetsthatspanawidearrayoftasks,
of LLMs constructed upon the Transformer architec-
andcarefullyextendtwoself-curatedmulti-modaldatasets.
ture (Vaswani et al., 2017), where progress has stemmed
Indetail,wecollectthepublicdatasetsfromtherealmsof
frombothanexpansionoftrainingdataandasignificant
vision,language,andvision-languagetasks,andreformu-
increaseinmodelparameters. Forinstance,GPT-3(Brown
late them into a unified multi-turn conversational format.
etal.,2020),boasting175Bparameters,excelsatfew-shot
Moreover,tospecificallyenhancethetargetedcapacityof
in-contextlearning,whileGPT-2(Radfordetal.,2019)with
MLLMs, we further construct an OCR-intensive dataset
1.5B parameters falls short of reaching this level of per-
andaSet-of-Mark(SoM)dataset. TheexpansionofOCR
formance. Inspired by GPT-3‚Äôs success, several LLMs
dataprocessedfromsubstantialPDFscanunlockthevisual
like PaLM (Chowdhery et al., 2022), OPT (Zhang et al.,
languageunderstandingpowerofMLLMs,e.g.,textspot-
2022b),BLOOM(Workshopetal.,2022),andLLaMAhave
tinganddocumentlayoutdetection. ThespecializedSoM
emerged. Mistral(Jiangetal.,2023)furtherintroducedwin-
dataalsocompensatesfortheSoMprompting(Yangetal.,
dowattentionforenhancedlong-contextmodeling,while
2023c) potentials of SPHINX-X, for which we construct
Mixtral8√ó7BleveragedsparseMoElayers(Fedusetal.,
delicateSoMannotationsindiversedomainsbyGPT-4.
2022;Lepikhinetal.,2020;Shazeeretal.,2017)toupscale
‚ûÇLLMParameterScalingofSPHINX-X. Withtheafore- parametersefficiently,outperformingwithfeweractivepa-
mentioned techniques and large-scale datasets, we marry rameters. Concurrently,modelssuchasQwen(Baietal.,
SPHINX-XwithvariousbaseLLMsofincreasingparameter 2023),Baichuan(Yangetal.,2023a),andInternLM(Team,
scales: TinyLlama-1.1B(Zhangetal.,2024b),InternLM2- 2023)haveadvancedbilingualLLMcapabilities,whereas
7B (Team, 2023), LLaMA2-13B (Touvron et al., 2023b), TinyLlama (Zhang et al., 2024b) and Phi-2 (Microsoft,
andMixtral-8√ó7B(Jiangetal.,2024a). Respectively,wede- 2023)focusonreducingparametersforedgedeployment.
velopafamilyofMLLMsthatfacilitatefastmobiledeploy- OurSPHINXfamilyextendsLLMstomultimodaldomains
ment(SPHINX-Tiny),providebilingualsupport(SPHINX- for visual understanding and reasoning. We select four
Intern2), possess moderate parameters with data scaling LLMswithdifferentpre-trainingandparameterscales,com-
(SPHINX-Plus),andexhibitstrongreasoningcapabilities paringtheirperformanceundermulti-modalscenarios.
throughMixture-of-Expertarchitectures(SPHINX-MoE).
Extensiveevaluationsacrossawiderangeofbenchmarks Multi-modalLargeLanguageModels(MLLMs) Con-
revealthatSPHINX-PlussurpassestheoriginalSPHINX, tinual attempts are made to connect non-text encoders to
confirmingthatenrichingdatasetscalesanddiversitycan LLMs for perception beyond natural languages, forming
benefittheperformance. Furthermore,acomparisonofbase MLLMs. EffortstoextendLLMstoperceivebeyondtext
LLMsfrom1.1Bto7√ó8Bdemonstratesthatunderthesame have birthed MLLMs, with vision as the primary modal-
trainingpipeline,scalinguptheparameterscanconsistently ity. Representative architectures include BLIP (Li et al.,
boostthemulti-modalunderstandingcapabilities. Overall, 2022),BLIP-2(Lietal.,2023b),andMiniGPT-4(Zhuetal.,
wesummarizethekeycontributionsasfollows: 2023a), whichemployqueryTransformerstosummarize
visualfeaturesandalignthemtoLLMs;Flamingo(Alayrac
‚Ä¢ Wereleaseafamilyofwell-performingMLLMstailored etal.,2022),whichusesgatedcross-attentionformixing
fromfastinferenceonmobiledevicestocomplexreason- visualrepresentationsandsupportsinterleavedimage-text
ingtasksonhigh-endcomputers. Acomprehensiverange inputs; The LLaMA-Adapter series (Zhang et al., 2024c;
ofexperimentsdemonstratesthatthescaleoftrainingdata Gao et al., 2023) which introduce zero-initialized atten-
andthesizeofLLMparametersbothplayacriticalrole tiontominimizeinterferencebetweenvisualandlanguage
intheperformanceofMLLMs. tokens;andLLaVA(Liuetal.,2023b;a),whichconnectsvi-
‚Ä¢ WeperformseveralmodificationsoverSPHINXbyelimi- sualtokenstoLLMswithasimplelinearlayeranddirectly
natingredundantvisualencoders,avoidingfully-padded fine-tunesLLMparameterstoincorporatevisualknowledge.
sub-imageswithlearnableskiptokens,aswellasstream- Therearealsorecentadvancesinfine-grainedMLLMsthat
lining the complex multi-stage training pipeline into a havedemonstratedremarkablecapabilitiesinunderstand-
single-stageall-in-oneparadigm. ingdetailedinformation. Forexample,Shikra(Chenetal.,
3SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
2023a) and VisionLLM (Wang et al., 2023b) excel in re- mentsoverSPHINX-X,enablingthetrainingpipelineand
ferringobjectdetection,whileChartAssistant(Mengetal., modelarchitecturetobeconcise.Wepresenttheupgraded
2024), mPLUG-DocOwl/PaperOwl(Yeetal.,2023a;Hu SPHINX-XtrainingpipelineinFigure3.
etal.,2023)focusonspecializeddomainssuchastables,
documents,andscientificdiagramsanalysis. Manyefforts EliminatingRedundantVisualEncoders. SPHINXem-
alsoextendLLMsintomoremodalities,suchasImageBind- ploys four complementary vision encoders to capture di-
LLM (Hanetal.,2023a), Point-LLM(Guoetal.,2023), versevisualrepresentations. Althoughthemixtureofvisual
andothers(Zhuetal.,2023b;Zhangetal.,2022a;2023b). expertscanimprovetheperformance,itinevitablyleadsto
Inthispaper,weupgradeSPHINX(Linetal.,2023)toan asignificantincreaseincomputationalcosts,especiallyfor
MLLMfamilyformoregeneralvisualinstructionfollowing, agroupofsub-imagesgeneratedfromahigh-resolutionin-
achievingsuperiorperformanceovervariousbenchmarks. put. Toobtainbettercomputationalefficiency,weeliminate
theCLIP-ViTandQ-formerencoders,onlypreservingtwo
3.Method visualencoders‚ÄìDINOv2andCLIP-ConvNeXt. Asthese
twomodelsarepre-trainedbydistinctlearningapproaches
We first revisit the design principles of SPHINX in Sec- (self-supervisedvs. weakly-supervised)andnetworkarchi-
tion3.1. Wethenrespectivelydetailthethreeimprovements tectures(ViTvs. CNN),theycanalreadyprovidethemost
madetoSPHINX-XinSection3.2concerningthesuccinct- complementaryandrefinedvisualknowledge. Wedenote
nessofvisualencoders, learnableskiptokensforuseless themastheMixtureofVisualexperts(MoV).
visualsignals,andsimplifiedone-stagetraining. Lastly,we
illustratethecompositionofourlarge-scalemulti-modality BypassingFully-paddedSub-imageswithSkipTokens.
datasetinSection3.3, aswellasintroducedifferentbase ThesuperiorperformanceofSPHINXcanbeattributedto
LLMsadoptedbytheSPHINX-XfamilyinSection3.4. itseffectivehandlingofhigh-resolutionimageswithseveral
localsub-imagesandoneglobaldownsampledimage. Dur-
3.1.ARevisitofSPHINX ingthetrainingstagesofSPHINX,allimagesbydefaultare
scaledandzero-paddedtoahighresolution448√ó448,and
SPHINX (Lin et al., 2023) proposes three types of mix-
thendividedintofour224√ó224sub-images. However,for
ingstrategiestodevelopamulti-purposeMLLM‚Äìmixing
imageswithlargeaspectratios,say2:1,thisoperationwill
of model weights, tuning tasks, and visual embeddings.
resultinfully-paddedsub-imagesfilledentirelywithzero-
Following previous efforts (Gao et al., 2023; Liu et al.,
valuepixels.Suchfully-paddedsub-imagesnotonlycontain
2023b), SPHINXadoptsatwo-stagetrainingpipeline, in
noisyanduselessvisualsignals,butalsoproducesparevi-
whichthefirststagealignspre-trainedvisionencoderswith
sualtokensthatwastecomputationalresourceswithinboth
LLaMA2 (Touvron et al., 2023b), and the second stage
MoVandLLM.Toalleviatetheissue,weproposealearn-
integrates a variety of tasks for instruction tuning. For
ableskiptokentoreplacethefully-paddedsub-image,which
morerobustvisualrepresentations,SPHINXincorporates
providesexplicitrelativepositionalinformationforLLMs
theembeddingsoffourdifferentvisionencoders,including
toidentifythepositionsbetweenusefulsub-images. Inthis
CLIP-ViT(Radfordetal.,2021;Dosovitskiyetal.,2020),
way,MoVcanavoidencodingthesezero-pixelsub-images,
CLIP-ConvNeXt(Liuetal.,2022),DINOv2(Oquabetal.,
whichallowsforareductionintheinputsequencelength
2023),andQ-former(Lietal.,2023c). SPHINXthenintro-
forLLMs,achievingenhancedcomputationalefficiency.
ducesamulti-scalemixingstrategytotacklehigh-resolution
images,whichdividesthehigh-resolutioninputintoseveral
One-Stage All-in-One Training. The original training
sub-imagesalongwithadownsampledimageforconcurrent
pipeline of SPHINX comprises two stages and utilizes a
encoding. Inaddition,tofurthermixvariousdomainseman-
weightmixingstrategy. However,itrequirestomanuallyas-
tics,SPHINXfusesthefirst-stageweightsofLLMsthatare
signvarioustunableparametersanddatasetcombinationsto
tunedbydifferentdatadomains. Despiteitssuperiorper-
differenttrainingstages,whichisalabor-intensivetask. To
formance,SPHINXisstillconstrainedbythecumbersome
simplifytheoverallparadigm,wedesignasingle-stageall-
two-stagetrainingprocessandmixedarchitectures,andit
in-onetrainingpipeline,whichtreatsallcollecteddatasets
hasyettofullycapitalizeonthepotentialbenefitsofdata
equally and uniformly transforms them into multi-modal
andmodelscaling. Motivatedbythis,wedevelopSPHINX-
multi-turn dialog formats. During the one-stage training,
X,anextensiveseriesofMLLMstoexploreamoregeneral
weunfreezealltheparametersofSPHINX(i.e.,LLMand
andcomprehensivemulti-modallearningparadigm.
intermediateprojectionlayers)exceptforthetwovisualen-
codersinMoV.Duetothelargevolumeoftrainingdataand
3.2.SPHINX-X
highreasoningcapacityofLLMs,ourone-stageall-in-one
To better handle large-scale multi-task and multi-modal strategycansignificantlystreamlinethetrainingprocedure
instruction-tuning, we perform the following improve- forMLLMswhilemaintaininghighperformance.
4SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
A squirrel is standing on a tomato
SPHINX-X Family
TinyLlama/ InternLM2 / LLaMA2 / Mixtral
A squirrel is standing on a
Skip Skip
Token Token
Unfreeze Freeze
MoV MoV MoV
Zero
Padding
Region
Divide
Fully-padded Fully-padded
Sub-image Sub-image
Downsample
Low-resolution Image Sub-images High-resolution Image
Figure3.OverallparadigmofSPHINX-Xfamily.OntopofSPHINX(Linetal.,2023),weadoptthreemodificationsforamoregeneral
andconcisearchitecture:removingredundantvisualencodersinMixtureofVisualExperts(MoV),bypassingfully-paddedsub-images
withskiptokens,andsimplifyingmulti-stagetrainingintoaone-stageall-in-oneapproach.
3.3.TrainingDataofSPHINX-X requiredtocollectahigh-qualitydatasetcombinationfor
languageinstruction-following. Thedatasetincludesmulti-
Toobtainremarkablemulti-modalcapabilities,wewidely
turndialog,question-answering,codegeneration,andmath
convert three categories of public training data into
wordproblems. Indetail,UltraChat(Dingetal.,2023)and
instruction-followingformats(language,vision,andvision-
OpenOrca (Lian et al., 2023) are utilized for basic multi-
language), andcarefullycuratetwotargetedmulti-modal
turnconversationabilities. MetaMath(Yuetal.,2023a)and
datasets (OCR-intensive and Set-of-Mark) for SPHINX.
MathInstruct(Yueetal.,2023b)arehigh-qualitymathemat-
Alldataiscombinedfortheone-stageall-in-onetraining.
ical datasets with reasoning process. WizardCoder (Luo
Specifically,fornaturallanguagedata,weutilizeddatasets
etal.,2023)isadoptedforincreasingthecodingabilityof
thatincludemulti-turndialog,mathematicalreasoning,and
LLMs. Flan-mini(Ghosaletal.,2023)isasubsetofFLAN
code generation. For vision data, we convert data from
datasetsandisincludedforquestion-answeringcapabilities.
diverse computer vision tasks including image-level and
object-levelunderstandingintomulti-turnconversationfor- Visual Instruction-following Data. For comprehensive
mats. Forvision-languagedata,wecollectvariousvisual visualunderstanding,weexpandthedatascaleofSPHINX
question-answering,visualinstruct-tuning,andfine-grained toincorporateavarietyofvisiontasksandtransformtheir
imagecaptioningdatasets. Ontopofthis,wegeneratean annotationsintoaunifiedquestion-answeringformat. The
OCRdatasetfromlarge-scalePDFdata,andamulti-domain tasksincludeimageclassification(Russakovskyetal.,2014),
Set-of-Marksdatasetwithfine-grainedmulti-modalknowl- object detection such as COCO (Lin et al., 2014),Open-
edge. ThedatastatisticinformationisprovidedinTable1. Images (Kuznetsova et al., 2018),Object365 (Shao et al.,
2019),Lvis(Guptaetal.,2019),humanposeestimationsuch
Language Instruction-following Data. Unlike previous
as UniPose (Yang et al., 2023b), COCO-Pose (Lin et al.,
works (Zhu et al., 2023a; Liu et al., 2023b;a) that uti-
2014), and visual grounding. We utilize a task-specific
lizeinstruction-tunedLLMssuchasVicuna(Chiangetal.,
promptasthequestion,andregardtheground-truthlabels
2023), SPHINX-X is directly trained on top of the basic
astheanswerbytextualizingtheminlanguagespace. For
pre-trained LLM, i.e., LLaMA2 (Touvron et al., 2023b).
generality,wedonotutilizeanyspecialtokensfordifferent
This is to investigate the training characteristics of multi-
tasks,andtreatthemallaspurelanguageproblems. This
modalmodelsfromLLMsmoreclearly. Therefore,weare
5SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
visualsupervisedfine-tuningenhancesSPHINX-Xwiththe Table1.One-stagetrainingdatasummaryofSPHINX-X.
performanceofimageparsing,objectlocalization,andrela-
tionreasoning,empoweringMLLMswithin-builtcapacity Tasks #Samples Datasets
tobeauniversalvisualgeneralist. LanguageInstruction-followingData
Multi-turnDialog 1.8M UltraChat,Flan-mini,OpenOrca
Math 0.6M MetaMathQA,MathInstruct
Vision-language Instruction-following Data. To align Coding 80k WizardCoder
MoV with LLMs and enable visual instruction follow- VisualInstruction-followingData
Detection 4.9M V3Det,OpenImages,Lvis,COCO,Object365
ing, we gather large-scale datasets from established vi- HumanPose 0.3M Unipose,COCO-Pose
Classification 1M ImageNet1K
sualquestion-answeringsourcessuchasVQAV2(Agrawal Grounding 1M VisualGenome,RefCOCO,RefCOCO+,RefCOCOg,Flickr30k
et al., 2015), GQA (Hudson & Manning, 2019), OK- Vision-languageInstruction-followingData
VQAV2,OKVQA,GQA,VisualGenome
VQA(Marinoetal.,2019),VisualGenome(Krishnaetal., CLEVR,ChartQA,DeepForm,DocVQA
VQA 0.7M DVQA,InfographicsVQA,KleisterCharity
2017), and CLEVR (Johnson et al., 2016). To specifi- VisualMRC,WikiTableQuestions
TextVQA,TabFact
cally boost SPHINX-X‚Äôs text-oriented VQA capabilities, Caption 0.5M MSCOCO,ShareGPT4V,LaionGPV4V
VisualInstruction 0.4M LLaVA,LVIS-INSTRUCT4V,LLaVAR
weincorporatedatasetsincludingTextVQA(Singhetal.,
OCR-intensiveData
2019),DocVQA(Mathewetal.,2021b),ChartQA(Masry OCR 3M PaperText:Arxiv,CommonCrawl
TextLayout&Spotting 1.0M DocBank,M6Doc,Publaynet,DocLayNet,ICDAR,CTW1500
et al., 2022), AI2D (Kembhavi et al., 2016), Deep-
Set-of-MarksInstruction-followingData
form (Svetlichnaya, 2020), DVQA (Kafle et al., 2018),
NaturalImages 5k COCO,LVIS,VisualGenome
InfographicsVQ (Mathew et al., 2021a), KleisterChar- W OCeb Rs -i rt ee l/ aM teo dbile/Desktopagent 1 2k k S Toee taC lTli ec xk t,CTW1500,IC13,IC15
DocumentImages 1k M6Doc,DoclayNet,PublayNet
ity(Stanislaweketal.,2021),TabFact(Chenetal.,2019), MultipanelImages 1k In-housedataset
VisualMRC (Tanaka et al., 2021), and WikiTableQues-
tions(Pasupat&Liang,2015). Leveragingtherichknowl-
edge embedded in large foundation models, we also en- domaindatasetsimilartoSet-of-Markstechniques(Yang
compass high-quality MLLM-generated data, e.g., dense et al., 2023c) to endow MLLMs with dense multi-modal
captioningdataofShareGPT4V(Chenetal.,2023b)andvi- captioningknowledge. Initially,wecollectdiverseimage
sualinstructiondatafromLLaVA(Liuetal.,2023b),LVIS- datasetsfromvariousdomains. Then,weutilizedatasetan-
INSTRUCT4V(Wangetal.,2023a),andLLaVAR(Zhang notationssuchasboundingboxesandobjectmaskstoplace
etal.,2023c). Additionally,weemployGeometry3K(Lu variousmarkslikepoints,boxes,polygons,andidentifiers,
et al., 2021a) to enhance the model‚Äôs geometry problem- on the raw images. After that, we craft domain-specific
solvingabilities. instructionsforeachdatatype,andpromptGPT-4Vwith
themaskedimagesformulti-scalecaptioning,whichgener-
OCR-intensive Data. Most previous MLLMs can only
atescaptionsofglobalimageunderstanding,detailedregion
leverage external tools and pre-extracted OCR tokens to
captioning,andobject-relationanalysis. SuchSoMprompt-
obtainsatisfactoryOCR-relatedunderstanding. Toenhance
ingforGPT-4Vcanmotivateitspowertoproducehigher-
suchcapabilitiesforMLLMs,wecompileanOCR-intensive
qualityandfine-grainedmulti-modaldata. Duringtraining,
datasetfromextensiveInternetPDFdata. Differentfrom
wedonotutilizethemarkedimages, buttherawimages,
previoussyntheticOCRdata(Yimetal.,2021;Kimetal.,
anddescribethemarksbylanguagewithinthemulti-turn
2021) that are too simple and far from real-world appli-
conversationsforuniformitywithotherdatadomains.
cation, our dataset is more challenging and larger-scale.
Specifically,wefirstcollectlarge-scalePDFdatasetsfrom
Common Crawl 1 and arXiv websites. Then, we utilize 3.4.SPHINX-XwithDifferentLLMs
PyMuPDF 2 to get the rendering results of each page in
Builtupontheaforementionedtechniquesandlarge-scale
the PDF file and also save all the text annotations along
datasets, we provide four choices of base LLMs in
with their bounding boxes. To ensure the OCR quality,
SPHINX-Xwithincreasingparameterscales: TinyLlama-
weadoptmultipleprocessingmethods,includingUnicode
1.1B (Zhang et al., 2024a), InternLM2-7B (Team, 2023),
characteristicchecking,textsplitsmerge,etc. Inthisway,
LLaMA2-13B (Touvron et al., 2023b), and Mixtral-
we constructed an in-house PaperText dataset with about
8√ó7B (Jiang et al., 2024a). We introduce their features
3M text-dense pages. Finally, we transform them into a
comparedtotheoriginalSPHINXwithLLaMA2-13B.
unifiedquestion-answeringformattostrengthentheOCR
documentationunderstandingability. SPHINX-Tiny with TinyLlama-1.1B. TinyLlama can be
regardedasalightweightversionofLLaMA.Thecompact-
Multi-DomainSet-of-MarkData. Wenoticethatexisting
ness of 1.1B parameters allows TinyLlama to apply to a
multi-modaldatasetslackthefine-grainedcorrespondence
diversityofscenarioswithlimitedcomputationresources.
between images and texts. Thus, we construct a multi-
Therefore,wetrainSPHINX-Tinytoobservehowthemulti-
1CommonCrawl:https://commoncrawl.org/ modalperformancevariesgiventhesmaller-scaleLLM.
2PyMuPDF:https://github.com/pymupdf/PyMuPDF
SPHINX-MoEwithMixtral-8√ó7B.AsasparseMixture-of-
6SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
Table2. Performancecomparisonwithstate-of-the-artmethodsonpopularMLLMbenchmarks.
Methods POPE MMEP MMEC MMB SEED LLaVAW MM-Vet CCbench MathVista TinyLVLM BenchLMM InfiMM-Eval Qbench
BLIP-2 85.3 1293.8 - - 46.4 38.1 22.4 - - 284.7 - - -
InstructBLIP-7B - - - 36.0 53.4 60.9 26.2 12.1 25.3 300.6 44.63 - 56.7
InstructBLIP-13B 78.9 1212.8 - - - 58.2 25.6 - - - 45.03 - -
LLaMA-AdapterV2 - 1328.4 356.4 - - - - - - 229.2 - 30.5 59.5
Qwen-VL-7B - - - 38.2 56.3 - - 5.5 - - - - 59.4
Qwen-VL-7B-Chat - 1487.6 360.7 60.6 58.2 - - 39.3 - 316.8 - 37.4 -
LLaVA1.5-7B 85.9 1510.7 - 64.3 58.6 63.4 30.5 16.4 - - 46.8 - 58.7
LLaVA1.5-13B 85.9 1531.3 295.4 67.7 61.6 70.7 35.4 26.5 - 307.2 55.5 32.62 62.1
SPHINX 90.8 1560.2 310.0 67.1 71.6 74.3 36.6 27.9 27.5 288.9 - 30.7 65.8
SPHINX-Tiny 82.2 1261.2 242.1 56.6 17.1 52.3 23.8 17.5 26.4 301.5 50.0 21.9 19.7
SPHINX-Intern2 86.9 1260.4 294.6 57.9 68.8 57.6 36.5 21.0 35.5 312.9 47.0 31.5 60.0
SPHINX-Plus 89.1 1457.7 283.6 71.0 74.8 71.7 47.9 25.6 36.8 282.1 57.4 39.5 68.6
SPHINX-MoE 89.6 1485.3 367.1 71.3 73.0 70.2 40.9 15.4 42.7 335.3 50.7 38.6 66.2
Experts(MoE)LLM,Mixtral-8√ó7Butilizes8feed-forward Table3.Performanceon7academicVQAbenchmarks.
networksateachtransformerlayerasexperts,andrelieson
Method OKVQA VQAV2 VizWiz GQA SQA IVQA
arouternetworktoactivatetwoexpertseachtime. Withthis
sparsemechanism,weexpecttoanalyzethecharacteristics BLIP-2 45.9 - 19.6 41.0 - 40.6
InstructBLIP - - 33.4 49.5 - 44.8
ofdifferentexpertsformulti-modalinstructionfollowing. LLaMA-AdapterV2 49.6 70.7 39.8 45.1 - -
Shikra 47.2 77.4 - - - -
SPHINX-PluswithLLaMA2-13B.SPHINX-Plusutilizes Fuyu-8B 60.6 74.2 - - - -
MiniGPT-v2 57.8 - 53.6 60.1 - 51.5
the same scaled LLaMA, i.e., 13B parameters, with the
Qwen-VL-7B 58.6 79.5 35.2 59.3 67.1 -
original SPHINX, but is tuned by our constructed multi- Qwen-VL-7B-Chat 56.6 78.2 38.9 57.5 68.2 -
LLaVA1.5-7B - 78.5 50.0 62.0 66.8 -
modaldataset(morediverseandlarger-scale)withonestage.
LLaVA1.5-13B - 80.0 53.6 63.3 71.6 -
Thistosomeextentcanillustratetheefficacyofdatascaling- SPHINX 62.2 80.2 46.8 62.9 69.1 52.7
up.Notethat,referringtoSPHINX-2K(Linetal.,2023),we SPHINX-Tiny 53.6 74.7 49.2 58.0 21.5 40.7
SPHINX-Intern2 55.5 75.5 49.6 56.2 70.4 49.0
alsoperformanimprovedversion,termedSPHINX-Plus-
SPHINX-Plus - - 57.8 - 74.2 54.7
2K,whichincreasestheimageresolutionfrom448√ó448 SPHINX-MoE 62.7 81.1 61.9 63.8 74.5 57.3
to672√ó672andsplitstheinputto3√ó3sub-imagesfor
fine-grainedvisualunderstanding.
The model parallel size is set to 8 for SPHINX-MoE, 2
SPHINX-Intern2 with InternLM2-7B. InternLM (Team, for SPHINX-Plus, and 1 for others. The effective batch
2023)isastrongbilingualLLMpre-trainedonlargeChinese sizeis256. NotethatSPHINX-Plusisinitializedfromthe
andEnglishcorpus. TherecentlyreleasedInternLM2-7B SPHINX model, while other SPHINX-X models use the
shows stronger bilingual language understanding ability. originalvisualencoders/LLMsandrandomlyinitializedlin-
We adopt InternLM2-7B as the base LLM to explore the earprojectionlayers. InFigure1,wereportthecumulative
performancepatternundertheregular7B-parameterLLM scoresofSPHINX-Xmodelsoverbenchmarksmentioned
setupandthepotentialofbilingualmulti-modalreasoning. in Tables 2, 5, and 6. However, due to the excessively
high overall score, we excluded MME (Fu et al., 2023b)
4.Experiment perceptiontobalancetheresults.
4.1.ExperimentalSettings
4.2.PerformanceEvaluation
All SPHINX-X models presented in the paper follow the
In this section, we conduct a thorough assessment and
one-stageall-in-onetrainingstrategy,andallmodulesex-
presentoutcomesacrossvariousbenchmarks,offeringan
ceptthevisualencodersareoptimized. Thelearningrateis
extensiveoverviewandevaluationofourSPHINX-Xfamily.
setto5e-6forSPHINX-MoE,and2e-5forothers. During
training,thelearningratefirstlinearlywarmupstothetarget MLLM Benchmarks. We evaluate SPHINX-X on re-
valuewithinthefirst0.01epoch,andthengraduallydecays cently introduced benchmarks, such as MME (Fu et al.,
to 0 following the cosine schedule. We use the AdamW 2023a), Seedbench (Li et al., 2023a), POPE (Li et al.,
optimizer with weight decay = 0 and betas = (0.9,0.95). 2023f), LLaVA-Bench (In-the-Wild) (Liu et al., 2023b),
To accommodate the large model volume, a combination MM-Vet(Yuetal.,2023b),MathVista(Luetal.,2023),MM-
ofZeRO2-style(Rajbhandarietal.,2020)dataparalleland bench (Liu et al., 2023d), CCbench (Contributors, 2023),
Megatron-style(Shoeybietal.,2019)modelparallelisused. TinyLVLM(Shaoetal.,2023)andBenchLLM(Caietal.,
2023a),InfiMM-Eval(Hanetal.,2023b),Qbench(Caietal.,
7SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
Table4. EvaluationresultsofSPHINX-MoEonotherMLLMbenchmarks.
HallusinBench AesBench CMMMU ScreenSpot
Methods MMVP MMMU-val
qAcc fAcc EasyaAcc HardaAcc aAcc AesP AesE AesA AesI val test Mobile Desktop Web
OpenSourceMLLM
LLaVA 6.0 - - - - - 62.43 64.68 45.96 1.125 - - - - - -
MiniGPT-4 12.7 8.79 10.12 31.87 27.67 35.78 41.93 39.35 38.57 0.999 26.8 - - 6.4 3.7 2.8
InstructBLIP 16.7 9.45 10.11 35.60 45.12 45.26 54.29 53.89 46.54 1.126 32.9 - - - - -
LLaVA-v1.5 24.7 10.55 24.86 49.67 29.77 46.94 66.32 68.32 45.46 1.157 36.4 - - - - -
CogAgent-Chat - - - - - - - - - - - 24.6 23.6 46.6 46.5 49.7
Qwen-VL-7B-Chat - 5.93 6.65 31.43 24.88 39.15 63.21 64.18 46.25 1.192 35.9 30.7 31.3 6.9 5.9 0
ClosedSourceMLLM
Gemini-Pro 40.7 - - - - - 71.99 71.37 49.38 1.222 47.9 - - - - -
GPT-4V 38.7 28.79 39.88 75.60 37.67 65.28 72.08 70.16 50.86 1.301 56.8 42.5 43.7 - - -
SPHINX-MoE 49.3 16.48 23.12 55.16 37.91 52.08 72.93 73.32 49.93 1.267 31.1 29.3 29.6 55.1 50.5 37.3
Table5.Performanceontext-orientedVQAtasks.‚Äò‚Ä†‚Äôdenotes Table6.Performance(Top-1Accuracy@0.5)onReferringEx-
tousedgroundtruthOCRtokensduringinferenceandtraining. pressionComprehension(REC)tasks.
Text OCR Doc Chart AI Deep Info TabVisual RefCOCO+ RefCOCO RefCOCOg
Method KLCWTQ Method
VQAVQAVQA QA 2D FormVQA Fact MRC val test-A test-B val test-A test-B val-u test-u
Specialistmodels Specialistmodels
Donut 43.5 - 67.5 41.8 - 61.6 11.6 30.0 18.8 54.6 93.9 UNINEXT 85.24 89.63 79.79 92.64 94.33 91.46 88.73 89.37
UReader 57.6 - 65.4 59.3 - 49.5 42.2 32.8 29.4 67.6 221.7 G-DINO-L 82.75 88.95 75.92 90.56 93.19 88.24 86.13 87.02
Generalistmodels Generalistmodels
BLIP-2 42.5‚Ä† 40.6 - - - - - - - - - OFA-L 68.29 76.00 61.75 79.96 83.67 76.39 67.57 67.58
InstructBLIP 50.7‚Ä† 44.8 - - - - - - - - - Shikra13B 82.89 87.79 74.41 87.83 91.11 81.81 82.64 83.16
LLaMA-AdapterV2 37.4 - - - - - - - - - - MiniGPT-v2-7B 79.97 85.12 74.45 88.69 91.65 85.33 84.44 84.66
Qwen-VL-7B 63.8 75.7 65.1 65.7 62.3 - - - - - - MiniGPT-v2-7B
79.58 85.52 73.32 88.06 91.29 84.30 84.19 84.31
Qwen-VL-7B-Chat 61.5 70.5 62.6 62.6 62.6 - - - - - - -Chat
LLaVA1.5-7B 58.2 - - - - - - - - - - Qwen-VL-7B 83.12 88.25 77.21 89.36 92.26 85.34 85.58 85.48
LLaVA1.5-13B 61.3 - - - - - - - - - - Qwen-VL-7B
SPHINX 58.8 70.0 35.8 22.5 38.1 0 24.0 0 13.8 52.9 95.3 -Chat 82.82 88.59 76.79 88.55 92.27 84.51 85.96 86.32
SPHINX-Tiny 57.8 60.3 53.0 34.1 24.6 11.8 26.3 22.2 15.3 51.1 147.5 SPHINX 86.64 91.08 80.35 91.05 92.65 86.56 88.19 88.35
SPHINX-Intern2 58.1 53.0 56.3 39.7 63.0 6.5 31.6 10.5 21.1 51.4 149.3 SPHINX-Tiny 71.34 78.49 63.71 82.89 86.89 77.91 78.50 78.86
SPHINX-Plus 65.7 70.1 61.2 53.4 46.0 9.2 34.7 23.9 27.1 52.8 171.0
SPHINX-Intern2 76.80 84.86 69.01 86.08 89.70 81.78 83.99 83.40
SPHINX-Plus-2K 70.6 68.9 71.6 55.1 47.4 23.2 39.1 31.1 31.1 54.0 178.4
SPHINX-Plus 87.59 92.08 82.96 92.44 94.22 90.06 90.11 90.56
SPHINX-MoE 68.0 64.8 68.4 55.0 55.6 20.7 41.8 25.5 29.9 52.7 184.4
SPHINX-MoE 85.50 90.48 79.88 90.64 93.74 86.85 88.26 88.51
2023b)formulti-modallanguagemodels(MLLM)topro- bhavi et al., 2016), DeepForm (Svetlichnaya, 2020), In-
videacomprehensiveassessmentofitscharacteristics. The foVQA (Mathew et al., 2021a), TabFact (Chen et al.,
results,presentedinTable2,showcaseSPHINX-X‚Äôsstate- 2019), VisualMRC (Tanaka et al., 2021). As shown in
of-the-art performance across various multi-modal tasks, Table5,SPHINX-Xachievescompetitiveperformanceon
including mathematical reasoning, complex scene under- text-relatedbenchmarkswithalimitedportionofOCRdata.
standing,low-levelvisiontasks,andvisualqualityassess-
ment,aswellasresiliencewhenfacingillusions. Visual grounding. To evaluate SPHINX-X‚Äôs ability to
preciselylocateandcomprehendreferredobjectsorregions
Visual Question Answering. The evaluation on general
within images, we conduct experiments on Referring Ex-
visual question answering (VQA) benchmarks such as
pressionComprehension(REC)benchmarks,includingRe-
VQAV2 (Agrawal et al., 2015), GQV (Hudson & Man-
fCOCO(Kazemzadehetal.,2014),RefCOCO+(Maoetal.,
ning,2019),OK-VQA(Marinoetal.,2019),VizWiz(Gurari
2015), and RefCOCOg (Mao et al., 2015). The results
etal.,2018),ScienceQA(Luetal.,2022),IconQA(Luetal.,
are presented in Table 6, SPHINX-X consistently outper-
2021b)arepresentedinTable3. SPHINX-Xexcelsacross
forms the majority of state-of-the-art models, surpassing
diversevisualquestion-answeringbenchmarks,showcasing
even specialist model G-DINO-L (Liu et al., 2023c) and
itsstate-of-the-artperformanceingeneralvisualunderstand-
othervisual-languagegeneralistmodels.
ing,relationalreasoning,scientificcontexts,andsymbolic
visualreasoning.Moreover,weconductexperimentsontext-
4.3.SPHINX-MoEonotherMLLMBenchmarks
orientedVQAbenchmarkssuchasTextVQA(Singhetal.,
2019),OCRVQA(Mishraetal.,2019),DocVQA(Mathew ToinvestigatetheabilityofSPHINX-MoEmoreconcretely
etal.,2021b),ChartQA(Masryetal.,2022),AI2D(Kem- andlocateitsabilitylevelamongmanydevelopedMLLMs,
8SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
Table7.Comparisonwithstate-of-the-artmethodsonVideo-Bench. ‚Äò*‚ÄôdenotestheQA-pairsarere-constructedorannotatedby
Video-Bench.‚ÄòV‚Äôdenotesavideotrainingversionofthemodelused.
Video-ExclusiveUnderstanding PriorKnowledge-basedQA ComprehensionandDecision-Making
Methods Avg.
Activitynet-QAMSVD-QA*MSRVTT-QA*TGIF-QAYouCook2*UCF-Cirme*MOT* TV-QA*MV-QA*NBA-QA* LicenseExam*Decision-Making*SQA3D*
Video-basedMLLM
Video-LLaMA 31.8 39.9 41.2 34.1 31.3 28.9 27.6 16.7 24.8 32.4 26.2 30.6 49.1 31.2
mPLUG-OwlV 32.7 41.5 42.5 36.3 31.7 27.1 22.8 27.8 24.0 30.2 25.1 33.3 51.0 32.0
VideoChat 34.6 44.6 42.2 37.4 33.7 27.7 22.4 27.8 26.2 34.1 28.6 38.9 55.4 31.4
Chat-UniVi 35.2 49.0 48.6 41.7 41.3 29.0 28.3 16.7 23.1 33.6 25.7 38.9 53.1 29.1
PandaGPT 36.7 45.0 50.4 44.6 29.7 33.0 33.0 16.7 27.9 37.1 31.1 41.7 56.0 30.8
OtterV 37.1 44.3 55.0 47.0 34.3 32.7 22.4 16.7 27.7 37.1 34.3 52.8 48.7 29.7
Video-ChatGPT 38.3 46.6 57.5 46.3 35.6 34.8 24.1 27.8 28.8 36.5 22.5 41.7 58.2 37.2
Image-basedMLLM
SPHINX 39.0 50.1 56.7 45.4 42.8 37.0 25.2 5.6 29.8 33.3 30.9 50.0 52.8 47.7
SPHINX-Plus 45.1 53.1 68.5 54.0 53.4 42.0 27.6 11.1 36.5 44.0 45.0 47.2 55.6 48.8
Table8.Comparisonwithstate-of-the-artmethodsonMVBench.‚ÄòV‚Äôdenotesavideotrainingversionofthemodelused.
Methods Avg. AS AP AA FA UA OE OI OS MD AL ST AC MC MA SC FP CO EN ER CI
Video-basedMLLM
OtterV 26.8 23.0 23.0 27.5 27.0 29.5 53.0 28.0 33.0 24.5 23.5 27.5 26.0 28.5 38.5 22.0 18.0 22.0 23.5 19.0 19.5
mPLUG-OwlV 29.7 22.0 28.0 34.0 29.0 29.0 40.5 27.0 31.5 27.0 23.0 29.0 31.5 27.0 44.0 24.0 40.0 31.0 26.0 20.5 29.5
Video-ChatGPT 32.7 23.5 26.0 62.0 22.5 26.5 54.0 28.0 40.0 23.0 20.0 31.0 30.5 25.5 48.5 29.0 39.5 33.0 29.5 26.0 35.5
Video-LLaMA 34.1 27.5 25.5 51.0 29.0 39.0 48.0 40.5 38.0 22.5 22.5 43.0 34.0 22.5 45.5 32.5 32.5 40.0 30.0 21.0 37.0
VideoChat 35.5 33.5 26.5 56.0 33.5 40.5 53.0 40.5 30.0 25.5 27.0 48.5 35.0 20.5 46.0 26.5 42.5 41.0 23.5 23.5 36.0
VideoChat2 51.1 66.0 47.5 83.5 49.5 60.0 58.0 71.5 42.5 23.0 23.0 88.5 39.0 42.0 44.0 49.0 58.5 36.5 35.0 40.5 65.5
Image-basedMLLM
SPHINX 37.5 32.5 31.5 65.0 38.5 43.5 54.0 37.5 28.5 22.5 26.5 45.5 39.0 41.0 47.5 40.0 23.5 37.5 31.0 35.0 30.5
SPHINX-Plus 39.7 47.5 32.0 58.0 42.5 43.5 45.0 44.0 35.5 29.0 27.0 52.0 38.0 41.0 59.5 37.5 23.0 41.0 29.0 40.0 29.5
weevaluateSPHINX-MoEonsomerecentlycuratedbench- forSPHINX-MoEtosolvethemulti-disciplinetasks,i.e.,
marks,whicharelistedbelow: theMMMUandCMMMUbenchmark. Andwethinkthis
isduetothelackofmulti-modalmulti-disciplinarydatadur-
- MMVP(Tongetal.,2024):Abenchmarkspeciallycrafted
tomeasureMLLMs‚Äôvisualunderstandingcapability. ingthetrainingstage. Thuswewouldconsidertoinvolving
moremulti-disciplinarydatainSPHINX-MoE‚Äôstraining.
- HallusionBench(Guanetal.,2023):Abenchmarktoagnos-
ticMLLMs‚Äôlanguagehallucinationandvisualillusion.
4.4.PerformanceofSPHINX-PlusonVideoAnalysis
- AesBench(Huangetal.,2024):Anexpertbenchmarkaim-
ing to comprehensively evaluate the aesthetic perception
Tofurtherassessthevisualcomprehensioncapabilitiesof
capacitiesofMLLMs.
our method, we conduct additional experiments on chal-
- MMMU(Yueetal.,2023a)&CMMMU(Geetal.,2024):
lengingvideotasks. SinceSPHINX-Plusisanimage-based
AnEnglishandaChinesebenchmark,respectively,aiming
tosolvemassivemulti-disciplinetasks,whichneedcollege- MLLM and is not trained on any video data, we need to
levelsubjectknowledgeanddeliberatereasoningability. do additional processing on video inputs. To be specific,
- ScreenSpot(Chengetal.,2024):Abenchmarkacrossvari- weevenlysampledvideosandselectedthemiddleframeas
ousGUIplatformsanddesignedtoassessMLLM‚Äôscapability therepresentativeframeforinputintothemodel. Wecon-
tolocalizeelementsbasedonthehuman‚Äôsinstructions. ductextensiveexperimentsontwobenchmarks. Thefirst
benchmark,Video-Bench(Ningetal.,2023),evaluatesthe
TheresultsareincludedinTable4. AswecanseeSPHINX-
performanceofmodelsacrossthreedistinctcapabilitylev-
MoEperformswellonallbenchmarks,sowecaninferthat
els: (i)video-exclusiveunderstanding,(ii)priorknowledge-
(i)SPHINX-MoEhasabettervisualunderstandingability
based question-answering, and (iii) comprehension and
andlesslanguagehallucinationthanothercompetitors. (ii)
decision-making. The second benchmark, MVBench (Li
SPHINX-MoEcandealwiththewebandmobiledomain
etal.,2023e),furtherbreaksdownvideounderstandinginto
data well. It should be noted that on some tasks or met-
20sub-aspects,allowingforamoredetailedcomparisonof
rics,SPHINX-MoEperformsevenbetterthanGPT-4V,e.g.,
modelperformanceatafinergranularity.
MMVPandAesP,AesEinAesBench. However,itishard
9SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
Set-of-Masks Prompting Task Detailed Captioning
SPHINX-X:
<Mark 1><Mark 2>: The cat and the bottle are both
User: situated outdoors, with the cat appearing to be on a
<Mark 1> P ofr fo ev rid ee d :a < Mde at ra ki le 1d > [d 0e .s 5c 6r ,i p 0t .i 2on 4 ]f , o <r M e aa rc kh 2 o >f [t 0h .7e 6 m , a 0r .5ks 0 ]I've l ine d tg he e a fn od r et gh re o ub no dt ,t l we h o iln e t th he e g br oo tu tn led. isT h ine tc ha et bis a cp ko gsi rt oi uon ne d.d
There is no direct interaction between the two, but they
share the same environment.
SPHINX-X:
Mark 1: The object is a cat, specifically a calico, characterized by its
tricolor fur pattern of white, black, and orange. The cat is wearing a Visual Question Answering
<Mark 2> collar, which is a common accessory for domestic cats to help identify
them and keep them safe. The collar appears to be of a standard
design, likely made of nylon or a similar material, and is fitted around User:
the cat's neck. The collar is not fully visible, but it is clear that it is Answer the following questions based on the marks I‚Äôve
s Mn au rg kly 2 f : it Tt he id s ia sn ad bis e ea r p br oa tc tt li ec ,a l l ika ec lc ye s mso ar dy e f oo fr gp le at s so , w wn ite hrs a. label that is provided: <Mark 1> [0.56, 0.24], <Mark 2> [0.76, 0.50]
partially visible. The label contains text and possibly branding or
product information. The bottle is standing upright on a ledge,
Short Captioning suggesting it has been placed there intentionally. The bottle appears SPHINX-X:
to be full or nearly full, as the liquid level is near the top. The Sure, please ask your questions.
bottle's cap is not visible, indicating it may have been removed or is
User: not currently in place.
Provide a short description for each of the marks I've User:
offered: <Mark 1> [0.56, 0.24],<Mark 2> [0.76, 0.50] Can you tell me what breed the cat marked <Mark 1>
Relation Analysis is? What is his coat color like?
SPHINX-X: User: SPHINX-X:
Mark 1: Cat wearing a collar Analyze the relationships among the marks I've offered: The cat marked <Mark 1>is a calico, which is a breed
Mark 2: Beer bottle on a ledge <Mark 1> [0.56, 0.24], <Mark 2> [0.76, 0.50] with a tricolor coat, typically white, black, and orange.
OCR-intensive Task
SPHINX-X:
There are ‚ÄúVirtual[0.25,0.30,0.52,0.38], Office[0.53,0.31,0.72,0.38],
of[0.23,0.39,0.30,0.46], the[0.34,0.39,0.47,0.46], ‚ÄùFuture‚Äú[0.49,0.39,0.72,0.45],
Text Spotting 1st[0.25,0.68,0.30,0.72], floor[0.32,0.68,0.39,0.72],
Deutsches[0.61,0.48,0.74,0.52], Forschungszentrum[0.61,0.52,0.79,0.56],
for[0.61,0.56,0.64,0.59], Kunstliche[0.65,0.56,0.77,0.60],
User: Intelligenz[0.61,0.59,0.76,0.63], GmbH[0.74,0.59,0.78,0.62],
Detect all regions in the 1.21[0.76,0.67,0.80,0.71], and Competence[0.24,0.20,0.50,0.25] in the image.
image that contain text and
identify their content.
DocumentLayout Detection
User:
Detect the layout of the document in the image.
SPHINX-X:
There are seven
text[0.16,0.50,0.48,0.64;0.16,0.64,0.48,0.74;0.16,0.73,0.48,0.85;0.16,0.85,
0.48,0.98;0.50,0.43,0.83,0.59;0.50,0.59,0.83,0.67;0.50,0.72,0.83,0.98],
figure[0.51,0.06,0.82,0.31], list[0.16,0.06,0.48,0.49], and
title[0.50,0.69,0.71,0.71] in the image.
Figure4.Set-of-Marks(SoM)promptingandOCR-intensivecapabilitiesofSPHINX-X.Withourconstructedtwodatasets,SPHINX-
XexhibitsoutstandingvisualperformanceonSoMpromptingandOCR-relatedtasks.NotethattheSoMmarksareonlyutilizedinthe
textualprompt,withoutrenderingoninputimages.
Video-Bench. As shown in Table 7, SPHINX-Plus, de- MVBench. AsshowninTable8,SPHINX-Plusalsoout-
spitebeinganimage-basedmodel,significantlyoutperforms performs most existing video-based methods (Li et al.,
existing models (Jin et al., 2023; Su et al., 2023; Zhang 2023d) except VideoChat2 (Li et al., 2023e). It is worth
et al., 2023a) specifically tailored for video tasks. Espe- notingthatVideoChat2istrainedonmorethan700Kvideo
ciallyintheaspectsofvideo-exclusiveunderstandingand instruction-tuning data, while SPHINX-Plus relies solely
priorknowledge-basedquestion-answering,SPHINX-Plus on image data. Despite this discrepancy, SPHINX-Plus
showcases outstanding performance, signifying its profi- stilloutperformsVideoChat2incertaintasks,e.g.,Moving
ciencyinvisualperceptionandknowledgeextractioncapa- Direction(MD).Theseresultsshowtheremarkablegeneral-
bilities.InchallengingdatasetssuchasMOT,SPHINX-Plus izationcapabilityofourmethodindiversevideotasks.
demonstratesslightlylowerperformancecomparedtoex-
istingstate-of-the-artmethods(Maazetal.,2023;Lietal., 4.5.DemonstrationsofSPHINX-X
2023d). Weattributethistotheneedformodelingtimingre-
InFigure2,thedemonstratesofSPHINX-Xindicatethat
lationshipsinvideos. GiventhatSPHINX-Plushasnotbeen
ourmodelscan1)conductfine-grainedobjectdetectionin
fine-tuned by any video data, its performance marginally
high-resolutionimagesbytheproposedsub-imagedivision
underperformsothers.
10SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
strategy;2)conducttextspottingwithaccuratecontentand Aggarwal, K., Som, S., Piao, S., and Wei, F. Vlmo:
positions;3)engageinbilingualimage-basedconversations, Unified vision-language pre-training with mixture-of-
andgeneratecoherent,accurate,anddetailedChinesede- modality-experts. InKoyejo,S.,Mohamed,S.,Agarwal,
scriptionsforsyntheticimages;4)generateaccuratecode A.,Belgrave,D.,Cho,K.,andOh,A.(eds.),Advancesin
forvisualprogrammingbasedonthepreciseunderstanding NeuralInformationProcessing,2022.
ofthegivenplot;5)analyzeAppscreenshotsbasedonthe
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
functionaldescriptionandoutputthecorrespondingbound-
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
ingbox;6)accuratelyinterpretgeometricquestionsfrom
Askell,A.,etal. Languagemodelsarefew-shotlearners.
images,thankstothemathandextensiveOCRdatasetsin-
In Advances in neural information processing systems,
cludedinourtrainingcorpus; and7)estimatethecorrect
pp.1877‚Äì1901,2020.
posewithrigorousbodykeypoints.
InFigure4,werespectivelyshowtheSet-of-marks(SoM) Cai, R., Song, Z., Guan, D., Chen, Z., Luo, X., Yi, C.,
promptingandOCRunderstandingcapabilitiesofSPHINX- and Kot, A. Benchlmm: Benchmarking cross-style
X. With our curated SoM dataset, SPHINX-X can well visual capability of large multimodal models. ArXiv,
understand the marks given in the prompt, i.e., a cat and abs/2312.02896,2023a.
bear bottle, and analyze the appearance and relations of
Cai,R.,Song,Z.,Guan,D.,Chen,Z.,Luo,X.,Yi,C.,and
designatedobjects. BythetrainingofOCR-intensivedata,
Kot, A. Benchlmm: Benchmarking cross-style visual
ourmodelcanconductaccuratedocumentlayoutdetection
capability of large multimodal models. arXiv preprint
andcharacterrecognition.
arXiv:2312.02896,2023b.
5.Conclusion Cao,Y.,Xu,X.,Sun,C.,Huang,X.,andShen,W. Towards
generic anomaly detection and understanding: Large-
Inthispaper,weintroduceSPHINX-X,aseriesofMLLMs scale visual-linguistic model (gpt-4v) takes the lead.
formulti-purposemulti-modalinstructiontuningwithLLM arXivpreprintarXiv:2311.02782,2023.
parametersrangingfrom1Bto8√ó7B.Ontopoftheorig-
Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and
inalSPHINX,weproposethreeaspectsofimprovements,
Zhao,R. Shikra:Unleashingmultimodalllm‚Äôsreferential
i.e.,removingredundantvisualencoders,bypassingfully-
dialoguemagic. arXivpreprintarXiv:2306.15195,2023a.
paddedsub-imageswithskiptokens,andsimplifyingmulti-
stage training into a one-stage all-in-one paradigm. We
Chen, L., Li, J., wen Dong, X., Zhang, P., He, C.,
alsocuratealarge-scalemulti-domaindatasetforMLLM
Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improv-
training, which contains a wide range of public datasets
ing large multi-modal models with better captions.
andourconstructedtargeteddata. Extensivebenchmarks
ArXiv, abs/2311.12793, 2023b. URL https:
andevaluationsdemonstratethesuperiorperformanceand
//api.semanticscholar.org/CorpusID:
generalizationcapacityofSPHINX-X.Wehopeourwork
265308687.
maycastalightonfutureMLLMresearch.
Chen,W.,Wang,H.,Chen,J.,Zhang,Y.,Wang,H.,LI,S.,
Zhou,X.,andWang,W.Y. Tabfact: Alarge-scaledataset
References
fortable-basedfactverification. ArXiv,abs/1909.02164,
Agrawal,A.,Lu,J.,Antol,S.,Mitchell,M.,Zitnick,C.L., 2019.
Parikh,D.,andBatra,D.Vqa:Visualquestionanswering.
Cheng,K.,Sun,Q.,Chu,Y.,Xu,F.,Li,Y.,Zhang,J.,and
International Journal of Computer Vision, 123:4 ‚Äì 31,
Wu,Z. Seeclick: Harnessingguigroundingforadvanced
2015.
visualguiagents.arXivpreprintarXiv:2401.10935,2024.
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z.,
Hasson,Y.,Lenc,K.,Mensch,A.,Millican,K.,Reynolds,
Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y.,
M.,etal.Flamingo:avisuallanguagemodelforfew-shot
Gonzalez, J. E., Stoica, I., and Xing, E. P. Vi-
learning. Advances in Neural Information Processing
cuna: An open-source chatbot impressing gpt-4 with
Systems,35:23716‚Äì23736,2022.
90%*chatgptquality.https://lmsys.org/blog/
Bai,J.,Bai,S.,Yang,S.,Wang,S.,Tan,S.,Wang,P.,Lin, 2023-03-30-vicuna/,March2023.
J.,Zhou,C.,andZhou,J. Qwen-vl: Aversatilevision-
Chowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,Mishra,
languagemodelforunderstanding,localization,textread-
G., Roberts, A., Barham, P., Chung, H.W., Sutton, C.,
ing,andbeyond. arXivpreprintarXiv:2308.12966,2023.
Gehrmann,S.,etal. Palm: Scalinglanguagemodeling
Bao,H.,Wang,W.,Dong,L.,Liu,Q.,Mohammed,O.K., withpathways. arXivpreprintarXiv:2204.02311,2022.
11SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
Contributors, O. Opencompass: A universal evaluation Geng, H., Wei, S., Deng, C., Shen, B., Wang, H., and
platform for foundation models. https://github. Guibas,L. Sage: Bridgingsemanticandactionableparts
com/open-compass/opencompass,2023. forgeneralizablearticulated-objectmanipulationunder
languageinstructions. arXivpreprintarXiv:2312.01307,
Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,
2023.
W.,Li,B.,Fung,P.,andHoi,S. Instructblip: Towards
general-purposevision-languagemodelswithinstruction Ghosal,D.,Chia,Y.K.,Majumder,N.,andPoria,S. Fla-
tuning,2023. cuna: Unleashingtheproblemsolvingpowerofvicuna
usingflanfine-tuning,2023.
Ding,N.,Chen,Y.,Xu,B.,Qin,Y.,Zheng,Z.,Hu,S.,Liu,
Z.,Sun,M.,andZhou,B. Enhancingchatlanguagemod- Guan,T.,Liu,F.,Li,X.W.R.X.Z.,Wang,X.L.X.,Yacoob,
els by scaling high-quality instructional conversations. L.C.F.H.Y.,andZhou,D.M.T. Hallusionbench: An
arXivpreprintarXiv:2305.14233,2023. advanceddiagnosticsuiteforentangledlanguagehalluci-
nation&visualillusioninlargevision-languagemodels.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, arXive-prints,pp.arXiv‚Äì2310,2023.
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
Guo, Z., Zhang, R., Zhu, X., Tang, Y., Ma, X., Han, J.,
M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,andHoulsby,N.
Chen, K., Gao, P., Li, X., Li, H., et al. Point-bind &
Animageisworth16x16words: Transformersforimage
point-llm: Aligningpointcloudwithmulti-modalityfor
recognitionatscale. ArXiv,abs/2010.11929,2020.
3dunderstanding,generation,andinstructionfollowing.
Fedus, W., Zoph, B., and Shazeer, N. Switch transform- arXivpreprintarXiv:2309.00615,2023.
ers: Scaling to trillion parameter models with simple
Gupta,A.,Dolla¬¥r,P.,andGirshick,R.B.Lvis:Adatasetfor
andefficientsparsity. TheJournalofMachineLearning
largevocabularyinstancesegmentation. 2019IEEE/CVF
Research,23(1):5232‚Äì5270,2022.
ConferenceonComputerVisionandPatternRecognition
Fu,C.,Chen,P.,Shen,Y.,Qin,Y.,Zhang,M.,Lin,X.,Qiu, (CVPR),pp.5351‚Äì5359,2019.
Z.,Lin,W.,Yang,J.,Zheng,X.,etal. Mme: Acompre-
Gurari,D.,Li,Q.,Stangl,A.,Guo,A.,Lin,C.,Grauman,
hensiveevaluationbenchmarkformultimodallargelan-
K.,Luo,J.,andBigham,J.P. Vizwizgrandchallenge:
guagemodels. arXivpreprintarXiv:2306.13394,2023a.
Answering visual questions from blind people. 2018
IEEE/CVFConferenceonComputerVisionandPattern
Fu,C.,Chen,P.,Shen,Y.,Qin,Y.,Zhang,M.,Lin,X.,Yang,
J.,Zheng,X.,Li,K.,Sun,X.,Wu,Y.,andJi,R. Mme:
Recognition,pp.3608‚Äì3617,2018.
Acomprehensiveevaluationbenchmarkformultimodal
Han, J., Zhang, R., Shao, W., Gao, P., Xu, P., Xiao, H.,
largelanguagemodels. arXivpreprintarXiv:2306.13394,
Zhang,K.,Liu,C.,Wen,S.,Guo,Z.,etal. Imagebind-
2023b.
llm: Multi-modalityinstructiontuning. arXivpreprint
arXiv:2309.03905,2023a.
Fu, C., Zhang, R., Lin, H., Wang, Z., Gao, T., Luo, Y.,
Huang,Y.,Zhang,Z.,Qiu,L.,Ye,G.,etal. Achallenger
Han,X.,You,Q.,Liu,Y.,Chen,W.,Zheng,H.,Mrini,K.,
togpt-4v?earlyexplorationsofgeminiinvisualexpertise.
Lin, X., Wang, Y., Zhai, B., Yuan, J., Wang, H., and
arXivpreprintarXiv:2312.12436,2023c.
Yang,H. Infimm-eval: Complexopen-endedreasoning
evaluationformulti-modallargelanguagemodels.2023b.
Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A.,
Zhang,W.,Lu,P.,He,C.,Yue,X.,Li,H.,andQiao,Y. He,H.,Yao,W.,Ma,K.,Yu,W.,Dai,Y.,Zhang,H.,Lan,
Llama-adapterv2: Parameter-efficientvisualinstruction Z., and Yu, D. Webvoyager: Building an end-to-end
model. arXivpreprintarXiv:2304.15010,2023. webagentwithlargemultimodalmodels. arXivpreprint
arXiv:2401.13919,2024.
Ge, Z., Xinrun, D., Bei, C., Yiming, L., Tongxu, L.,
Tianyu,Z.,Kang,Z.,Yuyang,C.,Chunpu,X.,Shuyue, Hu, A., Shi, Y., Xu, H., Ye, J., Ye, Q., Yan, M., Li, C.,
G., Haoran, Z., Xingwei, Q., Junjie, W., Ruibin, Y., Qian, Q., Zhang, J., and Huang, F. mplug-paperowl:
Yizhi,L.,Zekun,W.,Yudong,L.,Yu-Hsuan,T.,Fengji, Scientific diagram analysis with the multimodal large
Z., Chenghua, L., Wenhao, H., Wenhu, C., and Jie, languagemodel. arXivpreprintarXiv:2311.18248,2023.
F. Cmmmu: A chinese massive multi-discipline mul-
timodal understanding benchmark. arXiv preprint Huang,Y.,Yuan,Q.,Sheng,X.,Yang,Z.,Wu,H.,Chen,P.,
arXiv:2401.20847,2024. Yang,Y.,Li,L.,andLin,W. Aesbench: Anexpertbench-
mark for multimodal large language models on image
GeminiTeam,G. Gemini: afamilyofhighlycapablemulti- aestheticsperception. arXivpreprintarXiv:2401.08276,
modalmodels. arXivpreprintarXiv:2312.11805,2023. 2024.
12SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
Hudson,D.A.andManning,C.D. Gqa: Anewdatasetfor Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,
real-worldvisualreasoningandcompositionalquestion Kravitz,J.,Chen,S.,Kalantidis,Y.,Li,L.-J.,Shamma,
answering. 2019 IEEE/CVF Conference on Computer D.A.,etal. Visualgenome: Connectinglanguageand
VisionandPatternRecognition(CVPR),pp.6693‚Äì6702, visionusingcrowdsourceddenseimageannotations. In-
2019. ternationaljournalofcomputervision,123:32‚Äì73,2017.
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Kuznetsova,A.,Rom,H.,Alldrin,N.G.,Uijlings,J.R.R.,
Chaplot,D.S.,Casas,D.d.l.,Bressand,F.,Lengyel,G., Krasin,I.,Pont-Tuset,J.,Kamali,S.,Popov,S.,Malloci,
Lample,G.,Saulnier,L.,etal. Mistral7b. arXivpreprint M.,Kolesnikov,A.,Duerig,T.,andFerrari,V. Theopen
arXiv:2310.06825,2023. images dataset v4. International Journal of Computer
Vision,128:1956‚Äì1981,2018.
Jiang,A.Q.,Sablayrolles,A.,Roux,A.,Mensch,A.,Savary,
B.,Bamford,C.,Chaplot,D.S.,Casas,D.d.l.,Hanna, Lepikhin,D.,Lee,H.,Xu,Y.,Chen,D.,Firat,O.,Huang,Y.,
E. B., Bressand, F., et al. Mixtral of experts. arXiv Krikun,M.,Shazeer,N.,andChen,Z. Gshard: Scaling
preprintarXiv:2401.04088,2024a. giantmodelswithconditionalcomputationandautomatic
sharding. arXivpreprintarXiv:2006.16668,2020.
Jiang,A.Q.,Sablayrolles,A.,Roux,A.,Mensch,A.,Savary,
Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y.
B.,Bamford,C.,Chaplot,D.S.,deLasCasas,D.,Hanna,
Seed-bench: Benchmarkingmultimodalllmswithgener-
E.B.,Bressand,F.,Lengyel,G.,Bour,G.,Lample,G.,
ativecomprehension. ArXiv,abs/2307.16125,2023a.
Lavaud,L.R.,Saulnier,L.,Lachaux,M.,Stock,P.,Sub-
ramanian,S.,Yang,S.,Antoniak,S.,Scao,T.L.,Gervet,
Li,J.,Li,D.,Xiong,C.,andHoi,S. Blip: Bootstrapping
T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E.
language-imagepre-trainingforunifiedvision-language
Mixtralofexperts. Arxiv2401.04088,2024b.
understandingandgeneration. InInternationalConfer-
ence on Machine Learning, pp. 12888‚Äì12900. PMLR,
Jin, P., Takanobu, R., Zhang, C., Cao, X., and Yuan, L.
2022.
Chat-univi: Unifiedvisualrepresentationempowerslarge
language models with image and video understanding.
Li, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: Boot-
arXivpreprintarXiv:2311.08046,2023.
strappinglanguage-imagepre-trainingwithfrozenimage
encodersandlargelanguagemodels. InKrause,A.,Brun-
Johnson,J.,Hariharan,B.,vanderMaaten,L.,Fei-Fei,L.,
skill,E.,Cho,K.,Engelhardt,B.,Sabato,S.,andScarlett,
Zitnick,C.L.,andGirshick,R.B. Clevr: Adiagnostic
J.(eds.),Proceedingsofthe40thInternationalConfer-
datasetforcompositionallanguageandelementaryvisual
enceonMachineLearning,volume202ofProceedings
reasoning. 2017IEEEConferenceonComputerVision
ofMachineLearningResearch,pp.19730‚Äì19742.PMLR,
andPatternRecognition(CVPR),pp.1988‚Äì1997,2016.
23‚Äì29 Jul 2023b. URL https://proceedings.
mlr.press/v202/li23q.html.
Kafle, K., Cohen, S. D., Price, B. L., and Kanan, C.
Dvqa: Understanding data visualizations via question
Li, J., Li, D., Savarese, S., and Hoi, S. C. H. Blip-
answering. 2018 IEEE/CVF Conference on Com-
2: Bootstrapping language-image pre-training with
puter Vision and Pattern Recognition, pp. 5648‚Äì5656,
frozen image encoders and large language models.
2018. URL https://api.semanticscholar.
In International Conference on Machine Learning,
org/CorpusID:4445015.
2023c. URL https://api.semanticscholar.
org/CorpusID:256390509.
Kazemzadeh,S.,Ordonez,V.,andreMatten,M.,andBerg,
T.L. Referitgame: Referringtoobjectsinphotographs Li,K.,He,Y.,Wang,Y.,Li,Y.,Wang,W.,Luo,P.,Wang,
ofnaturalscenes. InConferenceonEmpiricalMethods Y.,Wang,L.,andQiao,Y. Videochat: Chat-centricvideo
inNaturalLanguageProcessing,2014. understanding. arXivpreprintarXiv:2305.06355,2023d.
Kembhavi,A.,Salvato,M.,Kolve,E.,Seo,M.,Hajishirzi, Li,K.,Wang,Y.,He,Y.,Li,Y.,Wang,Y.,Liu,Y.,Wang,Z.,
H.,andFarhadi,A. Adiagramisworthadozenimages. Xu,J.,Chen,G.,Luo,P.,etal. Mvbench: Acomprehen-
ArXiv, abs/1603.07396, 2016. URL https://api. sivemulti-modalvideounderstandingbenchmark. arXiv
semanticscholar.org/CorpusID:2682274. preprintarXiv:2311.17005,2023e.
Kim, G., Hong, T., Yim, M., Park, J., Yim, J., Hwang, Li,Y.,Du,Y.,Zhou,K.,Wang,J.,Zhao,W.X.,andWen,
W., Yun, S., Han, D., and Park, S. Donut: Document J.-R. Evaluating object hallucination in large vision-
understanding transformer without ocr. arXiv preprint language models. arXiv preprint arXiv:2305.10355,
arXiv:2111.15664,7:15,2021. 2023f.
13SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
Lian, W., Goodson, B., Pentland, E., Cook, A., Vong, C., Mathvista: Evaluatingmathreasoninginvisualcontexts
and ‚ÄùTeknium‚Äù. Openorca: An open dataset of gpt with gpt-4v, bard, and other large multimodal models.
augmented flan reasoning traces. https://https: ArXiv,abs/2310.02255,2023.
//huggingface.co/Open-Orca/OpenOrca,
2023. Luo,Z.,Xu,C.,Zhao,P.,Sun,Q.,Geng,X.,Hu,W.,Tao,C.,
Ma,J.,Lin,Q.,andJiang,D. Wizardcoder: Empowering
Lin,T.-Y.,Maire,M.,Belongie,S.J.,Hays,J.,Perona,P., code large language models with evol-instruct. arXiv
Ramanan, D., Dolla¬¥r, P., and Zitnick, C. L. Microsoft preprintarXiv:2306.08568,2023.
coco: Commonobjectsincontext. InEuropeanConfer-
enceonComputerVision,2014. Lv, T., Huang, Y., Chen, J., Cui, L., Ma, S., Chang,
Y., Huang, S., Wang, W., Dong, L., Luo, W., et al.
Lin,Z.,Liu,C.,Zhang,R.,Gao,P.,Qiu,L.,Xiao,H.,Qiu,
Kosmos-2.5:Amultimodalliteratemodel. arXivpreprint
H., Lin, C., Shao, W., Chen, K., et al. Sphinx: The
arXiv:2309.11419,2023.
joint mixing of weights, tasks, and visual embeddings
formulti-modallargelanguagemodels. arXivpreprint Maaz, M., Rasheed, H., Khan, S., and Khan, F. S.
arXiv:2311.07575,2023. Video-chatgpt: Towards detailed video understanding
via large vision and language models. arXiv preprint
Liu, H., Li, C., Li, Y., andLee, Y.J. Improvedbaselines
arXiv:2306.05424,2023.
withvisualinstructiontuning,2023a.
Mao, J., Huang, J., Toshev, A., Camburu, O.-M., Yuille,
Liu, H., Li, C., Wu, Q., andLee, Y.J. Visualinstruction
A. L., and Murphy, K. P. Generation and comprehen-
tuning. InNeurIPS,2023b.
sion of unambiguous object descriptions. 2016 IEEE
Liu,S.,Zeng,Z.,Ren,T.,Li,F.,Zhang,H.,Yang,J.,yueLi, ConferenceonComputerVisionandPatternRecognition
C.,Yang,J.,Su,H.,Zhu,J.-J.,andZhang,L. Grounding (CVPR),pp.11‚Äì20,2015.
dino: Marryingdinowithgroundedpre-trainingforopen-
setobjectdetection. ArXiv,abs/2303.05499,2023c. Marino,K.,Rastegari,M.,Farhadi,A.,andMottaghi,R.Ok-
vqa: Avisualquestionansweringbenchmarkrequiring
Liu,Y.,Duan,H.,Zhang,Y.,Li,B.,Zhang,S.,Zhao,W., external knowledge. 2019 IEEE/CVF Conference on
Yuan,Y.,Wang,J.,He,C.,Liu,Z.,etal. Mmbench: Is Computer Vision and Pattern Recognition (CVPR), pp.
your multi-modal model an all-around player? arXiv 3190‚Äì3199,2019.
preprintarXiv:2307.06281,2023d.
Masry, A., Long, D., Tan, J. Q., Joty, S., and
Liu,Z.,Mao,H.,Wu,C.-Y.,Feichtenhofer,C.,Darrell,T., Hoque, E. ChartQA: A benchmark for question
andXie,S. Aconvnetforthe2020s. InProceedingsof answering about charts with visual and logical rea-
theIEEE/CVFconferenceoncomputervisionandpattern soning. In Findings of the Association for Com-
recognition,pp.11976‚Äì11986,2022. putational Linguistics: ACL 2022, pp. 2263‚Äì2279,
Dublin, Ireland, May 2022. Association for Computa-
Lu, P., Gong, R., Jiang, S., Qiu, L., Huang, S.,
tionalLinguistics. doi: 10.18653/v1/2022.findings-acl.
Liang, X., and Zhu, S.-C. Inter-gps: Inter-
177. URL https://aclanthology.org/2022.
pretable geometry problem solving with formal lan-
findings-acl.177.
guage and symbolic reasoning. In Annual Meet-
ing of the Association for Computational Linguistics,
Mathew,M.,Bagal,V.,Tito,R.P.,Karatzas,D.,Valveny,E.,
2021a. URL https://api.semanticscholar.
andJawahar,C. Infographicvqa. 2022IEEE/CVFWinter
org/CorpusID:234337054.
ConferenceonApplicationsofComputerVision(WACV),
pp.2582‚Äì2591,2021a.
Lu,P.,Qiu,L.,Chen,J.,Xia,T.,Zhao,Y.,Zhang,W.,Yu,
Z.,Liang,X.,andZhu,S.-C. Iconqa: Anewbenchmark
Mathew, M., Karatzas, D., and Jawahar, C. Docvqa: A
forabstractdiagramunderstandingandvisuallanguage
dataset for vqa on document images. In Proceedings
reasoning. ArXiv,abs/2110.13214,2021b.
of the IEEE/CVF winter conference on applications of
Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, computervision,pp.2200‚Äì2209,2021b.
S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to
Meng,F.,Shao,W.,Lu,Q.,Gao,P.,Zhang,K.,Qiao,Y.,
explain: Multimodalreasoningviathoughtchainsforsci-
andLuo,P.Chartassisstant:Auniversalchartmultimodal
encequestionanswering. ArXiv,abs/2209.09513,2022.
languagemodelviachart-to-tablepre-trainingandmulti-
Lu, P., Bansal, H., Xia, T., Liu, J., yue Li, C., Hajishirzi, taskinstructiontuning. arXivpreprintarXiv:2401.02384,
H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. 2024.
14SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
Microsoft. Phi-2,2023. URLhttps://huggingface. Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,
co/microsoft/phi-2. Ma,S.,Huang,Z.,Karpathy,A.,Khosla,A.,Bernstein,
M.S.,Berg,A.C.,andFei-Fei,L. Imagenetlargescale
Mishra,A.,Shekhar,S.,Singh,A.K.,andChakraborty,A. visual recognition challenge. International Journal of
Ocr-vqa: Visual question answering by reading text in ComputerVision,115:211‚Äì252,2014.
images. 2019 International Conference on Document
AnalysisandRecognition(ICDAR),pp.947‚Äì952,2019. Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,
R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and
Ning,M.,Zhu,B.,Xie,Y.,Lin,B.,Cui,J.,Yuan,L.,Chen, Komatsuzaki, A. Laion-400m: Open dataset of clip-
D.,andYuan,L. Video-bench: Acomprehensivebench- filtered 400 million image-text pairs. arXiv preprint
mark and toolkit for evaluating video-based large lan- arXiv:2111.02114,2021.
guagemodels. arXivpreprintarXiv:2311.16103,2023.
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C.,
OpenAI. GPT-4V(ision) system card, 2023. Wightman,R.,Cherti,M.,Coombes,T.,Katta,A.,Mullis,
URL https://openai.com/research/ C.,Wortsman,M.,etal. Laion-5b: Anopenlarge-scale
gpt-4v-system-card. dataset for training next generation image-text models.
AdvancesinNeuralInformationProcessingSystems,35:
Oquab,M.,Darcet,T.,Moutakanni,T.,Vo,H.,Szafraniec,
25278‚Äì25294,2022.
M.,Khalidov,V.,Fernandez,P.,Haziza,D.,Massa,F.,El-
Nouby,A.,etal. Dinov2: Learningrobustvisualfeatures Shao,S.,Li,Z.,Zhang,T.,Peng,C.,Yu,G.,Zhang,X.,Li,
withoutsupervision. arXivpreprintarXiv:2304.07193, J.,andSun,J. Objects365: Alarge-scale,high-quality
2023. dataset for object detection. 2019 IEEE/CVF Interna-
tionalConferenceonComputerVision(ICCV),pp.8429‚Äì
Ordonez,V.,Kulkarni,G.,andBerg,T. Im2text:Describing
8438,2019.
images using 1 million captioned photographs. In
Shawe-Taylor,J.,Zemel,R.,Bartlett,P.,Pereira,F.,and Shao, W., Hu, Y., Gao, P., Lei, M., Zhang, K., Meng, F.,
Weinberger,K.(eds.),AdvancesinNeuralInformation Xu,P.,Huang,S.,Li,H.,Qiao,Y.,etal. Tinylvlm-ehub:
ProcessingSystems,volume24.CurranAssociates,Inc., Earlymultimodalexperimentswithbard. arXivpreprint
2011. URL https://proceedings.neurips. arXiv:2308.03729,2023.
cc/paper_files/paper/2011/file/
5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper. Sharma,P.,Ding,N.,Goodman,S.,andSoricut,R. Con-
pdf. ceptualcaptions: Acleaned,hypernymed,imagealt-text
datasetforautomaticimagecaptioning. InProceedings
Pasupat, P. and Liang, P. Compositional semantic ofthe56thAnnualMeetingoftheAssociationforCompu-
parsing on semi-structured tables. In Annual Meet- tationalLinguistics(Volume1: LongPapers),pp.2556‚Äì
ing of the Association for Computational Linguistics, 2565,2018.
2015. URL https://api.semanticscholar.
org/CorpusID:9027681. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,
Q.,Hinton,G.,andDean,J. Outrageouslylargeneural
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., networks: Thesparsely-gatedmixture-of-expertslayer.
Sutskever,I.,etal. Languagemodelsareunsupervised arXivpreprintarXiv:1701.06538,2017.
multitasklearners. OpenAIblog,1(8):9,2019.
Shoeybi,M.,Patwary,M.,Puri,R.,LeGresley,P.,Casper,
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, J., and Catanzaro, B. Megatron-lm: Training multi-
G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., billion parameter language models using model paral-
Clark,J.,Krueger,G.,andSutskever,I. Learningtrans- lelism. arXivpreprintarXiv:1909.08053,2019.
ferable visual models from natural language supervi-
sion. InInternationalConferenceonMachineLearning, Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,
2021. URL https://api.semanticscholar. Batra, D., Parikh, D., and Rohrbach, M. Towards vqa
org/CorpusID:231591445. models that can read. 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp.
Rajbhandari,S.,Rasley,J.,Ruwase,O.,andHe,Y. Zero: 8309‚Äì8318,2019.
Memoryoptimizationstowardtrainingtrillionparameter
models. InSC20: InternationalConferenceforHighPer- Stanislawek,T.,Grali‚Äônski,F.,Wr‚Äôoblewska,A.,Lipi‚Äônski,
formanceComputing,Networking,StorageandAnalysis, D.,Kaliska,A.,Rosalska,P.,Topolski,B.,andBiecek,P.
pp.1‚Äì16.IEEE,2020. Kleister: Keyinformationextractiondatasetsinvolving
15SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
longdocumentswithcomplexlayouts. InIEEEInterna- F.,etal. Bloom: A176b-parameteropen-accessmultilin-
tionalConferenceonDocumentAnalysisandRecogni- guallanguagemodel. arXivpreprintarXiv:2211.05100,
tion,2021. 2022.
Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., and Cai, D. Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin,
Pandagpt: One model to instruction-follow them all. C.,Lv,C.,Pan,D.,Wang,D.,Yan,D.,etal. Baichuan
arXivpreprintarXiv:2305.16355,2023. 2: Open large-scale language models. arXiv preprint
arXiv:2309.10305,2023a.
Svetlichnaya, S. Deepform, 2020. URL
https://wandb.ai/stacey/deepform_ Yang, J., Zeng, A., Zhang, R., and Zhang, L. Unipose:
v1/reports/DeepForm-Understand-/ Detectinganykeypoints. ArXiv,abs/2310.08530,2023b.
Structured-Documents-at-Scale.
Yang, J., Zhang, H., Li, F., Zou, X., Li, C., and Gao, J.
Tanaka, R., Nishida, K., and Yoshida, S. Visualmrc: Set-of-mark prompting unleashes extraordinary visual
Machine reading comprehension on document images. groundingingpt-4v. arXivpreprintarXiv:2310.11441,
ArXiv,abs/2101.11272,2021. 2023c.
Team, I. Internlm: A multilingual language model with
Yang, S., Liu, J., Zhang, R., Pan, M., Guo, Z., Li, X.,
progressivelyenhancedcapabilities,2023.
Chen, Z., Gao, P., Guo, Y., and Zhang, S. Lidar-llm:
Exploringthepotentialoflargelanguagemodelsfor3d
Tong,S.,Liu,Z.,Zhai,Y.,Ma,Y.,LeCun,Y.,andXie,S.
lidar understanding. arXiv preprint arXiv:2312.14074,
Eyes wide shut? exploring the visual shortcomings of
2023d.
multimodalllms. Arxiv2401.06209,2024.
Yang,Z.,Li,L.,Lin,K.,Wang,J.,Lin,C.-C.,Liu,Z.,and
Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
Wang,L. Thedawnoflmms: Preliminaryexplorations
M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
withgpt-4v(ision). arXivpreprintarXiv:2309.17421,9
Azhar,F.,etal. Llama:Openandefficientfoundationlan-
(1):1,2023e.
guagemodels. arXivpreprintarXiv:2302.13971,2023a.
Yang, Z., Liu, J., Han, Y., Chen, X., Huang, Z., Fu, B.,
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
andYu,G. Appagent: Multimodalagentsassmartphone
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
users. arXivpreprintarXiv:2312.13771,2023f.
Bhosale,S.,etal. Llama2: Openfoundationandfine-
tuned chat models. arXiv preprint arXiv:2307.09288,
Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Dan, Y., Zhao,
2023b.
C.,Xu,G.,Li,C.,Tian,J.,etal. mplug-docowl: Modu-
larizedmultimodallargelanguagemodelfordocument
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
understanding. arXivpreprintarXiv:2307.02499,2023a.
L.,Gomez,A.N.,Kaiser,≈Å.,andPolosukhin,I. Atten-
tionisallyouneed. InAdvancesinneuralinformation
Ye,Q.,Xu,H.,Xu,G.,Ye,J.,Yan,M.,Zhou,Y.,Wang,J.,
processingsystems,2017.
Hu,A.,Shi,P.,Shi,Y.,Jiang,C.,Li,C.,Xu,Y.,Chen,H.,
Wang,J.,Meng,L.,Weng,Z.,He,B.,Wu,Z.,andJiang, Tian,J.,Qian,Q.,Zhang,J.,andHuang,F. mplug-owl:
Y.-G. To see is to believe: Prompting gpt-4v for bet- Modularization empowers large language models with
ter visual instruction tuning. ArXiv, abs/2311.07574, multimodality,2023b.
2023a. URL https://api.semanticscholar.
Ye, Q., Xu, H., Ye, J., Yan, M., Hu, A., Liu, H., Qian,
org/CorpusID:265150580.
Q., Zhang, J., Huang, F., and Zhou, J. mplug-owl2:
Wang,W.,Chen,Z.,Chen,X.,Wu,J.,Zhu,X.,Zeng,G., Revolutionizingmulti-modallargelanguagemodelwith
Luo,P.,Lu,T.,Zhou,J.,Qiao,Y.,etal. Visionllm: Large modalitycollaboration,2023c.
languagemodelisalsoanopen-endeddecoderforvision-
Yim, M., Kim, Y., Cho, H.-C., and Park, S. Synthtiger:
centrictasks. arXivpreprintarXiv:2305.11175,2023b.
Synthetictextimagegeneratortowardsbettertextrecog-
Wen,L.,Fu,D.,Li,X.,Cai,X.,Ma,T.,Cai,P.,Dou,M., nitionmodels. InInternationalConferenceonDocument
Shi,B.,He,L.,andQiao,Y. Dilu: Aknowledge-driven AnalysisandRecognition,pp.109‚Äì124.Springer,2021.
approachtoautonomousdrivingwithlargelanguagemod-
Yu,L.,Jiang,W.,Shi,H.,Yu,J.,Liu,Z.,Zhang,Y.,Kwok,
els. arXivpreprintarXiv:2309.16292,2023.
J. T., Li, Z., Weller, A., and Liu, W. Metamath: Boot-
Workshop,B.,Scao,T.L.,Fan,A.,Akiki,C.,Pavlick,E., strapyourownmathematicalquestionsforlargelanguage
Ilic¬¥,S.,Hesslow,D.,Castagne¬¥,R.,Luccioni,A.S.,Yvon, models. arXivpreprintarXiv:2309.12284,2023a.
16SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
Yu,W.,Yang,Z.,Li,L.,Wang,J.,Lin,K.,Liu,Z.,Wang, Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.
X.,andWang,L. Mm-vet: Evaluatinglargemultimodal Minigpt-4: Enhancing vision-language understanding
modelsforintegratedcapabilities.ArXiv,abs/2308.02490, with advanced large language models. arXiv preprint
2023b. arXiv:2304.10592,2023a.
Yue,X.,Ni,Y.,Zhang,K.,Zheng,T.,Liu,R.,Zhang,G.,
Zhu, X., Zhang, R., He, B., Guo, Z., Zeng, Z., Qin, Z.,
Stevens,S.,Jiang,D.,Ren,W.,Sun,Y.,Wei,C.,Yu,B.,
Zhang,S.,andGao,P. Pointclipv2: Promptingclipand
Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu,
gpt for powerful 3d open-world learning. ICCV 2023,
Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mmmu:
2023b.
A massive multi-discipline multimodal understanding
andreasoningbenchmarkforexpertagi. arXivpreprint
arXiv:2311.16502,2023a. A.Appendix
Yue,X.,Qu,X.,Zhang,G.,Fu,Y.,Huang,W.,Sun,H.,Su, A.1.AnalysisofRoutingMechanismsinSPHINX-MoE
Y.,andChen,W. Mammoth: Buildingmathgeneralist
A.1.1.INFERENCEWITHDIFFERENTNUMBERSOF
modelsthroughhybridinstructiontuning. arXivpreprint
ACTIVATINGEXPERTS
arXiv:2309.05653,2023b.
ForSPHINX-MoE,theLLMbackboneisbasedonMixtral-
Zhang,H.,Li,X.,andBing,L.Video-llama:Aninstruction-
8√ó7B(Jiangetal.,2024b),whichisamixture-of-experts-
tunedaudio-visuallanguagemodelforvideounderstand-
based large language model. Thus, during the inference
ing. arXivpreprintarXiv:2306.02858,2023a.
time,onlysomeoftheexpertswillbeactivatedwhendealing
Zhang, P., Zeng, G., Wang, T., and Lu, W. Tinyl- with each token. In the training stage of SPHINX-MoE,
lama: An open-source small language model. onlytwooftheeightexpertswillbeactivated,sowesetthe
ArXiv, abs/2401.02385, 2024a. URL https: defaultnumber ofactivatingexpertsto2 wheninference.
//api.semanticscholar.org/CorpusID: Toinvestigatehowtheactivatingexperts‚Äôamountwillaffect
266755802. theinferenceperformance,wechangeitfromonetoeight,
andtheresultsareshowninFigure6.
Zhang, P., Zeng, G., Wang, T., and Lu, W. Tinyllama:
Aswecansee,onmostdatasets,i.e.,ScienceQA(Luetal.,
Anopen-sourcesmalllanguagemodel. arXivpreprint
2022),TextQA(Singhetal.,2019),RefCOCO(Linetal.,
arXiv:2401.02385,2024b.
2014)andMathvista(Luetal.,2023),whenactivatingtwo
Zhang,R.,Guo,Z.,Zhang,W.,Li,K.,Miao,X.,Cui,B., experts,keepingitthesamewithtrainingsetting,SPHINX-
Qiao, Y., Gao, P., and Li, H. Pointclip: Point cloud MoE performs the best. However, for MME (Fu et al.,
understandingbyclip. InCVPR2022,2022a. 2023b), when setting the number of activating experts to
four,SPHINX-MoEworksthebest. Twoactivatingexperts
Zhang,R.,Hu,X.,Li,B.,Huang,S.,Deng,H.,Li,H.,Qiao,
actuallymakethesecondlow-performance. Thisinconsis-
Y.,andGao,P. Prompt,generate,thencache: Cascadeof
tencywiththetrainingsettingisinteresting.
foundationmodelsmakesstrongfew-shotlearners.CVPR
2023,2023b.
A.1.2.EXPERTS‚ÄôUSAGEDISTRIBUTIONONDIFFERENT
Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., DOMAINSANDDIFFERENTMODALITIES
Li, H., Gao, P., and Qiao, Y. LLaMA-adapter: Effi-
In some previous works, each expert in the mixture-of-
cient fine-tuning of large language models with zero-
expertsmodelisaspecialistforaspecificdomainormodal-
initializedattention. InTheTwelfthInternationalConfer-
ity,e.g.,VLMO(Baoetal.,2022). Soweexplorethat,in
enceonLearningRepresentations,2024c. URLhttps:
SPHINX-MoE, how each expert in each layer deals with
//openreview.net/forum?id=d4UiXAHN2W.
data from different domains and different modalities. So
wepicktheartwork,celebrityandOCRsubtasksfromthe
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
MME (Fu et al., 2023b) benchmark, and infer SPHINX-
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,
MoEonthesesubtaskswithtwoactivatingexperts,record-
etal.Opt:Openpre-trainedtransformerlanguagemodels.
ingtheexpert‚Äôsusagedistributionofeachlayer,asshown
arXivpreprintarXiv:2205.01068,2022b.
inFigure5. Subfigure(a),(b)and(c)showtheresultson
Zhang,Y.,Zhang,R.,Gu,J.,Zhou,Y.,Lipka,N.,Yang,D., visionmodality,languagemodalityandvision&language
andSun,T.Llavar:Enhancedvisualinstructiontuningfor modalities separately. From the distribution record, we
text-richimageunderstanding. ArXiv,abs/2306.17107, don‚Äôtseeanobviouspatternthatexpertsarespecialistsfor
2023c. URL https://api.semanticscholar. differentdomainsormodalities. (i)Fordifferentdomains,
org/CorpusID:259287523. theexperts‚Äôusageissimilarforthethreedifferentdomain
17SPHINX-X:ScalingDataandParametersforaFamilyofMulti-modalLargeLanguageModels
(a) Vision Modality
(b) Language Modality
(c) Vision&LanguageModality
Figure5. Experts‚Äôusagedistributionondifferentdomainsanddifferentmodalities.
e ro
cS
ero
cS
EM
E M
M
M
Figure6.Performance with different numbers of activating
expertswheninference.Werespectivelyreporttheperformance
onMMEandotherbenchmarks.
data: artwork,celebrityandOCR.(ii)Fordifferentmodali-
Figure7.Performancechangewhenpruningdifferentnumbers
ties,therearenospecificexpertsthatmainlydealwithone
ofexpertsineachlayer.Theblackdottedlineistheaverageof
specificmodality. Butthereisaninterestingscenariothat
thethreerunsofrandompruning.
theexperts‚Äôusagedistributionofthelayersatbothendsof
themodelismoreflatthanthatofthethemiddlelayers.
Wefindthatsomeexpertsare‚Äúimportant‚Äù, i.e., thereisa
A.1.3.PRUNESOMEOFTHEEXPERTSWHENINFERENCE hugeperformancevarianceinthethreeruns. Ifweprune
somespecificexperts,SPHINX-MoEwilllosemostability,
Differentfromthedensemodel,forthesparsemodelonly
whileinanotherrun,themostabilityofthemodelwillbe
partoftheparametersareactivatedduringtheinferencetime.
retained even if we prune the same number of experts in
Soifweprunesomeexperts,theabilityofthemodelcould
eachlayer. Thus,ifwekeepthese‚Äúimportant‚Äùexpertsand
bepartlysaved.Toinvestigatehowthenumberofprunedex-
pruneotherless‚Äúimportant‚ÄùexpertsinSPHINX-MoE,most
pertswillaffectSPHINX-MoE‚Äôsability,weprunedifferent
abilitycouldbesaved,astheuppertendencyinFigure7.
numbersofexpertsofSPHINX-MoE‚Äôseachlayer,andthe
resultsareshowninFigure7. InFigure7,thex-axismeans
thenumberofretainedexpertsafterpruning. Foreachvalue
onthex-axis, termedas‚Äún‚Äù, werandomlychoose8‚àín
experts in each layer to be pruned in SPHINX-MoE. For
eachn,werunitthreetimesforaverageperformance.
18