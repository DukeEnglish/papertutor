ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars
ZHENWEIWANGâˆ—,CityUniversityofHongKong,China
TENGFEIWANGâ€ ,ShanghaiArtifcialIntelligenceLaboratory,China
GERHARDHANCKE,CityUniversityofHongKong,China
ZIWEILIU,S-Lab,NanyangTechnologicalUniversity,Singapore
RYNSONW.H.LAU,CityUniversityofHongKong,China
Code&video:https://3dthemestation.github.io/
Fig.1. ThemeStationcangenerateagalleryof3Dassets(right)fromjustoneorafewexemplars(left).Thesynthesizedmodelsshareconsistentthemeswith
thereferencemodels,showingtheimmensepotentialofourapproachfortheme-aware3D-to-3Dgenerationandexpandingthescaleofexisting3Dmodels.
Real-worldapplicationsoftenrequirealargegalleryof3Dassetsthatshare
âˆ—WorkdonewheninterningatShanghaiArtificialIntelligenceLaboratory. aconsistenttheme.Whileremarkableadvanceshavebeenmadeingen-
â€ Correspondingauthor. eral3Dcontentcreationfromtextorimage,synthesizingcustomized3D
assetsfollowingthesharedthemeofinput3Dexemplarsremainsanopen
andchallengingproblem.Inthiswork,wepresentThemeStation,anovel
Authorsâ€™addresses:ZhenweiWang,zhenwwang2-c@my.cityu.edu.hk,CityUniver- approachfortheme-aware3D-to-3Dgeneration.ThemeStationsynthesizes
sityofHongKong,HongKongSAR,China;TengfeiWang,tfwang@connect.ust.hk,
customized3Dassetsbasedongivenfewexemplarswithtwogoals:1)unity
ShanghaiArtifcialIntelligenceLaboratory,Shanghai,China;GerhardHancke,gp.
hancke@cityu.edu.hk,CityUniversityofHongKong,HongKongSAR,China;Ziwei forgenerating3Dassetsthatthematicallyalignwiththegivenexemplars
Liu,ziwei.liu@ntu.edu.sg,S-Lab,NanyangTechnologicalUniversity,Singapore;Rynson and2)diversityforgenerating3Dassetswithahighdegreeofvariations.To
W.H.Lau,rynson.lau@cityu.edu.hk,CityUniversityofHongKong,HongKongSAR, thisend,wedesignatwo-stageframeworkthatdrawsaconceptimagefirst,
China.
4202
raM
22
]VC.sc[
1v38351.3042:viXra2 â€¢ ZhenweiWang,TengfeiWang,GerhardHancke,ZiweiLiu,andRynsonW.H.Lau
followedbyareference-informed3Dmodelingstage.Weproposeanovel 3Dmodelingprocess[Bob2022;CGHero2022].Inthefirststage,
dualscoredistillation(DSD)losstojointlyleveragepriorsfromboththe wefine-tuneawell-trainedimagediffusionmodel[Rombachetal.
inputexemplarsandthesynthesizedconceptimage.Extensiveexperiments 2022]onrenderedimagesofthegiven3Dexemplarstoproduce
andauserstudyconfirmthatThemeStationsurpassespriorworksinproduc- diverseconceptimages.Unlikepreviousfine-tuningtechniques[Gal
ingdiversetheme-aware3Dmodelswithimpressivequality.ThemeStation
etal.2022a;Ruizetal.2023]thataresubject-driven,ourgoalisto
alsoenablesvariousapplicationssuchascontrollable3D-to-3Dgeneration.
personalizethepre-traineddiffusionmodelwithaspecifictheme
tosynthesizeimageswithnovelsubjects.Inthesecondstage,we
1 INTRODUCTION
convertthesynthesizedconceptimagesinto3Dmodels.Ourset-
In applications such as virtual reality or video games, we often tingdiffersfromimage-to-3Dtasksinthat(1)weonlyregardthe
needtocreatealargenumberof3Dmodelsthatarethematically conceptimagesasintermediateoutputstoprovideroughguidance
consistentwitheachotherwhilebeingdifferent.Forexample,we ontheoverallstructureandappearanceofthegenerated3Dmodels
mayneedtocreateanentire3Dgalleryofbuildingstoforman and(2)wetaketheinput3Dexemplarsasauxiliaryguidanceto
ancienttownormonsterstoformanecosysteminavirtualworld. provideadditionalgeometryandmulti-viewappearanceinforma-
Whileitiseasyforahighlytrainedcraftsmantocreateoneorafew tion.Toleverageboththesynthesizedconceptimagesandinput3D
coherent3Dmodels,itcanbechallengingandtime-consumingto exemplars(alsoreferredtoasthereferencemodelsinthispaper),
createalarge3Dgallery.Weconsiderifwecanautomatethislabor- weproposereference-informeddualscoredistillation(DSD)to
intensiveprocess,andwhetheragenerativesystemcanproduce guidethe3Dmodelingprocessusingtwodiffusionmodels:one
manyunique3Dmodelsthataredifferentfromeachotherwhile (ConceptPrior)forenforcingcontentfidelityinconceptimage
sharingaconsistentstyle. reconstruction,similarto[Rajetal.2023],andtheother(Reference
Recently,diffusionmodels[Hoetal.2020]haverevolutionized Prior)forreconstructingmulti-viewconsistentfinedetailsfromthe
the3Dcontentcreationtaskbysignificantlyloweringtheamountof exemplars.Insteadofnaivelycombiningthetwolosses,whichmay
manualwork.Thisallowsevenbeginnerstocreate3Dassetsfrom leadtoseverelossconflict,weapplythetwopriorsbasedonthe
textprompts(i.e., text-to-3D)orreferenceimages(i.e., image-to-3D) noiselevels(denoisingtimesteps).Whiletheconceptpriorisapplied
withminimaleffort.Earlyworks[Pooleetal.2023]focusonusing tohighnoiselevelsforguidingthegloballayout,thereferenceprior
well-trained image diffusion models to generate 3D assets from isappliedtolownoiselevelsforguidinglow-levelvariations.
atextpromptwithscoredistillationsampling(SDS).Subsequent Toevaluateourapproach,wehavecollectedabenchmarkthat
works [Melas-Kyriazi et al. 2023; Tang et al. 2023b] extend this containsstylized3Dmodelswithvaryingcomplexity.Asshownin
approachtoenable3Dcreationfromasingleimage.Whilethese Fig.1,ThemeStationcanproduceacreativegalleryof3Dassetscon-
methodshaveshownimpressiveperformances,theystillsufferfrom formingtothethemeoftheinputexemplars.Extensiveexperiments
the3Dambiguityandinconsistencyproblemduetothelimited3D andauserstudyshowthatThemeStationcangeneratecompelling
informationfromtheinputmodality. anddiverse3Dmodelswithmoredetails,evenwithjustasinglein-
Toaddresstheselimitations,inthiswork,weproposetolever- putexemplar.ThemeStationalsoenablesvariousapplications,such
age 3D exemplars as input to guide the 3D generation process. ascontrollable3D-to-3Dgeneration,showingimmensepotentialfor
Givenoneorafewexemplar3Dmodelsasinput(Fig.1left),we generatingcreative3Dcontentandexpandingthescaleofexisting
presentThemeStation,anovelapproachforthetheme-aware3D- 3Dmodels.Ourmaincontributionscanbesummarizedas:
to-3Dgenerationtask,whichaimstogenerateadiverserangeof
â€¢ WeproposeThemeStation,atwo-stageframeworkfortheme-
unique 3D models that are theme-consistent (i.e., semantically
aware3D-to-3Dgeneration,whichaimsatgeneratingnovel
andstylisticallythesame)withtheinputexemplarswhilebeing
3Dassetswithunityanddiversitygivenjustoneorafew
differentfromeachother.Comparedtotextpromptsandimages,
3Dexemplars.
3Dexemplarsofferarichersourceofinformationwithrespectto
â€¢ Wemakeafirstattempttotacklethechallengingproblemof
bothgeometryandappearance,reducingambiguityin3Dmodeling.
extendingdiffusionpriorsfor3D-to-3Dcontentgeneration.
This,inturn,makesitpossibletocreatehigher-quality3Dmodels.
â€¢ We introduce dual score distillation (DSD) to enable the
ThemeStationenablestheautomaticsynthesisof,forexample,a
jointusageoftwoconflicteddiffusionpriorsfor3D-to-3D
groupofbuildings/characterswithasharedtheme(Fig.1right).
generationbyapplyingthereferencepriorandconceptprior
Itaimstosatisfytwogoalsinthe3Dgenerationprocess:unity
atdifferentnoiselevels.
anddiversity.Forunity,weexpectthegeneratedmodelstoalign
withthethemeofthegivenexemplars.Fordiversity,weaimforthe
2 RELATEDWORK
generatedmodelstoexhibitahighdegreeofvariations.
However,wenotethatsimplytrainingagenerativemodelona 3D Generative Models. Remarkable advancements have been
few3Dexemplars[Wuetal.2023;WuandZheng2022]leadstoonly madetogenerativeadversarialnetworks(GANs)anddiffusionmod-
limitedvariation,primarilyrestrictedtoresizingtheinputmodels elsforimagesynthesis[Brocketal.2018;Karrasetal.2019;Rombach
(todifferentscalesandaspectratios)orrepeatingthemrandomly etal.2022;Sahariaetal.2022].Manyresearchershaveexplored
(Fig.6),withoutintroducingsignificantmodificationstotheappear- howtoapplythesemethodstogenerate3Dgeometriesusingdiffer-
anceofthegeneratedmodels.Toaddressthisproblem,wedesign entrepresentations,suchaspointclouds[Nicholetal.2023;Zhou
atwo-stagegenerativeschemetomimicthemanual3Dmodeling etal.2021],meshes[Nashetal.2020;Pavlloetal.2021]andneural
workflowoffirstdrawingaconceptartandthenusingaprogressive fields[Chanetal.2022;ErkoÃ§etal.2023;NiemeyerandGeiger2021].ThemeStation:GeneratingTheme-Aware3DAssetsfromFewExemplars â€¢ 3
Fig.2. OverviewofThemeStation.Givenjustone(thisfigure)orafewreferencemodels(exemplars),ourapproachcangeneratetheme-consistent3Dmodelsin
twostages.Inthefirststage,wefine-tuneapre-trainedtext-to-image(T2I)diffusionmodeltoformacustomizedtheme-drivendiffusionmodelthatproduces
variousconceptimages.Inthesecondstage,weconductreference-informed3Dassetmodelingbyprogressivelyoptimizingaroughinitialmodel(omittedin
thisfigureforbrevity),whichisobtainedusinganoff-the-shelfimage-to-3Dmethodgiventheconceptimage,intoafinalgeneratedmodelwithfinerdetails.
Weuseanoveldualscoredistillation(DSD)lossforoptimization,whichappliesconceptpriorandreferenceprioratdifferentnoiselevels(denoisingtimesteps).
Recentworkscanfurthergenerate3Dtexturedshapes[Chenetal. extendtheideaof2DSinGAN[Shahametal.2019]totraina3D
2023b;Guptaetal.2023;Hongetal.2024;JunandNichol2023;Tang generativemodel[Wuetal.2023;WuandZheng2022]withasingle
etal.2024;Wangetal.2023b].Thesemethodsrequirealarge3D 3Dexemplar.Somemethods[Lietal.2023]liftclassic2Dpatch-
datasetfortraining,whichlimitstheirperformanceonin-the-wild basedframeworksto3Dgenerationwithouttheneedforoffline
generation. training.Whilethesemethodssupport3Dvariationsofsizesand
DiffusionPriorsfor3DGeneration.Dreamfusion[Pooleetal. aspectratios,theydonotunderstandandpreservethesemanticsof
2023]proposedtodistillthescoreofimagedistributionfromapre- the3Dexemplars.Asaresult,theirresultsareprimarilyrestricted
trainedtext-to-image(T2I)diffusionmodelandshowpromising toresizing,repeating,orreorganizingtheinputexemplarsinsome
resultsintext-to-3Dgeneration.Subsequentworksenhancethe way(Fig.6),whichisdifferentfromoursettingthataimstoproduce
scoredistillationscheme[Pooleetal.2023]andachievehighergen- theme-consistent3Dvariations.
erativequalityfortext-to-3Dgeneration[Chenetal.2023a;Linetal.
2023;Metzeretal.2023].Somerecentworksalsoapplythediffusion 3 APPROACH
priorstoimage-to-3Dgeneration[Chenetal.2024;Melas-Kyriazi
Ourframeworkisdesignedtofollowthereal-worldworkflowof
etal.2023;Sunetal.2023;Tangetal.2023a,b].Toenhancemulti-
3Dmodelingbyintroducingaconceptartdesignstepbeforethe
viewconsistencyofthegenerated3Dcontent,someresearchers
3Dmodelingprocess.AsillustratedinFig.2,wefirstcustomizea
seektofine-tunethepre-trainedimagediffusionmodelswithmulti-
pre-trainedtext-to-image(T2I)diffusionmodeltoproduceaseries
viewdatasetsforconsistentmulti-viewimagegeneration[Liuetal.
ofconceptimagesthatshareaconsistentthemeastheinputex-
2023b,a;Longetal.2023;Yichunetal.2023].Althoughdiffusion
emplars,mimickingtheconceptartdesigningprocessinpractice
priorshaveshowngreatpotentialfor3Dcontentgenerationfrom
(Sec.3.1).Wethenutilizeanoptimization-basedmethodtolifteach
textorimageinputs,theirapplicabilityto3Dcustomizationbased
conceptimagetoafinal3Dmodel,followingthepracticalmodel-
on3Dexemplarsisstillanopenandchallengingproblem.
ingworkflowofpushingabaseprimitiveintoawell-crafted3D
Exemplar-BasedGeneration.Theexemplar-based2Dimagegen-
model(Sec.3.2).Tothisend,wepresentnoveldualscoredistillation
erationtaskhasbeenwidelyexplored[Avrahamietal.2023;Galetal.
(DSD)thatleveragesthepriorsofboththeconceptimagesandthe
2022b;Ruizetal.2023].Recently,DreamBooth3D[Rajetal.2023]
exemplarsintheoptimizationprocess(Sec.3.3).
fine-tunespre-traineddiffusionmodelswithonlyafewimagesto
achievesubject-driventext-to-3Dgenerationbutstillsuffersincon-
3.1 StageI:Theme-DrivenConceptImageGeneration
sistencyduetothelackof3Dinformationfromtheinputimages.
Anotherlineofworktakes3Dexemplarsasinputtogenerate3D Conceptimagedesignisavisualtooltoconveytheideaandpreview
variations.Forexample,assembly-basedmethods[Chaudhurietal. thefinal3Dmodel.Itisusuallythefirststepinthe3Dmodeling
2011;Kimetal.2013;Schoretal.2019;Xuetal.2012;Zhengetal. workflow and serves as a bridge between the designer and the
2013]focusonretrievingcompatiblepartsfromacollectionof3D modeler[Bob2022;CGHero2022].Followingthispractice,inthis
examplesandorganizingthemintoatargetshape.Somemethods stage,ourgoalistogenerateavarietyofconceptimages{ğ’™ğ‘}ofa4 â€¢ ZhenweiWang,TengfeiWang,GerhardHancke,ZiweiLiu,andRynsonW.H.Lau
specificthemebasedontheinputexemplars{ğ’ğ‘Ÿ},asshowninFig.2
top.Whiletherearesomeexistingworksonsubject-drivenimage
generation[Galetal.2022a;Ruizetal.2023],whichfine-tuneapre-
trainedT2Idiffusionmodel[Rombachetal.2022]togeneratenovel
contextsforaspecific(exactlythesame)subject,theyarenotaligned
withourtheme-drivensetting.Ourgoalistogenerateadiverseset
ofsubjectsthatexhibitthematicconsistencybutdisplaycontent
variationsrelativetotheexemplars.Thus,insteadofstimulating
thesubjectretentioncapabilityofthepre-traineddiffusionmodel
throughoverfittingtheinputs,weseektopreserveitsimaginative
capabilitywhilepreservingthethemeoftheinputexemplars.
Weobservethatthediffusionmodel,fine-tunedwithfeweriter-
ationsontherenderedimages{ğ’™ğ‘Ÿ}oftheinputexemplars{ğ’ğ‘Ÿ}, Fig.3. Comparisonofthekeyideasbetweenimagestyletransfer(top)and
isalreadyabletolearnthethemeoftheexemplars.Hence,itis ourdualscoredistillation(bottom).ImagesarefromGatysetal.[2016](top)
abletogeneratenovelsubjectsthatarethematicallyinlinewith andDibia[2022](bottom).
theinputexemplars.Tofurtherdisentanglethetheme(semantics
andstyle)andthecontent(subject)oftheexemplars,weexplicitly
Preliminaries.DreamFusionachievestext-to-3Dgenerationby
indicatethelearningofthethemeusingasharedtextpromptacross
optimizinga3Drepresentationwithparameterğœƒ sothattheran-
allexemplars,e.g.,â€œa3Dmodelofanowl,inthestyleof[V]â€,during
domlyrenderedimagesğ’™ =ğ‘”(ğœƒ)underdifferentcameraposeslook
thefine-tuningprocess.
like2Dsamplesofapre-trainedT2Idiffusionmodelforagiventext
promptğ‘¦.Here,ğ‘”isaNeRF-likerenderingengine.TheT2Idiffu-
3.2 StageII:Reference-Informed3DAssetModeling
sionmodelğœ™ worksbypredictingthesamplednoiseğœ–
ğœ™
(ğ’™ğ‘¡;ğ‘¦,ğ‘¡)
Givenonesynthesizedconceptimageğ’™ğ‘ andtheinputexemplars ofarenderedviewğ’™ğ‘¡ atnoiselevelğ‘¡ foragiventextpromptğ‘¦.
{ğ’ğ‘Ÿ},weconductreference-informed3Dassetmodelinginthesec- Tomoveallrenderedimagestoahigherdensityregionunderthe
ondstage.Similartotheworkflowofpractical3Dmodelingthat text-conditioneddiffusionprior,scoredistillationsampling(SDS)
startswithabaseprimitive,webeginwitharoughinitial3Dmodel estimatesthegradientforupdatingğœƒ as:
ğ’ğ‘–ğ‘›ğ‘–ğ‘¡,generatedusingoff-the-shelfimage-to-3Dtechniques[Liu (cid:20) (cid:16) (cid:17) ğœ•ğ’™(cid:21)
etal.2023c,a;Longetal.2023]giventheconceptimageğ’™ğ‘,toac- âˆ‡ğœƒL SDS(ğœ™,ğ‘¥)=E ğ‘¡,ğœ– ğœ”(ğ‘¡) ğœ– ğœ™ (ğ’™ğ‘¡;ğ‘¦,ğ‘¡)âˆ’ğœ– ğœ•ğœƒ , (1)
celerateour3Dassetmodelingprocess.Asthesynthesizedconcept
image,alongwiththeinitial3Dmodel,mayhaveinconsistentspatial
whereğœ”(ğ‘¡)isaweightingfunction.
structuresandunsatisfactoryartifacts,wedonotenforceourfinal FollowingSDS,variationalscoredistillation(VSD)[Wangetal.
generatedmodeltobestrictlyalignedwiththeconceptimage.We 2023a]furtherimprovesgenerationdiversityandquality,which
thentaketheconceptimageandtheinitialmodelasintermediate regardsthetext-conditioned3Drepresentationasarandomvariable
outputsandmeticulouslydeveloptheinitialmodelintothefinal ratherthanasingledatapointinSDS.Thegradientiscomputedas:
generated3Dmodelğ’ğ‘œ.Differentfrompreviousoptimization-based (cid:20) (cid:16) (cid:17) ğœ•ğ’™(cid:21)
methodsthatperformscoredistillationsamplingusingasinglediffu- âˆ‡ğœƒL VSD=E ğ‘¡,ğœ– ğœ”(ğ‘¡) ğœ– ğœ™ (ğ’™ğ‘¡;ğ‘¦,ğ‘¡)âˆ’ğœ– lora(ğ’™ğ‘¡;ğ‘¦,ğ‘¡,ğ‘) ğœ•ğœƒ , (2)
sionmodel[Pooleetal.2023;Wangetal.2023a],weproposeadual
whereğ‘ isthecameraparameter,andğœ– computesthescoreof
scoredistillation(DSD)losstoleveragetwodiffusionpriorsas lora
noisyrenderedimagesbyalow-rankadaption(LoRA)[Huetal.
guidancesimultaneously.Here,onediffusionmodel,denotedasğœ™ ğ‘,
2021]ofthepre-trainedT2Idiffusionmodel.Despitethepromising
functionsasthebasicconcept(conceptprior),providingdiffusion
quality,bothVSDandSDSmainlyworkondistillingtheunitary
priorsfromtheconceptimageğ’™ğ‘ toensureconceptreconstruction,
priorfromasinglediffusionmodelandmaycollapsewhenencoun-
whiletheother,denotedasğœ™ ğ‘Ÿ,operatesasanadvisoryreference
teringmixedpriorsfromconflicteddiffusionmodels.
(referenceprior),generatingdiffusionpriorspertinenttothein-
Learningofconceptprior.Tolearnconceptprior,weleverage
putreferencemodels{ğ’ğ‘Ÿ}toassistwithrestoringsubtlefeatures
notonlytheconceptimageitselfbutalsothe3Dconsistentinforma-
andalleviatingmulti-viewinconsistency.Wefurtherpresentaclear
tioninitsinitial3Dmodelğ’ğ‘–ğ‘›ğ‘–ğ‘¡.Weobservethattheinitialmodel
designofourDSDlossinSec.3.3.
suffersblurrytextureandover-smoothedgeometry,whichisinsuf-
ficienttoprovideahigh-qualityconceptprior.Thus,weaugment
3.3 DualScoreDistillation
theinitialrenderedviews{ğ’™ğ‘–ğ‘›ğ‘–ğ‘¡}ofğ’ğ‘–ğ‘›ğ‘–ğ‘¡ intoaugmentedviews
Inthissubsection,weelaborateonthecriticalcomponentofour {ğ’™ğ‘–Ë†ğ‘›ğ‘–ğ‘¡},i.e.,{ğ’™ğ‘–Ë†ğ‘›ğ‘–ğ‘¡}=ğ‘({ğ’™ğ‘–ğ‘›ğ‘–ğ‘¡}),whereğ‘(Â·)istheimage-to-image
approach,dualscoredistillation(DSD)fortheme-aware3D-to-3D translationoperation,similarto[Rajetal.2023].Theseaugmented
generation.DSDcombinesthebestofbothpriors,conceptprior viewsserveaspseudo-multi-viewimagesoftheconceptualsub-
andreferenceprior,toguidethegenerationprocess.Bothpriorsare ject,providingadditional3Dinformationforfurther3Dmodeling.
derivedthroughfine-tuningapre-trainedT2Idiffusionmodel.Next, Finally,thediffusionmodelğœ™ ğ‘ withconceptpriorisderivedbyfine-
wediscussthepreliminariesandshowthestepsoflearningthetwo tuningaT2Idiffusionmodelgiven{ğ‘¥ ğ‘,{ğ’™ğ‘–Ë†ğ‘›ğ‘–ğ‘¡},ğ‘¦},whereğ‘¦isthe
priorsandthedesignofDSDloss. textpromptwithaspecialidentifier,e.g.,â€œa3Dmodelof[V]owlâ€.ThemeStation:GeneratingTheme-Aware3DAssetsfromFewExemplars â€¢ 5
Learningofreferenceprior.Tolearnreferenceprior,weleverage imagepriorandnormalpriorfromthereferencemodels.Thegradi-
boththecolorimages{ğ’™ğ‘Ÿ}andthenormalmaps{ğ’ğ‘Ÿ}renderedfrom entgiventhereferenceprioris:
thereferencemodels{ğ’ğ‘Ÿ}underrandomviewpoints.Whiletheren-
d the ere rd enco dl eo rr ei dm na og re msm ala min aly pspr foov cuid se o3 nD ec no cn os dis inte gnt dp etr aio ilr es do gn et oe mxt eu tr re is c, âˆ‡ğœƒL ref(ğœ™ ğ‘Ÿ,ğ‘¡ ğ‘™)=E ğ‘¡ğ‘™,ğœ– (cid:20) ğœ”(cid:16) ğœ– ğœ™ğ‘Ÿ (cid:0) ğ’™ğ‘¡ğ‘™;ğ‘¦ ğ‘¥,ğ‘¡ ğ‘™(cid:1)âˆ’ğœ– lora(cid:17) ğœ• ğœ•ğ’™ ğœƒ(cid:21)
(4)
i tn of bo urm ildat ui pon a. mT oh re ej co oin mt pu rs ea hg ee no sif vt eh re es fe ertw eno ceki pn rd ios ro ff or re in nd tre or din ug cs inh gel 3p Ds +E
ğ‘¡ğ‘™,ğœ–
(cid:20) ğœ”(cid:16) ğœ–
ğœ™ğ‘Ÿ
(cid:0) ğ’ğ‘¡ğ‘™;ğ‘¦ ğ‘›,ğ‘¡ ğ‘™(cid:1)âˆ’ğœ– lora(cid:17) ğœ• ğœ•ğ’™ ğœƒ(cid:21) ,
consistentdetailsduringoptimization.Todisentanglethelearning
ofimagepriorandnormalprior,wealsoincorporatedifferenttext whereğ’™ğ‘¡ğ‘™ andğ’ğ‘¡ğ‘™ aretherenderedcolorimageandnormalmap
prompts,ğ‘¦ ğ‘¥ andğ‘¦ ğ‘›,forcolorimages,e.g.,â€œa3Dmodelofanowl,in atlownoiselevelğ‘¡ ğ‘™,andğ‘¦ ğ‘¥ andğ‘¦ ğ‘› aretheircorrespondingtext
thestyleof[V]â€,andnormalmaps,e.g.,â€œa3Dmodelofanowl,inthe prompts.Finally,thegradientofourDSDlossis:
styleof[V],normalmapâ€,respectively.Finally,thediffusionmodel
ğœ™ ğ‘Ÿ withreferencepriorisderivedbyfine-tuningapre-trainedT2I âˆ‡ğœƒL DSD=ğ›¼âˆ‡ğœƒL concept(ğœ™ ğ‘,ğ‘¡ â„)+ğ›½âˆ‡ğœƒL ref(ğœ™ ğ‘Ÿ,ğ‘¡ ğ‘™), (5)
diffusionmodelgiven{{ğ’™ğ‘Ÿ},ğ‘¦ ğ‘¥,{ğ’ğ‘Ÿ},ğ‘¦ ğ‘›}.Althoughweconvertthe
whereğ›¼ andğ›½areweightstobalancethestrengthoftwoguidance.
3Dreferencemodelsinto2Dspace,their3Dinformationhasstill
beenimplicitlyreservedacrosstheconsistentmulti-viewrendered
4 EXPERIMENTS
colorimagesandnormalmaps.Besides,asthepre-trainedT2Idif-
fusionmodelshavebeenshowntopossessrich2Dand3Dpriors Weshowthegenerated3Dmodelsbasedonafew3Dexemplars
aboutthevisualworld[Liuetal.2023c],wecanalsoinheritthese inFig.11andFig.13.Wecanseethatourapproachcangenerate
priorstoenhanceourmodelingqualitybyprojectingthe3Dinputs variousnovel3Dassetsthatshareconsistentthemeswiththeinput
into2Dspace. exemplars.Thesegenerated3Dassetsexhibitelaboratetextureand
Howdoesdualscoredistillationwork?Astraightforwardaggre- geometry,readyforreal-worldusage(Fig.1).Ourapproachcan
gationofthesetwopriorsisperformingthevanillascoredistillation evenworkwithonlyoneexemplar,asshowninFig.10andFig.12.
samplingtwiceindiscriminatelyforbothdiffusionmodelsğœ™ ğ‘ and Fortherestofthissection,wefirstconductexperimentsandauser
ğœ™ ğ‘Ÿ andsummingupthelosses.However,thisnaivestackoftwo studytocompareourresultswiththosegeneratedbythestate-
priorsleadstolossconflictsduringoptimizationandgeneratesdis- of-the-artmethods.Wealsoconductexperimentstoanalyzethe
tortedresults((b)ofFig.7).Toresolvethis,weintroduceadual effectivenessofseveralimportantdesignchoicesofourapproach.
scoredistillation(DSD)loss,whichappliesthetwodiffusionpriors Weshowimplementationdetailsandapplicationsofcontrollable
atdifferentnoiselevels(denoisingtimesteps)duringthereverse generationinsupplementarymaterials.
diffusionprocess.
4.1 ComparisonswithState-of-the-ArtMethods
Thismethodisbasedonourobservationthatthereisacoarse-to-
finetimestep-baseddynamicduringthereversediffusionprocess. 4.1.1 Benchmark. Wehavecollectedadatasetof66referencemod-
High noise levels, i.e., the early denoising timesteps ğ‘¡ â„, control elscoveringabroadrangeofthemes.These3Dmodelscomprise
thegloballayoutandroughcolordistributionoftheimagebeing threemaincategories,including15dioramas,25individualobjects,
denoised.Asthereversediffusiongraduallygoesintolownoise and26characters,suchassmallislands,buildings,andcharacters,
levels,i.e.,thelatedenoisingtimestepsğ‘¡ ğ‘™,high-frequencydetails asshowninFig.10-13.Modelsinthisdatasetareexportedfrom
aregenerated.Thisintriguingtimestep-baseddynamicprocessof thebuilt-in3DlibraryofMicrosoft3DViewerordownloadedfrom
T2Idiffusionmodelsisincrediblyinlinewiththefunctionalities Sketchfab1.Thetextpromptsforeach3Dmodelareautomatically
ofourconceptpriorandreferencepriors.Inspiredbyimagestyle generatedbyfeedingthemodelâ€™ssubjectname,i.e.,filenamein
transfer[Gatysetal.2016]thatleveragesdifferentlayersofapre- mostcases,intothepre-definedpatternspresentedinSec.3.
trainedneuralnetworktocontroldifferentlevelsofimagecontent,
asshowninFig.3,weapplytheconceptpriorğœ™ ğ‘athighnoiselevels 4.1.2 ComparisonMethods. Tothebestofourknowledge,weare
ğ‘¡ â„ toenforcetheconceptfidelitybyadjustingthelayoutandcolor thefirstworkfocusingontheme-aware3D-to-3Dgenerationwith
holistically,andapplythereferencepriorğœ™ ğ‘Ÿ atlownoiselevelsğ‘¡ ğ‘™ diffusionpriors. Asnoexistingmethodscansimultaneouslytake
torecoverthefinerelementsindetail. bothimageand3Dmodelasinputs,wecompareourmethodwith
BasedonEq.2,thegradientforupdatingthe3Drepresentationğœƒ sevenbaselinemethodsfromtwoaspects.Ontheonehand,we
ofthemodelbeingoptimizedgiventheconceptprioris: comparewithfiveimage-to-3Dmethods,includingmulti-view-
âˆ‡ğœƒL concept(ğœ™ ğ‘,ğ‘¡ â„)=E ğ‘¡â„,ğœ– (cid:20) ğœ”(cid:16) ğœ– ğœ™ğ‘ (cid:0) ğ’™ğ‘¡â„;ğ‘¦,ğ‘¡ â„(cid:1)âˆ’ğœ– lora(cid:17) ğœ• ğœ•ğ’™ ğœƒ(cid:21) , (3) b etas ae l.d 2, 0i 2.e 3., aW ],fo en ed de -r f3 oD rw[ aL ro dn ,g i.ee .t ,La Rl. M20 [2 H3] o, nS gyn etcD alr .e 2a 0m 23e ]r ,( SS hy an pc eD -. E) [[ JL ui nu
andNichol2023],andoptimization-based,i.e.,Magic123[Qianetal.
whereğœ”isaweightingfunction,ğœ– ğœ™ğ‘ (cid:0) ğ’™ğ‘¡â„;ğ‘¦,ğ‘¡ â„(cid:1) isthesamplednoise 2023],toevaluateoursecondstagethatliftsaconceptimagetoa3D
oftherenderedcolorimageğ’™ğ‘¡â„ athighnoiselevelğ‘¡ â„ conditioned model.DuetotheunavailablecodeofLRM,weuseitsopen-source
onpromptğ‘¦,andğœ– ğ‘™ğ‘œğ‘Ÿğ‘isthescoreofnoisyrenderedimagesparam- reproductionOpenLRM [HeandWang2023].Ontheotherhand,
eterizedbyaLoRAofapre-traineddiffusionmodel.Forreference wealsocomparewithtwo3Dvariationmethods:Sin3DM [Wu
prior,weapplyitonbothrenderedcolorimagesandnormalmapsto etal.2023]andSin3DGen[Lietal.2023],toevaluatetheoverall
jointlyrecoverthedetailedtextureandgeometrywiththelearned 3D-to-3Dperformanceofourmethod.6 â€¢ ZhenweiWang,TengfeiWang,GerhardHancke,ZiweiLiu,andRynsonW.H.Lau
Table1. Quantitativecomparisonwithpriorimage-to-3Dmethods.
Wonder3D OpenLRM SyncD. Shap-E Magic123 Ours
CLIPâ†‘ 0.777 0.840 0.803 0.761 0.868 0.890
Contextualâ†“ 3.206 4.137 4.189 3.399 3.345 3.168
Table2. Quantitativecomparisonwithprior3Dvariationmethods.
Sin3DM Sin3DGen Ours Fig.4. Resultsoftheuserstudy.Wecompareourmethodwithsevenbaseline
VisualDiversityâ†‘ 0.180 0.201 0.315 methodsusing2AFCpairwisecomparisons.Allpreferencesarestatistically
GeometryDiversityâ†‘ 0.344 0.634 0.465 significant(ğ‘<0.05,chi-squaredtest).
VisualQualityâ†‘ 5.221 5.127 5.848
GeometryQualityâ†‘ 5.638 5.607 5.616
ofthetwomodelsdoyouprefer(e.g.,higherqualityandmorede-
tails)onthepremiseofaligningwiththeinputview?"For3D-to-3D,
weshowtwosetsofgenerated3Dvariationsbesideareference
4.1.3 QuantitativeResults. Forimage-to-3D,asourapproachis
modelandaskthequestion:â€œWhichofthetwosetsdoyouprefer
not targeted to strictly reconstruct the input view, we focus on
(e.g.,higherqualityandmorediversity)onthepremiseofsharing
evaluating the semantic coherence between the input view and
consistentthemeswiththereference?"WecanseefromFig.4that
randomlyrenderedviewsofgeneratedmodels.Thus,weadopttwo
ourapproachsignificantlyoutperformsexistingmethodsinboth
metrics:1)CLIP score[Radfordetal.2021]tomeasuretheglobal
image-to-3Dand3D-to-3Dtasksintermsofhumanpreferences.
semanticsimilarity,and2)Contextualdistance[Mechrezetal.2018]
toestimatethesemanticdistanceatthepixellevel.Bothmetricsare 4.1.5 QualitativeResults. Forimage-to-3Dcomparison(Fig.5),we
commonlyusedinimage-to-3D[Sunetal.2023;Tangetal.2023b]. canseethatShap-E,SyncDreamer,andOpenLRMsufferfromlower
For3D-to-3D,weusethepairwiseIoUdistance(1-IoU)amonggen- qualitywithincompleteshape,blurryappearance,andmulti-view
eratedmodelsandtheaverageLPIPSscoreacrossdifferentviewsto inconsistency.ResultsofWonder3DandMagic123cangenerate3D
measuretheVisualDiversityandGeometryDiversity,respectively.To consistentmodelswithhigherquality.However,Wonder3Dstill
measuretheVisualQualityandGeometryQuality,weusetheLAION generatesvaguetextureandincompleteshape,e.g.,theseveredtail
aestheticspredictor2topredictthevisualandgeometryaesthetics ofthetriceratops,andMagic123hasproblemswithoversaturation
scoresgiventhemulti-viewrenderedimages(visual)andnormal andoversmooth.Allbaselinemethodslackdelicatedetails,espe-
maps(geometry).ThequantitativeresultsinTab.1andTab.2show ciallyinnovelviews,e.g.,epidermalfoldsinthelastline.Incontrast,
thatourapproachsurpassesthebaselinesingenerativediversity, oursgeneratesmulti-viewconsistent3Dmodelswithfinerdetails
qualityandmulti-viewsemanticcoherency.Sin3DGengenerates ingeometryandtexture.
variationsatthepatchlevel,achievinghighergeometrydiversity. For3D-to-3Dcomparison(Fig.6),wecanseethatthebaseline
Sin3DMgeneratesvariationsviaadiffusionmodeltrainedwith methodstendtorandomlyresize,repeat,orreorganizetheinput,
onlyoneexemplar,achievinghighergeometryquality.However, whichmayproduceweirdresults,e.g.,multi-headcharacterand
bothmethodstendtooverfittheinputandgeneratemeaninglessly stumpabovetreetop.Duetotheirtheme-unaware3Drepresenta-
repeatedorreorganizedcontentswithlowervisualdiversityand tionlearnedfromjustafewexemplars,itishardforthemtopre-
quality(Fig.6).Incontrast,oursgeneratestheme-consistentnovel serveorevenunderstandthesemanticsoftheinput3Dexemplars.
3Dassetswithdiverseandplausiblevariationsintermsofboth Instead,ourapproachcombinespriorsfrominput3Dexemplars
geometryandtexture. andpre-trainedT2Idiffusionmodels,yieldingdiversesemantically
meaningful3Dvariationsthatexhibitsignificantmodificationson
4.1.4 UserStudy. Themetricsusedabovemainlymeasuretheinput-
contentwhilethematicallyaligningwiththeinputexemplars.
outputsimilarityandpixel/voxel-leveldiversity,whicharenotable
topresenttheoverallperformanceofdifferentmethods.Wethus 4.2 AblationStudy
conductauserstudytoestimatereal-worlduserpreferences.
4.2.1 Settings. To evaluate the effectiveness of our key design
Werandomlyselect20modelsfromourdatasetandgenerate3
choices,weconductablationstudiesonfivesettings:(a)Baseline,
variationsforeachmodel.Weinviteatotalof30users,recruited
whichonlyusesconceptprioracrossallnoiselevels,(b)+Ref.prior
publicly,tocompleteaquestionnaireconsistingof30pairwisecom-
naive,whichnaivelyappliesconceptpriorandreferenceprioracross
parisons(15forimage-to-3Dand15for3D-to-3D)inperson,totaling
allnoiselevels,(c)+Ref.priorDSD(fullmodel),whichappliescon-
900answers.Forimage-to-3D,weshowtwogenerated3Dmod-
ceptpriorathighnoiselevelsandreferenceprioratlownoiselevels,
els(onebyourmethodandonebythebaselinemethod)besidea
(d)ReverseDSD,whichreversesthechoiceofnoiselevelsbyapply-
conceptimageandasktheuserstoanswerthequestion:â€œWhich
ingconceptprioratlownoiselevelsandreferencepriorathigh
1https://sketchfab.com/ noiselevels,and(e)Ref.dominated,whichappliesconceptprior
2https://laion.ai/blog/laion-aesthetics/ athighnoiselevelsandreferenceprioracrossallnoiselevels.WeThemeStation:GeneratingTheme-Aware3DAssetsfromFewExemplars â€¢ 7
Fig.5. Qualitativecomparisonswithfiveimage-to-3Dmethodstoevaluateoursecondstagethatliftsaconceptimagetoa3Dmodel.Weshowthefrontal
viewasprimaryforthefirstlineandshowthebackviewasprimaryforthelasttwolines.Comparedwithbaselinemethods,oursproduces3Dmodelswith
finerdetailsinbothgeometryandtexture,especiallyonnovelviews.
Fig.6. Qualitativecomparisonswithtwo3Dvariationmethodstoevaluatetheoverallgenerativediversityandqualityofourmethod.Foreachcase,weshow
threegenerated3Dmodels.Duetothetheme-unawarefeatureofbaselinemethods,theyonlyproduceresized,repeated,orreorganizedresults.Instead,ours
generatestheme-consistentnovel3Dsubjectswithdiverseandplausiblegeometryandtexture.
Table3. Quantitativeresultsofablationstudy.
measurethesemanticcoherence,visualqualityandgeometryqual-
ityasinimage-to-3Dand3D-to-3Dcomparisonsfortheablation
study.ThequantitativeandqualitativeresultsareshowninTab.3 Baseline +Ref. +Ref. Reverse Ref.
andFig.7. naive DSD DSD dominated
CLIPâ†‘ 0.877 0.876 0.890 0.863 0.874
4.2.2 EffectofreferencepriorandDSDloss. Asshownin(a)and(c) Contextualâ†“ 3.182 3.177 3.168 3.186 3.179
ofFig.7andTab.3,itisevidentthattheintroductionofreference
VisualQualityâ†‘ 5.639 5.726 5.848 5.578 5.701
priorandDSDsignificantlyenhancesthemodelqualityinterms
GeometryQualityâ†‘ 4.789 5.336 5.616 4.926 5.296
ofsemanticcoherence,textureandgeometry.From(b)ofFig.7,
wecanseethenaivecombinationofreferencepriorandconcept
priorresultsinseverelossconflictandproducesbumpysurfaceand
(Sec.3.3).Besides,bycomparing(c)with(e)inFig.7,wecanseethat
blurrytexture,whichfurthershowstheeffectivenessofourDSD
extendingthenoiselevelsforreferencepriorhasnopositiveeffect
foralleviatinglossconflicts.
butleadstoaworseresult,indicatingthatthedesignofseparating
4.2.3 EffectofthechoicesofnoiselevelsforDSDloss. Bycomparing twopriorsatdifferentnoiselevelscanhelpreducelossconflict.
(c)with(d)inFig.7,wecanseeasignificantperformancedegrada-
5 CONCLUSION
tionafterreversingthenoiselevels,whichprovesourclaimthatthe
timestep-baseddynamicprocessofT2Idiffusionmodelsisconsis- Inthiswork,weproposedThemeStation,anovelapproachforthe
tentwiththefunctionalitiesofourconceptpriorandreferenceprior theme-aware3D-to-3Dgenerationtask.Givenjustoneorafew3D8 â€¢ ZhenweiWang,TengfeiWang,GerhardHancke,ZiweiLiu,andRynsonW.H.Lau
Fig.7. Ablationstudyontwotypesofeffects:(1)referencepriorandDSDloss(Sec.4.2.2)and(2)thechoiceofnoiselevelsforDSDloss(Sec.4.2.3).(a)without
referenceprior;(b)anaivecombinationofconceptpriorandreferenceprior;(c)usingtheproposeddualscoredistillation(DSD);(d)reversingthechoiceof
noiselevelsforDSD;(e)extendingreferencepriortoallnoiselevelsforDSD.Weshowthebackviewforeachcase.Pleasezoominfordetails.
floaters.Trainingafeed-forwardtheme-aware3D-to-3Dgeneration
modelisapotentialsolution,whichweleaveasaninterestingfuture
work.FailurecasesareshowninFig.8.
REFERENCES
OmriAvrahami,KfirAberman,OhadFried,DanielCohen-Or,andDaniLischinski.
2023. Break-A-Scene:ExtractingMultipleConceptsfromaSingleImage. arXiv
preprintarXiv:2305.16311(2023).
Fig.8. Failurecases.(a)Ourapproachmayfailtofixhugeconcepterrors
Bob.2022.3DModeling101:ComprehensiveBeginnersGuide. RetrievedJan03,2024from
whentheconceptimagecontainssignificantartifactsormistakes,e.g.,the https://wow-how.com/articles/3d-modeling-101-comprehensive-beginners-guide
tailgrowsinfrontofthebody.(b)Ourapproachmayfailtogenerateperfect AndrewBrock,JeffDonahue,andKarenSimonyan.2018.LargescaleGANtrainingfor
3Dmodelsofregularshapes,suchasaâ€œMinecraftâ€buildingwithcubic highfidelitynaturalimagesynthesis.arXivpreprintarXiv:1809.11096(2018).
TimBrooks,AleksanderHolynski,andAlexeiAEfros.2023.Instructpix2pix:Learning
regularization,duetothelackofexplicitgeometryconstraints.
tofollowimageeditinginstructions.InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition.18392â€“18402.
CGHero.2022. TheStagesofCreatinga3DModel. RetrievedJan02,2024from
https://cghero.com/articles/stages-of-creating-3d-model
exemplars,weaimtogenerateagalleryofuniquetheme-consistent
EricRChan,ConnorZLin,MatthewAChan,KokiNagano,BoxiaoPan,Shalini
3Dmodels.ThemeStationachievesthisgoalfollowingatwo-stage DeMello,OrazioGallo,LeonidasJGuibas,JonathanTremblay,SamehKhamis,etal.
generativeschemethatfirstdrawsaconceptimageasroughguid- 2022.Efficientgeometry-aware3Dgenerativeadversarialnetworks.InCVPR.
SiddharthaChaudhuri,EvangelosKalogerakis,LeonidasGuibas,andVladlenKoltun.
anceandthenconvertsitintoa3Dmodel.Our3Dmodelingprocess 2011.Probabilisticreasoningforassembly-based3Dmodeling.InACMSIGGRAPH
involvestwopriors,onefromtheinput3Dexemplars(reference 2011papers.1â€“10.
HanshengChen,JiataoGu,AnpeiChen,WeiTian,ZhuowenTu,LingjieLiu,andHao
prior)andtheotherfromtheconceptimage(conceptprior)gener-
Su.2023b.Single-StageDiffusionNeRF:AUnifiedApproachto3DGenerationand
atedinthefirststage.Adualscoredistillation(DSD)lossfunction Reconstruction.arXivpreprintarXiv:2304.06714(2023).
isproposedtodisentanglethesetwopriorsandalleviatelosscon- RuiChen,YongweiChen,NingxinJiao,andKuiJia.2023a.Fantasia3D:Disentangling
geometryandappearanceforhigh-qualitytext-to-3Dcontentcreation. arXiv
flict.Wehaveconductedauserstudyandextensiveexperimentsto
preprintarXiv:2303.13873(2023).
validatetheeffectivenessofourapproach. YongweiChen,TengfeiWang,TongWu,XingangPan,KuiJia,andZiweiLiu.2024.
Limitationsandfailurecases.WhileThemeStationproduces ComboVerse:Compositional3DAssetsCreationUsingSpatially-AwareDiffusion
Guidance.
high-quality3Dassetsgivenjustoneorafew3Dexemplarsand VictorDibia.2022.LatentDiffusionModels:ComponentsandDenoisingSteps. Retrieved
opensupanewvenuefortheme-aware3D-to-3Dgeneration,itstill Jan04,2024fromhttps://victordibia.com/blog/stable-diffusion-denoising/
ZiyaErkoÃ§,FangchangMa,QiShan,MatthiasNieÃŸner,andAngelaDai.2023.Hyper-
hasseverallimitationsforfurtherimprovement.Whileuserscan
Diffusion:GeneratingImplicitNeuralFieldswithWeight-SpaceDiffusion.
obtainaconceptimageandacorrespondinginitialmodelinminutes, RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitH.Bermano,GalChechik,
similartoprioroptimization-based3Dgenerationmethods,itstill andDanielCohen-Or.2022a.AnImageisWorthOneWord:PersonalizingText-to-
ImageGenerationusingTextualInversion.
takeshoursforourcurrentpipelinetooptimizetheinitialmodel
RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHBermano,GalChechik,
intoafinal3Dassetwithfinerdetails.Webelieveadvanceddiffusion andDanielCohen-Or.2022b.Animageisworthoneword:Personalizingtext-to-
modelsandneuralrenderingtechniquesinthefuturecanhelpto imagegenerationusingtextualinversion.arXivpreprintarXiv:2208.01618(2022).
LeonAGatys,AlexanderSEcker,andMatthiasBethge.2016.Imagestyletransferusing
alleviatethisproblem.Besides,liketwosidesofacoin,asatwo-stage convolutionalneuralnetworks.InProceedingsoftheIEEEconferenceoncomputer
pipeline,althoughThemeStationcanbeeasilyadaptedtoemerging visionandpatternrecognition.2414â€“2423.
AnchitGupta,WenhanXiong,YixinNie,IanJones,andBarlasOÄŸuz.2023.3DGen:Tri-
image-to-3Dmethodsforobtainingabetterinitialmodel,itmay
planelatentdiffusionfortexturedmeshgeneration.arXivpreprintarXiv:2303.05371
alsosufferfromabadinitializationsometimes,e.g.,3Dartifactsand (2023).ThemeStation:GeneratingTheme-Aware3DAssetsfromFewExemplars â€¢ 9
ZexinHeandTengfeiWang.2023. OpenLRM:Open-SourceLargeReconstruction Learningtransferablevisualmodelsfromnaturallanguagesupervision.InInterna-
Models.https://github.com/3DTopia/OpenLRM. tionalconferenceonmachinelearning.PMLR,8748â€“8763.
AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDaniel AmitRaj,SrinivasKaza,BenPoole,MichaelNiemeyer,NatanielRuiz,BenMildenhall,
Cohen-Or.2022. Prompt-to-promptimageeditingwithcrossattentioncontrol. ShiranZada,KfirAberman,MichaelRubinstein,JonathanBarron,etal.2023.Dream-
(2022). booth3D:Subject-driventext-to-3Dgeneration. arXivpreprintarXiv:2303.13508
JonathanHo,AjayJain,andPieterAbbeel.2020. Denoisingdiffusionprobabilistic (2023).
models.Advancesinneuralinformationprocessingsystems33(2020),6840â€“6851. RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rnOmmer.
FangzhouHong,JiaxiangTang,ZiangCao,MinShi,TongWu,ZhaoxiChen,Tengfei 2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.InProceedings
Wang,LiangPan,DahuaLin,andZiweiLiu.2024. 3DTopia:LargeText-to-3D oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.10684â€“10695.
GenerationModelwithHybridDiffusionPriors. arXivpreprintarXiv:2403.02234 LeonidIRudin,StanleyOsher,andEmadFatemi.1992.Nonlineartotalvariationbased
(2024). noiseremovalalgorithms.PhysicaD:nonlinearphenomena60,1-4(1992),259â€“268.
YicongHong,KaiZhang,JiuxiangGu,SaiBi,YangZhou,DifanLiu,FengLiu,Kalyan NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfir
Sunkavalli,TrungBui,andHaoTan.2023. Lrm:Largereconstructionmodelfor Aberman.2023.Dreambooth:Finetuningtext-to-imagediffusionmodelsforsubject-
singleimageto3D.arXivpreprintarXiv:2311.04400(2023). drivengeneration.InProceedingsoftheIEEE/CVFConferenceonComputerVision
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang, andPatternRecognition.22500â€“22510.
LuWang,andWeizhuChen.2021.Lora:Low-rankadaptationoflargelanguage ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyLDenton,
models.arXivpreprintarXiv:2106.09685(2021). KamyarGhasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
HeewooJunandAlexNichol.2023.Shap-e:Generatingconditional3Dimplicitfunc- etal.2022.Photorealistictext-to-imagediffusionmodelswithdeeplanguageunder-
tions.arXivpreprintarXiv:2305.02463(2023). standing.AdvancesinNeuralInformationProcessingSystems35(2022),36479â€“36494.
TeroKarras,SamuliLaine,andTimoAila.2019.Astyle-basedgeneratorarchitecture NadavSchor,OrenKatzir,HaoZhang,andDanielCohen-Or.2019.Componet:Learning
forgenerativeadversarialnetworks.InProceedingsoftheIEEE/CVFconferenceon togeneratetheunseenbypartsynthesisandcomposition.InProceedingsofthe
computervisionandpatternrecognition.4401â€“4410. IEEE/CVFInternationalConferenceonComputerVision.8759â€“8768.
VladimirGKim,WilmotLi,NiloyJMitra,SiddharthaChaudhuri,StephenDiVerdi,and TamarRottShaham,TaliDekel,andTomerMichaeli.2019.SinGAN:Learningagenera-
ThomasFunkhouser.2013.Learningpart-basedtemplatesfromlargecollectionsof tivemodelfromasinglenaturalimage.InProceedingsoftheIEEE/CVFinternational
3Dshapes.ACMTransactionsonGraphics(TOG)32,4(2013),1â€“12. conferenceoncomputervision.4570â€“4580.
WeiyuLi,XuelinChen,JueWang,andBaoquanChen.2023.Patch-based3DNatural TianchangShen,JunGao,KangxueYin,Ming-YuLiu,andSanjaFidler.2021. Deep
SceneGenerationfromaSingleExample.InProceedingsoftheIEEE/CVFConference marchingtetrahedra:ahybridrepresentationforhigh-resolution3Dshapesynthesis.
onComputerVisionandPatternRecognition.16762â€“16772. AdvancesinNeuralInformationProcessingSystems34(2021),6087â€“6101.
Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,XiaohuiZeng,XunHuang, JingxiangSun,BoZhang,RuizhiShao,LizhenWang,WenLiu,ZhendaXie,andYebin
KarstenKreis,SanjaFidler,Ming-YuLiu,andTsung-YiLin.2023.Magic3D:High- Liu.2023.DreamCraft3D:Hierarchical3DGenerationwithBootstrappedDiffusion
ResolutionText-to-3DContentCreation.InConferenceonComputerVisionand Prior.https://arxiv.org/abs/2310.16818(2023).
PatternRecognition(CVPR). JiaxiangTang,ZhaoxiChen,XiaokangChen,TengfeiWang,GangZeng,andZiwei
MinghuaLiu,RuoxiShi,LinghaoChen,ZhuoyangZhang,ChaoXu,XinyueWei, Liu.2024.LGM:LargeMulti-ViewGaussianModelforHigh-Resolution3DContent
HanshengChen,ChongZeng,JiayuanGu,andHaoSu.2023b.One-2-3-45++:Fast Creation.arXivpreprintarXiv:2402.05054(2024).
SingleImageto3DObjectswithConsistentMulti-ViewGenerationand3DDiffusion. JiaxiangTang,JiaweiRen,HangZhou,ZiweiLiu,andGangZeng.2023a. Dream-
arXivpreprintarXiv:2311.07885(2023). Gaussian: Generative Gaussian Splatting for Efficient 3D Content Creation.
RuoshiLiu,RundiWu,BasileVanHoorick,PavelTokmakov,SergeyZakharov,and arXiv:2309.16653[cs.CV]
CarlVondrick.2023c.Zero-1-to-3:Zero-shotoneimageto3Dobject.InProceedings JunshuTang,TengfeiWang,BoZhang,TingZhang,RanYi,LizhuangMa,andDong
oftheIEEE/CVFInternationalConferenceonComputerVision.9298â€“9309. Chen.2023b. Make-It-3D:High-Fidelity3DCreationfromASingleImagewith
YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,LingjieLiu,TakuKomura,and DiffusionPrior.InInternationalConferenceonComputerVisionICCV.
WenpingWang.2023a. SyncDreamer:GeneratingMultiview-consistentImages TengfeiWang,BoZhang,TingZhang,ShuyangGu,JianminBao,TadasBaltrusaitis,
fromaSingle-viewImage.arXivpreprintarXiv:2309.03453(2023). JingjingShen,DongChen,FangWen,QifengChen,andBainingGuo.2023b.RODIN:
XiaoxiaoLong,Yuan-ChenGuo,ChengLin,YuanLiu,ZhiyangDou,LingjieLiu,Yuexin AGenerativeModelforSculpting3DDigitalAvatarsUsingDiffusion.IEEEConfer-
Ma,Song-HaiZhang,MarcHabermann,ChristianTheobalt,etal.2023.Wonder3D: enceonComputerVisionandPatternRecognition(CVPR)(2023).
Singleimageto3Dusingcross-domaindiffusion.arXivpreprintarXiv:2310.15008 TengfeiWang,TingZhang,BoZhang,HaoOuyang,DongChen,QifengChen,and
(2023). FangWen.2022.PretrainingisAllYouNeedforImage-to-ImageTranslation.In
RoeyMechrez,ItamarTalmi,andLihiZelnik-Manor.2018.Thecontextuallossforimage arXiv.
transformationwithnon-aligneddata.InProceedingsoftheEuropeanconferenceon ZhengyiWang,ChengLu,YikaiWang,FanBao,ChongxuanLi,HangSu,andJun
computervision(ECCV).768â€“783. Zhu.2023a.ProlificDreamer:High-FidelityandDiverseText-to-3DGenerationwith
LukeMelas-Kyriazi,ChristianRupprecht,IroLaina,andAndreaVedaldi.2023.Real- VariationalScoreDistillation.https://arxiv.org/abs/2305.16213(2023).
Fusion:360ReconstructionofAnyObjectfromaSingleImage.InConferenceon RundiWu,RuoshiLiu,CarlVondrick,andChangxiZheng.2023.Sin3DM:Learninga
ComputerVisionandPatternRecognition(CVPR). DiffusionModelfromaSingle3DTexturedShape.arXivpreprintarXiv:2305.15399
GalMetzer,EladRichardson,OrPatashnik,RajaGiryes,andDanielCohen-Or.2023. (2023).
Latent-NeRFforshape-guidedgenerationof3Dshapesandtextures.InProceedings RundiWuandChangxiZheng.2022.Learningtogenerate3Dshapesfromasingle
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.12663â€“12673. example.arXivpreprintarXiv:2208.02946(2022).
CharlieNash,YaroslavGanin,SMAliEslami,andPeterBattaglia.2020. Polygen: KaiXu,HaoZhang,DanielCohen-Or,andBaoquanChen.2012.Fitanddiverse:Set
Anautoregressivegenerativemodelof3Dmeshes.InInternationalconferenceon evolutionforinspiring3Dshapegalleries.ACMTransactionsonGraphics(TOG)31,
machinelearning.PMLR,7220â€“7229. 4(2012),1â€“10.
Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. ShiYichun,WangPeng,YeJianglong,MaiLong,LiKejie,andYangXiao.2023.MV-
2023.Point-E:ASystemforGenerating3DPointCloudsfromComplexPrompts. Dream:Multi-viewDiffusionfor3DGeneration. https://arxiv.org/abs/2308.16512
https://arxiv.org/abs/2212.08751(2023). (2023).
MichaelNiemeyerandAndreasGeiger.2021.Giraffe:Representingscenesascomposi- YouyiZheng,DanielCohen-Or,andNiloyJMitra.2013.Smartvariations:Functional
tionalgenerativeneuralfeaturefields.InProceedingsoftheIEEE/CVFConferenceon substructuresforpartcompatibility.InComputerGraphicsForum,Vol.32.Wiley
ComputerVisionandPatternRecognition.11453â€“11464. OnlineLibrary,195â€“204.
DarioPavllo,JonasKohler,ThomasHofmann,andAurelienLucchi.2021. Learning LinqiZhou,YilunDu,andJiajunWu.2021.3Dshapegenerationandcompletionthrough
generativemodelsoftextured3Dmeshesfromreal-worldimages.InProceedingsof point-voxeldiffusion.InProceedingsoftheIEEE/CVFInternationalConferenceon
theIEEE/CVFInternationalConferenceonComputerVision.13879â€“13889. ComputerVision.5826â€“5835.
BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall.2023.DreamFusion:Text-
to-3Dusing2DDiffusion.InInternationalConferenceonLearningRepresentations
(ICLR).
GuochengQian,JinjieMai,AbdullahHamdi,JianRen,AliaksandrSiarohin,BingLi,
Hsin-YingLee,IvanSkorokhodov,PeterWonka,SergeyTulyakov,andBernard
Ghanem.2023.Magic123:OneImagetoHigh-Quality3DObjectGenerationUsing
Both2Dand3DDiffusionPriors.https://arxiv.org/abs/2306.17843(2023).
AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,Sandhini
Agarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.2021.10 â€¢ ZhenweiWang,TengfeiWang,GerhardHancke,ZiweiLiu,andRynsonW.H.Lau
APPENDIX Table4. Quantitativeevaluationoftheme-drivendiffusionmodel.
A MORERESULTS.
Iteration100 Iteration200 Iteration300 Iteration400
Fig.10toFig.13showmoreresultsofThemeStation.
LPIPS-diversityâ†‘ 0.627 0.617 0.403 0.347
B IMPLEMENTATIONDETAILS LAION-aesthetic-scoreâ†‘ 6.262 6.355 6.367 5.941
Inthefirststage,werender20imagesforeachreferencemodel
withafixedelevation,i.e.,0or20,andrandomizedazimuth.We
fine-tunethepre-trainedStableDiffusion[Rombachetal.2022]
modelfor200iterations(asingleexemplar)or400iterations(afew
exemplars)withabatchsizeof8.Wesetthelearningrateas2Ã—10âˆ’6,
theimagesizeas512Ã—512,andtheCFGweightatinferenceas
7.5.Wealsotakethecameraposeoftherenderedimagesasan
additionalconditionduringthemodelfine-tuningsteptoensure
thegeneratedconceptimageshaveacorrectviewpointforaccurate
image-to-3Dinitialization.
In the second stage, we employ an off-the-shelf image-to-3D
method [Long et al. 2023] to lift the synthesized concept image
intoaninitial3Dmodel,representedasaneuralimplicitsigned
distancefield(SDF).Weusetheconceptimageand20augmented Fig.9. Applicationresultsofcontrollable3D-to-3Dgeneration.ThemeStation
viewsoftheinitialmodelforconceptpriorlearninganduse30 allowsuserstospecifyadesired3Dvariationviatextpromptmanipulation.
normalmaps,and30colorimagesoftheinput3Dexemplarsfor
referencepriorlearning.Duringoptimization,weconverttheSDF
controllable3D-to-3Dgeneration.Specifically,givenuser-specified
intoDMTet[Shenetal.2021]ata192gridand512resolutionto
text prompts, ThemeStation allows users to control the concept
directlyoptimizethetexturedmeshateachoptimizationiteration.
imagegenerationprocessandobtainspecific3Dvariations,suchas
Werenderboththenormalmapandthecolorimage,underran-
anowlmadeofstone(materialspecification),anowlwearingasuit
domizedviewpoints,asguidancetocomputetheDSDloss(Eq.5).
(specificaccessorizing),andanowlinpink/red(colorspecification),
Weusedynamicdiffusiontimestepthatsampleslargertimestep
asshowninFig.9.Theresultsofthissampleapplicationdemonstrate
fromrange[0.5,0.75]whenapplyingtheconceptpriorandsamples
theimmensepotentialofThemeStationtobeseamlesslycombined
smallertimestepfromrange[0.1,0.25]forthereferenceprior.We
withemergingcontrollableimagegenerationtechniques[Brooks
setğ›¼ as0.2andğ›½ as1.0.Thetotaloptimizationstepis5000.We
etal.2023;Hertzetal.2022;Wangetal.2022]formoreinteresting
alsoadoptthetotalvariationloss[Rudinetal.1992]andcontextual
3D-to-3Dapplications.
loss[Mechrezetal.2018]toenhancethetexturequality.Specially,
thecontextuallossisappliedbetweentherenderedcolorimageand E POTENTIALETHICSISSUES
the20augmentedviewsoftheinitialmodel.Thewhole3D-to-3D
Asagenerativemodel,ThemeStationmayposeethicalissuesifused
generationprocesstakesaround2hoursusingasingleNVIDIA
tocreatebalefulandfakecontent,whichrequiresmorevigilanceand
A100GPU.
care.Wecanadoptthecommonlyusedsafetycheckerinexisting
text-to-imagediffusionmodelstofilteroutmaliciouslygenerated
C EVALUATIONOFTHEME-DRIVENDIFFUSION conceptimagesinourfirststagetoalleviatethepotentialethics
MODEL issues.
Toevaluatetheinfluenceofdifferentfine-tuningiterationsforthe
theme-drivendiffusionmodelthatgeneratesconceptimagesinthe
firststage,weconductablationstudiesonfoursettings,i.e.,fine-
tuningthetheme-drivendiffusionmodelgivenoneexemplarfor
100, 200, 300 and 400 iterations. We use LPIPS-diversity (LPIPS
differencesacrossgeneratedimages)andLAION-aesthetic-scoreto
estimatethediversityandqualityofgeneratedconceptimages.The
quantitativeresultsareshowninTab.4.Ascanbeseen,diversity
significantlydropswheniterationis300,andqualitydropswhen
iterationis400,bothcausedbyoverfitting.Wethussetthefine-
tuningiterationto200forasingleexemplar(Sec.B).
D CONTROLLABLE3D-TO-3DGENERATION
Giventhetwo-stagegenerativeschemeandpriordisentanglefeature
oftheproposedDSDloss,ThemeStationsupportstheapplicationofThemeStation:GeneratingTheme-Aware3DAssetsfromFewExemplars â€¢ 11
Fig.10. VisualresultsofThemeStation,whichgenerates3Dmodelsfromonlyone3Dexemplar.Foreachcase,weshowthereferencemodel(left)andthree
generatedmodelsontheright.Foreachgeneratedmodel,weshowaprimaryview(top)withitsnormalmap(bottomright)andasecondaryview(bottomleft).
Fig.11. VisualresultsofThemeStation,whichgenerates3Dmodelsfromafew3Dexemplars.Foreachcase,weshowthereferencemodelsontheleftandsix
generatedmodelsontheright.Foreachgeneratedmodel,weshowaprimaryview(top)withitsnormalmap(bottomright)andasecondaryview(bottomleft).12 â€¢ ZhenweiWang,TengfeiWang,GerhardHancke,ZiweiLiu,andRynsonW.H.Lau
Fig.12. VisualresultsofThemeStation,whichgenerates3Dmodelsfromonlyone3Dexemplar.Foreachcase,weshowthereferencemodel(left)andthree
generatedmodelsontheright.Foreachgeneratedmodel,weshowaprimaryview(top)withitsnormalmap(bottomright)andasecondaryview(bottomleft).
Fig.13. VisualresultsofThemeStation,whichgenerates3Dmodelsfromafew3Dexemplars.Foreachcase,weshowthereferencemodelsontheleftandsix
generatedmodelsontheright.Foreachgeneratedmodel,weshowaprimaryview(top)withitsnormalmap(bottomright)andasecondaryview(bottomleft).