Collaborative AI Teaming in Unknown Environments
via Active Goal Deduction
ZuyuanZhang HanhanZhou
TheGeorgeWashingtonUniversity TheGeorgeWashingtonUniversity
zuyuan.zhang@gwu.edu hanhan@gwu.edu
MahdiImani TaeyoungLee
NortheasternUniversity TheGeorgeWashingtonUniversity
m.imani@northeastern.edu tylee@gwu.edu
TianLan
TheGeorgeWashingtonUniversity
tlan@gwu.edu
Abstract
Withtheadvancementsofartificialintelligence(AI),weâ€™reseeingmorescenarios
thatrequireAItoworkcloselywithotheragents,whosegoalsandstrategiesmight
notbeknownbeforehand. However,existingapproachesfortrainingcollaborative
agents often require defined and known reward signals and cannot address the
problemofteamingwithunknownagentsthatoftenhavelatentobjectives/rewards.
Inresponsetothischallenge,weproposeteamingwithunknownagentsframework,
whichleverageskerneldensityBayesianinverselearningmethodforactivegoal
deductionandutilizespre-trained,goal-conditionedpoliciestoenablezero-shot
policy adaptation. We prove that unbiased reward estimates in our framework
aresufficientforoptimalteamingwithunknownagents. Wefurtherevaluatethe
frameworkofredesignedmulti-agentparticleandStarCraftIImicromanagement
environmentswithdiverseunknownagentsofdifferentbehaviors/rewards. Em-
piricalresultsdemonstratethatourframeworksignificantlyadvancestheteaming
performanceofAIandunknownagentsinawiderangeofcollaborativescenarios.
1 Introduction
Advancementsinmachinelearningandartificialintelligence(ML/AI)areenablingmoreandmore
scenarioswhereAIagentsaretocollaboratewithotherautonomoussystemsorhumans,whichare
oftenconsideredunfamiliarentitiesoutsidetheenvironmentswithunknownobjectivesJohnsonetal.
[2020],Dafoeetal.[2020],Taoetal.[2022]. Examplesincludeteamingwithautonomousagents
thatwerebuiltbyotherdeveloperswithunknowndesigns/parametersTraegeretal.[2020],Albrecht
andStone[2018],orhumansinasharedworkenvironmentwithundefinedoronlypartiallydefined
intents/goalsSimmlerandFrischknecht[2021],BehymerandFlach[2016]. Theabilitytoteamup
withsuchunknownagentsandtoeffectivelycollaboratetowardcommon(yetoftenlatent)objectives
canbecrucialforsolvingcomplextasksthatwouldbeotherwiseimpossibleDafoeetal.[2020].
ExistingmethodsoftrainingAIagents,suchasmulti-agentreinforcementlearning(MARL)Spaan
[2012]andtransferlearningWeissetal.[2016],Yuetal.[2021],oftencannotsupportsynergistic
teamingwithunknownagents,duetotheabsenceofpre-definedgoalsandrewards.
Inthispaper, weproposeanovelframeworktodevelopAIagentsforSynergisticTeamingwith
UNknownagents(STUN),throughactivegoalinferenceandzero-shotpolicyadaptation. Specifically,
Preprint.Underreview.
4202
raM
22
]IA.sc[
1v14351.3042:viXrain collaborative task environments, we leverage inverse learning to enable AI agents to actively
reason and infer the reward signals (i.e., the posterior distribution) of the unknown agents from
theirobservedtrajectoriesonthefly. Then,weshowthatobtaininganunbiasedrewardestimateis
sufficienttoensuretheoptimalityoflearningcollaborativepolicies. Basedonthisresult,weutilize
theinferredrewardsignaltoachieveazero-shotpolicyadaptationbypre-trainingcollaborativeAI
agentpolicieswithrespecttorandomlysampledsurrogatemodels. Thisnovelteamingframework
goesbeyondexistingapproaches,whicheitherfailtooperateintheabsenceofrewardsignals(e.g.,
policy re-training and transfer learning Weiss et al. [2016]) or resort to general one-size-fits-all
policieswithnon-optimalteamingperformance(e.g.,multi-tasklearningZhangandYang[2018]
withrespecttoassumedunknownrewarddistributions).
For active goal inference, we propose a Ker-
nel Density Bayesian Inverse Learning (KD-
Unknown Agents
BIL) to obtain a posterior estimate of the la-
with Latent Reward ğ‘¹ ? ğ… ğŸ
tentgoal/rewardfunctionoftheunknownagents. ?
T thh eis nm eee dth to od reis fis ta pm op lil ce iee sffi fc oi ren eat ca hnd sae mlim pli en dat re es - ğ… ğŸ ğ… ğŸ’ â€¦â€¦ ğ… ğŸ“
ward function by utilizing the kernel density
? ğ… Collaborative AI Agents
estimationtoapproximatethelikelihoodfunc- ğŸ‘
tion. Thekerneldensityfunctionrepresentsthe Figure1: Weconsidertheproblemofenablingsyner-
probability of observing certain states and ac- gisticteamingofAIagentswithotherunknownagents
tions given a reward function Mandyam et al. (e.g.,humanorautonomousagentsthatcouldhavelatent
[2022]. Itallowsefficientposteriorinferenceof rewards/objectives)incollaborativetaskenvironments.
rewardfunctionsinlightofobservedsequencesfromagents.
Interestingly,directlyusingthemaximumaposteriori(MAP)oflatentrewardscannotensurethe
optimalityoflearningcollaborativepolicies. ToensureconvergenceandoptimalityoftheBellman
equation,weprovethatobtaininganunbiasedestimationisnecessary. Thisresultmotivatesusto
developazero-shotpolicyadaptationstrategyforteamingwithunknownagentsinSTUN.Itleverages
adecentralizedpartiallyobservableMarkovDecisionProcess(dec-POMDP)modelOliehoeketal.
[2016]topre-traincollaborativeagentpolicieswithrespecttorandomlysampledsurrogatemodels
(of the unknown agents), such that the learned collaborative policies Ï€ (a |s ,R) (known as the
i i i
goal-conditionedpolicies)areconditionedonpotentialrewardsR. Toteamupwithunknownagents,
azero-shotpolicyadaptationcanbeeasilyachievedbyconditioningthelearnedcollaborativepolicies
ontheunbiasedestimationRË† (obtainedbyactivegoalinferencefromobservedunknownagentsâ€™
trajectories),i.e.,Ï€ (a |s ,RË†).TheproposedSTUNframeworkisscalableasitisbasedoncentralized
i i i
pre-trainingandfullydecentralizedexecutionsKraemerandBanerjee[2016].
TovalidatetheeffectivenessoftheSTUNframework,wecreatedthefirstcoopenvironmentsforteam-
ingandcollaboratingwithunknownagentsbymodifyingtheMPEandSMACenvironmentsLowe
etal.[2017],Whitesonetal.[2019]. Weredesignedtherewardsystemtoreflectvariouslatentplay
styles. TheblueteamisthencomposedofbothcollaborativeAIagentsandunknownagents(e.g.,
downloadedfrompublicrepositoriesortrainedwithlatentrewardsusingpopularMARLalgorithms
like MAPPO, IPPO, COMA, and IA2C Yu et al. [2022], Schulman et al. [2017], Foerster et al.
[2018],Mnihetal.[2016]). Comparedwithawiderangeofbaselines,STUNagentsconsistently
achieveclose-to-optimalteamingperformancewithunknownagentsinalmostallscenarios/tasks.
Onsuperhardmapslike27m_vs_30mormmm2,itimprovestherewardofunknownagentsbyup
to50%. Italsodemonstratestheabilitytocognitivelyreasonandadapttothenon-stationarityof
unknownagents.
2 RelatedWork
HumanAIteaming: Existingworkonhuman-AIteamingoftenfocusesonteamdynamicsand
organizationalbehaviorcontributestounderstandinghowtobuildeffectivehuman-AIteamsDafoe
etal.[2020],AlbrechtandStone[2018],leveragedcognitivesciencetobettermodelandcomplement
humandecision-makingprocessesHuandSadigh[2023],Traegeretal.[2020],andconsideredrelated
issuessuchascommunication,trust,andcollaborationstrategiesBaueretal.[2023]. Otherrelated
work isasfollowsChen etal.[2024]. However, modeling humansin asharedtask environment
asunknownagentsandsupportinghuman-AIteamingthroughactivegoalinferenceandPOMDP
modelshavenotbeenconsidered.
2Multi-agentReinforcementLearning: MostofthesuccessfulRLapplications,e.g.,gamingIqbal
andSha[2019],Foersteretal.[2017],Meietal.[2024a,2023]androboticsKnappetal.[2013],
Robinetteetal.[2016],involvetheparticipationofmultipleagents.ForcollaborativeagentsMatignon
etal.[2007],Panaitetal.[2006],MARLlearnsjointdecision-makingpoliciestooptimizeashared
reward. TheseproblemsareoftenformulatedasdecentralizedPOMDPsandsolvedusingpolicy-
orvalue-basedmethodslikeMAPPOYuetal.[2022],MATRPOLiandHe[2023],andfactoriza-
tionRashidetal.[2020],Zhouetal.[2022]. Existingworkhasalsoconsideredtransferlearningin
thiscontextYangetal.[2021]. OtherrelatedworkisasfollowsZhouetal.[2023a,b],Chenetal.
[2021,2023a]. However,teamingandcollaboratingwithunknownagentswithundefinedrewardsare
underexplored.
Multi-taskLearning: Multi-tasklearningaimstotrainintelligencetosolvemultipletaskssimulta-
neouslyCaruana[1997]. NegativeTransfer(NT)betweentasksisamajorchallengeinmulti-task
RL,i.e.,knowledgelearnedwhilesolvingonetaskmayinterferewiththelearningofothertasks
Rusuetal.[2016],Omidshafieietal.[2017],Gurulinganetal.[2023]. MethodslikeModularNeural
Networks(MNNs)AudaandKamel[1999]andAttentionMechanisms(AMs) Niuetal.[2021]
areproposedtoreducenegativetransferVezhnevetsetal.[2017],Wangetal.[2023]. Otherrelated
workisasfollowsChenetal.[2023b]. Multi-tasklearningcanproducegeneralpoliciesthatworkin
differenttaskenvironmentsbutmaynotachieveoptimalteamingperformancewithspecificunknown
agents.
InverseReinforcementLearning: InverseReinforcementLearning(IRL)infersgoals/rewardsfrom
observationsofactiontrajectories. ItwasfirstproposedinNgetal.[2000]showingthattheIRL
problem has infinite solutions. Several solutions, such as MaxEntropy IRL Ziebart et al. [2008],
Max-MarginIRLAbbeelandNg[2004],andBayesianIRLRamachandranandAmir[2007]have
beenproposedtoresolvetheambiguityofIRL.OtherrelatedworkisasfollowsMeietal.[2022,
2024b]. Incontrast,kernel-basedIRLinthispaperismoresample-efficientandsupportssynergistic
teamingwithunknownagentsonthefly.
3 OurProposedSolution
3.1 ProblemStatement
Consideradec-POMDPmodelOliehoeketal.[2016]involvingbothunknownagentsandcollabora-
tiveAIagents(alsodenotedasSTUNagents),givenbyatupleM=âŸ¨N ,N,S,A,T,R,â„¦,O,Î³âŸ©.
u
N denotesasetofn unknownagentswithalatentrewardR,whileN denotesasetofnAI-agents
u u
supportingtheunknownagentstowardthelatentgoal. S isthejointstatespaceforallagentsN âˆªN.
u
Foreachagenti,A isitsactionspaceandO itsobservationspace.Thus,A=A Ã—A Ã—Â·Â·Â·Ã—A
i i 1 2 nu+n
denotesthejointactionspaceofallagents, andO = O Ã—O Ã—Â·Â·Â·Ã—O denotesthejoint
1 2 nu+n
observationspaceofallagents. WeuseT : S Ã—A â†’ â„¦(S)todenotethetransitionprobability
function,withâ„¦(S)representingasetofstatedistributions.
EachunknownagentiâˆˆN behavesaccordingtosomelatentpolicyÏ€u :O Ã—A â†’[0,1],which
u i i i
isaprobabilitydistributionrepresentingtheprobabilityoftheagenttakingeachactioninA under
i
observationO . Weassumethattheunknownagentsarelogicaldecision-makersâ€“theirbehaviors
i
Ï€uarealignedwithandmaximizethelatentrewardR. ThelatentrewardfunctionR:SÃ—Aâ†’Ris
unknownandnon-observabletootherAIagentsoperatinginthesharedtaskenvironment,whiletheir
behaviortrajectoriesareobservableforreasoningandinferringthelatentreward(whichcouldbe
time-varying).
OurgoalinthispaperistolearnthepoliciesofcollaborativeAIagents(i.e.,STUNagents): Ï€ :
i
O Ã—A â†’ [0,1]fori âˆˆ N,toeffectivelyteamupwiththeunknownagentsandcollaboratively
i i
maximizetheexpectedreturnG=(cid:80)HÎ³tRt,whereÎ³ isadiscountfactor,Rtisthelatentreward
t
received at time t, and H is the time horizon. It is easy to see that while the problem follows a
dec-POMDP structure, it cannot be solved with existing MARL algorithms because training the
MARLagentswouldrequirehavingaccesstothelatentrewardsignalR,andthusisnotpossiblein
tasks/scenariosteamingupwithunknownagents. ThecollaborativeAIagentsmustsimultaneously
address two problems: (i) inferring the latent reward by collecting observations of the unknown
agentsinthesharedtaskenvironmentand(ii)adaptingtheirpoliciesontheflytosupporteffective
teamworktowardthelearnedrewardwithoutincurringsignificantoverheadsuchasre-training.
33.2 OverviewofOurSTUNframework
AsillustratedinFigure2, tosupportsynergisticteaming, ateamofSTUNagentsisdeployedin
a shared task environment to collaborate with unknown agents. The STUN agents can observe
thetrajectories{Ï„u} = {(ou,au)}n oftheunknownagents(definedasasequenceoftheirjoint
i i i=1
observationsou âˆˆOu =O Ã—O Ã—Â·Â·Â·Ã—O andjointactionsau âˆˆAu =A Ã—A Ã—Â·Â·Â·Ã—A ).
i 1 2 nu i 1 2 nu
However,theydonothaveaccesstothelatentrewardRthatiscontrolledonlybytheunknownagents.
Toavoidtheoverheadofre-trainingortransfer-learning,weproposeazero-shotpolicyadaptation
framework. Thekeyideaistopre-trainaclassofgoal-conditionedpoliciesÏ€(a|o,R)fortheSTUN
agents,whichareconditionedonpotentialrewardfunctionsR. Thispre-trainingissupportedbythe
useofsurrogateunknownagentmodelswithrespecttorandomlysampledlatentrewardfunctionR.
The pre-trained STUN agents (with goal- Pre-training Collaboration with Unknown Agents
conditioned policies {Ï€ (a |o ,R)}) are then deployed in a shared tasi k ei nvi ironment to col- ğ… ğŸğ’– + ğ… ğŸğ’– ğ‰ ğŸğ’– (a)
laborate with unknown agents and to support ğ… ğŸğ’– + ğ… ğŸğ’– ğ‰ ğŸğ’–
commongoals/objectives. Weproposeakernel- â€¦ â€¦ â€¦
density Bayesian inverse learning (KD-BIL) ğ… ğ’ğ’– ğ’– + ğ… ğ’ğ’– ğ’– ğ‰ ğ’ğ’– ğ’–
algorithm to obtain the posterior distribution
of latent reward function P(R|{(ou i,au i)}n i=1) ğ… ğŸ(â‹…|()+ ğ… ğŸ + ğ‘¹* IL
from the observed trajectories of {Ï„u} =
{(ou,au)}n . The proposed inverse learning ğ… ğŸ(â‹…|()+ ğ… ğŸ + ğ‘¹* IL
i i i=1 â€¦ â€¦ â€¦ â€¦
algorithmusesthekernel-densitymethodtoes-
timatetheposteriorandthuseliminatestheneed ğ… ğ’(â‹…|()+ ğ… ğ’ + ğ‘¹* IL
(c) (b)
torefitpoliciesforeachsampledrewardfunc- ğ‘¹~ğ’‘(â€™)
Figure 2: An illustration of our proposed frame-
tion. Itallowsobtainingtheposteriorestimates
work. STUN agents Ï€ (Â·|R) are pre-trained us-
fromevenlimitedobservation(i.e.,withsmall i
ing surrogate agent models with sampled latent
n)orwhentherewardistime-varying(i.e.,up-
rewardsR. Tocollaboratewithunknownagents,
datingthelatestnobservations).
theyuseinverselearning(stepb)ontheobserved
Weshowthatamaximumaposteriori(MAP)of trajectories{Ï„u}oftheunknownagents(stepa)
i
latentrewardcannotensureconvergenceofthe andperformazero-shotpolicyadaptationbased
Bellmanequationtoachieveoptimalexpected onanunbiasedestimateRË†(stepc).
return G =
(cid:80)HÎ³tRt.
Instead, an unbiased
t
estimateRËœ isproventobenecessaryforlearningtheoptimalcollaborativepolicy. Basedonthis
result,weleveragethepre-trained,goal-conditionedpolicies{Ï€ }andperformazero-shotpolicy
i
adaptationusingtheunbiasedestimate,i.e,{Ï€ (a |o ,RËœ)}. Theproposedpolicyadaptationrequires
i i i
nore-trainingorre-learning(thuszero-shot),whileensuringoptimalityoftheadaptedpolicy.
3.3 ActiveGoalInferencewithInverseLearning
OurproposedKD-BILisamethodforapproximatingtherewardprobabilitydistributionusingkernel
densityestimates. Itnotonlyallowsanefficientestimateoftheunknownagentsâ€™reward(whichis
oftenassociatedwithsubstantialuncertainty)butalsosupportssample-efficientcomputationsusing
limitedobservationdata. Specifically,wesetupatrainingdatasetDbysamplingmdemonstrations,
eitherfromavailabletrajectoriesofknownagentswithobservablerewardorbytrainingsurrogate
unknownagentsusingsampledrewardfunctions. Thisdatasetconsistsofm3-tupledemonstrations
{(o ,a ,R )}m , where R is the reward function used to generate the demonstrations (o ,a ).
j j j j=1 j j j
Usingeitherknownagentsorsurrogateagentmodels,weconstructthetrainingdatasetthatcontains
demonstrationsofvariouspotentialbehaviors.
Givenobservedunknownagenttrajectories(ou,au)ofsizen,wecannowestimatetheposterior
i i
p (R|ou,au)byformulatingconditionaldensitypË† (ou,au|R)withrespecttothetrainingdataset
m i i m i i
andourchoiceofkerneldensityfunctionK(Â·,Â·).
Usingdemonstrationsinthetrainingdataset,theconditionaldensityforastate-actionpair(ou,au)
i i
givenalatentrewardfunctionRis
pË†
(ou,au|R)âˆ(cid:88)m K((ou i,au i),(o j,a j))Â·K R(R,R j)
(1)
m i i (cid:80)m K (R,R )
j=1 l=1 R l
4
sledoM
etagorruS
stnegA
NUTS
ğ‘¹tnetal
delpmas
htiw
gniniart-erP
stnegA
nwonknU
stnegA
NUTSwhereK(Â·,Â·)andK (Â·,Â·)aretwodifferentkerneldensityfunctionsforthestate-actionpairandfor
R
thereward,respectively. TheproposedKD-BILmethodworkswithanykernelfunctions,suchas
theGaussiankernelandtheMaternkernelMandyametal.[2022]. WeconsidertheGaussiankernel
functioninthefollowingderivations. ThisimpliesthattheconditionaldensitypË† (ou,au|R)isgiven
m i i
by:
pË† m(ou i,au
i|R)âˆ(cid:88)m eâˆ’ds((ou i,a (cid:80)u i), m(oj,a ej âˆ’) d)2 r/ (R(2 ,h R) le )âˆ’ 2/d (r 2( hR â€²),Rj)2/(2hâ€²)
(2)
j=1 l=1
whered :(OÃ—A)Ã—(OÃ—A)â†’Risadistancemetrictocompare(o,a)tuples,d :RÃ—Râ†’R
s r
isadistancemetrictocomparerewardfunctions,andh,hâ€² >0aresmoothinghyperparameters. We
notethatthesehyperparameterscanbefurtheroptimizedusingthetrainingdataset,e.g.,similarto
optimizingsurrogatemodelsinBayesianOptimizationRamachandranandAmir[2007]. Next,we
obtainthefollowingposteriorestimateoflatentreward.
Lemma3.1. Theestimatedposterioroftheunknownagentrewardisgivenby
pË† m(R|{ou i,au i}n
i=1)âˆp(R)(cid:89)n (cid:88)m eâˆ’ds((ou i,a (cid:80)u i), m(oj,a ej âˆ’) d)2 r/ (R(2 ,h R) le )âˆ’ 2/d (r 2( hR â€²),Rj)2/(2hâ€²)
(3)
i=1j=1 l=1
Importantly, we note that this conditional density pË† can be easily computed from the training
m
dataset and using kernel density functions. There is no need to refit policies or perform value
iterations to evaluate the posterior for a given reward function R. This drastically reduces the
computationalcomplexitycomparedtoexistingBayesianIRLalgorithmsMandyametal.[2022],
RamachandranandAmir[2007],ChoiandKim[2012],ChanandvanderSchaar[2021]andthus
makesitpossibletoperformBayesianinverselearningfortheunknownagentâ€™slatentreward,even
incomplexenvironmentswithlargestatespaces(asshowninourevaluationonSMACWhiteson
etal.[2019]). Further,p(R)inEquation(3)denotesthepriordistributionofrewardfunctions. Itcan
beauniformdistributionorestimatedfromavailableunknownagentstatistics. Asm â†’ âˆ,itis
shownthatpË† convergestothetruelikelihoodofreinforcementlearningpoliciesandtheposterior
m
convergestothetrueposteriorVanderVaart[2000].
Inpractice,wecanconsiderageneralrepresentationofthelatentrewardfunction,i.e.,R (s,a)with
B
latentparametersB âˆˆRk,whichkisthelatentdimensionofB. Thisrepresentationcaptureslatent
reward that can be expressed as a linear function R (s,a) = BTR(s,a) of possible underlying
B
componentsR = (R ,R ,...,R )oftheunknownagentsâ€™potentialobjectives,aswellasmore
1 2 k
generalformsofrewardfunctionsthataredefinedthroughaneuralnetworkR (s,a)parameterized
B
by B. For instance, in our evaluations using the MPE environment Lowe et al. [2017], we can
construct a mixing network (e,g., a single-layer neural network parameterized by B) to combine
underlyingcomponentssuchasgreedy,safety,cost,andpreference,intoamorecomplexandrealistic
rewardfunctionrepresentationforgoalinference. Thus,activegoalinferenceinthispaperaimsto
estimatethelatentrewardparametersBfromobservedunknownagentsâ€™trajectories{(ou,au)}n ,
i i i=1
usingtheproposedKD-BILmethod,byestimatingposteriorpË† (R |{ou,au}n )overthelatent
m B i i i=1
rewardparametersBinstead. Thisapproachenablesefficientestimateofcomplexrewardfunctions
inMPEandSMACenvironments.
3.4 Zero-shotPolicyAdaptation
Withtheposteriorestimatesoflatentreward,itwouldbetemptingtoconsidertheMAPestimate
BË†âˆ— = argmax pË† (R |{ou,au}n ) and use it directly toadapt collaborative AI agent policies.
BË† m BË† i i i=1
However,asshowninournexttheorem,unbiasedestimatesofthelatentrewardR areneededto
BËœ
ensuretheconvergenceofBellmanequationstotheoptimalvalues. Furthermore,directlyemploying
theestimatedrewardforre-trainingandre-learningthecollaborativeAIagentpoliciesontheflycan
resultinsignificantoverheadandunstableteamingperformances. Tothisend,weproposeanovel
zero-shotpolicyadaptationforteamingwithunknownagents. Itpre-trainsasetofgoal-conditioned
policies {Ï€ (a |o ,R)} for the collaborative AI agents (by leveraging surrogate unknown agent
i i i
models)andthenmakesazero-shotpolicyadaptationusingunbiasedrewardestimates.
Toestablishoptimalityoftheproposedapproach,weconsideraQ-learningalgorithm(e.g., Watkins
andDayan[1992]thatareoftenemployedfortheoreticalanalysis)overthejointactionandstatespace
5andundertheunbiasedrewardestimatesR . Thus,theQ-values,QÏ€(s,a)=E [(cid:80)HÎ³tRt|S =
BËœ B Ï€ t t
s,A = a,Rt âˆ¼ R ], are now defined with respect to the reward estimates R . We show that
t B BËœ
unbiased reward estimates are sufficient to ensure convergence to the optimal Q-values that are
achievedbyhavingtheactualreward.
Theorem3.2. (UnbiasedEstimatesEnsuresOptimality.) GivenafiniteMDP,denotingasM,the
Q-learningalgorithmwithunbiasedestimaterewardssatisfyingE[R ]=R ,givenbytheupdate
BËœ B
rule,
Q BËœ,t+1(s t,a t)=(1âˆ’Î± t)Q BËœ,t(s t,a t)+Î± t[R BËœ+Î³maxQ B,t(s t+1,b)] (4)
bâˆˆA
convergesw.p.1totheoptimalQ-functionaslongas(cid:80) Î± =âˆand(cid:80) Î±2 <âˆ
t t t t
NotethattherighthandofEquation(4)reliesonunbiasedestimaterewardR (s,a). Theorem3.2
BËœ
states that collaborative agent policies will converge to optimal w.p.1 when replacing the actual
rewardswithunbiasedestimates.
Estimatingunbiasedrewards. Next,weobtainanunbiasedrewardestimateR . Forlinearreward
BËœ
functionRoverthelatentparameters,theproblemisequivalenttoobtaininganunbiasedestimateof
thelatentparameterssatisfyingE[BËœ]=B. LetBË† betheposteriordistributionofthelatentparameters.
WecanestimateunbiasedBËœfromBË†.
Lemma 3.3. If the reward function is linear over the latent parameters, BËœ = E[B|BË†] gives an
unbiasedestimate,i.e.,E[R (s,a)]=R (s,a).
BËœ B
Since the conditional distribution d may not be available, we can leverage a neural network
B|BË†
during the pre-training stage to estimate it â€“ as actual B of the surrogate models and posterior
estimatesBË†bothareavailableduringpre-training. Anotherideaistousetheposteriordistribution
asanapproximation,i.e.,d âˆ¼pË† (Â·|{ou,au}n ). Ourevaluationsshowthatthismethodallows
B|BË† m i i i=1
efficientapproximationoftheunbiasedestimatewithnegligibleerror. Forgeneralnon-linearreward
functions,wedenoteRâˆ’1asaninverseinthesensethatRâˆ’1(R )recoverstheunderlyingparameter
B
BoftherewardfunctionR . Anunbiasedestimatecanthenbeobtainedasfollows.
B
Lemma3.4. BËœ=Râˆ’1(E[R |BË†])isanunbiasedestimateofB,i.e.,E[R ]=E[R ].
B BËœ B
Inpractice,wecanalwaystrainaneuralnetworktorecoverBËœfromtheposteriordistributionofBË†,
withthegoalofminimizingtheresultingbiasoftherecoveredrewardfunctionR . Letg with
BËœ Î¶
parameterÎ¶ betheneuralnetwork. WeconsidertheestimateBËœ=g (BË†)tominimizealossfunction
Î¶
themeansquareerroroftheresultingrewardbias:
L =E[(R âˆ’R )2], s.t.BËœ=g (BË†) (5)
Î¶ BËœ B Î¶
Suchg canbeoptimizedduringthepre-trainingstage(asshowninFigure2)usingtheactualR of
Î¶ B
thesurrogatemodelsandtheposteriorBË†fromproposedKD-BIL.
Zero-shot adaptation with pre-training. We pre-train a set of goal-conditioned policies
{Ï€ (a |o ,B)} â€“ which are now conditioned on the latent reward function parameters B instead
i i i
â€“forthecollaborativeAIagents. Thepre-trainingisillustratedinFigure2,wheresurrogatemodels
oftheunknownagentareleveraged. WeemployMARLtotrainthesurrogatemodelsandthecol-
laborativeAIagentpolicies,usingrewardsignalsR withrandomlysampledparametersB âˆ¼p(Â·).
B
Thus,azero-shotpolicyadaptationcanbeachievedduringtheteamingstagebyfeedingtheunbiased
estimate BËœ(of unknown agent reward) into the goal-conditioned policies, i.e., {Ï€ (a |o ,BËœ)}. It
i i i
ensuresoptimalteamingperformancewithunknownagents.
Thepseudo-codeofourproposedSTUNframeworkcanbefoundinAppendixA.Inparticular,the
pre-trainingofgoal-conditionedpolicies(togetherwithsurrogatemodels)canleverageanyMARL
algorithmstomaximizetheexpectedreturnJ (Î¸)under-sampledreward:
B
J (Î¸)=E [VÏ€Î¸(s )] (6)
Ï€,B s0âˆ¼Âµ,s,a B 0
wheres isdrawnfromtheinitialstatedistribution. Inthispaper,weusepolicygradientalgorithms
0
topre-trainthegoal-conditionpolicies.
Lemma3.5. (LatentStylePolicyGradientforpre-training).
âˆ‡ J (Î¸)=E [QÏ€Î¸(s,a)âˆ‡ logÏ€ (a|o,B)] (7)
Î¸ Ï€,B Ï€Î¸ B Î¸ Î¸
6   
 * U H H G \  5 H Z D U G    
 6 D I H W \  5 H Z D U G  
   
     O L Q H U   G L P
           O  Q L Q  R H  Q U  O   L Q   H G  U L P    G L P
 Q R Q O L Q H U   G L P
    N=3,M=1  Q R Q O L Q H U   G L P
        N N= =3 3, ,M M= =2 4    I  P L [  X O W L  W D V N
 V D I H  V D I H W \  E L D V H G E D O D Q F H G J U H H G  E L D V H G  J U H H G \
                                   
 O D W H Q W  V W \ O H  H S R F K      H S R F K
(a) Interpretingteamingbehavior. (b) Teamingwithchangingunknown (c) Ablationandscalabilitystudy.
agents.
Figure 3: (a) Illustration of the underlying reward tradeoff when STUN agents team up with un-
knownagentsrangingfromplayingsafetobeinggreedy. (b)AbilityofSTUNagentstoquickly
reasoning/inferingthetime-varyingrewardoftheunknownagents(changingevery20epochs)and
thenperformingzero-shotpolicyadaptationonthefly. (c)Ablationstudiesshowingtheimpactof
differentdesignmodules,aswellasrobustperformanceofSTUNunderunknownrewardfunction
withincreasingcomplexity(e.g.,increasingfrom2to6dimensionsofrewardcomponentsandusing
nonlinearmixingfunctions).
 6 7 8 1
       0 $ 3 3 2  
       &  , 3 2  3 0  2 $  6  0 7  $ 8  3 1  3 2    
 6 7 8 1  
 0  , $ $   $  &  &    &  , 3 2  3 0  2 $
     0 $ 3 3 2  0 $ $  &
   
 &  , 3 2  3 0  2 $      , $  &
 0 $ $  &
     , $  &  
                                                     
     H S R F K      H S R F K      H S R F K
(a) 3s_vs_5z (b) 5m_vs_6m (c) 6h_vs_8z
 
   6  0 7  $ 8  3 1  3 2      6  0 7  $ 8  3 1  3 2
 & 2 0 $    & 2 0 $
 , 3 3 2      6 7 8 1  , 3 3 2
   0  , $ $   $  &  &
   
 0  &  , 3 2 $  3 3  0  2 3  $ 2    0  , $ $   $  &  &
 0 $ $  &  
       , $  &
 
   
                                                     
     H S R F K      H S R F K      H S R F K
(d) 27m_vs_30m (e) corridor (f) MMM2(hard)
Figure4:PerformancecomparisonofourproposedSTUNagentsandselectedbaselinesonredesigned
SMACtasks.
4 Experiments
We redesigned two multi-agent simulation environments based on multiagent-particle-
envs(MPE)Loweetal.[2017]andSMACWhitesonetal.[2019]tocreatecollaborativeteaming
taskswithunknownagents. Theseenvironmentsincludeblue(friendly)andred(adversarial)teams.
Theredteamiscontrolledbybuilt-inpolicieswithintheenvironments,whiletheblueteamconsists
ofbothunknownagentsandcollaborativeAIagents. Weconsiderdifferentmethodsforcreating
theunknownagents,suchastrainingwithrandomly-generatedlatentrewardparametersandusing
availableagentsfrompopularMARLalgorithmslikeMAPPO,IPPO,COMA,andIA2C Yuetal.
[2022],Schulmanetal.[2017],Foersteretal.[2018],Mnihetal.[2016]. WedeploySTUNagents
(and other baseline agents obtained by multi-task learning) alongside these unknown agents and
evaluatetheteamingperformanceineachcollaborativetask. Relatedcodeandimplementationdetails
canbefoundatGitHub(seesupplementaryFiles).
7
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H USynergisticAgents Fixed-BehaviorAgents Multi-taskAgents
UnknownAgents
STUN FBA-C FBA-B FBA-A MAPPO IPPO COMA MAA2C IA2C
FBA-C 1.10 1.04 1.04 0.99 0.88 0.85 0.84 0.69 0.68
FBA-B 2.38 2.06 2.36 2.19 1.99 1.84 1.83 1.71 1.73
FBA-A 3.73 2.88 3.55 3.88 3.08 2.94 2.68 2.35 2.67
MAPPO 2.43 2.04 2.05 2.08 2.11 1.97 1.80 1.87 1.56
IPPO 2.25 1.82 2.22 2.19 1.95 2.22 2.06 1.84 1.87
COMA 2.12 1.62 1.86 1.95 1.98 1.81 2.18 1.55 1.58
MAA2C 2.22 1.58 1.97 2.05 1.25 1.83 1.79 2.13 2.12
IA2C 2.08 1.73 1.89 1.87 1.84 1.62 1.55 1.43 1.97
Average 2.29 1.85 2.12 2.15 1.89 1.89 1.84 1.70 1.77
Normalized 99.2 81.1 91.6 92.1 81.5 81.7 80.2 73.7 76.7
Table1: Experimentalresultsofaveragerewardon3s_vs_5zmapoftheredesignedSMACenviron-
ment. STUNandselectedbaselines(aftertraining)areeachteamedupwith8differentunknown
agents,respectively. Eachrowrepresentstheteamingperformancewithadifferentunknownagent.
STUNachievesnearlyoptimalperformanceinnearlyallscenarios,demonstratingitsrobustperfor-
mance.
4.1 Multi-AgentParticleEnvironment
WefirstconsiderPredator-PreyfromtheMPEenvironment,whichisapartiallyobservablemulti-
agentenvironmentthatinvolvesN AI-controlledblueagentsandM adversaryagents. Halfofthe
blueagentsareunknownagentswithlatentrewards,whiletherestoftheblueagentsarecollaborative
AIagents. Meanwhile,theadversariesfollowafixedstrategy: theymovetowardsthenearestagent,
anddifferentadversarieswillchoosedifferenttargets.
Tocreateunknownagentswithdiversebehaviors/objectives,weconsiderfourmethodsofcreating
rewardcomponentsandcombinethemintomorecomplexrewardfunctionsR (s,a)usingeither
B
alinearfunctionoranon-linearmixingnetwork(e.g., asingle-layerneuralnetwork)withlatent
parametersB. Weconsider (i)GreedyReward: AsPreysgetclosertoeachother,theyareless
greedyintermsofexploration,thusresultinginanegativereward. (ii)SafetyReward:Preyattempts
toevadePredatorsandreceivesanegativerewardproportionaltothedistance. (iii)CostReward:
MovementbythePreyconsumesenergy,sowhenthePreymoves,italsoreceivesanegativereward.
(iv)PreferenceReward:DifferentweightscanbeassignedtoPredator-Praypairsintheotherclasses
toreflectindividualpreferences/importance. Wenotethatthecombinationofthesemethodsallows
thecreationofcomplexrewardfunctionswithmanydimensions.
We evaluate the performance of STUN agents teaming up with unknown agents and focus on
two key aspects: (1) Adaptability: evaluating whether trained STUN agents can maintain high
teamingperformancewhencollaboratingwithnew,unknownagentsoragentswithtime-varying
behaviors/objectives. (2)Interpretabilityofbehavior: Assessinghowcollaborativeagentsâ€™behaviors
varyunderdifferentunknownagentswithlatentB. Wealsoperformablationstudiestoverifythe
impactof(i)zero-shotadaptationusinggoal-conditionedpoliciesand(ii)activegoalinferenceofour
proposeddesign,aswellastheimpactofhigh-dimensional,linear,andnonlinearrewardfunctions.
Teamingbehaviorinterpretation. TobetterinterpretSTUNagentbehaviors,wefocusonGreedy
RewardsandSafetyRewardsinthisexperiment. WhilemoredetailsareprovidedinAppendixC.1.2,
weshowinFig.3(a)(a)theachievedtradeoffbetweenthetworewardcomponentswhencollaborating
withunknownagentsofdifferentlatentB. Astheunknownagentstendtomovefromplayingsafe
(i.e.,stayingawayfrompredators)tobeinggreedy(i.e.,moreaggressivelyexploring),STUNagents
adapt their policies and also become more greedy â€“ as shown by diminishing safety return and
increasinggreedyreturn. MoreteaminganalysisandillustrationsareprovidedinAppendixC.1.2.
Collaboratingwithchangingunknownagents. Duringtheteaming/executionstage,wedeploy
trainedSTUNagentsalongsideunknownagentswithchangingbehavior. Specifically,theunknown
agents vary their policies at the beginning of every 20 epochs. This requires STUN agents to
continuallyreason/inferthetime-varyingrewardoftheunknownagentsandthenperformzero-shot
policyadaptationonthefly. Fig.3(a)(b)showsthatSTUNagentscanswiftlyadapttheirpolicies
in just 5-10 epochs (with goal inference and zero-shot policy adaptation) and ramp up teaming
performanceindifferentenvironmentswithM adversaries.
8Ablationstudies. Wenowperformanablationstudyto(i)removethezero-shotpolicyadaptation
inSTUNagentsbyinsteadperformingadditionalonlinereinforcementlearningusingtheinferred
reward and (ii) remove the active goal inference by conditioning STUN agents on fixed reward
parameters â€“ labeled â€œmulti-task" and â€œfix" respectively in Fig. 3(a)(c). Significant performance
degradationsareobservedcomparedtoSTUNagentslabeledâ€œnonlinear-4dm". Forscalability,in
Fig. 3(a)(c), we further vary the dimensions of underlying reward components from 2 to 6 and
evaluateSTUNagentsoverbothlinearandnon-linearrewardfunctions(e.g.,soft-maxandsingle-
layernetworkwithparametersB). ThenumericalresultsdemonstrateSTUNagentsâ€™robustteaming
performancewithincreasinglycomplexunknownrewardstructures.
4.2 StarCraftMulti-AgentChallenge
Inthissection,weperformextensiveevaluationsoftheproposedframeworkonSMACtasks(e.g.,
hardandsuper-hardmaps)andcompareitwitharangeofbaselinealgorithms. Notethattocreate
unknown agents with different latent rewards, we have redesigned the SMAC environment1 to
considertwobroadclassesofrewards: ConservativeRewardsthatarerepresentedbythehealth
valuesofsurvivingfriendlyblue-teamagentsandAggressiveRewardsthatarerepresentedbythe
totaldamageinflictedonadversarialred-teamagents.Thisdesignallowsustocreatediverseunknown
agentswithdifferentlatentrewardfunctionsandplaystyles,rangingfromconservativetoaggressive
as parameterized by the latent B. Teaming performance is measured using the achieved (latent)
reward. AllotherenvironmentsettingsremainthesameasstandardSMAC.Detailedinformationon
oursettingsandtrainingconfigurations,likehyperparametersused,canbefoundintheAppendix.
We consider 6 maps selected from SMAC with varying levels of difficulty and create 7 types of
unknownagentseitherbytrainingwithfixedunknownbehaviorsorbydirectlyusingagentsfrom
popularMARLalgorithms,includingMAPPO,IPPO,COMA,andIA2C Yuetal.[2022],Schulman
etal.[2017],Foersteretal.[2018],Mnihetal.[2016]. WedeploytrainedSTUNagentsinthese
collaborativetasksagainsteachtypeofunknownagentsandcomparetheperformancetoanumber
ofbaselinessuchasoptimalfixed-behavioragents(e.g.,withconservative(FBA-C),balanced(FBA-
B),andaggressive(FBA-A)playstyles)andcollaborativeagentsthatemploymulti-tasklearning
by randomly sampling the unknown agentsâ€™ latent parameters. In the following evaluations, we
willdemonstrate: (1)TheproposedKD-BILcanaccuratelyinferlatentrewardparametersB;(2)
STUN agents can efficiently team up with unknown agents and outperform baselines on various
SMACtasks;and(3)STUNagentsdemonstraterobustperformancewithdiverseunknownagent
behaviors/objectives.
Evaluatinggoalinferenceagainstgroundtruth. Wevalidatetheeffectivenessofourproposed
goalinferencealgorithm,KD-BIL,byshowingthecorrelationbetweentheestimateposteriorand
thegroundtruth(intermsofthelatentrewardparameterB)inFig.5ontwomaps,3s_vs_5z and
corridor. Eachrowintheheatmapshowstheposteriordistributionofonegivenrewardparameter
(whichintheidealcasewouldconcentrateonthediagonalline). Theresultshowsthatourproposed
goalinferencecanaccuratelyestimatethelatentreward,evenincomplextasksthatinvolvealarge
numberofblue/redagentsandrequireadvancedstrategies(e.g,onthecorridormap). Theanalysisof
goalinferenceonothermapsareprovidedintheappendix.
Evaluatingperformanceondifferentmaps. WedeploytrainedSTUNagentsalongsideunknown
agentson6differentmapsandrepeattheexperimentswithseveralSOTAbaselinesforcomparison:
MAPPO Yu et al. [2022], COMA Foerster et al. [2018], MAA2C, IPPO Schulman et al. [2017],
IA2CMnihetal.[2016]. Thesebaselineagentsaretrainedusingamulti-tasklearningapproach
by randomly sampling latent B, so that they can collaborate with unknown agents of different
behaviors/objectives. The results, as shown in Fig. 4, demonstrate that the STUNâ€™s pre-training
method can effectively converge and significantly improve teaming performance (up to 50% on
certainsuper-hardmaps). ThesettingsaredetailedintheAppendix.
Teaming performance with various unknown agents. We deploy the trained STUN agents
alongside 8 different unknown agents on 3s_v_5z map and compare the teaming performance
againsttwogroupsofbaselinesâ€“fixed-behavioragentsandmulti-taskagentstrainedusingdifferent
algorithms â€“ which are also deployed alongside the same unknown agents. Table 1 summarizes
1StandardSMACenvironmentconsidersonlywinningrateasthereward,whichisinsufficientforcreating
diverseunknownagentswithlatentrewardsforourevaluation.
90.9
1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.9
0.39 0.47 0.01 0.00 0.00 0.00 0.00 0.00 0.00
0.8
0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.8
0.22 0.46 0.23 0.03 0.00 0.00 0.00 0.00 0.00
0.7
0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00
0.7
0.03 0.26 0.36 0.33 0.02 0.00 0.00 0.00 0.00
0.6
0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00
0.6
0.00 0.04 0.32 0.31 0.31 0.02 0.00 0.00 0.00
0.5
0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00
0.5
0.00 0.00 0.02 0.13 0.71 0.13 0.02 0.00 0.00
0.4
0.00 0.00 0.00 0.00 0.08 0.92 0.00 0.00 0.00
0.4
0.00 0.00 0.00 0.04 0.22 0.38 0.33 0.03 0.00
0.3
0.00 0.00 0.00 0.00 0.00 0.00 0.06 0.94 0.00
0.3
0.00 0.00 0.00 0.00 0.01 0.33 0.34 0.30 0.02
0.2
0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00
0.2
0.00 0.00 0.00 0.00 0.00 0.06 0.10 0.84 0.00
0.1
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00
0.1
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.23 0.77
0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
(a) 3s_vs_5z (b) corridor
Figure5: Anillustrationofthecorrelationbetweentheposteriorestimateofrewardparameters
(shownineachrow)usingKD-BILandtheground-truthrewardparameters. Ourproposedactive
goalinferencecanaccuratelyinferthelatentrewardfromobservedunknownagenttrajectories.
numericalresults,witheachrowcomparingtheteamingperformanceofvariouscollaborativeagents
alongside the same unknown agent. In particular, we calculate a normalized teaming score by
assigning100pointstothebest-performingagentineachrowandthentakingtheaverageoverall8
unknownagents. STUNagentsachieveanormalizedscoreof99.2outofamaximumof100,with
thebestperformanceinnearlyallscenariosanddemonstratingrobustteamingperformancewitha
diverserangeofunknownagents.
5 Conclusions
Inthispaper,wepresentSTUN,anovelframeworkforenhancingAIandunknown-agentteaming
in collaborative task environments. Considering unknown agents with latent rewards, we show
that unbiased reward estimates are sufficient for optimal collaboration. By leveraging the KD-
BIL algorithm for active goal inference and enabling a zero-shot policy adaptation, STUN can
synergisticallyteamupwithunknownagentstowardlatentrewards. Evaluationsusingmulti-agent
environments including MPE and SMAC demonstrate robust teaming performance with diverse
unknownagentsandalsoagentswithtime-varyingreward,outperforminganumberofbaselines.
Synergisticteamingwithunknownagentsinnon-stationarytasksorunderrestrictedobservationsare
avenuesforfuturework.
References
Matthew Johnson, Micael Vignatti, and Daniel Duran. Understanding human-machine teaming
throughinterdependenceanalysis. InContemporaryResearch,pages209â€“233.CRCPress,2020.
AllanDafoe,EdwardHughes,YoramBachrach,TantumCollins,KevinRMcKee,JoelZLeibo,Kate
Larson,andThoreGraepel. OpenproblemsincooperativeAI. arXivpreprintarXiv:2012.08630,
2020.
YoumingTao,YulianWu,PengZhao,andDiWang. Optimalratesof(locally)differentiallyprivate
heavy-tailed multi-armed bandits. In International Conference on Artificial Intelligence and
Statistics,pages1546â€“1574.PMLR,2022.
MargaretLTraeger,SarahStrohkorbSebo,MalteJung,BrianScassellati,andNicholasAChris-
takis. Vulnerablerobotspositivelyshapehumanconversationaldynamicsinahumanâ€“robotteam.
ProceedingsoftheNationalAcademyofSciences,117(12):6370â€“6375,2020.
StefanoVAlbrechtandPeterStone. Autonomousagentsmodellingotheragents: Acomprehensive
surveyandopenproblems. ArtificialIntelligence,258:66â€“95,2018.
10MonikaSimmlerandRuthFrischknecht. Ataxonomyofhumanâ€“machinecollaboration: Capturing
automationandtechnicalautonomy. Ai&Society,36(1):239â€“250,2021.
KyleJBehymerandJohnMFlach. Fromautonomoussystemstosociotechnicalsystems: Designing
effectivecollaborations. SheJi: TheJournalofDesign,Economics,andInnovation,2(2):105â€“114,
2016.
Matthijs TJ Spaan. Partially observable markov decision processes. In Reinforcement learning:
State-of-the-art,pages387â€“414.Springer,2012.
KarlWeiss,TaghiMKhoshgoftaar,andDingDingWang. Asurveyoftransferlearning. Journalof
Bigdata,3(1):1â€“40,2016.
DongxiaoYu,ZongruiZou,ShuzhenChen,YoumingTao,BingTian,WeifengLv,andXiuzhenCheng.
Decentralizedparallelsgdwithprivacypreservationinvehicularnetworks. IEEETransactionson
VehicularTechnology,70(6):5211â€“5220,2021.
Yu Zhang and Qiang Yang. An overview of multi-task learning. National Science Review, 5(1):
30â€“43,2018.
Aishwarya Mandyam, Didong Li, Diana Cai, Andrew Jones, and Barbara Engelhardt. Efficient
Bayesian inverse reinforcement learning via conditional kernel density estimation. In Fourth
SymposiumonAdvancesinApproximateBayesianInference,2022.
Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs,
volume1. Springer,2016.
Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for
decentralizedplanning. Neurocomputing,190:82â€“94,2016.
RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch. Multi-agentactor-
criticformixedcooperative-competitiveenvironments. NeuralInformationProcessingSystems
(NIPS),2017.
SWhiteson,MSamvelyan,TRashid,CSDeWitt,GFarquhar,NNardelli,TGJRudner,CMHung,
PHSTorr,andJFoerster. Thestarcraftmulti-agentchallenge. InProceedingsoftheInternational
Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS, pages 2186â€“2188,
2019.
ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu. The
surprisingeffectivenessofPPOincooperativemulti-agentgames. AdvancesinNeuralInformation
ProcessingSystems,35:24611â€“24624,2022.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShimonWhiteson.
Counterfactualmulti-agentpolicygradients. InProceedingsoftheAAAIconferenceonartificial
intelligence,volume32,2018.
VolodymyrMnih,AdriaPuigdomenechBadia,MehdiMirza,AlexGraves,TimothyLillicrap,Tim
Harley,DavidSilver,andKorayKavukcuoglu. Asynchronousmethodsfordeepreinforcement
learning. InInternationalconferenceonmachinelearning,pages1928â€“1937.PMLR,2016.
HengyuanHuandDorsaSadigh. Languageinstructedreinforcementlearningforhuman-AIcoordi-
nation. arXivpreprintarXiv:2304.07297,2023.
JakobBauer,KateBaumli,FeryalBehbahani,AvishkarBhoopchand,NathalieBradley-Schmieg,
MichaelChang,NatalieClay,AdrianCollister,VibhavariDasagi,LucyGonzalez,etal. Human-
timescale adaptation in an open-ended task space. In International Conference on Machine
Learning,pages1887â€“1935.PMLR,2023.
JiayuChen,BhargavGanguly,YangXu,YongshengMei,TianLan,andVaneetAggarwal. Deep
generativemodelsforofflinepolicylearning:Tutorial,survey,andperspectivesonfuturedirections.
arXivpreprintarXiv:2402.13777,2024.
11ShariqIqbalandFeiSha. Actor-attention-criticformulti-agentreinforcementlearning. InInterna-
tionalconferenceonmachinelearning,pages2961â€“2970.PMLR,2019.
JakobFoerster,NantasNardelli,GregoryFarquhar,TriantafyllosAfouras,PhilipHSTorr,Pushmeet
Kohli,andShimonWhiteson. Stabilisingexperiencereplayfordeepmulti-agentreinforcement
learning. InInternationalconferenceonmachinelearning,pages1146â€“1155.PMLR,2017.
YongshengMei,HanhanZhou,andTianLan. Projection-optimalmonotonicvaluefunctionfactoriza-
tioninmulti-agentreinforcementlearning. InProceedingsofthe2024InternationalConference
onAutonomousAgentsandMultiagentSystems,2024a.
YongshengMei,HanhanZhou,TianLan,GuruVenkataramani,andPengWei. Mac-po: Multi-agent
experiencereplayviacollectivepriorityoptimization. arXivpreprintarXiv:2302.10418,2023.
Mark L Knapp, Judith A Hall, and Terrence G Horgan. Nonverbal communication in human
interaction. CengageLearning,2013.
PaulRobinette,WenchenLi,RobertAllen,AyannaMHoward,andAlanRWagner. Overtrustof
robotsinemergencyevacuationscenarios. In201611thACM/IEEEInternationalConferenceon
Human-RobotInteraction(HRI),pages101â€“108.IEEE,2016.
LaÃ«titiaMatignon,GuillaumeJLaurent,andNadineLeFort-Piat.Hystereticq-learning:analgorithm
for decentralizedreinforcement learning incooperative multi-agentteams. In 2007 IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems,pages64â€“69.IEEE,2007.
LiviuPanait,KeithSullivan,andSeanLuke. Lenientlearnersincooperativemultiagentsystems.
InProceedingsofthefifthinternationaljointconferenceonAutonomousagentsandmultiagent
systems,pages801â€“803,2006.
HepengLiandHaiboHe. Multiagenttrustregionpolicyoptimization. IEEETransactionsonNeural
NetworksandLearningSystems,2023.
TabishRashid,GregoryFarquhar,BeiPeng,andShimonWhiteson. Weightedqmix: Expanding
monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning. Advancesin
neuralinformationprocessingsystems,33:10199â€“10210,2020.
HanhanZhou,TianLan,andVaneetAggarwal. PAC:Assistedvaluefactorizationwithcounterfactual
predictionsinmulti-agentreinforcementlearning. AdvancesinNeuralInformationProcessing
Systems,35:15757â€“15769,2022.
TianpeiYang, WeixunWang, HongyaoTang, JianyeHao, ZhaopengMeng, HangyuMao, Dong
Li,WulongLiu,YingfengChen,YujingHu,etal. Anefficienttransferlearningframeworkfor
multiagent reinforcement learning. Advances in Neural Information Processing Systems, 34:
17037â€“17048,2021.
Hanhan Zhou, Tian Lan, and Vaneet Aggarwal. Value functions factorization with latent state
informationsharingindecentralizedmulti-agentpolicygradients. IEEETransactionsonEmerging
TopicsinComputationalIntelligence,2023a.
HanhanZhou,TianLan,andVaneetAggarwal. Doublepolicyestimationforimportancesampling
insequencemodeling-basedreinforcementlearning. InNeurIPS2023FoundationModelsfor
DecisionMakingWorkshop,2023b.
JingdiChen,YimengWang,andTianLan. Bringingfairnesstoactor-criticreinforcementlearning
for network utility optimization. In IEEE INFOCOM 2021-IEEE Conference on Computer
Communications,pages1â€“10.IEEE,2021.
Jingdi Chen, Tian Lan, and Nakjung Choi. Distributional-utility actor-critic for network slice
performanceguarantee. InProceedingsoftheTwenty-fourthInternationalSymposiumonTheory,
AlgorithmicFoundations,andProtocolDesignforMobileNetworksandMobileComputing,pages
161â€“170,2023a.
RichCaruana. Multitasklearning. Machinelearning,28:41â€“75,1997.
12AndreiARusu,NeilCRabinowitz,GuillaumeDesjardins,HubertSoyer,JamesKirkpatrick,Koray
Kavukcuoglu,RazvanPascanu,andRaiaHadsell. Progressiveneuralnetworks. arXivpreprint
arXiv:1606.04671,2016.
ShayeganOmidshafiei,JasonPazis,ChristopherAmato,JonathanPHow,andJohnVian.Deepdecen-
tralizedmulti-taskmulti-agentreinforcementlearningunderpartialobservability. InInternational
ConferenceonMachineLearning,pages2681â€“2690.PMLR,2017.
NareshKumarGurulingan,BahramZonooz,andElaheArani. Multi-taskstructurallearningusing
localtasksimilarityinducedneuroncreationandremoval. arXivpreprintarXiv:2305.00441,2023.
GasserAudaandMohamedKamel. Modularneuralnetworks: asurvey. Internationaljournalof
neuralsystems,9(02):129â€“151,1999.
ZhaoyangNiu,GuoqiangZhong,andHuiYu. Areviewontheattentionmechanismofdeeplearning.
Neurocomputing,452:48â€“62,2021.
AlexanderSashaVezhnevets,SimonOsindero,TomSchaul,NicolasHeess,MaxJaderberg,David
Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In
InternationalConferenceonMachineLearning,pages3540â€“3549.PMLR,2017.
YipingWang,YifangChen,KevinJamieson,andSimonShaoleiDu. Improvedactivemulti-task
representation learning via lasso. In International Conference on Machine Learning, pages
35548â€“35578.PMLR,2023.
JingdiChen,HanhanZhou,YongshengMei,GinaAdam,NathanielDBastian,andTianLan. Real-
time network intrusion detection via decision transformers. arXiv preprint arXiv:2312.07696,
2023b.
AndrewYNg,StuartRussell,etal. Algorithmsforinversereinforcementlearning. InIcml,volume1,
page2,2000.
BrianDZiebart,AndrewLMaas,JAndrewBagnell,AnindKDey,etal. Maximumentropyinverse
reinforcementlearning. InAAAI,volume8,pages1433â€“1438.Chicago,IL,USA,2008.
PieterAbbeelandAndrewYNg. Apprenticeshiplearningviainversereinforcementlearning. In
Proceedingsofthetwenty-firstinternationalconferenceonMachinelearning,page1,2004.
DeepakRamachandranandEyalAmir. Bayesianinversereinforcementlearning. InIJCAI,volume7,
pages2586â€“2591,2007.
YongshengMei,TianLan,MahdiImani,andSureshSubramaniam. Abayesianoptimizationframe-
workforfindinglocaloptimainexpensivemulti-modalfunctions.arXivpreprintarXiv:2210.06635,
2022.
YongshengMei,MahdiImani,andTianLan. Bayesianoptimizationthroughgaussiancoxprocess
modelsforspatio-temporaldata. arXivpreprintarXiv:2401.14544,2024b.
Jaedeug Choi and Kee-Eung Kim. Nonparametric Bayesian inverse reinforcement learning for
multiplerewardfunctions. Advancesinneuralinformationprocessingsystems,25,2012.
AlexJChanandMihaelavanderSchaar. ScalableBayesianinversereinforcementlearning. arXiv
preprintarXiv:2102.06483,2021.
AadWVanderVaart. Asymptoticstatistics,volume3. Cambridgeuniversitypress,2000.
ChristopherJCHWatkinsandPeterDayan. Q-learning. Machinelearning,8:279â€“292,1992.
TommiJaakkola,MichaelJordan,andSatinderSingh. Convergenceofstochasticiterativedynamic
programmingalgorithms. Advancesinneuralinformationprocessingsystems,6,1993.
13A Pseudo-code
Thepseudo-codeoftheproposedSTUNframeworkispresentedinAlgorithm1. Theprocesscan
be divided into three stages: 1) Pre-train STUN agent, where STUN agents adaptable to various
latentstylesaretrained. 2)DeploySTUNagent,wheretheSTUNagentisdeployedalongsideother
Unknownagents,andthetrajectoriesoftheUnknownagentsarecollected. 3)Inverselearningthe
latentreward,wherethelatentstyleoftheUnknownagentisestimated. Furthermore,theSTUN
agentwillcomputeBË†andperformalongsidetheUnknownagentsusingR . Overall,thesecondand
BË†
thirdstagescanbecombinedintotheEvaluationstage.
Algorithm1OurProposedSTUNFramework
//Pre-trainSTUNagent
ConsidernAIagentsalongsiden surrogateagents.
u
forepisode=1toM do
SamplelatentstyleBfromauniformdistribution
fori=1ton+n do
u
UpdatepolicywithB
endfor
endfor
//DeploySTUNagent
ReplacesurrogateagentswithUnknownagentsÏ€
u
STUNagentsobservetrajectoriesÏ„ufromunknownagentsÏ€
u
//Inverselearningthelatentreward
fori=1ton do
InverselearnBË†asthelatentstyleBofÏ€
u
UseR inthepolicyexecution,whereBËœisthemeanofBË†.
BËœ
endfor
B MathematicalDetails
Proof. 3.1AccordingtoBayesâ€™theorem,wecandefineeachtermasfollows: thepriorprobability
p(R),thelikelihoodprobabilitypË† ({ou,au}n |R)andthemarginalprobabilitypË† ({ou,au}n ):
m i i i=1 m i i i=1
pË† ({ou,au}n |R)p(R)
pË† (ou,au|R)= m i i i=1
m i i pË† ({ou,au}n )
m i i i=1
(cid:81)n pË† (ou,au|R)p(R)
= i=1 m i i
pË† ({ou,au}n )
m i i i=1
n
(cid:89)
âˆ pË† (ou,au|R)p(R)
m i i
(8)
i=1
âˆp(R)(cid:89)n (cid:88)m K((ou i,au i),(o j,a j))Â·K R(R,R j)
(cid:80)m
K (R,R )
i=1j=1 l=1 R l
(cid:89)n (cid:88)m eâˆ’ds((ou i,au i),(oj,aj))2/(2h)eâˆ’dr(R,Rj)2/(2hâ€² )
âˆp(R)
(cid:80)m eâˆ’dr(R,Rl)2/(2hâ€²)
i=1j=1 l=1
WebeginbyformulatinganexpressionbasedonBayesâ€™theorem. Next,weincorporatepË† (ou,au|R)
m i i
intoouranalysis.
LemmaB.1. Therandomprocess{âˆ† (x)}definedas
t
âˆ† (x)=(1âˆ’Î± (x))âˆ† (x)+Î± (x)F (x)
t+1 t t t t
convergestozerow.p.1underthefollowingassumptions:
14â€¢ 0â‰¤Î± â‰¤1,(cid:80) Î± (x)=âˆand(cid:80) Î±2(x)<âˆ
t t t t t
â€¢ âˆ¥E[F (x)]âˆ¥ â‰¤Î³âˆ¥âˆ† âˆ¥ ,withÎ³ â‰¤1.
t q t q
â€¢ Var(F (x))â‰¤C(1+âˆ¥âˆ† âˆ¥2),forC >0.
t t q
Here,Î± (x)isallowedtodependonthepastinsofarastheaboveconditionsremainvalid.
t
Proof. B.1SeethelectureJaakkolaetal.[1993]
Proof. 3.2weabbreviates ,s ,Q ,Q andÎ± ass,sâ€²,Q ,Qâ€² andÎ±respectively.
t t+1 B,t B,t+1 t B B
SubtracttheoptimalQâˆ—(s,a)frombothsidesinEqn.4:
B
Qâ€² (s,a)âˆ’Qâˆ—(s,a)=(1âˆ’Î±)(Q (s,a)âˆ’Qâˆ—(s,a))+Î±[R +Î³maxQ (sâ€²,b)âˆ’Qâˆ—(s,a)]
B B B B BËœ B B
bâˆˆA
Letâˆ† =Q (s,a)âˆ’Qâˆ—(s,a)andF (s,a)=R +Î³maxQ (sâ€²,b)âˆ’Qâˆ—(s,a)
t B B t BËœ B B
bâˆˆA
âˆ† (sâ€²,a)=(1âˆ’Î±)âˆ† (s,a)+Î±F (s,a)
t+1 t t
(cid:88) (cid:88)
E[F (s,a)]= p(R |s,a)R + p(sâ€²|s,a)Î³maxQ (sâ€²,b)âˆ’Qâˆ—(s,a)
t BËœ BËœ B B
bâˆˆA
R BËœâˆˆR sâ€²âˆˆS
(cid:88) (cid:88) (cid:88) (cid:88)
= p(R |s,a)R + p(sâ€²|s,a)Î³maxQ (sâ€²,b)âˆ’ p(R |s,a)R âˆ’ p(sâ€²|s,a)Î³maxQâˆ—(sâ€²,b)
BËœ BËœ B B B B
bâˆˆA bâˆˆA
R BËœâˆˆR sâ€²âˆˆS RBâˆˆR sâ€²âˆˆS
(cid:88) (cid:88) (cid:88)
= p(R |s,a)R âˆ’ p(R |s,a)R + p(sâ€²|s,a)Î³[maxQ (sâ€²,b)âˆ’maxQâˆ—(sâ€²,b)]
BËœ BËœ B B B B
bâˆˆA bâˆˆA
R BËœâˆˆR RBâˆˆR sâ€²âˆˆS
(cid:88)
= p(sâ€²|s,a)Î³[maxQ (sâ€²,b)âˆ’maxQâˆ—(sâ€²,b)]
B B
bâˆˆA bâˆˆA
sâ€²âˆˆS
(cid:88)
â‰¤Î³ p(sâ€²|s,a) max |Q (sâ€²,b)âˆ’Qâˆ—(sâ€²,b)|
B B
bâˆˆA,sâ€²âˆˆS
sâ€²âˆˆS
(cid:88)
=Î³ p(sâ€²|s,a) max âˆ¥Q âˆ’Qâˆ—âˆ¥ =Î³âˆ¥Q âˆ’Qâˆ—âˆ¥ =Î³âˆ¥âˆ† âˆ¥
B B âˆ B B âˆ t âˆ
bâˆˆA,sâ€²âˆˆS
sâ€²âˆˆS
(9)
Ourinitialstepinvolvessubstitutingtheexpressionandsimplifyingit. WethenapplyLemma3.3
(cid:80) (cid:80)
andLemma3.4toeliminatetheterms p(R |s,a)R âˆ’ p(R |s,a)R . Thefinalstep
BËœ BËœ B B
R BËœâˆˆR RBâˆˆR
involvesascalingargumenttocompletetheproof.
Finally,
Var[F (s,a)]=E[(F (s,a)âˆ’E[F (s,a)])2]
t t t
ï£®ï£« ï£¶2ï£¹
(cid:88) (cid:88)
=Eï£¯ ï£°ï£­R BËœ+Î³maxQ B(sâ€²,b)âˆ’[ p(R BËœ|s,a)R BËœ+ p(sâ€²|s,a)Î³maxQ B(sâ€²,b)]ï£¸ ï£º
ï£»
bâˆˆA bâˆˆA
R BËœâˆˆR sâ€²âˆˆS
=Var[R +Î³maxQ (sâ€²,b)]
BËœ B
bâˆˆA
(10)
BecauserË†isbounded,itcanbeclearlyverifiedthat
Var[F (s,a)]â‰¤C(1+âˆ¥âˆ† âˆ¥2)
t t q
for some constant C. Then, due to the Lemma B.1,âˆ† converges to zero w.p.1, i.e. Qâ€² (s,a)
t B
convergestoQâˆ—(s,a)
B
15Proof. 3.3BasedonthelinearassumptionofR (s,a),wecanrewritetheexpectedrewardas:
B
E[R (s,a)]=E[BËœTR(s,a)]
BËœ
SinceBËœ=E[B|BË†],wecanreplaceBËœintheaboveequationwithE[B|BË†]:
E[R (s,a)]=E[E[B|BË†]TR(s,a)]
BËœ
UtilizingtheLawofIteratedExpectations,wecancombinetheinnerconditionalexpectationwith
theouterexpectation:
E[R (s,a)]=E[BTR(s,a)]
BËœ
SinceBisafixedbutunknownparameter,wecandirectlyremovetheexpectation:
E[R (s,a)]=BTR(s,a)=R (s,a)
BËœ B
Therefore,whentherewardfunctionisalinearfunctionoflatentparameters,BËœ= E[B|BË†]indeed
providesanunbiasedestimate.
Proof. 3.4WeconsiderR ,whichisR(Râˆ’1(E[R |BË†])). SinceRandRâˆ’1areinversefunctionof
BËœ B
eachother,thissimplifiestoE[R |BË†].
B
SinceBËœisdefinedbasedonE[R |BË†],wecaninferthatE[R ]=E[E[R |BË†]].
B BËœ B
AccordingtotheLawofIteratedExpectations,E[E[R |BË†]]=E[R ].
B B
Therefore,wehaveproventhatE[R ]=E[R ],whichimpliesthatBËœisanunbiasedestimatorofB.
BËœ B
Proof. 3.5First,Letâ€™sstartwiththederivationofthestatevaluefunction:
(cid:88)
âˆ‡ VÏ€Î¸(s)=âˆ‡ ( Ï€ (a|o,B)QÏ€Î¸(s,a))
Î¸ B Î¸ Î¸ B
aâˆˆA
(cid:88)
= (âˆ‡ Ï€ (a|o,B)QÏ€Î¸(s,a)+Ï€ (a|o,B)âˆ‡ QÏ€Î¸(s,a))
Î¸ Î¸ B Î¸ Î¸ B
aâˆˆA
(cid:88) (cid:88)
= (âˆ‡ Ï€ (a|o,B)QÏ€Î¸(s,a)+Ï€ (a|o,B)âˆ‡ p(sâ€²,R |s,a)(R +Î³VÏ€Î¸(sâ€²)))
Î¸ Î¸ B Î¸ Î¸ B B B
aâˆˆA sâ€²,RB
(cid:88) (cid:88)
= (âˆ‡ Ï€ (a|o,B)QÏ€Î¸(s,a)+Î³Ï€ (a|o,B) p(sâ€²,R |s,a)âˆ‡ VÏ€Î¸(sâ€²))
Î¸ Î¸ B Î¸ B Î¸ B
aâˆˆA sâ€²,RB
(cid:88) (cid:88)
= (âˆ‡ Ï€ (a|o,B)QÏ€Î¸(s,a)+Î³Ï€ (a|o,B) p(sâ€²|s)âˆ‡ VÏ€Î¸(sâ€²))
Î¸ Î¸ B Î¸ Î¸ B
aâˆˆA sâ€²
(11)
Tosimplifytherepresentation,letÏ•(s)=(cid:80) aâˆˆAâˆ‡ Î¸Ï€ Î¸(a|o,B)QÏ€ BÎ¸(s,a),anddefinedÏ€Î¸(sâ†’x,k)
astheprobabilitythatastrategyarrivesatastateafterstartingastepfromthestate.
16âˆ‡ VÏ€Î¸(s)=Ï•(s)+Î³ (cid:88) Ï€ (a|o,B) (cid:88) p(sâ€² |s,a)âˆ‡ VÏ€Î¸(sâ€² )
Î¸ B Î¸ Î¸ B
aâˆˆA sâ€²âˆˆS
=Ï•(s)+Î³
(cid:88) (cid:88)
Ï€
(a|o,B)p(sâ€²
|s,a)âˆ‡
VÏ€Î¸(sâ€²
)
Î¸ Î¸ B
aâˆˆAsâ€²âˆˆS
=Ï•(s)+Î³
(cid:88) dÏ€Î¸(sâ†’sâ€²
,1)âˆ‡ Î¸V
BÏ€Î¸(sâ€²
)
sâ€²âˆˆS
=Ï•(s)+Î³
(cid:88) dÏ€Î¸(sâ†’sâ€² ,1)[Ï•(sâ€²
)+Î³
(cid:88) dÏ€Î¸(sâ€² â†’sâ€²â€²
,1)âˆ‡ Î¸V
BÏ€Î¸(sâ€²â€²
)] (12)
sâ€²âˆˆS sâ€²â€²âˆˆS
=Ï•(s)+Î³ (cid:88) dÏ€Î¸(sâ†’sâ€² ,1)Ï•(sâ€² )+Î³2 (cid:88) dÏ€Î¸(sâ†’sâ€²â€² ,2)âˆ‡ Î¸V BÏ€Î¸(sâ€²â€² )]
sâ€²âˆˆS sâ€²â€²âˆˆS
=...
âˆ
(cid:88)(cid:88)
= Î³kdÏ€Î¸(sâ†’x,k)Ï•(x)
xâˆˆSk=0
DefineÎ·(s)=E s0âˆ¼Âµ,s,a[(cid:80)âˆ k=0Î³kdÏ€Î¸(s
0
â†’s,k)]. Withthis,wereturntotheobjectivefunction:"
J (Î¸)=E [VÏ€Î¸(s )]
Ï€,B s0âˆ¼Âµ,s,a B 0
âˆ
(cid:88) (cid:88)
= E s0âˆ¼Âµ,s,a[ Î³kdÏ€Î¸(sâ†’x,k)]Ï•(s)
sâˆˆS k=0
(cid:88)
= Î·(s)Ï•(s)
sâˆˆS
(cid:88) (cid:88) Î·(s)
=( Î·(s)) Ï•(s)
(cid:80) Î·(s) (13)
sâˆˆS sâˆˆS
sâˆˆS
Î·(s)
âˆ Ï•(s)
(cid:80)
Î·(s)
sâˆˆS
(cid:88) (cid:88)
= VÏ€Î¸(s) QÏ€Î¸(s,a)âˆ‡ Ï€ (a|o,B)
B B Î¸ Î¸
sâˆˆS aâˆˆA
=E [QÏ€Î¸(s,a)âˆ‡ logÏ€ (a|o,B)]
Ï€Î¸ B Î¸ Î¸
C EnvironmentDetails
C.1 MPE
C.1.1 ImplementationdetailsandHyper-parameters
WemodifiedthePredator-preytaskinMPEtoillustratedifferentbehaviorsofvariouslatentstyles.
Specifically,severalagentsmustnavigatea300Ã—3002-dimensionalEuclideanspace,wherethemap
sizeiscustomizable. Eachagentandadversarycanperformoneoffiveactions: remainstationaryor
moveinoneoffourdirections. Eachagenthasaresourceradius,r . Thisradiusimpliesthat
source
whenanotheragententersit,arewardofâˆ’1isreceived,whichisreferredtoasthegreedyreward.
Everyadversaryhasapredationradius,r . Thisradiusmeansthatwhenanotheragententersit,
adv
theagentreceivesarewardofâˆ’1,knownastheSafetyreward. Fromthis,wecandistinguishtwo
differentlatentstyles. Ofcourse,therewardvaluescanalsobesetdifferently,suchasbasedonthe
distancebetweentwounits. CostrewardandPreferencerewardareadditionalsupplementsformore
latentbehavioralstyles. TheCostrewardoccurswhenanagentâ€™schosenactionistomove,thereby
17consumingenergyandreceivingarewardofâˆ’1. ThePreferencereward,ontheotherhand,reflectsa
preferencefordifferentvalueswhenthepreviouslyobtainedrewardisnotâˆ’1.
Theoverallexecutionprocessbeginsatthestartofeachepoch. N agentsandM adversariesare
randomlyplacedontheplane. Thestrategyforadversariesisalwaystochoosethenearestagent,and
differentadversarieswillselectdifferentagents. Then,theprocessrunsfor100stepstocountall
rewardoutcomes,whicharereturnedastheenvironmentâ€™soutput.
C.1.2 Interpretability
Inthereconfiguredenvironmentwesetup,therelationshipbetweenpredatorsandpreyfromthe
naturalworldisauthenticallysimulated. Whenobservinglionsandherdsofcattleonthegrassland,
wefoundthatthecattlewouldclustertogethertofleefrompredators. Wewantedtoreplicatethis
process. WediscoveredthatwhenonlyinfluencedbytheSafetyreward,allagentswouldalways
gathertogethertoescapefromthepursuitofadversaries. Thisissimilartothepatternsobservedin
nature(althoughinnature,thereisbehaviortoprotectyounganimalsinthemiddleoftheherd). To
exhibitdifferentstyles,weintroducedtheGreedyreward. Thismeansthatwhenanagentismoving
awayfromanadversary,theagentalsoexhibitsacertainlevelofselfishness,desiringtopossessmore
resources. ThisimpliesthatwhenonlytheGreedyrewardisineffect,agents,inpursuitofmaximum
resources,willbeevenlydistributedacrosstheentiretwo-dimensionalspace.Bycombiningthesetwo
typesofrewardsinvaryingmagnitudes,differentcontractionradiiemergeastheymoveawayfrom
adversaries. WehaveshowcasedthebehaviorsweobservedinFig.C.1.2. Wecanalsocharacterize
differenttypesofagentsthroughthePreferencereward. Ifanagentreceivesalargerrewardwhen
itisclosertootheragents,thisindicatesthattheagenthasalesserimpactonotheragents. Inthe
naturalworld,thismightcorrespondtoyoungerprey,whichpossessfewerresources. Asaresult,
suchanagentwillalwaysbepositionedinthemiddleofotheragents.
Adversary
STUNagent 1
Unknownagent
STUNagent 2
Adversary
(a) Typeofunits (b) Greedy (c) Safety (d) Greedy (e) Balance (f) Safety
Figure6: ThisisanillustrativediagramexplainingdifferentparameterstylesB. Here,Fig.6(a)
displaysthetypesofunitsinthediagram. Fig.6(b)referstothescenariowhereiftheagentâ€™sstyle
iscompletelygreedy,thenallagentsâ€™actionswouldbeasdepicted: adversariesmovetowardsthe
nearestagent,whileagentsignoreadversariesandrepeleachother,eventuallyachievingauniform
distributioninspace. Iftheagentâ€™sstyleiscompletelysafe,asshowninFig.6(c),thenallagents
willstayawayfromadversariesandclustertogetheratadistance. InfiguresFig.6(d)to6(f),the
transitionfromagreedytoasafeagentstyleisshown,startingfromdistancingfromeachotherto
clusteringtogethertoescapetheadversary,withthedistancebetweenclusteringagentsbeinggreater
formoregreedyagentsandsmallerforsaferones.
TobetterexploretherelationshipbetweendifferentvaluesofN andM,weconductedthefollowing
experiment: WesetupscenarioswithvaryingquantitiesofN andM. Thisillustratesboththeimpact
ofdifferentnumbersofagentsonbehaviorandtheinfluenceoftherelativevaluesofdifferentrewards.
ByobservingtheplotsinFig.C.1.2,wefoundthatthetrendobtainedbetweentheGreedyreward
andSafetyrewardforallagentsisthesameandthatdifferentnumbersofagentsleadtovariationsin
themagnitudeofrewardsobtained.
C.2 Collaboratingwithdynamicagentswithvarying
WeadoptedsixcombinationsofN andM fortesting. Wedividedtheresultsintotwogroupsbased
onthedifferentvaluesofN. Ineachgroupâ€™stestforce,wecooperatedwithSurrogateagentsand
STUNagents. TheSurrogateagentundergoesaBchangeevery20epochs,andeachepochconsists
of25stepsoftraining. EachSTUNagentmaintainsaqueueof300lengths,storingtheobserved
trajectoriesoftheSurrogateagent. Atthebeginningofeachepoch,eachSTUNupdatesBË†usingthe
18           
 * U H H G \  5 H Z D U G
     6 D I H W \  5 H Z D U G
       
   
           
           
     *  6 D U H  I H H  W G  \ \   5  5  H H  Z Z  D D  U G U G          *  6 D U H  I H H  W G  \ \   5  5  H H  Z Z  D D  U G U G
   
 V D I H  V D I H W \  E L D V H G E D O D Q F H G J U H H G  E L D V H G  J U H H G \  V D I H  V D I H W \  E L D V H G E D O D Q F H G J U H H G  E L D V H G  J U H H G \  V D I H  V D I H W \  E L D V H G E D O D Q F H G J U H H G  E L D V H G  J U H H G \
 O D W H Q W  V W \ O H  O D W H Q W  V W \ O H  O D W H Q W  V W \ O H
(a) N=3,M=1 (b) N=3,M=2 (c) N=3,M=4
          
      *  6 D U H  I H H  W G  \ \   5  5  H H  Z Z  D D  U G U G      *  6 D U H  I H H  W G  \ \   5  5  H H  Z Z  D D  U G U G  
    
          
    
          
          
      * U H H G \  5 H Z D U G
            6 D I H W \  5 H Z D U G
 V D I H  V D I H W \  E L D V H G E D O D Q F H G J U H H G  E L D V H G  J U H H G \  V D I H  V D I H W \  E L D V H G E D O D Q F H G J U H H G  E L D V H G  J U H H G \  V D I H  V D I H W \  E L D V H G E D O D Q F H G J U H H G  E L D V H G  J U H H G \
 O D W H Q W  V W \ O H  O D W H Q W  V W \ O H  O D W H Q W  V W \ O H
(d) N=5,M=2 (e) N=5,M=4 (f) N=5,M=7
Figure7: DifferentquantitiesofGreedrewardandSafetyrewardcomponents,denotedbyN andM
respectively.
datafromthequeueandcooperateswiththeSurrogateagentusingBË†toobtainthetestreward. From
thisfigure,wecanseethatwhentheBoftheSurrogateagentchanges,thesystemâ€™srewarddecreases.
ThisisbecausetheSTUNagentcannotcooperatewellwiththeSurrogateagent. However,asthe
datainthequeueisupdated,theSTUNcanestimateBË†well,andthesystemâ€™srewardwillincrease
andstabilizewithinarange.
   
   
   
   
   
   
   
N=3,M=1 N=5,M=2
    N=3,M=2     N=5,M=4
N=3,M=4 N=5,M=7
                                   
 H S R F K  H S R F K
(a) N=3 (b) N=5
Figure8: Every20epochs,theSurrogateagentchangesB. Meanwhile,eachSTUNagentcontinu-
ouslycollectsthetrajectoriesoftheSurrogateagent. Withthegradualcollectionofnewtrajectories
fromtheSurrogateagent,theSTUNagentcanbetterupdatetheestimatedB,therebyachievingan
increaseinreward.
C.3 Ablationstudy
Tovalidatethatouralgorithmcansatisfyhigh-dimensional,linear,andnonlinearrewardfunctions,
weconductedPre-trainingexperiments. Moreover,toverifythenecessityoftheSurrogateduring
thePre-trainingphase,wereplaceditwithbothafixed-styleUnknownagentandarandom-style
Unknownagentfortrainingtogether,resultinginthefollowingsetofplotsinFig.C.3. Eachsetof
resultsrepresentsadifferentcombinationofN andM. Fromobservations,itcanbefoundthatthe
STUNPre-trainingframeworkcanconvergeunderhigh-dimensional,linear,andnonlinearreward
functionscenarios. Furthermore,theSurrogateagentcaneffectivelyassistinconvergence,whereas
19
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H Utrainingwitheitherafixed-styleorarandom-styleUnknownagentdoesnotachievesatisfactory
results.
   
 
 
 O L Q H U   G L P  O L Q H U   G L P  O L Q H U   G L P
 O L Q H U   G L P  O L Q H U   G L P    O L Q H U   G L P
   Q R Q O L Q H U   G L P    Q R Q O L Q H U   G L P  Q R Q O L Q H U   G L P
 Q R Q O L Q H U   G L P  Q R Q O L Q H U   G L P  Q R Q O L Q H U   G L P
   Q R Q O L Q H U   G L P  Q R Q O L Q H U   G L P    Q R Q O L Q H U   G L P
 I L [  I L [  I L [
 P X O W L  W D V N    P X O W L  W D V N  P X O W L  W D V N
 
                                                     
     H S R F K      H S R F K      H S R F K
(a) N=3,M=1 (b) N=3,M=2 (c) N=3,M=4
   
 
  
 O L Q H U   G L P     O L Q H U   G L P  O L Q H U   G L P
 O L Q H U   G L P  O L Q H U   G L P  O L Q H U   G L P
    Q R Q O L Q H U   G L P  Q R Q O L Q H U   G L P     Q R Q O L Q H U   G L P
 Q R Q O L Q H U   G L P     Q R Q O L Q H U   G L P  Q R Q O L Q H U   G L P
 Q R Q O L Q H U   G L P  Q R Q O L Q H U   G L P     Q R Q O L Q H U   G L P
 I L [  I L [  I L [
    P X O W L  W D V N     P X O W L  W D V N  P X O W L  W D V N
  
                                                     
     H S R F K      H S R F K      H S R F K
(d) N=5,M=2 (e) N=5,M=4 (f) N=5,M=7
Figure9: MultiplesetsofplotsfromtheAblationstudy. WecanobservethatourSTUNpre-training
frameworkachievedsuperiorrewardsinallcases.
C.4 SMAC
C.4.1 ImplementationdetailsandHyper-parameters
Inthissection,weintroducetheimplementationdetailsandhyperparametersusedinourexperiments.
Fortheenvironment,weremovedthewinrateastherewardandinsteadusedthehealthofalliedunits
andthedamageinflictedonenemyunitsastherewardfortraining. Thisapproachmayleadtosome
convergenceissuesbecausealargerrewardisalwaysreceivedatthebeginningofthegame. Asthe
gameprogresses,theeffectofthesecondrewardgraduallybecomesapparent,meaningthatthesetwo
typesofrewardscannotreflectdifferentlatentstylesinanyparticularsituation. Moreover,thetwo
rewardsarenotonthesamescale,whichcanalsoaffecttheresults. Westillneedtoimprovethese
issuesintheenvironment. Weusedasetofhyperparametersforeachenvironment,thatis,wedid
notadjusthyperparametersforindividualmaps. Unlessotherwisestated,wekeptthesamesettings
forthecommonhyperparameterssharedbyallalgorithms,suchasthelearningrate,andkepttheir
uniquehyperparametersattheirdefaultsettings.
Batchsizebs=128,replaybuffersize=10000
Targetnetworkupdateinterval: every200episodes
Learningratelr =0.001
tdlambdaÎ»=0.6
Performanceforeachalgorithmisevaluatedfor100(Enumeratedallstylesandtooktheaverage)
episodesevery1000trainingsteps.
C.5 Compareinverselearningwithgroundtruth
Thisisasupplementtothecorrespondingsectionofthemaintext,wherewetestedvariousmap
images. The results can be found in Fig. C.5. The specific experimental setup is as follows: we
testedusingaSurrogateagenttogetherwithaSTUNagent. WetestedalloftheSurrogateagentâ€™sB
separately. WefirstfixedtheSurrogateagentâ€™sBandperformedtestswithSTUNusingalloftheBË†,
wherethetrajectorysizecollectedfortheSurrogateagentwasn=300andfortheSTUNagentwas
20
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H U
 G U D Z H Um=3000. Thevaluesareh=0.03,hâ€² =0.03. KD-BILrequiresustodefinetwotypesofdistances,
thefirstbeingthedistancebetweenrewardfunctions(dr)andthesecondbeingthedistancebetween
trajectories(ds). For(dr),iftherewardfunctionislinear,weusethecosinedistance,andifitis
nonlinear,weusetheEuclideandistance. For(ds),weusetheEuclideandistanceinallcases.
0.91.000.000.000.000.000.000.000.000.00 0.91.000.000.000.000.000.000.000.000.00 0.91.000.000.000.000.000.000.000.000.00 0.90.540.360.000.000.000.000.000.000.00 0.90.890.020.000.000.000.000.000.000.00 0.90.390.470.010.000.000.000.000.000.00
0.80.001.000.000.000.000.000.000.000.00 0.80.001.000.000.000.000.000.000.000.00 0.80.001.000.000.000.000.000.000.000.00 0.80.001.000.000.000.000.000.000.000.00 0.80.010.980.000.000.000.000.000.000.00 0.80.220.460.230.030.000.000.000.000.00
0.70.000.001.000.000.000.000.000.000.00 0.70.000.001.000.000.000.000.000.000.00 0.70.000.100.870.030.000.000.000.000.00 0.70.000.980.020.000.000.000.000.000.00 0.70.000.020.980.000.000.000.000.000.00 0.70.030.260.360.330.020.000.000.000.00
0.60.000.000.001.000.000.000.000.000.00 0.60.000.000.001.000.000.000.000.000.00 0.60.000.000.050.940.010.000.000.000.00 0.60.000.010.420.500.060.000.000.000.00 0.60.000.000.830.160.010.000.000.000.00 0.60.000.040.320.310.310.020.000.000.00
0.50.000.000.000.010.980.010.000.000.00 0.50.000.000.000.001.000.000.000.000.00 0.50.000.000.000.020.970.000.000.000.00 0.50.000.000.000.040.960.000.000.000.00 0.50.000.000.000.400.600.000.000.000.00 0.50.000.000.020.130.710.130.020.000.00
0.40.000.000.000.000.040.950.010.000.00 0.40.000.000.000.000.080.920.000.000.00 0.40.000.000.000.000.120.880.000.000.00 0.40.000.000.000.000.000.001.000.000.00 0.40.000.000.000.000.030.970.000.000.00 0.40.000.000.000.040.220.380.330.030.00
0.30.000.000.000.000.000.001.000.000.00 0.30.000.000.000.000.000.000.060.940.00 0.30.000.000.000.000.000.210.790.010.00 0.30.000.000.000.000.000.040.110.850.00 0.30.000.000.000.000.000.000.920.080.00 0.30.000.000.000.000.010.330.340.300.02
0.20.000.000.000.000.000.000.020.970.01 0.20.000.000.000.000.000.000.001.000.00 0.20.000.000.000.000.000.000.001.000.00 0.20.000.000.000.000.000.000.001.000.00 0.20.070.070.070.070.070.110.120.270.10 0.20.000.000.000.000.000.060.100.840.00
0.10.000.000.000.000.000.000.000.020.98 0.10.000.000.000.000.000.000.000.001.00 0.10.000.000.000.000.000.000.050.060.90 0.10.000.000.000.000.000.000.000.001.00 0.10.000.000.000.000.000.000.000.001.00 0.10.000.000.000.000.000.000.000.230.77
0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
(a) 5m_vs_6m (b) 3s_vs_5z (c) MMM2 (d) (e) 6h_vs_8z (f) corridor
27m_vs_30m
Figure10: TheestimationresultsofBobtainedbyKD-BILindifferentmapdata. Wecanobserve
that in each map, the probability distribution of predicted BË†are closely concentrated around B,
demonstratingtheeffectivenessoftheprediction.
C.6 Comparerewardondifferentmap
Inthissection,weprimarilysupplementthesettingsforPre-training. DuringPre-training,wesample
Bfromauniformdistribution,withresamplingoccurringateachepoch. Additionally,weretestto
obtainrewardsunderallstylesevery100epochsoftraining.
C.7 Tablecomparingtheperformancewithdifferentunknownagents
ThissectionpresentstheresultsofrunningdifferentunknownagentswithdifferentAIagentsonother
maps. WecanobservethattheSTUNagentisalwaysabletocollaboratewithdifferentunknown
agentstoachievesuperiorresults.
SynergisticAgents Fixed-BehaviorAgents Multi-taskAgents
UnknownAgents
STUN FBA-C FBA-B FBA-A MAPPO IPPO COMA MAA2C IA2C
FBA-C 2.724 2.558 1.992 1.939 1.750 1.962 1.313 0.784 1.071
FBA-B 3.490 2.716 3.377 3.264 2.547 2.735 2.320 1.132 0.679
FBA-A 6.290 4.475 5.275 6.543 3.675 4.928 3.373 1.554 1.456
MAPPO 3.792 3.188 2.754 2.981 3.584 2.849 1.396 1.566 1.339
IPPO 3.584 3.452 2.679 2.735 2.924 3.113 1.660 1.113 1.886
COMA 3.679 1.716 2.886 2.622 2.826 2.641 2.735 1.622 1.679
MAA2C 2.509 2.132 2.320 2.018 1.698 2.264 1.113 2.398 1.037
IA2C 2.094 1.924 1.377 1.113 1.792 1.962 1.622 1.452 2.075
Average 3.520 2.770 2.833 2.902 2.600 2.807 1.942 1.453 1.403
Table2: 5m_vs_6m
SynergisticAgents Fixed-BehaviorAgents Multi-taskAgents
UnknownAgents
STUN FBA-C FBA-B FBA-A MAPPO IPPO COMA MAA2C IA2C
FBA-C 1.551 1.505 1.383 1.454 1.464 1.340 1.224 1.297 1.181
FBA-B 3.912 3.486 4.128 3.777 3.783 3.682 3.25 3.087 2.844
FBA-A 6.135 5.967 5.945 6.340 5.470 5.621 5.551 4.605 4.437
MAPPO 4.168 3.608 4.668 4.094 3.939 3.844 3.128 2.885 3.094
IPPO 4.479 3.689 3.777 4.060 3.945 3.757 3.006 3.101 2.925
COMA 3.959 3.541 3.763 3.668 3.5743 3.081 3.682 2.898 2.256
MAA2C 3.75 3.304 3.655 3.608 3.398 3.378 3.3108 3.837 3.033
IA2C 3.614 3.540 3.479 3.421 3.256 3.304 2.939 3.252 3.565
Average 3.946 3.580 3.849 3.803 3.604 3.501 3.261 3.120 2.917
Table3: 6h_vs_8z
21SynergisticAgents Fixed-BehaviorAgents Multi-taskAgents
UnknownAgents
STUN FBA-C FBA-B FBA-A MAPPO IPPO COMA MAA2C IA2C
FBA-C 3.033 3.018 2.732 2.192 1.398 1.128 1.141 1.063 1.024
FBA-B 6.129 5.124 5.351 5.643 3.675 0.870 0.762 0.827 0.881
FBA-A 7.846 6.683 6.972 7.180 5.913 1.398 0.555 1.364 1.718
MAPPO 6.172 5.052 5.075 5.886 3.140 0.729 0.686 1.043 0.848
IPPO 6.048 4.572 5.470 5.654 3.70 0.924 0.762 0.902 0.859
COMA 6.216 4.740 5.189 5.118 3.459 0.675 2.697 0.982 0.913
MAA2C 5.529 4.713 5.459 4.816 2.670 0.718 0.805 1.345 0.740
IA2C 5.410 5.362 4.399 4.551 2.800 0.859 0.827 0.935 1.643
Average 5.798 4.908 5.081 5.130 3.344 0.913 1.029 1.058 0.986
Table4: 27m_vs_30m
SynergisticAgents Fixed-BehaviorAgents Multi-taskAgents
UnknownAgents
STUN FBA-C FBA-B FBA-A MAPPO IPPO COMA MAA2C IA2C
FBA-C 1.696 1.582 1.376 1.394 0.725 0.688 0.704 0.882 1.071
FBA-B 2.588 2.043 2.376 1.820 2.105 2.067 1.354 0.923 1.768
FBA-A 3.423 2.347 2.832 3.325 2.473 2.065 2.390 1.181 2.248
MAPPO 2.823 1.607 1.957 2.066 3.029 2.271 1.523 1.255 1.695
IPPO 2.490 1.521 1.995 1.343 1.932 2.183 1.361 1.712 2.053
COMA 2.546 1.976 1.285 1.436 1.560 2.141 2.410 1.594 1.761
MAA2C 2.360 1.723 1.651 0.951 1.882 1.860 1.253 2.100 1.555
IA2C 2.288 1.760 1.626 1.947 1.975 1.556 1.170 1.335 1.854
Average 2.527 1.820 1.887 1.785 1.960 1.854 1.521 1.373 1.751
Table5: corridor
SynergisticAgents Fixed-BehaviorAgents Multi-taskAgents
UnknownAgents
STUN FBA-C FBA-B FBA-A MAPPO IPPO COMA MAA2C IA2C
FBA-C 2.070 1.948 1.702 1.653 1.678 1.396 0.941 0.998 0.989
FBA-B 3.955 3.736 4.332 3.833 3.689 2.480 0.959 1.094 1.236
FBA-A 7.507 4.846 5.395 7.436 4.507 5.273 1.526 1.741 1.278
MAPPO 5.798 3.496 3.852 4.189 2.704 2.822 1.274 1.145 1.050
IPPO 4.273 3.586 3.766 3.642 3.201 2.654 1.367 1.125 1.226
COMA 3.362 3.285 2.156 2.512 2.143 1.305 2.341 1.433 1.015
MAA2C 3.161 2.773 3.082 2.899 2.220 2.385 1.078 1.945 1.156
IA2C 3.245 2.248 2.691 2.975 2.046 1.869 1.292 0.749 1.452
Average 4.171 3.240 3.372 3.642 2.774 2.523 1.347 1.279 1.175
Table6: MMM2
22