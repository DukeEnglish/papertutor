Towards Generalizability of Multi-Agent Reinforcement Learning
in Graphs with Recurrent Message Passing
JannisWeil ZhenghuaBao
TechnicalUniversityofDarmstadt TechnicalUniversityofDarmstadt
Darmstadt,Germany Darmstadt,Germany
jannis.weil@tu-darmstadt.de zhenghua.bao@stud.tu-darmstadt.de
OsamaAbboud TobiasMeuser
HuaweiTechnologies TechnicalUniversityofDarmstadt
Munich,Germany Darmstadt,Germany
osama.abboud@huawei.com tobias.meuser@tu-darmstadt.de
ABSTRACT havelimitedaccesstolocalinformationduringexecution.Decen-
Graph-basedenvironmentsposeuniquechallengestomulti-agent tralizedapproachesaremorereactivethancentralizedapproaches,
reinforcementlearning.Indecentralizedapproaches,agentsop- asagentscandirectlyreacttolocalchanges.However,havingonly
eratewithinagivengraphandmakedecisionsbasedonpartial localinformationmayleadtosuboptimaldecisions.Acommonway
oroutdatedobservations.Thesizeoftheobservedneighborhood tocounteractthisistoexpandtheobservationsofeachagentby
limitsthegeneralizabilitytodifferentgraphsandaffectsthereac- informationfromtheirdirectneighborhood[30].Includingmore
tivityofagents,thequalityoftheselectedactions,andthecom- nodesintheobservedneighborhoodimprovesdecisionmaking[5]
municationoverhead.Thisworkfocusesongeneralizabilityand butincreasesthecommunicationoverhead.
resolvesthetrade-offinobservedneighborhoodsizewithacontin- Recentworksshowthatgraphneuralnetworks[34]andneural
uousinformationflowinthewholegraph.Weproposearecurrent messagepassing[11]arewellsuitedforapplicationsingraph-based
message-passingmodelthatiterateswiththeenvironmentâ€™ssteps environments,especiallybecausetheycangeneralizetounseen
andallowsnodestocreateaglobalrepresentationofthegraph graphs[33].However,theapproachesoftenassumeacentralized
byexchangingmessageswiththeirneighbors.Agentsreceivethe view[1],explicitcoordinationacrossallagents[2],ortheavailabil-
resultinglearnedgraphobservationsbasedontheirlocationinthe ityoflabeleddatainordertoapplysupervisedlearning[10].
graph.Ourapproachcanbeusedinadecentralizedmanneratrun- Ourgoalistoresolvethetrade-offinlimitedobservedneigh-
timeandincombinationwithareinforcementlearningalgorithm borhoodsinthecontextofdecentralizedmulti-agentsystemsand
ofchoice.Weevaluateourmethodacross1000diversegraphsin graph-basedenvironments.Toachievethis,weproposearecurrent
thecontextofroutingincommunicationnetworksandfindthatit approachinwhichnodesexchangelocalinformationviamessage
enablesagentstogeneralizeandadapttochangesinthegraph. passingtoimprovetheirunderstandingoftheglobalstate.Nodes
refinetheirlocalstatesovertheenvironmentâ€™ssteps,allowinginfor-
KEYWORDS mationtoiterativelytravelthroughthewholegraph.Basedonthese
nodestates,agentsreceivelocation-dependentgraphobservations.
Multi-Agent Reinforcement Learning; Graph Neural Networks;
Our approach provides a novel foundation for learning com-
CommunicationNetworks
municationsystemsinmulti-agentreinforcementlearningandis
ACMReferenceFormat: jointlytrainedinanend-to-endfashion.Ourcontributionsare:
JannisWeil,ZhenghuaBao,OsamaAbboud,andTobiasMeuser.2024.To-
â€¢ Weproposetodecouplelearninggraph-specificrepresenta-
wardsGeneralizabilityofMulti-AgentReinforcementLearninginGraphs
tionsandcontrolbyseparatingnodeandagentobservations.
withRecurrentMessagePassing.InProc.ofthe23rdInternationalConference
â€¢ Tothebestofourknowledge,wearethefirsttoaddressthe
onAutonomousAgentsandMultiagentSystems(AAMAS2024),Auckland,
NewZealand,May6â€“10,2024,IFAAMAS,11pages. problemoflimitedobservedneighborhoodsingraph-based
environmentswithrecurrentgraphneuralnetworks.
1 INTRODUCTION â€¢ Weshowthatthelearnedgraphobservationsenablegener-
alizationover1000diversegraphsinaroutingenvironment,
Thecapabilityofanadaptivesystemdependsonthequalityofits
achievingsimilarthroughputasagentsthatspecializeon
input.Ideally,ithasaccesstothestateandmakesfullyinformedde-
singlegraphswhencombinedwithactionmasking.
cisionsatalltimes.Researchinmulti-agentreinforcementlearning
â€¢ We show that our approach enables agents to adapt to a
rangesfromcentralizedtodecentralizedapproaches[22].Ourfocus
changeinthegraphontheflywithoutretraining.
liesondecentralizedsystemsingraph-basedenvironments.While
Our code is available at https://github.com/jw3il/graph-marl.
thecompletestatemaybeleveragedduringtraining,thesesystems
Theremainderofthispaperisstructuredasfollows.Webeginwith
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems theproblemstatementinSec.2andthenintroduceourapproach
(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6â€“10,2024, inSec.3.WedescribeourevaluationsetupinSec.4,theresultsare
Auckland,NewZealand.Â©2024InternationalFoundationforAutonomousAgents
presentedanddiscussedinSec.5.ThefollowingSec.6providesan
andMultiagentSystems(www.ifaamas.org).ThisworkislicencedundertheCreative
CommonsAttribution4.0International(CC-BY4.0)licence. overviewofrelatedworkandSec.7concludesthepaper.
4202
beF
7
]AM.sc[
1v72050.2042:viXra2 REINFORCEMENTLEARNINGINGRAPHS 3 LEARNEDGRAPHOBSERVATIONS
Reinforcementlearningingraph-basedenvironmentshasgained Weconsiderenvironmentsthatarebasedonagraphandproposeto
muchpopularityinmanyapplicationdomains[28],includingcom- decouplebothcomponents,i.e.agentsdonothavetokeeptrackof
municationnetworks[22].Weconsidermulti-agentenvironments thewholegraphstateandcanbuilduponalower-levelmechanism
that build upon a graphğº (cid:17) (ğ‘‰,ğ¸) âˆˆ G, representing a com- thataggregatesgraphinformation.Theinformationflowisillus-
municationnetworkofnodesğ‘‰ connectedviaundirectededges tratedinFig.1andwillbeexplainedinthefollowingsubsections.
ğ¸ âŠ†ğ‘‰Ã—ğ‘‰.Weassumethattheagentsğ¼arepartofapartiallyobserv-
ablestochasticgame[16]thatrequiresthemtoconsiderthegraph, encode node observations send state to neighbors
i.e.stateğ‘  ğ‘¡ âˆˆğ‘†atstepğ‘¡ containsğº andmayaugmentitwithstate into state and aggregate received states
informationthatcharacterizesthenetwork.Examplesincludeedge 1 2
delaysandcomputeresourcesofnodes.Eachagentğ‘– âˆˆğ¼ receives update local state to
partialobservationsğ‘œ ğ‘¡ğ‘– âˆ¼ğ‘‚ğ‘–(Â·|ğ‘  ğ‘¡)andselectsanactionğ‘ğ‘–
ğ‘¡
âˆˆğ´using with received states
itspolicyğ‘ğ‘–
ğ‘¡
âˆ¼ğœ‹ğ‘–(Â·|ğ‘œ ğ‘¡ğ‘–).Theagentâ€™sgoalistomaximizeitsexpected 3
d anis dco tu imnt eed hore rit zu orn nğ‘‡E ,(cid:2)(cid:205) wğ‘‡ ğ‘¡ hâ€² e= rğ‘¡ eğ›¾ğ‘¡ thâ€²âˆ’ eğ‘¡ğ‘… inğ‘¡ğ‘– dâ€²(cid:3) ivw idit uh ald ris ec wou arn dt sfa ğ‘…c ğ‘¡ğ‘–to =rğ›¾ ğ‘…ğ‘–âˆˆ
(ğ‘ 
ğ‘¡[0 ,ğ‘,1 ğ‘¡]
) local agent
a o cf rft
e
e s atr te ep ls
o
r 2 ce ap a lne gt di
r
t a3io p:n hs
arebasedonthejointactionofallagentsğ‘ ğ‘¡=(ğ‘ğ‘– ğ‘¡)ğ‘–âˆˆğ¼. observation 4 observation
Howagentsobservethegraphandthenetworkâ€™sstateisusually for agent
notdiscussedindepthbyrelatedworks.Acentralizedviewallows
Node
forthebestdecisionmakingbutdoesnotscaletobiggergraphs.
Adecentralizedviewtradesoffreactivityandamountofavailable inter-agent Agent
informationwiththesizeoftheobservedneighborhood.Authors Action communication
usually decide for one of these views and their approaches are
therefore,bydesign,limitedtocertainproblemsorgraphstructures.
Consideradecentralizedapproachwhereanagentislocatedon Figure1:Ourgraphobservationmechanismiterativelydis-
anodeandobservesitsğ‘›-hopneighborhood.Dependingonthe tributesnodestatesviamessagepassing.Agentsinthegraph
environment,theassumptionabouttheobservableneighborhood receivelocalgraphobservationsfordecisionmaking.
iscriticalwithrespecttogeneralizability.Letâ€™simaginetheagent
issupposedtofindtheshortestpathtosomedestinationnodein 3.1 RecurrentMessagePassing
thegraphthatisğ‘›+1hopsaway.Howcanitidentifytheshortest
Thecoreideaofourgraphobservationsisleveragingthemessage-
pathtoadestinationnodethatâ€™snotincludedinitsobservation?
passing framework of graph neural networks [13] to distribute
Therearetwostraight-forwardsolutions:
localinformationinthenetwork.Relatedapproachesareusually
(1) Specializationonasinglegraph.Agentsexplorethewhole used in a centralized manner with a global view [3]. However,
graphduringtraining.Ifthegraphisfixedandnodesare recurrentaggregationfunctions[23,37]spreadmultiplemessage
uniquelyidentifiablebasedontheagentâ€™sobservation,agents passingiterationsovertimeandenabledecentralization[10].We
canspecializeandfindtheoptimalpathtoanynode. introduceasecondrecurrentloopbacktotheinputofthegraph
(2) Expansionoftheobservationspacetoincludethemissing neuralnetworkandlabelthisapproachrecurrentmessagepassing.
information,e.g.to(ğ‘›+1)-hopneighborhoods. Weassumethateachnodeğ‘£ âˆˆğ‘‰ receiveslocalnodeobservations
Withspecialization,thelearnedsolutionwillnotgeneralizeto
ğ‘šğ‘£ âˆ¼ğ‘€ğ‘£(Â· |ğ‘ )basedonanunknownsystemstateğ‘  âˆˆğ‘†.Instead
othergraphs.Ifthetargetgraphdoesnotchange,thiswouldbea ofdirectlyusingthisasaninputofagraphneuralnetwork,each
sufficientbuthighlyinflexiblesolution.Asrealnetworkshavedi- nodeğ‘£ âˆˆğ‘‰ embedsitslocalobservationintoitscurrentnodestate
verseunderlyinggraphsandareusuallydynamic,manyresearchers
â„ğ‘£
withanarbitrarydifferentiablefunctionencode:
consideronlinetrainingonthetargetgraph.However,asreinforce- â„ğ‘£ (cid:17)encode(â„ğ‘£,ğ‘š ğ‘£). (1)
0
ment learning requires agents to explore and make suboptimal Thenodestateâ„ğ‘£
isinitializedtozerointhefirststepandwill
decisions,onlinetrainingfromscratchmightbeunacceptablein
serveasarecurrentloopbetweensubsequentenvironmentsteps.
practice.Incontrast,expandingtheobservationspaceallowsagents
Thisallowsnodestoconsiderpreviouslyaggregatedinformation
togeneralize.However,theobservationrangetoconsidergreatly
when processing observations, similar to auto-regressive graph
dependsontheconcreteproblem.Forexample,forrouting,global
models[31]thatusepredictionsofpreviousstepsastheirinput.
knowledgeisnecessaryifanynodecouldbethedestination.But Atiterationğ‘˜ â‰¥ 0,eachnodesendstheirstateâ„ğ‘£ toalldirect
thentheapproachisnotdecentralizedanymoreandwillnotscale. ğ‘˜
neighborsğ‘¤ âˆˆğ‘(ğ‘£) (cid:17){ğ‘¤ | (ğ‘£,ğ‘¤) âˆˆğ¸}.Eachneighborgenerates
Ourideaistoaddressthisissuebyexpandingtheobservation
anewstatebyfirstaggregatingincomingnodestatesandthen
spacewithlearnedgraphobservationsthatleveragerecurrentmes-
ğ‘˜
updatingitsstateusingarbitrarydifferentiablefunctionsaggregate
sagepassing.Agentsarestillreactiveanddonâ€™thavetogatherin-
ğ‘˜
andupdate .Onemessagepassingiterationisdefinedas:
formationaboutthewholegraphbeforemakingadecision.While
initialdecisionsmaybesuboptimal,thequalityofthelearnedgraph ğ‘€ ğ‘˜ğ‘£ (cid:17)aggregateğ‘˜ ((â„ ğ‘˜ğ‘¤ )ğ‘¤âˆˆğ‘(ğ‘£)) (2)
observationsshouldincreaseovertime.Ideally,theyconvergetoa
â„ğ‘£ (cid:17)updateğ‘˜ (â„ğ‘£,ğ‘€ğ‘£ ). (3)
globalviewandallowagentstomakeoptimaldecisions. ğ‘˜+1 ğ‘˜ ğ‘˜
tnemnorivnE
hparG
stnegAAlgorithm1DistributedNodeStateUpdate
(1) (2) (3)
Input: Nodeğ‘£ withneighborsğ‘(ğ‘£),stateğ‘ ,previousnodestate
â„ğ‘£ ,nodeobservationğ‘šğ‘£
Output:
Updatednodestateâ„ğ‘£ andintermediatestatesğ»ğ‘£
ğ¾
1:
â„ğ‘£ â†encode(â„ğ‘£,ğ‘šğ‘£) âŠ²Encodenodeobservation
0
2: forğ‘˜ â†0toğ¾âˆ’1do âŠ²Updatewithmessagepassing
3:
Sendâ„ ğ‘˜ğ‘£ toallneighborsğ‘¤ âˆˆğ‘(ğ‘£)
4:
Receiveâ„ ğ‘˜ğ‘¤ fromallneighborsğ‘¤ âˆˆğ‘(ğ‘£)
5:
ğ‘€ ğ‘˜ğ‘£ â†aggregateğ‘˜((â„ ğ‘˜ğ‘¤)ğ‘¤âˆˆğ‘(ğ‘£))
6:
â„ ğ‘˜ğ‘£ +1â†updateğ‘˜ (â„ ğ‘˜ğ‘£,ğ‘€ ğ‘˜ğ‘£)
7:
ğ»ğ‘£ â†(â„ ğ‘˜ğ‘£)ğ‘˜ âˆ¥(â„ ğ‘˜ğ‘¤)ğ‘¤âˆˆğ‘(ğ‘£),ğ‘˜ âŠ²Getallintermediatestates
8:
returnâ„ ğ¾ğ‘£,ğ»ğ‘£ Figure 2: Our recurrent message passing model leverages
LSTMcellstoencodethenodeobservationandupdatethe
nodestate.Thehiddenstatesofneighbornodesareaggre-
gatedviasummation,cellstatesremainlocaltoeachnode.
Alg.1showspseudocodeforrecurrentmessagepassingthatis
executedbyallnodesğ‘£ âˆˆğ‘‰.Thereareğ¾ âˆˆN iterationsbetween
oftheLSTMmodulesarenotseparated.Instead,asinglepairof
stepsintheenvironment,i.e.equations(2)and(3)arerepeatedly
applieduntilğ‘˜+1=ğ¾.Thefinalaggregateâ„ğ‘£
andallintermediate
hiddenandcellstatesispassedonbetweenthemodules.
ğ¾
nodestatesğ»ğ‘£ receivedandcalculatedbyğ‘£ canthenbeusedby
3.3 IntegrationinDeepRL
agents,aswewilldetaillater.Inthenextenvironmentstep,weset
â„ğ‘£ =â„ğ‘£ andrepeatthealgorithm.Thenumberofiterationscanbe Graphobservationsarecompatiblewithalldeepreinforcement
ğ¾
adjustedtofittherequirementsofthelearningtask.Anincreased learningapproachesthatallowforbackpropagationthroughthe
numberofiterationsperstepcausesinformationtotraversethe policyâ€™sinput,i.e.theobservationspace.Tothebestofourknowl-
networkfasterbutalsoincreasesthecommunicationoverhead. edge,mostalgorithmsdonothaveanylimitationsinthatregard,
Thisnodestateupdateisperformedateachstepintheenvi- asthepolicyisusuallybasedonadifferentiablefunction.
ronment.Weassumethateachagentğ‘– âˆˆ ğ¼ isassignedtoexactly InordertointegrateourmethodwithdeepRLalgorithms,the
onenodeğ‘£ğ‘– âˆˆğ‘‰ ateachstep.Inadditiontoitsobservationinthe nodestateupdatehastobeperformedateachenvironmentstep
environment,agentğ‘– âˆˆğ¼ thenreceivesalocalgraphobservation duringinferenceandtraining,resultinginanexpandedobservation
ğœ“ğ‘– ofthenodeitisassignedtobasedontheinformationğ»ğ‘£ğ‘– this spacefortheagents.Theintegrationatinferencetimecanbedone
nodereceivedinthisstepviaadifferentiablereadoutfunctionÎ¨: withasimpleenvironmentwrapper.Dependingontheconsidered
algorithm,theintegrationintothetrainingloopwillrequireaddi-
ğœ“ğ‘– (cid:17)Î¨(ğ»ğ‘£ğ‘– ). (4) tionaleffort.Forexample,algorithmsbasedonQ-learningbootstrap
thevaluetargetusingtheobservationsoffuturestates.Inorder
Inthesimplestcase,Î¨couldbetheidentityfunctionofthelatest
tocomputethecorrespondinggraphobservations,wetherefore
nodestateÎ¨(ğ»ğ‘£ğ‘– ) (cid:17)â„ ğ¾ğ‘£ğ‘– ineachiteration,i.e.agentsreceivethe havetocomputeorsamplenodestatesforthesefuturesteps.Ad-
currentstateofthenodetowhichtheyareassigned.Accesstointer- ditionally,asnodestatesareupdatedovermultipleenvironment
mediatenodestatesisnecessarytoallowforskipconnectionsand steps,wehavetounrollthenodestateupdateoverasequenceof
aggregationmechanismslikejumpingknowledgenetworks[41]. stepsandapplybackpropagationthroughtimeinordertolearn
stableupdatefunctions.Thisisalreadyincludedinalgorithmsthat
3.2 ModelArchitecture considerstatefulagentswithrecurrentmodels[20],butwillrequire
Basedonthedesignfromtheprevioussection,weproposeasim- adjustmentsforotherreinforcementlearningalgorithms.
plerecurrentmessagepassingarchitecture.Theencodefunction
3.4 ExemplaryIntegrationinDeepQ-Learning
isrepresentedbyafullyconnectednetworktoembedthenode
observationandanLSTM[17]toupdatethepreviousnodestate Inthissection,weexemplarydescribehowtointegrateourmethod
withthenewembedding.Theaggregatefunctionisthesumofall intoindependentDQN[27]withparametersharingacrossagents.
neighborsâ€™hiddenstates,butcouldbeanygraphconvolutionfrom OurapproachissummarizedinAlg.2,noteworthychangestothe
relatedwork.Finally,updateismodeledbyanotherLSTM.Weshare originalalgorithmarehighlightedinlightgray.Themaindifference
parametersforalliterations,i.e.âˆ€ğ‘˜.updateğ‘˜ =update.Weprovide liesintheintroductionofnodestatesâ„ ğ‘¡ thatareupdatedinparallel
anoverviewofthearchitecturewithFig.2.AnLSTMcelltakesan totheenvironmentstepsbasedonthenodeobservationsğ‘š ğ‘¡ and
inputtensorandapairofhiddenandcellstatetensors(â„,ğ‘)and thepreviousnodestates.Fornotationalsimplicity,wedenotere-
yieldsnewhiddenandcellstates.Inourarchitecture,thehidden
currentmessagepassingcombinedwiththereadoutfunctionÎ¨as
stateisexchangedwithneighbornodesduringaggregation,the adifferentiablefunctionğ‘ˆ(â„ ğ‘¡,ğ‘š ğ‘¡,ğ‘  ğ‘¡;ğœƒ ğ‘ˆ)parameterizedbyğœƒ ğ‘ˆ (see
cellstateremainslocaltothenode.Theinnerloopfromupdateto line9).Itreturnsthenextnodestateofallnodesâ„ ğ‘¡+1 (cid:17) (â„ ğ‘¡ğ‘£ +1)ğ‘£âˆˆğ‘‰
aggregatedepictsiterationswithinastep,theouterlooprepresents andthegraphobservationsofallagentsğœ“ ğ‘¡ (cid:17) (ğœ“ ğ‘¡ğ‘–)ğ‘–âˆˆğ¼ basedon
theforwardingofstatesbetweenenvironmentsteps.Thestates thenodestatesofallnodesâ„ ğ‘¡,allnodeobservationsğ‘š ğ‘¡ andthe
CF
A
MTSL
B
MTSLstateğ‘  ğ‘¡.Theonlyinformationrequiredfromğ‘  ğ‘¡ arethegraphand 4.1 ModelsandTrainingAlgorithms
themappingofagentstonodes.Thegraphobservationğœ“ ğ‘¡ğ‘– ofagent Our design consist of two parts, a model that generates graph
ğ‘–dependsonitspositioninthegraphandisconcatenatedwiththe
observationsandareinforcementlearningagent.
agentâ€™sobservationğ‘œ ğ‘¡ğ‘– receivedfromtheenvironment(seeline12).
ğ‘„Ë† andğ‘ˆË† denotethattherespectivegradientcalculationisdisabled. GraphObservations. Thecoreofthegraphobservationsisthe
messagepassingframework,asdescribedinSec.3.1.Anygraph
Duringtraining,wesampleasequenceoftransitionsfromthere-
neuralnetworkcanbeusedtogeneratesuchgraphobservations.
playmemoryandperformbackpropagationthroughtimeanalogous
WeuseourproposedarchitecturefromSec.3.2andconsiderthree
tothestoredstatemethodfromrecurrentexperiencereplay[20].
Theinitialnodestateâ„â€² ğ‘— isloadedfromthereplaymemoryand baselinegraphneuralnetworkarchitecturesfromrelatedworkwith
0 implementationsbyPyTorchGeometric[9]andPyTorchGeometric
subsequentnodestatesinthesequencearerecomputed(seelines18
Temporal[32].Twoarchitecturesarefeed-forwardgraphneural
and19).TheQ-learningtargetrequiresgraphobservationsforthe
networkswithoutrecurrency.GraphSAGE[14]isaGNNwithmul-
nextstep,whichwecomputetemporarily.Wethenaggregatethe
tiplegraphconvolutionallayersthatuseindividualparameters.
squarederroroverallstepsinthesequence(seeline22)andper-
formgradientdescentwithrespecttotheagentâ€™sparametersğœƒ
ğ‘„
A-DGN[12]aimstoimprovelearninglong-rangedependencies
andtheparametersofthemessagepassingmoduleğœƒ ğ‘ˆ. withanaddeddiffusiontermandperformsmultipleiterationswith
thesameparameters.Asarecurrentbaseline,GCRN-LSTM[37]
combinesanLSTMwithChebyshevspectralgraphconvolutions[7].
Algorithm2IndependentDQNwithLearnedGraphObservations
Whileourarchitectureusesasinglesumtoaggregatehiddenstates,
1: Initializereplaymemoryğ· GCRN-LSTMutilizes8Chebyshevconvolutionstoaggregateinter-
2: Initializeaction-valuefunctionğ‘„withweightsğœƒ ğ‘„ mediatecomputationsofanLSTMcell.
3:
InitializetargetweightsğœƒË†
ğ‘„
WedefinethereadoutfunctionÎ¨ofaggregatednodestateup-
4: Initializenodestateupdatefunctionğ‘ˆ withweightsğœƒ ğ‘ˆ
dateinformationğ»ğ‘£ (seeSec.3.1)tographobservationsğœ“ğ‘–
asa
5: forepisodeâ†0... do concatenationofthecurrentnodestateâ„ ğ¾ğ‘£ andthelastnodestates
6: â„ 0â†0 âŠ²Initializenodestates â„ ğ¾ğ‘¤ âˆ’1 thatthisnodereceivedfromitsneighbors.Thisservesas
7: Obtainğ‘  0,ğ‘œ 0, andğ‘š 0byresettingtheenvironment askipconnectionoverthelastiteration.Notethatnoadditional
8: forğ‘¡ â†0toğ‘‡ âˆ’1do messageexchangeisnecessaryforthisskipconnection.Weapply
9: â„ ğ‘¡+1,ğœ“ ğ‘¡ â†ğ‘ˆË†(â„ ğ‘¡,ğ‘š ğ‘¡,ğ‘  ğ‘¡;ğœƒ ğ‘ˆ) âŠ²Nodestateupdate thesamereadoutfunctiontoallgraphobservationmethods.
10: forğ‘– âˆˆğ¼ do Agents. We consider independent DQN [27], recurrent DQN
11:
Selectrandomactionğ‘ğ‘–
ğ‘¡
withprobabilityğœ–
(DQNR)[20],CommNet[38]andDGN1[19].Webuildupontheim-
12: otherwiseselectaction plementationofDGN2andreimplementtheremainingapproaches.
ğ‘ğ‘–
ğ‘¡
=argmaxğ‘ğ‘„Ë†(ğ‘œ ğ‘¡ğ‘– âˆ¥ğœ“ ğ‘¡ğ‘–,ğ‘;ğœƒ ğ‘„)
Allvariantssharethesametrainingsetupbutdifferintheagentâ€™sar-
13: Performenvironmentstepwithactionsğ‘ ğ‘¡ andget chitecture.DQNisafeed-forwardnetworkwithfully-connectedlay-
rewardğ‘Ÿ ğ‘¡,stateğ‘  ğ‘¡+1,obsğ‘œ ğ‘¡+1,nodeobsğ‘š ğ‘¡+1 ersthatistrainedwithaQ-learningloss.DQNRaddsanLSTM[17]
14: Store(â„ ğ‘¡,â„ ğ‘¡+1,ğ‘š ğ‘¡,ğ‘š ğ‘¡+1,ğ‘  ğ‘¡,ğ‘  ğ‘¡+1,ğ‘œ ğ‘¡,ğ‘ ğ‘¡,ğ‘Ÿ ğ‘¡,ğ‘œ ğ‘¡+1)inğ· layer and is trained on sequences. Both approaches do not fea-
15: Initializelossğ¿â†0 tureanyinformationexchangebetweenagents,theirpoliciesare
16: forbatchsequenceindicesinğ·jâ† ğ‘— 0toğ‘— 0+(ğ½ âˆ’1)do completelyseparatedduringexecution.CommNetextendsDQNR
17: if ğ‘— = ğ‘— 0then withtwocommunicationroundswhereagentsexchangetheirhid-
18: â„â€² ğ‘— â†â„ ğ‘— âŠ²Loadnodestatefromreplaymemory denstatesbeforeselectinganaction.DGNextendsDQNwithtwo
19:
â„â€² ğ‘—+1,ğœ“ ğ‘—â€² â†ğ‘ˆ(â„â€² ğ‘—,ğ‘š ğ‘—,ğ‘  ğ‘—;ğœƒ ğ‘ˆ)âŠ²Trainnodestateupdate c lao rm izm atu ion nic ta et ri mon tr oou thn eds lou ss si .n Wg is te hl if n-a ott ne ent ci oo mn m[3 u9 n] ia cn ad tioa ndd rs oa unre dg ou f-
20: â„â€² ğ‘—â€² +2,ğœ“ ğ‘—â€²â€² +1â†ğ‘ˆË†(â„â€² ğ‘—+1,ğ‘š ğ‘—+1,ğ‘  ğ‘—+1;ğœƒ ğ‘ˆ) âŠ²Targetinput CommNetandDGN,agentscommunicatewithotheragentsthat
21: ğ‘¦ ğ‘— â†ğ‘Ÿ ğ‘— +Zğ‘—ğ›¾maxğ‘ğ‘„Ë†(ğ‘œ ğ‘—+1âˆ¥ğœ“ ğ‘—â€²â€² +1,ğ‘;ğœƒË† ğ‘„) resideonthesamenodeoronanodeintheirdirectneighborhood.
(cid:40)
withZğ‘–
ğ‘—
= 0 ifagentğ‘–isdoneatstepğ‘—+1 4.2 GraphGenerationandOverview
1 otherwise
Weextendthegraphgenerationusedintheroutingenvironment
22: ğ¿â†ğ¿+(ğ‘¦ ğ‘— âˆ’ğ‘„(ğ‘œ ğ‘— âˆ¥ğœ“ ğ‘—â€²,ğ‘ ğ‘—;ğœƒ ğ‘„))2 fromJiangetal.[19].Itplacesğ¿nodesrandomlyona2Dplane
23: Performgradientdescentonğ¿withrespect andthenconnectsclosenodeswithedgesuntilallnodesreach
toparametersğœƒ ğ‘„ andğœƒ ğ‘ˆ degreeğ·.Havingafixednodedegreeisnotamandatoryconstraint
24: UpdatetargetweightsğœƒË† ğ‘„ forourapproach,butresultsinadiscreteactionspaceoffixed
sizethatsimplifiesreinforcementlearning.Technically,thiscanbe
extendedtographswithnodesofvariabledegrees,e.g.viaaction
4 EXPERIMENTSETUP masking[36].Thedelayofanedgeinstepsisdeterminedbyalinear
functionofthedistancebetweentheconnectednodes,roundedto
Weevaluateourapproachindiversegraphsbasedonarouting
thenextinteger.Disconnectedgraphsarefilteredout.
environment.Thefollowingsectionsdescribeconsideredmodels,
algorithmsandgraphswithgreaterdetail.Thenwebrieflydescribe 1NottobeconfusedwiththegraphneuralnetworkA-DGN.
theroutingenvironmentandasimplifiedsupervisedlearningtask. 2https://github.com/PKU-RL/DGN/,includingthePyTorchversion.ğº ğ´ (0.02,0.19, 0.11) ğº ğµ (0.00,0.53,0.23) ğº ğ¶ (0.00,0.70,0.16)
0.7 0.7 0.70.7 0.7
4.0
11
11
3 21 1 2 3 3 21
21
2
122 2 2 2 1 000 ... 2 456 2 1
1 1
2
112121121 222 22
1 1
000 2... 456
4
2
411 1111 12 11 11 2111 1111 000 ... 456 000 ... 456 233 ... 505 00 .. 56
2.0 0.4
2 2 0.3 2 0.3 0.30.3
5 2 222 2 12 2 2 2 1 00 .. 12 11 2 12 2 2 221 5 2615 2 2 3001 .. 22 2 212 21 22 31 22 2 221 2 00 .. 12 00 .. 12 011 ... 505
2 3 4 5
00 .. 23
0.0 0.0 0.00.0 Throughputwithoutbandwidthlimitation
(a)Exemplarytestgraphsğº ğ´,ğº ğµ,andğº ğ¶ withincreasingmaximumbetweennesscentrality. (b)Meanthroughputon100episodesfor
Thesuffixindicatesthe(min,max,mean)betweennesscentralityintherespectivegraph. alltestgraphs.Eachdotrepresentsagraph.
Figure3:Overviewoftheconsideredgraphswith(a)threeexemplarygraphsfromthetestsetand(b)themeanthroughputof
shortestpathsroutingwithandwithoutbandwidthlimitationinall1000testgraphs.
Wegenerate1000distinctgraphsfortestingwithğ¿ (cid:17)20andğ· (cid:17) ofsizeğ‘”istransmittedviaaselectededgeifthecumulativesizeofall
3.Themeandiameteris7.21Â±1.42hopsand12.84Â±2.72steps.The packetsthatarecurrentlytransmittedviathisedgeissmallerthan
meanall-pairsshortestpaths(APSP)lengthsare3.26Â±1.92hopsand 1âˆ’ğ‘”.Thepacketthentraversestheedgeaccordingtothenumber
5.7Â±3.6steps.ThemaximumAPSPlengthsequalthemaxdiameters ofstepsinitstransmissiondelay.Otherwise,thepacketisforcedto
of12hopsand23steps.Thebetweennesscentralityin [0, 1] of stayatitscurrentpositionandreceivesapenaltyofâˆ’0.2.Anagentâ€™s
anodereflectstheproportionofshortestpathsbetweenanytwo localobservationsincludeitscurrentposition,itsdestinationand
nodesinthegraphthatcontainthisnode.Inrouting,highvalues packetsize.Foreachoutgoingedgeoftheircurrentnode,itobserves
indicatepotentialbottlenecksinthegraph.Weshowexemplary thedelay,thecumulativesizeofpacketsonthatedgeandthere-
graphswithincreasingmaximumbetweennesscentralityinFig.3a, spectiveneighborâ€™snodeid.Anodeobservesitsownid,thenumber
where the nodes are repositioned to provide a better overview. andsizeofpacketsthatresideonthenode,andlocalinformation
Graphğº ğ´ with a low maximum betweenness centrality is well aboutoutgoingedges.Allnodeidsaregivenasone-hotencodings.
balanced.Graphğº ğµhasahighmeanbetweennesscentralitydueto Thethroughputreferstothenumberofpacketsperstepthatar-
itsline-likestructure.Graphğº ğ¶ hasahighmaximumbetweenness riveattheirdestination.Delaydescribesthelengthoftheirepisodes.
centralitybecauseofthebottlenecknodeinthecenter. Notethatthedelayshouldneverbeconsideredonitsown.Forex-
Apartfromthenodeconnectivityandpotentialbottlenecks,the ample,agentsthatonlyroutetodestinationsintheir1-hopneigh-
diameterofthegraphsandthedistributionoftheshortestpathsare borhoodwouldachievelowdelaysbutalsoalowthroughput.
expectedtoinfluenceourapproach.Inthegraphneuralnetwork Asabaseline,weconsiderheuristicagentswithaglobalview
architectureconsideredinthispaper,messagesonlytraversethe thatalwayschoosetheshortestpathswithrespecttotheedgede-
graphthroughitsedges.Thenumberofiterationsforinformation lays.Fig.3bshowsthethroughputwithandwithoutbandwidth
fromnodeğ‘£ 1tobeforwardedtonodeğ‘£ 2equalsthelengthofthe limitationswhenusingthisheuristicfor100episodesinalltest
shortestpathbetweenthesenodes.Theminimumnumberofitera- graphs.Eachdotiscoloredaccordingtothemaximumbetweenness
tionsrequiredtocollectinformationfromallnodesisthereforethe centralityoftherespectivegraph.Wecanseethatbandwidthlimi-
diameterofthegraph.Whilethemaximumdiameteris12hops,we tationscauseasignificantdropinthroughputandthatgraphswith
foundthatover99%oftheshortestpathsinalltestgraphshaveat highmaximumbetweennesstendtoresultinlowerthroughput.
most8hops.Furtherdetailsareprovidedintheappendix.
4.4 ShortestPathsRegressionTask
4.3 RoutingEnvironment
Theroutingenvironmentrequiresagentstolearnpathsfromsource
WeextendtheroutingenvironmentfromJiangetal.[19]andfix todestinationnodes.Toquicklyevaluatetheefficacyofdifferent
abugthatcausedpacketstoskipedges.Atalltimes,thereareğ‘ graphneuralnetworkarchitectures,wedesignamulti-targetre-
packetsofrandomsizesin[0,1)thathavetoberoutedfromrandom gressionproblemasasimplificationoftheroutingenvironment.
sourcetodestinationnodesinagivengraph.Wefocusongeneral- Weexpectthattheperformanceofdifferentarchitecturesinthis
izabilityacrossgraphsanduseğ‘ (cid:17)20packetsinourexperiments. taskwillindicatetheirsuitabilityfortheroutingenvironment.The
Eachpacketisanagentthatreceivesarewardof10whenitreaches training dataset contains node observations for 100000 graphs
itsdestination.Onanode,agentsselectoneof1+ğ·discreteactions generatedbyresettingtheroutingenvironment.Weexclude1000
thatcorrespondtowaitingandchoosinganoutgoingedge.Each ofthesegraphsforvalidation.Thetargetsforeachnodearethe
edgehasatransmissiondelaygiveninsteps.Weconsidertwoenvi- shortestpathlengthstoallothernodes.Forthetestdataset,weuse
ronmentmodes.Thefirstmodehasnorestrictionsandpacketsare thenodeobservationsandtargetsofthe1000graphsfromSec.4.2.
alwaystransmittedviatheirselectededges.Inthesecondmode,we Thelossisthemeansquarederrorbetweenthepredictedandreal
takepacketsizesandlimitededgecapacitiesintoaccount.Apacket distancesforeachsourceanddestinationnode.
ytilartnecssenneewteB ytilartnecssenneewteB ytilartnecssenneewteB ytilartnecssenneewteB
noitatimilhtdiwdnabhtiwtuphguorhT
ytilartnecssenneewtebxaM5 RESULTS Table1:Resultsfortheshortestpathsregressionproblem.ğ¾
denotesthenumberofmessagepassingiterationsandğ½ the
Wefirstpresentindependentresultsforourtwocorecomponents,
unrolldepthforrecurrentapproaches.ShownistheMSEon
thegraphneuralnetworkarchitectures(seeSec.5.1)andagents
alltesttopologiesafterğ‘¡ forwardstepswithğ¾ iterations.All
trainedintheroutingenvironmentwithsinglegraphs(seeSec.5.2).
resultsareaveragedoverthreeseeds.
Section5.3combinesbothcomponentsandprovidestheresultsfor
generalizedrouting,followedbyadiscussioninSec.5.4.
WeusetheAdamWoptimizer[25]forallexperiments.Details Architecture ğ¾ ğ½ MSEatForwardStepğ‘¡
regardingthehyperparametersareprovidedintheappendix. 1 2 4 8 16 32
8 - 1.16(allseeds:1.14,1.22,1.13)
GraphSAGE
16 - 3.57(allseeds:4.18,3.46,3.06)
5.1 ShortestPathsRegression
8 - 1.50(allseeds:1.49,1.56,1.46)
Wefirstevaluatetheconsideredgraphneuralnetworkarchitectures A-DGN
16 - 1.18(allseeds:1.16,1.20,1.18)
intheshortestpathsregressiontask(seeSec.4.4).Wetraineach
1 8 4.98 2.98 1.12 0.60 1.61 4.27
architecturewiththreeseedsfor50000iterationsofbatchsize32.
1 16 5.03 3.09 1.28 0.60 0.53 1.08
BasedontheobservationsregardingtheAPSPdistributionfrom GCRN-LSTM
2 8 3.17 1.18 0.47 0.38 0.50 0.94
Sec.4.2,ğ¾ =8messagepassingiterationsallowtopassinformation
4 8 3.18 1.22 0.49 0.40 0.51 1.08
betweenover99%ofallnodepairsinthetestgraphs.Therefore,we
hypothesizethatğ¾ =8shouldleadtogoodperformanceonthetest 1 8 4.98 2.91 0.94 0.43 0.99 3.75
1 16 5.02 2.98 1.02 0.39 0.37 0.46
graphsforthenon-recurrentmodels.Theresultsfordifferentmes- Ours
2 8 3.02 1.07 0.39 0.35 0.57 1.78
sagepassingiterationsğ¾ andunrolldepthsğ½ areshowninTab.1. 4 8 1.26 0.48 0.34 0.34 0.42 0.81
In GraphSAGE,ğ¾ refers to the number of graph convolutional
layers.ForGCRN-LSTM,wesetthefiltersizeoftheChebyshev
convolutionstoğ¾+1.Bothresultinğ¾ messagepassingiterations.
GraphSAGE(K=8)
Allapproacheslearntoapproximatetheshortestpathlengthsand A-DGN(K=8)
10
achieveameansquarederror(MSE)ofaroundorbelow1forat GCRN-LSTM(K=1,J=8)
leastoneconfiguration.InthecaseofGraphSAGE,increasingthe Ours(K=1,J=8)
numberoflayersto16leadstounstabletrainingandahightest
5
lossinthistask.Asthenon-recurrentarchitecturesarestateless,
theyyieldthesameresultsateachforwardstepğ‘¡.
Fortherecurrentapproaches,wewanttousealownumberof 0
iterationsperstep(i.e.ğ¾ =1)toreducethecommunicationover- 0 10000 20000 30000 40000 50000
Trainingiteration
head.Weevaluatethemwithdifferentunrolldepthsğ½ andhigher
valuesofğ¾ forcomparison.Asexpected,therecurrentapproaches
performpoorlyinthefirstforwardstepwithğ‘¡ = 1.Theyrefine Figure4:ValidationlossoftheselectedGNNarchitectures
intheshortestpathsproblemduringtraining.Theshaded
theirhiddenstatesinsubsequentstepsandapproximatelyreach
theirminimumlossesattheunrolldepthğ½ thatwasusedduring areashowsthestandarddeviationoverthreemodels.
training.Afterwards,wecanseethattheirlossesincreaseagain.
5.2 RoutinginSingleGraphs
Increasingtheunrolldepthimproveslong-termstability,butleads
toincreasedtrainingtime.Ahighernumberofiterationsğ¾ pre- Beforeweevaluateourmethodonmultiplegraphs,wetrainagents
dominantlyleadstoimprovedpredictions,atthecostofincreasing withoutgraphobservationsintheroutingenvironmentusingsingle
thecommunicationoverheadperstep. graphs.Theagentsaretrainedfor250000totalstepswith24000
Forourexperimentsincombinationwithreinforcementlearn- iterationsofbatchsize32.Episodesaretruncatedafter300steps.
ing,weselectğ¾ =8forthenon-recurrentmodelsandğ¾ =1, ğ½ =8 InTab.2,weshowtheresultsfortheoutliergraphfromFig.3bat
fortherecurrentmodels.Thisisacompromisebetweenperfor- around(4.5, 2.0).Thetophalfshowstheresultswithoutbandwidth
mance,stabilityandcommunicationoverhead.Fig.4showsthe limitations,thebottomhalfwithbandwidthlimitations.Without
validationlossoftheselectedapproachesduringtraining.There- limitations,allmethodslearntheoptimalshortestpaths.Thisisnot
currentarchitecturesconvergefastertoalowlossvaluethanthe surprising,asthegraphisstaticandagentscanlocatethemselves
non-recurrentones.Thiscouldpossiblybecausedbybettergra- inthegraphusingthenodeidtheyreceiveintheirobservation.
dients,aswecomputeseparatelossesforeachmessagepassing Theagentsspecializeonthegraph.Withbandwidthlimitations,the
iterationwhenunrollingthenetwork.Bothrecurrentapproaches throughputdrasticallydecreasesandthedelayincreases.Wecan
achievesimilarvalidationlosses,althoughourarchitectureissim- seethatinthisgraph,alllearningapproachesareabletooutperform
plerandexchangeslessinformationduringtheforwardsteps.The theshortestpathsolutionintermsofmeanrewardandthroughput.
highstandarddeviationofGCRN-LSTMinthebeginningiscaused Except for DQN, the delay of arrived packets is slightly higher.
byoneofthethreeruns,wherethevalidationlossdoesnotde- Surprisingly,theeffectofcommunicationisverysmall.Usingthe
creaseinitially.Inthereinforcementlearningsetting,weexpect sametrainingsetupforallagentarchitectures,wecannotreproduce
recurrentexperiencereplaywithstoredstatestofurtherimprove theresultsfromJiangetal.[19]inthisparticulargraphandfind
thelong-termstabilityoftherecurrentapproaches. thattheperformanceofDQNisveryclosetoDGN.
ssolnoitadilaVTable 2: Results for routing in a selected graph, averaged Table4:Resultsforroutingin1000testgraphsfor300steps
over1000episodesand3models.Thelearningapproaches usingDQNwithgraphobservationsprovidedbythelisted
outperformtheshortestpathsheuristic. methods.Anasterisk(*)indicatesactionmaskingattesttime.
Mode Agent Metrics Mode Method Metrics
Reward Delay Throughput Reward Delay Throughput
ShortestPath 2.26Â±0.0 4.39Â±0.0 4.52Â±0.0 ShortestPath 1.77Â±0.00 5.59Â±0.00 3.54Â±0.00
DQN 2.26Â±0.0 4.39Â±0.0 4.52Â±0.0 GraphSAGE 0.02Â±0.00 4.49Â±0.35 0.04Â±0.00
DQNR 2.26Â±0.0 4.39Â±0.0 4.52Â±0.0 A-DGN 1.15Â±0.02 5.72Â±0.02 2.29Â±0.04
CommNet 2.26Â±0.0 4.39Â±0.0 4.52Â±0.0 GCRN-LSTM 1.55Â±0.01 5.60Â±0.01 3.09Â±0.01
DGN 2.26Â±0.0 4.39Â±0.0 4.52Â±0.0 Ours 1.56Â±0.03 5.57Â±0.02 3.12Â±0.07
ShortestPath 0.88Â±0.0 7.06Â±0.01 1.98Â±0.0 Ours* 1.74Â±0.00 5.65Â±0.00 3.49Â±0.00
DQN 1.05Â±0.00 6.81Â±0.08 2.15Â±0.00 ShortestPath 1.05Â±0.00 7.93Â±0.00 2.26Â±0.00
DQNR 1.09Â±0.00 7.20Â±0.06 2.22Â±0.01 GraphSAGE 0.02Â±0.02 14.58Â±6.85 0.08Â±0.05
CommNet 1.11Â±0.00 7.22Â±0.03 2.26Â±0.01 A-DGN 0.30Â±0.14 10.85Â±1.34 0.67Â±0.28
DGN 1.10Â±0.00 7.23Â±0.13 2.25Â±0.01 GCRN-LSTM 0.99Â±0.01 8.37Â±0.02 2.03Â±0.01
Ours 1.02Â±0.01 8.26Â±0.02 2.10Â±0.02
Table3:Througputforroutingagentsindividuallytrainedon Ours* 1.10Â±0.00 7.59Â±0.01 2.38Â±0.01
thegraphsfromFig.3awithvaryingbetweennesscentrality.
Shownaretheresultsforthebandwidthlimitationmode. DQN+GraphSAGE DQN+GCRN-LSTM
DQN+A-DGN DQN+Ours
Agent Throughput Nolimitation Withbandwidthlimitation
Graphğº ğ´ Graphğº ğµ Graphğº ğ¶ 1.5 1.00
ShortestPath 3.20Â±0.00 1.53Â±0.00 1.02Â±0.00 1.0 0.75
DQN 3.28Â±0.01 1.43Â±0.02 0.98Â±0.01 0.50
DQNR 3.28Â±0.00 1.48Â±0.01 0.99Â±0.01 0.5 0.25
CommNet 3.31Â±0.00 1.53Â±0.00 1.01Â±0.01 0.00
0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5
DGN 3.29Â±0.00 1.43Â±0.05 1.00Â±0.00 Environmentsteps 106 Environmentsteps 106
Ã— Ã—
Figure5:RewardofDQNwithgraphobservationsduring
Whilewedidnottrainagentsforall1000testgraphs,wehave trainingwithout(left)andwith(right)bandwidthlimitations.
madesimilarobservationsfortheothergraphsweinvestigated. Theshadedareashowsthestandarddeviationover3models.
Tab.3showsthethroughputinthethreegraphsfromFig.3a,aver-
agedover1000episodesand3models.Weomittheresultswithout TheresultsareshowninTab.4andtherewardduringtraining
limitations,asallapproachesmatchshortestpathswithamean isshowninFig.5.Thehighpositiverewardandthroughputof
throughputof4.05ingraphğº ğ´,2.38ingraphğº ğµand2.98ingraph
GCRN-LSTMandourproposedarchitectureshowthatgraphob-
ğº ğ¶. In the limited bandwidth setting, we again find no notable servationsindeedenableagentstogeneralizeoverdifferentgraphs.
differencebetweenDQNandDGNinthesegraphsandnoticethat Ourmethodachievescomparableresultswhilehavingalowercom-
thelearningapproachesoutperformshortestpathsonlyforgraph municationoverhead.However,wefindthattheresultsareworse
ğº ğ´.Thethroughputachievedbythelearningapproachesisapprox- thanforagentsthatspecializeonsinglegraphs(seeSec.5.2).Forthe
imatelyonparwithshortestpathsforgraphğº ğµ andğº ğ¶. non-recurrentapproaches,A-DGNlearnsgraphobservationsbut
convergesmuchslowerthantherecurrentapproaches.GraphSAGE
5.3 GeneralizedRouting
failstolearn,eveninexperimentswithbatchsize256andjumping
Thissectionpresentstheresultsforourlearnedgraphobserva- knowledgenetworks[41]thatarenotshownhere.Consideringthe
tions.Weexpectthemtoenableagentstogeneralizeoverdifferent resultsofSec.5.1,itisunclearwhythenon-recurrentapproaches
graphs.Asallagentarchitecturesachievesimilarresultsforsingle perform poorly in this setting. We hypothesize that the targets
graphs,weselectDQNastheunderlyingagentarchitecturedue providedbybackpropagationthroughtimefacilitatelearning,but
toitssimplicity.Insteadofonlyreceivingobservationsfromthe furtherexperimentsindifferentenvironmentswouldberequiredto
environment,agentsnowalsoreceivegraphobservationsfrom verifythis.Allmethodshavealowerthroughputthantheshortest
nodestheyarelocatedat.Wetraingraphobservationsandagents pathsheuristicwithoutadditionalmodifications.
end-to-endwithreinforcementlearningonrandomlygenerated Uponcloserinspectionofthebehaviorofamodeltrainedwith
graphsfor2.5milliontotalsteps,240000iterationsandbatchsize ourarchitecture,wenoticethataround12%ofthe1000testepisodes
32.Episodesaretruncatedafter50stepstoincreasethenumberof containpacketsthatneverarriveattheirdestinationwithin300
generatedgraphs.Theresultingmodelsareevaluatedonour1000 steps.Thisiscausedbyroutingloops,acommonissuethatcanbe
testgraphsand300episodestepsforcomparabilitywithSec.5.2. addressedwithpost-processingofthelearnedpolicy[18].When
htdiwdnab
noitatimilon
noitatimil
draweR
htdiwdnab
noitatimilon
noitatimil
draweR6 RELATEDWORK
3 Reinforcementlearningforgraph-basedenvironmentsexempli-
2 fied by routing has been investigated since the introduction of
Baselines
Q-learning[4].Recentworkshaveshowntoimproveoverprevious
1 SP(stepwise)
SP(static) algorithmsinvariousdomainsandnetworkconditions[22,28].
DQN(best) Manyoftheseapproachesassumecentralizedcontrolwitha
DQN+GraphObs. globalview[1,6,18,21,35].Thisnotonlylimitstheirscalability,
10âˆ’1 Peaksatstep52 GCRN-LSTM(best) butalsotheirreactivity.Decentralizedapproaches[5,36]aremore
withvalue0.06 Ours(best)
reactive,buttheirpartialobservabilitymaydegradeperformance.
10âˆ’2
0 50 100 150 200 250 300
Learningdirectlyinthetargetnetworkallowsagentstospecial-
Episodesteps ize[35].However,thisischallenginginpracticebecausesubopti-
malactionscanresultinunacceptablereal-worldcosts.Specialized
Figure6:Throughputovertimeandnodestatedifferencesof agentscanalsogetstuckinlocaloptima,requiringretrainingfrom
selectedmodelsingraphğº ğ´fromFig.3aaveragedover100 scratchifthegraphorthenetworkconditionschange[3].
episodes.Thedelayofasingleedgeisincreasedfrom2to10 Ideally,onewouldpre-trainagentstoperformwellinallgraphs
atstep50.Theshadedareashowsthestandarddeviation. andnetworkconditionsandoptionallyfine-tunethemonline.Graph
neuralnetworkshaveshowntoenablegeneralizationinrouting
repeatingtheexperimentwithdifferentseeds,weobserverouting
scenarios[8,33],asopposedtotraditionalmodelswithfixedin-
loopsindifferentgraphs.Weinvestigateanactionmaskingmecha-
putdimensionsthatspecializeonindividualgraphs[21].Tothe
nismthatstoresthepathofapacketandmasksactionsthatlead
bestofourknowledge,relatedworkswithgraphneuralnetworks
toalreadyvisitednodes.Iftherearenolegalactions,thepacketis
aremainlyrestrictedtocentralizedapproachesandagent-to-agent
droppedandanewpacketspawnsatarandomlocation.
communication[19,29].Anoteworthyexceptionistheworkby
Actionmaskingresultsinthroughputimprovementsthatmatch
GeyerandCarle[10],whoproposeadistributedmessagepassing
ourexpectationsfromthefixedtopologysetting,asshowninTab.4.
schemetolearnroutinginasupervisedsetting.Withourwork,we
However,itintroduces0.01and0.1droppedpacketsperstepfor
addressthegapofgeneralizabilityovergraphsinthecontextof
routingwithoutandwithbandwidthlimitations,respectively.
multi-agentreinforcementlearningwithdecentralizedexecution.
5.4 AdaptationandLimitations
7 CONCLUSION
Inthissection,weinvestigatehowagentsreacttoanovelsituation
Inthispaper,weinvestigatepartialobservabilityingraphenviron-
anddiscussthelimitationsofourwork.
mentsinthecontextofdeepmulti-agentreinforcementlearning.
Weexemplaryincreasethedelayofabottleneckedgeingraph
Weproposetodecouplethelearningofgraphrepresentationsand
ğº ğ´from2to10atstep50andevaluateitseffecton100episodesof
decisionmaking.Nodesexchangeandupdatetheirhiddenstatesin
300stepswithbandwidthlimitations.Fig.6showsthethroughput
adecentralizedmannertoimprovetheirviewofthegraph.Agents
ofdifferentapproachesinthisscenario,combinedwiththestepwise
thenreceivegraphobservationsbasedontheirlocationinthegraph.
meanabsolutedifferenceofnodestatevaluesfromtherecurrent
Weevaluateourgraphobservationsbasedonfourgraphneural
approaches.Staticshortestpaths(SP)ignoresthedelaychange,
networkarchitecturesacross1000diversegraphsinthecontextofa
stepwise SP considers it. For each learning approach, we show
routingenvironment.Ourresultsindicatethatrecurrentgraphneu-
theresultsofthebestmodel.Therecurrentapproachesquickly
ralnetworkscanbetrainedend-to-endwithreinforcementlearning
convergetosmallnodestatedifferencesatthebeginningandreact
andsparserewards.Weshowthattheresultinggraphobservations
tothechangeatstep50,althoughchangingedgedelayswerenever
donotonlyallowagentstogeneralizeoverdifferentgraphs,but
encounteredduringtraining.Beforestep50,DQNperformsslightly
alsotoadapttochangesinthegraphwithoutretraining.Depend-
betterthanourapproach.Afterthechange,allthreeDQNmodels
ingontheenvironment,however,havingnoconstraintsonthe
failtoadaptanddisplayapoorthroughput,whileoneoutofthree
exchangedmessagesandresultingpoliciescanleadtodeteriorated
modelswithourapproachisabletooutperformstepwiseshortest
behaviorcomparedtoagentsthatspecializeonasinglegraph.This
paths.Followingresearchcouldconsiderdynamicgraphchanges
isreflectedbyroutingloopsintheroutingenvironment,whichwe
duringtrainingandexploreadaptivityinmoredetail.
showcanbealleviatedwithactionmasking.
Theseimprovementsingeneralizabilityandadaptivitycomeat
Ourcontributionsopenupmultipledirectionsforfutureresearch.
thecostofexchangingmessageswithallneighbornodesateach
Toreducethecommunicationoverhead,aselectivemessageex-
step.Fig.6showsthatthereiscomparativelylittlechangeinthe
changebetweennodescanbestudied.Graphobservationsfurther
nodestatesafterconvergence,suggestingareducedneedforcom-
provideanewapplicationwithdistinctrequirementsforgraphneu-
munication.Whileweshowthatasinglemessagepassingiteration
ralnetworks.Forexample,asynchronousmessagepassingwithout
perstepsufficestolearngraphobservationsforgeneralizedrouting,
aneedforsynchronizationbetweennodeswouldincreasethere-
futureworkcouldinvestigatethefurtherreductionofcommuni-
activityinadistributedsetting.Finally,theeffectofincorporating
cationoverhead.Weseegreatpotentialforsynergieswithrecent
graphobservationsindifferentenvironmentsisworthinvestigating,
worksintheareaofagent-to-agentcommunication,whereagents
especiallywhencooperationbetweenagentsisrequired.
decidewhentosendmessages[15,24,40]toselectedrecipients
insteadofbroadcastingthemtoallotheragents[26].
tuphguorhT
ecnereffidetatsedoNACKNOWLEDGMENTS
[21] GyungminKim,YohanKim,andHyukLim.2022.DeepReinforcementLearning-
BasedRoutingonSoftware-DefinedNetworks. IEEEAccess10(2022),18121â€“
ThisworkhasbeenfundedbytheFederalMinistryofEducation
18133.
andResearchofGermany(BMBF)throughSoftwareCampusGrant [22] TianxuLi,KunZhu,NguyenCongLuong,DusitNiyato,QihuiWu,YangZhang,
01IS17050(AC3Net)andtheprojectâ€œOpen6GHubâ€(grantnumber: andBingChen.2022.ApplicationsofMulti-AgentReinforcementLearningin
FutureInternet:AComprehensiveSurvey. IEEECommunicationsSurveys&
16KISK014).Ithasbeenco-fundedbytheGermanResearchFounda- Tutorials24,2(2022),1240â€“1279.
tion(DFG)intheCollaborativeResearchCenter(CRC)1053MAKI. [23] YujiaLi,DanielTarlow,MarcBrockschmidt,andRichardS.Zemel.2016.Gated
GraphSequenceNeuralNetworks.InProceedingsofthe4thInternationalConfer-
TheauthorsthankAmirkasraAminiforthevaluablediscussions.
enceonLearningRepresentations(ICLR).
[24] Yen-ChengLiu,JunjiaoTian,NathanielGlaser,andZsoltKira.2020.When2com:
REFERENCES Multi-AgentPerceptionviaCommunicationGraphGrouping.In2020IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR).4105â€“4114.
[1] PaulAlmasan,JosÃ©SuÃ¡rez-Varela,KrzysztofRusek,PereBarlet-Ros,andAlbert [25] IlyaLoshchilovandFrankHutter.2019.DecoupledWeightDecayRegularization.
Cabellos-Aparicio.2022.Deepreinforcementlearningmeetsgraphneuralnet- In7thInternationalConferenceonLearningRepresentations(ICLR).
works:Exploringaroutingoptimizationusecase.ComputerCommunications [26] ZiyuanMa,YudongLuo,andJiaPan.2022.LearningSelectiveCommunication
196(2022),184â€“194. forMulti-AgentPathFinding.IEEERoboticsandAutomationLetters7,2(2022),
[2] GuillermoBernÃ¡rdez,JosÃ©SuÃ¡rez-Varela,AlbertLÃ³pez,BoWu,ShihanXiao, 1455â€“1462.
XiangleCheng,PereBarlet-Ros,andAlbertCabellos-Aparicio.2021. IsMa- [27] VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiA.Rusu,JoelVeness,
chineLearningReadyforTrafficEngineeringOptimization?.In2021IEEE29th MarcG.Bellemare,AlexGraves,MartinA.Riedmiller,AndreasFidjeland,Georg
InternationalConferenceonNetworkProtocols(ICNP).1â€“11. Ostrovski,StigPetersen,CharlesBeattie,AmirSadik,IoannisAntonoglou,Helen
[3] SaiShreyasBhavanasi,LorenzoPappone,andFlavioEsposito.2023.DealingWith King,DharshanKumaran,DaanWierstra,ShaneLegg,andDemisHassabis.2015.
Changes:ResilientRoutingviaGraphNeuralNetworksandMulti-AgentDeep Human-levelcontrolthroughdeepreinforcementlearning. Nature518,7540
ReinforcementLearning.IEEETransactionsonNetworkandServiceManagement (2015),529â€“533.
20,3(2023),2283â€“2294. [28] MingshuoNie,DongmingChen,andDongqiWang.2023.ReinforcementLearn-
[4] JustinBoyanandMichaelLittman.1993.PacketRoutinginDynamicallyChang- ingonGraphs:ASurvey.IEEETransactionsonEmergingTopicsinComputational
ingNetworks:AReinforcementLearningApproach.InAdvancesinNeuralInfor- Intelligence7,4(2023),1065â€“1082.
mationProcessingSystems,Vol.6.Morgan-Kaufmann,671â€“678. [29] YaruNiu,RohanPaleja,andMatthewGombolay.2021. Multi-AgentGraph-
[5] FlorianBrandherm,JulienGedeon,OsamaAbboud,andMaxMÃ¼hlhÃ¤user.2022. AttentionCommunicationandTeaming.InProceedingsofthe20thInternational
BigMEC:ScalableServiceMigrationforMobileEdgeComputing.InIEEE/ACM ConferenceonAutonomousAgentsandMultiAgentSystems(AAMASâ€™21).IFAA-
7thSymposiumonEdgeComputing(SEC).136â€“148. MAS,964â€“973.
[6] DanielaM.Casas-Velasco,OscarMauricioCaicedoRendon,andNelsonL.S.da [30] MilenaRadenkovicandVuSanHaHuynh.2020.CognitiveCachingattheEdges
Fonseca.2021.IntelligentRoutingBasedonReinforcementLearningforSoftware- forMobileSocialCommunityNetworks:AMulti-AgentDeepReinforcement
DefinedNetworking.IEEETransactionsonNetworkandServiceManagement18, LearningApproach.IEEEAccess8(2020),179561â€“179574.
1(2021),870â€“881. [31] DavisRempe,JonahPhilion,LeonidasJ.Guibas,SanjaFidler,andOrLitany.2022.
[7] MichaÃ«lDefferrard,XavierBresson,andPierreVandergheynst.2016. Convo- GeneratingUsefulAccident-ProneDrivingScenariosviaaLearnedTrafficPrior.
lutionalNeuralNetworksonGraphswithFastLocalizedSpectralFiltering.In InProceedingsofthe2022IEEE/CVFConferenceonComputerVisionandPattern
AdvancesinNeuralInformationProcessingSystems,Vol.29.3844â€“3852. Recognition(CVPR).IEEE,17284â€“17294.
[8] MiquelFerriol-GalmÃ©s,JordiPaillisse,JosÃ©SuÃ¡rez-Varela,KrzysztofRusek,Shi- [32] BenedekRozemberczki,PaulScherer,YixuanHe,GeorgePanagopoulos,Alexan-
hanXiao,XiangShi,XiangleCheng,PereBarlet-Ros,andAlbertCabellos- derRiedel,MariaAstefanoaei,OliverKiss,FerencBeres,GuzmanLopez,Nicolas
Aparicio.2023.RouteNet-Fermi:NetworkModelingWithGraphNeuralNetworks. Collignon,andRikSarkar.2021.PyTorchGeometricTemporal:Spatiotemporal
IEEE/ACMTransactionsonNetworking31,6(2023),3080â€“3095. SignalProcessingwithNeuralMachineLearningModels.InProc.ofthe30thACM
[9] MatthiasFeyandJanE.Lenssen.2019.FastGraphRepresentationLearningwith InternationalConferenceonInformationandKnowledgeManagement.4564â€“4573.
PyTorchGeometric.InICLRWorkshoponRepresentationLearningonGraphsand [33] KrzysztofRusek,JosÃ©SuÃ¡rez-Varela,PaulAlmasan,PereBarlet-Ros,andAlbert
Manifolds. Cabellos-Aparicio.2020. RouteNet:LeveragingGraphNeuralNetworksfor
[10] FabienGeyerandGeorgCarle.2018. LearningandGeneratingDistributed NetworkModelingandOptimizationinSDN.IEEEJournalonSelectedAreasin
RoutingProtocolsUsingGraph-BasedDeepLearning.InProceedingsofthe2018 Communications38,10(2020),2260â€“2270.
WorkshoponBigDataAnalyticsandMachineLearningforDataCommunication [34] FrancoScarselli,MarcoGori,AhChungTsoi,MarkusHagenbuchner,andGabriele
Networks(Big-DAMAâ€™18).AssociationforComputingMachinery,40â€“45. Monfardini.2009.TheGraphNeuralNetworkModel.IEEETransactionsonNeural
[11] JustinGilmer,SamuelS.Schoenholz,PatrickF.Riley,OriolVinyals,andGeorgeE. Networks20,1(2009),61â€“80.
Dahl.2017.NeuralMessagePassingforQuantumChemistry.InProceedingsof [35] StefanSchneider,RaminKhalili,AdnanManzoor,HaydarQarawlus,RafaelSchel-
the34thInternationalConferenceonMachineLearning(ICML).PMLR,1263â€“1272. lenberg,HolgerKarl,andArturHecker.2021. Self-LearningMulti-Objective
[12] AlessioGravina,DavideBacciu,andClaudioGallicchio.2023.Anti-Symmetric ServiceCoordinationUsingDeepReinforcementLearning.IEEETransactionson
DGN:astablearchitectureforDeepGraphNetworks.InThe11thInternational NetworkandServiceManagement18,3(2021),3829â€“3842.
ConferenceonLearningRepresentations(ICLR). [36] StefanSchneider,HaydarQarawlus,andHolgerKarl.2021.DistributedOnline
[13] WilliamL.Hamilton.2020.GraphRepresentationLearning.SynthesisLectures ServiceCoordinationUsingDeepReinforcementLearning.In2021IEEE41st
onArtificialIntelligenceandMachineLearning14,3(2020),1â€“159. InternationalConferenceonDistributedComputingSystems(ICDCS).539â€“549.
[14] WilliamL.Hamilton,ZhitaoYing,andJureLeskovec.2017.InductiveRepresen- [37] YoungjooSeo,MichaÃ«lDefferrard,PierreVandergheynst,andXavierBresson.
tationLearningonLargeGraphs.InAdvancesinNeuralInformationProcessing 2018.StructuredSequenceModelingwithGraphConvolutionalRecurrentNet-
Systems,Vol.30.1024â€“1034. works.InNeuralInformationProcessing-25thInternationalConference(ICONIP)
[15] ShuaiHan,MehdiDastani,andShihanWang.2023.Model-BasedSparseCom- (LectureNotesinComputerScience,Vol.11301).Springer,362â€“373.
municationinMulti-AgentReinforcementLearning.InProceedingsofthe2023 [38] SainbayarSukhbaatar,ArthurSzlam,andRobFergus.2016. LearningMultia-
InternationalConferenceonAutonomousAgentsandMultiagentSystems(AAMAS gentCommunicationwithBackpropagation.InAdvancesinNeuralInformation
â€™23).IFAAMAS,439â€“447. ProcessingSystems,Vol.29.2244â€“2252.
[16] EricA.Hansen,DanielS.Bernstein,andShlomoZilberstein.2004. Dynamic [39] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
ProgrammingforPartiallyObservableStochasticGames.InProceedingsofthe AidanNGomez,ÅukaszKaiser,andIlliaPolosukhin.2017. AttentionisAll
19thNationalConferenceonArtificialIntelligence.AAAIPress,709â€“715. youNeed.InAdvancesinNeuralInformationProcessingSystems30.5998â€“6008.
[17] SeppHochreiterandJÃ¼rgenSchmidhuber.1997. LongShort-TermMemory. [40] XuefengWang,XinranLi,JiaweiShao,andJunZhang.2023.AC2C:Adaptively
NeuralComputation9,8(1997),1735â€“1780. ControlledTwo-HopCommunicationforMulti-AgentReinforcementLearning.
[18] OliverHopeandEikoYoneki.2021.GDDR:GNN-basedData-DrivenRouting.In InProceedingsofthe2023InternationalConferenceonAutonomousAgentsand
Proceedingsofthe2021IEEE41stInternationalConferenceonDistributedComputing MultiagentSystems(AAMASâ€™23).IFAAMAS,427â€“435.
Systems(ICDCS).517â€“527. [41] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
[19] JiechuanJiang,ChenDun,TiejunHuang,andZongqingLu.2020.GraphConvolu- Kawarabayashi,andStefanieJegelka.2018.RepresentationLearningonGraphs
tionalReinforcementLearning.InProceedingsofthe8thInternationalConference withJumpingKnowledgeNetworks.InProceedingsofthe35thInternational
onLearningRepresentations(ICLR). ConferenceonMachineLearning(ICML),Vol.80.PMLR,5453â€“5462.
[20] StevenKapturowski,GeorgOstrovski,JohnQuan,RÃ©miMunos,andWillDabney.
2019.RecurrentExperienceReplayinDistributedReinforcementLearning.In
Proceedingsofthe7thInternationalConferenceonLearningRepresentations(ICLR).APPENDIX DGN. Agentsperformtworoundsofmessageexchangeusing
self-attention.Weusehyperparametersbasedontheofficialimple-
A ARCHITECTURE mentationofDGN,i.e.2attentionlayerswith8attentionheads,
Thissectionprovidesfurtherdetailsregardingthegraphneural andkeyandvaluesize16.However,weusein-andoutputsize256
networkandagentarchitecturesusedinthiswork. insteadof128forconsistency.Theoutputsoftheattentionlayers
areconcatenatedwiththeobservationencodingandprojectedto
A.1 GraphNeuralNetworks actionsusingalinearlayerofsizes(3Â·256,ğ·+1).
Node observations are encoded using a fully connected neural
B EXPERIMENTDETAILS
networkwith(ğ‘‘ ğ‘š, 512, 256,ğ‘‘ â„)hiddenunits,followedbyLeaky
ReLU activation functions. The input dimensionğ‘‘ ğ‘š is the size Tab.5providesanoverviewoftheparametersusedduringtraining
ofthenodeobservations.Theoutputdimensionğ‘‘ â„ isthehidden andtesting.Thelearningrateissetto0.001intheshortestpaths
dimensionoftherespectivegraphneuralnetworks.Wesetğ‘‘ â„ =128 regressiontask(seeSec.5.1).Forourevaluations,weusethemodels
forallexperiments.Allgraphneuralnetworksusethesamenetwork from the last training iteration. The following sections provide
toencodenodeobservations. furtherdetailsregardingourimplementationofAlg.2.
GraphSAGE. Weusethedefaultconfigurationprovidedbythe B.1 DeepQ-Learning
implementationofGraphSAGEinPyTorchGeometric[9]withğ‘‘ â„ =
AtthebeginningofAlg.2,weinitializetheparametersofthetarget
128andğ¾ layers.ItusestheReLUactivationfunctionbetween
action-valuenetworkasğœƒË† ğ‘„ â†ğœƒ ğ‘„.Asintheimplementationof
graphconvolutionallayers.ExperimentswithLeakyReLUforim-
provedconsistencyresultedininstabilitiesduringtraining,espe- DGN,3thetargetparametersğœƒË† ğ‘„ arethensmoothlyupdatedwith
ciallyforahighernumberoflayers.Wealsoexperimentedwith ğœƒË† ğ‘„â€² â†ğœğœƒ ğ‘„ +(1âˆ’ğœ)ğœƒË† ğ‘„ ineachiteration.DGNfurtheraugments
JumpingKnowledgeNetworks[41].Whiletheyallowedforanim- theregularDQNloss (ğ‘¦ ğ‘— âˆ’ğ‘„(ğ‘œ ğ‘—,ğ‘ ğ‘—;ğœƒ ğ‘„))2 witharegularization
provedconvergencespeedinthesupervisedsetting,theydidnot term(ğ‘„Ë†(ğ‘œ ğ‘—,ğ‘;ğœƒ ğ‘„)âˆ’ğ‘„(ğ‘œ ğ‘—,ğ‘;ğœƒ ğ‘„))2forallotheractionsğ‘â‰ ğ‘ ğ‘—.
improvelearningofgraphobservations.
B.2 GraphObservations
A-DGN. Weusethedefaultconfigurationprovidedbytheim-
plementation of AntiSymmetricConv in PyTorch Geometric [9] Duringexecution,weimplementgraphobservationsasanenviron-
withğ‘‘ â„ =128andğ¾ iterations.Itusesthetanhactivationfunction mentwrapper.Duringtraining,asdescribedinSec.3.4,wesample
betweenmessagepassingiterations. aminibatchofasequenceofstepstoperformbackpropagation
throughtime.Inourimplementation,thesesequencesareallowed
GCRN-LSTM. Weusethedefaultconfigurationprovidedbythe
tocrossepisodeboundaries.Whenreachingtheendofanepisode,
wim itp hle ğ‘‘m â„e =nt 1a 2ti 8on ano df fiGC lto en rv sL izS eT ğ¾Mi +n 1P ,y rT eo sr uc lh tinG geo inm ğ¾etr mic eT se sam gp eo pr aa sl s[ i3 n2 g] weresetthenodestatesâ„â€² ğ‘— tozero.Thisisdoneindependentlyfor
eachsequenceintheminibatch.
iterations.Thearchitectureusesinternalhiddenandcellstatesof
sizeğ‘‘ â„ foreachnode. Table5:Parametersusedfortrainingandtesting.
Ours. WeusetwoLSTMcellsthatsharesinglehiddenandcell
statesofsizeğ‘‘ â„ =128foreachnode. Parameter EnvironmentMode
SingleGraph Generalized
A.2 AgentArchitectures
Optimizer AdamW[25]
Allagentarchitecturesuseafullyconnectedneuralnetworkwith
Learningrate 0.001
(ğ‘‘ ğ‘œ, 512, 256) hidden units, followed by Leaky ReLU activation
TotalSteps 250000 2500000
functionstoencodetheagentâ€™sobservation.Theinputdimension
Stepsbeforetraining 10000 100000
ğ‘‘ ğ‘œ isdeterminedbythesizeoftheagentâ€™sobservations.Notethat
Replaymemorysize 200000steps
graphobservationsleadtoanextendedobservationspace.
Stepsbetweeniterations 10
DQN. DQNusesasinglelinearlayerofinputandoutputsizes Episodelength 300steps 50steps
(256,ğ·+1)tomaptheobservationencodingtothediscreteaction Minibatchsize 32
spacewithğ·+1actions.Forourexperiments,wesetğ· =3. Targetnetworkupdateğœ 0.01
Discountfactorğ›¾ 0.9
DQNR. Theoutputoftheobservationencoderisfollowedby
Initialexplorationğœ– 1.0
anLSTMcellwithhiddenandcellstateofsize256.Theactionis
ğœ–-decay(per100steps) 0.996 0.999
predictedwithafullyconnectedlinearlayerwithinputandoutput
Minimumexplorationğœ– 0.01
size(256,ğ·+1),usingthehiddenstateasinput.
Testexplorationğœ– 0.0
CommNet. ThearchitectureofCommNetbuildsuponDQNRand
Testepisodes 1000
addstworoundsofmessageexchangeusingtheupdatedhidden
Testepisodelength 300steps
stateoftheLSTM.Oneroundofmessageexchangeisthesumofthe
agentâ€™shiddenstateandthemeanofallneighborsâ€™hiddenstates,
excludingtheagentâ€™sownhiddenstate. 3https://github.com/PKU-RL/DGN/,includingthePyTorchversion.C TESTGRAPHS Table6:Metricsforthetestgraphs.Shownaretheminimum,
maximum,mean,andstandarddeviationofeachmetric.The
Whilethetraininggraphsarerandomlygeneratedontheflyand
firstthreemetricshavethesamevaluesforallgraphs.
thereforedependontheusedtrainingseed,thetestgraphsarefixed
andindependentofthetrainingseed.Thissectionsupplements
Metric Min Max Mean Std
Sec.4.2andprovidesfurtherdetailsregardingthetestgraphs.
Order(#nodes) 20 0
C.1 Metrics Nodedegree(#incidentedges) 3 0
Size(total#edges) 30 0
Tab.6liststhemetricsweusedtocomparethegraphs,including Diameter(hops) 5.00 12.00 7.21 1.42
detailsregardingthebetweennesscentralitybrieflymentionedin Diameter(delays) 8.00 23.00 12.84 2.72
thepaper.Consideringthegraphmaxbetweennesscentrality,the APSP(hops) 0.00 12.00 3.26 1.92
standarddeviationof0.1andthehighdifferencebetweenthemin- APSP(delays) 0.00 23.00 5.70 3.60
imumvalueof0.19andthemaximumvalueof0.7suggestthat Nodebetweennesscentrality 0.00 0.70 0.15 0.12
thetestsetcontainsdiversegraphswithandwithoutbottleneck Graphmaxbetweennesscentrality 0.19 0.70 0.39 0.10
nodes.Wethinkthatitisimportantforfutureworkingraph-based Graphmeanbetweennesscentrality 0.11 0.23 0.15 0.02
environmentstoprovidesimilarmetricsfortheirtestgraphsto
improvecomparability.Onecouldalsodrawinspirationfromcom-
municationnetworksandclassifythegraphsaccordingtotheir 1.0
structure.Whenconsideringdynamicgraphsinfuturework,itwill
beessentialtofurtherquantifythedynamicityofthegraphs.
0.8
C.2 ShortestPathsDistribution
0.6
Fig.7showsthecumulativedistributionoftheall-pairsshortest
paths(APSP)lengthsofthetestgraphsinhops.Whilethelongest
0.4
shortestpathtakes12hops,over99%ofallshortestpathstakeat
most8hops.Wethereforeconsidered8tobeasuitablecandidate
forthenumberofmessagepassingiterationsğ¾ofthenon-recurrent 0.2
approachesandunrolldepthğ½oftherecurrentapproaches.Ahigher
numbermayleadtoimprovedperformanceintheory,atthecost 0.0
ofahighercomputationaloverheadduringtrainingandahigher 0 1 2 3 4 5 6 7 8 9 10 11 12
Numberofhops
communicationoverheadduringexecution.Ourexperimentsshow
thattheeffectofusingmoremessagepassingiterationsdependson
theusedarchitectureandisnotnecessarilypositive(seeSec.5.1). Figure7:CumulativedistributionofAPSPlengthsonthetest
graphs.Theerrorbarsindicatethestandarddeviation.
shtaptsetrohsderevocfonoitroporP