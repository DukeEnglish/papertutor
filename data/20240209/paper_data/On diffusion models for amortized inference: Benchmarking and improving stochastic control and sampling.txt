On diffusion models for amortized inference:
Benchmarking and improving stochastic control and sampling
MarcinSendera123 MinsuKim124 SarthakMittal12 PabloLemos12567 LucaScimeca12
JarridRector-Brooks127 AlexandreAdam125 YoshuaBengio128 NikolayMalkin12
Abstract chainMonteCarlo(MCMC)â€“suchasMetropolis-adjusted
Langevin(MALA;Grenander&Miller,1994;Roberts&
We study the problem of training diffusion
Tweedie,1996;Roberts&Rosenthal,1998)andHamilto-
models to sample from a distribution with a
nianMC(HMC;Duaneetal.,1987;Hoffmanetal.,2014)
given unnormalized density or energy function.
â€“maybeslowtomixbetweenmodesandhaveahighcost
We benchmark several diffusion-structured
persample. WhilevariantssuchassequentialMC(SMC;
inference methods, including simulation-based
Halton, 1962; Chopin, 2002; Del Moral et al., 2006) and
variational approaches and off-policy methods
nested sampling (Skilling, 2006; Buchner, 2021; Lemos
(continuous generative flow networks). Our
etal.,2023)havebettermodecoverage,theircostmaygrow
results shed light on the relative advantages of
prohibitivelywiththedimensionalityoftheproblem. This
existingalgorithmswhilebringingintoquestion
motivates the use of amortized variational inference, i.e.,
someclaimsfrompastwork. Wealsoproposea
fittingparametricmodelsthatsamplethetargetdistribution.
novelexplorationstrategyforoff-policymethods,
basedonlocalsearchinthetargetspacewiththe Diffusionmodels,continuous-timestochasticprocessesthat
useofareplaybuffer,andshowthatitimproves graduallyevolveasimpledistributiontoacomplextarget,
the quality of samples on a variety of target arepowerfuldensityestimatorswithprovenmode-mixing
distributions. Ourcodeforthesamplingmethods properties (De Bortoli, 2022); as such, they have been
and benchmarks studied is made public at widely used in the setting of generative models learned
github.com/GFNOrg/gfn-diffusion fromdata(Sohl-Dicksteinetal.,2015;Songetal.,2021b;
asabaseforfutureworkondiffusionmodelsfor Hoetal.,2020;Nichol&Dhariwal,2021;Rombachetal.,
amortizedinference. 2021). However,theproblemoftrainingdiffusionmodels
tosamplefromadistributionwithagivenblack-boxdensity
orenergyfunctionhasattractedlessattention. Recentwork
1.Introduction hasdrawnconnectionsbetweendiffusion(learningthede-
noisingprocess)andstochasticcontrol(learningtheFoÂ¨llmer
Approximating and sampling from complex multivariate drift),leadingtoapproachessuchasthepathintegralsam-
distributionsisafundamentalprobleminprobabilisticdeep pler(PIS;Zhang&Chen,2022),denoisingdiffusionsam-
learning(e.g.,HernaÂ´ndez-Lobato&Adams,2015;Izmailov
pler(DDS;Vargasetal.,2023),andtime-reverseddiffusion
et al., 2021; Harrison et al., 2024) and in scientific appli- sampler(DIS;Berneretal.,2022);suchapproacheswere
cations(Albergoetal.,2019;NoeÂ´ etal.,2019;Jingetal., recently unified by Richter et al. (2023). Another line of
2022;Adametal.,2022;Holdijketal.,2023). Theprob- work(Lahlouetal.,2023;Zhangetal.,2024)isbasedon
lem of drawing samples from a distribution given only continuousgenerativeflownetworks(GFlowNets),which
an unnormalized probability density or energy is partic- aredeepreinforcementlearningalgorithmsadaptedtovaria-
ularly challenging in high-dimensional spaces and when tionalinferencethatofferstableoff-policytrainingandthus
thedistributionofinteresthasmanyseparatedmodes(Ban- flexibleexploration(Malkinetal.,2023).
deira et al., 2022). Sampling methods based on Markov
Unfortunately, the advances in sampling methods have
1Mila â€“ QueÂ´bec AI Institute 2UniversiteÂ´ de MontreÂ´al not been accompanied by comprehensive benchmarking:
3Jagiellonian University 4KAIST 5Ciela Institute 6Center the reproducibility of the results is often unclear, with
for Computational Astrophysics, Flatiron Institute
the works differing in the choice of model architectures,
7Dreamfold 8CIFAR. Correspondence to: Marcin Sendera
using unstated hyperparameters, and even disagreeing in
<marcin.sendera@{mila.quebec,gmail.com}>,NikolayMalkin
<nikolay.malkin@mila.quebec>. their definitions of the same target densities.1 The first
Preprint.Copyright2024bytheauthor(s).
1Forexample,Vargasetal.(2023)notethatZhang&Chen
1
4202
beF
7
]GL.sc[
1v89050.2042:viXraOndiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
main contribution of this paper is a unified library for Chen, 2022) leverage stochastic optimal control for sam-
diffusion-structured amortized inference methods. The pling from unnormalized densities, albeit still struggling
library has a focus on off-policy methods (continuous withscalabilityinhigh-dimensionalspaces.
GFlowNets)butalsoincludessimulation-basedvariational
Finally,GFlowNets,originallydefinedinthediscretecase
objectives such as PIS. Using this codebase, we are able
byBengioetal.(2021;2023),viewhierarchicalsampling
tobenchmarkmethodsfrompastworkundercomparable
(i.e.,stepwisegeneration)asasequentialdecision-making
conditionsandconfirmclaimsaboutexplorationstrategies
processandrepresentasynthesisofreinforcementlearning
anddesirableinductivebiases,whilecallingintoquestion
andvariationalinferenceapproaches(Malkinetal.,2023;
other claims on the robustness and efficiency of credit
Zimmermannetal.,2023;Tiapkinetal.,2023),expanding
assignment. Ourlibraryalsoincludesseveralnewmodeling
from specific scientific domains (e.g., Jain et al., 2022;
and training techniques, and we provide preliminary
Atanackovic et al., 2023; Zhu et al., 2023) to amortized
evidenceoftheirutilityinpossiblefuturework(Â§5.3).
inference over a broader array of latent structures (e.g.,
Oursecondmaincontributionisastudyofmethodsfor van Krieken et al., 2023; Hu et al., 2024). Their ability
improvingcreditassignmentâ€“thepropagationoflearning to efficiently navigate trajectory spaces via off-policy
signalsfromthetargetdensitytotheparametersofearlier explorationhasbeencrucial,yettheyencounterchallenges
samplingstepsâ€“indiffusion-structuredsamplers(Â§4).First, in training dynamics, such as credit assignment and
our results (Â§5.2) suggest that the technique of utilizing exploration efficiency (Malkin et al., 2022; Madan et al.,
partialtrajectoryinformation(Madanetal.,2022;Panetal., 2022; Pan et al., 2023; Rector-Brooks et al., 2023; Shen
2023),asdoneinthediffusionsettingbyZhangetal.(2024), et al., 2023; Kim et al., 2023; Jang et al., 2024). These
offerslittlebenefit,andahighertrainingcost,overon-policy challenges have repercussions in the scalability of these
(Zhang&Chen,2022)oroff-policy(Lahlouetal.,2023) methodsinmorecomplexscenarios,achallengethispaper
trajectory-based optimization. Second, we examine the addressesinthecontinuouscase.
utilityofagradient-basedvariantwhichparametrizesthe
denoisingdistributionasacorrectiontoaLangevinprocess
3.Setting: Diffusion-structuredsampling
(Zhang&Chen,2022). Weshowthatthisinductivebiasis
alsobeneficialintheoff-policy(GFlowNet)settingdespite Let E : Rğ‘‘ â†’ R be a differentiable energy function and
higher computational cost. Finally, motivated by recent define ğ‘…(x) = exp(âˆ’E(x)), the reward or unnormalized
approachesinthediscretestatespacesetting,weproposean target density. Assuming the integral ğ‘ := âˆ« ğ‘…(x)ğ‘‘x
Rğ‘‘
efficientexplorationtechniquebasedonlocalsearchinthe exists,E definesaBoltzmanndensity ğ‘ (x) = ğ‘…(x) on
target ğ‘
targetspacewiththeuseofareplaybuffer,whichimproves Rğ‘‘
. We are interested in the problems of sampling from
samplequalityacrossvarioustargetdistributions. ğ‘ and approximating the partition function ğ‘ given
target
accessonlytoE andpossiblytoitsgradientâˆ‡E.
2.Priorwork
Wedescribetwocloselyrelatedperspectivesonthisprob-
Amortizedvariationalinference(VI)approachesutilizea lem: vianeuralSDEsandstochasticcontrol(Â§3.1)andvia
parametric model ğ‘ ğœƒ to approximate a given target den- continuousgenerativeflownetworks(Â§3.2).
sity ğ‘ throughstochasticoptimization(Hoffmanetal.,
target
2013; Ranganath et al., 2014; Agrawal & Domke, 2021). 3.1.Euler-Maruyamahierarchicalsamplers
Notably,explicitdensitymodelslikeautoregressivemod-
GenerativemodelingwithSDEs. Diffusionmodelsas-
els and normalizing flows have been extensively utilized
sumeacontinuous-timegenerativeprocessgivenbyaneu-
indensityestimation(Rezende&Mohamed,2015;Dinh
ralstochasticdifferentialequation(SDE;Tzen&Raginsky,
etal.,2017;Wuetal.,2020;Gaoetal.,2020;Nicolietal.,
2019a;Ã˜ksendal,2003;SaÂ¨rkkaÂ¨ &Solin,2019):
2020). However,thesemodelsimposestructuralconstraints,
therebylimitingtheirexpressivepower(Cornishetal.,2020; ğ‘‘xğ‘¡ =ğ‘¢(xğ‘¡,ğ‘¡;ğœƒ)ğ‘‘ğ‘¡+ğ‘”(xğ‘¡,ğ‘¡;ğœƒ)ğ‘‘wğ‘¡, (1)
Grathwohletal.,2019;Zhang&Chen,2021). wherex followsafixedtractabledistributionğœ‡ (suchasa
0 0
Theadoptionofdiffusionprocessesingenerativemodels Gaussianorapointmass). Theinitialdistribution ğœ‡ 0 and
hasstimulatedarenewedinterestinhierarchicalmodelsas the stochastic dynamics specified by (1) induce marginal
densityestimatorsandpavedthewayformoresophisticated densities ğ‘ ğ‘¡ onRğ‘‘ foreachğ‘¡ > 0. Thefunctionsğ‘¢ andğ‘”
sampling methodologies (Vincent, 2011; Ho et al., 2020; havelearnableparametersthatwewishtooptimize,using
Tzen&Raginsky,2019b). ApproacheslikePIS(Zhang& someobjective,soastomaketheterminaldensity ğ‘ 1close
to ğ‘ . Samplescanbedrawnfrom ğ‘ bysamplingx âˆ¼
target 1 0
(2022)usesadifferentvarianceofthefirstcomponentintheFunnel ğœ‡ andsimulatingtheSDE(1)totimeğ‘¡ =1.
density, and the reference partition function value for the log- 0
GaussianCoxprocessbenchmarkalsovariesbetweenauthors. TheSDEdrivingğœ‡ to ğ‘ isnotunique. However,ifone
0 target
2Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
fixesareverse-timeSDE,ornoisingprocess, thatpushes asÎ”ğ‘¡ â†’ 0(i.e., incrementsareinfinitesimallyGaussian),
ğ‘ atğ‘¡ = 1to ğœ‡ atğ‘¡ = 0,thenitsreverse,theforward an application of the central limit theorem that is key to
target 0
SDE(1),isuniquelydeterminedundermildconditionsand stochasticcalculus(Ã˜ksendal,2003).
is called the denoising process. For usual choices of the
noisingprocess,therearestochasticregressionobjectives SDElearningashierarchicalvariationalinference. The
for learning the drift ğ‘¢ of the denoising process given problem of learning the parameters ğœƒ of the forward pro-
samplesfrom ğ‘ target,andthediffusionrateğ‘”isavailablein cesssoastoenforce(6)isoneofhierarchicalvariational
closedform(Hoetal.,2020;Songetal.,2021b). inference. Thebackwardprocesstransformsx intox viaa
1 0
sequenceoflatentvariablesx 1âˆ’Î”ğ‘¡,...,x 0,andtheforward
Time discretization. In practice, the integration of the processaimstomatchtheposteriordistributionoverthese
SDE (1) is approximated by a discrete-time scheme, the variablesandthustoapproximatelyenforce(6).
simplest of which is Euler-Maruyama integration. The
Inthesettingofdiffusionmodelslearnedfromdata,where
process (1) is replaced by a discrete-time Markov chain
onehassamplesfrom ğ‘ ,onecanoptimizetheforward
x 0 â†’ x Î”ğ‘¡ â†’ x 2Î”ğ‘¡ â†’ Â·Â·Â· â†’ x 1,whereÎ”ğ‘¡ = ğ‘‡1 isthetime process by minimizingta tr hge et KL divergence ğ· (ğ‘ Â·
incrementandandğ‘‡ isthenumberofsteps: KL target
ğ‘ ğµâˆ¥ğœ‡ 0Â·ğ‘ ğ¹)betweenthedistributionovertrajectoriesgiven
x âˆ¼ ğœ‡ , (2) bythereverseprocessandthat givenby theforwardpro-
0 0
âˆš cess. Thisisequivalenttothetypicaltrainingofdiffusion
xğ‘¡+Î”ğ‘¡ =xğ‘¡ +ğ‘¢(xğ‘¡,ğ‘¡;ğœƒ)Î”ğ‘¡+ğ‘”(xğ‘¡,ğ‘¡;ğœƒ) Î”ğ‘¡zğ‘¡ zğ‘¡ âˆ¼N(0,Iğ‘‘).
models, which optimizes a variational bound on the data
log-likelihood(seeSongetal.,2021a).
The density of the transition kernel from xğ‘¡ to xğ‘¡+Î”ğ‘¡ can
explicitlybewrittenas However,inthesettingofanintractabledensityğ‘ ,unbi-
target
asedestimatorsofthisdivergencearenotavailable. Instead,
ğ‘ ğ¹(xğ‘¡+Î”ğ‘¡ | xğ‘¡) =N(xğ‘¡+Î”ğ‘¡;xğ‘¡+ğ‘¢(xğ‘¡,ğ‘¡;ğœƒ)Î”ğ‘¡,ğ‘”(xğ‘¡,ğ‘¡;ğœƒ)Î”ğ‘¡Iğ‘‘), onecanoptimizethereverseKL:3
(3)
where ğ‘ ğ¹ denotesthetransitiondensityofthediscretized ğ· KL(ğœ‡ 0Â· ğ‘ ğ¹âˆ¥ğ‘ targetÂ· ğ‘ ğµ) = (7)
f tro ar jw eca tr od riS eD ssE t. arT tih ni gsd ae tn xs 0i :tydefinesajointdistributionover âˆ« log ğ‘ tağœ‡ rg0 et( (ğ‘¥ ğ‘¥0 1) )ğ‘ ğ‘ğ¹ ğµ( (x xÎ” 0ğ‘¡ ,, .. .. .. ,, xx 11 âˆ’Î”| ğ‘¡x 0 |) x 1)ğœ‡ 0(x 0)ğ‘‘x Î”ğ‘¡ ... ğ‘‘x 1.
ğ‘‡âˆ’1
(cid:214)
ğ‘ ğ¹(x Î”ğ‘¡,...,x 1 | x 0) = ğ‘ ğ¹(x (ğ‘–+1)Î”ğ‘¡ | xğ‘–Î”ğ‘¡). (4) Various estimators of this objective are available. For in-
stance,thepathintegralsamplerobjective(PIS;Zhang&
ğ‘–=0
Chen,2022)usesthereparametrizationtricktoexpress(7)
Similarly, adiscrete-timereverseprocessx 1 â†’ x 1âˆ’Î”ğ‘¡ â†’ asanexpectationovernoisevariableszğ‘¡ thatparticipatein
x 1âˆ’2Î”ğ‘¡ â†’Â·Â·Â·â†’x 0withtransitiondensities ğ‘ ğµ(xğ‘¡âˆ’Î”ğ‘¡ | xğ‘¡) thehierarchicalsamplingofx Î”ğ‘¡,...,x 1,yieldinganunbi-
definesajointdistribution2via
asedgradientestimator,butonethatrequiresbackpropaga-
tionintothesimulationoftheforwardprocess. Therelated
ğ‘‡
(cid:214)
ğ‘ ğµ(x 0,...,x 1âˆ’Î”ğ‘¡ | x 1) = ğ‘ ğµ(x (ğ‘–âˆ’1)Î”ğ‘¡ | xğ‘–Î”ğ‘¡). (5) denoisingdiffusionsampler(DDS;Vargasetal.,2023)ap-
pliesthesameprincipleinadifferentintegrationscheme.
ğ‘¡=1
If the forward and backward processes (starting from ğœ‡
0 3.2.Euler-MaruyamasamplersasGFlowNets
and ğ‘ , respectively) are reverses of each other, then
target
theydefinethesamedistributionovertrajectories,i.e.,for Continuousgenerativeflownetworks(Lahlouetal.,2023)
allx 0 â†’x Î”ğ‘¡ â†’Â·Â·Â·â†’x 1, express the problem of enforcing (6) as a reinforcement
learningtask. Inthissection,wesummarizethisinterpreta-
ğœ‡ 0(x 0)ğ‘ ğ¹(x Î”ğ‘¡,...,x 1|x 0) = ğ‘ target(x 1)ğ‘ ğµ(x 0,...,x 1âˆ’Î”ğ‘¡|x 1). tion,itsconnectiontoneuralSDEs,theassociatedlearning
(6) objectives,andtheirrelativeadvantagesanddisadvantages.
Inparticular,themarginaldensitiesofx undertheforward
1
andbackwardprocessesarethenequalto ğ‘ ,andthefor- Theconnectionbetweengenerativeflownetworksanddif-
target
fusion models was first made informally by Malkin et al.
wardprocesscanbeusedtosamplethetargetdistribution.
(2023) in the distribution-matching setting and by Zhang
Inpractice,(6)canbeenforcedonlyapproximately,since etal.(2023a)inthemaximum-likelihoodsetting. Continu-
thereverseofaprocesswithGaussianincrementsis,ingen- ousGFlowNets,andtheirconnectiontosamplingviaEuler-
eral,notitselfGaussian. However,thediscrepancyvanishes
3Tobeprecise,thefractionin(7)shouldbeunderstoodasa
2Inthecasethatğœ‡ isapointmass,weassumethedistribution Radon-Nikodymderivative,whichmakessensewhetherğœ‡ isa
0 0
x
0
|xÎ”ğ‘¡ toalsobeapointmass,whichhasdensityğ‘ ğµ(x
0
|xÎ”ğ‘¡)= pointmassoracontinuousdistributionandgeneralizestocontinu-
1withrespecttothemeasureğœ‡ . oustime(Berneretal.,2022;Richteretal.,2023).
0
3Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Maruyamaintegration,werefirstsetonasolidtheoretical ThelearningproblemsolvedbyGFlowNetsistofindthe
foundationbyLahlouetal.(2023). parametersğœƒ ofapolicy ğ‘ ğ¹ whoseterminatingdensity ğ‘âŠ¤ ğ¹
isequalto ğ‘ ,i.e.,
target
Stateandactionspace. Toformulatesamplingasase-
ğ‘…(x )
quential decision-making problem, one must define the ğ‘âŠ¤(x ;ğœƒ) = 1 âˆ€x âˆˆRğ‘‘. (9)
ğ¹ 1 ğ‘ 1
spaces of states and actions. In the case of sampling by
ğ‘‡-stepEuler-Maruyamaintegration,assumingğœ‡ isapoint However, because the integral (8) is intractable and ğ‘ is
0
massat0,thestatespaceis unknown, auxiliary objects must be introduced into opti-
mizationobjectivestoenforce(9),asdiscussedbelow.
S = {(0,0)âˆª(cid:8) (x,ğ‘¡) :xâˆˆRğ‘‘,ğ‘¡ âˆˆ {Î”ğ‘¡,2Î”ğ‘¡,...,1}(cid:9),
Notably,ifthepolicyisaGaussianwithmeanandvariance
withthepoint(x,ğ‘¡)representingthatthesamplingagentis given by neural networks taking xğ‘¡ and ğ‘¡ as input, then
atpositionxattimeğ‘¡.
learningthepolicyamountstolearningthedriftğ‘¢(xğ‘¡,ğ‘¡;ğœƒ)
anddiffusionğ‘”(xğ‘¡,ğ‘¡;ğœƒ) ofaSDE(1),i.e.,fittinganeural
Samplingbeginswiththeinitialstatex 0 := (0,0),proceeds SDE.ThelearningprobleminÂ§3.1isthusthesameas
throughasequenceofstates(x Î”ğ‘¡,Î”ğ‘¡),(x 2Î”ğ‘¡,2Î”ğ‘¡),...,and thatoffittingaGFlowNetwithGaussianpolicies.
ends at a state (x ,1); states (x,ğ‘¡) with ğ‘¡ = 1 are called
1
terminalstatesandtheircollectionisdenotedX. Fromnow Backwardpolicyandtrajectorybalance. Abackward
on,wewilloftenwritexğ‘¡ inplaceofthestate(xğ‘¡,ğ‘¡)when policy is a collection of conditional probability densities
the time ğ‘¡ is clear from context. The sequence of states ğ‘ ğµ(xğ‘¡âˆ’Î”ğ‘¡ | xğ‘¡;ğœ“),representingaprobabilitydensityoftran-
x 0 â†’x Î”ğ‘¡ â†’Â·Â·Â·â†’x 1iscalledacompletetrajectory. sitioningfromxğ‘¡ toanancestorstatexğ‘¡âˆ’Î”ğ‘¡. Thebackward
Theactionsfromanonterminalstate (xğ‘¡,ğ‘¡) correspondto policy induces a distribution over complete trajectories ğœ
thepossiblenextstates (xğ‘¡+Î”ğ‘¡,ğ‘¡+Î”ğ‘¡) thatcanbereached conditionedontheirterminalstate(cf.(5)):
from (xğ‘¡,ğ‘¡) by a single step of the Euler-Maruyama ğ‘‡
(cid:214)
integrator.4 ğ‘ ğµ(ğœ | x 1;ğœ“;ğœ“) = ğ‘ ğµ(x (ğ‘–âˆ’1)Î”ğ‘¡ | xğ‘–Î”ğ‘¡;ğœ“),
ğ‘–=1
Forwardpolicyandlearningproblem. A(forward)pol- whereexceptionally ğ‘ ğµ(x
0
| x Î”ğ‘¡) =1asğœ‡ 0isapointmass.
icyisacollectionofcontinuousdistributionsoverthesuc-
cessorstatesâ€“statesreachablebyasingleactionâ€“ofevery Generalizingaresultinthediscrete-spacesetting(Malkin
nonterminalstate(x,ğ‘¡).Inourcontext,thisamountstoacol- et al., 2022), Lahlou et al. (2023) show that ğ‘ ğ¹ samples
lectionofconditionalprobabilitydensitiesğ‘ ğ¹(xğ‘¡+Î”ğ‘¡ | xğ‘¡;ğœƒ), from the target distribution (i.e., satisfies (9)) if and only
representingtheprobabilitydensityoftransitioningfrom ifthereexistsabackwardpolicy ğ‘ ğµ andascalar ğ‘ ğœƒ such
xğ‘¡ toxğ‘¡+Î”ğ‘¡. GFlowNettrainingoptimizestheparametersğœƒ, thatthetrajectorybalanceconditionsarefulfilledforevery
whichmaybetheweightsofaneuralnetworkspecifyinga completetrajectoryğœ = (x 0 â†’x Î”ğ‘¡ â†’Â·Â·Â·â†’x 1):
densityoverxğ‘¡+Î”ğ‘¡ conditionedonx Î”ğ‘¡.
ğ‘ ğœƒğ‘ ğ¹(ğœ;ğœƒ) = ğ‘ target(x 1)ğ‘ ğµ(ğœ | x 1;ğœ“). (10)
Apolicyğ‘ ğ¹inducesadistributionovercompletetrajectories
ğœ = (x 0 â†’x Î”ğ‘¡ â†’Â·Â·Â·â†’x 1)via I fuf nth ce tis oe nc ğ‘on =di âˆ«tio ğ‘…n (s xh )o ğ‘‘ld x.,t Th he en tğ‘ rağœƒ jee cq tu oa ryls bt ah le antr cu ee op ba jr et cit ti io vn
e
x
ğ‘‡âˆ’1 foratrajectoryğœ isthesquaredlog-ratioofthetwosides
(cid:214)
ğ‘ ğ¹(ğœ;ğœƒ) = ğ‘ ğ¹(x (ğ‘–+1)Î”ğ‘¡ | xğ‘–Î”ğ‘¡;ğœƒ). of(10),thatis:
ğ‘–=0
Inparticular,wegetamarginaldensityoverterminalstates: L TB(ğœ;ğœƒ,ğœ“) = (cid:18) log
ğ‘
target(ğ‘ xğœƒ 1ğ‘ )ğ‘ğ¹ ğµ(ğœ (ğœ;ğœƒ |)
x
1;ğœ“)(cid:19)2 . (11)
âˆ«
ğ‘âŠ¤ ğ¹(x 1;ğœƒ)= ğ‘ ğ¹(x 0 â†’x Î”ğ‘¡ â†’Â·Â·Â·â†’x 1;ğœƒ)ğ‘‘x Î”ğ‘¡...ğ‘‘x 1âˆ’Î”ğ‘¡. One can thus achieve (9) by minimizing to zero the loss
L (ğœ;ğœƒ,ğœ“)withrespecttotheparametersğœƒ andğœ“,where
TB
(8) thetrajectoriesğœusedfortrainingaresampledfromsome
4Formally,themathematicalfoundationsinLahlouetal.(2023) trainingpolicyğœ‹(ğœ). Whileitispossibletooptimize(11)
requireustodefinereferencemeasuresonthestateandaction with respect to the parameters of both the forward and
spaceswithrespecttowhichdensitiesmaybedefined.Here,we backwardpolicies,insomelearningproblems,onefixesthe
aredealingwithEuclideanspacesandalwaysassumetheLebesgue backwardpolicyandonlyoptimizestheparametersof ğ‘ ğ¹
measure,soreadersneednotburdenthemselveswithmeasurethe-
ory.Wenote,however,thatthisflexibilityallowseasygeneraliza-
andtheestimateofthepartitionfunctionğ‘ ğœƒ. Forexample,
tiontosamplingonmorecomplexspaces,suchasanyRiemannian formostexperimentsinÂ§5,wefixthebackwardpolicyto
manifolds,whereothermethodsdonotdirectlyapply. adiscretizedBrownianbridge,followingpastwork.
4Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Off-policy optimization. Unlike the KL objective (7), forward-lookingrewardshapingschemeproposedbyPan
whosegradientinvolvesanexpectationoverthedistribution etal.(2023). Ithasalsobeentestedinthecontinuouscase,
of trajectories under the forward process, (11) can be op- butourexperimentalresultssuggestthatitofferslittleben-
timizedoff-policy,i.e.,usingtrajectoriessampledfroman efitovertheTBobjectiveinthediffusionsetting(seeÂ§4.1).
arbitarydistributionğœ‹.BecauseminimizingL (ğœ;ğœƒ,ğœ“)to
TB It is also worth noting the off-policy VarGrad estimator
0forallğœinthesupportofğœ‹willachieve(9),ğœ‹canbetaken
(Richteretal.,2020),originallystatedforhierarchicalvari-
beanydistributionwithfullsupport,soastopromotedis-
ationalmodelsandrediscoveredforGFlowNetsbyZhang
coveryofmodesofthetargetdistribution. Variouschoices
et al. (2023b). Like TB, VarGrad can be optimized over
motivatedbyreinforcementlearningtechniqueshavebeen
trajectoriesdrawnoff-policy. Ratherthanenforcing(10)for
proposed,includingnoisyexplorationortempering(Bengio
everytrajectory,VarGradoptimizestheempiricalvariance
etal.,2021),replaybuffers(Deleuetal.,2022),Thompson
(overaminibatch)ofthelog-ratioofthetwosidesof(10).
sampling(Rector-Brooksetal.,2023),andbackwardtraces
AsnotedbyMalkinetal.(2023),thisisequivalenttomini-
from terminal states obtained by MCMC (Lemos et al.,
2023). Inthecontinuouscase,Malkinetal.(2023);Lahlou mizingL TB firstwithrespecttologğ‘ ğœƒ tooptimalityover
thebatch,thenwithrespecttotheparametersof ğ‘ ğ¹.
etal.(2023)proposedtosimplyaddasmallconstanttothe
policyvariancewhensamplingtrajectoriesfortraining. Off-
policyoptimizationisakeyadvantageofGFlowNetsover 4.CreditassignmentincontinuousGFlowNets
variationalmethodssuchasPIS,whichrequireon-policy
Themainchallengesintrainingoff-policysamplingmodels
optimization(Malkinetal.,2023).
areexplorationefficiency(discoveryofhigh-rewardstates)
However,whenL happenstobeoptimizedonpolicy,i.e., and credit assignment (propagation of reward signals to
TB
usingtrajectoriessampledfromthepolicy ğ‘ ğ¹ itself,weget the actions that led to them). We describe several new
anunbiasedestimatorofthegradientoftheKLdivergence and existing methods for addressing these challenges in
withrespectto ğ‘ ğ¹â€™sparametersuptoaconstant(Malkin the context of diffusion-structured GFlowNets. These
etal.,2023;Zimmermannetal.,2023),thatis: techniqueswillbeempiricallystudiedandcomparedinÂ§5.
E ğœâˆ¼ğ‘ğ¹(ğœ) [âˆ‡ğœƒL TB(ğœ;ğœƒ,ğœ“)] = 4.1.Partialenergiesandsubtrajectory-basedlearning
=2âˆ‡ğœƒğ· KL(ğ‘ ğ¹(ğœ;ğœƒ)âˆ¥ğ‘ target(x 1)ğ‘ ğµ(ğœ | x 1;ğœ“)).
Zhang et al. (2024) studied the continuous GFlowNet
Thisgradientestimatortendstohavehighervariancethan learning problem introduced by Lahlou et al. (2023), but
thereparametrization-basedestimatorof(7)usedbyPIS. replacedtheTBlearningobjectivewiththeSubTBobjec-
Ontheotherhand,itisstillunbiased,doesnotrequireback- tive. Inaddition, forthestateflowfunction, aninductive
propagationthroughthesimulationoftheforwardprocess, biasresemblingthegeometricdensityinterpolationinMaÂ´teÂ´
and can be used to optimize the parameters of both the &Fleuret(2023)wasused:
forwardandbackwardpolicies.
log ğ‘“(xğ‘¡;ğœƒ) = (1âˆ’ğ‘¡)logğ‘r ğ‘¡ef(xğ‘¡)+ğ‘¡logğ‘…(xğ‘¡)+NN(xğ‘¡,ğ‘¡;ğœƒ),
Otherobjectives. Thetrajectorybalanceobjective(11) (13)
is not the only possible objective that can be used to en- whereNNisaneuralnetworkand ğ‘r ğ‘¡ef(xğ‘¡) =N(xğ‘¡;0,ğœ2ğ‘¡)
force(9). Anotablegeneralizationissubtrajectorybalance isthemarginaldensityofaBrownianmotionwithrateğœat
(SubTB;Madanetal.,2022), whichinvolvesmodelinga xğ‘¡. Theuseofthetargetdensitylogğ‘…(xğ‘¡) =âˆ’E(xğ‘¡)inthe
scalarstateflow ğ‘“(xğ‘¡;ğœƒ)fassociatedwitheachstatexğ‘¡ â€“in- stateflowfunctionwashypothesizedtoprovideaneffective
tendedtomodelthemarginaldensityoftheforwardprocess signal driving the sampler to high-density states at early
atxğ‘¡ â€“andenforcingsubtrajectorybalanceconditionsfor stepsinthetrajectory. Suchaninductivebiasonthestate
allpartialtrajectoriesxğ‘šÎ”ğ‘¡ â†’x (ğ‘š+1)Î”ğ‘¡ â†’Â·Â·Â·â†’xğ‘›Î”ğ‘¡: flowwascalledforward-looking(FL)byPanetal.(2023),
andwewillrefertothismethodasFL-SubTBinÂ§5.
ğ‘›âˆ’1
(cid:214)
ğ‘“(xğ‘š;ğœƒ) ğ‘ ğ¹(x (ğ‘–+1)Î”ğ‘¡ | xğ‘–Î”ğ‘¡;ğœƒ) =
4.2.Langevindynamicsinductivebias
ğ‘–=ğ‘š
(cid:214)ğ‘› Zhang & Chen (2022) proposed an inductive bias on
= ğ‘“(xğ‘›;ğœƒ) ğ‘ ğµ(x (ğ‘–âˆ’1)Î”ğ‘¡ | xğ‘–Î”ğ‘¡;ğœ“), (12) the architecture of the drift of the neural SDE ğ‘¢(xğ‘¡,ğ‘¡;ğœƒ)
ğ‘–=ğ‘š+1
(in GFlowNet terms, the mean of the Gaussian density
where for terminal states ğ‘“(x ) = ğ‘…(x ). This approach ğ‘ ğ¹(xğ‘¡+Î”ğ‘¡ | xğ‘¡;ğœƒ)) that resembles a Langevin process on
1 1
thetargetdistribution. Onewrites
hassomecomputationaloverheadassociatedwithtraining
the state flow, but has been shown to be effective in
discrete-spacesettings,especiallywhencombinedwiththe ğ‘¢(xğ‘¡,ğ‘¡;ğœƒ) =NN 1(xğ‘¡,ğ‘¡;ğœƒ)+NN 2(ğ‘¡;ğœƒ)âˆ‡E(xğ‘¡), (14)
5Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
whereNN andNN areneuralnetworksoutputtingavector Trainingwithlocalsearch. Totrainsamplerswiththe
1 2
andascalar,respectively.Thesecondtermin(14)isascaled aidofthebuffer,wedrawasamplexfromD (uniformly
LS
gradientofthetargetenergyâ€“thedriftofaLangevinSDE orusingaprioritizationscheme,Â§D),sampleatrajectoryğœ
â€“andthefirsttermisalearnedcorrection. Thisinductive leadingtoxfromthebackwardprocess,andmakeagradient
bias,whichwenametheLangevinparametrization(LP), updateontheobjective(e.g.,TB)associatedwithğœ.
wasshowntoimprovetheefficiencyofPIS.Wewillstudy
When training with local search guidance, we alternate
forthefirsttimeitseffectoncontinuousGFlowNetsinÂ§5.
twosteps,inspiredbyLemosetal.(2023),whoalternate
Theinductivebias(14)placedonpoliciesrepresentsadiffer- trainingonforwardtrajectoriesandbackwardtrajectories
entwayofincorporatingtherewardsignalatintermediate initializedatafixedsetofMCMCsamples. StepAinvolves
steps in the trajectory and can steer the sampler towards training with on-policy or exploratory forward sampling
low-energyregions. Itcontrastswith(13)inthatitprovides while Step B uses samples drawn from the local search
thegradientoftheenergydirectlytothepolicy,ratherthan bufferdescribedabove. Thisallowsthesamplertoexplore
justusingtheenergytoprovidealearningsignaltopolicies bothdiversifiedsamples(StepA)andlow-energysamples
viatheparametrizationofthelog-stateflow(13). (StepB).SeeÂ§Dfordetailedpseudocodeofadaptive-step
parallelMALAandlocalsearch-guidedGFlowNettraining.
Considerationsofthecontinuous-timelimitleadustocon-
jecture that the Langevin parametrization (14) with NN
1
independentofxğ‘¡ isequivalenttotheforward-lookingflow 5.Experiments
(13) in the limit of small time increments Î”ğ‘¡ â†’ 0, i.e.,
Weconductcomprehensivebenchmarksofvariousdiffusion-
theyinducethesameasymptoticsofthediscrepancyinthe
structured samplers, encompassing both GFlowNet sam-
SubTBconstraints(12)overshortpartialtrajectories. Such
plersandmethodssuchasPIS.FortheGFlowNetsamplers,
theoreticalanalysiscanbethesubjectoffuturework.
we investigate a range of techniques, including different
explorationstrategiesandlossfunctions. Additionally,we
4.3.Efficientexplorationwithlocalsearch
examinetheefficacyoftheLangevinparametrizationand
TheFLandLPinductivebiasesbothinducecomputational thenewlyproposedlocalsearchwithbuffer.
overhead:eitherintheevaluationandoptimizationofastate
floworintheneedtoevaluatetheenergygradientatevery 5.1.Tasksandbaselines
stepofsampling. Wepresentanalternativetechniquethat
Weexploretwotypesoftasksinourstudy: firstly,sampling
does not induce additional computation cost per training
fromenergydistributionsâ€“a2-dimensionalGaussianmix-
trajectoryoverthatofregularTBtrainingofGFlowNets.
turemodelwith25modes(25GMM),the10-dimensional
Toenhancethequalityofsamplesduringtraining,wepro- Funnel,andthe32-dimensionalManywelldistributionâ€“
pose the incorporation of local search methodologies to and,secondly,conditionalsamplingfromthelatentposte-
steertheexplorationprocessofGFlowNets. Whilepromis- riorofavariationalautoencoder(VAE; Kingma&Welling
inglocalsearchstrategies(Zhangetal.,2022;Kimetal., (2014);Rezendeetal.(2014)). Thisallowsustoinvestigate
2024)andreplaybuffermethods(e.g.Deleuetal.,2022) both unconditional and conditional generative modeling
forGFlowNetsindiscretespaceshavebeenproposed,our techniques. AlltargetdensitiesaredescribedinÂ§B.
methodology leverages the parallel Metropolis-adjusted
Weevaluatethreealgorithmcategories:
Langevinalgorithm(MALA)forcontinuousdomains.
(1) Traditional sampling methods. Weconsider a stan-
Indetail,weinitiallysample ğ‘€ candidatesfromthesam-
pler: {x(1),...,x(ğ‘€)} âˆ¼ ğ‘âŠ¤(Â·). Subsequently,wedeploy dard Sequential Monte Carlo (SMC) implementation
ğ¹ andastate-of-the-artnestedsamplingmethod(GGNS,
parallelLangevindynamicssamplingacrossğ‘€ chains,with
theinitialstatesoftheMarkovchainbeing{x(1),...,x(ğ‘€)}. Lemosetal.(2023)).
(2) Simulation-driven variational approaches. Our
Thisprocessinvolvesexecutingğ¾ transitions,duringwhich
benchmarks include DIS (Berner et al., 2022), DDS
certaincandidatesmayberejectedbasedontheMetropolis-
(Vargasetal.,2023),andPIS(Zhang&Chen,2022).
Hastingsacceptancerule. Aftertheğ¾ burn-intransi-
burn-in (3) Diffusion-basedGFlowNetsamplers. Ourevaluation
tions, the accepted samples are stored in the local search
focuses on TB-based training and the enhancements
bufferD .WeoccasionallyupdatethebufferusingMALA
LS describedinÂ§4: theVarGradestimator(VarGrad),off-
stepsandreplaysamplesfromittominimizethecomputa-
policyexploration(Expl.), Langevinparametrization
tionaldemandsofiterativelocalsearch. MALAstepsare
(LP),andlocalsearch(LS).Additionally,weassessthe
farmoreparallelizablethansamplertrainingandneedtobe
FL-SubTB-basedcontinuousGFlowNetasstudiedby
madeonlyrarely,sotheoverheadoflocalsearchissmall.
Zhangetal.(2024)foracomprehensivecomparison.
6Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Table1.Log-partitionfunctionestimationerrorsand2-Wassersteindistancesforunconditionalmodelingtasks(meanandstandard
deviationover5runs).Thefourgroupsofmodelsare:MCMC-basedsamplers,simulation-drivenvariationalmethods,baselineGFlowNet
methodswithdifferentlearningobjectives,andmethodsaugmentedwiththeLangevinparametrizationandlocalsearchtechniques.
Energyâ†’ 25GMM(ğ‘‘=2) Funnel(ğ‘‘=10) Manywell(ğ‘‘=32)
Algorithmâ†“Metricâ†’ Î”logğ‘ Î”logğ‘RW W2 Î”logğ‘ Î”logğ‘RW W2 Î”logğ‘ Î”logğ‘RW W2
2 2 2
SMC 0.569Â±0.010 0.86Â±0.10 0.561Â±0.801 50.3Â±18.9 14.99Â±1.078 8.28Â±0.32
GGNS(Lemosetal.,2023) 0.016Â±0.042 1.19Â±0.17 0.033Â±0.173 25.6Â±4.75 0.292Â±0.454 6.51Â±0.32
DIS(Berneretal.,2022) 1.125Â±0.056 0.986Â±0.011 4.71Â±0.06 0.839Â±0.169 0.093Â±0.038 20.7Â±2.1 N/A N/A N/A
DDS(Vargasetal.,2023) 1.760Â±0.08 0.746Â±0.389 7.184Â±0.044 0.424Â±0.049 0.206Â±0.033 29.3Â±9.5 N/A N/A N/A
PIS(Zhang&Chen,2022) 1.769Â±0.104 1.274Â±0.218 6.37Â±0.65 0.534Â±0.008 0.262Â±0.008 22.0Â±4.0 3.85Â±0.03 2.69Â±0.04 6.15Â±0.02
+LP 1.799Â±0.051 0.225Â±0.583 7.16Â±0.11 0.587Â±0.012 0.285Â±0.044 22.1Â±4.0 13.19Â±0.82 0.07Â±0.85 6.55Â±0.34
TB(Lahlouetal.,2023) 1.176Â±0.109 1.071Â±0.112 4.83Â±0.45 0.690Â±0.018 0.239Â±0.192 22.4Â±4.0 4.01Â±0.04 2.67Â±0.02 6.14Â±0.02
TB+Expl.(Lahlouetal.,2023) 0.560Â±0.302 0.422Â±0.320 3.61Â±1.41 0.749Â±0.015 0.226Â±0.138 21.3Â±4.0 4.01Â±0.05 2.68Â±0.06 6.15Â±0.02
VarGrad+Expl. 0.615Â±0.241 0.487Â±0.250 3.89Â±0.85 0.642Â±0.010 0.250Â±0.112 22.1Â±4.0 4.01Â±0.05 2.69Â±0.06 6.15Â±0.02
FL-SubTB(Zhangetal.,2024) 1.150Â±0.054 1.043Â±0.054 4.74Â±0.23 0.541Â±0.010 0.227Â±0.106 22.1Â±4.0 4.05Â±0.05 2.71Â±0.02 6.15Â±0.01
TB+Expl.+LS(ours) 0.171Â±0.013 0.004Â±0.011 1.25Â±0.18 0.653Â±0.025 0.285Â±0.099 21.9Â±4.0 4.57Â±2.13 0.19Â±0.29 5.66Â±0.05
TB+Expl.+LP(ours) 0.206Â±0.018 0.011Â±0.010 1.29Â±0.07 0.666Â±0.615 0.051Â±0.616 22.3Â±3.9 7.46Â±1.74 1.06Â±1.11 5.73Â±0.31
TB+Expl.+LP+LS(ours) 0.190Â±0.013 0.007Â±0.011 1.31Â±0.07 0.768Â±0.052 0.264Â±0.063 21.8Â±3.9 4.68Â±0.49 0.07Â±0.17 5.33Â±0.03
Highlight:meannotdistinguishablefromminimumincolumnwithğ‘<0.05assumingGaussian-distributederrors(Welchunpairedğ‘¡-test).
0.0
0.5
1.0
1.5
Constant exploration
2.0 Decaying exploration
Ground truth
Figure1.Two-dimensionalprojectionsofManywellsamplesfrom 2.5
modelstrainedbydifferentalgorithms.Ourproposedreplaybuffer 0 0.1 0.2 0.3 0.4 0.5
Exploration rate
withlocalsearchiscapableofpreventingmodecollapse.
Figure2.EffectofexplorationvarianceonmodelstrainedwithTB
onthe25GMMenergy.Explorationpromotesmodediscovery,but
For(2)and(3),weemployaconsistentneuralarchitecture shouldbedecayedovertimetooptimallyallocatethemodeling
powertohigh-likelihoodtrajectories.
acrossmethods,benchmarkingthemwithinourcomprehen-
sivelibrary(detailsinÂ§C).
Benchmarkingmetrics. Toevaluatediffusion-basedsam-
Learningproblemandfixedbackwardprocess. Inour plers,weusetwometricsfrompastwork(Zhang&Chen,
main experiments, we borrow the modeling setting from 2022;Lahlouetal.,2023),whichwerestateinournotation.
Zhang&Chen(2022). WeaimtolearnaGaussianforward Givenanyforwardpolicy ğ‘ ğ¹,wehaveavariationallower
policy ğ‘ ğ¹ thatsamplesfromthetargetdistributioninğ‘‡ = boundonthelog-partitionfunctionlogğ‘ =âˆ« Rğ‘‘ ğ‘…(x)ğ‘‘x:
1
2
p0
0
ro0
2
c2s e;t se
L
sp as
ih
s( loÎ” fiuğ‘¡
xe
e=
t
d0
a
tl. o.0 ,1
2
a) 0. d2J i3u
s;
cs Zt rea
h
ts
a
izni en
g
dp ea
t
Bs
a
rt
l
o.w
,
w2o n0r ik
2 a4
n( )Z
,
bh
t
rha in
e
dg
gb
ea&
c
wkC
w
ith hae rn
d
a, logâˆ«
Rğ‘‘
ğ‘…(x)ğ‘‘x=logE ğœ=(Â·Â·Â·â†’x1)âˆ¼ğ‘ğ¹(ğœ)
(cid:20)ğ‘…(x
ğ‘1
ğ¹) (ğ‘ ğœğµ( |ğœ
x
1| )x 1)(cid:21)
noiserateğœthatdependsonthedomain;explicitly,
â‰¥ E ğœ=(Â·Â·Â·â†’x1)âˆ¼ğ‘ğ¹(ğœ)
(cid:20)
log
ğ‘…(x
ğ‘1
ğ¹) (ğ‘ ğœğµ( |ğœ
x
1| )x 1)(cid:21)
.
(cid:18) ğ‘¡âˆ’Î”ğ‘¡ ğ‘¡âˆ’Î”ğ‘¡ (cid:19)
ğ‘ ğµ(xğ‘¡âˆ’Î”ğ‘¡ | xğ‘¡)=N xğ‘¡âˆ’Î”ğ‘¡;
ğ‘¡
xğ‘¡,
ğ‘¡
ğœ2Î”ğ‘¡Iğ‘‘ , (15) Weuseağ¾-sampleMonteCarloestimateofthisexpectation,
logğ‘Ë†,asametric,whichalwaysequalsthetruelogğ‘ if ğ‘ ğ¹
and ğ‘ ğµ jointlysatisfy(10)andthus ğ‘ ğ¹ samplesfromthe
understoodtobeapointmassat0whenğ‘¡ = Î”ğ‘¡. Tokeep targetdistribution. (Inourexperiments,ğ¾ =2000.)
thelearningproblemconsistentwithpastwork,wefixthe
varianceoftheforwardpolicy ğ‘ ğ¹ toğœ2. Thissimplification We also employ an importance-weighted variant, which
isjustifiedincontinuoustime,whentheforwardandreverse emphasizesmodecoverageoveraccuratelocalmodeling:
wSD illE ps rh oa vv ie deth ee vs ida em ne cedi tf hfu atsi lo en arr nat ie n. gH tho ew fe ov re wr, ai rn dÂ§ p5 o.3 li, cyw â€™e
s logğ‘Ë†RW
:=logâˆ‘ï¸ğ¾ (cid:34)
log
ğ‘…(x 1(ğ‘–))ğ‘ ğµ(ğœ(ğ‘–) | x 1(ğ‘–))(cid:35)
,
varianceisquitebeneficialforshortertrajectories. ğ‘–=1 ğ‘ ğ¹(ğœ(ğ‘–) | x 1(ğ‘–))
7
ZgolOndiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
baselines,non-GFlowNetbaselines,andnon-amortizedsam-
Table2.Log-likelihoodestimatesonatestsetforapretrainedVAE
plingmethodsinmosttasksandmetrics. Thisadvantageis
decoderonMNIST.Thelatentbeingsampledis20-dimensional.
attributedtoefficientexplorationandtheabilitytoreplay
TheVAEâ€™strainingELBO(Gaussianencoder)wasâ‰ˆâˆ’101.
pastlow-energyregions,thuspreventingmodecollapsedur-
Algorithmâ†“Metricâ†’ logğ‘Ë† logğ‘Ë†RW
ingtraining(Fig.1). FurtherdetailsonLSenhancements
GGNS(Lemosetal.,2023) âˆ’82.406Â±0.882
arediscussedinÂ§DwithablationstudiesinÂ§D.2.
PIS(Zhang&Chen,2022) âˆ’102.54Â±0.437 âˆ’47.753Â±2.821
+LP âˆ’99.890Â±0.373 âˆ’47.326Â±0.777 Incorporating Langevin parametrization (LP) into TB re-
TB(Lahlouetal.,2023) âˆ’162.73Â±35.55 âˆ’61.407Â±17.83 sultsinnotableperformanceimprovements(despite2-3Ã—
VarGrad âˆ’102.54Â±0.934 âˆ’46.502Â±1.018 slowertimeperiteration),indicatingthattheobservations
TB+Expl.(Lahlouetal.,2023) âˆ’148.04Â±4.046 âˆ’49.967Â±5.683 ofZhang&Chen(2022)transfertooff-policyalgorithms.
FL-SubTB(Zhangetal.,2024) âˆ’202.78Â±144.9 âˆ’67.501Â±38.08 ComparedtoFL-SubTB,whichaimsforenhancedcredit
TB+Expl.+LS(ours) âˆ’245.78Â±13.80 âˆ’55.378Â±9.125 assignment through partial energy, LP achieves superior
TB+Expl.+LP(ours) âˆ’112.45Â±0.671 âˆ’48.827Â±1.787 creditassignmentleveraginggradientinformation,akinto
TB+Expl.+LP+LS(ours) âˆ’117.26Â±2.502 âˆ’49.157Â±2.051
partial energy in a continuous-time domain. LP is either
VarGrad+Expl.(ours) âˆ’103.39Â±0.691 âˆ’47.318Â±1.981
VarGrad+Expl.+LS(ours) âˆ’105.40Â±0.882 âˆ’48.235Â±0.891 superiororcompetitiveacrossmosttasksandmetrics.
VarGrad+Expl.+LP(ours) âˆ’99.472Â±0.259 âˆ’46.574Â±0.736
VarGrad+Expl.+LP+LS(ours) âˆ’99.783Â±0.312 âˆ’46.245Â±0.543
Conditional sampling. For the VAE task, we observe
thattheperformanceofthebaselineGFlowNet-basedsam-
whereğœ(1),...,ğœ(ğ¾) aretrajectoriessampledfrom ğ‘ ğ¹ and plersisgenerallyworsethanthatofthesimulation-based
leading to terminal states
x(1),...,x(ğ¾)
. The estimator
PIS(Table2). WhileLPandLSimprovetheperformance
1 1 ofTB,theydonotclosethegapinlikelihoodestimation;
logğ‘Ë†RW is also a lower bound on logğ‘ and approaches
however, with the VarGrad objective, the performance is
it as ğ¾ â†’ âˆ (Burda et al., 2016). In the unconditional
competitive with or superior to PIS. We hypothesize that
modelingbenchmarks,wecomparebothestimatorstothe
this discrepancy is due to the difficulty of fitting the con-
truelog-partitionfunction,whichisknownanalytically.
ditionallog-partitionfunctionestimator,whichisrequired
Inaddition,wecomputeasample-basedmetric,thesquared fortheTBobjectivebutnotforVarGrad,whichonlylearns
2-WassersteindistanceW2betweensetsofğ¾samplesfrom thepolicy. (InFig.C.1weshowdecodedsamplesencoded
2
thetruedistributionandgeneratedbyatrainedsampler. usingthebest-performingdiffusionencoder.)
5.2.Results 5.3.ExtensionstogeneralSDElearningproblems
Unconditional sampling. We report the metrics for all Ourimplementationofdiffusion-structuredgenerativeflow
algorithmsandenergiesinTable1. networksincludesseveraladditionalfeaturesthatdiverge
fromthemodelingassumptionsmadeinpastworkinthe
WeobservethatTBâ€™sperformanceisgenerallymodestwith-
field. Notably,itfeaturestheabilityto:
out additional exploration and credit assignment mecha-
nisms,exceptontheFunneltask,wherevariationsinper- â€¢ optimizethebackward(noising)process,notonlythe
formance across methods are negligible. This confirms denoisingprocess;
hypotheses from past work about the importance of off- â€¢ learn the forward processâ€™s diffusion rate ğ‘”(xğ‘¡,ğ‘¡;ğœƒ),
policyexploration(Malkinetal.,2023;Lahlouetal.,2023) notonlythemeanğ‘¢(xğ‘¡,ğ‘¡;ğœƒ);
andtheimportanceofimprovedcreditassignment(Zhang â€¢ assumeavaryingnoisescheduleforthebackwardpro-
etal.,2024). Ontheotherhand,ourresultsdonotshowa cess, making it possible to train models with standard
consistentandsignificantimprovementoftheFL-SubTB noisingSDEsusedfordiffusionmodelsforimages.
objectiveusedbyZhangetal.(2024)overTB.Replacing
Theseextensionswillallowotherstobuildonourimplemen-
TBwiththeVarGradobjectiveyieldssimilarresults.
tationandapplyittoproblemssuchasfinetuningdiffusion
Thesimpleoff-policyexplorationmethodofaddingvariance modelstrainedonimageswithaGFlowNetobjective.
tothepolicynotablyenhancesperformanceonthe25GMM
AsnotedinÂ§5.1,inthemainexperimentswefixedthedif-
task. We investigate this phenomenon in more detail in
fusionrateofthelearnedforwardprocess,anassumption
Fig.2,findingthatexplorationthatslowlydecreasesover
inheritedfromallpastworkandjustifiedinthecontinuous-
thecourseoftrainingisthebeststrategy.
timelimit. However,weperformanexperimenttoshowthe
Ontheotherhand,ourlocalsearch-guidedexplorationwith importanceofextensionssuchaslearningtheforwardvari-
a replay buffer (LS) leads to a substantial improvement anceindiscretetime. Fig.3showsthesamplesofmodels
inperformance,surpassingorcompetingwithGFlowNet onthe25GMMenergyfollowingtheexperimentalsetupof
8Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
0
1
2
3
4
Fixed variance
5
Learned variance
6 Ground truth
1 2 4 8 16 32 64 128
Number of discretization steps
Figure3.Left:Distributionofx 0,x 0.1,...,x 1learnedby10-stepsamplerswithfixed(top)andlearned(middle)forwardpolicyvariance
onthe25GMMenergy. Thelaststepofsamplingthefixed-variancemodeladdsGaussiannoiseofavarianceclosetothatofthe
componentsofthetargetdistribution,preventingthethesamplerfromsharplycapturingthemodes.Thelastrowshowsthepolicyvariance
learnedasafunctionofxğ‘¡ atvarioustimestepsğ‘¡(whiteishighvariance,blueislow),showingthatlessnoiseisaddedaroundthepeaks
nearğ‘¡ =1.Thetwomodelsâ€™log-partitionfunctionestimatesareâˆ’1.67andâˆ’0.62,respectively.Right:Forvaryingnumberofstepsğ‘‡,we
plotthelogğ‘Ë† obtainedbymodelswithfixedandlearnedvariance.Learningpolicyvariancesgivessimilarsamplerswithfewersteps.
Lemosetal.(2023). Weseethatwhentheforwardpolicyâ€™s helpfuldiscussions.
varianceislearned,themodelcanbettercapturethedetails
The authors acknowledge funding from UNIQUE, CI-
ofthetargetdistributions, choosingalowvarianceinthe
FAR,NSERC,Intel,RecursionPharmaceuticalsandSam-
vicinityofthepeakstoavoidâ€˜blurringâ€™themthroughthe
sung. Theresearchwasenabledinpartbycomputational
noiseaddedinthelaststepofsampling.
resources provided by the Digital Research Alliance of
Theabilitytomodeldistributionsaccuratelyinfewersteps Canada(https://alliancecan.ca),Mila(https:
isimportantforcomputationalefficiency. Futureworkcan //mila.quebec),andNVIDIA.
consider further extensions that improve performance in
coarsetimediscretizations,suchasnon-Gaussiantransition
Impactstatement
densities, whose utility in diffusion models trained from
datahasbeendemonstrated(Xiaoetal.,2022). Thisworkstudiesamortizedvariationalinferenceovercon-
tinuous variables, a problem of independent interest in
6.Conclusion Bayesian machine learning but also widely applicable in
thesciences. Weenvisionourworkasabuildingblockfor
We have presented a study of diffusion-structured sam- futureresearchinthisfield,settingnecessarycomparative
plers for amortized inference over continuous variables. standards amongst different methodologies and enabling
Our results suggest promising techniques for improving fairbenchmarking. Weultimatelyhopethatouralgorithms
the mode coverage and efficiency of these models. Fu- willbeusedresponsiblyandhelptoadvancescientificun-
tureworkonapplicationscanconsiderinferenceofhigh- derstandingoftheworld.
dimensionalparametersofdynamicalsystemsandinverse
problems. In probabilistic machine learning, extensions
References
ofthisworkshouldstudyintegrationofouramortizedse-
quentialsamplersasvariationalposteriorsinanexpectation- Adam, A., Coogan, A., Malkin, N., Legin, R., Perreault-
maximization loopfor traininglatent variable models, as Levasseur, L., Hezaveh, Y., and Bengio, Y. Posterior
wasrecentlydonefordiscretecompositionallatentsbyHu samplesofsourcegalaxiesinstronggravitationallenses
et al. (2023), and for sampling Bayesian posteriors over withscore-basedpriors.arXivpreprintarXiv:2211.03812,
high-dimensionalmodelparameters. Themostimportantdi- 2022.
rectionoftheoreticalworkisunderstandingthecontinuous-
timelimit(ğ‘‡ â†’âˆ)ofallthealgorithmswehavestudied. Agrawal, A. and Domke, J. Amortized variational infer-
enceforsimplehierarchicalmodels. NeuralInformation
ProcessingSystems(NeurIPS),2021.
Acknowledgments
WethankCheng-HaoLiuforassistancewithmethodsfrom Albergo, M. S., Kanwar, G., and Shanahan, P. E. Flow-
priorwork,aswellasJuliusBerner,VÂ´Ä±ctorElvira,Lorenz basedgenerativemodelsforMarkovchainMonteCarlo
Richter, Alexander Tong, and Siddarth Venkatraman for inlatticefieldtheory. PhysicalReviewD,100(3):034515,
2019.
9
ZgolOndiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Atanackovic,L.,Tong,A.,Wang,B.,Lee,L.J.,Bengio,Y., Gao, C., Isaacson, J., and Krause, C. i-flow: High-
andHartford,J. DynGFN:Towardsbayesianinference dimensionalintegrationandsamplingwithnormalizing
of gene regulatory networks with GFlowNets. Neural flows. MachineLearning: ScienceandTechnology,1(4):
InformationProcessingSystems(NeurIPS),2023. 045023,2020.
Bandeira,A.S.,Maillard,A.,Nickl,R.,andWang,S. On Grathwohl,W.,Chen,R.T.,Bettencourt,J.,Sutskever,I.,
freeenergybarriersinGaussianpriorsandfailureofcold andDuvenaud,D. FFJORD:Free-formcontinuousdy-
startMCMCforhigh-dimensionalunimodaldistributions. namicsforscalablereversiblegenerativemodels. Interna-
Philosophicaltransactions.SeriesA,Mathematical,phys- tionalConferenceonLearningRepresentations(ICLR),
ical,andengineeringsciences,381,2022. 2019.
Bengio,E.,Jain,M.,Korablyov,M.,Precup,D.,andBen-
Grenander,U.andMiller,M.I. Representationsofknowl-
gio,Y. Flownetworkbasedgenerativemodelsfornon-
edgeincomplexsystems. JournaloftheRoyalStatisti-
iterativediversecandidategeneration.NeuralInformation
calSociety: SeriesB(Methodological),56(4):549â€“581,
ProcessingSystems(NeurIPS),2021.
1994.
Bengio,Y.,Lahlou,S.,Deleu,T.,Hu,E.J.,Tiwari,M.,and
Halton, J. H. Sequential Monte Carlo. In Mathematical
Bengio,E. GFlowNetfoundations. JournalofMachine
ProceedingsoftheCambridgePhilosophicalSociety,vol-
LearningResearch,24(210):1â€“55,2023.
ume58,pp.57â€“78.CambridgeUniversityPress,1962.
Berner, J., Richter, L., and Ullrich, K. An optimal con-
trolperspectiveondiffusion-basedgenerativemodeling. Harrison,J.,Willes,J.,andSnoek,J. VariationalBayesian
arXivpreprintarXiv:2211.01364,2022. lastlayers. InternationalConferenceonLearningRepre-
sentations(ICLR),2024.
Buchner, J. Nested sampling methods. arXiv preprint
arXiv:2101.09675,2021. HernaÂ´ndez-Lobato,J.M.andAdams,R. Probabilisticback-
propagationforscalablelearningofBayesianneuralnet-
Burda, Y., Grosse, R. B., and Salakhutdinov, R. Impor-
works. InternationalConferenceonMachineLearning
tanceweightedautoencoders. InternationalConference
(ICML),2015.
onLearningRepresentations(ICLR),2016.
Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
Chopin, N. A sequential particle filter method for static
bilisticmodels. NeuralInformationProcessingSystems
models. Biometrika,89(3):539â€“552,2002.
(NeurIPS),2020.
Cornish, R., Caterini, A., Deligiannidis, G., and Doucet,
A. Relaxing bijectivity constraints with continuously Hoffman,M.,Sountsov,P.,Dillon,J.V.,Langmore,I.,Tran,
indexednormalisingflows. InternationalConferenceon D., and Vasudevan, S. NeuTra-lizing bad geometry in
MachineLearning(ICML),2020. HamiltonianMonteCarlousingneuraltransport. arXiv
preprintarXiv:1903.03704,2019.
DeBortoli,V. Convergenceofdenoisingdiffusionmodels
underthemanifoldhypothesis. TransactionsonMachine Hoffman,M.D.,Blei,D.M.,Wang,C.,andPaisley,J.W.
LearningResearch(TMLR),2022. Stochastic variational inference. Journal of Machine
LearningResearch(JMLR),14:1303â€“1347,2013.
DelMoral,P.,Doucet,A.,andJasra,A. SequentialMonte
Carlosamplers. JournaloftheRoyalStatisticalSociety Hoffman,M.D.,Gelman,A.,etal.TheNo-U-Turnsampler:
SeriesB:StatisticalMethodology,68(3):411â€“436,2006. adaptively setting path lengths in Hamiltonian Monte
Carlo. JournalofMachineLearningResearch(JMLR),
Deleu, T., GoÂ´is, A., Emezue, C., Rankawat, M., Lacoste-
15(1):1593â€“1623,2014.
Julien,S.,Bauer,S.,andBengio,Y. Bayesianstructure
learningwithgenerativeflownetworks. Uncertaintyin
Holdijk, L., Du, Y., Hooft, F., Jaini, P., Ensing, B., and
ArtificialIntelligence(UAI),2022.
Welling,M. Stochasticoptimalcontrolforcollectivevari-
Dinh,L.,Sohl-Dickstein,J.,andBengio,S. Densityesti- ablefreesamplingofmoleculartransitionpaths. Neural
mation using Real NVP. International Conference on InformationProcessingSystems(NeurIPS),2023.
LearningRepresentations(ICLR),2017.
Hu, E. J., Malkin, N., Jain, M., Everett, K., Graikos, A.,
Duane,S.,Kennedy,A.,Pendleton,B.J.,andRoweth,D. and Bengio, Y. GFlowNet-EM for learning composi-
HybridMonteCarlo. PhysicsLettersB,195(2):216â€“222, tionallatentvariablemodels. InternationalConference
1987. onMachineLearning(ICML),2023.
10Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Hu,E.J.,Jain,M.,Elmoznino,E.,Kaddar,Y.,Lajoie,G., Malkin, N., Lahlou, S., Deleu, T., Ji, X., Hu, E., Everett,
Bengio,Y.,andMalkin,N. Amortizingintractableinfer- K., Zhang, D., and Bengio, Y. GFlowNets and varia-
enceinlargelanguagemodels. InternationalConference tionalinference. InternationalConferenceonLearning
onLearningRepresentations(ICLR),2024. Representations(ICLR),2023.
Izmailov,P.,Vikram,S.,Hoffman,M.D.,andWilson,A.G. MaÂ´teÂ´,B.andFleuret,F. Learninginterpolationsbetween
WhatareBayesianneuralnetworkposteriorsreallylike? Boltzmanndensities. TransactionsonMachineLearning
InternationalConferenceonMachineLearning(ICML), Research(TMLR),2023.
2021.
Nichol,A.andDhariwal,P. Improveddenoisingdiffusion
Jain,M.,Bengio,E.,Hernandez-Garcia,A.,Rector-Brooks, probabili1sticmodels. InternationalConferenceonMa-
J., Dossou, B.F., Ekbote, C.A., Fu, J., Zhang, T., Kil- chineLearning(ICML),2021.
gour,M.,Zhang,D.,etal. Biologicalsequencedesign
with gflownets. International Conference on Machine Nicoli, K. A., Nakajima, S., Strodthoff, N., Samek, W.,
Learning(ICML),2022. MuÂ¨ller,K.-R.,andKessel,P. Asymptoticallyunbiased
estimationofphysicalobservableswithneuralsamplers.
Jang,H.,Kim,M.,andAhn,S. Learningenergydecompo-
PhysicalReviewE,101(2):023304,2020.
sitionsforpartialinferenceofGFlowNets. International
ConferenceonLearningRepresentations(ICLR),2024. NoeÂ´,F.,Olsson,S.,KoÂ¨hler,J.,andWu,H.Boltzmanngener-
ators:Samplingequilibriumstatesofmany-bodysystems
Jing,B.,Corso,G.,Chang,J.,Barzilay,R.,andJaakkola,T.
withdeeplearning. Science,365(6457):eaaw1147,2019.
Torsionaldiffusionformolecularconformergeneration.
NeuralInformationProcessingSystems(NeurIPS),2022. Ã˜ksendal,B. StochasticDifferentialEquations: AnIntro-
ductionwithApplications. Springer,2003.
Kim, M., Ko, J., Zhang, D., Pan, L., Yun, T., Kim, W.,
Park, J., and Bengio, Y. Learning to scale logits for
Pan, L., Malkin, N., Zhang, D., and Bengio, Y. Better
temperature-conditional GFlowNets. arXiv preprint
trainingofGFlowNetswithlocalcreditandincomplete
arXiv:2310.02823,2023.
trajectories. InternationalConferenceonMachineLearn-
ing(ICML),2023.
Kim,M.,Yun,T.,Bengio,E.,Zhang,D.,Bengio,Y.,Ahn,
S.,andPark,J. LocalsearchGFlowNets. International
Pillai, N. S., Stuart, A. M., and ThieÂ´ry, A. H. Optimal
ConferenceonLearningRepresentations(ICLR),2024.
scalinganddiffusionlimitsforthelangevinalgorithmin
Kingma,D.P.andWelling,M. Auto-encodingvariational highdimensions. TheAnnalsofAppliedProbability,22
Bayes. InternationalConferenceonLearningRepresen- (6),December2012.
tations(ICLR),2014.
Ranganath, R., Gerrish, S., and Blei, D. Black box vari-
Lahlou, S., Deleu, T., Lemos, P., Zhang, D., Volokhova, ational inference. Artificial Intelligence and Statistics
A., HernaÂ´ndez-GarcÄ±a, A., Ezzine, L. N., Bengio, Y., (AISTATS),2014.
andMalkin,N. Atheoryofcontinuousgenerativeflow
Rector-Brooks,J.,Madan,K.,Jain,M.,Korablyov,M.,Liu,
networks.InternationalConferenceonMachineLearning
C.-H.,Chandar,S.,Malkin,N.,andBengio,Y.Thompson
(ICML),2023.
samplingforimprovedexplorationinGFlowNets. arXiv
Lemos,P.,Malkin,N.,Handley,W.,Bengio,Y.,Hezaveh,Y., preprintarXiv:2306.17693,2023.
andPerreault-Levasseur,L. Improvinggradient-guided
nestedsamplingforposteriorinference. arXivpreprint Rezende,D.andMohamed,S. Variationalinferencewith
arXiv:2312.03911,2023. normalizingflows. InternationalConferenceonMachine
Learning(ICML),2015.
Madan,K.,Rector-Brooks,J.,Korablyov,M.,Bengio,E.,
Jain,M.,Nica,A.,Bosc,T.,Bengio,Y.,andMalkin,N. Rezende, D. J., Mohamed, S., and Wierstra, D. Stochas-
LearningGFlowNetsfrompartialepisodesforimproved ticbackpropagationandapproximateinferenceindeep
convergenceandstability. InternationalConferenceon generativemodels. InternationalConferenceonMachine
MachineLearning(ICML),2022. Learning(ICML),2014.
Malkin, N., Jain, M., Bengio, E., Sun, C., and Bengio, Richter, L., Boustati, A., NuÂ¨sken, N., Ruiz, F. J. R., and
Y. Trajectory balance: Improved credit assignment OÂ¨merDenizAkyildiz. VarGrad: Alow-variancegradient
in gflownets. Neural Information Processing Systems estimatorforvariationalinference. NeuralInformation
(NeurIPS),2022. ProcessingSystems(NeurIPS),2020.
11Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Richter,L.,Berner,J.,andLiu,G.-H. Improvedsampling Tzen,B.andRaginsky,M. Theoreticalguaranteesforsam-
vialearneddiffusions. arXivpreprintarXiv:2307.01198, plingandinferenceingenerativemodelswithlatentdif-
2023. fusions. ConferenceonLearningTheory(CoLT),2019b.
Roberts,G.O.andRosenthal,J.S. Optimalscalingofdis- van Krieken, E., Thanapalasingam, T., Tomczak, J., van
creteapproximationstolangevindiffusions. Journalof Harmelen,F.,andtenTeije,A. A-NeSI:Ascalableap-
theRoyalStatisticalSociety:SeriesB(StatisticalMethod- proximatemethodforprobabilisticneurosymbolicinfer-
ology),60(1):255â€“268,1998. ence. NeuralInformationProcessingSystems(NeurIPS),
2023.
Roberts,G.O.andTweedie,R.L. Exponentialconvergence
ofLangevindistributionsandtheirdiscreteapproxima- Vargas,F.,Grathwohl,W.,andDoucet,A. Denoisingdif-
tions. Bernoulli,pp.341â€“363,1996. fusionsamplers. InternationalConferenceonLearning
Representations(ICLR),2023.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer,B. High-resolutionimagesynthesiswithlatent Vincent,P. Aconnectionbetweenscorematchingandde-
diffusionmodels. ConferenceonComputerVisionand noisingautoencoders. Neuralcomputation,23(7):1661â€“
PatternRecognition(CVPR),2021. 1674,2011.
SaÂ¨rkkaÂ¨, S. and Solin, A. Applied stochastic differential Wu, H., KoÂ¨hler, J., and NoeÂ´, F. Stochastic normalizing
equations. CambridgeUniversityPress,2019. flows. NeuralInformationProcessingSystems(NeurIPS),
2020.
Shen,M.W.,Bengio,E.,Hajiramezanali,E.,Loukas,A.,
Cho,K.,andBiancalani,T. Towardsunderstandingand Xiao, Z., Kreis, K., and Vahdat, A. Tackling the genera-
improvingGFlowNettraining. InternationalConference tive learning trilemma with denoising diffusion GANs.
onMachineLearning(ICML),2023. InternationalConferenceonLeraningRepresentations
(ICLR),2022.
Skilling,J. NestedsamplingforgeneralBayesiancompu-
tation. Bayesian Analysis, 1(4):833 â€“ 859, 2006. doi: Zhang,D.,Malkin,N.,Liu,Z.,Volokhova,A.,Courville,
10.1214/06-BA127. URL https://doi.org/10. A., and Bengio, Y. Generative flow networks for dis-
1214/06-BA127. creteprobabilisticmodeling. InternationalConference
onMachineLearning(ICML),2022.
Sohl-Dickstein,J.,Weiss,E.A.,Maheswaranathan,N.,and
Ganguli,S. Deepunsupervisedlearningusingnonequi- Zhang, D., Chen, R. T. Q., Malkin, N., and Bengio, Y.
libriumthermodynamics. InternationalConferenceon UnifyinggenerativemodelswithGFlowNetsandbeyond.
MachineLearning(ICML),2015. arXivpreprintarXiv:2209.02606,2023a.
Song,Y.,Durkan,C.,Murray,I.,andErmon,S. Maximum Zhang,D.,Rainone,C.,Peschl,M.,andBondesan,R. Ro-
likelihoodtrainingofscore-baseddiffusionmodels. Neu- bustschedulingwithGFlowNets. InternationalConfer-
ralInformationProcessingSystems(NeurIPS),2021a. enceonLearningRepresentations(ICLR),2023b.
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er- Zhang,D.,Chen,R.T.Q.,Liu,C.-H.,Courville,A.,and
mon,S.,andPoole,B. Score-basedgenerativemodeling Bengio,Y. Diffusiongenerativeflowsamplers: Improv-
throughstochasticdifferentialequations. International inglearningsignalsthroughpartialtrajectoryoptimiza-
ConferenceonLearningRepresentations(ICLR),2021b. tion. InternationalConferenceonLearningRepresenta-
tions(ICLR),2024.
Tiapkin,D.,Morozov,N.,Naumov,A.,andVetrov,D. Gen-
erativeflownetworksasentropy-regularizedRL. arXiv Zhang,Q.andChen,Y. Diffusionnormalizingflow. Neural
preprintarXiv:2310.12934,2023. InformationProcessingSystems(NeurIPS),2021.
Tripp, A., Daxberger, E., and HernaÂ´ndez-Lobato, J. M. Zhang,Q.andChen,Y. Pathintegralsampler: astochastic
Sample-efficientoptimizationinthelatentspaceofdeep controlapproachforsampling. InternationalConference
generativemodelsviaweightedretraining. NeuralInfor- onLearningRepresentations(ICLR),2022.
mationProcessingSystems(NeurIPS),2020.
Zhu, Y., Wu, J., Hu, C., Yan, J., Hsieh, C.-Y., Hou, T.,
Tzen, B.andRaginsky, M. Neuralstochasticdifferential andWu, J. Sample-efficientmulti-objectivemolecular
equations: DeeplatentGaussianmodelsinthediffusion optimizationwithGFlowNets. NeuralInformationPro-
limit. arXivpreprintarXiv:1905.09883,2019a. cessingSystems(NeurIPS),2023.
12Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
Zimmermann, H., Lindsten, F., van de Meent, J.-W., and
Naesseth, C. A. A variational perspective on genera-
tiveflownetworks. TransactionsonMachineLearning
Research(TMLR),2023.
13Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
A.Codeandhyperparameters
Code is available https://github.com/GFNOrg/gfn-diffusion and will continue to be maintained and ex-
tended.
Below are commands to reproduce some of the results on Manywell and VAE with PIS and GFlowNet models as an
example,showingthehyperparameters:
PIS:
--mode_fwd pis --lr_policy 1e-3
PIS+Langevin:
--mode_fwd pis --lr_policy 1e-3 --langevin
GFlowNetTB:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd tb --lr_policy 1e-3 --lr_flow 1e-1
GFlowNetTB+Expl.:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd tb --lr_policy 1e-3 --lr_flow 1e-1
--exploratory --exploration_wd --exploration_factor 0.2
GFlowNetVarGrad+Expl.:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd tb-avg --lr_policy 1e-3 --lr_flow 1e-1
--exploratory --exploration_wd --exploration_factor 0.2
GFlowNetFL-SubTB:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd subtb --lr_policy 1e-3 --lr_flow 1e-2
--partial_energy --conditional_flow_model
GFlowNetTB+Expl. +LS:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd tb --lr_policy 1e-3 --lr_back 1e-3 --lr_flow 1e-1
--exploratory --exploration_wd --exploration_factor 0.1
--both_ways --local_search
--buffer_size 600000 --prioritized rank --rank_weight 0.01
--ld_step 0.1 --ld_schedule --target_acceptance_rate 0.574
GFlowNetTB+Expl. +LP:
python train.py
--t_scale 1. --energy many_well --pis_architectures --zero_init --clipping
--mode_fwd tb --lr_policy 1e-3 --lr_flow 1e-1
--exploratory --exploration_wd --exploration_factor 0.2
--langevin --epochs 10000
14Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
GFlowNetTB+Expl. +LS(VAE):
python train.py
--energy vae --pis_architectures --zero_init --clipping
--mode_fwd cond-tb-avg --mode_bwd cond-tb-avg --repeats 5
--lr_policy 1e-3 --lr_flow 1e-1 --lr_back 1e-3
--exploratory --exploration_wd --exploration_factor 0.1
--both_ways --local_search
--max_iter_ls 500 --burn_in 200
--buffer_size 90000 --prioritized rank --rank_weight 0.01
--ld_step 0.001 --ld_schedule --target_acceptance_rate 0.574
GFlowNetTB+Expl. +LP+LS(VAE):
python train.py
--energy vae --pis_architectures --zero_init --clipping
--mode_fwd cond-tb-avg --mode_bwd cond-tb-avg --repeats 5
--lr_policy 1e-3 --lr_flow 1e-1
--lgv_clip 1e2 --gfn_clip 1e4 --epochs 10000
--exploratory --exploration_wd --exploration_factor 0.1
--both_ways --local_search
--lr_back 1e-3 --max_iter_ls 500 --burn_in 200
--buffer_size 90000 --prioritized rank --rank_weight 0.01
--langevin
--ld_step 0.001 --ld_schedule --target_acceptance_rate 0.574
15Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
B.Targetdensities
Gaussian Mixture Model with 25 modes (25GMM). The model, termed as 25GMM, consists of a two-dimensional
Gaussianmixturemodelwith25distinctmodes. Eachmodeexhibitsanidenticalvarianceof0.3. Thecentersofthesemodes
arestrategicallypositionedonagridformedbytheCartesianproduct{âˆ’10,âˆ’5,0,5,10}Ã—{âˆ’10,âˆ’5,0,5,10},effectively
distributingthemacrossthecoordinatespace.
Funnel(Hoffmanetal.,2019). Thefunnelrepresentsaclassicalbenchmarkinsamplingtechniques,characterizedbya
ten-dimensionaldistributiondefinedasfollows: Thefirstdimension,ğ‘¥ ,followsanormaldistributionwithmean0and
0
variance 9, denotedas ğ‘¥ âˆ¼ N(0,9). Conditionalonğ‘¥ , theremainingdimensions, ğ‘¥ , aredistributedaccordingtoa
0 0 1:9
multivariatenormaldistributionwithmeanvector0andacovariancematrixexp(ğ‘¥ )I,whereIistheidentitymatrix. Thisis
0
succinctlyrepresentedasğ‘¥ | ğ‘¥ âˆ¼N (0,exp(ğ‘¥ )I).
1:9 0 0
Manywell(NoeÂ´ etal.,2019). Themanywellischaracterizedbya32-dimensionaldistribution,whichisconstructedasthe
productof16identicaltwo-dimensionaldoublewelldistributions. Eachofthesetwo-dimensionalcomponentsisdefinedby
apotentialfunction,ğœ‡(ğ‘¥ ,ğ‘¥ ),expressedasğœ‡(ğ‘¥ ,ğ‘¥ ) =exp(cid:0) âˆ’ğ‘¥4+6ğ‘¥2+0.5ğ‘¥ âˆ’0.5ğ‘¥2(cid:1) .
1 2 1 2 1 1 1 2
VAE(Kingma&Welling,2014). Thistaskinvolvessamplingfroma20-dimensionallatentposterior ğ‘(ğ‘§|ğ‘¥) âˆ ğ‘(ğ‘§)ğ‘(ğ‘¥|ğ‘§),
where ğ‘(ğ‘§)isafixedpriorand ğ‘(ğ‘¥|ğ‘§)isapretrainedVAEdecoder,usingaconditionalsamplerğ‘(ğ‘§|ğ‘¥)dependentoninput
data(image)ğ‘¥.
C.Experimentdetails
Samplingenergies. Inthissection,wedetailthehyperparametersusedforourexperiments. Animportantparameteris
thediffusioncoefficientoftheforwardpolicy,whichisdenotedbyğœandalsousedinthedefinitionofthefixedbackward
process. Thebasediffusionrateğœissetto5for25GMMand1forFunnelandManywell,consistentwithpastwork.
Forallourexperiments,weusedalearningrateof10âˆ’3. Additionally,weusedahigherlearningrateforlearningtheflow
parameterization,whichissetas10âˆ’1whenusingtheTBlossand10âˆ’2withtheSubTBloss. Thesesettingswerefoundto
beconsistentlystable(unlikethosewithhigherlearningrates)andconvergewithintheallottednumberofsteps(unlike
thosewithlowerlearningrates).
For the SubTB loss, we experimented with the settings of 10Ã— lower learning rates for both flow and policy models
communicatedbytheauthorsofZhangetal.(2024),butfoundtheresultstobeinferiorbothusingtheirpublishedcode(and
otherunstatedhyperparameterscommunicatedbytheauthors)andusingourreimplementation.
Formodelswithexploration,weuseanexplorationfactorof0.2(thatis,noisewithavarianceof0.2isaddedtothepolicy
whensamplingtrajectoriesfortraining),whichdecayslinearlyoverthefirsthalfoftraining,consistentwithLahlouetal.
(2023).
Wetrainallourmodelsfor25,000iterationsexceptthoseusingLangevindynamics,whicharetrainedfor10,000iterations.
Thisresultsinapproximatelyequalcomputationtimeowingtotheoverheadfromcomputationofthescoreateachsampling
step.
WeusethesameneuralnetworkarchitecturefortheGFlowNetasoneofourbaselines(Zhang&Chen,2022). Similarto
Zhang&Chen(2022),wealsouseaninitializationschemewithlast-layerweightssetto0atthestartoftraining. Since
theSubTBrequirestheflowfunctiontobeconditionedonthecurrentstatexğ‘¡ andtimeğ‘¡,wefollowZhangetal.(2024)
andparametrizetheflowmodelwiththesamearchitectureastheLangevinscalingmodelNN inZhang&Chen(2022).
2
Additionally,weperformclippingontheoutputofthenetworkaswellasthescoreobtainedfromtheenergyfunction,
typicallysettingtheclippingparameterofLangevinscalingmodelto102andpolicynetworkto104,similarlytoVargas
etal.(2023):
(cid:16) (cid:17)
ğ‘“ ğœƒ(ğ‘˜,ğ‘¥)=clip NN 1(ğ‘˜,ğ‘¥;ğœƒ)+NN 2(ğ‘˜;ğœƒ) âŠ™clip(cid:0) âˆ‡lnğœ‹(ğ‘¥),âˆ’102,102(cid:1),âˆ’104,104 . (16)
Allmodelsweretrainedwithabatchsizeof300.
VAEexperiment. IntheVAEexperiment, weusedastandardVAEmodelpretrainedfor100epochsontheMNIST
dataset. Theencoderğ‘(ğ‘§|ğ‘¥) containsaninputlinearlayer(784neurons)followedbyhiddenlinearlayer(400neurons),
16Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
ReLUactivationfunction,andtwolinearheads(20neuronseach)whoseoutputswerereparametrizedtobemeansand
scalesofmultivariateNormaldistribution. Thedecoderconsistsof20-dimensionalinput,onehiddenlayer(400neurons),
followedbytheReLUactivation,and784-dimensionaloutput. Theoutputisprocessedbythesigmoidfunctiontobescaled
properlyinto [0,1].
Thegoalistosampleconditionallyonğ‘¥thelatentğ‘§fromtheunnormalizeddensity ğ‘(ğ‘§,ğ‘¥) = ğ‘(ğ‘§)ğ‘(ğ‘¥ | ğ‘§)(where ğ‘(ğ‘§)is
thepriorand ğ‘(ğ‘¥|ğ‘§)isthelikelihoodcomputedfromthedecoder),whichisproportionaltotheposterior ğ‘(ğ‘§ | ğ‘¥). Wereuse
themodelarchitecturesfromtheunconditionalsamplingexperiments,butalsoprovideğ‘¥asaninputtothefirstlayerofthe
modelsexpressingthepolicydrift(aswellastheflow,forFL-SubTB)andaddonehiddenlayertoprocesshigh-dimensional
conditions. FormodelstrainedwithTB,logğ‘ ğœƒ alsobecomesaMLPtakingğ‘¥asinput.
TheVarGradandLStechniquesrequireadaptationsintheconditionalsetting. ForLS,buffers(D andD )muststore
buffer LS
theassociatedconditionsğ‘¥togetherwiththesamplesğ‘§andthecorrespondingunnormalizeddensityğ‘…(ğ‘§;ğ‘¥),i.e.,atupleof
(ğ‘¥,ğ‘§,ğ‘…(ğ‘§;ğ‘¥)). ForVarGrad,becausethepartitionfunctiondependsontheconditioninginformationğ‘¥,itisnecessaryto
computevarianceovermanytrajectoriessharingthesamecondition. Wechoosetosample10trajectoriesforeachcondition
occurringinaminibatchandcomputetheVarGradlossforeachsuchsetof10trajectories.
TheVAEmodelwastrainedontheentireMNISTtrainingsetandneverupdatedonthetestpartofMNIST.Inorderto
evaluatesamplers(withrespecttothevariationallowerbound)onauniquesetofexamples,wechosethefirst100elements
ofMNISTtestdata. AllofthesamplersweretrainedhavingaccesstotheMNISTtrainingdataandthefrozenVAEdecoder.
Forafaircomparison,samplersutilizingtheLPweretrainedfor10,000,whereastheremainingfor25,000iterations. In
eachiteration,abatchof300examplesfromMNISTwasgivenasconditions.
(a)Conditioningdata(MNISTtestset) (b)VarGrad+Expl.+LPsamplesdecoded (c)VAEreconstruction
FigureC.1.Oursampler(VarGrad+Expl.+LP)isconditionedbyasubsetofnever-seendatacomingfromthegroundtruthdistribution
(left).TheconditionalsampleswerethendecodedbythethefixedVAE(middle).Forthecomparison,weshowthereconstructionofthe
realdatabyVAE(right).Weobservedthatthedecodedsamplesarevisuallyverysimilartothereconstructionsmakingthesetwopictures
almostindistinguishable.Both,decodedsamplesandreconstruction,aremoreblurrythanthegroundtruthdata,whichiscausedbya
limitedcapacityoftheVAEâ€™slatentspace.
17Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
D.Localsearch-guidedGFlowNet
Prioritizedsamplingscheme. Wecanuseuniformorprioritizedsamplingtodrawsamplesfromthebufferfortraining.
Wefoundprioritizedsamplingtoworkslightlybetterinourexperiments(seeablationstudyinÂ§D.2),althoughthechoice
shouldbeinvestigatedmorethoroughlyinfuturework.
Weuserank-basedprioritization(Trippetal.,2020),whichfollowsaprobabilisticapproachdefinedas:
ğ‘(x;D ) âˆ (cid:0)ğ‘˜|D |+rank (x)(cid:1)âˆ’1, (17)
buffer buffer D buffer
whererank (x)representstherelativerankofasampleğ‘¥basedonarankingfunctionğ‘…(x)(inourcase,theunnormalized
D
buffer
targetdensityatsamplex). Theparameterğ‘˜ isahyperparameterforprioritization,wherealowervalueofğ‘˜ assignsahigher
probabilitytosampleswithhigherranks,therebyintroducingamoregreedyselectionapproach. Wesetğ‘˜ =0.01forevery
task. Giventhatthesamplingisproportionaltothesizeof D ,weimposeaconstraintonthemaximumsizeofthe
buffer
buffer: |D | =600,000withfirst-infirstout(FIFO)datastructureforeverytask,exceptweuse|D | =90,000for
buffer buffer
VAEtask. Seethealgorithmbelowforadetailedpseudocode.
Algorithm1GFlowNetTrainingwithLocalsearch
1: Initializepolicyparametersğœƒforğ‘ƒ ğ¹,andemptybuffersD buffer,D
LS
2: forğ‘–=1,2,...,ğ¼do
3: ifğ‘–%2==0then
4: Sampleğ‘€trajectories{ğœ 1,...,ğœ ğ‘€}âˆ¼ğ‘ƒ ğ¹(Â·|ğœ–-greedy)
5: UpdateD â†D âˆª{ğ‘¥|ğœâ†’ğ‘¥}
buffer buffer
6: Minimizeğ¿(ğœ;ğœƒ)using{ğœ 1,...,ğœ ğ‘€}toupdateğ‘ƒ ğ¹
7: else
8: ifğ‘–%100==0then
9: Sample{ğ‘¥ 1,...,ğ‘¥ ğ‘€}âˆ¼D
buffer
10: D LSâ†LocalSearch({ğ‘¥ 1,...,ğ‘¥ ğ‘€};D LS)
11: endif
12: Sample{ğ‘¥â€²,...,ğ‘¥â€² }âˆ¼ ğ‘ (Â·Â·Â· ;D )
1 ğ‘€ buffer LS
13: Sample{ğœ 1â€²,...,ğœ ğ‘€â€² }âˆ¼ğ‘ƒ ğµ(Â·Â·Â·|ğ‘¥â€²)
14: Minimizeğ¿(ğœâ€²;ğœƒ)using{ğœ 1â€²,...,ğœ ğ‘€â€² }toupdateğ‘ƒ ğ¹
15: endif
16: endfor
Weusethenumberoftotaliterationsğ¼ =25,000foreverytaskasdefault. NoteaslocalsearchisperformedtoupdateD
LS
occasionallythatper100iterations,thenumberoflocalsearchupdatesisdone25,000/100=250.
18Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
D.1.Localsearchalgorithm
This section describes a detailed algorithm for local search, which provides an updated buffer D , which contains
LS
low-energysamples.
Dynamicadjustmentofstepsizeğœ‚. ToenhancelocalsearchusingparallelMALA,wedynamicallyselecttheLangevin
stepsize(ğœ‚),whichgovernstheMHacceptancerate. Ourobjectiveistoattainanaverageacceptancerateof0.574,whichis
theoreticallyoptimalforhigh-dimensionalMALAâ€™sefficiency(Pillaietal.,2012). Whiletheusercancustomizethetarget
acceptancerate,theadaptiveapproacheliminatestheneedformanualtuning.
Computationalcostoflocalsearch. Thecomputationalcostoflocalsearchisnotsignificant. Localsearchforiteration
ofğ¾ =200requires6.04seconds(averagedwithfivetrialsinManywell),whereweonlyoccasionally(every100iterations)
updateD withMALA.ThespeedisevaluatedusingthecomputationalresourcesoftheIntelXeonScalableGold6338
LS
CPU(2.00GHz)andtheNVIDIARTX4090GPU.
Algorithm2Localsearch(ParallelMALA)
input Initialstates {ğ‘¥(0),...,ğ‘¥(0) }, currentbuffer D , totalsteps ğ¾, burninsteps ğ¾ , initialstepsizeğœ‚ , amplifyingfactor
1 ğ‘€ LS burn-in 0
ğ‘“ ,dampingfactor ğ‘“ ,unnormalizedtargetdensityğ‘…
increase decrease
output UpdatedbufferD
LS
Initializeacceptancecounterğ‘=0
Setğœ‚â†ğœ‚
0
forğ‘˜ =1:ğ¾do
Initializestepacceptancecountğ‘ ğ‘˜ =0
forğ‘š=1:ğ‘€do
Sampleğœâˆ¼N(0,ğ¼)
Proposeğ‘¥ ğ‘šâˆ— â†ğ‘¥ ğ‘š(ğ‘˜âˆ’1) +ğœ‚âˆ‡logğ‘…(ğ‘¥ ğ‘š(ğ‘˜âˆ’1) )+âˆšï¸ 2ğœ‚ğœ
Computeacceptanceratioğ‘Ÿ
â†min(cid:32)
1,
ğ‘…(ğ‘¥ ğ‘šâˆ— )exp(cid:16) âˆ’ 41 ğœ‚âˆ¥ğ‘¥ ğ‘š(ğ‘˜âˆ’1)âˆ’ğ‘¥ ğ‘šâˆ— âˆ’ğœ‚âˆ‡logğ‘…(ğ‘¥ ğ‘šâˆ— )âˆ¥2(cid:17) (cid:33)
ğ‘…(ğ‘¥ ğ‘š(ğ‘˜âˆ’1))exp(cid:16) âˆ’ 41 ğœ‚âˆ¥ğ‘¥ ğ‘šâˆ— âˆ’ğ‘¥ ğ‘š(ğ‘˜âˆ’1)âˆ’ğœ‚âˆ‡logğ‘…(ğ‘¥ ğ‘š(ğ‘˜âˆ’1))âˆ¥2(cid:17)
Withprobabilityğ‘Ÿ,accepttheproposal:ğ‘¥ ğ‘š(ğ‘˜) â†ğ‘¥ ğ‘šâˆ— andincrementğ‘ ğ‘˜ â†ğ‘ ğ‘˜+1
ifğ‘˜ >ğ¾ then
burn-in
Updatebuffer:D LSâ†D LSâˆª{ğ‘¥ ğ‘šâˆ— }
endif
endfor
Computestepacceptancerateğ›¼
ğ‘˜
=ğ‘ ğ‘˜/ğ‘€
ifğ›¼ ğ‘˜ >ğ›¼ targetthen
ğœ‚â†ğœ‚Ã— ğ‘“
increase
elseifğ›¼ ğ‘˜ <ğ›¼ targetthen
ğœ‚â†ğœ‚Ã— ğ‘“
decrease
endif
endfor
Weadoptdefaultparameters: ğ‘“ =1.1, ğ‘“ =0.9,ğœ‚ =0.01,ğ¾ =200,ğ¾ =100,andğ›¼ =0.574forthree
increase decrease 0 burn-in target
unconditionaltasks. ForconditionaltasksofVAE,wegivemoreiterationsoflocalsearch: ğ¾ =500,ğ¾ =200.
burn-in
Itisnoteworthythatbyadjustingtheinversetemperature ğ›½into ğ‘…ğ›½ duringthecomputationoftheMetropolis-Hastings
acceptanceratioğ‘Ÿ,wecanfacilitateagreedierlocalsearchstrategyaimedatexploringsampleswithlowerenergy(i.e.,
higherdensity ğ‘ ). Thisapproachprovesadvantageousfornavigatinghigh-dimensionalandsteeplandscapes,whichare
target
typicallychallengingforlocatinglow-energysamples. Forunconditionaltasks,weset ğ›½=1.
InthecontextoftheVAEtask(Table2),weutilizetwoGFlowNetlossfunctions: TBandVarGrad. Forlocalsearchwithin
TB,weset ğ›½ = 1,whileforVarGrad,weemploy ğ›½ = 5. AsillustratedinTable2,employingalocalsearchwith ğ›½ = 1
failstoenhancetheperformanceoftheTBmethod. Conversely,alocalsearchwith ğ›½=5resultsinimprovementsatthe
logğ‘Ë†RW metricovertheVarGrad+Expl. +LP,eventhoughtheperformanceofVarGrad+Expl. +LPsurpassesthatof
TBsubstantially. Thisunderscorestheimportanceofselectinganappropriate ğ›½value,whichiscriticalforoptimizingthe
exploration-exploitationbalancedependingonthetargetobjectives.
19Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
D.2.Ablationstudyforlocalsearch-guidedGFlowNets
Increasingcapacityofbuffer. Thecapacityofthereplaybufferinfluencesthedurationforwhichitretainspastexperi-
ences,enablingittoreplaytheseexperiencestothepolicy.Thismechanismhelpsinpreventingmodecollapseduringtraining.
TableD.1demonstratesthatenhancingthebufferâ€™scapacityleadstoimprovedsamplingquality. Furthermore,Figure1illus-
tratesthatincreasingthebufferâ€™scapacityâ€”therebyencouragingthemodeltorecallpastlow-energyexperiencesâ€”enhances
itsmode-seekingcapability.
TableD.1.ComparisonofthesamplingqualityofeachsamplertrainedwithvaryingreplaybuffercapacitiesinManywell.Fiveindependent
runshavebeenconducted,withboththemeanandstandarddeviationreported.
BufferCapacityâ†“Metricâ†’ Î”logğ‘ Î”logğ‘RW W2
2
30,000 4.41Â±0.10 2.73Â±0.15 6.17Â±0.02
60,000 4.06Â±0.05 2.38Â±0.38 6.14Â±0.04
600,000 4.57Â±2.13 0.19Â±0.29 5.66Â±0.05
(a)Capacity30,000 (b)Capacity60,000 (c)Capacity600,000
FigureD.1.Illustrationofeachsamplertrainedwithvaryingcapacitiesofreplaybuffers,depicting2,000samples.Asthecapacityofthe
bufferincreases,thenumberofmodescapturedbythesampleralsoincreases.
Benefitofprioritization. Rank-prioritizedsamplinggivesfasterconvergencecomparedwithnoprioritization(uniform
sampling),asshowninFig.D.2a.
Dynamic adjustment of ğœ‚ vs. fixed ğœ‚ = 0.01. As shown in Fig. D.2b, dynamic adjustment to target acceptance rate
ğ›¼ =0.574givesbetterperformancesthanfixedLangevinstepsizeofğœ‚showcasingtheeffectivenessofthedynamic
target
adjustment.
20Ondiffusionmodelsforamortizedinference:Benchmarkingandimprovingstochasticcontrolandsampling
165.00
165 No scheudling
164.75 Scheduling
160
164.50
155
164.25 150
164.00
145
163.75
140
163.50
135
No prioritization 163.25
130 Prioritization
163.00
0 5000 10000 15000 20000 25000 0 5000 10000 15000 20000 25000
Plot Plot
(a)Benefitofprioritizationinreplaybuffersampling.. (b)Benefitofschedulingğœ‚dynamically.
FigureD.2.Ablationstudyforprioritizedreplaybufferandstepsizeğœ‚schedulingoflocalsearch.Meanandstandarddeviationareplotted
basedonfiveindependentruns.
21
WRZgol WRZgol