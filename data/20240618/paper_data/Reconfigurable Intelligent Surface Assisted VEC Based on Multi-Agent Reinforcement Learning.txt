1
Reconfigurable Intelligent Surface Assisted VEC
Based on Multi-Agent Reinforcement Learning
Kangwei Qi, Qiong Wu, Senior Member, IEEE, Pingyi Fan, Senior Member, IEEE,
Nan Cheng, Senior Member, IEEE, Qiang Fan, Jiangzhou Wang, Fellow, IEEE
Abstract‚ÄîVehicular edge computing (VEC) is an emerging munication quality by provisioning an additional communica-
technology that enables vehicles to perform high-intensity tasks tion link to VU by adjusting phase-shifts [4]. Therefore, the
by executing tasks locally or offloading them to nearby edge
VEC system with RIS deserves extensive research. However,
devices. However, obstacles such as buildings may degrade the
duetolimitedhardware,thephase-shiftoftheRIScanonlybe
communications and incur communication interruptions, and
thusthevehiclemaynotmeettherequirementfortaskoffloading. selected from a limited number of values [5]. In addition, the
Reconfigurableintelligentsurfaces(RIS)isintroducedtosupport vehicle mobility and uncertain environment pose significant
vehicle communication and provide an alternative communica- challenges. Therefore, the joint RIS phase-shift and power
tion path. The system performance can be improved by flexibly
allocation optimization is a difficult problem to handle. For
adjustingthephase-shiftoftheRIS.ForRIS-assistedVECsystem
the optimization problem of RIS phase-shift, in [6], He et al. where tasks arrive randomly, we design a control scheme that
considersoffloadingpower,localpowerallocationandphase-shift employedalgorithmssuchasblockcoordinatedescent(BCD),
optimization. To solve this non-convex problem, we propose a alternating optimization (AO), and semi definite relaxation
newdeepreinforcementlearning(DRL)frameworkthatemploys (SDR) to solve it. In addition, for the VEC power allocation
modified multi-agent deep deterministic policy gradient (MAD-
problem, deep reinforcement learning (DRL) algorithms are
DPG)approachtooptimizethepowerallocationforvehicleusers
considered as an effective solution. In [7], Zhu et al. used
(VUs)andblockcoordinatedescent(BCD)algorithmtooptimize
the phase-shift of the RIS. Simulation results show that our the deep deterministic policy gradient (DDPG) algorithm to
proposedschemeoutperformsthecentralizeddeepdeterministic allocate the offloading power and local power of a single
policy gradient (DDPG) scheme and random scheme. VU, thereby achieving the minimal total power and buffer
Index Terms‚ÄîReconfigurable intelligent surface (RIS), vehic- length. However, in scenarios with multiple vehicle users, the
ular edge computing (VEC), multi-agents deep reinforcement centralized algorithms do not have advantages any more [8].
learning (MA-DRL). In this letter, we investigate the RIS assisted VEC problem
and propose a multi-agent DRL scheme with joint BCD1. To
I. INTRODUCTION simplifytheproblem,wedecomposeitintotwosubproblems:
VEHICLE edge computing (VEC) is considered to be VEC power allocation problem and RIS phase-shift matrix
optimization problem. To address it, we first use the BCD
a promising technology in supporting vehicle real-time
algorithm to optimize the RIS phase-shift matrix, and then
computing.ItcanoffloadtaskstoVECserverswithrichcom-
use the modified multi-agent DDPG (MADDPG) algorithm
putationandstorageresourceswhenthelocalCPUcapacityof
to solve the power allocation problem for vehicle offloading
thevehicleislimited[1],[2].However,insomescenarios,the
and local execution. The extensive simulations demonstrate
vehicle user (VU) is impacted by blocking obstacles such as
that the proposed scheme is superior to the centralized DDPG
buildings, making it unable to communicate with base station
algorithms in terms of rewards, convergence speed, stability,
(BS) in a timely manner within a certain distance [3], so that
and other aspects.
it cannot offload tasks in time.
Recently, reconfigurable intelligent surface (RIS) technol-
ogy has shown significant advantages in enhancing the com-
II. SYSTEMMODELANDPROBLEMFORMULATION
A. Scenario
ThisworkwassupportedinpartbytheNationalNaturalScienceFoundation
As shown in Fig. 1, we consider a RIS-assisted VEC
of China under Grant No. 61701197, in part by the National Key Research
andDevelopmentProgramofChinaunderGrantNo.2021YFA1000500(4),in network with multiple users, where the BS has M antennas
partbythe111ProjectunderGrantNo.B12018. and is associated with a VEC server, and the VU has a single
Kangwei Qi, Qiong Wu are with the School of Internet of Things
antenna. Since a VU has limited computational resources, it
Engineering, Jiangnan University, Wuxi 214122, China. (e-mail: kang-
weiqi@stu.jiangnan.edu.cn,qiongwu@jiangnan.edu.cn). canoffloadsometaskstotheVECserverforprocessing.When
Pingyi Fan is with the Department of Electronic Engineering, Beijing theVU‚Äôslinkisobstructed,itcannotcommunicatewiththeBS
NationalResearchCenterforInformationScienceandTechnology,Tsinghua
directlyandefficiently,whereaRISwithN reflectiveelements
University,Beijing100084,China(e-mail:fpy@tsinghua.edu.cn).
Nan Cheng is with the State Key Lab. of ISN and School of Telecom- can assist the VU in offloading tasks to the edge devices (we
munications Engineering, Xidian University, Xi‚Äôan 710071, China (e-mail: only consider one link, the VU communicating with the BS
dr.nan.cheng@ieee.org).
viatheRIS).Here,weconsideranurbanintersectionscenario
Qiang Fan is with Qualcomm, San Jose, CA 95110, USA (e-mail:
qf9898@gmail.com).
Jiangzhou Wang is with the School of Engineering, University of Kent, 1Thesourcecodehasbeenreleasedat:https://github.com/qiongwu86/RIS-
CT27NTCanterbury,U.K.(email:j.z.wang@kent.ac.uk). VEC-MARL.git
4202
nuJ
71
]AM.sc[
1v81311.6042:viXra2
where œÅ is the path loss at reference distance d = 1m,
0
CCoommmmuunniiccaattiioonn lliinnkkss BBlloocckkiinngg lliinnkkss d is the geometric distance from the RIS to the BS, Œ±
r,b r,b
is the path loss exponent of the RIS-BS link, and R is the
Rician coefficient associated with small-scale fading. LoS
components hLoS is defined as follows:
r,b
hL r,o bS =[1,e‚àíj2 ŒªœÄdrsin(Œ∏r,b),¬∑¬∑¬∑ ,e‚àíj2 ŒªœÄ(N‚àí1)drsin(Œ∏r,b)]T,
(4)
where Œª is the carrier length, d is the interval between RIS
r
elements, and Œ∏ is the departure angle of the signal from
r,b
the RIS to the BS. Similarly, the channel gain ht from the
k,r
kth VU to the RIS at time slot t is defined as
Fig.1. RISaidedvehicularedgecomputing (cid:114) (cid:114)
ht =
œÅ(cid:16)
dt
(cid:17)‚àíŒ±k,r R
htLoS,‚àÄk ‚ààK,‚àÄt‚ààT.
k,r k,r 1+R k,r
(5)
whereVUsarerandomlydistributedattheintersection,where
Here, dt is the geometric distance between the kth vehicle
there are K VUs, denoted by the set K = {1,2,¬∑¬∑¬∑ ,K}. k,r
and the RIS at time slot t, and Œ± is the path loss exponent
The VU has the flexibility to offload the tasks for processing k,r
betweenthevehicleandtheRIS.NotethathtLoS isexpressed
and local execution, and additionally, we divide the task k,r
as follows
processing into T equally time slots ‚àÜt of discrete time,
denotedbyT ={1,2,¬∑¬∑¬∑ ,T}.Ateachtimeslott,thevehicle htLoS =[1,e‚àíj2 ŒªœÄdrsin(Œ∏ kt ,r) ,¬∑¬∑¬∑ ,e‚àíj2 ŒªœÄ(N‚àí1)drsin(Œ∏ kt ,r) ]T,
randomlygeneratesakindoftasks,wheretaskarrivalsfollow k,r
(6)
a Poisson distribution with an arrival rate of Œ∑. Note that we where Œ∏t is the arrival angle of the signal from the kth
k,r
consider quasi-static scenarios where the channel conditions
vehicle to the RIS at time slot t.
keep constant within a time slot but may change between the
In this letter, we consider that the communication link
different time slots.
between the vehicle and the BS is completely blocked, and
the VU can only communicate through the RIS. Thus, we can
B. Queue Model obtainthesignal-to-noiseratio(SNR)betweenthekthVUand
the BS through the RIS at time slot t as
For VU k, we assume that its task arrival rate at time slot
t is Œ∑ (t), which follows a Poisson distribution, so we can (cid:12) (cid:12)2
k Pt (cid:12)(h )HŒòtht (cid:12)
calculate the tasks of VU k during time slot t as k,o(cid:12) r,b k,r(cid:12)
Œ≥ (t)= ,‚àÄk ‚ààK,‚àÄt‚ààT, (7)
k œÉ2
a (t)=Œ∑ (t)√ó‚àÜt. (1)
k k
where Pt ‚àà [0,P ] is the offloading power of
k,o max,o
The arriving tasks will be saved in the buffer, and processed kth VU at time slot t, and œÉ2 is thermal noise
at time slot t+1, thus we can get the buffer length of VU k power. The diagonal phase-shift matrix of RIS is Œòt =
at time slot t+1 is diag[Œ≤ 1ejŒ∏ 1t,¬∑¬∑¬∑ ,Œ≤ nejŒ∏ nt,¬∑¬∑¬∑ ,Œ≤ NejŒ∏ Nt ],‚àÄn ‚àà [1,N], Œ≤
n
‚àà
q (t+1)=[q (t)‚àíq (t)‚àíq (t)]++a (t), (2) [0,1]. Due to hardware constraint, phase-shift can only be
k k k,o k,l k selected from a finite discrete value set Œ∏t ‚àà Œ¶ =
(cid:110) (cid:111) n
whereq k,o(t)andq k,l(t)denotetheamountoftasksprocessed 0,2œÄ,¬∑¬∑¬∑ ,2œÄ(2b‚àí1) , where b controls the degree of phase-
by task offloading and locally, respectively. Note that [x]+ = shift2 db iscrete deg2b ree.
max(0,x).
When VU k chooses to offload tasks to the VEC server
associated with the BS, based on the formula (7), we can
C. Offloading Model obtain the number of offloaded tasks processed by VU k at
time slot t as
In our proposed model, it is assumed that the location
of the kth vehicle is (xt k,y kt,z kt) at time slot t, and the q k,o(t)=‚àÜt√óWlog 2(1+Œ≥ k(t)), (8)
coordinates of the BS and the RIS are (BS ,BS ,BS ) and
x y z
(RIS ,RIS ,RIS ),respectively.WhilethelinkbetweenBS where W represents the channel bandwidth.
x y z
and RIS is line-of-sight(LoS), the link between RIS and the
vehicles is the same. Therefore, these communication links
D. Local Execution
undergo small-scale fading, which is modeled as the Rician
fadingwiththepureLoScomponent[26],[27].Sinceboththe When VU k selects to process tasks locally, we can obtain
RIS and the BS are deployed at a fixed location, the RIS-BS the size of tasks that can be processed locally at time slot t as
link will remain static. Therefore, we can obtain the channel
q (t)=‚àÜtf (t)/L, (9)
gain h between the RIS and BS as k,l k
r,b
(cid:113) (cid:114) R where L is denoted as the CPU frequency required to process
h r,b = œÅ(d r,b)‚àíŒ±r,b 1+RhL r,o bS. (3) one bit of task, f k(t) ‚àà [0,F max] is the CPU frequency3
scheduled by utilizing DVFS technology to adjust the chip Algorithm 1: BCD to Optimize Phase-Shift Matrix
voltage, i.e., Inputs: Œ¶
(cid:113)
f (t)= 3 p (t)/c, (10) Outputs: Œ∏t
k k,l n
for n=1,...,N do
wherep k,l(t)‚àà[0,P max,l]isthelocalexecutionpowerofVU Fix n‚Ä≤,‚àÄn‚Ä≤ Ã∏=n,n‚Ä≤ ‚ààN
k at time slot t, and c is the effective selection capacitance. Set Œ∏t =argmax(12)
n
Œ¶
E. Problem Formulation Obtain (12)
The target is to minimize the power consumption of task
offloading and local processing as well as buffer length.
Therefore, the multi-objective optimization problem for each
B. DRL for Power Allocation
VU k can be formulated as follows:
(cid:40) T (cid:41) In this subsection, we use the MARL algorithm to solve
P1: min 1 (cid:88) (p (t)+p (t)+q (t)) , thepowerallocationproblemforVU.Here,eachVUinteracts
Œ∏ nt,pk,o(t),pk,l(t) T
t=1
k,o k,l k with the environment as an agent and makes power allocation
(11a) decisions through the corresponding strategies. At time slot
s.t. 0<p k,o(t)<P max,o,‚àÄk ‚ààK,‚àÄt‚ààT, (11b) t, the agent obtains the current state s t, the corresponding
action a and the corresponding reward r , and transitions to
0<p (t)<P ,‚àÄk ‚ààK,‚àÄt‚ààT, (11c) t t
k,l max,l
the next state s . This process can be formulated as e =
t+1 t
Œ∏ nt ‚ààŒ¶,‚àÄn‚ààN,‚àÄt‚ààT, (11d) (s t,a t,r t,s t+1) [18], [19], [20], [21], [22], [23], [24], [25],
where (11b) and (11c) represent the power constraints for andtherelevantstatespace,actionspace,andrewardfunction
VU k when offloading tasks and processing tasks locally, in this model are represented as follows:
respectively. Due to the limitation of RIS hardware, the size State space: At time slot t, the state of each VU k
of RIS phase-shift can only be selected within a limited range (agent k) consists of the following components: buffer length
constrained by (11d). Furthermore, the objective function is q k(t), the size of offloaded executed tasks q k,o(t), the size
non-convex, so this optimization problem is difficult to be of locallyexecuted tasks q k,l(t), andthe offloaded and locally
solved [9], [10], [11], [12], [13], [14], [15], [16], [17]. To processedtaskoverflowsq k,o(t)+q k,l(t)‚àíq k(t).Inaddition,
better address this issue, we propose a Multi-agents deep through equation (7), the SNR of VU k at time slot t, i.e.
reinforcement learning (MARL) scheme for joint BCD. Since Œ≥ k(t ‚àí 1) , depends on h r,b and ht k,r, which reflects the
theRISphase-shiftcontrollerandVUsarenotthesametypeof channel uncertainty of the VU at time slot t. Therefore, the
agents, their actions and reward are also different. Therefore, state space of the VU k at slot t can be formulated as s =
wewillfirstusetheBCDalgorithmtoobtaintheoptimalsize [q k(t),q k,o(t),q k,l(t),q k,o(t)+q k,l(t)‚àíq k(t),Œ≥ k(t‚àí1)].
of phase-shift, and then obtain the optimal power allocation Action space: as described above, agent k allocates the
scheme through the modified MADDPG algorithm. offloading power and the local power according to the corre-
sponding policy, so that the action space of agent k at time
III. SOLUTIONAPPROACH slot t is defined as a (t)=[p (t),p (t)].
k k,o k,l
We will describe the proposed scheme in detail. Firstly, we Reward function: in our proposed scheme, there are two
compute the channel information of VU based on the relevant aspects of rewards for each agent, one is the global reward
position of VU, BS and RIS, etc., where the RIS phase- reflecting the cooperation among agents, and the other is the
shift is a variable, and we use the BCD algorithm to obtain localrewardthatistohelpeachagenttoexploretotheoptimal
the optimal phase-shift with the objective of maximizing powerallocationscheme.InDRL,rewardscanbesetflexibly,
(cid:12) (cid:12)(h )HŒòtht (cid:12) (cid:12)2 . Then, the power allocation problem of VU andagoodrewardcanimprovetheperformanceofthesystem.
(cid:12) r,b k,r(cid:12)
Here, our goal is to optimize the agent k‚Äôs offloading and the
issolved accordingto themodifiedMADDPG algorithm.The
local power level, as well as the corresponding buffer length.
detailed algorithms are introduced as below.
To address it, we add two penalties Pen1 and Pen2, one for
the buffer length of agent k being greater than a threshold at
A. BCD for RIS Phase-Shift Coefficients
time slot t, and the other for the penalty incurred when agent
In this subsection, to handle the RIS phase-shift optimiza-
k allocates a certain amount of power to offloading execution
tion problem, the details related to the BCD algorithm can be
andlocalexecutionattimeslott,resultinginthetotalnumber
referred to [6]. At time slot t, we use the BCD algorithm to
of tasks processed being a certain value more than the buffer
maximizetheinstantaneousmodulussumofallVUsexpressed
length.ThusthelocalandglobalrewardsofVUk attimeslot
as follows:
t are
(cid:88)K (cid:12)
(cid:12)(h )HŒòtht
(cid:12) (cid:12)2
. (12)
(cid:12) r,b k,r(cid:12)
r =‚àí[w (p (t)+p (t))+w q (t)]‚àíPen1‚àíPen2,
k=1 k,l 1 k,o k,l 2 k
(13)
In each training iteration, the BCD algorithm is called to
and
optimize the phase-shift of the RIS. For M RIS elements, we
1 (cid:88)
select the optimal one among 2b values for each element. The r g = K r k,l. (14)
optimization procedure is described in detail in Algorithm 1. k‚ààK4
Algorithm 2: Modified MADDPG Algorithm for TABLEI
Power Allocation VALUESOFTHEPARAMETERSINTHEEXPERIMENTS.
Start environment simulator, generate vehicles
ParametersofSystemModel
Initialize global critic networks Qg1 and Qg2 Parameter Value Parameter Value
Initialize target global critic netwoœà r1 ks Qg1 œà an2 d Qg2 K 8 Œ∑ 3Mbps
Initialize each agent‚Äôs policy and critic
nœà e1‚Ä≤
tworks
œà 2‚Ä≤ N 40 œÉ2 -110dBm
b 3 Œ± r,b 2.5
for each episode do Œ± k,r 2.2 W 1MHz
Reset simulation paramaters for the RIS-assisted L 500cycles/bit c 10‚àí28
VEC system for each timestep t do Fmax 2.15GHz Pmax,o,P max,l 1W
for each agent k do ParametersofModifiedMADDPG
Parameter Value Parameter Value
Observe st and select action at =œÄ (st)
k k Œ∏ k Œ±C 0.001 Œ±A 0.0001
Receive local reward r kt
,l
œâ1 1 œâ2 0.6
pen1,pen2 2 d 2
s=(s ,s ,¬∑¬∑¬∑s ) and a=(a ,a ,¬∑¬∑¬∑a )
1 2 K 1 2 K Œ≥ 0.99 œÑ 0.005
Receive global reward r gt I 64 D 106
Store (st,st,rt,rt,st+1) in replay buffer D
l g
if the size of the replay buffer is larger than I then
Randomly sample mini-batch of I transitions
tuples from D  
Update global critics by minimizing the loss
  
according to Eq.(16)
Update global target networks parameters:   
œà‚Ä≤ ‚ÜêœÑœà +(1‚àíœÑ)œà‚Ä≤,j =1,2
j j j   
if episode mod d then
Train local critics and actors   
 3 U R S R V H G  D O J R U L W K P
for each agent k do
 ' ' 3 *
  
Update local critics by minimizing the
                      
loss according to Eq.(18)  ( S L V R G H
Update local actors according to Eq.(15) Fig.2. Rewardfunctionconvergence
Update local target networks
parameters: Œ∏ k‚Ä≤ ‚ÜêœÑŒ∏ k+(1‚àíœÑ)Œ∏ k‚Ä≤, where s = (s 1,s 2,¬∑¬∑¬∑s K) and a = (a 1,a 2,¬∑¬∑¬∑a K) are the
œï‚Ä≤
k
‚ÜêœÑœï k+(1‚àíœÑ)œï‚Ä≤
k
totalstateandactionvectors,D isthereplaybuffer,anda k =
œÄ (s ) is the action which is chosen for agent k according to
k k
its own policy œÄ . Then the twin global critic Qgj is updated
k œàj
to
(cid:104) (cid:105)
L(œà )=E (Qgj(s,a)‚àíy )2 ,j =1,2, (16)
WeproposeaMARLalgorithmthatlearnshowtomaximize j s,a,r,s‚Ä≤ œàj g
bothglobalandlocalrewards.Itcontainstwomaincritics,one where
(cid:12)
is a global critic, which is shared among all the agents and y =r +Œ≥minQgj(s‚Ä≤,a‚Ä≤)(cid:12) . (17)
inputs the states and actions of all the agents to maximize the g g j=1,2 œà j‚Ä≤ (cid:12) a‚Ä≤=œÄ‚Ä≤(s‚Ä≤)
k k k
globalreward;theotherislocalcritic,whichreceivesthestates Here, œÄ‚Ä≤ ={œÄ‚Ä≤,œÄ‚Ä≤,¬∑¬∑¬∑ ,œÄ‚Ä≤ } is the target policy with param-
1 2 K
andactionsofthespecificagentsandevaluatestheitsrewards. eter Œ∏‚Ä≤ = {Œ∏‚Ä≤,Œ∏‚Ä≤,¬∑¬∑¬∑ ,Œ∏‚Ä≤ }. Similarly, the local critic Qk of
Inaddition,weconsidertheimpactofapproximationerrorsin agent k is
up1 dat2
ed to
K œïk
strategy and value updates on the global critic in MADDPG
algorithm, according to [28], To improve the MADDPG algo- Lk(œà k)=E sk,ak,rk,s‚Ä≤
k(cid:104)
(Qk œïk(s k,a k)‚àíy
lk)2(cid:105)
, (18)
rithm,weemploytwin-delaydeterministicstrategygradientto
where
replace the global critic. (cid:12)
yk =rk+Œ≥Qk (s‚Ä≤,a‚Ä≤)(cid:12) . (19)
Specifically, we consider a vehicular environment with K l l œï‚Ä≤ k k k (cid:12) a‚Ä≤=œÄ‚Ä≤(s‚Ä≤)
k k k
vehicles (agents) and the policies for all agents are œÄ =
The detailed MARL algorithm is described in the table
{œÄ ,œÄ ,¬∑¬∑¬∑ ,œÄ }.Theagentk‚ÄôsstrategyœÄ ,Q-functionsQk
and1 tw2 in globaK l critic Q-functions (Qg1, Qk g2) are parameteœï rk - Algorithm 2.
œà1 œà2
ized by Œ∏ , œï , œà and œà , respectively. For each agent, the
k k 1 2
IV. SIMULATIONRESULTS
modified policy gradient can be written as
In this section, we perform simulation experiments to vali-
Global Critic date our proposed multi-agent DRL scheme with joint BCD.
(cid:122) (cid:125)(cid:124) (cid:123)
‚àáJ(Œ∏ )=E (cid:104) ‚àá œÄ (a |s )‚àá Qgj(s,a)(cid:105) + The simulation tool is Python 3.9. It is assumed that within
k s,a‚àºD Œ∏k k k k ak œàj (15) someroadsegments,thevehiclescannotcommunicatedirectly
E (cid:2) ‚àá œÄ (a |s )‚àá Qk (s ,a )(cid:3) , withtheBS(0,0,25)andthusneedtorelyontheassistanceof
sk,ak‚àºD Œ∏k k k k ak œïk k k
(cid:124) (cid:123)(cid:122) (cid:125) RIS (220,220,25) to communicate indirectly with the BS. 8
Local Critic
 G U D Z H 55
  1000 P Mr Aop Do Dse Pd G A Rlg ao nr dit oh mm ,, cc == 11 ee -- 22 77  3 U R S R V H G  $ O J R U L W K P
D D PG , c=1e-27      0 $ ' ' 3 *  5 D Q G R P
   3 U R S R V H G  $ O J R U L W K P   F    H    Proposed A lgorithm , c=1e-28
 
 0  ' ' $  3 '  * '    3  F *      5  H  D   Q   G R P   F    H    800 M
D
DA PD GD
,
P cG
=1
R e-a 2n 8dom , c=1e-28    
 3 U R S R V H G  $ O J R U L W K P   F    H    Proposed A lgorithm , c=1e-29
   0 $ ' ' 3 *  5 D Q G R P   F    H    600 M A D D PG R andom , c=1e-29    
 ' ' 3 *   F    H    50 D D PG , c=1e-29    3  0 U R  $ S  ' R  ' V H  3 G  *  $   5 O J  D R  Q U  G L W  R K  P P        F F          H H          400 344 505     3 U R S R V H G  $ O J R U L W K P
   ' ' 3 *   F    H    23 50  0 $ ' ' 3 *  5 D Q G R P
200 20   
  15
10
5
  0   
                            0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5                    
 7 D V N  D U U L Y D O  U D W H    0 E S V T ask arrival rate / M bps  7 K H  Q X P E H U  R I  5 , 6  H O H P H Q W V
(a) (b) (c)
Fig.3. Performance:(a)Totalpowerconsumption;(b)Bufferlength;(c)Totalpowerconsumptionandbufferlength
vehiclesareinvolvedintheexperiment,eachwitharandomly affecting the buffer length.
chosen speed within 10 to 15 km/h. The numbers of hidden
layers for actor and critic networks are 2 and 3, respectively.
V. CONCLUSION
The learning rates of critic/actor networks learning rate are
In this letter, in order to optimize the power consumption,
0.001and0.0001,respectively.Thekeyparametersareshown
bufferlength,andRISphase-shiftmatrixinRISassistedVEC
in Table I.
network, we developed a new framework, i.e., the MARL
Fig.2presentstheconvergenceperformanceoftheproposed
scheme with joint BCD algorithm. Simulation results demon-
scheme and the DDPG scheme in terms of rewards. The pro-
stratedthesuperiorityofourschemetotherandomRISphase-
posedframeworkisverystablewithlessoscillationscompared
shift and DDPG algorithms in terms of power consumption
to DDPG. Since the DDPG is fully centralized, it must take
and buffer length.
the observations and actions of all agents as input, which
cannot effectively address individual performance, leading
to suboptimal reward situations. The proposed framework
REFERENCES
combinesMARLwiththeBCDalgorithmandshowsexcellent [1] Y. Liu, H. Yu, S. Xie and Y. Zhang,‚ÄúDeep Reinforcement Learning for
convergenceperformance.Itcanalsosimultaneouslyselectthe Offloading and Resource Allocation in Vehicle Edge Computing and
Networks,‚ÄùIEEETrans.Veh.Technol.,vol.68,no.11,pp.11158-11168,
betterresultonmaximizingthelocalandglobalrewardsofall
Nov.2019.
agents, and thus facilitate the cooperation among agents. [2] N. Cheng et al., ‚ÄúSpace/Aerial-Assisted Computing Offloading for IoT
Applications:ALearning-BasedApproach,‚ÄùIEEEJ.Sel.AreasCommun.,
Figs.3(a)and3(b)showthechangesinpowerconsumption
vol.37,no.5,pp.1117-1129,May2019.
and buffer length for all VUs at different task arrival rates. In [3] A. Al-Hilo, M. Samir, M. Elhattab, C. Assi and S. Sharafeddine, ‚ÄúRe-
addition, we consider different effective selection capacity c configurableIntelligentSurfaceEnabledVehicularCommunication:Joint
UserSchedulingandPassiveBeamforming,‚ÄùIEEETrans.Veh.Technol.,
and different training methods. It can be seen that the power
vol.71,no.3,pp.2333-2345,March2022.
consumption and buffer length increase with the increment of [4] M.DiRenzo,A.Zappone,M.Debbah,C.Yuen,J.deRosny,S.Tretyakov,
taskarrivalrate.Theproposedalgorithmhasthelowestpower ‚ÄúSmart Radio Environments Empowered by Reconfigurable Intelligent
Surfaces:HowItWorks,StateofResearch,andTheRoadAhead,‚ÄùIEEE
consumption and relatively small buffer length at different ca-
J.Sel.AreasCommun.,vol.38,no.11,pp.2450-2525,Nov.2020.
pacitances. As the capacity increases, the power consumption [5] Y.Liu,X.Liu,X.Mu,T.Hou,J.Xu,M.D.Renzo,‚ÄúReconfigurableIn-
and buffer length also increase. This is because, according to telligentSurfaces:PrinciplesandOpportunities,‚ÄùIEEECommun.Surveys
Tuts.,vol.23,no.3,pp.1546-1577,3thQuarter2021.
the Eq. (9) and Eq. (10), q (t) is related to the effective
k,l [6] J.He,K.Yu,Y.Shi,Y.Zhou,W.ChenandK.B.Letaief,‚ÄúReconfigurable
selection capacity c, the larger the capacity, the fewer local Intelligent Surface Assisted Massive MIMO With Antenna Selection,‚Äù
processing tasks, which increases the power consumption and IEEETrans.WirelessCommun.,vol.21,no.7,pp.4769-4783,July2022.
[7] H.Zhu,Q.Wu,X.-J.Wu,Q.Fan,P.FanandJ.Wang,‚ÄúDecentralized
buffer length. Due to the poor performance of the DDPG
Power Allocation for MIMO-NOMA Vehicular Edge Computing Based
algorithm, it always spends some power to ensure that the onDeepReinforcementLearning,‚ÄùIEEEInternetofThingsJ.,vol.9,no.
bufferdoesnotaccumulatetoomanytasks.However,whenthe 14,pp.12770-12782,July,2022.
[8] M. Parvini, M. R. Javan, N. Mokari, B. Abbasi and E. A. Jorswieck,
capacity and task arrival rate are both large, it cannot achieve
‚ÄúAoI-Aware Resource Allocation for Platoon-Based C-V2X Networks
a good power allocation strategy, resulting in a large amount viaMulti-AgentMulti-TaskReinforcementLearning,‚ÄùIEEETrans.Veh.
of task accumulation. Technol.,vol.72,no.8,pp.9880-9896,Aug.2023.
[9] J. Fan, Q. Wu and J. Hao, ‚ÄúOptimal Deployment of Wireless Mesh
Fig. 3(c) reflects the impact of the number of RIS elements Sensor Networks based on Delaunay Triangulations,‚Äù in Proc. of IEEE
on the network performance, where the total power consump- International Conference on Information, Networking and Automation,
Kunming,China,Oct.2010,pp.1‚Äì5.
tion and buffer length decrease when the number of RIS
[10] Q. Wu and J. Zheng, ‚ÄúPerformance Modeling and Analysis of the
elementsincreases.Ourproposedschemeoutperformsrandom ADHOCMACProtocolforVehicularNetworks,‚ÄùWirelessNetworks,Vol.
RIS phase-shift because random RIS phase-shift reduces the 22,No.3,Apr.2016,pp.799-812.
[11] Q. Wu, S. Xia, Q. Fan and Z. Li, ‚ÄúPerformance Analysis of IEEE
data transmission rate during offloading, resulting in more
802.11p for Continuous Backoff Freezing in IoV,‚Äù Electronics, Vol. 8,
power consumption to ensure data transmission and also No.1404,Dec.2019.
 Q R L W S P X V Q R F  U H Z R S  O D W R 7
htgnel
reffu B
 Q R L W S P X V Q R F  U H Z R S  O D W R 7
 K W J Q H O  U H I I X %6
[12] W. Chen, L. Dai, K. B. Letaief and Z. Cao, ‚ÄùA Unified Cross-Layer
Framework for Resource Allocation in Cooperative Networks,‚Äù IEEE
TransactionsonWirelessCommunications,vol.7,no.8,pp.3000-3012,
August2008.
[13] Y.ZhangandK.B.Letaief,‚ÄùAdaptiveresourceallocationandschedul-
ing for multiuser packet-based OFDM networks,‚Äù 2004 IEEE Interna-
tional Conference on Communications, Paris, France, 2004, pp. 2949-
2953Vol.5.
[14] K.Xiong,C.Chen,G.Qu,P.FanandK.B.Letaief,‚ÄùGroupCooperation
WithOptimalResourceAllocationinWirelessPoweredCommunication
Networks,‚ÄùIEEETransactionsonWirelessCommunications,vol.16,no.
6,pp.3840-3853,June2017.
[15] T. Li, P. Fan, Z. Chen and K. B. Letaief, ‚ÄùOptimum Transmission
Policies for Energy Harvesting Sensor Networks Powered by a Mobile
Control Center,‚Äù IEEE Transactions on Wireless Communications, vol.
15,no.9,pp.6132-6145,Sept.2016.
[16] J. Zhang, P. Fan and K. B. Letaief, ‚ÄùNetwork Coding for Efficient
MulticastRoutinginWirelessAd-hocNetworks,‚ÄùIEEETransactionson
Communications,vol.56,no.4,pp.598-607,April2008.
[17] Z. Yao, J. Jiang, P. Fan, Z. Cao and V. O. K. Li, ‚ÄùA neighbor-table-
basedmultipathroutinginadhocnetworks,‚ÄùThe57thIEEESemiannual
VehicularTechnologyConference,2003.VTC2003-Spring.,Jeju,Korea
(South),pp.1739-1743vol.3,2003.
[18] Q. Wu, Y. Zhao, Q. Fan, P. Fan, J. Wang and C. Zhang, ‚ÄùMobility-
Aware Cooperative Caching in Vehicular Edge Computing Based on
Asynchronous Federated and Deep Reinforcement Learning,‚Äù IEEE J.
Sel.TopicsSignalProcess.,vol.17,no.1,pp.66-81,Jan.2023.
[19] S.Wang,Q.Wu,Q.Fan,P.FanandJ.Wang,‚ÄúMobility-AwareAsyn-
chronous Federated Learning for Edge-Assisted Vehicular Networks,‚Äù
IEEEInternationalConf.Commun.,Rome,Italy,2023,pp.3621-3626.
[20] Q.Wu,W.Wang,P.Fan,Q.Fan,J.WangandK.B.Letaief,‚ÄùURLLC-
Awared Resource Allocation for Heterogeneous Vehicular Edge Com-
puting,‚Äù IEEE Trans Veh. Technol., early access, February 2024, doi:
10.1109/TVT.2024.3370196.
[21] Q. Wu, S. Wang, H. Ge, P. Fan, Q. Fan and K. B. Letaief, ‚ÄúDelay-
sensitive Task Offloading in Vehicular Fog Computing-Assisted Pla-
toons‚Äú, IEEE Trans. Netw. Ser. Manag., Vol. 21, No. 2, pp. 2012-2026,
April2024.
[22] Q.Wu,X.Wang,Q.Fan,P.Fan,C.ZhangandZ.Li,‚ÄúHighStableand
AccurateVehicleSelectionSchemebasedonFederatedEdgeLearningin
Vehicular Networks‚Äù, China Communications, Vol. 20, No. 3, pp. 1-17,
Mar.2023.
[23] Q. Wu, S. Shi, Z. Wan, Q. Fan, P. Fan and C. Zhang, ‚ÄúTowards V2I
Age-aware Fairness Access: A DQN Based Intelligent Vehicular Node
TrainingandTestMethod‚Äù,ChineseJournalofElectronics,vol.32,no.
6,pp.1230-1244,2023.
[24] Q. Wu, W. Wang, P. Fan, Q. Fan, H. Zhu and K. B. Letaief,
‚ÄúCooperative Edge Caching Based on Elastic Federated and Multi-
Agent Deep Reinforcement Learning in Next-Generation Networks,‚Äù,
IEEE Transactions on Network and Service Management, 2024, doi:
10.1109/TNSM.2024.3403842.
[25] D.Long,Q.Wu,Q.Fan,P.Fan,Z.LiandJ.Fan,‚ÄúAPowerAllocation
SchemeforMIMO-NOMAandD2DVehicularEdgeComputingBased
onDecentralizedDRL‚Äù,Sensors,Vol.23,No.7,2023,Art.no.3449.
[26] M.Samir,M.Elhattab,C.Assi,S.SharafeddineandA.Ghrayeb,‚ÄúOp-
timizing Age of Information Through Aerial Reconfigurable Intelligent
Surfaces:ADeepReinforcementLearningApproach,‚ÄùIEEETrans.Veh.
Technol.,vol.70,no.4,pp.3978-3983,April2021.
[27] Y.Chen,Y.Wang,J.ZhangandM.D.Renzo,‚ÄúQoS-DrivenSpectrum
Sharing for Reconfigurable Intelligent Surfaces (RISs) Aided Vehicular
Networks,‚ÄùIEEETrans.WirelessCommun.,vol.20,no.9,pp.5969-5985,
Sept.2021.
[28] S. Fujimoto, H. van Hoof, and D. Meger, ‚ÄúAddressing function ap-
proximation error in actor-critic methods,‚Äù in Proceedings of the 35th
InternationalConferenceonMachineLearning,vol.80,pp.1587-1596,
July2018.