The Benefits of Power Regularization in Cooperative
Reinforcement Learning
MichelleLi MichaelDennis
MassachusettsInstituteofTechnology UniversityofCalifornia,Berkeley
Cambridge,MA Berkeley,CA
michelleli@alum.mit.edu michael_dennis@cs.berkeley.edu
ABSTRACT distributedratherthanconcentrated.Oneexampleofadistributed
CooperativeMulti-AgentReinforcementLearning(MARL)algo- systemofpoweristheUSgovernment:inprinciple,havingthree
rithms,trainedonlytooptimizetaskreward,canleadtoacon- brancheswithchecksandbalanceshelpspreventanysinglebranch
centrationofpowerwherethefailureoradversarialintentofa fromhavingtoomuchpower.
singleagentcoulddecimatetherewardofeveryagentinthesys- ThesamebasicideaappliesinMulti-AgentReinforcementLearn-
tem.Inthecontextofteamsofpeople,itisoftenusefultoexplicitly ing(MARL)systems:regardlessofwhetherthesettingisfullycoop-
considerhowpowerisdistributedtoensurenopersonbecomesa erative,fullycompetitive,orgeneralsum,itisoftenadvantageous
singlepointoffailure.Here,wearguethatexplicitlyregularizing foragentstolimittheamountofpowerotheragentshave.
theconcentrationofpowerincooperativeRLsystemscanresultin Powerresistsformalizationdespitebeingaprevalentandintu-
systemswhicharemorerobusttosingleagentfailure,adversarial itiveconcept.Inthispaper,wemakenonormativeclaimsabout
attacks,andincentivechangesofco-players.Tothisend,wedefine howpoweroughttobedefinedorregulated,justthatpowerisa
apracticalpairwisemeasureofpowerthatcapturestheabilityof conceptuallyusefultoolforcooperativemultiagentsystems.To
anyco-playertoinfluencetheegoagentâ€™sreward,andthenpro- focusonempiricalprogress,welimitourattentiontopowerasin-
poseapower-regularizedobjectivewhichbalancestaskreward fluenceonreward.Butregardlessofhowitisformalized,weargue
andpowerconcentration.Giventhisnewobjective,weshowthat thatavoidingconcentrationofpowercansimultaneouslymitigate
therealwaysexistsanequilibriumwhereeveryagentisplayinga threeproblemsthataregenerallythoughtofseparately:system
power-regularizedbest-responsebalancingpowerandtaskreward. failure,adversarialattacks,andincentivechanges.Itdoessoby
Moreover,wepresenttwoalgorithmsfortrainingagentstowards mitigatingtheeffectsofoffpolicybehavior,regardlessofthecause.
thispower-regularizedobjective:SampleBasedPowerRegulariza- Consider the following toy example: a group of agents need
tion(SBPR),whichinjectsadversarialdataduringtraining;and toworkcooperativelytomaximizeproduction.Agentsproduce
PowerRegularizationviaIntrinsicMotivation(PRIM),whichadds outputbystartingfromrawmaterialandapplyingaseriesofğ‘š
anintrinsicmotivationtoregulatepowertothetrainingobjective. actions.Theycaneitherworkindividuallyorformanassembly
Ourexperimentsdemonstratethatbothalgorithmssuccessfully line,whichismoreefficientbecauseofspecializationandbatch
balancetaskrewardandpower,leadingtolowerpowerbehavior productivitybutrequiresthateveryagentisasinglepointoffailure.
thanthebaselineoftask-onlyrewardandavoidcatastrophicevents Changesinanyagentâ€™sbehaviorwouldbringthewholesystemto
incaseanagentinthesystemgoesoff-policy. ahalt.Dependingonhowmuchwecareabouttaskrewardversus
robustness,wemightpreferonebehaviorortheother.
Ourcontributionsinthisworkareasfollows:1)Weproposea
KEYWORDS
practicalmeasureofpoweramenabletooptimizationâ€“howmuch
Multi-Agent Reinforcement Learning; Cooperative Multi-Agent
anotheragentcandecreaseourreturnbychangingtheiractionfor
ReinforcementLearning;GameTheory;IntrinsicMotivation;Fault
onetimestep.2)Weproposeaframeworkforbalancingmaximizing
Tolerance;AdversarialRobustness;DistributionofPower
taskrewardandminimizingpowerbyregularizingthetaskobjec-
ACMReferenceFormat: tiveforpower,andthenshowanequilibriumalwaysexistswith
MichelleLiandMichaelDennis.2023.TheBenefitsofPowerRegularization thismodifiedobjective.3)Wepresenttwoalgorithmsforachiev-
inCooperativeReinforcementLearning.InProc.ofthe22ndInternational ingpowerregularization:one,SampleBasedPowerRegularization
ConferenceonAutonomousAgentsandMultiagentSystems(AAMAS2023), (SBPR),whichinjectsadversarialdataduringtrainingbyadversari-
London,UnitedKingdom,May29â€“June2,2023,IFAAMAS,9pages.
allyperturbingoneagentâ€™sactionswithsomeprobabilityatany
timestep;andtwo,PowerRegularizationviaIntrinsicMotivation
1 INTRODUCTION
(PRIM),whichaddsanintrinsicrewardtoregularizepowerateach
Whenconsideringhowtooptimallystructureateam,institution, timestep.SBPRissimplerbutPRIMisbetterabletoachievethe
orsociety,akeyquestionishowresponsibility,power,andblame rightreward-powertradeoffsforverysmallvaluesofğœ†.Ourexper-
oughttobedistributed,andassuchitisabroadlystudiedconcept imentsinanOvercooked-inspiredenvironment[3]demonstrate
inthesocialsciences[21,22].Weoftenwanttoavoidtoomuch thatbothalgorithmscanachievevariouspower-rewardtradeoffs
powerlyinginthehandsofafewactors,preferringpowertobe andcanreducepowercomparedtothetaskreward-onlybaseline.
Proc.ofthe22ndInternationalConferenceonAutonomousAgentsandMultiagentSys-
tems(AAMAS2023),A.Ricci,W.Yeoh,N.Agmon,B.An(eds.),May29â€“June2,2023,
London,UnitedKingdom.Â©2023InternationalFoundationforAutonomousAgents
andMultiagentSystems(www.ifaamas.org).Allrightsreserved.
4202
nuJ
71
]GL.sc[
1v04211.6042:viXrağ‘ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’
ğ¸ğ‘”ğ‘œ=ğ‘ƒ1 ğ¸ğ‘”ğ‘œ=ğ‘ƒ0
ğ‘ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’
...
ğ‘‚ğ‘›âˆ’ğ‘ğ‘œğ‘™ğ‘–ğ‘ğ‘¦ ğ´ğ‘‘ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘™
P0 ğ‘ƒ0 ğ´ğ‘‘ğ‘£ğ‘’ğ‘Ÿğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘ƒ0
ğ‘ˆ ğ· ğ‘ˆ ğ· ğ‘ˆ ğ·
ğ‘ƒ1 ğ‘ƒ1
ğ‘ˆ ğ· ğ‘ˆ ğ· ğ‘ˆ ğ· ğ‘ˆ ğ· ğ‘ˆ ğ· ğ‘ˆ ğ·
(ğ‘,ğ‘) (ğ‘,ğ‘‘) (ğ‘’,ğ‘“) (ğ‘”,â„) (ğ‘,ğ‘) (ğ‘,ğ‘‘) (ğ‘’,ğ‘“) (ğ‘”,â„) (ğ‘,ğ‘) (ğ‘,ğ‘‘) (ğ‘’,ğ‘“) (ğ‘”,â„)
(a)Simplegeneral-sumgame (b)p-AdversarialGameof(a)
Figure1:ExtensiveFormdiagramsofasimple1timestepgameanditscorrespondingp-AdversarialgameatswhereNature
choosesanegoagentandwhethertousetheon-policyoradversarialco-playertowardtheegoagent.Thedashedlinesencircle
nodesinthesameinformationsetforPlayer1becauseagentsactsimultaneously.WeomitafinallayerofNaturenodesthat
modeltheprobabilistictransitionfunction.
2 RELATEDWORK andthejointactionğ‘ = (ğ‘1,ğ‘2,...,ğ‘ ğ‘›) âˆˆ ğ´isusedtotransition
Powerhasbeenbroadlystudiedinthesocialsciences[21,22]and
theenvironmentaccordingtoğ‘‡(ğ‘ â€²|ğ‘,ğ‘ ).Eachagentğ‘–receivestheir
formulti-agentsystems,buthasnotbeenstudiedinthecontextof ownrewardğ‘Ÿ ğ‘– = ğ‘… ğ‘–(ğ‘ ,ğ‘).Whileourtheoreticalresultsholdfor
deepmulti-agentRLtothebestofourknowledge.Instead,prior general-sumMarkovgames,weonlyempiricallyevaluateonfully
work has focused on graph-theoretic analysis [10] or symbolic cooperativeenvironments,thusassumingthatallagentshavethe
formulations[5].Therehavealsobeenproductiveformulations samereward:ğ‘… ğ‘– =ğ‘… ğ‘—âˆ€ğ‘–,ğ‘—.Weoperateinthefully-observedsetting:
oftherelatedconceptsofresponsibilityandblame[1,4,7,8,11], eachagentcandirectlyobservethetruestateğ‘ .Eachagentğ‘–aims
whichhavestrongconnectionstopower. toindependentlymaximizetheirtime-discounted,expectedreward
InAI,powerhasbeenformalizedinasingleagentcontext,with ğ‘ˆ ğ‘– = E[(cid:205)ğ‘‡ ğ‘¡=0ğ›¾ğ‘¡ğ‘Ÿ ğ‘¡] whereğ‘Ÿ ğ‘¡ istherewardattimeğ‘¡.Throughout,
recentworktowardsdefiningpowerandregularizinganagentâ€™s wewillassumefinite,discreteactionsandfinitetime.
ownbehaviorwithrespecttopower[27â€“29].Whileitisapromising Inanygame,itisusefulforaplayertoconsidertheirbestre-
directiontoextendtheseformalmeasurestoMARL,wefocuson sponses:optimalpoliciesgivenfixedpoliciesfortheco-players:
makingempiricalprogressonregularizingpowerinthiswork.
ThoughtheliteratureonpowerindeepMARLissparse,the ğœ‹
ğ‘–
âˆˆargmax{ğ‘ˆ ğ‘–(ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–)}=ğµğ‘…(ğœ‹ âˆ’ğ‘–).
literatureontheproblemsthatpowerregulationcanhelpmitigate ğœ‹ ğ‘–â€²âˆˆÎ ğ‘–
ismorerobust.Forinstance,thereisalargebodyofworkonde-
signingMARLsystemsthatcooperaterobustlyinsequentialsocial whereğœ‹ referstothepoliciesforallplayersotherthanğ‘–.Further-
âˆ’ğ‘–
dilemmas[6,14,15,17],inwhichbalancingpoweramongstthe more,wesaythatapolicyğœ‹
ğ‘–
isalocalbestresponseatstateğ‘ ifit
agentsiscritical.Thereisalsoasignificantbodyofworkshow- choosesanoptimaldistributionofactionsatğ‘ giventherestofits
ingtheexistenceofadversarialattacksforneuralnetworks[25] policyğœ‹ andco-playerpoliciesğœ‹ .Thatis,
ğ‘– âˆ’ğ‘–
andsingle-andmulti-agentRL[9,13,16,18].Insuchcases,power
reg Fu il na ar li lz ya ,t ti ho en ac la gn orh ite hlp mb su wil ed pfa reu sl et nto tl fe or ra pn oc we.
erregularizationmay
ğœ‹ ğ‘– âˆˆ argmax {ğ‘ˆ ğ‘–(ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–)}=ğµğ‘…ğ‘™ğ‘œğ‘ğ‘ğ‘™ (ğœ‹ âˆ’ğ‘–).
bewellsuitedforsituationswhereonedesiresagentsthatwork
ğœ‹ ğ‘–â€²âˆˆÎ ğ‘–(ğœ‹ğ‘–;ğ‘ )
wellwithunknownteammates,ifcombinedwithapproachesfrom
ad-hocteamwork[2,24]andzero-shotcoordination[12,26].This
whereÎ  ğ‘–(ğœ‹ ğ‘–;ğ‘ )={ğœ‹ ğ‘–â€² âˆˆÎ  ğ‘–|âˆ€ğ‘ â€² â‰ ğ‘ ,ğœ‹ ğ‘–â€²(ğ‘ â€²)=ğœ‹(ğ‘ â€²)}
Asetofpoliciesğœ‹ foreachplayerwhereeachpolicyğœ‹ isabest
isespeciallytrueinsituationswhereagentsmustinferwhototrust ğ‘–
responsetotheotherstrategiesiscalledaNashequilibriumthat
(i.e.whotoentrustpowerto)[20,23].
is,âˆ€ğ‘–,ğœ‹
ğ‘–
âˆˆğµğ‘…(ğœ‹ âˆ’ğ‘–)).Thisisa"stablepoint"wherenoagenthasan
incentivetounilaterallychangetheirpolicy.Furthermore,wesay
3 BACKGROUND
thatasetofpoliciesğœ‹ formalocalNashequilibriumatstateğ‘ ifall
WemodeloursettingasaMarkovgame[19]definedbythetuple
policiesarealocalbest-responsetotheotherpoliciesatğ‘ .
(ğ‘,ğ‘†,ğ´,ğ‘‡,ğ‘…,ğ›¾)whereğ‘ isthenumberofplayers,ğ‘† isthesetof
environmentstates,ğ´ = Ã—ğ‘–âˆˆ{0,...,ğ‘}ğ´ ğ‘– isthejointactionspace,
ğ‘‡ : ğ‘† Ã—ğ´ â†’ ğ‘† is the transition function,ğ‘… = Ã—ğ‘–âˆˆ{0,...,ğ‘}ğ‘… ğ‘– is 4 FORMALISM
therewardfunctionforeachplayer,andğ›¾ âˆˆ (0,1]isthediscount Todesignsystemsthatregularizeforpower,itisimportanttobe
factor.Ateverytimestep,eachagentğ‘–choosesanactionğ‘
ğ‘–
âˆˆğ´ ğ‘–, clearaboutourobjectiveandhowwedefinepower.Table1:Anexamplegamewhereplayer1hasnopowerover ofall ğ‘— onğ‘–.Inourexperimentswithmorethan2agentswelet
player2becauseallof1â€™sactionsareequallybadfor2. ğ‘… ğ‘–ğ‘ğ‘œğ‘¤ğ‘’ğ‘Ÿ (ğ‘ ,ğœ‹) =âˆ’ ğ‘1 âˆ’1(cid:205) ğ‘—power(ğ‘–,ğ‘—|ğ‘ ,ğœ‹)(themeanfunction).We
leavetheproblemofdeterminingthemostappropriatechoiceof
ğ‘‹ ğ‘Œ ğ‘ aggregationfunctiontofuturework.
ğ‘‹ (3,âˆ’10) (3,âˆ’9) (3,âˆ’8) Considerthe2-playergeneral-summatrixgamedefinedinTa-
ğ‘Œ (2,âˆ’10) (2,âˆ’9) (2,âˆ’8) ble 2 which we call the Attack-Defense game. This is a single
ğ‘ (1,âˆ’10) (1,âˆ’9) (1,âˆ’8) timestepgamesoğ‘ˆpower (ğœ‹|ğ‘ )=âˆ’power(ğ‘–,ğ‘—|ğ‘ ,ğœ‹).Ifeitherplayer
ğ‘–
playsğ‘‹,theotherplayercouldplayğ‘ reducingtheutility.playing
ğ‘Œ toguarantees2utility,payingasmallpricetoreducepower.
4.1 MeasuringPower
If optimizing purely for task reward, both agents play ğ‘‹ to
Ourmaingoalistomakeempiricalprogressonbuildingpower-
achievetheutility-maximizingNashequilibrium.However,playing
regularizingMARLsystems,sowewillnotaimtofindthemost
ğ‘‹ incurs3powerwhileplayingğ‘Œ incurs0power.Thus,byEq1we
p for rop pe or wo er rm oo fs ct og -pen lae yr ea rld ğ‘—e ofi vn ei rtio egn oof agp eo nw te ğ‘–r ,. wW he icd hefi wn ee ca am lle 1a -s su ter pe haveğ‘ˆ ğ‘ƒğ‘…(ğ‘‹) =3âˆ’3ğœ†andğ‘ˆ ğ‘ƒğ‘…(ğ‘Œ) =2,soforğœ† > 1
3
wepreferğ‘Œ.
SeeTable3foralargervariantofthegamewithanontrivialPareto
adversarialpower,asthedifferenceğ‘— couldmakeonğ‘–â€™srewardifğ‘—
frontier.Figure2ashowsthevalueofeachactionasafunctionofğœ†
hadinsteadactedadversariallytowardğ‘–foronetimestep.
assumingthecoplayerison-policy(i.e.doesnâ€™tplayF).
Definition4.1(1-stepadversarialpower). Letğ‘ˆ ğ‘–taskdenoteplayer Itisimportantfortheregularizedobjectivetoapplyatevery
ğ‘–â€™staskutility.Givenpoliciesğœ‹,the1-stepadversarialpoweragent state,eventhoseunreachableonpolicy.Anaiveapproachtopenal-
ğ‘— hasonagentğ‘–whenstartingfromstateğ‘ is: izingpowerwouldbetoonlypenalizethe1-stepadversarialpower
overtheagentintheinitialstate,thatis,onlyaimingtomaximize
power(ğ‘–,ğ‘—|ğ‘ ,ğœ‹)=ğ‘Ÿ+E[ğ‘ˆ ğ‘–task (ğ‘ â€²,ğœ‹)]âˆ’ ğ‘m ğ‘—âˆˆi ğ´n ğ‘—(ğ‘Ÿ ğ‘ğ‘—+E[ğ‘ˆ ğ‘–task (ğ‘  ğ‘â€² ğ‘—,ğœ‹)]) ğ‘ˆ ğ‘–(ğœ‹|ğ‘ 0)whereğ‘ 0istheinitialstate.However,suchameasurehas
afundamentalflaw,inthatonceanagentdeviatesfromtheusual
whereğ‘ â€² =ğ‘‡(ğ‘ ,ğœ‹),ğ‘  ğ‘â€²
ğ‘—
=ğ‘‡(ğ‘ ,ğ‘ ğ‘—;ğœ‹ âˆ’ğ‘—),andğ‘Ÿandğ‘Ÿ
ğ‘ğ‘—
areğ‘–â€™srewards
strategy,thereisnolongeranyincentivetoregulatepowerand
obtainedon-policyandwithğ‘—â€™sdeviationtoğ‘ ,respectively.
ğ‘— thusouragentwouldgaintrustinpotentiallyadversarialcoplayers.
Themin istakenoverthesetofdeterministicactions.Note Forinstance,supposetheoptimalpowerregularizedpolicywere
thatitisnoğ‘ tğ‘— nâˆˆğ´ ecğ‘— essarytoconsiderstochasticpoliciesasthemost toworkindependentlyinsteadofforminganassemblyline.Oncean
powerfulstochasticğœ‹ couldsimplyplaceprobability1onanyof agentdeviates,thesystemcouldbeinsomestateğ‘ notreachableon-
ğ‘—
thedeterministicactionsthatachievethelowestutilityforğ‘–. policy,onlyreachableviaanadversarialdeviationğ‘.Theonlyway
Counterintuitively,itispossiblethatallofğ‘—â€™simmediateactions behaviorinstateğ‘ influencestheutilityattheinitialstateğ‘ˆ ğ‘–(ğœ‹|ğ‘ 0)
exertsomecausaleffectonğ‘–â€™sutilitywithoutğ‘— havinganypower isthroughtheadversarialactiontermofthepowerregularization
power
overğ‘–.Thiscanhappenifallofğ‘—â€™sactionsreduceğ‘–â€™srewardby10, ğ‘ˆ ğ‘– (ğœ‹|ğ‘ 0).Sincethistermincreaseswhentaskrewardincreases,
forexample.Sincewearedefiningpowerinrelativeterms,ifall afteranydeviationthepolicywillnolongerregulatepower.Thus
actionshavethesameeffectonğ‘–,wesayğ‘— hasnopoweroverğ‘–as once one agent fails, all agents would revert to forming brittle
anycausaleffectsofğ‘—â€™sactionsonğ‘–areinevitable.SeeTable1for andpower-concentratingassemblylines.Thisistheoppositeof
anexampleworkedoutexplicitly.Suchnuanceisreminiscentof thedesiredbehavior:wewouldtakeafailureofanagentasan
thedifficultiesindefiningblame[4].Exploringsuchconnections indicationthattheyshouldbeentrustedwithmorepowerbecause
in-depth could be a path towards better metrics for measuring ourmodeldoesnotallowthemtodeviateagain.
power. Luckily,thestateconditionedregularizationweproposeisa
simplefix.Ratherthanregularizingforpowerjustatthefirststate,
4.2 RegularizingforPower weregularizepowerviaoptimizingEq1atallğ‘ .Thusevenafteran
agentfailsotherswillstillcontinuetoregularizeforpower.
Traditionally,cooperativeMARLalgorithmsaimtooptimizethe
discountedsumoftaskrewards,whichwecalltaskutility,without
5 EXISTENCEOFEQUILIBRIA
explicitconsiderationfortheamountofpowerheldbyotheragents.
Wearguethatwecanmakesystemsmorerobustbyoptimizingan InthestandardformulationofMarkovGames,theexistenceof
explicittrade-offbetweenmaximizingtaskrewardandminimizing anequilibriumsolutionisguaranteedbyNashâ€™sTheorem,which
power.Thisframeworkhastheadvantageofaddressingsystem showsthateveryfinitegamehasamixedNashequilibrium.How-
failure,adversarialattacks,andincentivechangesallatonceby ever,onceweregularizeforpower,Nashâ€™stheoremnolongerap-
mitigatingthenegativeeffectsofoff-policybehavior. pliesbecausethepayoffbecomesafunctionofthestrategy.
Wefocusonlineartrade-offs,thatisobjectivesoftheform Givenourpower-regularizedobjective,wecandefinenotionsof
bestresponseandequilibriumsimilartothestandardformulations.
ğ‘ˆ ğ‘–(ğœ‹|ğ‘ )=ğ‘ˆ ğ‘–task (ğœ‹|ğ‘ )+ğœ†ğ‘ˆ ğ‘–power (ğœ‹|ğ‘ ) (1)
ğ‘ 
aw ndhe ğ‘ˆr ğ‘–e poğ‘ˆ wğ‘–t ea rs (k ğœ‹(ğœ‹ |ğ‘ )|ğ‘ ) =is (cid:205)th
ğ‘‡
ğ‘¡=e 0t ğ‘…as ğ‘–ğ‘k ğ‘œğ‘¤u ğ‘’ti ğ‘Ÿli (t ğ‘ y ğ‘¡,f ğœ‹o )rp isla ty he er sğ‘– us mtar ot fin pg oi wn es rta rt ee
-
b ite
asD
ct
he rfi
ie
en
s
vpi et oi so nn
ts
he5 e. t1 oo. ptW
th
imee
p
as la oy
l ti
rct aih dea est
ğœ‹
oa ffâˆ’p
ğ‘–
bo
,
el ni tc
o
wy
ta
eğœ‹
t
eeğ‘– ndis
ta
aa
s
skğœ† ğœ‹-
ğ‘–
rp eâˆˆo ww
ğ‘ƒ
ae
ğ‘…
rr dğµr
ğ‘…
ae nğœ†g d(u ğœ‹l pa
âˆ’
or ğ‘–i w)z ,e eid
rf
ğ‘ğ‘œğ‘¤ğ‘’ğ‘Ÿ
wards ğ‘… at states starting from ğ‘  reached by unrolling ğœ‹. minimizationineverystate.Thatis,forallğ‘ wehave:
ğ‘–
ğ‘ğ‘œğ‘¤ğ‘’ğ‘Ÿ
Inthe2-agentsetting,ğ‘… ğ‘– (ğ‘ ,ğœ‹) = âˆ’power(ğ‘–,ğ‘—|ğ‘ ,ğœ‹),butwith ğœ‹ ğ‘– âˆˆargmax{ğ‘ˆ ğ‘–(ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘ )}=ğ‘ƒğ‘…ğµğ‘… ğœ†(ğœ‹ âˆ’ğ‘–|ğ‘ ).
moreagents,ğ‘… ğ‘ğ‘œğ‘¤ğ‘’ğ‘Ÿ mustaggregateinformationaboutthepowers ğœ‹ ğ‘–â€²âˆˆÎ ğ‘–Table3:TheLargerAttack-DefenseGame.
Table2:TheAttack-DefenseGame:anopponentcantake
awayyourutilityifyouplayğ‘‹,butyoucanpayasmall
ğ´ ğµ ğ¶ ğ· ğ¸ ğ¹
costtodefendagainstthatbyplayingğ‘Œ.
ğ´ (3,3) (3,2.5) (3,2) (3,1.5) (3,1) (âˆ’2,0)
ğµ (2.5,3) (2.5,2.5) (2.5,2) (2.5,1.5) (2.5,1) (0,0)
ğ‘‹ ğ‘Œ ğ‘
ğ¶ (2,3) (2,2.5) (2,2) (2,1.5) (2,1) (0.75,0)
ğ‘‹ (3,3) (3,2) (0,0)
ğ· (1.5,3) (1.5,2.5) (1.5,2) (1.5,1.5) (1.5,1) (1,0)
ğ‘Œ (2,3) (2,2) (2,0)
ğ¸ (1,3) (1,2.5) (1,2) (1,1.5) (1,1) (1,0)
ğ‘ (0,0) (0,2) (0,0)
ğ¹ (0,âˆ’2) (0,0) (0,0.75) (0,1) (0,1) (0,0)
Next,wedefinePowerRegularizingEquilibrium(PRE)tobea Proof. BaseCase.Withoutlossofgenerality,assumethatall
fixedpointofthepowerregularizedbestresponsefunctionand trajectories end in a single state where agentsâ€™ decisions affect
thenprovetheyareguaranteedtoexistinanygame. nothing.Suchastatecanbeaddedwithoutchangingthepoweror
utilityofanytrajectory.Atthisstateallpoliciesareequallyvalued,
Definition5.2(ğœ†-PowerRegularizingEquilibrium). Ağœ†-Power
sothebasecaseholdstrivially.
RegularizingEquilibrium(PRE)isapolicytupleğœ‹ suchthatall
policiesarepowerregularizedbestresponsestotheothers.Thatis,
InductiveStep.Assumethat,atanystateğ‘ â€²reachableattime
ineverystateğ‘ ,forallğ‘–,wehaveğœ‹
ğ‘–
âˆˆğ‘ƒğ‘…ğµğ‘… ğœ†(ğœ‹ âˆ’ğ‘–|ğ‘ ). ğ‘¡âˆ’1fromtheend,ğ‘ˆ ğ‘–p-Adv (ğœ‹ ğ‘–;ğœ‹ âˆ’ğ‘–|ğ‘ â€²)=ğ‘ˆ ğ‘–(ğœ‹ ğ‘–;ğœ‹ âˆ’ğ‘–|ğ‘ â€²).Ourgoalis
toshowthisequivalencealsoholdsforstatesreachableattimeğ‘¡.
Theorem5.3. Letğº beafinite,discreteMarkovGame,thena Expandingoutthedefinitionofthep-Adversarialgame,wehave:
ğœ†-powerregularizingequilibriumexistsforanyğœ†.
Intuitively,weprovethisbyconstructinganothergame,which
ğ‘ˆ ğ‘–p-Adv (ğœ‹ ğ‘–â€² ;ğœ‹ âˆ’ğ‘–|ğ‘ )
w cae nc ba ell at ph pe liğ‘ e- da ,d av ne drs sa hri oa wl g ta hm ate No af sğº h, et qo uw ilih bi rc iah iN na ts hh iâ€™ ss mth oe do ir fie em
d
=ğ‘… ğ‘–task (ğ‘ ,ğœ‹ âˆ’ğ‘–)+(1âˆ’ ğ‘‡ğœ† )E[ğ‘ˆ ğ‘–p-Adv (ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘ â€²)]
gamecorrespondtopowerregularizingequilibriaintheoriginal ğœ†
game.Thebasicideaofthisgameistoaddadversarialplayersthat
+
ğ‘‡
E[ğ‘ˆ ğ‘–task (ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘  ğ´â€² ğ‘‘ğ‘£)]
perturbco-playersâ€™actionsadversariallytowardtheegoagentwith ğœ†
probabilityğ‘.Wedefinetheğ‘-adversarialgameformallybelowand
=ğ‘… ğ‘–task (ğ‘ ) +(1âˆ’ ğ‘‡)E[ğ‘ˆ ğ‘–(ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘ â€²)]
depicttheextensiveformofa1-timestepgameinFigure1b. ğœ†
Definition5.4(p-AdversarialGameofğº atğ‘ ). Thep-adversarial
+
ğ‘‡
E[ğ‘ˆ ğ‘–task (ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘  ğ´â€² ğ‘‘ğ‘£)]
gameofğº atstateğ‘ addsadversarialagentsğœ‹ğ´âˆ—ğ‘– foreachplayer
ğ‘—
ğ‘– coa -n pd lac yo e- rp sla ty oe mr ğ‘— in.T imh ie zs ee ğ‘–a â€™g se rn et ts ura nre
.
Tra hn edo gm amly eg si tv ae rn tsc ao tn ğ‘ tr .o Nl ao tf uğ‘– râ€™ es whereğ‘ â€² âˆ¼ğ‘‡(ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–),ğ‘  ğ´â€²
ğ‘‘ğ‘£
âˆ¼ğ‘‡(ğœ‹ ğ‘–â€²;ğœ‹ğ´ ğ‘—âˆ—ğ‘–;ğœ‹ âˆ’{ğ‘–,ğ‘—}),andğ‘ˆ ğ‘–p-Adv (ğœ‹|ğ‘ )
istheutilityofagentğ‘–givenpoliciesğœ‹ startingatstateğ‘ oftheğ‘
randomlydecideswithprobabilityğ‘tolettheadversarywilltake
Adversarialgamebeforetheadversaryhastakencontrol.Thefirst
controlinthisepisode.Natureuniformlyrandomlyselectsanego-
lineabovefollowsfromthedefinitionoftheğ‘ adversarialgame
agentğ‘–whichwillbetheonlyagenttoberewardedinthegame,
andthesecondlinefollowsfromtheinductivehypothesis.Wecan
anduniformlyrandomlyselectsatimesteponwhichtheadversary
continuebyexpandingoutthedefinitionsandrearrangingterms:
willtakecontrol,iftheadversarygetscontrolthisepisode.Atthat
ft oim re onst ee sp teN pa .t Ru er we aw ri dll s, fc oh ro ğ‘–o as re ea cc ao lc- up ll aa ty ee dr ağ‘— st no ob re mr ae lp .lacedbyğœ‹ğ´ ğ‘—âˆ—ğ‘– ğ‘… ğ‘–task (ğ‘ ) +(1âˆ’ ğ‘‡ğœ† )E[ğ‘ˆ ğ‘–ğ‘¡ğ‘ğ‘ ğ‘˜ (ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘ â€²)]
Thefollowingtheoremestablishesacorrespondencebetween +ğœ†(1âˆ’ ğ‘‡ğœ† )E[ğ‘ˆ ğ‘–power (ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘ â€²)]+ ğ‘‡ğœ† E[ğ‘ˆ ğ‘–task (ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘  ğ´â€² ğ‘‘ğ‘£)]
thebestresponseinthep-adversarialgameofğº andthepower
regularizedbestresponseintheoriginalgame.Wewillusethis
=ğ‘… ğ‘–task (ğ‘ ) +E[ğ‘ˆ ğ‘–task (ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘ â€²)]
correspondencetoproveTheorem5.3. +ğœ†(1âˆ’ ğ‘‡ğœ† )E[ğ‘ˆ ğ‘–power (ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘ â€²)]
Theorem5.5. Consideranagentğ‘–inaMarkovgameğºwithtime
horizonğ‘‡,andanarbitrarystates.Theutilityofpoliciesinthep- âˆ’ ğ‘‡ğœ†(cid:16) E[ğ‘ˆ ğ‘–task (ğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘ â€²)] âˆ’E[ğ‘ˆ ğ‘–(taskğœ‹ ğ‘–â€²;ğœ‹ âˆ’ğ‘–|ğ‘  ğ´â€² ğ‘‘ğ‘£)](cid:17)
adversarialgameatstates,forğ‘ =ğœ†isequivalenttothecorresponding
policiesintheoriginalgame.Thatis,forallğœ‹ intheoriginalMarkov =ğ‘ˆ ğ‘–task (ğœ‹|ğ‘ ) +ğœ†ğ‘ˆ ğ‘–power (ğœ‹|ğ‘ )
game,wehave:ğ‘ˆ ğ‘–p-Adv(ğœ‹ ğ‘–;ğœ‹ âˆ’ğ‘–|ğ‘ )=ğ‘ˆ ğ‘–(ğœ‹ ğ‘–;ğœ‹ âˆ’ğ‘–|ğ‘ ).
Theproofideaisto,forap-adversarialgameonafixedstates, wherethefinallinefollowsfromthedefinitionoftaskandpower
inductivelyarguefortheequivalencestartingfromthelaststepof utility.Thus,thevalueofpoliciesinthep-Adversarialgameatstate
thegame.Ateachstep,thesetofğœ‹ğ´âˆ—ğ‘–forallco-playersğ‘—canbeseen ğ‘ timestepğ‘¡ isequivalenttothepower-regularizedvalue..
ğ‘—
asawaytocomputeğ‘–â€™spowerregularizationterm.Throughoutthe Byinduction,theequivalenceholdsforstatesreachableatany
p-Adv
proofweusestate-basedrewardstosimplifythenotation,which timestep.Thus,wehavethedesiredequivalenceğ‘ˆ
ğ‘–
(ğœ‹ ğ‘–;ğœ‹ âˆ’ğ‘–|ğ‘ )=
otherwisedoesnoteffectthemainstructureoftheproof. ğ‘ˆ ğ‘–(ğœ‹ ğ‘–;ğœ‹ âˆ’ğ‘–|ğ‘ ). â–¡Giventheequivalencebetweenthestandardbestresponsein Algorithm1SampleBasedPowerRegularization(SBPR)
thep-adversarialgameandthepower-regularizedbestresponsein 1: proceduretrainAgent(ğœ‹ ğ‘–,ğœ‹ ğ‘—,ğœ‹Ë† ğ‘—,ğ‘‰ ğ‘–,ğ‘)
theoriginalgame,wecanreturntoourtaskofprovingTheorem 2: Collecttrajectoriesforagentğ‘–:ateachtimestep,alwaysuse
5.3,toshowthatpowerregularizingequilibriaofğº alwaysexist. ğœ‹ ,andwithprobabilityğ‘useğœ‹Ë† ,otherwiseuseğœ‹ .
ğ‘– ğ‘— ğ‘—
3: UsePPOorotherRLalgorithmtoupdateğœ‹ ğ‘– andğ‘‰ ğ‘–.
Proof. Notethatwecanconstructatupleofpoliciesğœ‹ which 4: proceduretrainAdversarialAgent(ğœ‹ ğ‘–,ğœ‹Ë† ğ‘—,ğ‘‰ ğ‘–)
arealocalNashequilibriumatğ‘ intheğ‘-adversarialgameatğ‘ by 5: Collecttrajectoriesforadversarialagentğ‘—:ğœ‹Ë† ğ‘— hasonestep
standardbackwardsinductionargumentâ€“notingfirstthatthiscan toact,andğ‘Ÿ ğ‘– =ğ‘‰ ğ‘–(ğ‘ )âˆ’ğ‘Ÿ(ğ‘ ,ğœ‹ ğ‘–(ğ‘ ),ğœ‹Ë† ğ‘—(ğ‘ ))âˆ’ğ›¾ğ‘‰ ğ‘–(ğ‘ â€²).
bedoneintheterminalstates,andthennotingthatitcanthenbe 6: UsePPOorotherRLalgorithmtoupdateğœ‹Ë† ğ‘—.
doneateachofthepriorstatesbackwardsbyinduction. 7: procedureSBPR(ğ‘)
Considerthepolicyğœ‹ ğ‘– ofanarbitraryplayerğ‘–,byTheorem5.5, 8: Initializeğœ‹ ğ‘–,ğœ‹ ğ‘—,ğœ‹Ë† ğ‘–,ğœ‹Ë† ğ‘—,ğ‘‰ ğ‘–,ğ‘‰ ğ‘— arbitrarily.
ğœ‹ mustbeapowerregularizedbestresponseinğº atstateğ‘ .Since
ğ‘– 9: loop
weassumedğ‘–andğ‘ tobearbitrary,thisappliestoallğ‘–andğ‘ .Thus 10: trainAgent(ğœ‹ ğ‘–,ğœ‹ ğ‘—,ğœ‹Ë† ğ‘—,ğ‘)
thetupleğœ‹ ğ‘– representsapowerregularizingequilibriumofğº atall 11: trainAgent(ğœ‹ ğ‘—,ğœ‹ ğ‘–,ğœ‹Ë† ğ‘–,ğ‘)
statesğ‘ bydefinition. â–¡ 12: trainAdversarialAgent(ğœ‹ ğ‘—,ğœ‹Ë† ğ‘–,ğ‘‰ ğ‘—)
13: trainAdversarialAgent(ğœ‹ ğ‘–,ğœ‹Ë† ğ‘—,ğ‘‰ ğ‘–)
Wehaveshownthatthepowerregularizingequilibriaweseek
do,infact,exist,butmoreover,Theorem5.5givessomeideaabout
6.2 PowerRegularizationviaIntrinsic
amethodtoactuallyobtainthem.Wecanfindpoliciesinpower-
Motivation(PRIM)
regularizingequilibriumbyfindingpoliciesinNashequilibriumin
thep-adversarialgameofğº.Thisintuitionisthemotivationbehind PRIMaddsaper-stepintrinsicmotivationrewardtermthatpenal-
oneofourmethods,SampleBasedPowerRegularization,which izespowerontheagent.Eachagentâ€™srewardfunctionis
weintroduceinSection6.1andempiricallyevaluateinSection7. ğ‘… ğ‘–ğ‘ƒğ‘…ğ¼ğ‘€ (ğ‘ ,ğ‘,ğœ‹)=ğ‘… ğ‘–ğ‘¡ğ‘ğ‘ ğ‘˜ (ğ‘ ,ğ‘)+ğœ†ğ‘… ğ‘–ğ‘ğ‘œğ‘¤ğ‘’ğ‘Ÿ (ğ‘ ,ğœ‹)
whichispreciselythepowerregularizationobjectiveofEq1.
6 METHODS Ratherthanprobabilisticallyconsideringtheeffectofanadversary
Weintroducetwomethodsforpowerregularization,SampleBased likeSBPR,PRIMconsidersitateverystepbutdownweightsthe
PowerRegularization(SBPR)andPowerRegularizationviaIntrin- effectaccordingtoğœ†,whichreducesvariance.
sicMotivation(PRIM).SBPRisinspiredbythep-AdversarialGame Crucially,findingğ‘—â€™sadversarialactionforğ‘–tocomputeadver-
formulationintroducedinDefinition5.4;itinjectsadversarialdata sarialpowerrequirescounterfactualsfromaresettablesimulator.
duringtrainingbyperturbingactionsateachstepwithprobabil- Inthefutureonecouldtrytolearnthesimulatorinstead.
ityğ‘ = ğœ†.PRIMtrainsagentsdirectlyonthepowerregularized
objective,interpretingthepowerpenaltyasintrinsicmotivation. Algorithm2PowerRegularizationviaIntrinsicMotivation(PRIM)
Agents do not share weights, but we train them together of-
1: procedureComputePower(ğœ‹ ğ‘–,ğœ‹ ğ‘—,ğœ‹Ë† ğ‘—,ğ‘‰ ğ‘–)
fline.Ourtheoryandmethodsareamenabletobecombinedwith 2: ğ‘ â€² âˆ¼ğ‘‡(Â·|ğ‘ ,ğœ‹ ğ‘–(ğ‘ ),ğœ‹ ğ‘—(ğ‘ ))
approachesfromad-hocteamplay[2,24]andzero-shotcoordi- 3: ğ‘Ÿ â†ğ‘Ÿ(ğ‘ ,ğœ‹ ğ‘–(ğ‘ ),ğœ‹ ğ‘—(ğ‘ ))+ğ›¾ğ‘‰ ğ‘–(ğ‘ â€²)
n cha ati lo len ng[1 e2 s,, s2 o6] w, eth lo eau vg eh itth toes fe utd uo rm ewai on rs kb tr oin gg enw erit ah lizt ehe thm ist ah pe pir roo aw chn
.
4: ğ‘  ğ‘â€²
ğ‘‘ğ‘£
âˆ¼ğ‘‡(Â·|ğ‘ ,ğœ‹ ğ‘–(ğ‘ ),ğœ‹Ë† ğ‘—(ğ‘ ))
WetraineachagentusingProximalPolicyOptimization(PPO).
5: ğ‘Ÿ ğ‘ğ‘‘ğ‘£ â†ğ‘Ÿ(ğ‘ ,ğœ‹ ğ‘–(ğ‘ ),ğœ‹Ë† ğ‘—(ğ‘ ))+ğ›¾ğ‘‰ ğ‘–(ğ‘  ğ‘â€² ğ‘‘ğ‘£)
6: ğ‘ğ‘œğ‘¤ğ‘’ğ‘Ÿ â†ğ‘Ÿâˆ’ğ‘Ÿ ğ‘ğ‘‘ğ‘£
Theneuralnetworksparameterizingthepolicyconsistofseveral
7: returnğ‘ğ‘œğ‘¤ğ‘’ğ‘Ÿ
convolutionallayers,fullyconnectedlayers,acategoricaloutput
headfortheactor,andalinearlayervaluefunctionoutputhead. 8: proceduretrainAgent(ğœ‹ ğ‘–,ğœ‹ ğ‘—,ğœ‹Ë† ğ‘—,ğ‘‰ ğ‘–,ğœ†)
9: Collecttrajectoriesforagentğ‘–withrewardğ‘Ÿ ğ‘¡ğ‘ğ‘ ğ‘˜+ğœ†ğ‘Ÿ ğ‘ğ‘œğ‘¤ğ‘’ğ‘Ÿ.
10: UsePPOorotherRLalgorithmtoupdateğœ‹ ğ‘– andğ‘‰ ğ‘–.
6.1 Sample-BasedPowerRegularization(SBPR)
11: proceduretrainAdversarialAgent(ğœ‹ ğ‘–,ğœ‹Ë† ğ‘—,ğ‘‰ ğ‘–)
SBPRdirectlyinjectsadversarialrolloutsintothetrainingdata, 12: Collecttrajectoriesforadversarialagentğ‘—:ğœ‹Ë† ğ‘— hasonestep
playingthep-AdversarialGameintroducedinDefinition5.4.Atthe toact,andğ‘Ÿ
ğ‘–
=ğ‘‰ ğ‘–(ğ‘ )âˆ’ğ‘Ÿ(ğ‘ ,ğœ‹ ğ‘–(ğ‘ ),ğœ‹Ë† ğ‘—(ğ‘ ))âˆ’ğ›¾ğ‘‰ ğ‘–(ğ‘ â€²).
beginningofeveryrollout,wepickanagentğ‘–tobetheegoagent 13: UsePPOorotherRLalgorithmtoupdateğœ‹Ë† ğ‘—.
andanotheragent ğ‘— tobetheadversary.Ateverytimestep,with
14: procedurePRIM(ğœ†)
probabilityğ‘(independentofprevioustimesteps)weperturbagent
ğ‘—â€™sactionadversariallytoğ‘–.Onlyagentğ‘–receivestherollouttotrain 15: Initializeğœ‹ ğ‘–,ğœ‹ ğ‘—,ğœ‹Ë† ğ‘–,ğœ‹Ë† ğ‘—,ğ‘‰ ğ‘–,ğ‘‰ ğ‘— arbitrarily.
16: loop
on.Intuitively,thiscanbeseenastrainingontheğ‘-Adversarial
Gameatrandomstatesğ‘ . 17: ğœ‹ ğ‘–,ğ‘‰ ğ‘– â†trainAgent(ğœ‹ ğ‘–,ğœ‹ ğ‘—,ğœ‹Ë† ğ‘—,ğ‘‰ ğ‘–,ğœ†)
SBPRhastheadvantageofsimplicitybutmaynotregularizefor 18: ğœ‹ ğ‘—,ğ‘‰ ğ‘— â†trainAgent(ğœ‹ ğ‘—,ğœ‹ ğ‘–,ğœ‹Ë† ğ‘–,ğ‘‰ ğ‘—,ğœ†)
powersuccessfullyifwewant ğœ†tobeverysmall.Thisisbecause 19: ğœ‹Ë† ğ‘– â†trainAdversarialAgent(ğœ‹ ğ‘—,ğœ‹Ë† ğ‘–,ğ‘‰ ğ‘—)
wewonâ€™tseeenoughdeviationexamplesperbatch,sothegradient 20: ğœ‹Ë† ğ‘— â†trainAdversarialAgent(ğœ‹ ğ‘–,ğœ‹Ë† ğ‘—,ğ‘‰ ğ‘–)
signalisveryhighvariance,whichisreflectedinourexperiments.(a)Attack-DefenseGameActionPayoffs (b)CoinDivisionGameActionPayoffs (c)CoinDivisionGameParetoFrontier
Figure2:Power-regularizedobjectivevaluesachievedbydifferentactionsinsmallenvironments.
6.3 Optimizations
Ontheirown,bothPRIMandSBPRcanbesloworunstable,sowe
introduceanumberofoptimizations.
MonteCarloEstimatesofAdversarialPower.Whencom-
putingadversarialpowerforPRIM,weusetheinstantiatedactions
ineachrolloutratherthanthefullpoliciesğœ‹,effectivelyaMonte
Carlo estimate. This is because PPO looks at the advantages of
actionsacrossthebatch,soweneedtobeabletotellwhichactions
incurmoreorlesspower.Policiesdonâ€™tgiveusthisinformation (a)StartingState (b)HighPowerTimestep
becausetheyareconstantthroughoutabatch;wegetnodifferential
information.Asabonus,usingactionsgivesaspeedupbyafactor Figure3:OvercookedClose-Pot-Far-Pot.Agentscanusethe
ofthejointco-playeractionspace. sharedmiddlepotortheirprivatepots.Usingthemiddlepot
LearningtheAdversary.BothPRIMandSBPRrequirefind- isfasterbutincurshighpower(see(b))whereoneagentcan
ingtheco-player ğ‘—â€™sactionthatminimizestheegoagentğ‘–â€™sre- messuptheotherâ€™sworkbyputtinginawrongingredient.
ward.Forenvironmentswithlargeorcontinuousactionspaces,
conductinganexhaustivesearchmaybeintractable,sowelearn bysubtractingthevalueestimateofthestartingstate:
thereward-minimizingaction:theadversarialco-playerğ‘— forego
agentğ‘–istrainedtominimizeğ‘–â€™sreturn:ğ‘ˆ ğ‘—ğ‘– ,ğ‘ğ‘‘ğ‘£(ğ‘ ,ğ‘ ğ‘—)=âˆ’ğ‘… ğ‘–(ğ‘ ,ğ‘)âˆ’
ğ‘ˆ ğ‘—ğ‘– ,ğ‘ğ‘‘ğ‘£(ğ‘ ,ğ‘ ğ‘—)=ğ‘ˆ ğ‘–(ğ‘ )âˆ’ğ‘… ğ‘–(ğ‘ ,ğ‘)âˆ’E[ğ‘ˆ ğ‘–(ğ‘‡(ğ‘ ,ğ‘))]
E[ğ‘ˆ ğ‘–(ğ‘‡(ğ‘ ,ğ‘))]whereğ‘={ğ‘ ğ‘—,ğœ‹ âˆ’ğ‘—(ğ‘ )}.Eachagentmustmaintain whereğ‘={ğ‘ ğ‘—,ğœ‹ âˆ’ğ‘—(ğ‘ )}.
anadversarialmodelofeachco-player.
7 EXPERIMENTS
UsingtheValueFunctiontoApproximateReturnfrom
Rollouts.BothPRIMandSBPRrequirecomputingthevalueof Wefirstvalidateourmethodsinsmallenvironmentswherewecan
statesafteranadversarialco-playerhasacted.Thebenefitofthis computetheoptimalactionsandthenmovetolargerenvironments.
trickistwo-fold:one,itreducesvariancebecauserolloutscanbe
extremelynoisy,especiallyinthebeginningoftraining,andtwo,it 7.1 SmallEnvironments
speedsupruntimesignificantly. We evaluate in the larger version of the Attack Defense Game
DomainRandomization(DR).DRisusefulforspeedingup (payoffmatrixgiveninTable3andpower-regularizedobjective
and stabilizing convergence in both methods. Overcooked is a valuesperactioninFigure2a)andanotherenvironmentcalled
highlysequentialenvironment,requiringalongstringofactions theCoinDivisiongame.Therearefouragents,one"divider"agent
toreceiveareward,soitishelpfultotrainstartingfromrandom (P0),andthree"accepter"agents(P1,P2,andP3).Therearesixbins
statesandlearntheoptimalpolicybackwards.Furthermore,cru- withthefollowingassignmentofagentstobins:([],[P0],[P0,P1],
ciallyforPRIM,DRenablesaccuratevalueestimatesofstatesthat [P0,P2],[P1,P2],[P1,P2,P3]).Thedivideragentmustallocatefive
areoff-policyandthusnormallynotvisited.Thisallowsagentsto coinsamongstthebins.Foreachbin,theagentsassignedtothat
learnhowtorecoverfromadversarialdeviationsandupdatetheir binhavetheoptionofacceptingorrejecting.Ifeveryoneaccepts,
valueestimatesofsuchstatesaccordingly. everyonetakeshomethenumberofcoinsassignedtothatbintimes
NormalizationfortheAdversary.Theadversaryâ€™sobjectiveis thenumberofagentsassignedtothatbin.Ifoneormoreagents
highlydependentonthestartingstatebecauseitonlygetstoactfor reject,thecoinsassignedtothatbinaredestroyed.Weconsiderthe
onetimestep,thusthevalueishighvariancewhichisonlyworsened divideragentâ€™soptimalpolicyandassumethatallaccepteragents
byDR.Wereducevariancebynormalizingtheadversaryâ€™sreward alwaysaccept90%ofthetime(perbin).
SeeFigure2bforthepower-regularizedobjectivevaluesofeach
action(omittingactionswhicharestrictlydominated)andFigure2c(a)PRIMvsSBPRvsTask-OnlyBaseline (b)PRIMAblations (c)AdversaryPolicyConvergence
Figure4:ExperimentalResultsinOvercookedClose-Pot-Far-Pot.Errorbarsarestandarddeviationsover5trials.
Table4:End-of-trainingmetricsinOvercookedClose-Pot-Far-Pot.Errorvaluesarestandarddeviationsover5trials.
Name Taskreward PoweronAgent0 PoweronAgent1 PRObjectiveAgent0 PRObjectiveAgent1
Task-onlybaseline 104.9Â±0.2 217.1Â±47.9 203.8Â±26.1 50.6Â±12.1 53.9Â±6.6
SBPR 94.2Â±0.0 118.4Â±40.3 71.1Â±13.8 64.6Â±10.1 76.4Â±3.4
PRIM 94.4Â±0.1 86.0Â±9.2 76.6Â±18.6 72.9Â±2.2 75.2Â±4.7
PRIMablateadversarynorm 94.3Â±0.1 80.8Â±14.5 59.1Â±17.2 74.0Â±3.5 79.5Â±4.3
PRIMablateadversary 94.5Â±0.2 93.6Â±20.3 97.4Â±27.1 71.1Â±5.1 70.1Â±6.6
PRIMablateVF 0.0Â±0.0 0.0Â±0.0 0.0Â±0.0 0.0Â±0.0 0.0Â±0.0
forthecorrespondingPareofrontier.BothPRIMandSBPRachieve onionagentmakingfewersoups.ThestatedepictedinFigure3b
theoptimalactionsforvaluesofğœ†sampledintherange0to1. ison-policysincethetomatoagentmustmoveupbeforeturning
righttofaceitsprivatepottoplaceitstomatothere.
7.2 Overcooked:Close-Pot-Far-Pot Wecomparetheperformanceofourmethodstothetask-only
WeevaluatebothSBPRandPRIMinOvercooked,a2playergrid- baseline.Wecomputegroundtruthpowerthroughanexhaustive
worldgamewheretheobjectiveistoprepareanddeliversoups searchforthereturn-minimizingactionandconductfullrollouts
accordingtogivenrecipes.Recipesmaycallfortwotypesofin- toevaluateresultingstates.Thisisextremelyslowsoweonlycal-
gredients,tomatoesandonions.Agentsmustcollectandplaceall culateitonceeveryseveralhundredtrainingiterations.Ingeneral
ingredientsinapotoneatatime,cookthesoup,grabaplate,place rolloutsarehighvariancesomultipletrialsshouldbeperformed,
thefinishedsoupontotheplate,andfinallydeliverthesoup. butsinceouragentsconvergetowardsdeterministicpoliciesinour
Theactionspaceis{N,E,S,W,STAY,INTERACT}.Dependingon environments,wesimplydeterminizethepolicieswhenrollingout.
wheretheagentisfacing,INTERACTcanmeanpickupaningredient, Forğœ†=0.25,Figure4ashowsthatPRIMoutperformsthebase-
placeaningredientintoapot,startcooking,pickupadish,place lineofoptimizingforjustthetaskreward.PRIMalsoperforms
soupontothedish,ordeliverasoup.Itâ€™simpossibletoremove betterthanSBPRforoneagentduetoitsinherentlylowervariance
ingredientsfromapotoncetheyareplaced. trainingdatawhichmakesthelearningproblemeasier.
Wedesignalayout"Close-Pot-Far-Pot"withtworecipes,3toma- Nextweranaseriesofablationexperimentstobetterunderstand
toesor3onions,eachgivingğ‘… reward.Thetopagentcanonly PRIM, shown in Figure 4b. Ablating the learned adversary and
accessonionsandthebottomagentcanonlyaccesstomatoes.Each insteadconductinganexhaustivesearchovertheactionspacedid
agentcanaccesstwopots,onesharedinthecenterandtheother notmakemuchdifferenceontheobjectivevalueachieved.Thisis
isprivate,inaccessibletotheotheragent,butfurther.Theagents expected;thegoaloflearningtheadversaryissimplytospeedup
sharearewardfunctionandatrajectoryisğ‘‡ steps. thepowercomputation:ratherthaniteratingovertheactionspace,
Inourexperimentswesetğ‘‡ =105andğ‘…=20.Anassemblyline wepaya"fixedcost"totrainandquerytheadversary.Thisisis
(strategy1)usingthemiddlepotcanproduce7soups,oneagent necessaryinenvironmentswithlargeactionspaces.
independentlyusingthemiddlepotandtheotherusingtheirprivate Ablating normalization for the adversaryâ€™s objective did not
potcanproduce9soups(strategy2),andbothagentsindependently significantlychangetheobjectivevalueachieved,butitdidhurtthe
usingtheirprivatepotscanproduce8soups(strategy3). adversaryâ€™sconvergence.Figure4cdepictsthepoorconvergence
Strategy2maximizestaskrewardbutincurshighadversarial forthestateinFigure3bwheretheoptimalactionisINTERACT.
power:asshowninFigure3b,thetomatoagentcanmessupthe Finallyweablatedtheuseofvaluefunctiontoapproximatethe
onionagentâ€™ssoupbyputtinginawrongingredient,leadingtothe returnfromarollout.Weranthisexperimentforthesameamount(a)PRIMvsBaseline(Explosion) (b)PRIMvsSBPR(Explosion) (c)PRIMvsSBPR(Explosion)
Figure5:ComparisonofPRIM,SBPR,andTask-OnlyBaselineinOvercookedExplosionwithğœ†=0.0001.Insomerunsonlyone
agentisvisiblebecausetheplotscoincidecompletely;thepowersincurredaretoosmalltobedistinguishableaftermultiplying
byğœ†.Errorbarsarestandarddeviationsover5trialsexceptforSBPRinteractoraclewhichonlyhas3trials.
Table5:ExperimentalResultsinExplosionEnvironment.Errorvaluesarestandarddeviationsover5trialsexceptforSBPR
interactoraclewhichonlyhas3trials.
Name Taskreward PoweronAgent0 PoweronAgent1 PRObjectiveAgent0 PRObjectiveAgent1
Task-onlybaseline 104.7Â±0.4 139949.5Â±171647.1 242363.6Â±182484.0 90.7Â±17.3 80.5Â±18.1
SBPR 105.0Â±0.0 297351.0Â±148585.0 78369.8Â±156414.3 75.2Â±14.8 97.1Â±15.7
SBPRINTERACTadversary 65.5Â±16.2 174.0Â±176.0 217.8Â±212.6 65.5Â±16.2 65.5Â±16.2
PRIM 94.2Â±0.0 138.4Â±2.7 132.3Â±40.5 94.2Â±2.2 94.2Â±0.0
oftimeastheotherexperiments,butitwassoslowitwasonly AsshowninFigure5c,SBPRreliesontheadversarynotyet
abletofinish2e6agentstepsandachieved0onthetaskreward. convergingatthebeginningbecausethisallowstheagentstosolve
WesummarizetheresultsofourexperimentsinTable4. enoughoftheexplorationproblembeforeconsistentlyincurring
thepenalty.Replacingtheadversarywithanagentthatalways
7.3 Overcooked:Explosion playsINTERACT(aninteractoracle)causesSBPRtofail.
WesummarizetheExplosionresultsinTable5.PRIMistheonly
IntheClose-Pot-Far-Potlayout,anadversarialdeviationdoesnot
methodthatavoidsincurringcatastrophicallyhighpoweratthe
havelargeconsequences,butpowerregularizationmaybemoreuse-
costofabitoftaskreward.
fulinhighstakesevents.WecreateavariantofClose-Pot-Far-Pot
calledExplosionwhereweinterprettheingredientsaschemicals,
8 CONCLUSION
thepotsastesttubes,andtherecipesaschemicalformulas.Ifunlike
chemicalsaremixedtogether,adangerouschemicalreactioncauses Wedefinedanotionofpoweramenabletooptimizationandshowed
anexplosionwhichincursanimmediatepenaltyofğ‘ƒ =âˆ’100,000. thatequilibriaalwaysexistwhenagentsregularizeforpower.Next,
Figure5acomparesthetaskrewardonlybaselinetoPRIMwith wepresentedtwoalgorithms,SampleBasedPowerRegularization
ğœ† = 0.0001.Notethatthebluelineishiddenbeneaththeorange (SBPR)andPowerRegularizationviaIntrinsicMotivation(PRIM).
line.PRIMconvergestoverylowvariancewhilethebaselinehas Wevalidateourmethodsinaseriesofsmallenvironmentsand
highvariance.Thisisdueinlargeparttothefactthatagents0and intwovariantsofOvercooked,showingthatbothmethodsguide
1mayswitchrolesinwhousesthesharedpotsoeitheragentmay agentstowardlowerpowerbehavior.SBPRissimplerbutPRIMis
incurthelargepowerpenalty. betterabletohandleverylowvaluesofğœ†.
NowweexamineSBPRâ€™sperformance(seeFigure5b).Weex- Therearemanyavenuesforfuturework,includingexploring
pectedSBPRtofailsincetheprobabilityofadeviationğ‘ =0.0001 different definitions of power (empirically and philosophically)
issolowyettheexplosionpenaltyğ‘ƒ = âˆ’100,000issohigh,but andmodelingmultipletimestepdeviations.Ourtheoreticalresults
theobservedperformancewasbetterthanexpected.However,a holdforgeneral-sumgamesbutwehavenotexploredgeneral-sum
significantamountofhyperparametertuningwasnecessary:we gamesempirically.
adjustedthePPOclipparamandmaximumgradnormdownto0.1
andlengthenedtheentropyschedule.Dependingontheparticular ACKNOWLEDGMENTS
hyperparametervalues,theagentswouldeitherfailtooptimizefor Wearegratefulforinsightfulconversationsfromthemembersof
poweratallorwouldconvergeonanassemblylinethatavoidsthe the Center for Human-Compatible AI, including Micah Carroll,
explosionrisk(butissuboptimaltoPRIMâ€™ssolution). NiklasLauffer,AdamGleave,DanielFilan,LawrenceChan,andSamToyer,aswellasDerekYenfromMIT.Wearealsogratefulfor [14] EdwardHughes,JoelZLeibo,MatthewPhillips,KarlTuyls,EdgarDueÃ±ez-
fundingofthisworkasagiftfromtheBerkeleyExistentialRisk Guzman,AntonioGarcÃ­aCastaÃ±eda,IainDunning,TinaZhu,KevinMcKee,
RaphaelKoster,etal.2018.Inequityaversionimprovescooperationinintertem-
Initiative.
poralsocialdilemmas. Advancesinneuralinformationprocessingsystems31
(2018).
REFERENCES [15] NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,Pedro
Ortega,DJStrouse,JoelZLeibo,andNandoDeFreitas.2019.Socialinfluenceas
[1] NatashaAlechina,JosephYHalpern,andBrianLogan.2020.Causality,responsi- intrinsicmotivationformulti-agentdeepreinforcementlearning.InInternational
bilityandblameinteamplans.arXivpreprintarXiv:2005.10297(2020). conferenceonmachinelearning.PMLR,3040â€“3049.
[2] SamuelBarrett,PeterStone,andSaritKraus.2011.Empiricalevaluationofad [16] JernejKosandDawnSong.2017.Delvingintoadversarialattacksondeeppolicies.
hocteamworkinthepursuitdomain.InThe10thInternationalConferenceon arXivpreprintarXiv:1705.06452(2017).
AutonomousAgentsandMultiagentSystems-Volume2.567â€“574. [17] JoelZLeibo,ViniciusZambaldi,MarcLanctot,JanuszMarecki,andThoreGraepel.
[3] MicahCarroll,RohinShah,MarkKHo,TomGriffiths,SanjitSeshia,PieterAbbeel, 2017.Multi-agentreinforcementlearninginsequentialsocialdilemmas.arXiv
andAncaDragan.2019.Ontheutilityoflearningabouthumansforhuman-ai preprintarXiv:1702.03037(2017).
coordination.Advancesinneuralinformationprocessingsystems32(2019). [18] Yen-ChenLin,Zhang-WeiHong,Yuan-HongLiao,Meng-LiShih,Ming-YuLiu,
[4] HanaChocklerandJosephYHalpern.2004. Responsibilityandblame:A andMinSun.2017.Tacticsofadversarialattackondeepreinforcementlearning
structural-modelapproach.JournalofArtificialIntelligenceResearch22(2004), agents.arXivpreprintarXiv:1703.06748(2017).
93â€“115. [19] MichaelLLittman.1994.Markovgamesasaframeworkformulti-agentrein-
[5] VirginiaDignumandFrankDignum.2006.Coordinatingtasksinagentorgani- forcementlearning.InMachinelearningproceedings1994.Elsevier,157â€“163.
zations.InInternationalWorkshoponCoordination,Organizations,Institutions, [20] MichaelLLittmanetal.2001.Friend-or-foeQ-learningingeneral-sumgames.
andNormsinAgentSystems.Springer,32â€“47. InICML,Vol.1.322â€“328.
[6] JakobNFoerster,RichardYChen,MaruanAl-Shedivat,ShimonWhiteson,Pieter [21] StevenLukes.2021.Power:Aradicalview.BloomsburyPublishing.
Abbeel,andIgorMordatch.2017.Learningwithopponent-learningawareness. [22] MichaelMann.2012.Thesourcesofsocialpower:volume1,ahistoryofpowerfrom
arXivpreprintarXiv:1709.04326(2017). thebeginningtoAD1760.Vol.1.Cambridgeuniversitypress.
[7] MeirFriedenbergandJosephYHalpern.2019.Blameworthinessinmulti-agent [23] JackSerrino,MaxKleiman-Weiner,DavidCParkes,andJoshTenenbaum.2019.
settings.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.33. Findingfriendandfoeinmulti-agentgames. AdvancesinNeuralInformation
525â€“532. ProcessingSystems32(2019).
[8] TobiasGerstenberg,JosephYHalpern,andJoshuaBTenenbaum.2015.Respon- [24] PeterStone,GalA.Kaminka,SaritKraus,andJeffreyS.Rosenschein.2010.Ad
sibilityjudgmentsinvotingscenarios..InCogSci. HocAutonomousAgentTeams:CollaborationwithoutPre-Coordination.In
[9] AdamGleave,MichaelDennis,CodyWild,NeelKant,SergeyLevine,andStuart ProceedingsoftheTwenty-FourthConferenceonArtificialIntelligence.
Russell.2019.Adversarialpolicies:Attackingdeepreinforcementlearning.arXiv [25] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,
preprintarXiv:1905.10615(2019). IanGoodfellow,andRobFergus.2013.Intriguingpropertiesofneuralnetworks.
[10] DavideGrossi,FrankDignum,VirginiaDignum,MehdiDastani,andLÃ mber arXivpreprintarXiv:1312.6199(2013).
Royakkers.2006.Structuralaspectsoftheevaluationofagentorganizations.In [26] JohannesTreutlein,MichaelDennis,CasparOesterheld,andJakobFoerster.
InternationalWorkshoponCoordination,Organizations,Institutions,andNormsin 2021.Anewformalism,methodandopenissuesforzero-shotcoordination.In
AgentSystems.Springer,3â€“18. InternationalConferenceonMachineLearning.PMLR,10413â€“10423.
[11] JosephHalpernandMaxKleiman-Weiner.2018.Towardsformaldefinitionsof [27] AlexTurner,NealeRatzlaff,andPrasadTadepalli.2020.Avoidingsideeffectsin
blameworthiness,intention,andmoralresponsibility.InProceedingsoftheAAAI complexenvironments.AdvancesinNeuralInformationProcessingSystems33
ConferenceonArtificialIntelligence,Vol.32. (2020),21406â€“21415.
[12] HengyuanHu,AdamLerer,AlexPeysakhovich,andJakobFoerster.2020.â€œOther- [28] AlexanderMattTurner,DylanHadfield-Menell,andPrasadTadepalli.2020.
Playâ€forZero-ShotCoordination.InInternationalConferenceonMachineLearn- Conservativeagencyviaattainableutilitypreservation.InProceedingsofthe
ing.PMLR,4399â€“4410. AAAI/ACMConferenceonAI,Ethics,andSociety.385â€“391.
[13] SandyHuang,NicolasPapernot,IanGoodfellow,YanDuan,andPieterAbbeel. [29] AlexanderMattTurner,LoganSmith,RohinShah,AndrewCritch,andPrasad
2017. Adversarial attacks on neural network policies. arXiv preprint Tadepalli. 2019. Optimal Policies Tend to Seek Power. arXiv preprint
arXiv:1702.02284(2017). arXiv:1912.01683(2019).