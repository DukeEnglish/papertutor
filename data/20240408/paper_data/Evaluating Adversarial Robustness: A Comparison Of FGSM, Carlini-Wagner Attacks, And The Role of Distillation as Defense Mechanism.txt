Evaluating Adversarial Robustness: A
Comparison Of FGSM, Carlini-Wagner
Attacks, And The Role of Distillation As
Defense Mechanism
A CapstoneProjectreport submitted inpartial fulfillment ofthe requirementsfor the Post
GraduateProgram inData ScienceatPraxisTech School,Kolkata, India
By
TrilokeshRanjan Sarkar(Roll No:A23047)
Nilanjan Das(Roll No:A23028)
PralaySankar Maitra (RollNo: A23029)
Bijoy Some(Roll No:A23013)
Ritwik Saha (RollNo: BM22038)
Orijita Adhikary (Roll No:BM22053)
Bishal Bose(Roll No:A23015)
of Data Science2023FallBatch
Under the supervision of
Prof.Jaydip Sen
Praxis Tech School, Kolkata, India
11. Introduction
As artificial intelligence progresses, concerns over adversarial attacks - deliberate
manipulations of machine learning models through altered data - are mounting. This article
delves into the investigation of adversarial attacks specifically targeting Deep Neural
Networks (DNNs) used for image classification. We focus on understanding the impact of
two prominent attack approaches: the Fast Gradient Sign Method (FGSM) and the
Carlini-Wagner (CW) approach, on three pre-trained image classifiers. Adversarial attacks
pose a significant threat to the reliability and security of machine learning systems,
particularly in critical applications like image classification. These attacks involve subtly
manipulating input data to cause misclassification by the model, leading to potentially
harmful consequences [21]. The FGSM attack is awidely studiedmethodthat perturbs input
images by leveraging gradients of the loss function to generate adversarial examples
efficiently[1].
However, the CW approach represents a more sophisticated class of attacks, developed by
Carlini and Wagner, which surpasses earlier techniques in terms of success rates with
minimal perturbations [6]. The CW attack formulates the adversarial example generation as
an optimization problem, utilizing powerful gradient-based algorithms such as L0, L2, and
L‚àû norms, to find thesmallestperturbation necessaryfor misclassification[22]. Specifically,
the L0 norm counts the number of non-zero elements in the perturbation, the L2 norm
measures the Euclidean distance between the original and perturbed input, and theL‚àû norm
calculates the maximum absolute difference between corresponding elements of theoriginal
and perturbed input [23]. This advanced nature of the CW attack presents additional
challenges for defense tactics, as it can bypass traditional defense mechanisms designed to
mitigate attackslikeFGSM [24].
In the face of escalating adversarial threats, the research community has been actively
exploring various defense mechanisms to bolster therobustness of machinelearningmodels.
Defensive distillation, a technique that trains a model on softened probabilities from a
pre-trained model, has emerged as one such strategy. This approach aims to enhance the
model‚Äôs resilience against adversarial perturbations. However, evaluations indicate thateven
2with this modified defensive distillation method, there is a notable decline in performance
when contending with sophisticated attacks like the CW attack [11]. While defensive
distillation has demonstrated potential in mitigating simpler attacks such as the FGSM, its
efficacyisreduced againstmorecomplextechniques [1].
The intricacies of the CW attack highlight the necessity for defense mechanisms that can
effectively counteract gradient-based optimization strategies. This may entail the
development of innovative defense strategiesthat take intoaccountthe uniquecharacteristics
of advanced attacks like CW and adaptively modify the model‚Äôs architecture or training
process to improve resilience [6]. In summary, adversarial attacks onDeepNeuralNetworks
(DNNs) for image classification pose a formidable challenge in artificial intelligence.
Defense mechanisms like defensive distillation provide a measure of protection but are not
infallible and may falter against sophisticated techniques like CW. Future research should
concentrate on devising more robust defense strategies that can counter the effects of
advanced adversarial attacks, thereby ensuring the reliability and security of machine
learningsystemsin critical applications[8].
2. Related Work
Significant progress has been made in several domains since the introduction of deep
learning, such as autonomous systems, natural language processing, and picture and audio
recognition. The foundation of these advancements hasbeen establishedbyneuralnetworks,
especially Deep Neural Networks (DNNs), which are capable of learning intricate patterns
and producing remarkably accuratepredictions.Nonetheless,thesenetworks'vulnerability to
hostile attackspresents aseriousthreat totheirdependability andsecurity.
Types of AdversarialAttacks
1. FGSM Attack: The FGSM [1] assault was first presented as a straightforward but
powerful way to produce adversarial examples. To cause misclassification, the approach
uses the gradient of the loss function concerning the input data to perturb the input in a
direction that maximizes the loss. Deep neural networks (DNNs) have flaws when they
show how subtle changes to input photos could cause misclassification [2]. They
emphasized the necessity of resilience to these hostile assaults. Extending FGSM to the
3Jacobian-based Saliency Map Attack (JSMA) [3] introduced a more focused method for
producing adversarial cases. They underlined how crucial it is to assess how resilient
machine learningmodels aretohostileattacks.
2. Patch Attack: Patch attacks include applying a carefully designed patch to an image,
which can lead to the patched image being incorrectly classified by state-of-the-art
classifiers. These attacks pose a threat to real-world systems sincetheyarenotlimited to
being effective in digital space, but they may also be physically executed. In many
real-world applications, such as driverless carsandmedical diagnostics, imageclassifiers
are essential. Recent studies have brought to light these systems' weaknesses, notably in
relation to their vulnerability to hostile attacks. Patch assaults are one type ofattackthat
has gained a lot of attention lately. These attacks involve intentionally placing small
patches that might lead to misinterpretation. In this study, we look at relevant work on
patch attacks on image classifiers, withanemphasis onthetechniques createdto produce
thesekindsof attacks.
1. Single-Object Patch Attacks: This method involves adding a tiny patch to a
picture, usually aimed at a singleobjectin thescene.Evolutionarymethodscould
be used to maximize misclassification while minimizing visual distortion in the
patch. [4].
2. Universal Patch Attacks: Universal Adversarial Perturbations (UAPs) are
designed to develop patches that can misleadaclassifieracross several photos,as
opposed to optimizing patches for specific images. UAPs [5], also showed how
well they worked to produce subtle perturbations that led to themisclassification
of a varietyof images.
3. CW Attack: An adversarial strategy noted for its success against models that have
defensive measures inplaceis theCW attack,whichis optimization-based.It deliberately
creates small-scale perturbations with great care that are intended to produce
misclassifications with high confidence. The CW attack was first presented by Carlini
and Wagner, who also showed how well it worked to produce adversarial samples for a
variety of machine-learning models. They put forth an optimization-based method to
identify the smallest perturbations that result in misclassification, and it allows the
attack's intensity to be adjusted using various distance measures [6]. By putting forththe
robust optimization framework, [7] expanded on the CW approach and improved the
transferability of adversarial examples across other models and datasets. To improve the
4resilience of created adversarial examples,they introducedanovel objectivefunction that
promotes the perturbations to lie within a narrow zone surrounding the original input.
Adversarial training has been studied as a defencemechanism againstadversarial attacks
to improverobustness[8]. Theytrained modelsusingadversariallyaltered samples. Their
research demonstrated how well adversarial training using projected gradient descent
may strengthen deep learningmodels' defensesagainst arangeofattacks, suchas theCW
attack. By adding a momentum factor to speed up convergence during optimization,
improved upon previous attacks like CW and introduced the momentum iterative
approach [9]. Compared with conventional iterative methods, their methodology
achieved greater success rates and indicated enhanced effectiveness in producing
adversarial examples. To give a thorough assessment of model vulnerabilities, an
ensemble-based approach for evaluating adversarial robustness was proposed [10]. This
approach combined various varied attacks, including CW. By taking into account a
variety of attack techniques, their method enables a more dependable assessment of
model robustnesswithout the needfor extrahyperparameters.
4. Other Adversarial Attacks: "DeepFool is a simple and accurate method to fool deep
neural networks." To minimize disturbance while misclassifying an image, this method
iteratively moves the input image in the direction of the decision boundary [19]. An
effective iterative adversarial assault technique for creating adversarial instances is the
Projected Gradient Descent (PGD) assault. Subjectto amaximumpermittedperturbation
size, which is usually expressed in terms of the L‚àû norm, the goal is to identify the
perturbation thatmaximizesthe lossfunction [8].
Defensive Strategies
The tactics used to fight off hostile attacks change along with them. Numerous protective
strategieshavebeendeveloped inthearea toincreaseneuralnetworks'resilience.
Defensive Distillation: By using soft labels from a previously trained model to train a new
model, a technique known as defensive distillation can strengthen the model's resistance to
attacks such as FGSM [31]. The model's output surface is effectively smoothed by this
approach, which makes it more difficult for gradient-based attacks to identify successful
perturbations. A methodcalled"defensive distillation"was putforth[11] tostrengthen neural
networks' resistance to hostile attacks. A model is trained using softened labels created by
another model that was trained using the same set of data. Its effectiveness has been
5questioned, though, in light of more complex attacks like the CW attack, which contrasts
with its triumph over less complex ones likethe FGSM.Defensive distillationwaspresented
[11] to strengthen neural networks' resistance to hostile attacks. They suggested using
temperature scaling to train asecond modelwith softenedlabelsproducedbythe first model,
hence lowering prediction confidence. At first, this approach seemedto havethe potentialto
strengthen the model's resistance to attacks such as FGSM.In-depth testswere carriedoutto
assess defensive distillation's resilience to hostile assaults [12]. Defensivedistillation proved
to be ineffectual against more complex attacks such as the CW attack, but it proved to be
resilient against less complex ones like FGSM. The limitations of defensive distillation in
countering sophisticated hostile methods were brought to light by this study. This work
investigated deep learning models' vulnerabilities in hostile environments in more detail.
They discovered that robustness against adaptive adversaries using strategies like the CW
attack is not provided by defensive distillation. The necessity for stronger defense systems
that can withstand cunning adversary tactics was highlighted by this study. Defensive
distillation faces a major challenge fromthe CWassault, whichwasfirst described inCarlini
and Wagner's groundbreaking work on evaluating neural network resilience. Their research
exposed defensivedistillation'sshortcomingsin actualadversarial situationsbyshowing that,
although strong against less sophisticated attacks, it is not a reliable defense against
optimization-based attacks suchas CW [6].
To sum up, defensive distillation has been demonstrated to be ineffective against more
complex attacks like the CW attack, whilst originally showing promise in boosting model
resilience against adversarial attacks like FGSM [32]. The significance of creating more
all-encompassing defense mechanisms to lessen deep learningmodels'weaknesses inhostile
environmentsishighlighted bythesefindings.
Datasets
‚óè MNIST: MNIST is a commonly used dataset made up of handwritten numbers (0‚Äì9) in
28x28 grayscale photos. Because of its simplicity and ease of experimentation, it is a
popular choice for benchmarking image classification algorithms. It contains 10,000
testing photosinadditionto60,000training images[15].
6Fig. 1:MNIST Handwritten Dataset
‚óè CIFAR10: There are 60,000 32x32 color images in CIFAR-10, with 6,000 images in
each of the ten classes. It is harder for picture classification jobs since it covers a wider
rangeof things than MNIST,suchas automobiles, animals,andcommon objects[16].
Fig. 2:CIFAR-10 Dataset
7‚óè CIFAR100: CIFAR-100 is an expansion of CIFAR-10 that has 600 photos in each class
and 100 classes in total. Every image retains itsRGB format anddimensions of32by32
pixels. In comparison to CIFAR-10,CIFAR-100 offersa higherdegreeofdifficulty and a
wider rangeof objectclasses [16].
‚óè ImageNet: One of the biggest and most used datasets for image classification
applications is ImageNet. It has more than 20,000 categories and more than 14 million
photos. Deep learning models may be trained and evaluated on a huge scale because of
thedataset'sextensiverangeof objectsandsituations [17].
‚óè Tiny-ImageNet: With 200 object classes and 500 training photos per class, Tiny
ImageNet is a condensed version of the original ImageNet dataset. Every image has a
size of 64 by 64 pixels. By acting as a bridge between larger ImageNet and smaller
datasets such as CIFAR, tiny ImageNet offers a more difficult standard for image
classification applications[18].
Fig. 3: ImageNetDataset
83. Methodology
In addition to examining the feasibilityof DefensiveDistillationas apotentialdefense tactic,
the study conducted a thorough investigation into the vulnerability of three CNN models,
Resnext50_32x4d, DenseNet201, and VGG19, to adversarial attacks. We can outline the
research approachusedinthis study usingastep-by-step breakdown.
Initially, PyTorch's torchvision package, which made pre-trained CNN models accessible,
was used torigorously evaluatethe models.TheTiny ImageNetdataset wasused tocarefully
evaluate these models to define baselines for essential performance. The study sought to
measure the models' intrinsic abilitytoperform picture classificationtasksby computingkey
classification accuracy metrics, such as Top-1 and Top-5mistakes. Furthermore,a portion of
the photos was carefully chosen for close examination, providing insightful information
aboutthemodels' decision-making proceduresandclassificationresults.
The study next turned its attention to adversarial attacks, utilizing two well-known
techniques: the CW attack and the FGSM. The study aimed to investigate the effect of
perturbation magnitude on model susceptibility bysystematicallyaltering theepsilonvalues,
rangingfrom 1% to10%, for bothattackapproaches. Byinvestigatingclassification accuracy
metrics in detail and documenting classificationmistakes at variousepsilonvalues,thestudy
soughtto revealthecomplex behaviorof themodelsunder adversarial pressure.
Additionally, the research investigated Defensive Distillation as a possible countermeasure
against hostile assaults. A ResNet101 model was first trained on the CIFAR-10 dataset to
capture a plethoraof knowledgeusing ateacher-studentstructure.Later, this informationwas
condensed into a smaller student model, the Resnext50_32x4d architecture. Through the
evaluation of the student model's accuracy before and during FGSM attacks at different
epsilon values, the research examined the effectiveness of Defensive Distillationinreducing
thenegative consequences ofadversarial perturbations.
To put it briefly, the study technique used here was multimodal, ranging from a thorough
model review to the simulation of adversarial attacks and the evaluation of defense
mechanisms that followed. Thegoalof thestudy wastoobtain important insightsabout CNN
architectural weaknesses and defense strategy efficacy in the complex field of deeplearning
through thesepainstakingly constructedassessments.
9Fig. 4: Flowchartdepicting the methodology sectionofourresearch project
4. Background Theories and Model Architecture
FGSM Attack
The FGSM [1] assault is a straightforward yet powerful way toproduceadversarial cases.It
makes use of the loss function's gradients to produce perturbations that maximize loss and
causemisclassification.
Given an input image x, aneural networkmodel f withparameters Œ∏,aloss functionL, anda
small perturbationmagnitude œµ,the adversarialexample ùë• is computedas:
ùëéùëëùë£
ùë• = ùë• + œµ‚ãÖùë†ùëñùëîùëõ(‚àá ùêø(ùëì(ùë•;Œ∏),ùë¶ ))
ùëéùëëùë£ ùë• ùë°ùëüùë¢ùëí
Where,
10‚óè ùë• istheadversarialimage,
ùëéùëëùë£
‚óè xisan originalimage,
‚óè œµ isasmall scalarrepresenting themagnitudeof theperturbation,
‚óè ‚àá ùêø(ùëì(ùë•;Œ∏),ùë¶ isthe gradientof theloss functionL concerningthe inputimagex,
ùë• ùë°ùëüùë¢ùëí
‚óè ùë¶ istheproperlabelof theinput image,
ùë°ùëüùë¢ùëí
‚óè sign(‚ãÖ) denotes the sign function, which extracts the sign of each element of the
gradientvector.
CW Attack
The CW attack is an adversarial attacktechniquethat solvesanoptimizationissuetoprovide
undetectable adversarial samples. It seeks to identify the lowest perturbation that causes
misclassification while still being undetectable under a given distance measure. Through an
iterative optimization process that strikesa compromisebetweenperturbation magnitudeand
misclassification loss, the C&W approach produces adversarial samples that are highly
effective andchallengingfor neuralnetworksto identify.
The theory behindtheCW attack[6]wasexplained as,lettinga perturbation,representedas Œ¥
, is introduced for a given image x,aimingtominimize thedistance metric D(x;x+Œ¥)when
added to x. This perturbation is subject to the condition that the resulting image x + Œ¥ is
classified as the target classt.Theobjective isto achieveasubtlemodificationof xtoinduce
a change in its classification while ensuring the perturbed image remains recognizable and
valid. However, solving this problem directly is challenging due to the complex non-linear
constraintC(x+ Œ¥) = t.
Totackle thischallenge, analternativeapproach isproposed.In thisformulation, anobjective
function is defined, combining the distance metric D(x; x + Œ¥)with aregularizationtermf(x
+ Œ¥), scaled by a positive constant c. This modified formulation aims to simplify the
optimization problem. Importantly, the equivalence between the original and alternative
formulations suggests the existence of an appropriate constant c ensuring the optimal
solutionof thealternative problemcorrespondstothat ofthe originalone.
11By substituting the distance metric D with an Lp norm, the problem is reformulated as
minimizing (Œ¥+ c*f(x + Œ¥)), wherepindicates the chosennorm.
So inbrief after substitutingthe distanceD withanLp norm, theissuetransforms into:
ùëöùëñùëõùëñùëöùëñùëßùëí |Œ¥| + ùëê ¬∑ ùëì(ùë• + Œ¥)
ùëù
subjecttotheconstraint:
ùëõ
ùë• + Œ¥ ‚àà [0,1]
where:
-|Œ¥| denotesthe ùêø normof Œ¥,
ùëù ùëù
-ùëê isapositiveconstant,
-ùëì(ùë• + Œ¥)representstheregularizationterm,
ùëõ
-ùë• + Œ¥belongstothevalidimagespace [0, 1] .
Defensive Distillation
Defensive Distillation was presented asadefense mechanism tostrengthen machinelearning
models' resilience against adversarial attacks. Instead of using the hard labels directly, it
entails training a model using a softened version of the output probabilities produced by a
pre-trained model. The field of knowledge distillation, where the goal is to transfer
knowledge from a large, complicated model (teacher) to a smaller, simpler model (student),
iswhere theterm"distillation"originated.
Thetwoprimary steps inthe defensivedistillation trainingmethod areasfollows:
‚óè Pre-training: Using conventional supervised learning methods, asizable, well-trained
model (called the teacher model) is first trained on therelevant dataset. Thismodel provides
the knowledge base for the defensive distillation procedure since it has been trained to
generate precisepredictions onthedataset.
12‚óè DistillationTraining:After that,a smallermodel(thedistilled model)is trainedonthe
same dataset, but it does so by using the teacher model's softened probabilities as training
inputs rather than the actual hard labels. By adding atemperature parameterto theinstructor
model's softmax output, these softened probabilities are produced, which leads to a more
diffuse and smooth probability distribution. Based on these softer probabilities, the distilled
model iseducatedto imitate theinstructormodel's actions.
The objective during defensive distillation training isto minimizethe overallloss, whichis a
combination of the classification loss and the distillation loss. Mathematically, the training
objectivecanbe expressedas:
TotalLoss =Classification Loss +Œª √óDistillationLoss
Where Œª is a hyperparameter that controls the importance of the distillation loss relative to
theclassification loss.
Defensive distillation is justified by the fact that the smoothed probabilities give the student
model a more reliable and steady signal to work with, which reduces its susceptibility to
minute changes in the input data causedbyhostileattacks. Thestudentmodel becomesmore
tolerant to small fluctuations that may arise from adversarial perturbations by learning to
focusonthemost prominent characteristicsofthe data bytrainingonthe softerprobabilities.
ResNext50_32x4dModel
The convolutional neural network (CNN) model Resnext50_32x4d [25] contains 50 layers
and 32 x 4 dimensions. This Model belongs totheResNeXtfamily, whichisan extensionof
the ResNet (Residual Network) architecture. The "Next" in ResNeXt [28] refers to the
concept of "Next" or "Next Dimension." This is achieved through the introduction of a new
module called a "cardinality" module, which incorporates grouped convolutions. The
ResNeXt-50 model is constructed byatemplate withcardinality= 32andbottleneck width=
4d. The "32x4d" part of the model name refers to the cardinalityandbase widthparameters.
Cardinality refers to the number of groups in the grouped convolutions. In
ResNeXt50_32x4d, the cardinalityisset to32,meaningthat each convolutionallayer divides
its input into 32 groups andperformsseparateconvolutionoperations withineach group, and
13"4d" represents the base width of the bottleneck layer. Thebottlenecklayerhas abase width
of 4, meaning each group in the bottleneck layer has 4 channels. Since the cardinalityis 32,
there are32groupsin the bottlenecklayer.
Fig. 5:Resnext50_32x4d networkarchitecture
DenseNet201 Model
The DenseNet-201 [24,27] consists of a 201-layer convolutional neural network with
20,242,984 parameters. In DenseNet-201 [30], there are 98 blocks of densely connected
layers, including both 1x1 and 3x3 convolutional layers. A globally average pooling layer
and a fully connected layer come after theseblocks.Thenetwork is pre-trained,having been
trained on more than a million photos, using the ImageNet database Images of objects such
as keyboards, mice, pencils, and other animals will be classified by the network into 1000
distinct categories. Consequently, the network has picked up comprehensive feature
representations for a range of picture kinds. The key idea behind DenseNet is to allow each
layer to directly access the outputs of all preceding layers, making information flow more
efficiently. This approach reduces the number of parameters needed compared to traditional
CNNs, which helps save memory and speed up computations. In the DenseNet201 model,
the last layer utilizes a SoftMax activation to ascertain the classification class. Even though
DenseNet's structure might seem complex, it offers better performance in tasks like image
recognition.
14Fig. 6: DenseNet201 networkarchitecture
VGG 19Model
The VGG [23] architecture is a well-known model for its ability to classify images
effectively. There are five blocks including sixteen convolution layers in this model.
Following each block comes the Maxpool layer, which compresses the inputimage's sizeby
two and doubles the filters of the convolution layer. It uses blocks of convolutional layers,
where each block includes 3x3 filters, 1x1 padding, and 2x2 max-pooling. VGG-19[26], a
version with 19 layers, stands out with 143 million parameters, setting a standard for CNN
performance. These convolution layers extract features from images, enhanced by ReLU
activation for recognizing complex patterns. Max-pooling then reduces complexity while
keeping important details. Dropout layers help prevent overfitting by randomly turning off
some neurons during training. ReLU activation also helps by addressing the vanishing
gradient problem, making training more efficient. In this model, the last layer typically
consists of a SoftMax activation function. The SoftMax layer takes the output of the
preceding fully connected layers and computes the probabilities for each class, enabling the
model to classify images effectively based on the highest probability class. Finally, these
fully connected layers at the end allow for high-level decision-making and accurate
classification basedonlearned features.
15Fig. 7: VGGnetwork architecture
5. Performance Results and Analysis
Resultsof FGSM and CW AttacksandDefensive Distillation
The objective of this research was to assess the impact of two distinct adversarial attack
methods,namelyFGSM andCW attack,onthreewidely usedpre-trained CNNarchitectures:
resnext50_32xd, Densenet201, and VGG19. These architectures, readily accessible in
PyTorch's torchvision package, were initially trained on the ImageNet dataset. Additionally,
the study explored the efficacy of Defensive Distillation in mitigating the effects of these
attacks. Defensive Distillation, a method aimed at enhancing model robustness against
adversarial perturbations, was examined as a potential defense strategy in this context. The
investigation sought to provide insights into thevulnerabilitiesof theseCNNmodelsandthe
effectivenessof DefensiveDistillationinbolstering theirresilience toadversarial attacks.
16Classification PerformanceBefore Attack
We chose to use the Tiny ImageNet dataset, which has 200 classes, for our researchbecause
the ImageNet dataset has 1000 classes. We took this decision in order to better meet our
computing limitations and research goals. Since it would not always be possible to assign a
single, unique label to a picture from the Tiny ImageNet classes, weusedbothtop 1andtop
5 accuracy scores. The percentage of successfully categorized images, when the top
prediction made by the model matchesthe actuallabel,is indicatedbythe top1accuracy.To
give a more complete picture of the model's predictive ability, we also evaluated the top 5
accuracy because of the wider range of possibilities found in the top 5 forecasts. If the true
label occurs in any of the top 5 projected classes by the model, the image is deemed
successfully identified according to this criterion. We attempted to convey the subtlety of
image classification within the limitations of the Tiny ImageNet dataset by combining both
top 1andtop 5accuracy metrics.
Table 1.Theclassification models' performancein theabsenceofan attack
Metric Resnext50_3 DenseNet201 VGG-19
2x4d Model Model
Model
Top-1error 10.16% 13.92% 19.88%
Top-5error 1.20% 2.22% 4.38%
Three classification models were tested using the large Tiny ImageNet dataset, which has
200 different image classes. The performance results are shown in Table 1. The low top-%
error rates of all three models indicate their remarkable precision. The Resnext50_32x4d
model, in particular, has exceptional performance, combining the lowest error rates withthe
highest precision. Specifically, this model's Top-1 and Top-5 error rates are 10.16% and
1.20%, respectively.
After a thorough analysis of all three models on the Tiny ImageNet dataset, we looked at
individual photos from the dataset. In ordertoachievethis, weinvestigatedthe classification
results of the models using a random selection of photos indexed at 12, 18, and 23.
17Interestingly, the classifications "great white shark," "tiger shark," and "hammerhead" are
representedbythesepictures,in thatorder.
Table 2.Theselectedphotos'categorization outcomes usingthe ResNext50_32x4dmodel
Image Image True Top-5 Predicted Predicted Top-5
Index Class Classes Confidences
12 great white great white 0.8236
shark shark 0.0924
hammerhead 0.0824
tiger shark 0.0002
killer whale 0.0001
submarine
18 hammerhead hammerhead 0.9935
tiger shark 0.0032
great white 0.0021
shark 0.0002
gar 0.0002
barracouta
23 tiger shark tiger shark 0.9677
gar 0.0092
eel 0.0041
hammerhead 0.0031
sturgeon 0.0029
The results of the resnext50_32x4d model's classification of these three given photos are
shown in Table 2. With the exception of the imageof the"greatwhite shark,"all casesshow
very excellent categorization accuracy, with confidence levels for the real class reaching
90%. In this case, "confidence" refers to the likelihood that the model attributes to the
relevant class. For example, the resnext50_32x4d model assigns a confidence rating of
180.8236 when it correctly predicts the class "great white shark" for an image. This indicates
that there is a high probability (82.36%) that the image belongs to the ‚Äúgreat white shark‚Äù
class.
The classification resultsfor the threephotosusing theresnext50_32x4d model aredisplayed
in Fig. 8. The input image is shown on the left of the figure, and the model's confidence
values for the top five predicted classes are shown on the right. Horizontal bars represent
theseconfidence values.
Fig. 8: A fewparticular photographs' categorizations usingthe Resnext50_32x4dmodel
19Classification UnderThe InfluenceOfThe FGSM Attack
Initially, we employed the FGSM attack with an epsilon value of 2%, indicating that pixel
values were perturbed by approximately1withinthe rangeof0to 255.Theperturbationwas
deliberately subtle,renderingthe altered imageindistinguishablefromthe original.Our focus
was on evaluating the performance of the resnext50_32x4d model under this attackscenario
with Œµ=2% in Table3.
Even with a low Œµ value of 0.02, the FGSM attack significantly compromises the
performance of theresnext50_32x4dmodel. Furthermore,discerning between theadversarial
images andtheoriginal onesprovestobe achallengingtask. Thevisualrepresentationof the
attack ontheimagesis showninFig 9.
Table 3.Theselectedimages'classification outcomes for theresnext50_32x4d modelunder
FGSM attackwithepsilon=2%
Image Index Image True Top-5 PredictedClassesandConfidences
Class Class Confidence
12 great white hammerhead 0.9886
shark tiger shark 0.0066
great whiteshark 0.0046
gar 0.0005
sturgeon 0.0001
18 tiger shark gar 0.2853
eel 0.1596
barracouta 0.0639
sea snake 0.0591
sturgeon 0.0258
23 hammerhead tiger shark 0.4591
great whiteshark 0.0686
barracouta 0.0670
gar 0.0400
sturgeon 0.0374
20Fig. 9: Theresnext50_32x4dmodel's classification resultsunder theFGSM attackwithŒµ =
2%
Table 4.Performanceof theResNext50_32x4d Classification modelunder FGSM Attack for
variousvaluesof Œµ
Noise Top-1 Top-5
Level(Œµin%) Error(%) Error(%)
1 77.88% 33.82%
2 87.62% 49.58%
3 90.34% 55.62%
4 91.38% 59.14%
5 91.80% 60.58%
6 91.64% 61.36%
7 91.34% 61.66%
8 91.16% 61.60%
9 90.96% 61.58%
10 90.74% 61.16%
21Table 5.DenseNet201Classification PerformanceforVarious Values ofŒµ underFGSM
Attack
Noise Top-1 Top-5
Level(Œµin%) Error(%) Error(%)
1 78.94% 34.62%
2 89.92% 52.28%
3 93.08% 59.90%
4 94.22% 63.96%
5 94.48% 66.22%
6 94.66% 67.48%
7 94.64% 67.74%
8 94.36% 67.82%
9 94.34% 67.94%
10 94.12% 67.86%
Table6.PerformanceofVGG19Classification underFGSM Attack forVarious Values ofŒµ
Noise Top-1 Top-5
Level(Œµin%) Error(%) Error(%)
1 92.86% 59.7%
2 96.92% 74.20%
3 97.80% 78.82%
4 98.10% 80.32%
5 98.08% 80.84%
6 98.02% 80.84%
7 97.68% 80.84%
8 97.68% 80.54%
9 97.50% 80.20%
10 97.36% 79.92%
The epsilon value was incremented from 1% to 10% in steps of 1%. Throughout this
progression, it was observed in Fig.10 that the error rate steadily increased from 0.01 to
0.04, after which it reached saturation. For the resnext50_32x4d model, the highest
classification errors were recorded as Top-1 Error 91.80% and Top-5 Error 61.66%.
Similarly, for the Densenet201 model, the errors were Top-1 Error 94.66% and Top-5 Error
67.94%. In the case of the VGG19 model, the errors peaked at Top-1 Error 98.10% and
22Top-5 Error 80.84%. The resnext50_32x4d, DenseNet201, and VGG19 models underwenta
total of 157 iterations each, with average iteration times of 2.35 seconds, 2.21 seconds, and
1.65 seconds, respectively. Theerror valuesafter theFGSM attackinpercentagefor different
Œµand thementionedthreemodelsarepresented inTable 4,Table 5,andTable 6.
Fig.10: Followingan FGSM attack, the top-1 andtop-5errorrates(%) foreach modelfor a
range of epsilonvalues
Classification PerformanceUnder TheCW Attack
The CW attack was implemented with an epsilon value of 2%, perturbing pixel values by
approximately 1 within the range of 0 to 255. This subtle perturbation was deliberately
designed to make the altered imagevisually indistinguishablefromthe original.Our primary
focus was to assess the performance of the resnext50_32x4d Model under this attack
scenario, specificallywith Œµ= 2%,as presentedin Table7.
23Table 7.ClassificationPerformanceofResNext50_32x4d under CWAttack
Image Index Image True Top-5 Predicted Classes and
Class Confidences
Class Confidence
12 great white hammerhead 0.9886
shark tiger shark 0.0066
great white 0.0046
shark 0.0005
gar 0.0001
sturgeon
18 tiger shark gar 0.2853
eel 0.1596
barracouta 0.0639
sea snake 0.0591
sturgeon 0.0258
23 hammerhead tiger shark 0.4591
great white 0.0686
shark 0.0670
barracouta 0.0400
gar 0.0374
sturgeon
Even with the relatively low Œµ value of 0.02, the impact of the CW attack on the
resnext50_32x4d model's performance was substantial. The model's classification accuracy
was significantly compromised under this attack. Moreover, distinguishing between the
adversarial images and the original ones posed a considerable challenge. Despite the
seemingly minor perturbations, the CWattack effectivelyundermined themodel'srobustness
and highlighted the vulnerability of the resnext50_32x4d model to adversarial attacks.
Furthermore, theresults aregraphicallypresentedin Fig 11.
24Fig. 11: Theclassificationresults under CWassaultwithepsilon=2% ofasubsetof
particularimages of theResNext50_32x4d model
25Table 8.ResNext50_32x4dClassification PerformanceforVarious Values ofŒµ underCWAttack
Noise Top-1 Top-5
Level(Œµin%) Error(%) Error(%)
1 77.88% 33.86%
2 87.62% 49.58%
3 90.34% 55.62%
4 91.38% 59.14%
5 91.80% 60.58%
6 91.64% 61.38%
7 91.34% 61.66%
8 91.16% 61.60%
9 90.96% 61.58%
10 90.74% 61.16%
Table 9.ClassificationPerformanceofDenseNet201 underCWAttack for VariousValues ofŒµ
Noise Top-1 Top-5
Level(Œµin%) Error(%) Error(%)
1 78.94% 34.64%
2 89.92% 52.28%
3 93.08% 59.90%
4 94.22% 63.96%
5 94.48% 66.22%
6 94.66% 67.48%
7 94.64% 67.74%
8 94.36% 67.82%
9 94.34% 67.94%
10 94.12% 67.86%
26Table10.VGG19Classification Performance for Various Valuesof Œµunder CWAttack
Noise Top-1 Top-5
Level(Œµin%) Error(%) Error(%)
1 92.86% 59.76%
2 96.92% 74.20%
3 97.80% 78.82%
4 98.10% 80.32%
5 98.08% 80.84%
6 98.02% 80.84%
7 97.68% 80.84%
8 97.68% 80.54%
9 97.50% 80.20%
10 97.36% 79.92%
As a result, it can be seen that all three of the models in Figure 12 perform terribly when it
comes to classification when subjected to the CW attack. For the resnext50_32x4d model,
the highest classification errors were recorded as Top-1 Error 91.80% and Top-5 Error
61.66%. Similarly, for the Densenet201 model, the errors were Top-1 Error 94.66% and
Top-5 Error 67.94%. In the case of the VGG19 model, the errors peaked at Top-1 Error
98.10% and Top-5 Error 80.84%. The resnext50_32x4d, DenseNet201, and VGG19 models
underwent a total of 157 iterations each, with average iteration times of 24.9 seconds, 27.4
seconds, and 34.7 seconds, respectively. The error values after the CW attack in percentage
for different Œµ and the mentioned three models are presented in Table 8, Table 9, and Table
10.
27Fig.12: Top-1 andTop-5errorrates (%)for variousepsilonvaluesfollowingCWassaultin
allmodels
Performance of Defensive Distillation onFGSM Attack
The CIFAR-10 dataset comprises 60,000 32x32 color images categorized into 10 classes,
each containing6,000photos. Thisdatasetserved asthe basisfor investigatingtheimpacts of
the FGSM attack and assessing the defensive distillation technique's efficacy in mitigating
the attack. Following this, a CNN model was trained for image classification using a
randomly selected subset of 40,000 samples, with an additional 10,000 samples reservedfor
validation. Subsequently, the model'sperformance wasevaluated usingthe remaining10,000
samplesfrom thedataset.
Distillation was originally used to reduce a huge model (called the instructor) to a smaller
version (called the distilled model). Training the distilled model with these softlabelsrather
than hard labels taken straight from the trainingset entails firsttraining theteacher model on
the dataset, and then applying soft labels to instances based on the teacher's output vector.
This method increases the efficiency with which the distilled model learns to predict
challenging labels and improves the accuracy of the testdataset.Theinstructor modelin this
caseisresnet101,while thestudent,or distilled,model is resnext50_32x4d.
28Using the Adam optimizer, an adaptive optimization approach, the parameter temperature is
adjusted to 100 and the instructor model is trained on the CIFAR-10 dataset. The optimizer
uses beta parameters (0.9, 0.99) to control the exponential moving means of gradients and
squared gradients, and it uses a minimum learning rate of 0.0001. Because they
accommodate different gradients across parameters, these parameters allow the neural
network to dynamically modify the learning rates for each parameter, resulting in smoother
convergenceduring training.
It usesthecategoricalcross-entropyloss function.Thevalidation loss istracked byalearning
rate scheduler, which lowers the learning ratewhenthe trackedparameter reachessaturation.
The mode parameter of the scheduler is set to a minimum which results in a drop in the
learning rate when the validation loss stopsdeclining. Sincethe factorparameteris equalsto
0.1, when the validation loss reaches a saturation, the learning rate will be lowered by a
factor of 0.1. Additionally, the parameter patience is set to 3, which indicates how many
epochs to wait in case the monitored measure stalls before modifying the learning rate. If,
throughout the allotted patience period, the monitored metric does not show improvement,
the scheduler will reduce the learning rate; in this case, after three epochs. Ten training
epochs are applied to the model. The teacher model's training and validation losses are
depicted inFig. 13(a), whilethe studentmodel isplottedagainstthe number ofepochs inFig.
13(b).
29The defensive distillation models underwent an FGSM attack using perturbed adversarial
samples across various epsilon values (epsilon = [0%, 0.7%, 1%, 2%, 3%, 5%, 10%, 20%,
30%]). The classification accuracy before and after the attack, with and without defensive
distillation, is depicted in Fig.14. The accuracy before defensive distillation is shown bythe
green line, while the accuracy post-defensive distillation is indicated by the blue line.
Initially, the accuracy of the resnext50_32x4d model, utilized as the student model in
defensive distillation and trained on the CIFAR10 dataset, was 0.79. After the attack, the
accuracy droppedto 0.55, butfollowing distillation,it improvedsignificantly to0.87.
Fig. 14: TheResNext50_32x4d model's accuracyvaluesthroughout arange ofepsilon
values, bothbefore andafterdefensivedistillation
Performance of Defensive Distillation onCW attack
The defensive distillation models were subjected to a CW attack utilizing perturbed
adversarial samples across a range of epsilon values ([0%, 0.7%, 1%, 2%, 3%, 5%, 10%,
20%, 30%]). Fig.15 illustrates the classification accuracy both before and after the attack,
with and without employing defensive distillation. The accuracy prior to applying defensive
distillation is represented by the green line, while the accuracy following defensive
30distillation is denoted by the blue line. The graph indicates that defensivedistillation didnot
improve accuracyafter theCW attack.Notably, bothteacher andstudentmodelswere trained
ontheCIFAR-10dataset, withparameters setunder the sameconditionsasthe FGSMattack.
Fig. 15: TheResNext50_32x4d(student) model'saccuracy valuesbeforeandafterdefensive
distillation forarange ofepsilonvalues
6. Conclusion and Future Work
Even if our defensive distillation model successfully thwarts attacks like FGSM, it is still
vulnerable to more sophisticated techniques like the CW attack. However, our findings
highlight thepossible effectiveness ofdefensive distillationagainst adversarialtactics such as
FGSM. In the future, it will be crucial to improve the model's defensive capabilities by
adding a richer, more diverse dataset to it. By strengthening the model's robustness, this
tactical improvement seeks to increase its usefulnessindefending againstadversarial attacks
in pictureclassificationtasks.
Protecting image classifiers from adversarial attacks is an important area of research in
artificial intelligence, with concerns about the effectiveness of defense strategies such as
distillation approaches. Defensive distillation has been implemented to improve robustness
against attacks; nonetheless, it has been demonstrated that this approach is insufficient
31against well-known adversarial tactics like the CW attack. This emphasizes how urgently
new defense mechanisms are needed to effectively counter the dynamic dangerlandscapein
artificialintelligence.
We have also compared the results of the CW L2 attackand FGSMattackand haveseen the
iterative taken for the former is more than that of the latter. Also, the test errors (top-1 and
top-5) are mostly the same in both cases with an increase in perturbation. This provides us
with ascope of workingdeeply into whyit happens inthefuture.
Researchers need touse strategiesotherthan traditionaldefenses, suchas adversarialtraining
and robustoptimization,to counteradversarial attacksonimageclassifiers.It is imperativeto
fortify current techniques, such as defensive distillation, against arangeofassault strategies,
including FGSM. To strengthen image classifiers' resilience in the face of the constantly
changing AIadversarial threatscenario, this collaborativeeffort is essential.
To sum up, the competition between defensive measures and adversarial attacks highlights
the intricacy and dynamic nature of the subject of adversarial machine learning. Deep
learning model reliability is seriously threatened by assaults like FGSM and CW L2,
although methods like defensive distillation present viable ways to increase model
robustness. It is crucial to explore multidisciplinary methods that integrate knowledge from
machine learning, optimization, and cognitive science as this field of study develops further
to create AI systems that are more reliable and durable. In the end, combating adversarial
attacks necessitates a thorough comprehension of the fundamental weaknesses of neural
networks as well as the creation of all-encompassing defense plans thatgive equal weightto
security andaccuracy.
32References
1. Goodfellow,I. J., Shlens, J.,&Szegedy, C.(2014). Explaining andharnessing adversarial
examples.arXiv (Cornell University).https://arxiv.org/pdf/1412.6572.pdf
2. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus,
R.(2013). Intriguingproperties ofneuralnetworks.arXiv (CornellUniversity).
https://doi.org/10.48550/arxiv.1312.6199
3. Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B., & Swami, A. (2016).
TheLimitations ofDeep LearninginAdversarialSettings.IEEE.
https://doi.org/10.1109/eurosp.2016.36
4. Brown, T. B., Man√©, D., Roy, A., Abadi, M., & Gilmer, J. (2017). Adversarial patch.
arXiv (CornellUniversity).https://arxiv.org/pdf/1712.09665.pdf
5. Moosavi-Dezfooli, S., Fawzi,A., Fawzi,O., &Frossard,P.(2017). Universal Adversarial
Perturbations. IEEE.https://doi.org/10.1109/cvpr.2017.17
6. Carlini,N., &Wagner, D. (2017). TowardsEvaluatingthe Robustness ofNeural
Networks. IEEE.https://doi.org/10.1109/sp.2017.49
7. Athalye, A.,Engstrom, L.,Ilyas, A.,& Kwok,K. S. (2017). Synthesizingrobust
adversarial examples.arXiv (CornellUniversity).
https://arxiv.org/pdf/1707.07397
8. MƒÖdry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2018). Towards deep
learning models resistant to adversarial attacks. arXiv (Cornell University).
http://arxiv.org/pdf/1706.06083.pdf
9. Dong, Y.,Liao, F., Pang,T.,Su, H., Zhu, J.,Hu, X.,& Li, J.(2018). BoostingAdversarial
Attackswith Momentum. IEEE.https://doi.org/10.1109/cvpr.2018.00957
10.Croce, F., & Hein, M. (2020). Reliable evaluation of adversarial robustness with an
ensemble of diverse parameter-free attacks. arXiv, 1, 2206‚Äì2216.
http://proceedings.mlr.press/v119/croce20b/croce20b.pdf
11. Papernot, N., McDaniel, P., Wu, X., Jha, S., & Swami, A. (2016). Distillation as a
Defense to Adversarial Perturbations Against Deep Neural Networks. IEEE.
https://doi.org/10.1109/sp.2016.41
12.Papernot, N., McDaniel, P., Goodfellow, I. J., Jha, S., Celik, Z. B., &Swami, A.(2017).
Practical Black-Box Attacks against Machine Learning. ACM.
https://doi.org/10.1145/3052973.3053009
3313.LeCun, Yann, Corinna Cortes, and Christopher J.C. Burges. "MNIST handwritten digit
database."AT&T Labs [Online].Available: http://yann.lecun. com/exdb/mnist2(2010).
14.Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images.
https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf
15.Deng, J., Dong, W., Socher, R., Li, L., Li, K., & Li, F. (2009). ImageNet: A large-scale
hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern
Recognition. https://doi.org/10.1109/cvpr.2009.5206848
16.Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,S., Ma,S., Huang,Z., Karpathy,
A., Khosla, A., Bernstein, M. S., Berg, A. C., & Li, F. (2015). ImageNet Large Scale
Visual Recognition Challenge. International Journal of Computer Vision, 115(3),
211‚Äì252.https://doi.org/10.1007/s11263-015-0816-y
17.Moosavi-Dezfooli, S., Fawzi, A., &Frossard,P.(2015). DeepFool: asimpleand accurate
method to fool deep neural networks. arXiv (Cornell University).
https://doi.org/10.48550/arxiv.1511.04599
18.Attacking machine learning with adversarial examples. (n.d.).
https://openai.com/research/attacking-machine-learning-with-adversarial-examples
19.Rosebrock, A. (2023, June 9). Adversarial attacks with FGSM (Fast Gradient Sign
Method) - PyImageSearch. PyImageSearch.
https://pyimagesearch.com/2021/03/01/adversarial-attacks-with-fgsm-fast-gradient-sign-
method/
20.Sciforce. (2022, September 7). Adversarial attacks explained (And how to defend ML
models against them). Medium.
https://medium.com/sciforce/adversarial-attacks-explained-and-how-to-defend-ml-model
s-against-them-d76f7d013b18
21.Adversarial Example Generation ‚Äî PyTorch Tutorials 2.2.1+cu121 documentation.
(n.d.). https://pytorch.org/tutorials/beginner/fgsm_tutorial.html
22.DeepAI. (2020, June 25). Defensive distillation. DeepAI.
https://deepai.org/machine-learning-glossary-and-terms/defensive-distillation
23.Simonyan, K., & Zisserman,A. (2015). Very deepconvolutional networksfor large-scale
imagerecognition. arXiv(CornellUniversity).https://arxiv.org/pdf/1409.1556
24.Huang, G., Liu, Z., VanDerMaaten, L.,& Weinberger,K. Q. (2017). DenselyConnected
Convolutional Networks. Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR).https://doi.org/10.1109/cvpr.2017.243
3425.Xie, S., Girshick, R., Doll√°r, P., Tu, Z., & He, K. (2017). Aggregated Residual
Transformations for Deep Neural Networks. IEEE Conference on Computer Vision and
Pattern Recognition (CVPR).https://doi.org/10.1109/cvpr.2017.634
26.Sudha, V. and Ganeshbabu, T.R. (2020). A Convolutional Neural Network Classifier
VGG-19 Architecture for Lesion Detection and Grading in Diabetic Retinopathy Based
onDeep Learning.DOI:10.32604/cmc.2020.012008.
27. Effati, M , and Nejat, G. (2023). A Performance Study of CNN Architectures for the
Autonomous Detection of COVID-19 Symptoms Using Cough and Breathing.
Computers2023,12,44.doi.org/10.3390/computers12020044
28.Jeevidha, S., Saraswathi, S. (2023). DEEP FAKE VIDEO DETECTION USING
RESNEXT CNNANDLSTM. ISSN:2320-2882.www.ijcrt.org
29.Sen, J. and Dasgupta, S. Adversarial attacks on Image classification models: FGSM and
patch attacks and their impact, in Sen, J. and Mayer, J. (eds) Information Security and
Privacy in the Digital World: Some Selected Topics, IntechOpen, London, UK. ISBN:
978-1-83768-196-9. DOI:10.5772/intechopen.112442.
30.Gharaibeh, M. and Hussien, M. (2021). Early Diagnosis of Alzheimer‚Äôs Disease Using
Cerebral Catheter Angiogram Neuroimaging: A Novel Model Based on Deep Learning
Approaches. DOI:10.3390/bdcc601002.
31.Sen J, Sen A, and Chatterjee A, Adversarial Attacks on Image Classification Models:
Analysis and Defense, In Proceedings of the 10th International Conference on Business
Analytics and Intelligence (ICBAI‚Äô23), Indian Institute of Science, Bangalore, India,
December18-20, 2023.
32.Sen J, The FGSM Attack on Image Classification Models and Distillationasits Defense
In: Proceedings of the 5th International Conference on Advances in Distributed
Computing and Machine Learning (ICADCML‚Äô2024), January 05-06, 2024. (The paper
will be published in a Springer ‚ÄúLecture Notes inNetworks andSystems(LNNS)‚ÄùBook
Series indexedbySCOPUS,DBLP,SCImago, andINSPEC. Expected inJul2024).
35