Watermark-based Detection and Attribution of AI-Generated Content
ZhengyuanJiang,MoyangGuo,YuepengHu,NeilZhenqiangGong
DukeUniversity
{zhengyuan.jiang,moyang.guo,yuepeng.hu,neil.gong}@duke.edu
ABSTRACT
Severalcompaniesâ€“suchasGoogle,Microsoft,andOpenAIâ€“havedeployedtechniquestowatermark
AI-generatedcontenttoenableproactivedetection. However,existingliteraturemainlyfocuseson
user-agnosticdetection. Attributionaimstofurthertracebacktheuserofagenerative-AIservice
whogeneratedagivencontentdetectedasAI-generated. Despiteitsgrowingimportance,attribution
islargelyunexplored. Inthiswork,weaimtobridgethisgapbyprovidingthefirstsystematicstudy
on watermark-based, user-aware detection and attribution of AI-generated content. Specifically,
wetheoreticallystudythedetectionandattributionperformanceviarigorousprobabilisticanalysis.
Moreover,wedevelopanefficientalgorithmtoselectwatermarksfortheuserstoenhanceattribution
performance. Bothourtheoreticalandempiricalresultsshowthatwatermark-baseddetectionand
attributioninherittheaccuracyand(non-)robustnesspropertiesofthewatermarkingmethod.
1 Introduction
GenerativeAI(GenAI)â€“suchasDALL-E3,Midjourney,andChatGPTâ€“cansynthesizeveryrealistic-lookingcontent
suchasimages,texts,andaudios. Beyonditssocietalbenefits,GenAIalsoraisesmanyethicalconcerns. Forinstance,
theycanbemisusedtogenerateharmfulcontent;theycanbeusedtoaiddisinformationandpropagandacampaigns
bygeneratingrealistic-lookingcontent[1];andpeoplecanfalselyclaimcopyrightownershipofcontentgeneratedby
them[2].
Watermark-baseddetectionandattributionofAI-generatedcontentisapromisingtechniquetomitigatetheseethical
concerns. For instance, several companiesâ€“such as Google, OpenAI, Stability AI, and Microsoftâ€“have deployed
suchtechniquestowatermarktheirAI-generatedimages. Specifically,OpenAIinsertsavisiblewatermarkintothe
imagesgeneratedbyitsDALL-E2[3];Googleâ€™sSynthID[4]insertsaninvisiblewatermarkintoimagesgenerated
byitsImagen;StabilityAIdeploysawatermarkingmethodinitsStableDiffusion[5];andMicrosoftwatermarksall
AI-generatedimagesinBing[6].
However,existingliteraturemainlyfocusesonuser-agnosticdetectionofAI-generatedcontent. Inparticular,thesame
watermarkisinsertedintoallthecontentgeneratedbyaGenAIservice;andacontentisdetectedasgeneratedbythe
GenAIserviceifasimilarwatermarkcanbedecodedfromit. Attributionaimstofurthertracebacktheregistered
useroftheGenAIservicewhogeneratedagivencontent.1 SuchattributioncanaidtheGenAIserviceproviderorlaw
enforcementinforensicanalysisofcyber-crimes,suchasdisinformationandpropagandacampaigns,thatinvolvea
givenAI-generatedcontent. Despitethegrowingimportanceofattribution,itislargelyunexplored. Inthiswork,we
aimtobridgethisgapbyprovidingasystematicstudyonwatermark-baseddetectionandattributionofAI-generated
content.
Wenotethatarelevantbutorthogonalresearchdirectionistodevelopwatermarkingmethodsthatarerobustagainst
post-processingofAI-generatedcontent. Westressthatitisstillanongoingefforttodeveloprobustwatermarkingand
thecommunityhasalreadymadesignificantprogressinthepastseveralyears. Forinstance,non-learning-basedimage
watermarking[7â€“9],whichhasbeenstudiedfordecades,isnotrobustagainstcommonpost-processingsuchasJPEG
compression,Gaussianblur,andBrightness/Contrast. However,recentlearning-basedimagewatermarking[10â€“14]
is robust against such common post-processing [11] because it can leverage adversarial training [15]. Although
learning-basedimagewatermarkingisnotrobustyetagainstadversarialpost-processinginthewhite-boxsetting[16];
1AttributioncouldalsorefertotracingbacktheGenAIservicethatgeneratedagivencontent,whichwediscussinSection7.
4202
rpA
5
]RC.sc[
1v45240.4042:viXraWatermark Database Watermark Database
01001001
ð‘¤"
AI-gen &
ð‘ˆ!: ð‘¤!
Detection
generated by ð‘ˆ"
GenAI Encoder Decoder 01001001 &
Attribution
Watermark 01101001 ð‘ˆ" Non-AI-gen
Selection
ð‘ˆ!
ð‘¤!
Registration Generation Detection & Attribution
Figure1: Illustrationofregistration,generation,anddetection&attributionphasesofwatermark-baseddetectionand
attribution.
ithasgoodrobustnessagainstadversarialpost-processingwhenanattackercanonlyquerythedetectionAPIfora
smallnumberoftimesintheblack-boxsettingordoesnothaveaccesstothedetectionAPI[16]. Forinstance,Google
restrictsaccessofitsdetectionAPItoonlytrustedcustomers[17]. Sinceourdetectionandattributionmethodrelieson
watermarkingtechniques,itinheritstheir(non-)robustnessproperties.
Ourwork: Inthiswork,weconductthefirstsystematicstudyonthetheory,algorithm,andevaluationofwatermark-
baseddetectionandattributionofAI-generatedcontent. Figure1illustratesourmethod. Whenauserregistersin
aGenAIservice,theserviceproviderselectsawatermark(i.e.,abitstring)forhim/herandstoresitinawatermark
database. WhenausergeneratesacontentusingtheGenAIservice,theuserâ€™swatermarkisinsertedintothecontent
usingthewatermarkencoder. AcontentisdetectedasgeneratedbytheGenAIserviceifthewatermarkdecodedfrom
thecontentissimilarenoughtoatleastoneuserâ€™swatermarkinthewatermarkdatabase. Moreover,thecontentis
furtherattributedtotheuserwhosewatermarkisthemostsimilartothedecodedone.
Theory. Wetheoreticallyanalyzetheperformanceofwatermark-baseddetectionandattribution. Specifically,wedefine
threekeyevaluationmetrics: truedetectionrate(TDR),falsedetectionrate(FDR),andtrueattributionrate(TAR).
TDR(orTAR)istheprobabilitythatanAI-generatedcontentiscorrectlydetected(orattributed),whileFDRisthe
probabilitythatanon-AI-generatedcontentisfalselydetectedasAI-generated. Weshowthatotherrelevantevaluation
metricscanbederivedfromthesethree. Basedonaformalquantificationofawatermarkingmethodâ€™sbehavior,we
derivelowerboundsofTDRandTAR,andanupperboundofFDRnomatterhowtheusersâ€™watermarksareselected.
Wealsodiscussmultipletheoreticalinsightsaboutthedetection/attributionperformancebasedonourderivedbounds.
Algorithm. Selecting watermarks for the users is a key component of watermark-based detection and attribution.
Intuitively, attribution is hard if the usersâ€™ watermarks are similar to each other. In fact, our derived lower bound
ofTARalsoalignswithsuchintuition. Therefore,toenhanceattributionperformance,weaimtoselectdissimilar
watermarksfortheusers. Formally,weformulateawatermarkselectionproblem,whichaimstoselectawatermarkfor
anewregistereduserviaminimizingthemaximumsimilaritybetweentheselectedwatermarkandtheexistingusersâ€™
watermarks. Wefindthatourwatermarkselectionproblemisequivalenttothewell-knownfartheststringproblem[18],
whichhasbeenstudiedextensivelyinthetheoreticalcomputersciencecommunity. Moreover,sincethefartheststring
problemisNP-hard,ourwatermarkselectionproblemisalsoNP-hard,whichimpliesthechallengesofdeveloping
efficient, exact solutions. Thus, we resort to efficient, approximate solutions. In particular, we adapt the bounded
searchtreealgorithm[19],astate-of-the-artinefficient,exactsolutiontothefartheststringproblem,asanefficient,
approximatealgorithmtoselectwatermarks.
Empiricalevaluation. WeempiricallyevaluateourmethodforAI-generatedimagesonthreeGenAImodels, i.e.,
StableDiffusion,Midjourney,andDALL-E2. WeuseHiDDeN[11],thestate-of-the-artlearning-basedwatermarking
method. Notethatourdetectionandattributioninheritthe(non-)robustnesspropertiesofHiDDeN.Inparticular,our
resultsshowthatdetectionandattributionareveryaccurate,i.e.,TDR/TARiscloseto1andFDRiscloseto0,when
AI-generatedimagesarenotpost-processed;detectionandattributionarestillaccuratewhencommonpost-processing,
suchasJPEGcompression,Gaussianblur,andBrightness/Contrast,isappliedtoAI-generatedimages;andadversarial
post-processing[16]withasmallnumberofqueriestothedetectionAPIdegradestheimagequalitysubstantiallyin
ordertoevadedetection/attribution. Moreover,weshowourwatermarkselectionalgorithmoutperformsbaselines,and
ourmethodisalsoapplicabletoAI-generatedtexts.
Tosummarize,ourcontributionsareasfollows:
â€¢ Weprovidethefirstsystematicstudyonwatermark-based,user-awaredetectionandattributionofAI-generated
content.
â€¢ Theory. Wetheoreticallyanalyzethedetectionandattributionperformanceforanywatermarkingmethodand
nomatterhowthewatermarksareselectedfortheusers.
2â€¢ Algorithm. Weformulateawatermarkselectionproblem,whichisinspiredbyourtheoreticalresults;andwe
developanefficient,approximatesolutionforit.
â€¢ Evaluation. Weconductextensiveevaluationofourmethodindifferentscenarios.
2 RelatedWork
2.1 WatermarkingMethods
A watermarking method typically consists of three components: watermark, encoder, and decoder. We consider
a watermark w to be a bitstring. An encoder E embeds a watermark into a content, while a decoder D decodes a
watermarkfroma(watermarkedorunwatermarked)content. Whenacontenthaswatermarkw,thedecodedwatermark
issimilartow. NotethattheencoderE andwatermarkwcanalsobeembeddedintotheparametersofaGenAImodel
suchthatitsgeneratedcontentisinherentlywatermarkedwithw[14].
Non-learning-basedvs. learning-based: Watermarkingmethodscanbecategorizedintotwogroupsbasedonthe
designoftheencoderanddecoder: non-learning-basedandlearning-based. Non-learning-basedmethods[7â€“9,20,
21]designtheencoderanddecoderbasedonsomehand-craftedheuristics,whilelearning-basedmethods[10â€“14,22]
useneuralnetworksastheencoder/decoderandautomaticallylearnthemusingacontentdataset. Forinstance,Tree-
Ring[20]andLM-watermarking[21]respectivelyarenon-learning-basedwatermarkingmethodsforimagesandtexts;
whileHiDDeN[11]andAWT[22]respectivelyarelearning-basedmethodsforimagesandtexts. Ourwatermark-based
detectionandattributionmethod,theory,andalgorithmareapplicabletobothcategoriesofwatermarkingmethods.
However,sincelearning-basedwatermarkingmethodsaremorerobustduetoadversarialtraining[11],weadopta
learning-basedwatermarkingmethodinourexperiments.
Standardtrainingvs. adversarialtraining: Inlearning-basedwatermarkingmethods,theencoderanddecoderare
automaticallylearntusingacontentdataset. Specifically,givenacontentC andarandomwatermarkw,thedecoded
watermarkD(E(C,w))forthewatermarkedcontentE(C,w)shouldbesimilartow,i.e.,D(E(C,w))â‰ˆw. Based
onthisintuition,standardtrainingaimstolearnanencoderE anddecoderDsuchthatD(E(C,w))issimilartow
foracontentdataset[10]. AwatermarkedcontentE(C,w)maybepost-processed,e.g.,awatermarkedimagemay
be post-processed by JPEG compression during transmission on the Internet. Zhu et al. [11] extended adversarial
training [15, 23], a standard technique to train robust classifiers, to train watermarking encoder and decoder that
aremorerobustagainstpost-processing. Specifically,adversarialtrainingaimstolearnanencoderE anddecoder
D suchthatD(P(E(C,w)))issimilartow, whereP standsforapost-processingoperationandP(E(C,w))isa
post-processedwatermarkedcontent. Ineachepochofadversarialtraining,aP israndomlysampledfromagivenset
ofthemforeachcontentinthecontentdataset.
Robustnessofwatermarking: Westressthatbuildingrobustwatermarkingmethodsisorthogonaltoourworkandis
stillanongoingeffort. Non-learning-basedwatermarkingmethods[7â€“9,20,21]areknowntobenon-robusttocommon
post-processingsuchasJPEGcompressionforimages[11,14]andparaphrasingfortexts[24],i.e.,suchcommon
post-processingcanremovethewatermarkfromawatermarkedcontent. Learning-basedwatermarkingmethods[10â€“14,
22]aremorerobusttosuchcommonpost-processingbecausetheycanleverageadversarialtraining. Forinstance,
common post-processing has to substantially decrease the quality of a watermarked image in order to remove the
watermark[12,13].
Jiangetal.[16]proposedadversarialpost-processingtoimagewatermarking,whichstrategicallyperturbsawater-
markedimagetoremovethewatermark. AccordingtoJiangetal.,learning-basedimagewatermarkingmethodsarenot
yetrobusttoadversarialpost-processinginthewhite-boxsettingwhereanattackerhasaccesstothedecoder. However,
they have good robustness to adversarial post-processing when an attacker can only query the detection API for a
smallnumberoftimesintheblack-boxsettingordoesnothaveaccesstothedetectionAPI.Inparticular,adversarial
post-processingsubstantiallydecreasesthequalityofawatermarkedimageinordertoremovethewatermarkinsuch
scenarios. Toenhancerobustness,aGenAIservicecankeepitswatermarkingencoder/decoderprivateandrestrictthe
accessofitsdetectionAPItoasmallnumberoftrustedcustomers. Forinstance,Googleâ€™sSynthID[4]adoptssuch
strategy.
Weacknowledgethatourwatermark-baseddetectionandattributioninheritthewatermarkingmethodâ€™s(non-)robustness
propertiesdiscussedabove.
32.2 Watermark-basedDetection
WatermarkhasbeenusedforproactivedetectionofAI-generatedcontent[21]. Inparticular,multiplecompaniesâ€“such
asStabilityAI,OpenAI,Google,andMicrosoftâ€“havedeployedwatermark-baseddetectionasdiscussedinIntroduction.
However,existingliteraturemainlyfocusesonuser-agnosticdetection. Specifically,aGenAIserviceproviderpicksa
watermark;wheneveracontentisgeneratedbytheGenAIservice,thewatermarkisembeddedintoitbeforereturningit
toauser;andacontentisdetectedasgeneratedbytheGenAIserviceifasimilarwatermarkcanbedecodedfromit. In
thiswork,westudywatermark-based,user-awaredetectionandattributionofAI-generatedcontent. Afterdetectinga
contentasgeneratedbytheGenAIservice,wefurthertracebacktheuseroftheGenAIservicewhogeneratedit.
3 ProblemFormulation
Problemsetup: SupposewearegivenagenerativeAImodel,whichisdeployedasaGenAIservice. Aregistereduser
sendsaprompt(i.e.,atext)totheGenAIservice,whichreturnsanAI-generatedcontenttotheuser. Thecontentcanbe
image,text,oraudio. Inthiswork,weconsiderdetectionandattributionofAI-generatedcontent. Detectionaimsto
decidewhetheragivencontentwasgeneratedbytheGenAIserviceornot;whileattributionfurthertracesbacktheuser
oftheGenAIservicewhogeneratedacontentdetectedasAI-generated. SuchattributioncanaidtheGenAIservice
providerorlawenforcementinforensicanalysisofcyber-crimes,e.g.,disinformationorpropagandacampaigns,that
involveagivenAI-generatedcontent. Weformallydefinethedetectionandattributionproblemsasfollows:
Definition1(DetectionofAI-generatedcontent). GivenacontentandaGenAIservice,detectionaimstoinferwhether
thecontentwasgeneratedbytheGenAIserviceornot.
Definition 2 (Attribution of AI-generated content). Given a content, a GenAI service, and s users U =
{U ,U ,Â·Â·Â· ,U }oftheGenAIservice,attributionaimstofurtherinferwhichuserusedtheGenAIservicetogenerate
1 2 s
thecontentafteritisdetectedasAI-generated.
WenotethatthesetofsusersU inattributioncouldincludeallregisteredusersoftheGenAIservice,inwhichsmaybe
verylarge. Alternatively,thissetmayconsistofasmallernumberofregisteredusersiftheGenAIserviceproviderhas
somepriorknowledgeonitsregisteredusers. Forinstance,theGenAIserviceprovidermayexcludetheregisteredusers,
whoareverifiedofflineastrusted,fromthesetU toreduceitssize. HowtoconstructthesetofusersU inattributionis
outofthescopeofthiswork. GivenanysetU,ourmethodaimstoinferwhichuserinU mayhavegeneratedagiven
content. WealsonotethatanotherrelevantattributionproblemistotracebacktheGenAIservicethatgeneratedagiven
content. OurmethodcanalsobeusedforsuchGenAI-serviceattribution,whichwediscussinSection7.
Threatmodel: AnAI-generated, watermarkedcontentmaybepost-processedbysomecommonpost-processing
techniquesinnon-adversarialsettings. Forinstance,animagemaybepost-processedbyJPEGcompressionduring
transmissionontheInternet,orausermayuseGaussianblurorBrightness/Contrasttoeditanimageinanimageeditor.
Inadversarialsettings,amalicioususermaypost-processanAI-generatedcontenttoevadedetectionand/orattribution.
Otherthanthecommonpost-processingtechniques,amalicioususermayalsouseadversarialpost-processing[16]
toremovethewatermarkinanAI-generatedcontent. Weassumethewatermarkencoder/decoderisprivateandthe
malicioususerhaslimitedaccesstothedetectionAPI,inwhichstate-of-the-artwatermarkingmethodshavegood
robustnesstopost-processing[16]. SuchthreatmodelariseswhenaGenAIserviceproviderrestrictstheaccessof
itsdetectionAPItoasmallsetoftrustedcustomers,e.g.,Googleâ€™sSynthIDadoptsthisthreatmodel. Notethatour
theoreticalanalysisinSection5canexplicitlyquantifyandincorporatetheimpactofpost-processingonthedetection
andattributionperformance.
4 Watermark-basedDetectionandAttribution
4.1 Overview
Weproposeawatermark-baseddetectionandattributionmethod,whichisillustratedinFigure1. Whenauserregisters
intheGenAIservice,theserviceproviderselectsauniquewatermarkfortheuser. Wedenotebyw thewatermark
i
selectedforuserU ,whereiistheuserindex. Duringgeneration,whenauserU sendsaprompttotheGenAIservice
i i
togenerateacontent,theproviderusesthewatermarkencoderE toembedwatermarkw intothecontent. During
i
detectionandattribution,awatermarkisdecodedfromagivencontent;thegivencontentisdetectedasgeneratedby
theGenAIserviceifthedecodedwatermarkissimilarenoughtoatleastoneoftheusersâ€™watermarks;andthegiven
contentisfurtherattributedtotheuserwhosewatermarkisthemostsimilartothedecodedwatermarkafteritisdetected
asAI-generated.
4Next,wedescribethedetailsofdetectionandattribution. Moreover,wediscusshowtoselectwatermarksfortheusers
tomaximizetheattributionperformance.
4.2 Detection
Recall that we denote by U = {U ,U ,Â·Â·Â· ,U } the set of s users of the GenAI service for atribution. Each user
1 2 s
U hasawatermarkw ,wherei = 1,2,Â·Â·Â· ,s. Forconvenience,wedenotebyW = {w ,w ,Â·Â·Â· ,w }thesetofs
i i 1 2 s
watermarks. GivenacontentC,weusethedecoderDtodecodeawatermarkD(C)fromit. Ifthereexistsauserâ€™s
watermarkthatissimilarenoughtoD(C),wedetectC asAI-generated. Weusebitwiseaccuracytomeasuresimilarity
betweentwowatermarks,whichweformallydefineasfollows:
BitwiseAccuracy(BA): Givenanytwowatermarkswandwâ€²,theirbitwiseaccuracy(denotedasBA(w,wâ€²))isthe
fractionofmatchedbitsinthem. Formally,wehavethefollowing:
n
1 âˆ‘ï¸‚
BA(w,wâ€²)= I(w[k]=wâ€²[k]), (1)
n
k=1
wherenisthewatermarklength,w[k]isthekthbitofw,andIistheindicatorfunctionthathasavalue1ifw[k]=wâ€²[k]
and0otherwise. AcontentC isdetectedasAI-generatedifandonlyifthefollowingsatisfies:
max BA(D(C),w )â‰¥Ï„, (2)
i
iâˆˆ{1,2,Â·Â·Â·,s}
whereÏ„ >0.5isthedetectionthreshold.
4.3 Attribution
AttributionisappliedonlyafteracontentC isdetectedasAI-generated. Intuitively,weattributethecontenttotheuser
whosewatermarkisthemostsimilartothedecodedwatermarkD(C). Formally,weattributecontentC touserU ,
iâˆ—
whereiâˆ—isasfollows:
iâˆ— = argmax BA(D(C),w ). (3)
i
iâˆˆ{1,2,Â·Â·Â·,s}
4.4 WatermarkSelection
Akeycomponentofwatermark-baseddetectionandattributionishowtoselectwatermarksfortheusers. Next,wefirst
formulatewatermarkselectionasanoptimizationproblem,andthenproposeamethodtoapproximatelysolveit.
4.4.1 FormulatingaWatermarkSelectionProblem
Intuitively,iftwousershavesimilarwatermarks,thenitishardtodistinguishbetweenthemfortheattribution. An
extremeexampleisthattwousershavethesamewatermark,makingitimpossibletoattributeeitherofthem. Infact,our
theoreticalanalysisinSection5showsthatattributionperformanceisbetterifthemaximumpairwisebitwiseaccuracy
betweentheusersâ€™watermarksissmaller. Thus,toenhanceattribution,weaimtoselectwatermarksforthesusers
tominimizetheirmaximumpairwisebitwiseaccuracy. Formally,weformulatewatermarkselectionasthefollowing
optimizationproblem:
min max BA(w ,w ), (4)
i j
w1,w2,Â·Â·Â·,wsi,jâˆˆ{1,2,Â·Â·Â·,s},iÌ¸=j
whereBAstandsforbitwiseaccuracybetweentwowatermarks. Thisoptimizationproblemjointlyoptimizesthes
watermarkssimultaneously. Asaresult,itisverychallengingtosolvetheoptimizationproblembecausetheGenAI
serviceproviderdoesnotknowthenumberofregisteredusers(i.e.,s)inadvance. Inpractice,usersregisterinthe
GenAIserviceatverydifferenttimes. Toaddressthechallenge,weselectawatermarkforauseratthetimeofhis/her
registrationintheGenAIservice. ForthefirstuserU ,weselectawatermarkuniformlyatrandom. Supposewehave
1
selectedwatermarksforsâˆ’1users. Then,thesthuserregistersandweaimtoselectawatermarkw whosemaximum
s
bitwiseaccuracywiththeexistingsâˆ’1watermarksisminimized. Formally, weformulateawatermarkselection
problemasfollows:
min max BA(w ,w ). (5)
i s
ws iâˆˆ{1,2,Â·Â·Â·,sâˆ’1}
54.4.2 SolvingtheWatermarkSelectionProblem
NP-hardness: WecanshowthatourwatermarkselectionprobleminEquation5isNP-hard. Inparticular,wecan
reducethewell-knownfartheststringproblem[18],whichisNP-hard,toourwatermarkselectionproblem. Inthe
fartheststringproblem,weaimtofindastringthatisthefarthestfromagivensetofstrings. Wecanviewastringasa
watermarkinourwatermarkselectionproblem,thegivensetofstringsasthewatermarksofthesâˆ’1users,andthe
similaritymetricbetweentwostringsasourbitwiseaccuracy. Then,wecanreducethefartheststringproblemtoour
watermarkselectionproblem,whichmeansthatourwatermarkselectionproblemisalsoNP-hard. ThisNP-hardness
impliesthatitisverychallengingtodevelopanefficientexactsolutionforourwatermarkselectionproblem. Wenote
thatefficiencyisimportantforwatermarkselectionasweaimtoselectawatermarkforauseratthetimeofregistration.
Therefore,weaimtodevelopanefficientalgorithmthatapproximatelysolvesthewatermarkselectionproblem.
Random: ThemoststraightforwardmethodtoapproximatelysolvethewatermarkselectionprobleminEquation5is
togenerateawatermarkuniformlyatrandomasw . WedenotethismethodasRandom. Thelimitationofthismethod
s
isthattheselectedwatermarkw maybeverysimilartosomeexistingwatermarks,i.e.,max BA(w ,w )
s iâˆˆ{1,2,Â·Â·Â·,sâˆ’1} i s
islarge,makingattributionlessaccurate,asshowninourexperiments.
Decision problem: To develop an efficient algorithm to approximately solve our watermark selection problem,
wefirstdefineitsdecisionproblem. Specifically,giventhemaximumnumberofmatchedbitsbetweenw andthe
s
existingsâˆ’1watermarksasm,thedecisionproblemaimstofindsuchaw ifthereexistsoneandreturnNotExist
s
otherwise. Formally, thedecisionproblemistofindanywatermarkw inthefollowingsetifthesetisnonempty:
s
w âˆˆ{w|max BA(w ,w)â‰¤m/n},wherenisthewatermarklength. Next,wediscusshowtosolvethe
s iâˆˆ{1,2,Â·Â·Â·,sâˆ’1} i
decisionproblemandthenturnthealgorithmtosolveourwatermarkselectionproblem.
Boundedsearchtreealgorithm(BSTA)[19]: Recallthatourwatermarkselectionproblemisequivalenttothefarthest
stringproblem. Thus,ourdecisionproblemisequivalenttothatofthefartheststringproblem,whichhasbeenstudied
extensivelyinthetheoreticalcomputersciencecommunity. Inparticular,BSTAisthestate-of-the-artexactalgorithmto
solvethedecisionproblemversionofthefartheststringproblem. WeapplyBSTAtosolvethedecisionproblemversion
ofourwatermarkselectionproblemexactly,whichisshowninAlgorithm1inAppendix. ThekeyideaofBSTAisto
initializew asÂ¬w (i.e.,eachbitofw flips),andthenreducethedecisionproblemtoasimplerproblemrecursively
s 1 1
untilitiseasilysolvableortheredoesnotexistasolutionw . Inparticular,givenaninitialw ,BSTAfirstfindsthe
s s
existingwatermarkw thathasthelargestbitwiseaccuracywithw . IfBA(w ,w ) â‰¤ m/n,thenw isalreadya
iâˆ— s iâˆ— s s
solutiontothedecisionproblemandthusBSTAreturnsw . Otherwise,BSTAchoosesanym+1bitsthatw andw
s s iâˆ—
match. Foreachofthechosenm+1bits,BSTAflipsthecorrespondingbitinw andrecursivelysolvesthedecision
s
problemusingtheneww asaninitialization. Therecursionisappliedmtimesatmost,i.e.,therecursiondepthdisset
s
asmwhencallingAlgorithm1.
AkeylimitationofBSTAisthatithasanexponentialtimecomplexity[19]. Infact,sincethedecisionproblemis
NP-hard,allknownexactsolutionshaveexponentialtimecomplexity. Therefore,toenhancecomputationefficiency,
weresorttoapproximatesolutions. Next,wediscussthestate-of-the-artapproximatesolutionthatadaptsBSTAanda
newapproximatesolutionthatwepropose.
NonRedundantGuess(NRG)[25]: LikeBSTA,thisapproximatesolutionalsofirstinitializesw asÂ¬w andfinds
s 1
theexistingwatermarkw thathasthelargestbitwiseaccuracywithw . IfBA(w ,w )â‰¤m/n,thenNRGreturns
iâˆ— s iâˆ— s
w . Otherwise,NRGsamplesnÂ·BA(w ,w )âˆ’mbitsthatw andw matchuniformlyatrandom. Then,NRGflips
s iâˆ— s s iâˆ—
thesebitsinw andrecursivelysolvethedecisionproblemusingtheneww asaninitialization. NotethatNRGstops
s s
therecursionwhenmbitsoftheinitialw havebeenflipped. Algorithm2inAppendixshowsNRG.
s
Approximateboundedsearchtreealgorithm(A-BSTA): WeadaptBSTAasanefficientapproximatesolutionto
ourdecisionproblem. Specifically,A-BSTAmakestwoadaptionsofBSTA.First,weconstraintherecursiondepthd
tobeaconstant(e.g.,8inourexperiments)insteadofm,whichmakesthealgorithmapproximatebutimprovesthe
efficiencysubstantially. Second,insteadofinitializingw asÂ¬w ,weinitializew asanuniformlyrandomwatermark.
s 1 s
AsourexperimentsinTable3inAppendixshow,ourinitializationfurtherimprovestheperformanceofA-BSTA.This
isbecausearandominitializationismorelikelytohavesmallbitwiseaccuracywithallexistingwatermarks. Notethat
BSTA,NRG,andA-BSTAallreturnNotExistiftheycannotfindasolutionw tothedecisionproblem.
s
Solvingourwatermarkselectionproblem: Givenanalgorithm(e.g.,BSTA,NRG,orA-BSTA)tosolvethedecision
problem,weturnitasasolutiontoourwatermarkselectionproblem. Specifically,ourideaistostartfromasmall
m, and then solve the decision problem. If we cannot find a watermark w for the given m, we increase it by 1
s
andsolvethedecisionproblemagain. Werepeatthisprocessuntilfindingawatermarkw . Notethatwestartfrom
s
m = max nÂ·BA(w ,w ),i.e.,themaximumnumberofmatchedbitsbetweenw andtheother
iâˆˆ{1,2,Â·Â·Â·,sâˆ’2} i sâˆ’1 sâˆ’1
6sâˆ’2watermarks. Thisisbecauseanmsmallerthanthisvalueisunlikelytoproduceawatermarkw asitfailedtodo
s
sowhenselectingw . Algorithm3inAppendixshowsourmethod.
sâˆ’1
Notethatbinarysearchisanotherwaytofindaproperm. Specifically,westartwithasmallm(denotedasm )that
l
doesnotproduceaw andalargem(denotedasm )thatdoesproduceaw . Ifm=(m +m )/2producesaw ,
s u s l u s
weupdatem =(m +m )/2;otherwiseweupdatem =(m +m )/2. Thesearchprocessstopswhenm â‰¥m .
u l u l l u l u
However,wefoundthatincreasingmby1asinourAlgorithm3ismoreefficientthanbinarysearch. Thisisbecause
increasingmby1expandsthesearchspaceofw substantially,whichoftenleadstoavalidw . Onthecontrary,binary
s s
searchwouldrequiresolvingthedecisionproblemmultipletimeswithdifferentmuntilfindingthatm+1isenough.
Timecomplexity: Weanalyzethetimecomplexityofthealgorithmstosolvethedecisionproblem. ForRandom,the
timecomplexityisO(n). ForBSTA,thetimecomplexitytosolvethedecisionproblemwithparametermisO(snmm)
âˆš
accordingto[19]. ForNRG,thetimecomplexityisO(sn+s mÂ·5m)accordingto[25]. ForA-BSTA,thetime
complexityisO(snmd),wheredisaconstant.
5 TheoreticalAnalysis
Wetheoreticallyanalyzethedetectionandattributionperformanceofourwatermark-basedmethod. Wefirstformally
defineseveralkeymetricstoevaluatetheperformanceofdetectionandattribution. Then,wetheoreticallyanalyzethe
evaluationmetrics. AllourproofsareshowninAppendix.
5.1 ContentDistributions
SupposewearegivensusersU ={U ,U ,Â·Â·Â· ,U },eachofwhichhasanuniquewatermarkw ,wherei=1,2,Â·Â·Â· ,s.
1 2 s i
WedenotetheswatermarksasasetW ={w ,w ,Â·Â·Â· ,w }. WhenauserU generatescontentviatheGenAIservice,
1 2 s i
theserviceproviderusestheencoderE toembedthewatermarkw intothecontent. WedenotebyP theprobability
i i
distributionofthewatermarkedcontentgeneratedbyU .NotethattwousersU andU mayhavedifferentAI-generated,
i i j
watermarkedcontentdistributionsP andP . Thisisbecausethetwousershavedifferentwatermarksandtheymay
i j
be interested in generating different types of content. Moreover, we denote by Q the probability distribution of
non-AI-generatedcontent.
5.2 EvaluationMetrics
(User-dependent) True Detection Rate (TDR): TDR is the probability that an AI-generated content is correctly
detected. NotethatdifferentusersmayhavedifferentAI-generatedcontentdistributions. Therefore,TDRdependson
users. WedenotebyTDR thetruedetectionrateforthewatermarkedcontentgeneratedbyuserU ,i.e.,TDR isthe
i i i
probabilitythatacontentC sampledfromtheprobabilitydistributionP uniformlyatrandomiscorrectlydetectedas
i
AI-generated. Formally,wehave:
TDR =Pr( max BA(D(C),w )â‰¥Ï„), (6)
i j
jâˆˆ{1,2,Â·Â·Â·,s}
whereBAisthebitwiseaccuracybetweentwowatermarks,Disthedecoder,C âˆ¼P ,andÏ„ isthedetectionthreshold.
i
Thenotationâˆ¼indicatesacontentissampledfromadistributionuniformlyatrandom.
FalseDetectionRate(FDR): FDRistheprobabilitythatacontentC sampledfromthenon-AI-generatedcontent
distributionQuniformlyatrandomisdetectedasAI-generated. NotethatFDRdoesnotdependonusers. Formally,we
have:
FDR=Pr( max BA(D(C),w )â‰¥Ï„), (7)
j
jâˆˆ{1,2,Â·Â·Â·,s}
whereC âˆ¼Q.
(User-dependent)TrueAttributionRate(TAR): TARistheprobabilitythatanAI-generatedcontentiscorrectly
attributedtotheuserthatgeneratedthecontent. LikeTDR,TARalsodependsonusers. WedenotebyTAR thetrue
i
attributionrateforthewatermarkedcontentgeneratedbyuserU ,i.e.,TAR istheprobabilitythatacontentsampled
i i
fromP uniformlyatrandomiscorrectlyattributedtouserU . Formally,wehave:
i i
TAR =Pr( max BA(D(C),w )â‰¥Ï„ âˆ§BA(D(C),w )> max BA(D(C),w )), (8)
i j i j
jâˆˆ{1,2,Â·Â·Â·,s} jâˆˆ{1,2,Â·Â·Â·,s}/{i}
7Non-AI-gen AI-gen
Ground-truth label
Content Content
ð¶ð¶~ð’¬ð’¬ ð¶ð¶~ð’«ð’«ð‘–ð‘–
Detection Non-AI-gen AI-gen Non-AI-gen AI-gen
â‘  â‘¢
Incorrect Correct Incorrect
Attribution
attribution attribution attribution
â‘¡ â‘£ â‘¤
Figure2: Taxonomyofdetectionandattributionresults. Nodeswithredcolorindicateincorrectdetection/attribution.
whereC âˆ¼P ,thefirsttermmax BA(D(C),w )â‰¥Ï„ meansthatC isdetectedasAI-generated,andthe
i jâˆˆ{1,2,Â·Â·Â·,s} j
secondtermBA(D(C),w )>max BA(D(C),w )meansthatC isattributedtouserU . Notethatwe
i jâˆˆ{1,2,Â·Â·Â·,s}/{i} j i
havethefirsttermbecauseattributionisonlyappliedafterdetectingacontentasAI-generated.
OtherevaluationmetricscanbederivedfromTDR ,FDR,andTAR : Wenotethattherearealsootherrelevant
i i
detectionandattributionmetrics,e.g.,theprobabilitythatanAI-generatedcontentisincorrectlyattributedtoauser.
WeshowthatotherrelevantdetectionandattributionmetricscanbederivedfromTDR ,FDR,andTAR ,andthuswe
i i
focusonthesethreemetricsinourwork. Specifically,Figure2showsthetaxonomyofdetectionandattributionresults
fornon-AI-generatedcontentandAI-generatedcontentgeneratedbyuserU . Inthetaxonomytrees,thefirst-level
i
nodesrepresentground-truthlabelsofcontent;thesecond-levelnodesrepresentpossibledetectionresults;andthe
third-levelnodesrepresentpossibleattributionresults(notethatattributionisonlyperformedafteracontentisdetected
asAI-generated).
Inthetaxonomytrees,thereare5branchesintotal,whicharelabeledasâ‘ ,â‘¡,â‘¢,â‘£,andâ‘¤inthefigure. Eachbranch
startsfromarootnodeandendsataleafnode,andcorrespondstoametricthatmaybeofinterest. Forinstance,our
TDR istheprobabilitythatacontentC âˆ¼ P goesthroughbranchesâ‘£orâ‘¤;FDRistheprobabilitythatacontent
i i
C âˆ¼ Q goes through branch â‘¡; and TAR is the probability that a content C âˆ¼ P goes through branch â‘£. The
i i
probabilitythatacontentgoesthroughotherbranchescanbecalculatedusingTDR ,FDR,and/orTAR . Forinstance,
i i
theprobabilitythatanon-AI-generatedcontentC âˆ¼Qiscorrectlydetectedasnon-AI-generatedistheprobabilitythat
Cgoesthroughthebranchâ‘ ,whichcanbecalculatedas1âˆ’FDR.TheprobabilitythatanAI-generatedcontentC âˆ¼P
i
isincorrectlydetectedasnon-AI-generatedistheprobabilitythatC goesthroughthebranchâ‘¢,whichcanbecalculated
as1âˆ’TDR . TheprobabilitythatauserU â€™sAI-generatedcontentC âˆ¼P iscorrectlydetectedasAI-generatedbut
i i i
incorrectlyattributedtoadifferentuserU istheprobabilitythatC goesthroughthebranchâ‘¤,whichcanbecalculated
j
asTDR âˆ’TAR .
i i
5.3 FormalQuantificationofWatermarking
Intuitively,totheoreticallyanalyzethedetectionandattributionperformance(i.e.,TDR ,FDR,andTAR ),weneed
i i
aformalquantificationofawatermarkingmethodâ€™sbehavioratdecodingwatermarksinAI-generatedcontentand
non-AI-generatedcontent. Towardsthisend,weformallydefineÎ²-accurateandÎ³-randomwatermarkingasfollows:
Definition 3 (Î²-accurate watermarking). For a randomly sampled AI-generated content C âˆ¼ P embedded with
watermark w, the bits of the decoded watermark D(C) are independent and each bit matches with that of w with
probabilityÎ²,whereÎ² âˆˆ[0,1]. Formally,wehavePr(D(C)[k]=w[k])=Î²,whereC âˆ¼P,Disthedecoder,and[k]
representsthekthbitofawatermark. WesayawatermarkingmethodisÎ²-accurateifitsatisfiestheabovecondition.
Definition 4 (Î³-random watermarking). For a randomly sampled non-AI-generated content C âˆ¼ Q without any
watermarkembedded,thebitsofthedecodedwatermarkD(C)areindependentandeachbitis1withprobabilityat
least0.5âˆ’Î³andatmost0.5+Î³,whereÎ³ âˆˆ[0,0.5]. Formally,wehave|Pr(D(C)[k]=1)âˆ’0.5|â‰¤Î³,whereC âˆ¼Q
and[k]representsthekthbitofawatermark. WesayawatermarkingmethodisÎ³-randomifitsatisfiestheabove
condition.
8TheparameterÎ² isusedtocharacterizetheaccuracyofthewatermarkingmethodatencoding/decodingawatermark
in an AI-generated content. In particular, the watermarking method is more accurate when Î² is closer to 1. For a
Î²-accuratewatermarkingmethod,thenumberofmatchedbitsbetweenthedecodedwatermarkD(C)forawatermarked
contentC andtheground-truthwatermarkfollowsabinomialdistributionwithparametersnandÎ²,wherenisthe
watermark length. The parameter Î³ characterizes the behavior of the watermarking method for non-AI-generated
content. In particular, the decoded watermark for a non-AI-generated (i.e., unwatermarked) content is close to a
uniformlyrandomwatermark,whereÎ³ quantifiesthedifferencebetweenthem. Thewatermarkingmethodismore
randomfornon-AI-generatedcontentifÎ³ iscloserto0.
User-dependentÎ² : Sincetheusersâ€™AI-generatedcontentmayhavedifferentdistributionsP ,thesamewatermarking
i i
methodmayhavedifferentÎ² fordifferentusers. Tocapturethisphenomena,weconsiderthewatermarkingmethod
isÎ² -accurateforuserU â€™sAI-generatedcontentembeddedwithwatermarkw . NotethatthesameÎ³ isusedacross
i i i
differentuserssinceitisusedtocharacterizethebehaviorofthewatermarkingmethodfornon-AI-generatedcontent,
whichisuser-independent. TheparametersÎ² andÎ³canbeestimatedusingasetofAI-generatedandnon-AI-generated
i
content,asshowninourexperiments.
Incorporating post-processing: Our definition of Î²-accurate and Î³-random watermarking can also incorporate
post-processing(e.g.,JPEGcompression)thatanattacker/usermayapplytoAI-generatedornon-AI-generatedcontent.
In particular, we can replace D(C) as D(P(C)) in our definitions, where P stands for post-processing of content
C. WhenAI-generatedcontentispost-processed,thewatermarkingmethodmaybecomelessaccurate,i.e.,Î² may
decrease.
5.4 DetectionPerformance
DerivingalowerboundofTDR : Intuitively,anuserU â€™sAI-generatedcontentC âˆ¼P canbecorrectlydetectedas
i i i
AI-generatedintwocases:
â€¢ CaseI.ThedecodedwatermarkD(C)issimilarenoughtotheuserU â€™swatermarkw .
i i
â€¢ CaseII.ThedecodedwatermarkD(C)isdissimilartow butsimilarenoughtosomeotheruserâ€™swatermark.
i
Case II is more likely to happen when w is more dissimilar to some other userâ€™s watermark, i.e., when Î± =
i i
min BA(w ,w )issmaller. ThisisbecausethefactthatD(C)isdissimilartow andw isdissimilar
jâˆˆ{1,2,Â·Â·Â·,s}/{i} i j i i
tosomeotheruserâ€™swatermarkimpliesthatD(C)issimilartosomeotheruserâ€™swatermark. Formally,wecanderivea
lowerboundofTDR asfollows:
i
Theorem1(LowerboundofTDR ). SupposewearegivensuserswithanyswatermarksW ={w ,w ,Â·Â·Â· ,w }.
i 1 2 s
WhenthewatermarkingmethodisÎ² -accurateforuserU â€™sAI-generatedcontent,wehavealowerboundofTDR as
i i i
follows:
TDR â‰¥Pr(n â‰¥Ï„n)+Pr(n â‰¤nâˆ’Ï„nâˆ’Î± n), (9)
i i i i
where n follows a binomial distribution with parameters n and Î² , i.e., n âˆ¼ B(n,Î² ), Î± =
i i i i i
min BA(w ,w ),nisthewatermarklength,and0.5<Ï„ <Î² .
jâˆˆ{1,2,Â·Â·Â·,s}/{i} i j i
ThetwotermsinthelowerboundrespectivelyboundtheprobabilitiesforCaseIandCaseIIofcorrectlydetectinguser
U â€™sAI-generatedcontent. BasedonTheorem1,wehavethefollowingcorollary.
i
Corollary1. Whenthewatermarkingmethodismoreaccurate,i.e.,Î² iscloserto1,thelowerboundofTDR islarger.
i i
DerivinganupperboundofFDR: Intuitively,anon-AI-generatedcontentC âˆ¼ Qisalsoincorrectlydetectedas
AI-generatedintwocases: 1)thedecodedwatermarkD(C)issimilarenoughwithsomeuserâ€™swatermark,e.g.,w ;
1
and2)thedecodedwatermarkD(C)isdissimilartow butsimilarenoughtosomeotheruserâ€™swatermark. Basedon
1
thisintuition,wecanderiveanupperboundofFDRasfollows:
Theorem2(UpperboundofFDR). SupposewearegivensuserswithswatermarksW = {w ,w ,Â·Â·Â· ,w }and
1 2 s
watermarkw isselecteduniformlyatrandom. WehaveanupperboundofFDRasfollows:
1
FDRâ‰¤Pr(n â‰¥Ï„n)+Pr(n â‰¤nâˆ’Ï„n+Î± n), (10)
1 1 1
where n follows a binomial distribution with parameters n and 0.5, i.e., n âˆ¼ B(n,0.5), and Î± =
1 1 1
max BA(w ,w ).
jâˆˆ{2,3,Â·Â·Â·,s} 1 j
9NotethattheupperboundofFDRinTheorem2doesnotdependonÎ³-randomwatermarkingsinceweconsiderw
1
ispickeduniformlyatrandom. However,wefoundsuchupperboundisloose. Thisisbecausethesecondtermof
the upper bound considers the worst-case scenario of the s watermarks. The next theorem shows that when the s
watermarksareconstrained,inparticularselectedindependently,wecanderiveatighterupperboundofFDR.
Theorem3(AlternativeupperboundofFDR). SupposewearegivensuserswithswatermarksW ={w ,w ,Â·Â·Â· ,w }
1 2 s
selectedindependently. WhenthewatermarkingmethodisÎ³-randomfornon-AI-generatedcontent,wehaveanupper
boundofFDRasfollows:
FDRâ‰¤1âˆ’Pr(nâ€² <Ï„n)s, (11)
wherenâ€² âˆ¼B(n,0.5+Î³).
BasedonTheorem3,wehavethefollowingcorollary.
Corollary2. Whenthewatermarkingmethodismorerandomfornon-AI-generatedcontent,i.e.,Î³ iscloserto0,the
upperboundofFDRissmaller.
Impactofsonthebounds: Intuitively,whentherearemoreusers,i.e.,sislarger,itismorelikelytohaveatleastone
userwhosewatermarkhasabitwiseaccuracywiththedecodedwatermarkD(C)thatisnosmallerthanÏ„. Asaresult,
bothTDR andFDRmayincreaseassincreases,i.e.,scontrolsatrade-offbetweenTDR andFDR.Ourtheoretical
i i
resultsalignwiththisintuition. Ononehand,ourTheorem1showsthatthelowerboundofTDR islargerwhens
i
islarger. Inparticular,whensincreases,theparameterÎ± maybecomesmaller. Therefore,thesecondtermofthe
i
lowerboundincreases,leadingtoalargerlowerboundofTDR . Ontheotherhand,theupperboundofFDRinboth
i
Theorem2andTheorem3increasesassincreases. Inparticular,inTheorem2,theparameterÎ± becomeslargerwhen
1
sincreases,leadingtoalargersecondtermoftheupperbound.
User-agnostic vs. user-aware detection: Existing watermark-based detection is user-agnostic, i.e., it does not
distinguishbetweendifferentuserswhenembeddingawatermarkintoanAI-generatedcontent. Thefirsttermofthe
lowerboundinourTheorem1isalowerboundofTDRforuser-agnosticdetection;thefirsttermoftheupperboundin
ourTheorem2isanupperboundofFDRforuser-agnosticdetection;andtheupperboundwiths=1inourTheorem3
isanalternativeupperboundofFDRforuser-agnosticdetection. Therefore,comparedtouser-agnosticdetection,our
user-awaredetectionachieveslargerTDRbutalsolargerFDR.
5.5 AttributionPerformance
SupposewearegivenauserU â€™sAI-generatedcontentC âˆ¼P . Intuitively,ifthewatermarkw isverydissimilarto
i i i
theothersâˆ’1watermarks,i.e.,Î± =max BA(w ,w )issmall,thenC canbecorrectlyattributedto
i jâˆˆ{1,2,Â·Â·Â·,s}/{i} i j
U onceC isdetectedasAI-generated,i.e.,thedecodedwatermarkD(C)issimilarenoughtow . Ifthewatermarkw
i i i
issimilartosomeotherwatermark,i.e.,Î± islarge,thenthedecodedwatermarkD(C)hastobeverysimilartow in
i i
ordertocorrectlyattributeC toU . Formally,wecanderivealowerboundofTAR inthefollowingtheorem.
i i
Theorem4(LowerboundofTAR ). SupposewearegivensuserswithanyswatermarksW ={w ,w ,Â·Â·Â· ,w }.
i 1 2 s
WhenthewatermarkingmethodisÎ² -accurateforuserU â€™sAI-generatedcontent,wehavealowerboundofTAR as
i i i
follows:
1+Î±
TAR â‰¥Pr(n â‰¥max{âŒŠ inâŒ‹+1,Ï„n}), (12)
i i 2
where n follows a binomial distribution with parameters n and Î² , i.e., n âˆ¼ B(n,Î² ), Î± =
i i i i i
max BA(w ,w ),nisthewatermarklength,andÏ„ isthedetectionthreshold.
jâˆˆ{1,2,Â·Â·Â·,s}/{i} i j
OurTheorem4showsthatthelowerboundofTAR islargerwhenÎ² iscloserto1,i.e.,attributionperformanceis
i i
betterwhenthewatermarkingmethodismoreaccurate. Moreover,thelowerboundislargerwhenÎ± issmallerbecause
i
itiseasiertodistinguishbetweenusers. Thisisatheoreticalmotivationonwhyourwatermarkselectionproblemaims
toselectwatermarksfortheuserssuchthattheyhavesmallpairwisebitwiseaccuracy.
Detectionimpliesattribution: WhenÏ„ > 1+ 2Î±i,thelowerboundofTAR iinTheorem4becomesTAR
i
â‰¥Pr(n
i
â‰¥Ï„n).
ThesecondtermofthelowerboundofTDR inTheorem1isusuallymuchsmallerthanthefirstterm. Inotherwords,
i
thelowerboundofTDR
i
isalsoroughlyPr(n
i
â‰¥Ï„n). Therefore,whenÏ„ islargeenough(i.e.,> 1+ 2Î±i),TDR
i
and
TAR areveryclose,whichisalsoconfirmedinourexperiments. ThisresultindicatesthatonceanAI-generatedcontent
i
iscorrectlydetected,itwouldalsobecorrectlyattributed.
101.000
0.975
0.950
StableDiffusion
0.925 Midjourney
DALLÂ·E2
0.900
0 1 2 3 4 5
RankIndex(Log10Scale)
Figure3: RankedTARsofthe100,000users.
6 Experiments
Inourmajorexperiments,wefocusondetectionandattributionofAI-generatedimages. InSection7,wealsoshow
resultsforAI-generatedtexts.
6.1 ExperimentalSetup
Datasets: WeconsiderbothAI-generatedandnon-AI-generatedimagesasfollows:
AI-generated. WeconsiderthreeGenAImodels,i.e.,StableDiffusion,Midjourney,andDALL-E2,whichcorrespond
tothreedatasetsofAI-generatedimages. ForStableDiffusion,weusepubliclyavailabledatasetDiffusionDB[26]. For
Midjourney,wecollectitsgeneratedimagesfromawebsite[27]. ForDALL-E2,wealsocollectitsgeneratedimages
from a website [28]. Following HiDDeN [11], for each dataset, we sample 10,000 images for training watermark
encodersanddecoders;andwesample1,000imagesfortestingtheperformanceofwatermark-baseddetectionand
attribution.
Non-AI-generated. To evaluate the likelihood that a non-AI-generated image is falsely detected as AI-generated,
weneednon-AI-generatedimages. Forthispurpose,wecombinetheimagesinthreebenchmarkdatasets,including
COCO[29],ImageNet[30],andConceptualCaption[31],andsample1,000imagesfromthecombinedsetuniformly
atrandomasournon-AI-generatedimagedataset.
Wescaletheimagesizeinalldatasetstobe128Ã—128.
Watermarkingmethod: Weusethestate-of-the-artlearning-basedwatermarkingmethodHiDDeN[11]. Unless
otherwisementioned,weusestandardtrainingwiththedefaultparametersettingsinthepubliclyavailablecode,except
thatweuseResNet18asthedecodertoenlargethecapacitytoencode/decodelongerwatermarks. ForeachGenAI
model,wetrainawatermarkencoder/decoderusingthecorrespondingAI-generatedimagetrainingsetandevaluatethe
detectionandattributionperformanceonthetestingset.
Watermarkselectionmethods: WeevaluateRandom,NRG,andA-BSTAwatermarkselectionmethods. Unless
otherwisementioned,weuseA-BSTA.NotethatwedonotuseBSTAbecauseitisnotscalable. Forinstance,ittakes
BSTAmorethan8hourstogenerateeven16watermarks.
Evaluationmetrics: AsdiscussedinSection5.2,wemainlyusethreeevaluationmetrics,i.e.,TrueDetectionRate
(TDR),FalseDetectionRate(FDR),andTrueAttributionRate(TAR).FDRisthefractionofthe1,000non-AI-generated
imagesthatarefalselydetectedasAI-generated. FDRdoesnotdependonusers. Incontrast,TDRandTARdependon
usersbecausetheyusedifferentwatermarks,leadingtodifferentdistributionsofAI-generatedimages. Foreachofthes
users,weembeditswatermarkinto100imagesrandomlysampledfromatestingAI-generatedimagedataset;andthen
wecalculatetheTDRandTARfortheuser.
In most of our experiments, we report the average TDR and average TAR, which respectively are the TDR and
TARaveragedamongthesusers. However, averageTDRandaverageTARcannotreflectthedetection/attribution
performancefortheworst-caseusers,i.e.,someusersmayhavequitesmallTDR/TAR,buttheaverageTDR/TARmay
stillbeverylarge. Therefore,wefurtherconsiderthe1%users(atleast1user)withthesmallestTDR(orTAR)and
reporttheiraverageTDR(orTAR),whichwecallworst1%TDR(orworst1%TAR).
Parametersettings: Bydefault,wesetthenumberofuserss=100,000,watermarklengthn=64,anddetection
threshold Ï„ = 0.9. To compute TAR of an user, we need to compute the bitwise accuracy between the decoded
watermarkandeachuserâ€™swatermarkforeachwatermarkedimage,andthuswesets=100,000duetoourlimited
11Table1: TheaverageÎ² ofallusersandoftheworst1%ofusers,whereÎ² ofauserisestimatedusingthetesting
i i
images.
WatermarkLengthn 32 48 64 80
AverageÎ² 0.99 0.99 0.99 0.97
i
Worst1%Î² 0.92 0.97 0.90 0.84
i
1.00 1.00 1.00
0.75 averageTDR 0.75 averageTDR 0.75 averageTDR
averageTAR averageTAR averageTAR
0.50 worst1%TDR 0.50 worst1%TDR 0.50 worst1%TDR
worst1%TAR worst1%TAR worst1%TAR
0.25 FDR 0.25 FDR 0.25 FDR
0.00 0.00 0.00
1 2 3 4 5 6 32 48 64 80 0.70 0.75 0.80 0.85 0.90 0.95
NumberofUserss(Log10Scale) WatermarkLengthn DetectionThresholdÏ„
(a) Impactofs (b) Impactofn (c) ImpactofÏ„
Figure4: Impactofnumberofuserss,watermarklengthn,anddetectionthresholdÏ„ ondetectionandattribution
performance.
computationresources,butwewillalsoexplores=1,000,000inoneofourexperimentstoshowtheresultswhen
thenumberofusersinattributionisverylarge. Whenpost-processingmethodsareappliedtowatermarkedimages,
thewatermarkingmethodmaybecomelessaccurate(i.e.,Î² maydecrease)andthuswereduceÏ„ tobe0.85. Unless
otherwisementioned,weshowresultsfortheStableDiffusiondataset.
6.2 WithoutPost-processing
In this section, we show results when the AI-generated, watermarked images are not post-processed. Specifically,
weexploretheimpactofthethreeparameters,includingthenumberofuserss,watermarklengthn,anddetection
thresholdÏ„,onthedetectionandattributionperformance. Whenexploringtheimpactofoneparameter,wefixtheother
twoparametersastheirdefaultsettings.
Mainresults: ForeachGenAImodel,wecomputetheTDR/TARofeachuserandtheFDR.TheFDRsforthethree
GenAImodelsarenearly0. Then,weranktheusersâ€™TARs(orTDRs)inanon-descendingorder. Figure3showsthe
rankedTARsofthe100,000usersforthethreeGenAImodels. NotethatthecurveofTDRoverlapswiththatofTARfor
aGenAImodelandthusisomittedinthefigureforsimplicity. TDRandTARoverlapbecauseÏ„ =0.9> 1+Î±i (0.89in
2
ourexperiments),whichisconsistentwithourtheoreticalanalysisinSection5.5thatshowsdetectionimpliesattribution
insuchsettings. Ourresultsshowthatwatermark-baseddetectionandattributionareaccuratewhentheAI-generated,
watermarkedimagesarenotpost-processed. Specifically,theworstTARorTDRofauserislargerthan0.94;lessthan
0.1%ofusershaveTARs/TDRssmallerthan0.98;and85%ofusershaveTARs/TDRsof1forMidjourneyandDALL-E
2,and60%ofusershaveTARs/TDRsof1forStableDiffusion.
Impactofnumberofuserss: Figure4ashowstheaverageTDR,averageTAR,worst1%TDR,worst1%TAR,and
FDRwhensvariesfrom10to1,000,000. Wehavetwoobservations. First,bothaverageTDRandaverageTARare
consistentlycloseto1,andFDRisconsistentlycloseto0,whichmeansourdetectionandattributionareaccurate.
Second,worst1%TDRandworst1%TARdecreaseassincreases. Thisisbecausewhentherearemoreusers,the
worst1%ofthemhavesmallerTDRsandTARs. Moreover,theseworst1%ofusersalsomaketheaverageTDRand
averageTARdecreaseslightlywhensincreasesfrom100,000to1,000,000.
Impactofwatermarklengthn: Figure4bshowstheaverageTDR,averageTAR,worst1%TDR,worst1%TAR,
andFDRwhenthewatermarklengthnvariesfrom32to80. TheaverageTDRandaverageTARslightlydecrease
whennincreasesfrom64to80,whiletheworst1%TDR/TARslightlyincreasesasnincreasesfrom32to48andthen
decreasesasnfurtherincreases. Table1showstheestimatedaverageÎ² ofallusersandaverageÎ² oftheworst1%
i i
ofusersinÎ²-accuratewatermarking. WeobservethatthepatternsofaverageTDR/TARandworst1%TDR/TARare
consistentwiththoseofaverageÎ² andworst1%Î² ,respectively. Theseobservationsareconsistentwithourtheoretical
i i
analysiswhichshowsthatTDRorTARincreasesasÎ² increases. OurresultalsoimpliesthatHiDDeNwatermarking
i
maybeunabletoaccuratelyencode/decodeverylongwatermarks.
121.00 1.00 1.00 1.00
averageTDR averageTDR
0.75 0.75 averageTAR 0.75 0.75 averageTAR
averageTDR FDR averageTDR FDR
0.50 averageTAR 0.50 SSIM 0.50 averageTAR 0.50 SSIM
FDR FDR
SSIM SSIM
0.25 0.25 0.25 0.25
0.00 0.00 0.00 0.00
99 90 80 60 40 20 0.05 0.10 0.20 0.30 0.1 0.4 0.7 1.0 1.2 1.01.3 2.0 3.0 4.0
QualityFactorQ StandardDeviationÏƒ StandardDeviationÏƒ Parametera
(a)JPEG (b)Gaussiannoise (c)Gaussianblur (d)Brightness/Contrast
Figure 5: Detection and attribution results when AI-generated and non-AI-generated images are post-processed
bycommonpost-processingmethodswithdifferentparameters. SSIMmeasuresthequalityofanimageafterpost-
processing.
ImpactofdetectionthresholdÏ„: Figure4cshowstheaverageTDR,averageTAR,worst1%TDR,worst1%TAR,and
FDRwhenthedetectionthresholdÏ„ variesfrom0.7to0.95. WhenÏ„ increases,bothTDRandTARdecrease,while
FDRalsodecreases. Suchtrade-offofÏ„ isconsistentwithTheorem1,3,and4.
6.3 CommonPost-processing
Commonpost-processingmethods: Commonpost-processingmethodsareoftenusedtoevaluatetherobustnessof
watermarkinginnon-adversarialsettings. Eachpost-processingmethodhasspecificparametersthatgoverntheextent
ofperturbationintroducedtoanimage. Inparticular,weconsidercommonpost-processingmethodsasfollows.
JPEG.JPEG[32]methodcompressesanimageviaadiscretecosinetransform. Theperturbationintroducedtoan
imageisdeterminedbythequalityfactorQ. AnimageisperturbedmorewhenQissmaller.
Gaussiannoise. ThismethodperturbsanimageviaaddingarandomGaussiannoisetoeachpixel. Inourexperiments,
themeanoftheGaussiandistributionis0. Theperturbationintroducedtoanimageisdeterminedbytheparameter
standarddeviationÏƒ.
Gaussianblur. ThismethodblursanimageviaaGaussianfunction. Inourexperiments,wefixkernelsizes=5. The
perturbationintroducedtoanimageisdeterminedbytheparameterstandarddeviationÏƒ.
Brightness/Contrast. Thismethodperturbsanimageviaadjustingthebrightnessandcontrast. Formally,themethod
hascontrastparameteraandbrightnessparameterb,whereeachpixelxisconvertedtoax+b. Inourexperiments,we
fixb=0.2andvaryatocontroltheperturbation.
Adversarialtraining[11]: WeuseadversarialtrainingtotrainHiDDeN.Specifically,duringtraining,werandomly
sampleapost-processingmethodfromnopost-processingandcommonpost-processingwitharandomparameterto
post-processeachwatermarkedimageinamini-batch. Followingpreviouswork[11],weconsiderthefollowingrange
ofparametersduringadversarialtraining: Q âˆˆ[10,99]forJPEG,Ïƒ âˆˆ[0,0.5]forGaussiannoise,Ïƒ âˆˆ[0,1.5]for
Gaussianblur,andaâˆˆ[1,20]forBrightness/Contrast.
Results: Figure 5 shows the detection/attribution results when a common post-processing method with different
parametersisappliedtothe(AI-generatedandnon-AI-generated)images. SSIM[33]isapopularmetrictomeasure
visual similarity between two images. The SSIM in Figure 5 is the average between (AI-generated and non-AI-
generated)imagesandtheirpost-processedversions. WenotethatwhenHiDDeNistrainedusingstandardtraining,
detectionandattributionbecomeinaccurateafterAI-generatedimagesarepost-processed,asshowninFigure10in
Appendix. OurresultsshowthatdetectionandattributionusinganadversariallytrainedHiDDeNarerobusttocommon
post-processing. In particular, the average TDR and TAR are still high when a common post-processing does not
sacrificeimagequalitysubstantially. Forinstance,averageTDRandTARstarttodecreasewhenthequalityfactorQof
JPEGissmallerthan90. However,theaverageSSIMbetweenwatermarkedimagesandtheirpost-processedversions
alsodropsquickly. NotethatGaussianblurwithÏƒ =1.2alreadyinfluencesvisualqualitysubstantiallyevenifSSIM
islargerthan0.75. Figure11inAppendixshowsawatermarkedimageandtheversionspost-processedbydifferent
methods.
6.4 AdversarialPost-processing
Inadversarialsettings, anattackermayapplyadversarialpost-processing[16]toperturbawatermarkedimageto
evadedetection/attribution. HiDDeNisnotrobusttoadversarialpost-processinginthewhite-boxsetting[16],i.e.,
130.4
0.2
0.0
100 5001k 2k 4k 10k 100k
QueryBudget
Figure6: AverageSSIMbetweenwatermarkedimagesandtheiradversariallypost-processedversionsasafunctionof
thequerybudgetintheblack-boxsetting.
1.00 1.0
0.9
0.75
0.8
0.50
0.7
0.25 Random 0.6 R Na Rn Gdom
NRG
0.00 A-BSTA 0.5 A-BSTA
0.75 0.80 0.85 0 1 2 3
Î±i RankIndex(Log10Scale)
(a) CDFofÎ± (b) RankedTARs
i
Figure7: (a)Thecumulativedistributionfunction(CDF)ofÎ± and(b)rankedTARsoftheworst1,000usersforthe
i
threewatermarkselectionmethods.
Table2: Theaveragerunningtimefordifferentwatermarkselectionmethodstogenerateawatermark.
Random NRG A-BSTA
Time(ms) 0.01 2.11 24.00
adversarialpost-processingcanremovethewatermarkfromawatermarkedimagewithoutsacrificingitsvisualquality.
Thus,HiDDeN-baseddetection/attributionisalsonotrobusttoadversarialpost-processinginthewhite-boxsetting,i.e.,
TDR/TARcanbereducedto0whilemaintainingimagequality.
Figure6showstheaverageSSIMbetweenwatermarkedimagesandtheiradversariallypost-processedversionsas
afunctionofquerybudgetintheblack-boxsetting(i.e.,WEvade-B-Q[16]),wherethequerybudgetisthenumber
of queries to the detection API for each watermarked image. HiDDeN is trained via adversarial training in these
experiments. Both TDR and TAR are 0 in these experiments since WEvade-B-Q always guarantees evasion [16].
However,adversarialpost-processingsubstantiallysacrificesimagequalityintheblack-boxsetting(i.e.,SSIMissmall)
evenifanattackercanquerythedetectionAPIforalargenumberoftimes. Figure12inAppendixshowsseveral
examplesofadversariallypost-processedimageswithdegradedvisualquality. OurresultsshowthatHiDDeNandthus
ourHiDDeN-baseddetection/attributionhavegoodrobustnesstoadversarialpost-processingintheblack-boxsetting.
WenotethatJiangetal.[16]showedadversarialpost-processingdoesnotsacrificeimagevisualqualityintheblack-box
settingwhenevadingHiDDeN,whichwecanreproduceusingtheirpubliclyavailablecodeandthesameparameter
setting. However,theyusewatermarklength30,whileweuse64;andtheyuseasimpleneuralnetworkasthedecoder,
whileweuseResNet18asthedecoder. Moreover,weusestrongeradversarialtrainingwithalargerrangeofparameters
forthepost-processing. Ourresultsshowthatlongerwatermarks,moreexpressivedecoder,andstrongeradversarial
trainingcanfurtherenhancerobustnessofHiDDeN.
6.5 DifferentWatermarkSelectionMethods
Runningtime: Table2showstherunningtimetogenerateawatermarkaveragedamongthe100,000watermarks.
AlthoughA-BSTAisslowerthanRandomandNRG,therunningtimeisacceptable,i.e.,ittakesonly24mstogenerate
awatermarkonaverage.
DistributionofÎ± : RecallthatTAR ofauserdependsonthemaximumbitwiseaccuracybetweenthewatermarkw
i i i
andtheremainingwatermarks,i.e.,Î± =max BA(w ,w ). Figure7ashowsthecumulativedistribution
i jâˆˆ{1,2,Â·Â·Â·,s}/{i} i j
14
noitcnuFnoitubirtsiDevitalumuC1.0 Theoretical 1.0 Theoretical
Empirical Empirical
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
TDR TAR FDR TDR TAR FDR
(a)Nopost-processing (b)JPEG
Figure8: Theoreticalvs. empiricalresults.
1.00 1.00
0.75 0.75
averageTDR averageTDR
0.50 averageTAR 0.50 averageTAR
FDR FDR
0.25 0.25
0.00 0.00
1 2 3 4 5 1 2 3 4 5
NumberofUserss(Log10Scale) NumberofUserss(Log10Scale)
(a) Nopost-processing (b) Paraphrasing
Figure9: Resultsofwatermark-baseddetectionandattributionforAI-generatedtexts.
functionofÎ± amongtheswatermarksgeneratedbydifferentwatermarkselectionmethods. Ourresultsshowthatall
i
watermarksgeneratedbyA-BSTAhaveÎ± smallerthan0.74. However,RandomandNRGgeneratemanywatermarks
i
withlargerÎ± ,andRandomistheworstamongthethreemethods. ThisisbecauseRandomselectiondoesnotexplicitly
i
minimizeÎ± whengeneratingwatermarks.
i
TARs: Figure7bshowstherankedTARsoftheworst1,000users,wheretheAI-generatedimagesarepost-processed
by JPEG compression with quality factor Q = 90 and HiDDeN is adversarially trained. The results indicate that
A-BSTAoutperformsNRG,whichoutperformsRandom. ThisisbecauseA-BSTAselectswatermarkswithsmallerÎ± ,
i
whileRandomselectswatermarkswithlargerÎ± asshowninFigure7a.
i
6.6 Theoreticalvs. EmpiricalResults
ThetheoreticallowerboundsofTDRandTARofauserarerespectivelycalculatedusingTheorem1and4,whilethe
theoreticalupperboundofFDRiscalculatedusingTheorem3. WeestimateÎ² asthebitwiseaccuracybetweenthe
i
decodedwatermarkandw averagedamongthetestingAI-generatedimages,andestimateÎ³ usingthefractionofbits
i
inthedecodedwatermarksthatare1amongthenon-AI-generatedimages. Figure8showstheaveragetheoreticalvs.
empiricalTDR/TAR,andtheoreticalvs. empiricalFDR,whennopost-processingorJPEGwithQ=90isapplied. The
resultsshowthatourtheoreticallowerboundsofTDRandTARmatchwithempiricalresultswell,whichindicatesthat
ourderivedlowerboundsaretight. ThetheoreticalupperboundofFDRisnotablyhigherthantheempiricalFDR.This
isbecausesomebitsmayhavelargerprobabilitiestobe1or0intheexperiments,butourtheoreticalanalysistreatsthe
bitsequally,leadingtoalooseupperboundofFDR.
7 DiscussionandLimitations
AI-generatedtexts: OurmethodcanalsobeusedforthedetectionandattributionofAI-generatedtexts. Fortext
watermarking,weusealearning-basedmethodcalledAdversarialWatermarkingTransformer(AWT)[22]. Givenatext,
AWTencoderembedsabitstringwatermarkintoit;andgivena(watermarkedorunwatermarked)text,AWTdecoder
decodesawatermarkfromit. Followingtheoriginalpaper[22],wetrainAWTontheword-levelWikiText-2dataset,
whichisderivedfromWikipediaarticles[34]. Weusemostofthehyperparametersettingsinthepubliclyavailablecode
ofAWTexcepttheweightofthewatermarkdecodingloss. Tooptimizewatermarkdecodingaccuracy,weincreasethis
weightduringtraining. ThedetailedhyperparametersettingsfortrainingcanbefoundinTable4inAppendix.
We use A-BSTA to select usersâ€™ watermarks. For each user, we sample 10 text segments from the test corpus
uniformlyatrandom,andperformwatermark-baseddetectionandattribution. Moreover,weusetheunwatermarked
testcorpustocalculateFDR.Figure9showsthedetectionandattributionresultswhenthereisnopost-processingand
paraphrasing[35]isappliedtotexts,wheren=64,Ï„ =0.85,andsrangesfrom10to100,000.Duetothefixed-length
15natureofAWTâ€™sinput,weconstraintheoutputlengthoftheparaphrasertoacertainrange. Whenparaphrasingis
used,weextendadversarialtrainingtotrainAWT,andSectionGinAppendixshowsthedetails. Notethattheaverage
TDR/TARandFDRareallnearly0whenAWTistrainedbystandardtrainingandparaphrasingisappliedtotexts.
TheresultsshowthatourmethodisalsoapplicableforAI-generatedtexts,andadversariallytrainedAWThasbetter
robustnesstoparaphrasing.
AttributionofGenAIservices: Inthiswork,wefocusonattributionofcontenttousersforaspecificGenAIservice.
AnotherrelevantattributionproblemistotracebacktheGenAIservice(e.g.,Googleâ€™sImagen,OpenAIâ€™sDALL-E3,
orStableDiffusion)thatgeneratedagivencontent. OurmethodcanalsobeappliedtosuchGenAI-service-attribution
problembyassigningadifferentwatermarktoeachGenAIservice. Moreover,wecanperformattributiontoGenAI
serviceandusersimultaneously. Specifically,wecandividethewatermarkspaceintomultiplesubspaces;andeach
GenAIserviceusesasubspaceofwatermarksandassignswatermarksinitssubspacetoitsusers. Inthisway,wecan
tracebackboththeGenAIserviceanditsuserthatgeneratedagivencontent.
8 ConclusionandFutureWork
Wefindthatwatermarkcanbeusedforuser-awaredetectionandattributionofAI-generatedcontent. Moreover,via
boththeoreticalanalysisandempiricalevaluation,wefindthatsuchdetectionandattributioninherittheaccuracy/(non-
)robustness properties of the watermarking method. For instance, learning-based watermarking methods [11] are
accurate and robust to common post-processing; and thus detection and attribution based on such a watermarking
methodarealsoaccurateandrobusttocommonpost-processing. However,sincewatermarkingisnotyetrobustto
adversarialpost-processinginthewhite-boxsetting[16],detectionandattributionarenotyetrobustinsuchadversarial
settings. Wealsofindthatselectingdissimilarwatermarksfortheusersenhancesattributionperformance. Animportant
futureworkistodeveloprobustwatermarkingmethodsinadversarialsettings.
References
1. Dhaliwal, S. Elon Musk isnâ€™t dating GMâ€™s Mary Barra: he has this to say though on the photos. https:
//www.benzinga.com/news/23/03/31505898/elon-musk-isnt-dating-gms-mary-barra-he-has-
this-to-say-though-on-the-photos.2023.
2. Escalante-De Mattei, S. US Copyright Office: AI Generated Works Are Not Eligible for Copyright. https:
//www.artnews.com/art-news/news/ai-generator-art-text-us-copyright-policy-1234661683.
2023.
3. Ramesh,A.,Dhariwal,P.,Nichol,A.,Chu,C.&Chen,M.Hierarchicaltext-conditionalimagegenerationwith
cliplatents.arXivpreprintarXiv:2204.06125(2022).
4. Gowal,S.&Kohli,P.IdentifyingAI-generatedimageswithSynthID.https://deepmind.google/discover/
blog/identifying-ai-generated-images-with-synthid.2023.
5. Rombach,R.,Blattmann,A.,Lorenz,D.,Esser,P.&Ommer,B.High-resolutionimagesynthesiswithlatent
diffusionmodels.IEEE/CVFConferenceonComputerVisionandPatternRecognition(2022).
6. Mehdi,Y.AnnouncingMicrosoftCopilot,youreverydayAIcompanion.https://blogs.microsoft.com/
blog/2023/09/21/announcing-microsoft-copilot-your-everyday-ai-companion.2023.
7. Pereira,S.&Pun,T.Robusttemplatematchingforaffineresistantimagewatermarks.IEEETransactionson
ImageProcessing(2000).
8. Bi,N.,Sun,Q.,Huang,D.,Yang,Z.&Huang,J.Robustimagewatermarkingbasedonmultibandwaveletsand
empiricalmodedecomposition.IEEETransactionsonImageProcessing(2007).
9. Wang,Q.invisible-watermarkhttps://github.com/ShieldMnt/invisible-watermark.2021.
10. Kandi,H.,Mishra,D.&Gorthi,S.R.S.Exploringthelearningcapabilitiesofconvolutionalneuralnetworksfor
robustimagewatermarking.Computers&Security(2017).
11. Zhu,J.,Kaplan,R.,Johnson,J.&Fei-Fei,L.Hidden:Hidingdatawithdeepnetworks.EuropeanConferenceon
ComputerVision(2018).
12. Luo, X., Zhan, R., Chang, H., Yang, F. & Milanfar, P. Distortion agnostic deep watermarking. IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(2020).
13. Wen,B.&Aydore,S.Romark:Arobustwatermarkingsystemusingadversarialtraining.ConferenceonNeural
InformationProcessingSystemsWorkshop(2019).
14. Fernandez,P.,Couairon,G.,JÃ©gou,H.,Douze,M.&Furon,T.TheStableSignature:RootingWatermarksin
LatentDiffusionModels.InternationalConferenceonComputerVision(2023).
1615. Goodfellow,I.,Shlens,J.&Szegedy,C.ExplainingandHarnessingAdversarialExamples.InternationalConfer-
enceonLearningRepresentations(2015).
16. Jiang,Z.,Zhang,J.&Gong,N.Z.EvadingWatermarkbasedDetectionofAI-GeneratedContent.ACMConference
onComputerandCommunicationsSecurity(2023).
17. Brain,G.Imagen.https://imagen.research.google.2023.
18. Lanctot,J.K.,Li,M.,Ma,B.,Wang,S.&Zhang,L.Distinguishingstringselectionproblems.Informationand
Computation(2003).
19. Gramm,J.,Niedermeier,R.&Rossmanith,P.Fixed-parameteralgorithmsforcloseststringandrelatedproblems.
Algorithmica(2003).
20. Wen,Y.,Kirchenbauer,J.,Geiping,J.&Goldstein,T.Tree-RingWatermarks:FingerprintsforDiffusionImages
thatareInvisibleandRobust.ConferenceonNeuralInformationProcessingSystems(2023).
21. Kirchenbauer,J.etal.AWatermarkforLargeLanguageModels.InternationalConferenceonMachineLearning
(2023).
22. Abdelnabi,S.&Fritz,M.Adversarialwatermarkingtransformer:Towardstracingtextprovenancewithdata
hiding.IEEESymposiumonSecurityandPrivacy(2021).
23. Madry, A., Makelov, A., Schmidt, L., Tsipras, D. & Vladu, A. Towards Deep Learning Models Resistant to
AdversarialAttacks.InternationalConferenceonLearningRepresentations(2018).
24. Sadasivan,V.S.,Kumar,A.,Balasubramanian,S.,Wang,W.&Feizi,S.CanAI-GeneratedTextbeReliably
Detected?arXivpreprintarXiv:2303.11156(2023).
25. Chen,Z.-Z.,Ma,B.&Wang,L.Randomizedfixed-parameteralgorithmsforthecloseststringproblem.Algorith-
mica(2016).
26. Wang,Z.J.etal.DiffusionDB:ALarge-ScalePromptGalleryDatasetforText-to-ImageGenerativeModels.
AnnualMeetingoftheAssociationforComputationalLinguistics(2023).
27. Turc,I.&Nemade,G.MidjourneyUserPrompts&GeneratedImages(250k)https://www.kaggle.com/ds/
2349267.2022.
28. Images,D.https://dalle2.gallery.2023.
29. Lin,T.-Y.etal.Microsoftcoco:Commonobjectsincontext.EuropeanConferenceonComputerVision(2014).
30. Deng,J.etal.Imagenet:Alarge-scalehierarchicalimagedatabase.IEEE/CVFConferenceonComputerVision
andPatternRecognition(2009).
31. Sharma,P.,Ding,N.,Goodman,S.&Soricut,R.Conceptualcaptions:Acleaned,hypernymed,imagealt-text
datasetforautomaticimagecaptioning.AnnualMeetingoftheAssociationforComputationalLinguistics(2018).
32. Zhang, C., Karjauv, A., Benz, P. & Kweon, I. S. Towards robust data hiding against (jpeg) compression: A
pseudo-differentiabledeeplearningapproach.arXivpreprintarXiv:2101.00973(2020).
33. Wang,Z.,Bovik,A.C.,Sheikh,H.R.&Simoncelli,E.P.Imagequalityassessment:fromerrorvisibilityto
structuralsimilarity.IEEETransactionsonImageProcessing(2004).
34. Merity,S.,Xiong,C.,Bradbury,J.&Socher,R.Pointersentinelmixturemodels.arXivpreprintarXiv:1609.07843
(2016).
35. Damodaran,P.Parrot:ParaphrasegenerationforNLU.versionv1.0.2021.
171.00 1.00 averageTDR 1.00 1.00 averageTDR
averageTAR averageTAR
0.75 0.75 FDR 0.75 0.75 FDR
averageTDR SSIM averageTDR SSIM
0.50 a Fv Der RageTAR 0.50 0.50 a Fv Der RageTAR 0.50
SSIM SSIM
0.25 0.25 0.25 0.25
0.00 0.00 0.00 0.00
99 95 90 80 70 0.05 0.10 0.20 0.30 0.1 0.4 0.7 1.0 1.2 1.01.3 2.0 3.0 4.0
QualityFactorQ StandardDeviationÏƒ StandardDeviationÏƒ Parametera
(a)JPEG (b)Gaussiannoise (c)Gaussianblur (d)Brightness/Contrast
Figure10: DetectionandattributionresultswhenAI-generatedandnon-AI-generatedimagesarepost-processedby
commonpost-processingmethodswithdifferentparameters. HiDDeNistrainedusingstandardtraining.
(a)Watermarked (b)JPEG (c)Gaussiannoise (d)Gaussianblur (e)Brightness/Contrast
Figure11: Awatermarkedimageandtheversionspost-processedbyJPEGwithQ=60,GaussiannoisewithÏƒ=0.3,
GaussianblurwithÏƒ=1.2,andBrightness/Contrastwitha=4.0.
(a)100 (b)500 (c)1k (d)10k (e)100k
Figure12: Perturbedwatermarkedimagesobtainedbyadversarialpost-processingwithdifferentnumberofqueriesto
thedetectionAPIintheblack-boxsetting.
Table3: ThemaximumpairwisebitwiseaccuracyamongthewatermarksgeneratedbyNRGandA-BSTAfordifferent
initializations.
Â¬w initialization Randominitialization
1
NRG 0.766 0.750
A-BSTA 0.875 0.734
A ProofofTheorem1
ForC âˆ¼P ,wedenotew =D(C),n =BA(w,w )n,andn =BA(w,w )nforj âˆˆ{1,2,Â·Â·Â· ,s}/{i}. Thenwe
i i i j j
havethefollowing:
|wâˆ’Â¬w | =n ,
i 1 i
|Â¬w âˆ’w | =BA(w ,w )n,
i j 1 i j
|wâˆ’w | =nâˆ’n ,
j 1 j
18whereÂ¬w meansflippingeachbitofthewatermarkw ,|Â·| isâ„“ distancebetweentwobinaryvectors. Accordingto
i i 1 1
thetriangleinequality,wehave:
|wâˆ’w | â‰¤|wâˆ’Â¬w | +|Â¬w âˆ’w |
j 1 i 1 i j 1
=n +BA(w ,w )n.
i i j
Therefore,wederivethelowerboundofn forj âˆˆ{1,2,Â·Â·Â· ,s}/{i}asfollows:
j
n =nâˆ’|wâˆ’w |
j j 1
â‰¥nâˆ’n âˆ’BA(w ,w )n.
i i j
Thus,wederivethelowerboundofTDR asfollows:
i
TDR =1âˆ’Pr(n <Ï„nâˆ§ max n <Ï„n)
i i j
jâˆˆ{1,2,Â·Â·Â·,s}/{i}
â‰¥1âˆ’Pr(n <Ï„nâˆ§ max nâˆ’n âˆ’BA(w ,w )n<Ï„n)
i i i j
jâˆˆ{1,2,Â·Â·Â·,s}/{i}
=1âˆ’Pr(n <Ï„nâˆ§nâˆ’n âˆ’Î± n<Ï„n)
i i i
=1âˆ’Pr(nâˆ’Ï„nâˆ’Î± n<n <Ï„n)
i i
=Pr(n â‰¥Ï„n)+Pr(n â‰¤nâˆ’Ï„nâˆ’Î± n),
i i i
wheren âˆ¼B(n,Î² )andÎ± =min BA(w ,w ).
i i i jâˆˆ{1,2,Â·Â·Â·,s}/{i} i j
B ProofofCorollary1
According to Theorem 1, the lower bound of TDR is 1 âˆ’ Pr(n âˆ’ Ï„n âˆ’ Î± n < n < Ï„n). For an integer r âˆˆ
i i i
(nâˆ’Ï„nâˆ’Î± n,Ï„n)andn âˆ¼B(n,Î² ),wehavethefollowing:
i i i
Pr(n =r)=(ï¸n)ï¸ Î²r(1âˆ’Î² )nâˆ’r.
i r i i
ThenwecomputethepartialderivativeoftheprobabilitywithrespecttoÎ² asfollows:
i
âˆ‚Pr(n i =r) =(ï¸n)ï¸ Î²râˆ’1(1âˆ’Î² )nâˆ’râˆ’1(r(1âˆ’Î² )âˆ’(nâˆ’r)Î² )
âˆ‚Î² r i i i i
i
<(ï¸n)ï¸ Î²râˆ’1(1âˆ’Î² )nâˆ’râˆ’1(Ï„ âˆ’Î² )n.
r i i i
Thepartialderivativeissmallerthan0whenÏ„ <Î² . Therefore,theprobabilityPr(n =r)decreasesasÎ² increases
i i i
foranyintegerr âˆˆ(nâˆ’Ï„nâˆ’Î± n,Ï„n). Thus,thelowerboundofTDR increasesasÎ² becomescloserto1.
i i i
C ProofofTheorem2
ForC âˆ¼Q,wedenoten =BA(D(C),w )nandn =BA(D(C),w )nforj âˆˆ{1,2,Â·Â·Â· ,s}. Then,wehavethe
1 1 j j
following:
FDR=1âˆ’Pr( max n <Ï„n)
j
jâˆˆ{1,2,Â·Â·Â·,s}
=1âˆ’Pr(n <Ï„nâˆ§ max n <Ï„n).
1 j
jâˆˆ{2,3,Â·Â·Â·,s}
ToderiveanupperboundofFDR,wedenote:
|wâˆ’w | =nâˆ’n ,
1 1 1
|w âˆ’w | =nâˆ’BA(w ,w )n,
1 j 1 1 j
|wâˆ’w | =nâˆ’n .
j 1 j
Accordingtothetriangleinequality,wehavethefollowing:
|wâˆ’w | â‰¥|w âˆ’w | âˆ’|wâˆ’w |
j 1 1 j 1 1 1
=n âˆ’BA(w ,w )n.
1 1 j
Therefore,wederivetheupperboundofn forj âˆˆ{2,3,Â·Â·Â· ,s}asfollows:
j
n =nâˆ’|wâˆ’w |
j j 1
â‰¤nâˆ’n +BA(w ,w )n.
1 1 j
19Thus,wederivetheupperboundofFDRasfollows:
FDR=1âˆ’Pr(n <Ï„nâˆ§ max n <Ï„n)
1 j
jâˆˆ{2,3,Â·Â·Â·,s}
â‰¤1âˆ’Pr(n <Ï„nâˆ§ max nâˆ’n +BA(w ,w )n<Ï„n))
1 1 1 j
jâˆˆ{2,3,Â·Â·Â·,s}
=1âˆ’Pr(n <Ï„nâˆ§nâˆ’n +Î± n<Ï„n))
1 1 1
=1âˆ’Pr(nâˆ’Ï„n+Î± n<n <Ï„n)
1 1
=Pr(n â‰¥Ï„n)+Pr(n â‰¤nâˆ’Ï„n+Î± n),
1 1 1
wheren âˆ¼B(n,0.5)andÎ± =max BA(w ,w ).
1 1 jâˆˆ{2,3,Â·Â·Â·,s} 1 j
D ProofofTheorem3
ForC âˆ¼Q,wedenoten =BA(D(C),w )nforj âˆˆ{1,2,Â·Â·Â· ,s},andwehavethefollowing:
j j
FDR=1âˆ’Pr( max n <Ï„n)
j
jâˆˆ{1,2,Â·Â·Â·,s}
âˆï¸‚
=1âˆ’ Pr(n <Ï„n).
j
jâˆˆ{1,2,Â·Â·Â·,s}
AccordingtoDefinition4,foranyk âˆˆ{1,2,Â·Â·Â· ,n}andanyj âˆˆ{1,2,Â·Â·Â· ,s},thedecodingofeachbitisindependent
andtheprobabilitythatD(C)[k]matcheswithw [k]isatmost0.5+Î³ nomatterw [k]is1or0. Therefore,wehave
j j
thefollowing:
âˆï¸‚
FDR=1âˆ’ Pr(n <Ï„n)
j
jâˆˆ{1,2,Â·Â·Â·,s}
â‰¤1âˆ’Pr(nâ€² <Ï„n)s,
wherenâ€²followsthebinomialdistributionwithparametersnand0.5+Î³,i.e.,nâ€² âˆ¼B(n,0.5+Î³).
E ProofofCorollary2
AccordingtoTheorem3,theprobabilityPr(nâ€² <Ï„n)increaseswhenÎ³decreases. Therefore,theupperboundofFDR
decreasesasÎ³ becomescloserto0.
F ProofofTheorem4
ForC âˆ¼P ,wedenotew =D(C),n =BA(w,w )n,andn =BA(w,w )nforj âˆˆ{1,2,Â·Â·Â· ,s}. Thenwehave
i i i j j
thefollowing:
|wâˆ’Â¬w | =n ,
i 1 i
|Â¬w âˆ’w | =BA(w ,w )n,
i j 1 i j
|wâˆ’w | =nâˆ’n .
j 1 j
Accordingtothetriangleinequality,wehave:
|wâˆ’w | â‰¥|wâˆ’Â¬w | âˆ’|Â¬w âˆ’w |
j 1 i 1 i j 1
=n âˆ’BA(w ,w )n.
i i j
Therefore,wederivetheupperboundofn forj âˆˆ{1,2,Â·Â·Â· ,s}/{i}asfollows:
j
n =nâˆ’|wâˆ’w |
j j 1
â‰¤nâˆ’n +BA(w ,w )n.
i i j
20Table4: DefaultparametersettingsforthetrainingofAWT.
Phase StandardTraining Fine-Tuning
Optimizer Adam
#epochs 200 10
Batchsize 16
Learningrate 3Ã—10âˆ’5
#warm-upiterations 6000 1000
Lengthoftext 250 250Â±16
Generationweight 1.5 1
Messageweight 10000
Reconstructionweight 1.5 2
Thus,wederivethelowerboundofTAR asfollows:
i
TAR =Pr( max n â‰¥Ï„nâˆ§n > max n )
i j i j
jâˆˆ{1,2,Â·Â·Â·,s} jâˆˆ{1,2,Â·Â·Â·,s}/{i}
â‰¥Pr( max n â‰¥Ï„nâˆ§n > max nâˆ’n +BA(w ,w )n)
j i i i j
jâˆˆ{1,2,Â·Â·Â·,s} jâˆˆ{1,2,Â·Â·Â·,s}/{i}
n+Î± n
=Pr( max n â‰¥Ï„nâˆ§n > i )
jâˆˆ{1,2,Â·Â·Â·,s} j i 2
n+Î± n
=Pr( max n â‰¥Ï„nâˆ§n > i |n â‰¥Ï„n)Â·Pr(n â‰¥Ï„n)
jâˆˆ{1,2,Â·Â·Â·,s} j i 2 i i
n+Î± n
+Pr( max n â‰¥Ï„nâˆ§n > i |n <Ï„n)Â·Pr(n <Ï„n)
jâˆˆ{1,2,Â·Â·Â·,s} j i 2 i i
n+Î± n
â‰¥Pr(n > i |n â‰¥Ï„n)Â·Pr(n â‰¥Ï„n)
i 2 i i
n+Î± n
=Pr(n > i âˆ§n â‰¥Ï„n)
i 2 i
1+Î±
=Pr(n â‰¥max{âŒŠ inâŒ‹+1,Ï„n}),
i 2
wheren âˆ¼B(n,Î² )andÎ± =max BA(w ,w ).
i i i jâˆˆ{1,2,Â·Â·Â·,s}/{i} i j
G AdversarialTrainingofAWT
Inadversarialtraining,weemployT5-basedparaphrasertopost-processthewatermarkedtextsgeneratedbyAWT.
Duetothenon-differentiablenatureoftheparaphrasingprocess,wecannotjointlyadversariallytraintheencoderand
decodersincethegradientscannotback-propagatetotheencoder. Toaddressthechallenge,wefirstusethestandard
trainingtotrainAWTencoderanddecoder. Then,weusetheencodertogeneratewatermarkedtexts,paraphrasethem,
andusetheparaphrasedwatermarkedtextstofine-tunethedecoder. Thedetailparametersettingsoffine-tuningare
showninTable4.
21Algorithm1BSTA(w ,d,m)
s
Input: Initialwatermarkw ,recursiondepthd,andm.
s
Output: w orNotExist.
s
1: ifd<0then
2: returnNotExist
3: iâˆ— â†argmax iâˆˆ{1,2,Â·Â·Â·,sâˆ’1}BA(w i,w s)
4: ifBA(w iâˆ—,w s)>(m+d)/nthen
5: returnNotExist
6: elseifBA(w iâˆ—,w s)â‰¤m/nthen
7: returnw s
8: B â†{k|w s[k]=w iâˆ—[k],k =1,2,Â·Â·Â· ,n}
9: ChooseanyBâ€² âŠ‚Bwith|Bâ€²|=m+1
10: forallk âˆˆBâ€²do
11: w sâ€² â†w s
12: wâ€²[k]â†Â¬wâ€²[k]
s s
13: wâ€² â†BSTA(wâ€²,dâˆ’1,m)
s s
14: ifwâ€² isnotNotExistthen
s
15: returnwâ€²
s
16: returnNotExist
Algorithm2NRG(w ,m)
s
Input: Initialwatermarkw andm.
s
Output: w orNotExist.
s
1: Fâ†âˆ…
2: dâ†m
3: whiled>0do
4: iâˆ— â†argmax iâˆˆ{1,2,Â·Â·Â·,sâˆ’1}BA(w i,w s)
5: ifBA(w iâˆ—,w s)>2m/nthen
6: returnNotExist
7: elseifBA(w iâˆ—,w s)â‰¤m/nthen
8: returnw s
9: B â†{k|w s[k]=w iâˆ—[k]âˆ§k âˆˆ/ F,k =1,2,Â·Â·Â· ,n}
10: lâ†nÂ·BA(w iâˆ—,w s)âˆ’m
11: SampleBâ€² âŠ‚Bwith|Bâ€²|=luniformlyatrandom
12: forallk âˆˆBâ€²do
13: w s[k]â†Â¬w s[k]
14: dâ†dâˆ’l
15: F â†F âˆªBâ€²
16: returnNotExist
Algorithm3Solvingourwatermarkselectionproblem
Input: Existingsâˆ’1watermarksw ,w ,Â·Â·Â· ,w .
1 2 sâˆ’1
Output: Watermarkw .
s
1: mâ†max iâˆˆ{1,2,Â·Â·Â·,sâˆ’2}nÂ·BA(w i,w sâˆ’1)
2: whilew sisNotExistdo
3: ifBSTAthen
4: w s â†Â¬w 1
5: w s â†BSTA(w s,m,m)
6: ifNRGthen
7: w s â†Â¬w 1
8: w s â†NRG(w s,m)
9: ifA-BSTAthen
10: w s â†sampleduniformlyatrandom
11: w s â†BSTA(w s,d,m)
12: ifw sisNotExistthen
13: mâ†m+1
14: returnw s
22