Lifelong Intelligence Beyond the Edge using Hyperdimensional
Computing
XiaofanYu AnthonyThomas IvanniaGomezMoreno
x1yu@ucsd.edu ahthomas@ucsd.edu ivannia.gomez@cetys.edu.mx
UniversityofCaliforniaSanDiego UniversityofCaliforniaSanDiego CETYSUniversity,CampusTijuana
LaJolla,California,USA LaJolla,California,USA Tijuana,Mexico
LouisGutierrez TajanaÅ imuniÄ‡Rosing
l8gutierrez@ucsd.edu tajana@ucsd.edu
UniversityofCaliforniaSanDiego UniversityofCaliforniaSanDiego
LaJolla,California,USA LaJolla,USA
ABSTRACT andmemoryresourcesforon-devicetraining[15,34].Nevertheless,
On-devicelearninghasemergedasaprevailingtrendthatavoids theseeffortsoftenrelyonstaticmodelsforinferenceorlackthe
theslowresponsetimeandcostlycommunicationofcloud-based adaptabilitytoaccommodatenewenvironments.
learning.Theabilitytolearncontinuouslyandindefinitelyina Tofundamentallyaddresstheseissues,sensordevicesshouldbe
changingenvironment,andwithresourceconstraints,iscritical capableof"lifelonglearning"[42]:tolearnandadaptwithlimited
forrealsensordeployments.However,existingdesignsareinade- supervisionafterdeployment.On-devicelifelonglearningreduces
quateforpracticalscenarioswith(i)streamingdatainput,(ii)lack theneedforexpensivedatacollection(includinglabels)andof-
ofsupervisionand(iii)limitedon-boardresources.Inthispaper, flinemodeltraining,operatinginadeploy-and-runmanner.This
wedesignanddeploythefirston-devicelifelonglearningsystem approachenablesautonomouslearningsolelyfromtheincoming
calledLifeHDforgeneralIoTapplicationswithlimitedsupervi- sampleswithminimalsupervision,andisthusabletoprovidereal-
sion.LifeHDisdesignedbasedonanovelneurally-inspiredand time decision-making even without a network connection. The
lightweightlearningparadigmcalledHyperdimensionalComput- lifelongaspectisessentialforhandlingdynamicreal-worldenvi-
ing(HDC).Weutilizeatwo-tierassociativememoryorganization ronments,representingthefutureofIoT.
tointelligentlystoreandmanagehigh-dimensional,low-precision Althoughextensiveresearchhasinvestigatedlifelonglearning
vectors,whichrepresentthehistoricalpatternsasclustercentroids. acrossvariousscenarios[42],existingtechniquesfacechallenges
WeadditionallyproposetwovariantsofLifeHDtocopewithscarce that render them unsuitable for real-world deployments. These
labeledinputsandpowerconstraints.WeimplementLifeHDonoff- challengesinclude:
the-shelfedgeplatformsandperformextensiveevaluationsacross (C1) Streamingdatainput.Edgedevicescollectstreamingdata
threescenarios.OurmeasurementsshowthatLifeHDimprovesthe fromadynamicenvironment.Thisonlinelearningwithnon-
unsupervisedclusteringaccuracybyupto74.8%comparedtothe iid data contrasts with the default offline and iid setting
state-of-the-artNN-basedunsupervisedlifelonglearningbaselines wheremultiplepassesontheentiredatasetareallowed[16].
withasmuchas34.3xbetterenergyefficiency.Ourcodeisavailable (C2) Lackofsupervision.Obtainingground-truthlabelsand
athttps://github.com/Orienfish/LifeHD. expertguidanceisoftenchallengingandexpensive.Most
lifelonglearningmethodsrelyonsomeformofsupervision,
KEYWORDS suchasclasslabels[28]orclassshiftboundaries[46],which
aretypicallyunavailableinreal-worldscenarios.
EdgeComputing,LifelongLearning,HyperdimensionalComputing
(C3) Limiteddeviceresources.Neuralnetworks(NN)areknown
fortheirhighresourcedemands[60].Furthermore,themain
1 INTRODUCTION techniquesforlifelonglearningbasedonNN,suchasreg-
ThefusionofartificialintelligenceandInternetofThings(IoT)has ularization[28]andmemoryreplay[35],addextracompu-
becomeaprominenttrendwithnumerousreal-worldapplications, tationalandmemoryrequirementsbeyondstandardNNs,
suchasinsmartcities[10],smartvoiceassistants[55],andsmart makingtheminadequateforedgedevices.
activityrecognition[62].However,thepredominantcurrentap- Real-WorldExample.Toillustratethechallengesfaced,wepresent
proachiscloud-centric,wheresensordevicessenddatatothecloud areal-worldscenarioinFig.1.Consideracameradeployedinthe
forofflinetrainingusingextensivedatasources.Thisapproach wildcontinuouslycollectingdatafromsurroundingenvironment.
faceschallengeslikeslowupdatesandcostlycommunication,in- Ourgoalistotrainanunsupervisedobjectrecognitionalgorithm
volving the exchange of large sensor data and models between ontheedgedevice,purelyfromthedatastream.Weconstructboth
theedgeandthecloud[53].Instead,recentresearchhasshifted iidandsequential(oneclassappearsaftertheother)streamsfrom
towardsedgelearning,wheremachinelearningisperformedon CIFAR-100[6],andadoptthesmallestMobileNetV3model[20]
resource-constrainededgedevicesrightnexttothesensors.While withthepopularBYOLunsupervisedlearningpipeline[16].As
most studies focused on inference-only tasks [32, 33, 50], some seeninFig.1,whilethemodelshowsimprovedaccuracywithiid
recentworkhasinvestigatedtheoptimizationofcomputational streams,ithasasignificantperformancelossundersequentially
4202
raM
7
]GL.sc[
1v95740.3042:viXraUnderreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaÅ imuniÄ‡Rosing
OurbasicapproachinLifeHDisfullyunsupervised.However,
inreality,labelsmaybeavailable(orcouldbeacquired)forasmall
numberofexamples.WeintroduceLifeHDsemitoexploitalimited
numberoflabeledsamplesasanextensiontothepurelyunsuper-
visedLifeHD.Additionally,weproposeLifeHDa,whichusesan
adaptiveschemeinspiredbymodelpruning,toadjusttheHDem-
beddingdimensionon-the-fly.LifeHDaallowsustofurtherreduce
resourceusage(powerin-particular),wherenecessary.
Figure1:Real-worldexampleofon-devicelifelonglearningeval-
Insummary,thecontributionsofthispaperare:
uatedusingtheunsupervisedclusteringaccuracymetric[63].The
traininglatencyismeasuredontwotypicaledgeplatforms. (1) WedesignLifeHD,thefirstend-to-endsystemforon-device
unsupervisedlifelongintelligenceusingHDC.LifeHDbuilds
uponHDCâ€™slightweightsingle-passtrainingcapabilityand
ordereddata,highlightingtheNNeffectofâ€œforgettingâ€inastream- incorporatesournovelclustering-basedmemorydesignto
ingandunsupervisedsetting.Intermsofefficiency,wemeasure addresschallenges(C1)-(C3).
thetraininglatencyofMobileNetV3(small)[20]ontwotypical (2) WefurtherproposeLifeHDsemiasanextensiontofullyuti-
edgeplatforms,RaspberryPi(RPi)4B[2]andJetsonTX2[1]by lizethescarcelabeledsamplesalongwiththestream.We
running10gradientdescentstepsonasinglebatchof32samples. deviseLifeHDathatenablesadaptivepruninginLifeHDto
Evenontheseverycapableedgeplatforms,trainingtakesupto reducereal-timepowerconsumption.
17.4seconds,clearlyunsuitableforreal-timeprocessingunder30 (3) WeimplementLifeHDonoff-the-shelfedgedevicesandcon-
FPS.Therefore,anovelapproachcapableofhandlingnon-iiddata ductextensiveexperimentsacrossthreetypicalIoTscenarios.
andofferingmoreefficientupdatesisnecessarytoaccommodate LifeHDimprovestheunsupervisedclusteringaccuracyupto
thecontinualchangesindata. 74.8%with34.3xbetterenergyefficiencycomparedtolead-
Toaddresschallenges(C1)-(C3),wedrawinspirationfrombiol- ingunsupervisedNNlifelonglearningmethods[13,14,54].
ogy,whereeventinyinsectsdisplayremarkablelifelonglearning (4) LifeHDsemiimprovestheunsupervisedclusteringaccuracy
abilities,anddosousingâ€œhardwareâ€thatrequiresverylittleen- byupto10.25%overtheSemiHD[22]baselineunderlimited
ergy[4].Hyperdimensionalcomputing(HDC)isanemergingpar- labelavailability.LifeHDa limitstheaccuracylosswithin
adigminspiredbytheinformationprocessingmechanismsfound 0.71%usingonly20%ofLifeHDâ€™sfullHDdimension.
in biological brains [24]. In HDC, all data is represented using The rest of the paper is organized as follows. We start by a
high-dimensional,low-precision(oftenbinary)vectorsknownas comprehensivereviewofrelatedworksinSec.2.Wethenintroduce
â€œhypervectors,â€whichcanbemanipulatedthroughsimpleelement- salientbackgroundonHDCinSec.3tohelpunderstanding.We
wiseoperationstoperformtaskslikememorizationandlearning. formally define the unsupervised lifelong learning problem we
HDCiswell-understoodfromatheoreticalstandpoint[56]and targettosolveinSec4.Afterwards,Sec.5describesthedetailsof
sharesintriguingconnectionswithbiologicallifelonglearning[52]. ourmajordesignLifeHD.Sec.6introducesLifeHDsemiandLifeHDa.
Furthermore,itsuseofbasicelement-wiseoperatorsalignswith Sec.7presentstheimplementationandresultsofLifeHD,whilethe
highlyparallelandenergy-efficienthardware,offeringsubstantial evaluationsofLifeHDsemiandLifeHDaarereportedinSec8.We
energysavingsinIoTapplications[11,23,27,65].WhileHDCisre- addthediscussionsandfutureworksinSec.9.Theentirepaperis
portedasapromisingavenue,theliteraturetodatehasnotexplored concludedinSec.10.
weakly-supervisedlifelonglearningusingHDC.
2 RELATEDWORK
Inthiswork,wedesignanddeployLifeHD,thefirstsystemfor
on-devicelightweightlifelonglearninginanunsupervisedanddy- LifelongandOn-DeviceLearning.Lifelonglearning(orcontin-
namicenvironment.LifeHDleveragesHDCâ€™sefficientcomputation uallearning)isalargeandactiveareaofresearchinthebroader
andadvantagesinlifelonglearning,whileeffectivelyhandlingunla- machinelearningcommunity.Catastrophicforgettingisamajor
beledstreaminginputs.Thesecapabilitiesextendbeyondthescope challengeinlifelonglearning,andreferstoacommonlyobserved
ofexistingHDCdesigns,whichhavefocusedoverwhelminglyon empiricalphenomenoninwhichupdatingcertainmachinelearning
thesupervisedsetting[23,27].Specifically,LifeHDrepresentsthe modelswithnewdataseverelydegradestheirabilitytoperform
input ashigh-dimensional, low-precisionvectors, and, drawing previouslylearnedtasks[36].Previousworksproposedtechniques
inspirationfromworkincognitivescience[5],organizesdatainto suchasdynamicarchitecture[31,49],regularizationbypenalizing
atwo-tiermemoryhierarchy:ashort-termâ€œworkingmemoryâ€and importantweights[28,66],knowledgedistillationfrompastmod-
along-termmemory.Theworkingmemoryprocessesincoming els[14]andexperiencereplayusingamemorybuffer[35,58].The
dataandsummarizesitintoagroupoffine-grainedclustersthatare lifelonglearningliteraturehasexaminedawiderangeofproblem
representedbyhypervectorscalledclusterHVs.Long-termmemory settings,rangingfromthefullysupervisedcase,inwhichtasks
consolidatesthefrequentlyappearedclusterHVsintheworking andclasslabelsareprovided,andthefullyunsupervisedcasewith-
memory,andwillberetrievedformergingandinferenceoccasion- outanylabelsandpriorknowledge[13,57].However,allofthese
ally. We emphasize that LifeHD is designed to suit a variety of worksarebasedondeepNNsandrequirebackpropagation,which
edgedeviceswithdiverseresourcelevels.Moreefficiencygains isproblematicforresource-constraineddevices.
canbeachievedbyemployingoptimizationssuchaspruningand Neurally-inspired lightweight algorithms have recently been
quantization[15,61],butthisisnotthefocusofourwork. proposed for lifelong learning applications. FlyModel [52] andLifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
SDMLP [8] use sparse coding and associative memory for life-
longlearning.However,bothapproachesassumefullsupervision.
STAM[54]isanexpandablememoryarchitectureforunsupervised
lifelonglearning,usinglayeredreceptivefieldsandatwo-tiermem-
oryhierarchy.Itlearnsviaonlinecentroid-basedclusteringpipeline,
noveltydetectionandmemoryupdates.Nevertheless,thememory
inSTAMissolelydedicatedtoimagestorage,whileourLifeHD
additionallyemphasizesmergingpastpatternsintocoarsegroups Figure2:SpatiotemporalHDCencodingfortime-seriesdata.Left:
andshowsmoreeffectivelearningperformance. randomgenerationoflevelhypervectors.Right:thecompleteen-
Recentworksoptimizetheresourceusageofon-devicetraining codingprocess.
viapruningandquantization[34,45],tuningpartialweights[9,47],
memoryprofilingandoptimization[15,61,64],aswellasgrowing andisintuitivelyusedtobuildsets.Thebundlingoperation
theNNonthefly[67].Alltheseworksoptimizetraininggiven isimplementedthroughaddition.
resourceconstraintsanddonotfocusonlifelonglearning.They (3) Permute:ğœŒ :H â†’H.Permutationcanbeusedtoencode
areorthogonaltothecontributionofLifeHDwhichfocuseson sequentialinformationandistypicallyimplementedusinga
adaptiveandcontinualtraining.LifeHDcanbefurtheroptimized cyclicshift.
bycombiningwithsuchtechniques. Theencodingfunctionğœ™ :Xâ†’H embedsdatafromitsambient
HyperdimensionalComputing.HDChasgarneredsubstantial representationintoHD-space.Ingeneral,encodingshouldpreserve
interestfromthecomputerhardwarecommunityasanenergy- somemeaningfulnotionofsimilaritybetweeninputpointsinthe
efficientandlow-latencyapproachtolearning,andhasbeensuc- sensethatğœ™(ğ‘¥)Â·ğœ™(ğ‘¥â€²)â‰ˆğ‘˜(ğ‘¥,ğ‘¥â€²),whereğ‘˜issomesimilarityfunc-
cessfullyappliedtoproblemssuchashumanactivityrecognition[27], tionofinterestonX.Inthispaper,weusespatiotemporalencoding
voicerecognition[23],imagerecognition[11,65],tonameafew. fortimeseriessensordata,andHDnnformorecomplexdata,such
ThelargemajorityofliteratureonHDChasfocusedonusingthe asimages,whichweexplaininthefollowing.
techniquetoperformsupervisedclassificationtasks.Amongthelim- SpatiotemporalEncoding.Thespatiotemporalmethod[38]
itedliteratureforweakly-supervisedlearningwithHDC,HDClus- jointlyencodestheanaloginformationfromeachsensor(spatial)
ter[21]enabledunsupervisedclusteringinHDCwithanewalgo- andateachtimestamp(temporal)toasinglehypervector.Sup-
rithmthatissimilartoK-Means.SemiHD[22]isasemi-supervised posethereareğ‘‘-differentsensorsğ‘  1,...,ğ‘  ğ‘‘,eachofwhichproducea
learningframeworkusingHDCwithiterativeself-labeling.Hyper- real-valuedreadingğ‘¥ 1,...,ğ‘¥ ğ‘‘,whereuponwemaymodeltheinput
seed[41],C-FSCIL[18]andFSL-HD[65]adoptedHDCorsimilar ataparticularmomentintimebyasetoftuples{(ğ‘  ğ‘–,ğ‘¥ ğ‘–)}ğ‘‘ ğ‘–=1.We
vectorsymbolicarchitectures(VSA)forunsupervisedorfew-shot pre-generateasetofbasehypervectorstorepresentthevaluesand
learning.Allaboveworksdidnotconsiderthelifelongaspectand sensorsrespectively.Torepresentarealvaluedfeatureğ‘¥ âˆˆR,we
usedofflinetrainingonastaticdataset.Tothebestoftheauthorsâ€™ quantizethesupportofğ‘¥intoasetofbinswithcentroidsğ‘ 1,...,ğ‘ ğ‘„,
knowledge,LifeHDisthefirstworkthatdesignsanddeployslife- andassigneachbinanembeddingğœ‘(ğ‘ ğ‘–),whichwecalllevelhy-
longlearninginedgeIoTapplicationsespeciallywithzeroormini- pervectors,suchthatğœ‘(ğ‘ ğ‘–)Â·ğœ‘(ğ‘ ğ‘—)ismonotonicallydecreasingin
malamountoflabels. |ğ‘ ğ‘– âˆ’ğ‘ ğ‘—|.AsshowninFig.2(left),weinitiallygeneratearandom
hypervectorforthefirstlevel.Tomaintainsimilaritybetweenad-
3 BACKGROUNDONHDC jacentlevelhypervectors,foreachsubsequentlevel,werandomly
HyperdimensionalComputing(HDC)isanemergingparadigmfor flipafractionofbitsfromthepreviouslevelasdescribedin[56].
informationprocessingfromthecognitive-neuroscienceliterature
Thefractionofflippingisdenotedasğ‘ƒ.Thisprocessisrepeated
[24].InHDC,allcomputationisperformedonlow-precisionand
untilallğ‘„levelhypervectorsaregenerated.Torepresentdifferent
distributedrepresentationsofdatathataccordnaturallywithhighly sensorğ‘ ,weassigneachsensorarandomembeddingğœ“(ğ‘  ğ‘–),which
parallelandlow-energyhardware.
wecallIDhypervectors,bysamplingğœ“(ğ‘  ğ‘–)âˆ¼Unif({Â±1}ğ·).
ThefirststepinHDCisencoding,whichmapsaninputğ‘¥ âˆˆX ThecompletespatiotemporalencodingisvisualizedinFig.2
toadistributedrepresentationğœ™(ğ‘¥)livinginsomeğ·-dimensional (right).Weencodeapair(ğ‘ ,ğ‘¥)viağœ“(ğ‘ )âŠ—ğœ‘(ğ‘(ğ‘¥)),whereğ‘(ğ‘¥)is
inner-productspaceH,thatwecalltheâ€œHD-space.â€Forinstance,
thecentroidofthebinclosesttoğ‘¥.Thispreservesboththeleveland
onemighttakeH âŠ‚{Â±1}ğ· ,orH âŠ‚Rğ· .Werefertopointsinthe sensorIDinformation.Toencodethereadingsforallsensorswe
HD-spaceashypervectors.Encodingsofdatacanbemanipulated bundletogethertheirindividualembeddingsandroundtobipolar
soastobuildmorecomplexcompositerepresentationsusingaset
(e.g.{Â±1})precision:ğœ™(ğ‘¥)=Sign(cid:16)(cid:201)ğ‘‘
ğ‘–=1ğœ“(ğ‘  ğ‘–)âŠ—ğœ‘(ğ‘(ğ‘¥
ğ‘–))(cid:17)
.Finally,
ofoperatorsdefinedasfollows: to represent a sequence ofğ‘‡ readings:ğ‘‹ = {ğ‘¥ 1,...,ğ‘¥ ğ‘‡}, we use
(1) Bind:âŠ— : H Ã—H â†’ H.Bindingtakestwohypervectors permutation:ğœ™(ğ‘‹)=(cid:203)ğ‘‡ ğœ=1ğœŒğœ(ğœ™(ğ‘¥ ğœ)).
as inputs and returns a hypervector that is dissimilar to HDnnEncoding.Inthisworkweusetherecentlyproposed
bothinputs,andisintuitivelyusedtorepresenttuples.For HDnnstyleencoding[11,65]thatcombinesapretrainedandfrozen
bipolarhypervectors(i.e.,H âŠ‚{Â±1}ğ‘‘ ),thebindingoperator NNfeatureextractorwithHDCâ€™sspatiotemporalencodingtoobtain
istypicallyelement-wisemultiplication. stateoftheartaccuracyforsoundandimages.InHDnntheinputs
(2) Bundle:âŠ•:HÃ—H â†’H.Bundlingtakestwohypervectors tothespatio-temporalencoding,ğ‘  1,...,ğ‘  ğ‘‘,areintermediatefeature
asinputandreturnsahypervectorsimilartobothoperands, outputs of the pretrained and frozen NN (Fig. 3). For example,Underreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaÅ imuniÄ‡Rosing
thatpreservestheoverall(im)balancebetweentheclasses.Note,
thatevenwhenoneclasshasnotappearedinthetrainingdata
stream,itisalwaysincludedinE.HenceEisaglobalviewofall
classesthatcanpotentiallyexistintheenvironment.
UnsupervisedClusteringAccuracy.Sincewedonotgiveclass
labelsorthetotalnumberofclassesduringtraining,thepredicted
labelcanbedifferentfromtheground-truthlabel.Therefore,for
Figure3:AnoverviewofsupervisedHDCpipelineincludingencod- evaluationmetric,wecannotadoptthesimplepredictionaccuracy
ing,trainingandinference(similaritycheckforclassification).An thatrequiresexactlabelmatching.Instead,weemployawidely
additionalpretrainedNNisusedasafeatureextractorintheHDnn. usedclusteringmetricknownasunsupervisedclusteringaccuracy
(ACC)[63],whichmirrorstheconventionalaccuracyevaluation
asectionofMobileNetpretrainedonImageNetcreatesfeatures
butwithinanunsupervisedcontext.
whicharethenencodedintoHDhypervectorsforobjectrecognition
Supposeğœ” ğ‘˜ isthepredictedclusteroftestingsample(ğ‘‹ ğ‘˜,ğ‘¦ ğ‘˜)
tasks.Thisonlymarginallyincreasesthecomputationalcostsas
notrainingisperformedonNN,allthetraininghappensinHD. inE.ACCiscomputedas:ğ´ğ¶ğ¶ =maxğ‘š |E1 | (cid:205) ğ‘˜|E =1| 1{ğ‘¦ ğ‘˜ =ğ‘š(ğœ” ğ‘˜)},
SupervisedTrainingandInference.Acommonusecaseof
whereğ‘šrangesoverallpossibleone-to-onemappingsbetween
HDC,summarizedinFig.3,istofitclassifiers.Inparticular,letus predictedclustersandground-truthclasses.Intuitively,thismetric
supposethatweseeasetofğ‘ labeledsamples{(ğ‘‹ ğ‘–,ğ‘¦ ğ‘–)} ğ‘–ğ‘ =1,where computestheaccuracyundertheâ€œbestâ€mappingbetweenclusters
ğ‘¥ ğ‘– isaninput,andğ‘¦ ğ‘– âˆˆ{ğ‘ 1,...,ğ‘ ğ½}isaclasslabel.Inthetraditional andlabels.ThebiggestadvantageofACCisthatitdoesnotrequire
approachtoclassification,onesimplyrepresentseachclassviathe thenumberofclustersandclassestobeequal.Forinstance,acluster
bundleofitstrainingdata.Thatis:ğœ™(ğ‘ ğ‘—) = (cid:201) ğ‘–:ğ‘¦ğ‘–=ğ‘ğ‘—ğœ™(ğ‘‹ ğ‘–).We o laf bp ei ln oe fs ta rn ed esa .Wclu est te rer ao tf sr ue cd hw co lo ud stb eo rit nh gbe relo sn ug ltt ao st ahe vag lr io du ln ed art nru inth
g
storethetrainedclasshypervectorsinanassociativememory.For
outcome,withaconcretevisualizationshowninSec.7.4.
example,inFig.3,wecomputeandstoretheclasshypervectors
of cats and dogs. During inference, we first encode the testing
sampleğ‘‹ ğ‘intoaqueryhypervectorğœ™(ğ‘‹ ğ‘)usingthesameencoding 5 LIFEHD
procedureasfortraining.Wethenpredictthelabelcorrespondingto
Inthissection,wepresentthedesignofLifeHD,thefirstunsuper-
themostsimilarclassasmeasuredbythecosinesimilarity,i.e.,ğ‘¦Ë†=
visedHDCframeworkforlifelonglearningingeneraledgeIoT
argmaxğ‘—cos(ğœ™(ğ‘‹ ğ‘),ğœ™(ğ‘ ğ‘—))âˆargmaxğ‘—(ğœ™(ğ‘‹ ğ‘)Â·ğœ™(ğ‘ ğ‘—))/âˆ¥ğœ™(ğ‘ ğ‘—)âˆ¥.
applications. Compared to operating in the original data space,
HDCimprovespatternseparabilitythroughsparsityandhighdi-
4 PROBLEMDEFINITION
mensionality,makingitmoreresilientagainstcatastrophicforget-
Beforedivingintoourmethod,wefirstrigorouslyformulatethe ting[52].LifeHDpreservestheadvantagesofHDCincomputa-
unsupervisedlifelonglearningproblemusingstreamingsources, tionalefficiencyandlifelonglearning,whilehandlingtheinputof
drivenbyreal-worldIoTapplications. unlabeledstreamingdata,whichhasnotbeenachievedinprevious
StreamingData.Torepresentcontinuouslychangingenviron- work[18,21,22,41,65].
ment,weassumeawell-knownclass-incrementalmodelinlifelong
learning,inwhichnewclassesemergeinasequentialmanner[46].
5.1 LifeHDOverview
Wealsoallowdatadistributionshiftwithinoneclass.Thissetting
modelsascenarioinwhichadeviceiscontinuouslysamplingdata Fig.4givesanoverviewofhowLifeHDworks.ThefirststepisHDC
whilethesurroundingenvironmentmaychangeimplicitlyover encodingofdataintohypervectorsasdescribedinSec.3.Training
time,e.g.,theself-drivingvehicleasshowninFig.1.Werequire samplesğ‘‹ areorganizedintobatchesofsizeğ‘ğ‘†ğ‘–ğ‘§ğ‘’andinputinto
thatallsamplesappearonlyonce(i.e.,single-passstreams). anoptionalfixedNNforfeatureextraction(e.g.forimagesand
Formally,weconsiderascenarioinvolvingğ‘‘sensors,eachpro- sound)andtheencodingmodule.Theencodedhypervectorsğœ™(ğ‘‹)
ducingarealvaluedreading.Wegroupreadingsintoslidingwin- areinputtoLifeHDâ€™stwo-tiermemorydesigninspiredbycognitive
dowsoflengthğ‘‡,andtreatonesuchbatchğ‘‹ ğ‘– âˆˆRğ‘‡Ã—ğ‘‘ asaninput sciencestudies[5],consistingofworkingmemoryandlong-term
sample.Eachinputğ‘‹ ğ‘– isassociatedwithanunknownlabelğ‘¦ ğ‘–.Im- memory.Thismemorysystemintelligentlyanddynamicallyman-
portantly,thelabelsarenotmadeavailableduringtraining,nor ageshistoricalpatterns,storedashypervectorsandreferredtoas
theboundariesofclassshift.Thereforetheentireprocessisun- clusterHVs.AsshowninFig.4,theworkingmemoryisdesigned
supervised. We represent the data stream associated with each withthreecomponents:noveltydetection,clusterHVupdateand
classbyDğ‘— ={ğ‘‹ 1,ğ‘‹ 2,...},andthesetofstreamsforallclassesby clusterHVmerge.ğœ™(ğ‘‹)isfirstinputintonoveltydetectionstep(â—‹1).
D ={D 1,...,Dğ½}.Notethattheclass-incrementalstreamscanhave AninsertiontotheclusterHVsismadeifanoveltyflagisraised,
imbalancedclasses,i.e.,|ğ· ğ‘–|â‰ |ğ· ğ‘—|,ğ‘– â‰  ğ‘—,andgradualdistribution otherwiseğœ™(ğ‘‹) updatestheexistingclusterHVs(â—‹2).Thethird
shiftwithineachclass. component,clusterHVmerge(â—‹3),retrievestheclusterHVsfrom
LearningProtocol.Ourgoalistobuildaclassificationalgo- long-termmemory,andmergessimilarclusterHVsintoasuper-
rithmthatmapsXâ†’Y.Forevaluation,weusethecommonevalu- clusterviaanovelspectralclustering-basedmergingalgorithm[59].
ationprotocolinstate-of-the-artlifelonglearningworks[13,14,54], Theinteractionbetweenworkingandlong-termmemoryhappens
inwhichweconstructaniiddatasetE = {(ğ‘‹ ğ‘˜,ğ‘¦ ğ‘˜)}forperiodic as commonly encountered cluster HVs are copied to long-term
testing,bysamplinglabeledexamplesfromeachclassinamanner memory,whichwecallconsolidation(â—‹4).Finally,whenthesizeLifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
Figure4:Theend-to-endalgorithmflowofLifeHD.
Table1:Listofimportantnotations. HVğ‘–:ğœ‡
ğ‘–
andğœË†ğ‘–,whichrepresentthemeancosinedifferenceand
standarddifferencebetweentheclusterHVanditsassignedinputs.
Symbol Meaning Givenğœ™(ğ‘‹),wefirstidentifythemostsimilarclusterHV,denoted
ğ‘‘ Numberofsensorsources byğ‘—.LifeHDmarksğœ™(ğ‘‹)asâ€œnovel"ifitsubstantiallydiffersfrom
ğ‘‡ Timewindowlengthofoneinputsampleğ‘‹
itsnearestclusterHV.Specifically,thisdissimilarityismeasuredby
ğ· DimensionoftheHD-space comparingcos(ğœ™(ğ‘‹),ğ‘š ğ‘—)withathresholdbasedonthehistorical
ğ‘„ Numberofquantizedlevelforencoding distancedistributionofclusterHVğ‘—:
ğ‘ƒ Fractionofrandombitfliptogeneratelevelhypervector
ğœ™ HDCencodingfunction
ğœ‘,ğœ“ LevelandIDhypervectorencodingfunction If cos(ğœ™(ğ‘‹),ğ‘š ğ‘—) <ğœ‡ ğ‘— âˆ’ğ›¾ğœË†ğ‘—,thenflagnovel. (1)
ğ‘ğ‘†ğ‘–ğ‘§ğ‘’ Batchsizeofinputsamples
M,L SetofclusterHVsstoredintheworkingandlong-termmemory Thehyperparameterğ›¾ fine-tunesthesensitivitytonovelties.
ğ‘€,ğ¿ MaximumnumberofclusterHVsintheworkingandlong-term LifeHDrecognizesnewğœ™(ğ‘‹)asprototypesandinsertstheminto
memory theworkingmemory.Whenreachingitssizelimitğ‘€,theworking
ğœ‡,ğœË† Meansimilarityandstandarddifferenceofbetweeneachcluster
memoryexperiencesforgetting(â—‹5 inFig.4).Theleastrecentlyused
HVanditsassignedinputsintheworkingmemory
â„ğ‘–ğ‘¡ ThenumberoftimesthateachclusterHVishitintheworking (LRU)clusterHV,representedbyğ¿ğ‘…ğ‘ˆ =argminğ‘–ğ‘€ =1ğ‘ ğ‘–,isreplaced.
memory
Hereğ‘correspondstothelatestbatchindexwheretheclusterHV
â„ğ‘–ğ‘¡ ğ‘¡â„ ThehitfrequencythresholdtoconsolidateclusterHVfrom wasaccessed.Asimilarforgettingmechanismisconfiguredforthe
workingtolong-termmemory long-termmemory,wherethelastbatchaccessedismarkedwithğ‘.
ğ‘,ğ‘ ThemostrecentbatchindexwheneachclusterHVisaccessed,
fortheworkingandlong-termmemoryclusterHVs
5.3 ClusterHVUpdate
ğ›¾ Hyperparameterfornoveltydetectionsensitivity
ğ›¼ MovingaverageupdaterateduringclusterHVupdate Ifnoveltyisnotdetected,indicatingthatğœ™(ğ‘‹)closelymatchesclus-
ğ‘” ğ‘¢ğ‘ ClusterHVmergesensitivity terHVğ‘—,weproceedtoupdatetheclusterHVanditsassociated
ğ‘“ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ ClusterHVmergefrequency information(â—‹2 inFig.4).Thisupdateprocessinvolvesbundling
ğ‘Ÿ AveragelabelingratioinLifeHDsemi ğœ™(ğ‘‹)withclusterHVğ‘š ğ‘—,akintohowclasshypervectorsareup-
ğ·ğ‘ DimensionofthemaskusedinLifeHDa dated as described in Sec. 3, and updaing ğœ‡ ğ‘— and ğœË†ğ‘— with their
movingaverage:
limitofeitherworkingorlong-termmemoryisreached,theleast
recentlyusedclusterHVsareforgotten(â—‹5). ğ‘š ğ‘— â†ğ‘š ğ‘— âŠ•ğœ™(ğ‘‹) (2a)
AllmodulesinLifeHDworkcollaboratively,makingitadaptive
ğœ‡ ğ‘— â†(1âˆ’ğ›¼)ğœ‡ ğ‘— +ğ›¼cos(ğœ™(ğ‘‹),ğ‘š ğ‘—) (2b)
androbusttocontinuouslychangingstreamswithoutrelyingon
anyformofpriorknowledge.Forexample,inscenariosofdistribu- ğœË†ğ‘— â†(1âˆ’ğ›¼)ğœË†ğ‘— +ğ›¼|cos(ğœ™(ğ‘‹),ğ‘š ğ‘—)âˆ’ğœ‡ ğ‘—| (2c)
tiondrift,LifeHDmaygeneratenewclusterHVsuponencountering â„ğ‘–ğ‘¡ ğ‘— â†â„ğ‘–ğ‘¡ ğ‘— +1,ğ‘ ğ‘— â†ğ‘–ğ‘‘ğ‘¥ (2d)
driftedsamplesinitially,whichcanlaterbemergedintocoarseclus-
ters.ThisapproachensuresthatLifeHDcanefficientlycaptureand Thehyperparameterğ›¼ adjuststhebalancebetweenhistoricaland
retainhistoricalpatterns. recentinputs,whereahigherğ›¼givesmoreweighttorecentsamples.
Inthefollowing,wediscussmoredetailsaboutthemajorcom- Properlymaintainingğœ‡ ğ‘— andğœË†ğ‘— isvitalfortrackingtheâ€œradiusâ€of
ponentsofLifeHD:noveltydetection(Sec.5.2),clusterHVupdate
eachclusterHV,affectingfuturenoveltydetection.Wealsoincrease
(Sec.5.3),andclusterHVmerging(Sec.5.4).Wesummarizethe thehitfrequencyâ„ğ‘–ğ‘¡
ğ‘—
andrefreshğ‘
ğ‘—
withcurrentbatchindexğ‘–ğ‘‘ğ‘¥.
importantnotationsusedinthispaperinTable1. â„ğ‘–ğ‘¡ ğ‘— isfurtherusedtocomparedwithapredeterminedthreshold
â„ğ‘–ğ‘¡ ğ‘¡â„ todecidewhenaworkingmemoryclusterHVappearssuffi-
5.2 NoveltyDetection
cientlyfrequentlytobeconsolidatedtolong-termmemory(â—‹4 in
Theinitialnoveltydetectionstep(â—‹1 inFig.4)iscrucialforidentify- Fig.4).ğ‘ ğ‘—determinesforgettingasdescribedintheprevioussection.
ingemergingpatternsintheenvironment.SupposeM ={ğ‘š 1,...,ğ‘š ğ‘€} Withthislightweightapproach,LifeHDcontinuallyrecordstempo-
isthesetofclusterHVsstoredintheworkingmemory.Wegauge ralclusterHVsfromtheenvironment,whilethemostprominent
the"radius"ofeachclusterbytrackingtwoscalarsforeachcluster clusterHVsaretransferredtolong-termmemory.Underreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaÅ imuniÄ‡Rosing
computationallatency.Bothğ‘” ğ‘¢ğ‘andğ‘“ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’areanalyzedinSec.7.8,
alongwithotherkeyhyperparametersinLifeHD.
TimeComplexityofMerging.Apotentiallylimitingissue
withspectralclusteringisitstimecomplexity,whichis,intheworst
caseğ‘‚(ğ¿3).However,thisisnotaconcerninoursetting.First,the
numberofclusterHVsinlong-termmemory(ğ¿)istypicallysmall,
Figure5:AnintuitivevisualizationofclusterHVmerging. around50inpractice,resultinginmodestworst-casecomplexity.
Secondly,worst-caseanalysisisoverlypessimistic,assumingafull
5.4 ClusterHVMerging eigendecompositionofthegraphLaplacian(ğ‘Š).Inpractice,ğ‘Š is
nearlyalwaysapproximatelylowrank,meaningthatonlythefirst
ClusterHVmerge(â—‹3 inFig.4)hasthedualbenefitofreducing
ğ‘˜ â‰ª ğ¿ eigenvectorsareneeded.Insuchcases,fastrandomized
memoryuseandofelucidatingunderlyingsimilaritystructurein
eigendecompositionalgorithmscanreducethetimecomplexity
thedata.Intuitively,agroupofclusterHVscanbemergedifthey
tolinearinğ¿ [17].Thus,whilespectralclusteringissometimes
aresimilartoeachotheranddissimilarfromotherclusterHVs.
colloquiallythoughtofasanâ€œexpensiveâ€procedure,thisistrueonly
Forinstance,onemightmergetheclusterHVsforBulldogand
inveryunfavorableâ€œworst-caseâ€settings.Inpractice,itscomplexity
Chihuahuaintoasingleâ€œDogâ€clusterHV,thatremainsdistinct
ismodestandacceptableforoursituation,asshowninSec.7.5.
fromtheclusterHVforâ€œTabbyCatâ€.
TomergetheclusterHVs,wefirstconstructasimilaritygraph
6 VARIANTSOFLIFEHD
defined over the cluster HVs from the long-term memory. The
cluster HVscorrespond to nodes, and a pairof cluster HVs are WhileLifeHDisdesignedtocatertogeneralIoTapplicationswith
connectedbyanedgeiftheyaresufficientlysimilar.Wethenmerge streaminginputandwithoutsupervision,real-worldscenariosmay
theclusterHVsbycomputingaparticulartypeofcutinthegraph vary.Somescenariosmighthaveafewlabeledsamplesinaddition
inamannersimilartospectralclustering[40].Thisgraphbased totheunlabeledstream,whileothersmayrequireoperationwithin
formalismforclusteringisabletocapturecomplextypesofcluster strictpowerconstraints.LifeHDoffersextensibilitytoaddressthese
geometryandoftensubstantiallyoutperformssimplerapproaches diverseneeds.Inthissection,weintroducetwosoftware-based
likeK-Means[59].WedetailthestepsofclusterHVmergingin extensions:LifeHDsemi,whichaddsaseparateprocessingpathto
LifeHDbelow,whileFig.5offersanillustrativeoverview. managelabeledsamples,andLifeHDa,whichadaptivelyprunes
theHDCmodelusingmaskingtohandlelow-powerscenarios.
Step1:Preprocessing.Giventhesetoflong-termmemoryclus-
terHVsL={ğ‘™ 1,...,ğ‘™ ğ¿},weconstructagraphGusingtheadjacency 6.1 LifeHD
semi
matrixğ´âˆˆ{0,1}ğ¿Ã—ğ¿ .Here,ğ´ ğ‘–ğ‘— =ğ´ ğ‘—ğ‘– =1[cos(ğ‘™ ğ‘–,ğ‘™ ğ‘—) â‰¥ğ›½],withğ›½ WhileLifeHDexcelsinunsupervisedscenarios,itdoesnothar-
asanadaptivethreshold.Inotherwords,anedgeconnectscluster
nesslabeleddatawhenavailable.Toaddressthislimitation,we
HVsğ‘™
ğ‘–
andğ‘™
ğ‘—
iftheirsimilarityinHD-spacesurpassesğ›½.Alargerğ›½
introduceLifeHDsemiasanextensiontoenhanceaccuracyutilizing
impliesthatclusterHVsmustbemoresimilartobeconsideredfor
merging.Inpractice,wesetğ›½ = ğ‘€1 (cid:205) ğ‘–ğ‘€ =1ğœ‡ ğ‘–,representingthemean t Fh oe rl eim aci hted inl pa ube tl bs. aI tn chFi ğ‘–g ğ‘‘. ğ‘¥6 ,, ww eep coro nv si id de eran two ove sr uv bie sw etso :f oL nif eeH laD bese lm edi.
oftheobservedclusterHVs. (ğ‘‹ ğ‘™,ğ‘–ğ‘‘ğ‘¥,ğ‘¦ ğ‘™,ğ‘–ğ‘‘ğ‘¥)andoneunlabeledğ‘‹ ğ‘¢ğ‘™,ğ‘–ğ‘‘ğ‘¥.Wedenotetheaveragela-
whS et re ep ğ·2: isD te hc eom diap go os nit ai lo mn. aW tre ixc io nm wpu ht ie chth ğ·e ğ‘–L ğ‘–a =pla (cid:205)ci ğ‘—a ğ´n ğ‘–ğ‘Š ğ‘—.W= eğ· tâˆ’ heğ´ n, belingratiothroughoutthedatastreamasğ‘Ÿ = (cid:205) ğ‘–ğ‘‘ğ‘¥(cid:205) |ğ‘‹ğ‘–ğ‘‘ ğ‘™,ğ‘¥ ğ‘–ğ‘‘| ğ‘¥ğ‘‹ |ğ‘™ +,ğ‘– |ğ‘‘ ğ‘‹ğ‘¥ ğ‘¢| ğ‘™,ğ‘–ğ‘‘ğ‘¥|.
computetheeigenvaluesğœ† 1,..,ğœ† ğ¿,sortedinincreasingorder,and Sinceobtainingexternalsupervisionisoftenchallengingindy-
eigenvectorsğœˆ 1,...,ğœˆ ğ¿ofğ‘Š.
namicenvironments,wefocusoncaseswhereğ‘Ÿ â‰¤0.01.
Step3:Grouping.Weinferğ‘˜ =maxğ‘–âˆˆ[ğ¿]ğœ† ğ‘– â‰¤ğ‘” ğ‘¢ğ‘,andmerge LifeHDsemi retains the two-tier memory structure of LifeHD
theclusterHVsbyrunningK-Meansonğœˆ 1,...,ğœˆ ğ‘˜.Theupperbound butintroducesmodificationstotheworkingmemorycomponents.
ğ‘” ğ‘¢ğ‘ isahyperparameterthatadjuststhegranularityofmerging, IntheLifeHDsemipipeline,theworkingmemoryundergoesthree
withasmallerğ‘”
ğ‘¢ğ‘
leadingtosmallerğ‘˜thusencouragingmerging keysteps.Firstly,labeledsamples(ğ‘‹ ğ‘™,ğ‘¦ ğ‘™)updatelabeledclasshy-
pervectorsfollowingtheconventionalHDCmethodsoutlinedin
moreaggressively.
Sec.3.Next,weprocessunlabeledsamplesğ‘‹ ğ‘¢ğ‘™ throughnovelty
Ourmergingapproachisformallygrounded,asdiscussedin
detectionandHVupdatemodules,mirroringLifeHD.Importantly,
[59].Itisawell-knownfactthattheeigenvectorsofğ‘Š encode
informationabouttheconnectedcomponentsofG.WhenGhas
inLifeHDsemi,theseoperationsareappliedtobothlabeledHVsand
ğ‘˜ connectedcomponents,theeigenvaluesğœ† 1 =ğœ† 2 = ... =ğœ† ğ‘˜ =0.
Torecoverthesecomponents,K-Meansclusteringonğœˆ 1,...,ğœˆ ğ‘˜ can
beemployed,asexplainedin[59].However,practicalscenarios
mayhaveafewinter-componentedgesthatshouldideallybedis-
tinct.Forinstance,whenthesimilaritythresholdisimprecisely
set,erroneousedgesmayappearinthegraph,causingğœ† 1,...,ğœ† ğ‘˜ to
beonlyapproximatelyzero.Ourmergingapproachisdesignedto
handlethissituationbyintroducingğ‘” ğ‘¢ğ‘.TheclusterHVmerging
isevaluatedeveryğ‘“ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ batches,whereğ‘“ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ isahyperparame- Figure6:AnoverviewofLifeHDsemiwhichisdesignedtohandle
terthatcontrolsthetrade-offbetweenmergingperformanceand scarcelabeledsamples.LifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
7 EVALUATIONOFLIFEHD
7.1 SystemImplementation
WeimplementLifeHDwithPythonandPyTorch[43]anddeployit
onthreestandardedgeplatforms:RaspberryPi(RPi)Zero2W[3],
Raspberry Pi 4B [2], and Jetson TX2 module [1]. The selection
Figure7:Left:AnexampleoftheimpactofmaskingonHDC.Teston
ofedgeplatformsrepresentthreetierswithsmall,mediumand
CIFAR-10[29].Right:TheintuitionsbehindthedesignofLifeHDa.
abundantresources.
RPiZero2Whasa1GHzquad-coreCortex-A53CPUand512MB
clusterHVs.Lastly,weintroduceamergingsteptogrouplabeled SDRAM.RPi4Benjoysa1.8GHzquad-coreCortex-A72CPUand
HVsandclusterHVsthatarecloselyrelated.Tohandlelabeled 4GBSDRAM.TheJetsonTXplatformisequippedwithadual-core
HVs, we modify the adjacency matrixğ´ by making it diagonal NVIDIADenver2CPU,aquad-coreARMCortex-A57MPCore,an
forlabeledentries.Forexample,ifthefirstğ½ HVscorrespondto NVIDIAPascafamilyGPUwith256NVIDIACUDAcores,and8GB
labeledHVs,weensurethatğ´ 1:ğ½,1:ğ½ = diag([1,...,1]),whilecal- RAM.Wemeasurethetraininglatencyperbatchandtheenergy
culatingtheremainingvaluesfollowingLifeHDprocedures.This consumptionusingtheHioki3334powermeter[19].
strategypreventsthemergingoflabeledHVswitheachother.With WeareawarethatallNN-basedfeatureextractorscanbepruned
theseadjustments,LifeHDsemioffersasolutionthatretainsthecore andquantizedtoattainmoreefficientdeploymentonedgeplat-
elementsofLifeHDwhilehandlingscarcelabeledinputs. forms[11],samefortheNN-basedbaselineswecompareto[13,14].
However,NNmodelcompressionisnottheprimaryfocusofLifeHD.
6.2 LifeHD Existingcompressiontechniques[26,39]canbeapplieddirectlyto
a
thefeatureextractorinLifeHD.WeleaveLifeHDwithacceleration
While HDC computation is typically lightweight, there may be
designandemerginghardwaredeploymentforfutureworks.
instancesofenergyscarcity(e.g.,whenpoweredbyasolarpanel)
thatcallforabalancebetweenaccuracyandpowerefficiency.Sim-
7.2 ExperimentalSetup
ilartoneuralnetworks,oneapproachistoprunetheHDCmodel
usingamask,retainingthemostcrucialHDCdimensionspost- WeconductcomprehensiveexperimentstoevaluateLifeHDon
encoding[25].Dimensionimportancecanbedeterminedbyag- threetypicaledgescenarios.Allthreescenariosincorporatecon-
gregatingallclasshypervectorsintooneandsortingthevalues tinuousdatastreamsandexpectlifelonglearningovertime.We
acrossalldimensions.Notably,directreductionoftheencoding summarizetheexperimentalsetupinTable2.
dimensionshouldbeavoided,asitcandegradeHDCâ€™sexpressive Application#1:PersonalHealthMonitoring.Continuous
capabilityandendupwithcorruption.Fig.7(left)visuallydemon- healthmonitoringhasemergedasapopularusecaseforIoT.We
stratestheimpactofmaskinginsupervisedHDCtasks:retaining utilizetheMHEALTH[6]datasetwhichincludesmeasurements
thetop6000bitsincursonlya3%accuracylosscomparedtousing ofacceleration,rateofturn,andmagneticfieldorientationona
thefull10K-bitprecision. smartwatch.MHEALTHdifferentiates12activitiesindailylives
WhiletheconceptofmaskinghasbeenemployedinpriorHDC andiscollectedfrom10subjects.Notably,MHEALTHemploysraw
studies[25],theyarenotdirectlyapplicabletoLifeHDduetotheir time-seriessignalsratherthanprocessedfrequencycomponentsas
offlinetrainingsettingwithiiddata.Withstreamingnon-iiddata inputs.Weusetimewindowsof2.56s(ğ‘‡ =128)with75%overlapto
inLifeHD,thesetofobservedclusterHVsmayonlyrepresenta generatethesamples.Incontrasttopreviousdatasets,westrictly
subsetofthepotentialclasses,andthelesssignificantbitscould adheretothetemporalorderduringdatacollection.
becomecrucialasnewclassesareintroduced. Application#2:SoundCharacterization.Continuoussound
WeintroduceLifeHDa,whichenhancesLifeHDthroughanadap- detectioncontributestothecharacterizationofurbanenvironments.
tivemaskingapproachappliedtoallclusterHVsinworkingand WechoosetheESC-50[44]datasettoemulatethisscenario.This
long-termmemory.Letğ· ğ‘representthetargetdimensionforre- datasetcomprises5-second-longrecordingscategorizedinto50
duction.TherationalebehindLifeHDaisdepictedinFig.7(right). semantically diverse classes, including animals, human sounds,
WheneveranoriginalLifeHDdetectsnovelty,wetemporarilyrevert andurbannoises.Weconstructtheclass-incrementalstreamsby
tothefulldimensionğ·for2batches,whichissufficientforLifeHD arrangingthedatainrandomorderwithineachclass.
toconsolidatenewpatternsinitsmemory.Afterthesetwobatches, Application#3:ObjectRecognition.Objectrecognitionisa
weassessthelong-termmemoryclusterHVsbyaggregatingthem commonusecaseforcamera-mountedmobilesystems,e.g.,self-
andrankingthedimensions,andderiveamaskretainingğ· ğ‘dimen- drivingvehicles.Wesetupaclass-incrementalstreamfromCIFAR-
sionswiththelargestabsolutevalues.Thismaskisthenappliedto 100[29],consistingof32Ã—32RGBimagesof20coarseclasses.We
thefollowingbatchesofğœ™(ğ‘‹)immediatelyafterencoding,upuntil furtherevaluatethecaseofdatadistributiondriftbyexamining
thenextnoveltyisdetected.Noveltydetectionisexecutedwiththe gradualrotationsoccurringwithineachCIFAR-100class.
maskedhypervectors.Importantly,LifeHDacanutilizethesame On MHEALTH, LifeHD is fully dependent on the HDC spa-
noveltydetectionsensitivityasLifeHDsincethemostsignificant tiotemporal encoder to process the raw time-series signals. For
dimensionsdominatethesimilaritycheck.Inotherwords,thesim- ESC-50andCIFAR-100,LifeHDutilizestheHDnnframeworkwith
ilarityresultsinLifeHDausingğ· ğ‘dimensionaresimilarasusing apretrainedfeatureextractorbeforeHDCencoding,sameasinthe
thefulldimension.LifeHDaoffersanadaptiveHDCmodelpruning state-of-the-artHDCworks[18,52].Specifically,weadaptapre-
interfacewithminimalaccuracylossandoverhead. trainedACDNetwithquantifiedweights[37]forESC-50.ACDNetUnderreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaÅ imuniÄ‡Rosing
Table2:ExperimentalsetupofLifeHDacrossalldatasets.
Dataset Application Classes Total TrainingDataOrder HDnn? Pretrained #of
Category (Balanced?) Samples Models Params
MHEALTH[6] Activity 12(N) 9K Temporalorderduringcollection N - -
ESC-50[44] Sound 50(Y) 2K Class-incremental,randomwithinclass Y ACDNet[37] 4.7M
Class-incremental,randomwithinclass MobileNetV2[51]or 2.2M
CIFAR-100[29] Image 20(Y) 60K Y
orgradualrotationwithinclass MobileNetV3small[20] 927K
Table3:ImportanthyperparametersconfigurationofLifeHD.
Dataset HDCEncoding LifeHDDesign
ğ· ğ‘„ ğ‘ƒ ğ‘ğ‘†ğ‘–ğ‘§ğ‘’ ğ‘€ ğ›¾ ğ‘”
ğ‘¢ğ‘
ğ‘“ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’
MHEALTH 1000 5 0.01 32 50 3.0 0.2 25
ESC-50 10000 100 0.02 32 100 1.0 0.1 5
CIFAR-100 10000 100 0.01 32 100 1.0 0.1 150
isacompactconvolutionalneuralnetworkarchitecturedesigned
forsmallembeddeddevices.ForCIFAR-100,weuseaMobileNet
V2[51]foraccuracyevaluationandMobileNetV3small[20]for
efficiencyevaluation,bothpretrainedonImageNet[48].Forall
pretrainedNNs,weremovethelastfullyconnectedlayerusedfor Figure8:Agraphicalexplanationofthepipelinesetup.Greenout-
classificationandkeeptheremainingweightsfrozen. linesdenotethemoduletrainedwiththestreamingdata.Blueout-
Table3summarizesthekeyhyperparametersinLifeHD,which linesdenotetheunsupervisedclassifiertrainedduringtesting.Gray
areselectedbasedonaseparatevalidationset.Weconfigureğ›¼ = denoteswhenthemoduleisfrozen&nottrainedfurther.
0.1formoving-averageupdate,â„ğ‘–ğ‘¡ ğ‘¡â„ =10forlong-termmemory
consolidation.Thelong-termmemorysizeğ¿issetto50inallcases. However,sincewedonotassumeawarenessoftaskshifts,
wesimplyfreezethemodelfromthepreviousbatch.
7.3 State-of-the-ArtBaselines â€¢ LUMP[13]employsamemorybufferforreplayandmit-
igatescatastrophicforgettingbyinterpolatingthecurrent
We conduct a comprehensive comparison between LifeHD and
batchwithpreviouslystoredsamplesinthememory.
state-of-the-artNN-basedunsupervisedlifelonglearningbaselines,
â€¢ STAM[54]isbrain-inspiredexpandablememoryarchitec-
whichcontinuouslytrainaNNforrepresentationlearning.Theloss
tureusingonlineclusteringandnoveltydetection.Weex-
functionsinthesesetupsaredefinedinthefeaturespacewithout
clusivelyapplySTAMtoCIFAR-100duetoitsdemandfor
relyingonlabelsupervision.Duringtesting,wefreezetheneural
intricatedataset-specifictuning(e.g.,numberofreceptive
networkandapplyK-Meansclusteringonthetestingfeatureem-
beddingstogeneratepredictedlabels.ğ‘˜ issetto50whichisthe fields),andbecausetheauthorsonlyreleasedtheimplemen-
tationfortheCIFARdatasets.
samenumberofclusterHVsasinLifeHD.Suchapipelineiswidely
â€¢ SupHDC[18,27]isthefullysupervisedHDCpipeline.
usedforlifelonglearningevaluations[46,54].
Fig.8presentsacomparisonofthepipelinesetupusingboth Allbaselinesareadaptedfromtheiroriginalopen-sourcecode.
thebaselinesandLifeHDonHDnnandnon-HDnnframeworks ForCaSSLeandLUMP,weemployBYOL[16]astheself-supervised
respectively.Toensurefaircomparisons,inHDnnframeworkon lossfunctionbecauseithasshowedsuperbempiricalperformance
ESC-50andCIFAR-100,weinitializetheNNwiththesamepre- inlifelonglearningtaskscomparedtootherself-supervisedlearning
trainedweightsforLifeHDandNNbaselines.FortheNNbaselines backbones[14].Weusethememorybuffersizeof256forLUMP
onMHEALTH,werandomlyinitializeaone-layerLSTMof64units whichisthesameasintheoriginalpaper.WeemploytheStochastic
followed by a fully connected layer of 512 units. This architec- GradientDescent(SGD)optimizerwithalearningrateof0.03across
turehasachievedcompetitiveaccuracyastheTransformers-based allmethods,trainingeachbatchfor10steps.Allexperimentsare
designsonMHEALTH[12]. executedfor3randomtrials.
WecompareLifeHDwiththefollowingbaselines,whichinclude
allmainlifelonglearningtechniques: 7.4 LifeHDAccuracy
â€¢ FinetuneisanaÃ¯vebaselinethatoptimizestheNNmodel ResultsonThreeApplicationScenarios.Fig.9(a)detailsthe
usingthecurrentbatchofdatawithoutanylifelonglearning ACCcurveofallmethodsasstreamingsamplesarereceived.All
techniques. NNbaselinesstartathigheraccuracy,especiallyinESC-50(sounds)
â€¢ CaSSLe[14]isadistillation-basedframeworkthatutilizes andCIFAR-100(images),owingtothepresenceofapretrainedNN
self-supervisedlosses.Itleveragesdistillationbetweenthe featureextractorwithintheHDnnframework.Meanwhile,LifeHD
representationsofthecurrentmodelandapastmodel.In beginswithloweraccuracyasboththeworkingandlong-term
theoriginalpaper,thepastmodeliscapturedattheendof memoriesareempty,needingtolearntheclusterHVsandtheop-
theprevioustaskandpriortotheintroductionofanewtask. timalnumberofclusters.Notably,asstreamingsamplescomein,LifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
(a)TheACCcurveofLifeHDvs.state-of-the-artNNbaselinesonthreescenarios. (b)ThefinalresultsofLifeHDinCIFAR-100withrotation.
Figure9:Theunsupervisedclusteringaccuracy(ACC)resultsofLifeHDonvariousinputdatastreams.
Table4:ThegapofACCsattheendofthestreambetweenLifeHD
andSupervisedHDC[18,27].
Method MHEALTH ESC-50 CIFAR-100
LifeHD 0.75 0.92 0.20
SupervisedHDC[18,27] 0.90 0.95 0.26
Gap -0.15 -0.03 -0.06
allNNbaselinesexperienceadeclineinACC,underscoringthein-
herentchallengesofunsupervisedlifelonglearningwithstreaming
non-iiddataandalackofsupervision.Thisisprimarilyduetothe
demandforextensiveiiddataandmulti-epochofflinetrainingfor
finetuningNNs,whichisnotfeasibleinoursetting.CaSSLe[14]
leadstoforgettingduetoitsinabilitytoidentifysuitablepastmod-
elsfromwhichtodistillknowledge.Similarly,LUMP[13]exhibits
Figure10:TheconfusionmatrixofLifeHDonMHEALTH.The
greenboxhighlightssmallerclusterHVsthatformasinglelarge
reducedACCinESC-50andCIFAR-100,withonlymarginalACC
groundtruthclass,whichisavalidlearningoutcome.Theredbox
improvementinMHEALTH(timeseries),suggestingthatitsmem-
highlights"boundary"clusterHVsthatspanmultipletruelabels,
oryinterpolationstrategymaynotbeuniversallysuitableforall
leadingtolowerACCs.
applications.Whilethememory-baseddesignofSTAM[54]can
mitigateforgetting,itsefficacyindistinguishingpatternsandac- loss.Inourexperiments,LifeHDdemonstratesminimalACCloss
quiringnewknowledgeremainsunsatisfactory.Onthecontrary, evenundersubstantialrotationshifts.
LifeHDdemonstratesincrementalaccuracyacrossallthreediffer- ComparisonwithFullySupervisedHDC.Table4compares
entscenarios,achievingupto9.4%,74.8%and11.8%accuracy theaverageACCsofsupervisedHDCmethod[18,27]andLifeHD.
increaseonMHEALTH(timeseries),ESC-50(sound)andCIFAR- Even without any supervision, LifeHD approaches the ACC of
100(images),comparedwiththeNN-basedunsupervisedlifelong supervisedHDCwithagapof15%,3%and6%onMHEALTH,
learningbaselinesattheend.Suchoutcomecanbeattributedto ESC-50andCIFAR-100.AminimalACCgapconfirmstheeffec-
HDCâ€™slightweightbutmeaningfulencodingandtheeffectivemem- tivenessofLifeHDinseparatingandmemorizingkeypatterns.To
orizationdesignofLifeHD. helpexplainthesmallACClossevenwithoutsupervision,wevi-
ResultsunderDataDistributionDrift.Wefurtherevaluate sualizetheconfusionmatrixofLifeHDonMHEALTHinFig.10.
LifeHDâ€™s performance under drifted data and present the final MHEALTHhas12trueclasses(yaxis),whereasLifeHDmaintains
ACC along with the number of discarded cluster HVs in Fig. 9 23clusterHVsinitslong-termmemory(xaxis).ACCisevaluated
(b).Specifically,weintroducegradualrotationtotheCIFAR-100 bymappingtheunsupervisedclusterHVstotruelabels.Although
sampleswithineachclass,rangingfromnorotationtoasubstantial LifeHDcannotachievepreciselabelmatchingwithtrueclasses,it
rotation angle of 80â—¦. The other parameter settings remain the canpreservetheessentialpatternsbyusingfiner-grainedclusters.
sameasinTable3.ThenumberofdiscardedclusterHVsaccounts Forexample,thegreenboxinFig.10highlightsavalidlearning
forthosethatareeitherforgottenormerged.FromFig.9(b),we outcome,whereLifeHDusespredictedclusterHVNo.0,9and23
canobservetheremarkableresilienceofLifeHDtodrifteddata, torepresentabiggertrueclassofâ€œLyingdownâ€.
withanACClossoflessthan2.3%evenunderasevererotationof
80â—¦.Thisrobustnessstemsfromthegeneralanduniformdesignof 7.5 TrainingLatencyandEnergy
LifeHDtoaccommodatevarioustypesofcontinuouslychanging Fig.11providescomprehensivelatencyandenergyconsumption
datastreams.Incasesofslightorminimaldistributiondrift,LifeHD resultstotrainonebatchofsamplesonallthreeedgeplatforms.
updatesexistingclusterHVs;ininstancesofseveredrift,newcluster ForCIFAR-100,weusethemostlightweightMobileNetversion,
HVsarecreatedandsubsequentlymergedifdeemedappropriate. V3small[20],asHDnnfeatureextractorandNNbaseline,toas-
However,duetothefinitememorycapacities,moreclusterHVsare sess LifeHDâ€™s efficiency gain over the most competitive mobile
subjecttoforgettingormergingunderlargerdrifts,asshownin computingsetup.OnRPiZero,wereportresultsfortherelatively
Fig.9(b)bythenumberofdiscardedclusterHVs,leadingtoACC lightweightNN-basedbaselines,FinetuneandLUMP[13],usingUnderreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaÅ imuniÄ‡Rosing
(a)LatencyonRPiZero (b)LatencyonRPi4B (c)LatencyonJetsonTX2
(d)EnergyonRPiZero (e)EnergyonRPi4B (f)EnergyonJetsonTX2
Figure11:LatencyandenergyconsumptiontotrainonebatchofdatausingLifeHDandallbaselinesonoff-the-shelfedgeplatforms.
themethodsintoNNtraining(Finetune,LUMP[13],CaSSLe[14],
STAM[54])andHDCtraining(SupervisedHDC[18,27]andour
LifeHD).Following[30],wecalculatethepeakmemoryofNNtrain-
ingasthesumofmodel,optimizerandactivationmemories,plus
additionalmemoryconsumptionforlifelonglearning.Specifically,
Figure12:PeakmemoryfootprintofallmethodsonMHEALTH CaSSLe[14]requiresadditionalmemoryfortrainingapredictor
(left)andCIFAR-100(right)withbatchsizeof1.Theresultsare andinferencefromafrozenmodel,LUMP[13]needsextramemory
representativefortimeseriesdataandimagedata. forreplay.ForHDC-basedmethods,eachdimensionofthecluster
HVisrepresentedasasignedintegerandstoredinabyte.Inad-
thesmallestdataset,MHEALTH,whilerunningCaSSLe[14]on ditiontotheworkingandlong-termmemories,wealsoconsider
MHEALTHwouldresultinout-of-memoryerrors.Asshownin thestorageofbipolarlevelandIDhypervectorsforencoding,and
Fig.11,LifeHDisupto23.7x,36.5xand22.1xfastertotrainon thefrozenMobileNetforHDnnencodinginCIFAR-100.Notice
RPiZero,RPi4BandJetsonTX2,respectively,whilebeingupto thatourfocushereisoncomparingfull-precisionmemoryusage,
22.5x,34.3xand20.8xmoreenergyefficientoneach,compared andoptimizationtechniqueslikequantizationcanbeappliedtoall
totheNN-basedunsupervisedlifelonglearningbaselines.Inmost methodsinthefuture.
settings,CaSSLe[14]isthemosttime-consumingbecauseofthe The results in Fig. 12 highlight LifeHDâ€™s memory efficiency.
expensivedistillation.LUMP[13]isslightlymoreexpensivethan LifeHDconserves80.1%-86.2%and84.1%-96.0%ofmemorycom-
Finetuneduetoitsreplaymechanism.STAM[54],implemented paredtoNNtrainingbaselinesonMHEALTH(non-HDnn)and
onlyonCPU,incursthelongesttraininglatencyonJetsonTX2,asit CIFAR-100(HDnn),respectively.Thisremarkableefficiencystems
doesnotuseGPUâ€™saccelerationcapabilities.LifeHDisclearlyfaster fromLifeHDâ€™sHDCdesign,whichdispenseswiththememory-
andmoreefficientthanallNN-basedunsupervisedlifelonglearning intensivegradientdescentsinNNs.STAM[54],withitshierarchical
baselines[13,14,54]duetoLifeHDâ€™slightweightnature.Theover- andexpandablememorystructure,consumes6.3xthememoryof
headofLifeHDalongsidefullysupervisedHDC,SupHDC[18,27], LifeHD,asitstoresrawimagepatchesacrossallhierarchies.Com-
isnegligibleonmorepowerfulplatformslikeRPi4BandJetsonTX2. paredtofullysupervisedHDC,SupHDC[18,27],LifeHDintroduces
Notably,inLifeHD,theclusterHVmergingstepforprocessing amodestmemoryincreasetoaccomplishthechallengingtaskof
about40LTMelementstakes7.4,0.86and0.66secondstorunon organizinglabel-freeclusterHVs.LifeHDprovesadvantageousfor
RPiZero,RPi4BandJetsonTX2,respectively,whichonlyexecutes edgeapplicationswithonly103KBand2.5MBofpeakmemory
onceeveryğ‘“ ğ‘šğ‘’ğ‘Ÿğ‘”ğ‘’ batches.Furtherenhancementscanbeachieved requiredforMHEALTHandCIFAR-100.
usingtheaccelerationtechniquesmentionedinSec.5.4.
Fig.11indicatesLifeHDimproveslatencyandenergyefficiency
themostonRPi4B,ascomparedtoRPiZeroandJetsonTX2that 7.7 AblationStudies
representmorelimitedorpowerfuldevices.Thisisbecausethehigh- ThedesignofLifeHDconsistsofseveralkeyelements:thetwo-tier
dimensionalnatureofLifeHDrequiresafairamountofmemory, memoryorganization,noveltydetectionandonlineupdate,and
thus it cannot run efficiently on the highly restricted RPi Zero. clusterHVmergingthatmanipulatespastpatterns.Weconduct
TheGPUresourcesonJetsonTX2boosttheNN-basedbaselines, experimentstoassessthecontributionofeachelement.Usingthe
narrowingthegapbetweenthemandLifeHD.Weexpectmuch configurationinTable3,weevaluatetheperformanceof(i)LifeHD
largerefficiencyimprovementswhenLifeHDisacceleratedusing withoutlong-termmemory,usingonlyasinglelayermemory,(ii)
emergingin-memorycomputinghardware[11,65]. LifeHDwithoutmerging,employingonlynoveltydetection,online
updateandforgetting,and(iii)completeLifeHD.Wepresentthe
7.6 MemoryUsage
ACCandthenumberofclusterHVsinLTMduringMHEALTH
Fig.12providesacomprehensivesummaryofpeakmemoryfoot- training in Fig. 13, chosen as a representative scenario. LifeHD
printforallmethodsonMHEALTHandCIFAR-100.Wecategorize withoutLTM(greendashdotline)forcesclusterHVmergingtoLifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
(a)Workingmemorysize (b)Noveltythreshold (c)Mergingsensitivity
Figure13:AblationstudyofLifeHDonMHEALTH.Thenumberof
clusterHVsreportedforLifeHDwithoutlong-termmemory(LTM)
isfortheworkingmemory,sincenoLTMisallowed.
(d)Mergefrequency (e)Encodinglevel (f)Encodingflippingratio
takeplaceinworkingmemory,wherethelargenumberoftem- Figure14:SensitivityofvarioushyperparametersinLifeHD,using
poraryclusterHVscreateslessimportantnodesinthegraphand MHEALTHdatasetasarepresentative.
corruptsthegraph-basedmergingprocess,asshowninFig.13(left).
Thisnecessitatesthedesignofthetwo-tiermemoryarchitecture
andmergingwithLTMelements.LifeHDwithoutmerging(blue
similarityinHD-spaceafterencoding.Optimalğ‘„dependsonthe
dashedline)consumes1xmorememoryintheLTM,makingit sensorsensitivity,withfiner-grainedsensorsrequiringmorequanti-
unsuitableforresource-constrainededgedevices.Ourdesignof
zationlevels.ğ‘ƒdeterminesthesimilaritybetweenadjacentlevelsof
LifeHD(redsolidline)strategicallycombinessimilarclusterHVs hypervectors.Forpersonalhealthmonitoring,suchasMHEALTH,
withminorlossontheclusteringquality,achievingACCsimilarto
ğ‘„ =10,ğ‘ƒ =0.01usuallygivesthebestresults.
thosewithoutmergingwhileconservingmemorystorage.
8 EVALUATIONOFLIFEHD ANDLIFEHD
semi a
7.8 SensitivityAnalysis
Inthissection,wecompareLifeHDsemiandLifeHDa,ourproposed
Fig. 14 summarizes the sensivitity results of key parameters in extensionsfromLifeHD,withexistingdesignsthataresimilar.
LifeHD,whilethelesssensitiveonessuchasğ›¼andâ„ğ‘–ğ‘¡ ğ‘¡â„areomitted PerformanceofLifeHDsemi.ToevaluateLifeHDsemiinalow-
duetospacelimitation.ThedefaultsettingisthesameasinTable3. labelscenario,wecompareitwithSemiHD[22],whichisthestate-
WorkingMemorySize.Fig.14(a)showsACCsusingworking of-the-artHDCmethodforsemi-supervisedlearning.Weadapt
memorysizesof20,50,100and200.Ingeneral,alargerworking SemiHD[22]forsingle-passsettings,introducingapseudolabel
memoryallowsmoretemporaryclusterHVsatthecostofhigher assignmentthreshold.Whenthecosinesimilarityofanunlabeled
memoryconsumption.ğ‘€ = 100producesoptimalresults,while sampletothenearestclasshypervectorsurpassesthethreshold,we
furtherincreasingthememorysizereducesclusteringquality.This assignthatclassasitspseudolabel.Thesampleisthenemployedto
occursbecauseexcessivelylargeworkingmemoryretainsoutdated updatetheclasshypervectorinSemiHD.Weexplorevariousthresh-
prototypes,degradinglifelonglearningperformance. oldvaluesandchoosetheoptimalresultforcomparison.Fig.15
NoveltyThreshold.InFig.14(b),wepresentthefinalACCs (a)comparesLifeHDsemiandSemiHD[22]onESC-50andCIFAR-
fordifferentnoveltydetectionthresholds(ğ›¾).Alowerğ›¾ resultsin 100 across various labeling ratiosğ‘Ÿ < 0.01. The advantages of
morefrequentnoveltydetectionsandincreasedloadsonthework- LifeHDsemiaremostprominentwhenlabelsarelimited,theweakly
ingmemory,whileahigherğ›¾ mayleadtooverlookingsignificant supervisedscenarioisLifeHDsemiâ€™smajorfocus.LifeHDsemi im-
changes.Remarkably,LifeHDdemonstratesresiliencetovariations provesACCbyupto10.25%and3.6%onESC-50andCIFAR-100
inğ›¾,aphenomenonthatweattributetothecombinedimpactof respectively.Thisoutcomearisesfromtheunsupervisednature
noveltydetectionandmergingprocesses. ofLifeHD,allowingittoautonomouslyorganizeprominentclus-
MergingSensitivity.Fig.14(c)showsACCusingvariousmerg- terHVs,especiallywhenallsamplesfromaclasslacklabels.As
ingthresholds(ğ‘” ğ‘¢ğ‘).ğ‘” ğ‘¢ğ‘ determinesthenumberofclusters(ğ‘˜)to thelabelingratioincreases,LifeHDsemiâ€™sadvantageoverSemiHD
mergeintheclusterHVmergingstep(Sec.5.4).Alowvaluefor diminishes,becausemorelabelsbolsterSemiHDâ€™sperformance.
ğ‘” ğ‘¢ğ‘ resultsinoverlyaggressivemerging,leadingtothefusionof PerformanceofLifeHDa.LifeHDa providesaninterfaceto
dissimilarclusterHVsandadegradedACC.Alargerğ‘” ğ‘¢ğ‘ adoptsa trademinimalperformancelossforefficiencygains,byadaptively
conservativemergingstrategyandencouragesfiner-grainedclus- pruningouttheinsignificantdimensions.WecompareLifeHDa
ters,albeitattheexpenseofincreasedresourcedemands. with previous HDC works employing a fixed mask throughout
MergingFrequency.Fig.14(d)showsthefinalACCsfordif- training[25],andtheresultsarepresentedinFig.15(b)forCIFAR-
ferentmergingfrequencies(ğ‘“ ğ‘ğ‘ğ‘¡ğ‘â„).LifeHDshowsitsrobustness 100,includingACCandtraininglatencyperbatchonRPi4B.Fixed
acrossvariousğ‘“ ğ‘ğ‘ğ‘¡ğ‘â„ values,partlyduetothepresenceofğ‘” ğ‘¢ğ‘ to masksnegativelyimpactHDClearning,especiallywithsmaller
preventaggressivemerging.Lessfrequentmerging(largerğ‘“ ğ‘ğ‘ğ‘¡ğ‘â„) dimensions.Suchmasksfailtoadapttonewhypervectorsinclass-
raisestheriskofforgettingimportantpatternsasofmemorycon- incrementalstreams,wherelesssignificantdimensionsmaybecome
straints.Morefrequentmerging(smallerğ‘“ ğ‘ğ‘ğ‘¡ğ‘â„)increasesthecom- cruciallaterintraining.LifeHDaaddressesthisissuebyadjusting
putationalburdenduetothespectralclustering-basedalgorithm. themaskuponnoveltydetection,leadingtoadegradationofonly
EncodingLevelandFlippingRatioforSpatiotemporalEn- 0.71%inACCand4.5xefficiencygaincomparedtothecomplete
coding.Fig.14(e)and(f)showtheACCsforvariousquantization LifeHD,usingonly20%ofthefullHDdimensionofLifeHD.The
encodinglevels(ğ‘„)andflippingratios(ğ‘ƒ)duringthespatiotem- overheadofadaptivelyadjustingthemaskisnegligiblewhennov-
poralencoding.Bothparametersareimportantforpreservingthe eltydetectionoccursinfrequently.Underreview,, XiaofanYu,AnthonyThomas,IvanniaGomezMoreno,LouisGutierrez,andTajanaÅ imuniÄ‡Rosing
10 CONCLUSION
Theabilitytolearncontinuouslyandindefinitelyinthepresenceof
change,andwithoutaccesstosupervision,onaresource-constrained
deviceisacrucialtraitforfuturesensorsystems.Inthiswork,we
designanddeploythefirstend-to-endsystemnamedLifeHDto
learncontinuouslyfromreal-worlddatastreamswithoutlabels.
(a)GainsofLifeHDsemioverSemiHD[22]inlowerlabelingratios. OurapproachisbasedonHyperdimensionalComputing(HDC),
anemergingneurally-inspiredparadigmforlightweightedgecom-
puting.LifeHDisbuiltonatwo-tiermemoryhierarchyincluding
a working and a long-term memory, with collaborative compo-
nentsofnoveltydetection,onlineclusterHVupdateandcluster
HVmergingforoptimallifelonglearningperformance.Wefur-
therproposetwoextensionstoLifeHD,LifeHDsemiandLifeHDa,
(b)LifeHDavs.usingfixedmask(Fixed)[25]undervariousğ·ğ‘. to handle scarce labeled samples and power constraints. Practi-
Figure15:ResultsofLifeHDsemiandLifeHDacomparedtoexisting caldeploymentsontypicaledgeplatformsandthreeIoTscenarios
HDCtechniquesforsimilargoals. demonstrateLifeHDâ€™simprovementofupto74.8%onunsupervised
clusteringaccuracyandupto34.3xonenergyefficiencycompared
tostate-of-the-artNN-basedunsupervisedlifelonglearningbase-
9 DISCUSSIONSANDFUTUREWORKS
lines[13,14,54].
ProblemScale.OnelimitationofLifeHDistherelativesmallprob-
lemscale(e.g.,theimagesizeofCIFAR-100isrestrictedto32x32)
ACKNOWLEDGMENTS
due to the essential difficulty of unsupervised lifelong learning
Theauthorswouldliketothanktheanonymousshepherd,review-
problem,includingsingle-passnon-iiddataandnosupervision.For
ers,andourcolleagueXiyuanZhangfortheirvaluablefeedback.
thesamereason,thereremainsadisparityinaccuracybetween
ThisworkwassupportedinpartbyNationalScienceFoundation
unsupervisedlifelonglearningandfullysupervisedNNs,assub-
underGrants#2003279,#1826967,#2100237,#2112167,#1911095,
stantiatedbypriorresearch[13,54].InordertoscaleLifeHDto
#2112665,andinpartbyPRISMandCoCoSys,centersinJUMP2.0,
morechallengingapplicationssuchasself-drivingvehicles,one
anSRCprogramsponsoredbyDARPA.
possibledirectionistoleveragethepretrainedfoundationmodelas
afrozenfeatureextractorintheHDnnframework,whichweleave
REFERENCES
forfutureinvestigation.
HyperparameterTuning.Whilewerecognizethathyperpa- [1] 2023.JetsonTX2Module.https://developer.nvidia.com/embedded/jetson-tx2.
[Online].
rameterscaninfluencetheperformanceofLifeHD,suchanissueis
[2] 2023.RaspberryPi4B.https://www.raspberrypi.com/products/raspberry-pi-4-
notexclusivetoLifeHD,buthaspersistentlybeenachallengein model-b/. [Online].
machinelearningresearch[7].InLifeHD,theimpactofhyperpa- [3] 2023.RaspberryPiZero2W.https://www.raspberrypi.com/products/raspberry-
pi-zero-2-w/. [Online].
rameterscanbemitigatedthroughpre-deploymentevaluationand
[4] AuroreAvarguÃ¨s-Weberetal.2012. Simultaneousmasteringoftwoabstract
componentco-design.Forexample,encodingparameterssuchas conceptsbytheminiaturebrainofbees.ProceedingsoftheNationalAcademyof
ğ‘„,ğ‘ƒ canbetunedonsimilarhealthmonitoringdatasourcesprior Sciences109,19(2012),7481â€“7486.
[5] AlanBaddeley.1992.Workingmemory.Science255,5044(1992),556â€“559.
todeployment.Meanwhile,thecomponentofclusterHVsmerging [6] GarciaRafaelBanos,OrestiandAlejandroSaez.2014.MHEALTHDataset.UCI
canincreaseLifeHDâ€™sresiliencytothenoveltydetectionthreshold MachineLearningRepository. DOI:https://doi.org/10.24432/C5TW22.
ğ›¾,asahigherquantityofnovelclusterscanbemergedinlaterstage [7] BerndBischletal.2023.Hyperparameteroptimization:Foundations,algorithms,
bestpractices,andopenchallenges.WileyInterdisciplinaryReviews:DataMining
oflearning. andKnowledgeDiscovery13,2(2023),e1484.
LimitationsofHDC.HDCservesasthefundamentalcoreof [8] TrentonBrickenetal.2023.SparseDistributedMemoryisaContinualLearner.
InInternationalConferenceonLearningRepresentations.
LifeHD.WhileHDCshowspromisewithitsnotablelightweight
[9] HanCaietal.2020. Tinytl:Reducememory,notparametersforefficienton-
design,itisburdenedbyseverallimitationsthatremainactiveareas devicelearning. AdvancesinNeuralInformationProcessingSystems33(2020),
ofresearch.First,forcomplexdatasetslikeaudioandimages,HDC 11285â€“11297.
[10] NingChenetal.2016.Smarturbansurveillanceusingfogcomputing.In2016
requiresapretrainedfeatureextractor(theHDnnencoding)which IEEE/ACMSymposiumonEdgeComputing(SEC).IEEE,95â€“96.
maynotexistforcertainapplications.Moreover,akintoanyother [11] ArpanDuttaetal.2022. Hdnn-pim:Efficientinmemorydesignofhyperdi-
mensionalcomputingwithfeatureextraction.InProceedingsoftheGreatLakes
architecture, HDC vectors face capacity limitations determined
SymposiumonVLSI2022.281â€“286.
bythedimensionofHDspace,encodingmethod,andpotential [12] EhabEssaandIslamRAbdelmaksoud.2023.Temporal-channelconvolutionwith
noiselevelsintheinputdata[56].Duetothesefactors,careful self-attentionnetworkforhumanactivityrecognitionusingwearablesensors.
Knowledge-BasedSystems278(2023),110867.
evaluationandsometimesmanualfeatureengineeringarerequired
[13] DivyamMadaanetal.2022. RepresentationalContinuityforUnsupervised
tosuccessfullydeployHDCfornewapplications. ContinualLearning.InInternationalConferenceonLearningRepresentations.
FutureWorks.AlthoughLifeHDfocusesonsingle-devicelife- [14] EnricoFinietal.2022.Self-supervisedmodelsarecontinuallearners.InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
longlearningforclassificationtasks,themethodcanbeextended [15] InGimandJeongGilKo.2022.Memory-efficientDNNtrainingonmobiledevices.
forothertypesoftasksandlearningsettings,suchasfederated InProceedingsofthe20thAnnualInternationalConferenceonMobileSystems,
ApplicationsandServices.464â€“476.
learningandreinforcementlearning.Weleavetheinvestigationof
[16] Jean-BastienGrilletal.2020. Bootstrapyourownlatent-anewapproachto
thesetopicsforfuturework. self-supervisedlearning.Advancesinneuralinformationprocessingsystems33LifelongIntelligenceBeyondtheEdgeusingHyperdimensionalComputing Underreview,,
(2020),21271â€“21284. [47] HaoyuRen,DarkoAnicic,andThomasARunkler.2021.Tinyol:Tinymlwith
[17] NathanHalko,Per-GunnarMartinsson,andJoelATropp.2011.Findingstructure online-learningonmicrocontrollers.In2021InternationalJointConferenceon
withrandomness:Probabilisticalgorithmsforconstructingapproximatematrix NeuralNetworks(IJCNN).IEEE,1â€“8.
decompositions.SIAMreview53,2(2011),217â€“288. [48] OlgaRussakovskyetal.2015.Imagenetlargescalevisualrecognitionchallenge.
[18] MichaelHerscheetal.2022.Constrainedfew-shotclass-incrementallearning.In Internationaljournalofcomputervision115(2015),211â€“252.
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. [49] Andrei A Rusu et al. 2016. Progressive neural networks. arXiv preprint
9057â€“9067. arXiv:1606.04671(2016).
[19] Hioki.2023.Hioki3334Powermeter.https://www.hioki.com/en/products/detail/ [50] SwapnilSayanSahaetal.2023.TinyNS:Platform-AwareNeurosymbolicAuto
?product_key=5812. TinyMachineLearning. ACMTransactionsonEmbeddedComputingSystems
[20] AndrewHowardetal.2019. Searchingformobilenetv3.InProceedingsofthe (2023).
IEEE/CVFInternationalConferenceonComputerVision.1314â€“1324. [51] MarkSandleretal.2018.Mobilenetv2:Invertedresidualsandlinearbottlenecks.
[21] MohsenImanietal.2019.Hdcluster:Anaccurateclusteringusingbrain-inspired InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
high-dimensionalcomputing.InDesign,Automation&TestinEuropeConference 4510â€“4520.
&Exhibition(DATE).IEEE,1591â€“1594. [52] YangShen,SanjoyDasgupta,andSaketNavlakha.2021.Algorithmicinsightson
[22] MohsenImanietal.2019. Semihd:Semi-supervisedlearningusinghyperdi- continuallearningfromfruitflies.arXivpreprintarXiv:2107.07617(2021).
mensionalcomputing.InIEEE/ACMInternationalConferenceonComputer-Aided [53] ShunShunhouandYangPeng.2022.AIoTonCloud.InDigitalTransformation
Design(ICCAD).IEEE,1â€“8. inCloudComputing.CRCPress,629â€“732.
[23] MohsenImani,DeqianKong,AbbasRahimi,andTajanaRosing.2017.Voicehd: [54] JamesSmithetal.2021. UnsupervisedProgressiveLearningandtheSTAM
Hyperdimensionalcomputingforefficientspeechrecognition.InIEEEInterna- Architecture.InProceedingsoftheThirtiethInternationalJointConferenceon
tionalConferenceonRebootingComputing(ICRC).IEEE,1â€“8. ArtificialIntelligence,IJCAI-21.2979â€“2987.
[24] PenttiKanerva.2009.Hyperdimensionalcomputing:Anintroductiontocom- [55] KeSun,ChenChen,andXinyuZhang.2020."Alexa,stopspyingonme!"speech
putingindistributedrepresentationwithhigh-dimensionalrandomvectors. privacyprotectionagainstvoiceassistants.InProceedingsofthe18thconference
CognitiveComputation1(2009),139â€“159. onEmbeddedNetworkedSensorSystems.298â€“311.
[25] BehnamKhaleghi,MohsenImani,andTajanaRosing.2020.Prive-hd:Privacy- [56] AnthonyThomas,SanjoyDasgupta,andTajanaRosing.2021. Atheoretical
preservedhyperdimensionalcomputing.InACM/IEEEDesignAutomationCon- perspectiveonhyperdimensionalcomputing. JournalofArtificialIntelligence
ference(DAC).IEEE,1â€“6. Research72(2021),215â€“249.
[26] HyejiKim,MuhammadUmarKarimKhan,andChong-MinKyung.2019.Effi- [57] MatteoTiezzietal.2022.StochasticCoherenceOverAttentionTrajectoryFor
cientneuralnetworkcompression.InProceedingsoftheIEEE/CVFconferenceon ContinuousLearningInVideoStreams.InProceedingsoftheThirty-FirstInterna-
computervisionandpatternrecognition.12569â€“12577. tionalJointConferenceonArtificialIntelligence,IJCAI-22.3480â€“3486.
[27] YeseongKim,MohsenImani,andTajanaSRosing.2018. Efficienthumanac- [58] RishabhTiwarietal.2022.Gcr:Gradientcoresetbasedreplaybufferselection
tivityrecognitionusinghyperdimensionalcomputing.InProceedingsofthe8th forcontinuallearning.InProceedingsoftheIEEE/CVFConferenceonComputer
InternationalConferenceontheInternetofThings.1â€“6. VisionandPatternRecognition.99â€“108.
[28] JamesKirkpatricketal.2017. Overcomingcatastrophicforgettinginneural [59] UlrikeVonLuxburg.2007. Atutorialonspectralclustering. Statisticsand
networks.Proceedingsofthenationalacademyofsciences(2017). computing17(2007),395â€“416.
[29] AlexKrizhevsky,GeoffreyHinton,etal.2009.Learningmultiplelayersoffeatures [60] ErweiWangetal.2019.Deepneuralnetworkapproximationforcustomhardware:
fromtinyimages.(2009). Whereweâ€™vebeen,whereweâ€™regoing.ACMComputingSurveys(CSUR)52,2
[30] YoungDKwonetal.2023.LifeLearner:Hardware-AwareMetaContinualLearn- (2019),1â€“39.
ingSystemforEmbeddedComputingPlatforms.InProceedingsofthe21stACM [61] QipengWangetal.2022.Melon:Breakingthememorywallforresource-efficient
ConferenceonEmbeddedNetworkedSensorSystems. on-devicemachinelearning.InProceedingsofthe20thAnnualInternational
[31] SoochanLeeetal.2020.ANeuralDirichletProcessMixtureModelforTask-Free ConferenceonMobileSystems,ApplicationsandServices.450â€“463.
ContinualLearning.InInternationalConferenceonLearningRepresentations. [62] GaryMWeissetal.2016. Smartwatch-basedactivityrecognition:Amachine
[32] JiLinetal.2020.Mcunet:Tinydeeplearningoniotdevices.AdvancesinNeural learningapproach.In2016IEEE-EMBSInternationalConferenceonBiomedical
InformationProcessingSystems33(2020),11711â€“11722. andHealthInformatics(BHI).IEEE,426â€“429.
[33] JiLinetal.2021.Memory-efficientpatch-basedinferencefortinydeeplearning. [63] JunyuanXie,RossGirshick,andAliFarhadi.2016.Unsuperviseddeepembedding
AdvancesinNeuralInformationProcessingSystems34(2021),2346â€“2358. forclusteringanalysis.InInternationalConferenceonMachineLearning.PMLR,
[34] JiLinetal.2022.On-devicetrainingunder256kbmemory.AdvancesinNeural 478â€“487.
InformationProcessingSystems35(2022),22941â€“22954. [64] DaliangXuetal.2022. Mandheling:Mixed-precisionon-devicednntraining
[35] DavidLopez-PazandMarcâ€™AurelioRanzato.2017.Gradientepisodicmemoryfor withdspoffloading.InProceedingsofthe28thAnnualInternationalConferenceon
continuallearning.Advancesinneuralinformationprocessingsystems30(2017). MobileComputingAndNetworking.214â€“227.
[36] MichaelMcCloskeyandNealJCohen.1989.Catastrophicinterferenceincon- [65] WeihongXu,JaeyoungKang,andTajanaRosing.2023.FSL-HD:Accelerating
nectionistnetworks:Thesequentiallearningproblem.InPsychologyoflearning Few-ShotLearningonReRAMusingHyperdimensionalComputing.In2023
andmotivation.Vol.24.Elsevier,109â€“165. Design,Automation&TestinEuropeConference&Exhibition(DATE).IEEE,1â€“6.
[37] MdMohaimenuzzamanetal.2023.EnvironmentalSoundClassificationonthe [66] JuntingZhangetal.2020.Class-incrementallearningviadeepmodelconsolida-
Edge:APipelineforDeepAcousticNetworksonExtremelyResource-Constrained tion.InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputer
Devices.PatternRecognition133(2023),109025. Vision.1131â€“1140.
[38] AliMoinetal.2021. Awearablebiosensingsystemwithin-sensoradaptive [67] YuZhang,TaoGu,andXiZhang.2020. MDLdroidLite:Arelease-and-inhibit
machinelearningforhandgesturerecognition. NatureElectronics4,1(2021), controlapproachtoresource-efficientdeepneuralnetworksonmobiledevices.
54â€“63. InProceedingsofthe18thConferenceonEmbeddedNetworkedSensorSystems.
[39] JamesOâ€™Neill.2020.Anoverviewofneuralnetworkcompression.arXivpreprint 463â€“475.
arXiv:2006.03669(2020).
[40] AndrewNg,MichaelJordan,andYairWeiss.2001.Onspectralclustering:Analysis
andanalgorithm.Advancesinneuralinformationprocessingsystems14(2001).
[41] EvgenyOsipovetal.2022.Hyperseed:Unsupervisedlearningwithvectorsym-
bolicarchitectures.IEEETransactionsonNeuralNetworksandLearningSystems
(2022).
[42] GermanIParisi,RonaldKemker,JoseLPart,ChristopherKanan,andStefan
Wermter.2019. Continuallifelonglearningwithneuralnetworks:Areview.
Neuralnetworks113(2019),54â€“71.
[43] AdamPaszkeetal.2019.Pytorch:Animperativestyle,high-performancedeep
learninglibrary.Advancesinneuralinformationprocessingsystems32(2019).
[44] KarolJPiczak.2015. ESC:Datasetforenvironmentalsoundclassification.In
Proceedingsofthe23rdACMinternationalconferenceonMultimedia.1015â€“1018.
[45] ChristosProfentzas,MagnusAlmgren,andOlafLandsiedel.2022. MiniLearn:
On-DeviceLearningforLow-PowerIoTDevices.InInternationalConferenceon
EmbeddedWirelessSystemsandNetworks.
[46] DushyantRao,FrancescoVisin,AndreiRusu,RazvanPascanu,YeeWhyeTeh,and
RaiaHadsell.2019.Continualunsupervisedrepresentationlearning.Advances
inneuralinformationprocessingsystems32(2019).