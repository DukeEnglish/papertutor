DiG: Scalable and Efficient Diffusion Models with
Gated Linear Attention
LianghuiZhu1,2,‚ãÑ ZilongHuang2(cid:0) BenchengLiao1 JunHaoLiew2 HanshuYan2
JiashiFeng2 XinggangWang1(cid:0)
1SchoolofEIC,HuazhongUniversityofScience&Technology 2ByteDance
Code&Models: hustvl/DiG
Abstract
Diffusionmodelswithlarge-scalepre-traininghaveachievedsignificantsuccess
in the field of visual content generation, particularly exemplified by Diffusion
Transformers(DiT).However,DiTmodelshavefacedchallengeswithscalability
andquadraticcomplexityefficiency. Inthispaper, weaimtoleveragethelong
sequence modeling capability of Gated Linear Attention (GLA) Transformers,
expanding its applicability to diffusion models. We introduce Diffusion Gated
LinearAttentionTransformers(DiG),asimple,adoptablesolutionwithminimal
parameter overhead, following the DiT design, but offering superior efficiency
andeffectiveness. InadditiontobetterperformancethanDiT,DiG-S/2exhibits
2.5√ó higher training speed than DiT-S/2 and saves 75.7% GPU memory at a
resolutionof1792√ó1792. Moreover,weanalyzethescalabilityofDiGacross
avarietyofcomputationalcomplexity. DiGmodels,withincreaseddepth/width
oraugmentationofinputtokens,consistentlyexhibitdecreasingFID.Wefurther
compareDiGwithothersubquadratic-timediffusionmodels. Withthesamemodel
size,DiG-XL/2is4.2√ófasterthantherecentMamba-baseddiffusionmodelata
1024resolution,andis1.8√ófasterthanDiTwithCUDA-optimizedFlashAttention-
2underthe2048resolution. Alltheseresultsdemonstrateitssuperiorefficiency
amongthelatestdiffusionmodels.
1 Introduction
Inrecentyears,diffusionmodels[20,51,4,54]haveemergedaspotentdeepgenerativemodels[46,
21,13]renownedfortheirabilitytogeneratehigh-qualityimages. Theirrapidevolutionhasspurred
extensiveapplicationsacrossvariousfields,includingimage-to-imagegeneration[8,63,62],text-
to-image generation [49, 45, 18, 6], speech synthesis [29, 7], video generation [19, 36, 34], and
3D generation [42, 64, 61]. Concurrent with the rapid development of sampling algorithms [52,
38, 33, 32, 22], the principal techniques have evolved into two main categories based on their
architecturalbackbones: U-Net-basedmethods[20,53]andViT-basedmethods[14]. U-Net-based
approachescontinuetoleveragetheconvolutionalneuralnetwork(CNN)architecture[31,48],whose
hierarchicalfeaturemodelingabilitybenefitsvisualgenerationtasks. Ontheotherhand,ViT-based
methods[60,1,39]innovatebyincorporatingself-attentionmechanisms[56]insteadoftraditional
samplingblocks,resultinginstreamlinedyeteffectiveperformance.
‚ãÑThisworkwasdonewhenLianghuiZhuwasinterningatByteDance.
(cid:0)Correspondingauthors:XinggangWang(xgwang@hust.edu.cn)andZilongHuang(zilong.huang2020@
gmail.com)
Preprint.Underreview.
4202
yaM
82
]VC.sc[
1v82481.5042:viXra80 OOM
6.44 DiT-S/2 DiS-S/2 DiG-S/2 DiT-S/2 DiS-S/2 DiG-S/2
6
65.73
5.88 4.85 60
3.97
4
4.33 3.30 40 38.98
3.57 2.77
2.21
2 2.76 22.16
2.10 1.71 1.21 20
12.10 19.64
1.38 4.20 5.07 7.08 12.5615.95
0 0.78 OOM 0 4.09 4.64 5.88 7.539.90
512 1024 1536 2048 512 1024 1536 2048
Resolution Resolution
(a)SpeedComparison (b)GPUMemoryComparison
Figure1: EfficiencycomparisonamongDiT[39],DiS[16],andourDiGmodel. DiGachieveshigher
trainingspeedwhilecostslowerGPUmemoryindealingwithhigh-resolutionimages. Forexample,
DiGis2.5√ófasterthanDiTandsaves75.7%GPUmemorywitharesolutionof1792√ó1792,i.e.,
12544tokensperimage. Patchsizeforallmodelsis2.
60 9
DiS DiT Flash-DiT DiG DiS Flash-DiT DiG
45
6
30
3
15
0 0
S/2 B/2 L/2 XL/2 S/2 B/2 L/2 XL/2
ModelSize ModelSize
(a)FPSComparisonw/ (b) FPS Comparison w/
image size = 1024 image size = 2048
Figure2: FPScomparisonamongDiS[16],DiT[39],DiTwithFlashAttention-2(Flash-DiT)[11]
andourDiGmodelvaryingfromdifferentmodelsizes.WetakeDiGasabaseline.Witharesolutionof
1024√ó1024,DiGis2.0√ófasterthanDiSatsmallsizewhile4.2√ófasteratXLsize.Furthermore,DiG-
XL/2is1.8√ófasterthanthemostwell-designedhigh-optimizedFlash-DiT-XL/2witharesolutionof
2048√ó2048.
Duetotheirexcellentscalabilityintermsofperformance,ViT-basedmethods[39]havebeenadopted
asbackbonesinthemostadvanceddiffusionworks,includingPixArt[6,5],Sora[3],StableDiffusion
3[15],etc. However,theself-attentionmechanisminViT-basedarchitecturesscalesquadratically
withtheinputsequencelength,makingthemresource-intensivewhendealingwithlongsequence
generationtasks,e.g.,high-resolutionimagegeneration,videogeneration,etc. Recentadvancements
insubquadratic-timemethods,i.e.,Mamba[17],RWKV[40]andGatedLinearAttentionTransformer
(GLA)[59],trytoimprovethelong-sequenceprocessingefficiencybyintegratingRecurrentNeural
Network(RNN)likearchitectureandhardware-awarealgorithms. Amongthem,GLAincorporates
data-dependent gating operation and hardware-efficient implementation to the Linear Attention
Transformer,showingcompetitiveperformancebuthigherthroughput.
MotivatedbythesuccessofGLAinthenaturallanguageprocessingdomain,itisappealingthat
wecantransferthissuccessfromlanguagegenerationtovisualcontentgeneration,i.e.,todesign
ascalableandefficientdiffusionbackbonewiththeadvancedlinearattention[26,10,25]method.
However,visualgenerationwithGLAfacestwochallenges,i.e.,unidirectionalscanningmodeling
andlackoflocalawareness.Toaddressthesechallenges,weproposetheDiffusionGLA(DiG)model,
whichincorporatesalightweightspatialreorient&enhancementmodule(SREM)forlayer-wise
scanningdirectioncontrollingandlocalawareness. Attheendofeachblock,theSREMwillchange
2
elacsgol/wSPF
SPF
retsaF
retsaF
retsaf√ó0.2
retsaf√ó5.2
retsaf√ó2.4
)BG(yromeMUPG
SPF
retsaf√ó5.1
retsaF
rellamS
yromem%7.57-
retsaf√ó8.2the sequence index with efficient matrix operation for different scanning of the next block. The
scanningdirectionscontainfourbasicpatternsandenableeachpatchinsequencestobeawareofother
patchesfollowingcrisscrossdirections. Furthermore,wealsoincorporateadepth-wiseconvolution
(DWConv)[9]intheSREMtoprovidelocalawarenesswithextremelysmallamountsofparameters.
Crucially,thispaperpresentsasystematicablationstudythatincludestheintegrationofanSREM
andthecomprehensiveevaluationofthemodel‚Äôsarchitecture. ItisimportanttohighlightthatDiG
adherestothefirstpracticesoflinearattentiontransformersindiffusiongeneration,renownedfor
theirsuperiorscalabilityandefficiencyinimagegenerationtasks.
ComparedwiththeViT-basedmethod,i.e.,DiT[39],DiGpresentssuperiorperformanceonIma-
geNet[12]generationwiththesamehyper-parameters. Furthermore,DiGismoreefficientinterms
oftrainingspeedandGPUmemoryforhigh-resolutionimagegeneration. Theefficiencyinterms
ofmemoryandspeedempowersDiGtoalleviatetheresourceconstraintproblemoflong-sequence
visual generation tasks. Notably, some Mamba-based subquadratic-time diffusion methods like
DiS[16]oftenshowlowerefficiencyasthemodelsizescalesduetothecomplicatedblockdesign
andinabilitytoefficientlyutilizetheGPUtensorcore,asshowninFig.2. Thankstothestreamlined
yeteffectivedesignofDiGblock,theDiGcankeephighefficiencywithlargermodelsizes,andeven
outperformsthemostwell-designedhigh-optimizedlinearattentionmethod,FlashAttention-2[11],
ataresolutionof1024√ó1024.
Ourmaincontributionscanbesummarizedasfollows:
‚Ä¢ WeproposeDiffusionGLA(DiG),whichincorporatesanefficientDiGblockforbothglobal
visualcontextmodelingthroughlayer-wisescanning,andlocalvisualawareness.Tothebest
ofourknowledge,DiGisthefirstexplorationfordiffusionbackbonewithlinearattention
transformer.
‚Ä¢ Withouttheburdenofquadraticattention,theproposedDiGexhibitshigherefficiencyin
bothtrainingspeedandGPUmemorycostwhilemaintainingasimilarmodelingability
as DiT. Specifically, DiG is 2.5√ó faster than DiT and saves 75.7% GPU memory at the
resolutionof1792√ó1792asshowninFig.1.
‚Ä¢ WeconductextensiveexperimentsontheImageNetdataset. Theresultsdemonstratethat
DiGpresentsscalableabilityandachievessuperiorperformancewhencomparedwithDiT.
DiG is promising to serve as the next-generation backbone for diffusion models in the
contextoflarge-scalelong-sequencegeneration.
2 RelatedWork
2.1 LinearAttentionTransformer
DifferentfromstandardautoregressiveTransformer[57]whichmodelstheglobalattentionmatrix,
theoriginallinearattention[26]isessentiallyalinearRNNwithmatrix-valued-formathiddenstates.
Linear attention introduces a similarity kernel k(x,y) with an associated feature map œï(¬∑), i.e.,
k(x,y)=‚ü®œï(x),œï(y)‚ü©. ThecalculationofoutputO‚ààRL√ód(hereListhesequencelengthanddis
thedimension)canberepresentedasfollows:
(cid:80)t k(Q ,K )V (cid:80)t œï(Q )œï(K )‚ä§V œï(Q )(cid:80)t œï(K )‚ä§V
O = i=1 t i i = i=1 t i i = t i=1 i i, (1)
t (cid:80)t k(Q ,K ) (cid:80)t œï(Q )œï(K )‚ä§ œï(Q )(cid:80)t œï(K )‚ä§
i=1 t i i=1 t i t i=1 i
wherequeryQ,keyK,valueVhaveshapesofL√ódandtistheindexofcurrenttoken.Bydenoting
hidden state S = (cid:80)t œï(K )V and normalizer z = (cid:80)t œï(K )‚ä§ where S ‚àà Rd√ód,z ‚àà
t i=1 i i t i=1 i t t
Rd√ó1,theEq.(1)canberewrittenas:
œï(Q )S
S =S +œï(K )V , z =z +œï(K )‚ä§, O = t t. (2)
t t‚àí1 i i t t‚àí1 i t œï(Q )z
t t
Recent works set œï(¬∑) to be the identity [35, 55] and remove z [43], resulting linear attention
t
Transformerwiththefollowingformat:
S =S +K‚ä§V , O =Q S . (3)
t t‚àí1 t t t t t
DirectlyusingalinearattentionTransformerforvisualgenerationleadstopoorperformancedueto
theunidirectionalmodeling,soweproposealightweightspatialreorient&enhancementmoduleto
takecareofbothmodelingglobalcontextincrisscrossdirectionsandlocalinformation.
32.2 BackbonesinDiffusionModels
Existing diffusion models typically employ U-Net as backbones [20, 47] for image generation.
Recently, VisionTransformer(ViT)-basedbackbones[39,1,6,5,3]receivesignificantattention
due to the scalability of transformer and its natural fit for multi-modal learning. However, ViT-
basedarchitecturessufferfromquadraticcomplexity,limitingtheirpracticabilityinlongsequence
generationtasks, suchashigh-resolutionimagesynthesis, videogenerationetc. Tomitigatethis,
recentworksexploresubquadratic-timeapproachestoefficientlyhandlelongsequences.Forexample,
DiS [16], DiffuSSM [58] and ZigMa [23] employ state-space models as diffusion backbones for
better computation efficiency. Diffusion-RWKV [58] adopt an RWKV architecture in diffusion
modelsforimagegeneration.
Our DiG also follows this line of research, aiming at improving the efficiency of long sequence
processingbyadoptingGatedLinearAttentionTransformer(GLA)asdiffusionbackbones. Our
proposedadaptationmaintainsthefundamentalstructureandbenefitsofGLAwhileintroducinga
fewcrucialmodificationsnecessaryforgeneratinghigh-fidelityvisualdata.
3 Method
3.1 Preliminaries
Gated Linear Attention Transformer. The Gated Linear Attention Transformer (GLA) [59]
combines a data-dependent gating mechanism and linear attention, achieving superior recurrent
modeling performance. Given an input X ‚àà RL√ód (here L is the sequence length and d is the
dimension),GLAcalculatesthequery,key,andvaluevectorsasfollows:
Q=XW
Q
‚ààRL√ódk, K=XW
K
‚ààRL√ódk, V=XW
V
‚ààRL√ódv, (4)
whereW ,W ,andW arelinearprojectionweights. d andd aredimensionnumbers. Next,
Q K V k v
GLAcomputethegatingmatrixGasfollows:
œÉ(XW +b ) œÉ(XW +b )
G
t
=Œ± t‚ä§Œ≤
t
‚ààRdk√ódv, Œ±= œÑŒ± Œ± ‚ààRL√ódk, Œ≤ = œÑŒ≤ Œ≤ ‚ààRL√ódv, (5)
wheretistheindexoftoken,œÉisthesigmoidfunction,bisthebiasterm,andœÑ ‚ààRisatemperature
term. AsshowninFig.3,thefinaloutputY isobtainedasfollows:
t
ùêí
!"#
‚äô ‚äï ùêí
!
S‚Ä≤
t‚àí1
=G t‚äôS
t‚àí1
‚ààRdk√ódv, (6)
‚äó ‚äó ‚äô ùêò ! S t =S‚Ä≤ t‚àí1+K‚ä§ t V t ‚ààRdk√ódv, (7)
O =Q‚ä§S ‚ààR1√ódv, (8)
ùêÜ ùêä ùêï ùêê ùêë t t t
! ! ! ! !
ùêó R =Swish(X W +b )‚ààR1√ódv, (9)
! t t r r
Y =(R ‚äôLN(O ))W ‚ààR1√ód, (10)
Figure3: PipelineofGLA. t t t O
whereSwishistheSwish[44]activationfunction,and‚äôistheelement-wisemultiplicationoperation.
Insubsequentsections,weuseGLA(¬∑)torefertothegatedlinearattentioncomputationforthe
inputsequence.
DiffusionModels. Beforeintroducingtheproposedmethod,weprovideaconcisereviewofsome
basicconceptsaboutdiffusionmodels(DDPM)[20]. TheDDPMtakesnoiseasaninputandsamples
imagesbyiterativedenoisingtheinput. TheforwardprocessofDDPMbeginswithastochastic
processwheretheinitialimagex isgraduallycorruptedbynoiseandisfinallytransformedintoa
0
simpler,noise-dominatedstate. Theforwardnoisingprocesscanberepresentedasfollows:
T
(cid:89)
q(x |x )= q(x |x ), (11)
1:T 0 t t‚àí1
t=1
‚àö
q(x |x )=N(x ; Œ±¬Øx ,(1‚àíŒ±¬Ø)I), (12)
t 0 t t 0 t
wherex isthesequenceofnoisedimagesfromtimet = 1tot = T. Then,DDPMlearnsthe
1:T
reverseprocessthatrecoverstheoriginalimagewithlearned¬µ andŒ£ :
Œ∏ Œ∏
p (x |x )=N(x ;¬µ (x ),Œ£ (x )), (13)
Œ∏ t‚àí1 t t‚àí1 Œ∏ t Œ∏ t
4whereŒ∏aretheparametersofthedenoiser,andaretrainedwiththevariationallowerbound[51]on
theloglikelihoodoftheobserveddatax .
0
(cid:88)
L(Œ∏)=‚àíp(x |x )+ D (q‚àó(x |x ,x )‚à•p (x |x )),
0 1 KL t‚àí1 t 0 Œ∏ t‚àí1 t (14)
t
whereListhefullloss. TofurthersimplifythetrainingprocessofDDPM,researchersreparameterize
¬µ asanoisepredictionnetworkœµ andminimizethemeansquarederrorlossL betweenœµ (x )
Œ∏ Œ∏ simple Œ∏ t
andthetrueGaussiannoiseœµ :
t
L (Œ∏)=‚à•œµ (x )‚àíœµ ‚à•2. (15)
simple Œ∏ t t 2
However,totrainadiffusionmodelthatcanlearnavariablereverseprocesscovarianceŒ£ ,weneed
Œ∏
tooptimizethefullD term. Inthispaper,wefollowDiT[39]totrainthenetworkwhereweuse
KL
thesimplelossL totrainthenoisepredictionnetworkœµ andusethefulllossLtotrainthe
simple Œ∏
covariancepredictionnetworkŒ£ . Afterthetrainingprocess, wefollowthestochasticsampling
Œ∏
processtogenerateimagesfromthelearnedœµ andŒ£ .
Œ∏ Œ∏
3.2 DiffusionGLA
WepresentDiffusionGLA(DiG),anewarchitecturefordiffusiongeneration. Ourgoalistobeas
faithfultothestandardGLAarchitectureaspossibletoretainitsscalingabilityandhigh-efficiency
properties. AnoverviewoftheproposedGLAisshowninFig.3. ThestandardGLAisdesignedfor
thecausallanguagemodelingof1-Dsequences. ToprocesstheDDPMtrainingofimages,wefollow
someofthebestpracticesofpreviousvisiontransformerarchitectures[14,39]. DiGfirsttakesa
spatialrepresentationzoutputbytheVAEencoder[27,47]asinput. Foran256√ó256√ó3imageto
VAEencoder,theshapeofspatialrepresentationzis32√ó32√ó4. DiGsubsequentlyconvertsthe
spatialinputintoatokensequencez ‚ààRT√ó(P2¬∑C)throughthepatchifylayer,whereT islengthof
p
tokensequence,C isthenumberofspatialrepresentationchannels,P isthesizeofimagepatches,
andhalvingP willquadrupleT. Next,welinearlyprojectthez tothevectorwithdimensionDand
p
addfrequency-basedpositionalembeddingsE ‚ààRT√óD toallprojectedtokens,asfollows:
pos
z =[z1W;z2W;¬∑¬∑¬∑ ;zTW]+E , (16)
0 p p p pos
where zt is the t-th patch of z , W ‚àà R(P2¬∑C)√óD is the learnable projection matrix. As for
p p
conditionalinformationsuchasnoisetimestepst‚ààR,andclasslabelsy ‚ààR,weadoptmulti-layer
perception(MLP)andembeddinglayerastimestepembedderandlabelembedder,respectively.
t=MLP(t), y=Embed(y), (17)
wheret ‚àà R1√óD istimeembeddingandy ‚àà R1√óD islabelembedding. Wethensendthetoken
sequence(z )tothel-thlayeroftheDiGencoder,andgettheoutputz . Finally,wenormalizethe
l‚àí1 l
outputtokensequencez ,andfeedittothelinearprojectionheadtogetthefinalpredictednoise
L
pÀÜ andpredictedcovariancepÀÜ ,asfollows:
noise covariance
z =DiG (z ,t,y), z =Norm(z ), pÀÜ ,pÀÜ =Linear(z ), (18)
l l l‚àí1 n L noise covariance n
whereDiG isthel-thdiffusionGLAblock,Listhenumberoflayers,andNormisthenormaliza-
l
tionlayer. ThepÀÜ andpÀÜ havethesameshapeastheinputspatialrepresentation,i.e.,
noise covariance
32√ó32√ó4.
3.3 DiGBlock
TheoriginalGLAblockprocessinputsequencewitharecurrentformat,whichonlyenablescausal
modelingfor1-Dsequence. Inthissection,weintroducetheDiGblock,whichincorporatesaspatial
reorient&enhancementmodule(SREM)thatenableslightweightspatialrecognitionandcontrols
layer-wisescanningdirections. TheDiGblockisshowninFig.4.
Specifically, wepresenttheforwardprocessofDiGblockinAlgo.1. Followingthewidespread
usageofadaptivenormalizationlayers[41]inGANs[2,24]anddiffusionmodels[13,39],weadd
andnormalizetheinputtimestepembeddingtandlabelembeddingytoregressthescaleparameter
Œ±,Œ≥,andshiftparameterŒ≤. Next,welaunchgatedlinearattention(GLA)andfeedforwardnetwork
(FFN)withtheadjustmentofregressedadaptivelayernorm(adaLN)parameters. Then,wereshape
5Spatial Reorient & Enhancement Module
1 2 3
SREM
4 5 6
‚äï
ùõº 7 8 9
Scale "
9 8 7
Feedforward
Noise Œ£
32x32x4 32x32x4
Œ≥",ùõΩ" 6 5 4
Scale&Shift
LinearandReshape 3 2 1
RMSNorm
RMSNorm ‚äï 1 4 7
ùõº
N√ó Scale ! 2 5 8
DiGBlock
Gated Linear 3 6 9
Attention
Œ≥ ,ùõΩ
Patchify Embed Scale&Shift ! ! 7 6 3
RMSNorm MLP 8 5 2
Noised Timestepùë°
Latent
InputTokens Conditioning 9 4 1
32x32x4 Labelùë¶
Latent DiG DiGBlock DiGScanning Directions
Figure 4: The overview of the proposed DiG model. The figure presents the whole Latent DiG,
DiGblock,detailsofspatialreorient&enhancementmodule(SREM),andlayer-wiseDiGscanning
directionscontrolledbytheSREM.Wemarkthescanningorderandindicesoneachpatch.
thesequenceto2Dandlaunchalightweight3√ó3depth-wiseconvolution(DWConv2d)layerto
perceivelocalspatialinformation. Specifically,usingtraditionalinitializationforDWConv2dleadsto
slowconvergencebecauseconvolutionalweightsaredispersedaround. Toaddressthisproblem,we
proposeidentityinitializationthatonlysetstheconvolutionalkernelcenteras1,andthesurroundings
to0. Last, wetransposethe2Dtokenmatrixeverytwoblocksandfliptheflattenedsequenceto
controlthescanningdirectionsofthenextblock. AsshownintherightpartofFig.4,eachlayeronly
processesscanninginonedirection.
3.4 ArchitectureDetails
WeuseatotalofN DiGblocks,eachoperatingatthehiddendimensionsizeD. Followingprevious
works[39,14,59],weusestandardtransformerconfigsthatscalesN,D,andattentionheadsnumber.
Specifically,weprovidefourconfigs: DiG-S,DiG-B,DiG-L,andDiG-XL,asshownintheTab.1.
Theycoverawiderangeofparametersandflopallocations,from31.5Mto644.6Mand1.09Gflops
to22.53Gflops,presentingawaytogaugethescalingperformanceandefficiency. Notably,DiG
onlyconsume77.0%to78.9%Gflopswhencomparedwiththesamesizebaselinemodels,i.e.,DiTs.
3.5 EfficiencyAnalysis
GPUcontainstwoimportantcomponents,i.e.,highbandwidthmemory(HBM)andSRAM.HBMhas
abiggermemorysizebutSRAMhasalargerbandwidth. TomakefulluseofSRAMandmodeling
sequencesinaparallelform,wefollowGLAtosplitawholesequenceintomanychunksthatcan
6
Reshape
2D
DW
Conv
Transpose
Flatten FlipAlgorithm1:DiGBlockProcess.
Input:tokensequencez :(B,T,D),timestepembedt:(B,1,D),labelembedy:(B,1,D)
l‚àí1
Output:tokensequencez :(B,T,D)
l
1 Œ± 1,Œ≤ 1,Œ≥ 1,Œ± 2,Œ≤ 2,Œ≥ 2:(B,1,D)‚ÜêMLP(t+y)//regressparametersofadaLN
2 z‚Ä≤ l‚àí1 :(B,T,D)‚Üêz l‚àí1+Œ± 1‚äôGLA(Norm(z l‚àí1)‚äô(1+Œ≥ 1)+Œ≤ 1))
3 z‚Ä≤ l‚Ä≤ ‚àí1 :(B,T,D)‚Üêz‚Ä≤ l‚àí1+Œ± 2‚äôFFN(Norm(z‚Ä≤ l‚àí1)‚äô(1+Œ≥ 2)+Œ≤ 2))
‚àö ‚àö
4 z‚Ä≤ l‚Ä≤ ‚àí1 :(B, T, T,D)‚ÜêDWConv2d(reshape2d(z‚Ä≤ l‚Ä≤ ‚àí1))//lightweightspatialmodeling
5 ifl%2==0then
‚àö ‚àö
6 z‚Ä≤ l‚Ä≤ ‚àí1 :(B, T, T,D)‚Üêtranspose(z‚Ä≤ l‚Ä≤ ‚àí1)//transposethetokenmatrixeverytwoblock
7 end
8 z l :(B,T,D)‚Üêflip(flatten(z‚Ä≤ l‚Ä≤ ‚àí1))//fliptokensequenceateachendofblock
9 Return:z l
Table1: DetailsofDiGmodels. WefollowDiT[39]modelconfigurationsfortheSmall(S),Base
(B),Large(L),andXLarge(XL)variants. GivenI =32,p=4.
Model LayersN HiddenSizeD Heads Parameters(M) Gflops Gflops DiG
Gflops
DiT
DiG-S 12 384 6 31.5 1.09 77.9%
DiG-B 12 768 12 124.6 4.31 77.0%
DiG-L 24 1024 16 443.4 15.54 78.9%
DiG-XL 28 1152 16 644.6 22.53 77.4%
completecalculationsonSRAM.WedenotethechunksizeasM,thetrainingcomplexityisthus
O(T (M2D+MD2))=O(TMD+TD2),whichislessthanthetraditionalattention‚Äôscomplexity
M
O(T2D)whenT >D. Furthermore,thelightweightDWConv2dandefficientmatrixoperationsin
DiGblockalsoguaranteetheefficiencyasshowninFig.1andFig.2.
4 Experiment
4.1 ExperimentalSettings
Datasetsandmetrics. Followingpreviousworks[39],weuseImageNet[12]forclass-conditional
imagegenerationlearningataresolutionof256√ó256. TheImageNetdatasetcontains1,281,167
trainingimagesvaryingfrom1,000differentclasses. Weusethehorizontalflipsasthedataaugmenta-
tion. WemeasurethegenerationperformancewithFrechetInceptionDistance(FID)[37],Inception
Score[50],sFID[37],andPrecision/Recall[30].
Implementationdetails. WeusetheAdamWoptimizerwithaconstantlearningrateof1e‚àí4.
Followingthepreviousworks[39],weutilizetheexponentialmovingaverage(EMA)ofDiGweights
duringtrainingwithadecayrateof0.9999. WegenerateallimageswiththeEMAmodel. Forthe
trainingofImageNet,weuseanoff-the-shelfpretrainedvariationalautoencoder(VAE)[46,28].
4.2 ModelAnalysis
Effectofspatialreorient&enhancementmodule. AsshowninTab.2,weanalyzetheeffective-
nessoftheproposedspatialreorient&enhancementmodule(SREM).WetaketheDiT-S/2asour
baselinemethod. ThenaiveDiGwithonlythecausalmodelinghassignificantlyfewerflopsand
parameters,butalsopoorFIDperformanceduetothelackofglobalcontext. Wefirstaddthebidirec-
tionalscanningtoDiGandobservesignificantimprovement,i.e.,69.28FID,whichdemonstratesthe
importanceofglobalcontext. ExperimentwithoutidentityinitializationforDWConv2d,i.e.,thehalf
rightandhalfwrongsymbol,leadstoworseFID,whiletheDWConv2dwithidentityinitialization
canimproveperformancealot. TheexperimentswithDWConv2dprovetheimportanceofidentity
initializationandlocalawareness. TheexperimentinthelastrowshowsthatthefullSREMcanbring
thebestperformance,takingcareofbothlocalinformationandglobalcontext.
7Table2: AblationoftheproposedSpatialReorient&EnhancementModule(SREM).Wevalidatethe
effectivenessofeachSREMcomponentandusethesamehyperparametersforallmodels. The‚Äúhalf
rightandhalfwrongsymbol‚ÄùmeansuseDWConv2dwithouttheproposedidentityinitialization.
SpatialReorient&EnhancementModule
Model Flops(G) Params(M) FID-50K
Bidirectional DWConv2d Crisscross
BaselineMethod.
DiT-S/2 6.06 33.0 68.4
Ours.
DiG-S/2 4.29 33.0 175.84
DiG-S/2 ‚úî 4.29 33.0 69.28
DiG-S/2 ‚úî ‚úî‚úó 4.30 33.1 96.83
DiG-S/2 ‚úî ‚úî 4.30 33.1 63.84
DiG-S/2 ‚úî ‚úî ‚úî 4.30 33.1 62.06
120 200
S/2 B/2 L/2 XL/2 DiG-S/8 DiG-S/4 DiG-S/2
100
160
80
120
60
80
40
40
20
0 0
100K 200K 300K 400K 100K 200K 300K 400K
Iteration Iteration
(a)ScalingDiGw/ModelSize (b) Scaling DiGw/ Patch Size
Figure5: ThescalinganalysiswithDiGmodelsizesandpatchsizes.
Scalingmodelsize. WeinvestigatethescalingabilityofDiGamongfourdifferentmodelscaleson
theImageNetdataset. AsdepictedinFig.5(a),theperformanceimprovesasthemodelsscalefrom
S/2toXL/2. TheresultsdemonstratethescalingabilityofDiG,indicatingitspotentialasalarge
foundationaldiffusionmodel.
Effectofpatchsize. WetrainDiG-Swithpatchsizevaryingfrom2,4,and8ontheImageNet
dataset.AsshowninFig.5(b),discernibleFIDenhancementscanbeobservedthroughoutthetraining
processbyaugmentingthepatchsizesofDiG.Consequently,optimalperformancenecessitatesa
smallerpatchsizeandlongersequencelength. WhiletheDiGismoreefficientindealingwiththe
long-sequencegenerationtaskswhencomparedtoDiT[39]baseline.
4.3 MainResults
WemainlycomparetheproposedDiGwithourbaselinemethod,DiT[39],withthesamehyperpa-
rameters. TheproposedDiGoutperformsDiTamongfourmodelscaleswith400Ktrainingiterations.
Furthermore,theDiG-XL/2-1200Kwithclassifier-freeguidancealsopresentscompetitiveresults
whencomparedwithpreviousstate-of-the-artmethods.
4.4 CaseStudy
Fig. 6 showcases a selection of samples from DiG-XL/2 that trained with ImageNet dataset at a
resolutionof256√ó256. Theresultsdemonstratecorrectsemanticandaccuratespatialrelationships.
8
K05-DIF K05-DIFTable3: Benchmarkingclass-conditionalimagegenerationonImageNet256√ó256. DiGmodels
adoptthesamehyperparametersasDiT[39]forfaircomparison. Wemarkthebestresultsinbold.
Model FID‚Üì sFID‚Üì IS‚Üë Precision‚Üë Recall‚Üë
Previousstate-of-the-artdiffusionmethods.
ADM[13] 10.94 6.02 100.98 0.69 0.63
ADM-U 7.49 5.13 127.49 0.72 0.63
ADM-G 4.59 5.25 186.70 0.82 0.52
ADM-G,ADM-U 3.94 6.14 215.84 0.83 0.53
CDM[21] 4.88 - 158.71 - -
LDM-8[46] 15.51 - 79.03 0.65 0.63
LDM-8-G 7.76 - 209.52 0.84 0.35
LDM-4-G(cfg=1.25) 3.95 - 178.22 0.81 0.55
LDM-4-G(cfg=1.50) 3.60 - 247.67 0.87 0.48
BaselinesandOurs.
DiT-S/2-400K[39] 68.40 - - - -
DiG-S/2-400K 62.06 11.77 22.81 0.39 0.56
DiT-B/2-400K 43.47 - - - -
DiG-B/2-400K 39.50 8.50 37.21 0.51 0.63
DiT-L/2-400K 23.33 - - - -
DiG-L/2-400K 22.90 6.91 59.87 0.60 0.64
DiT-XL/2-400K 19.47 - - - -
DiG-XL/2-400K 18.53 6.06 68.53 0.63 0.64
DiG-XL/2-1200K 11.96 7.39 106.65 0.65 0.67
DiG-XL/2-1200K(cfg=1.5) 2.84 5.47 250.36 0.82 0.56
Figure6: ImageresultsgeneratedfromtheproposedDiG-XL/2model.
5 Conclusion
Inthiswork,wepresentDiG,acost-effectivealternativetothevanillaTransformerfordiffusion
modelsinimagegenerationtasks. Inparticular,DiGexploresGatedLinearAttentionTransformers
(GLA), attaining superior efficiency and effectiveness in long-sequence image generation tasks.
Experimentally,DiGshowscomparableperformancetopriordiffusionmodelsonclass-conditional
ImageNetbenchmarkswhilesignificantlyreducingthecomputationalburden. Wehopethisworkcan
openupthepossibilityforotherlong-sequencegenerationtasks,suchasvideoandaudiomodeling.
Limitations. AlthoughDiGshowssuperiorefficiencyindiffusionimagegeneration,buildinga
largefoundationmodellikeSora[3]uponDiGisstillanareathatneedstobeexploredfurther.
9References
[1] FanBao,ShenNie,KaiwenXue,YueCao,ChongxuanLi,HangSu,andJunZhu. Allareworthwords:A
vitbackbonefordiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages22669‚Äì22679,2023.
[2] AndrewBrock,JeffDonahue,andKarenSimonyan. Largescalegantrainingforhighfidelitynatural
imagesynthesis. arXivpreprintarXiv:1809.11096,2018.
[3] TimBrooks,BillPeebles,ConnorHolmes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,JoeTaylor,
TroyLuhman,EricLuhman,ClarenceNg,RickyWang,andAdityaRamesh. Videogenerationmodelsas
worldsimulators. 2024.
[4] HanqunCao,ChengTan,ZhangyangGao,YilunXu,GuangyongChen,Pheng-AnnHeng,andStanZLi.
Asurveyongenerativediffusionmodels. IEEETransactionsonKnowledgeandDataEngineering,2024.
[5] JunsongChen,ChongjianGe,EnzeXie,YueWu,LeweiYao,XiaozheRen,ZhongdaoWang,PingLuo,
HuchuanLu,andZhenguoLi. Pixart-sigma: Weak-to-strongtrainingofdiffusiontransformerfor4k
text-to-imagegeneration. arXivpreprintarXiv:2403.04692,2024.
[6] JunsongChen, JinchengYu, ChongjianGe, LeweiYao, EnzeXie, YueWu, ZhongdaoWang, James
Kwok,PingLuo,HuchuanLu,etal. Pixart-alpha:Fasttrainingofdiffusiontransformerforphotorealistic
text-to-imagesynthesis. arXivpreprintarXiv:2310.00426,2023.
[7] NanxinChen,YuZhang,HeigaZen,RonJWeiss,MohammadNorouzi,andWilliamChan. Wavegrad:
Estimatinggradientsforwaveformgeneration. arXivpreprintarXiv:2009.00713,2020.
[8] JooyoungChoi,SungwonKim,YonghyunJeong,YoungjuneGwon,andSungrohYoon.Ilvr:Conditioning
methodfordenoisingdiffusionprobabilisticmodels. arXivpreprintarXiv:2108.02938,2021.
[9] Fran√ßoisChollet. Xception:Deeplearningwithdepthwiseseparableconvolutions. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pages1251‚Äì1258,2017.
[10] KrzysztofChoromanski,ValeriiLikhosherstov,DavidDohan,XingyouSong,AndreeaGane,TamasSarlos,
PeterHawkins,JaredDavis,AfrozMohiuddin,LukaszKaiser,etal. Rethinkingattentionwithperformers.
arXivpreprintarXiv:2009.14794,2020.
[11] TriDao. Flashattention-2:Fasterattentionwithbetterparallelismandworkpartitioning. arXivpreprint
arXiv:2307.08691,2023.
[12] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248‚Äì255.
Ieee,2009.
[13] PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advancesin
neuralinformationprocessingsystems,34:8780‚Äì8794,2021.
[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.Animageisworth
16x16words:Transformersforimagerecognitionatscale. arXivpreprintarXiv:2010.11929,2020.
[15] PatrickEsser,SumithKulal,AndreasBlattmann,RahimEntezari,JonasM√ºller,HarrySaini,YamLevi,
DominikLorenz,AxelSauer,FredericBoesel,etal. Scalingrectifiedflowtransformersforhigh-resolution
imagesynthesis. arXivpreprintarXiv:2403.03206,2024.
[16] ZhengcongFei,MingyuanFan,ChangqianYu,andJunshiHuang. Scalablediffusionmodelswithstate
spacebackbone. arXivpreprintarXiv:2402.05608,2024.
[17] AlbertGuandTriDao.Mamba:Linear-timesequencemodelingwithselectivestatespaces.arXivpreprint
arXiv:2312.00752,2023.
[18] ShuyangGu,DongChen,JianminBao,FangWen,BoZhang,DongdongChen,LuYuan,andBainingGuo.
Vectorquantizeddiffusionmodelfortext-to-imagesynthesis. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages10696‚Äì10706,2022.
[19] JonathanHo, WilliamChan, ChitwanSaharia, JayWhang, RuiqiGao, AlexeyGritsenko, DiederikP
Kingma, BenPoole, MohammadNorouzi, DavidJFleet, etal. Imagenvideo: Highdefinitionvideo
generationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022.
10[20] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesinneural
informationprocessingsystems,33:6840‚Äì6851,2020.
[21] JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet,MohammadNorouzi,andTimSalimans.
Cascadeddiffusionmodelsforhighfidelityimagegeneration. JournalofMachineLearningResearch,
23(47):1‚Äì33,2022.
[22] JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,
2022.
[23] VincentTaoHu,StefanAndreasBaumann,MingGui,OlgaGrebenkova,PingchuanMa,JohannesFischer,
andBjornOmmer. Zigma:Zigzagmambadiffusionmodel. arXivpreprintarXiv:2403.13802,2024.
[24] TeroKarras,SamuliLaine,andTimoAila. Astyle-basedgeneratorarchitectureforgenerativeadversarial
networks. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
4401‚Äì4410,2019.
[25] JungoKasai,HaoPeng,YizheZhang,DaniYogatama,GabrielIlharco,NikolaosPappas,YiMao,Weizhu
Chen,andNoahASmith. Finetuningpretrainedtransformersintornns. arXivpreprintarXiv:2103.13076,
2021.
[26] AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFran√ßoisFleuret. Transformersarernns:Fast
autoregressivetransformerswithlinearattention. InInternationalconferenceonmachinelearning,pages
5156‚Äì5165.PMLR,2020.
[27] DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,
2013.
[28] DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,
2013.
[29] ZhifengKong,WeiPing,JiajiHuang,KexinZhao,andBryanCatanzaro. Diffwave:Aversatilediffusion
modelforaudiosynthesis. arXivpreprintarXiv:2009.09761,2020.
[30] TuomasKynk√§√§nniemi,TeroKarras,SamuliLaine,JaakkoLehtinen,andTimoAila. Improvedprecision
andrecallmetricforassessinggenerativemodels. Advancesinneuralinformationprocessingsystems,32,
2019.
[31] Yann LeCun, L√©on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
documentrecognition. ProceedingsoftheIEEE,86(11):2278‚Äì2324,1998.
[32] LupingLiu,YiRen,ZhijieLin,andZhouZhao. Pseudonumericalmethodsfordiffusionmodelson
manifolds. arXivpreprintarXiv:2202.09778,2022.
[33] ChengLu,YuhaoZhou,FanBao,JianfeiChen,ChongxuanLi,andJunZhu. Dpm-solver: Afastode
solverfordiffusionprobabilisticmodelsamplinginaround10steps. AdvancesinNeuralInformation
ProcessingSystems,35:5775‚Äì5787,2022.
[34] XinMa,YaohuiWang,GengyunJia,XinyuanChen,ZiweiLiu,Yuan-FangLi,CunjianChen,andYuQiao.
Latte:Latentdiffusiontransformerforvideogeneration. arXivpreprintarXiv:2401.03048,2024.
[35] HuanruHenryMao. Fine-tuningpre-trainedtransformersintodecayingfastweights. arXivpreprint
arXiv:2210.04243,2022.
[36] Kangfu Mei and Vishal Patel. Vidm: Video implicit diffusion models. In Proceedings of the AAAI
ConferenceonArtificialIntelligence,volume37,pages9117‚Äì9125,2023.
[37] CharlieNash,JacobMenick,SanderDieleman,andPeterWBattaglia. Generatingimageswithsparse
representations. arXivpreprintarXiv:2103.03841,2021.
[38] AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilisticmodels. In
Internationalconferenceonmachinelearning,pages8162‚Äì8171.PMLR,2021.
[39] WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages4195‚Äì4205,2023.
[40] BoPeng,EricAlcaide,QuentinAnthony,AlonAlbalak,SamuelArcadinho,HuanqiCao,XinCheng,
MichaelChung,MatteoGrella,KranthiKiranGV,etal. Rwkv:Reinventingrnnsforthetransformerera.
arXivpreprintarXiv:2305.13048,2023.
11[41] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual
reasoningwithageneralconditioninglayer.InProceedingsoftheAAAIconferenceonartificialintelligence,
volume32,2018.
[42] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall.Dreamfusion:Text-to-3dusing2ddiffusion.
arXivpreprintarXiv:2209.14988,2022.
[43] ZhenQin,DongLi,WeigaoSun,WeixuanSun,XuyangShen,XiaodongHan,YunshenWei,BaohongLv,
FeiYuan,XiaoLuo,etal.Scalingtransnormerto175billionparameters.arXivpreprintarXiv:2307.14995,
2023.
[44] PrajitRamachandran,BarretZoph,andQuocV.Le. Searchingforactivationfunctions,2017.
[45] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.Hierarchicaltext-conditional
imagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,2022.
[46] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684‚Äì10695,2022.
[47] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684‚Äì10695,2022.
[48] OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedical
imagesegmentation.InMedicalimagecomputingandcomputer-assistedintervention‚ÄìMICCAI2015:18th
internationalconference,Munich,Germany,October5-9,2015,proceedings,partIII18,pages234‚Äì241.
Springer,2015.
[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistictext-to-
imagediffusionmodelswithdeeplanguageunderstanding. Advancesinneuralinformationprocessing
systems,35:36479‚Äì36494,2022.
[50] TimSalimans,IanGoodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen. Improved
techniquesfortraininggans. Advancesinneuralinformationprocessingsystems,29,2016.
[51] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learningusingnonequilibriumthermodynamics. InInternationalconferenceonmachinelearning,pages
2256‚Äì2265.PMLR,2015.
[52] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. arXivpreprint
arXiv:2010.02502,2020.
[53] YangSongandStefanoErmon. Generativemodelingbyestimatinggradientsofthedatadistribution.
Advancesinneuralinformationprocessingsystems,32,2019.
[54] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
arXiv:2011.13456,2020.
[55] YutaoSun,LiDong,ShaohanHuang,ShumingMa,YuqingXia,JilongXue,JianyongWang,andFuruWei.
Retentivenetwork:Asuccessortotransformerforlargelanguagemodels.arXivpreprintarXiv:2307.08621,
2023.
[56] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,≈Åukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,
30,2017.
[57] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,≈Åukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,
30,2017.
[58] JingNathanYan,JiataoGu,andAlexanderMRush. Diffusionmodelswithoutattention. arXivpreprint
arXiv:2311.18257,2023.
[59] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention
transformerswithhardware-efficienttraining. arXivpreprintarXiv:2312.06635,2023.
12[60] XiulongYang,Sheng-MinShih,YinlinFu,XiaotingZhao,andShihaoJi. Yourvitissecretlyahybrid
discriminative-generativediffusionmodel. arXivpreprintarXiv:2208.07791,2022.
[61] TaoranYi,JieminFang,GuanjunWu,LingxiXie,XiaopengZhang,WenyuLiu,QiTian,andXinggang
Wang. Gaussiandreamer:Fastgenerationfromtextto3dgaussiansplattingwithpointcloudpriors. arXiv
preprintarXiv:2310.08529,2023.
[62] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-imagediffusion
models. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages3836‚Äì3847,
2023.
[63] Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde: Unpaired image-to-image translation via
energy-guidedstochasticdifferentialequations. AdvancesinNeuralInformationProcessingSystems,
35:3609‚Äì3623,2022.
[64] ZhizhuoZhouandShubhamTulsiani. Sparsefusion:Distillingview-conditioneddiffusionfor3drecon-
struction.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
12588‚Äì12597,2023.
13