Spatial-Temporal Large Language Model for Traffic Prediction
ChenxiLiu1, SunYang2, QianxiongXu1, ZhishuaiLi3, ChengLong1, ZiyueLi4, RuiZhao3
1S-Lab,NanyangTechnologicalUniversity
2PekingUniversity
3SenseTimeResearch
4InformationSystemDepartment,UniversityofCologne
{chenxi.liu,qianxiong.xu,c.long}@ntu.edu.sg,{lizhishuai,zhaorui}@sensetime.com,
2201210484@stu.pku.edu.cn,zlibn@wiso.uni-koeln.de
Abstract Theevolutionoftrafficpredictionhasseenashiftfromtra-
ditionaltimeseriesmodelstodeeplearningtechniques[Cam-
Traffic prediction, a critical component for intel-
pos et al., 2023; Chang et al., 2023; Yuan et al., 2018;
ligent transportation systems, endeavors to fore-
Kumar and Vanajakshi, 2015]. Initially, time series mod-
see future traffic at specific locations using his-
els such as the Autoregressive Integrated Moving Average
torical data. Although existing traffic prediction
andKalmanFilterwereadaptedfortheirfitwithtimeseries
models often emphasize developing complex neu-
data. However, these models are not good at capturing the
ral network structures, their accuracy has not seen
spatial-temporal dependencies within traffic data, leading to
improvements accordingly. Recently, Large Lan-
deep learning solutions using convolutional neural networks
guageModels(LLMs)haveshownoutstandingca-
(CNNs)forspatialandrecurrentneuralnetworks(RNNs)for pabilities in time series analysis. Differing from
temporal dependencies [Yin et al., 2021; Shen et al., 2018;
existing models, LLMs progress mainly through
Yuan et al., 2018]. Despite these advancements, the non-
parameter expansion and extensive pre-training
Euclidean spatial structure and the complex periodicity of
whilemaintainingtheirfundamentalstructures. In
traffic data present challenges for CNNs and RNNs in cap-
this paper, we propose a Spatial-Temporal Large
turingspatialandtemporaldependencieswell.
Language Model (ST-LLM) for traffic prediction.
Graphconvolutionalnetwork(GCN)basedmodelsgained
Specifically, ST-LLM redefines the timesteps at
popularity for their ability to model local spatial dependen-
each location as tokens and incorporates a spatial-
cies [Li et al., 2023; Bai et al., 2020; Bai et al., 2020;
temporalembeddingmoduletolearnthespatiallo-
Wu et al., 2019; Li et al., 2018; Yu et al., 2018]. How-
cation and global temporal representations of to-
ever,thesemodelsoftenencounteredover-smoothingissues,
kens. Then these representations are fused to pro-
which limits their ability to capture global spatial patterns.
vide each token with unified spatial and temporal
Thisshortcomingpromptedashifttoattention-basedmodels,
information. Furthermore,weproposeanovelpar-
which effectively model dynamic spatial correlations with-
tiallyfrozenattentionstrategyoftheLLM,whichis
out depending on an adjacency matrix [Jiang et al., 2023;
designed to capture spatial-temporal dependencies
Guo et al., 2022; Lin et al., 2020]. These attention-based
for traffic prediction. Comprehensive experiments
approacheshavesinceemergedasaleadingtrend,offeringa
onrealtrafficdatasetsofferevidencethatST-LLM
superiorabilityofhandlingspatial-temporaldependenciesin
outperforms state-of-the-art models. Notably, the
trafficprediction[Zhengetal.,2020;Guoetal.,2019]. Nev-
ST-LLM also exhibits robust performance in both
ertheless,withthisevolution,thestructuresofexistingtraffic
few-shotandzero-shotpredictionscenarios.
predictionmodelshavebecomeprogressivelycomplex.
Foundation models, including large language models
1 Introduction
(LLMs), have advancements in fields such as computer vi-
Traffic prediction, which aims to predict future traffic fea- sion [Ko et al., 2023; Yu et al., 2023] and natural language
tureslikeflowandspeedatspecificlocationsusinghistorical processing [Ramezani and Xu, 2023; Maynez et al., 2023].
data,isacrucialcomponentforintelligenttransportationsys- More recently, LLMs also have shown superb performance
tems [Li et al., 2023; Jiang et al., 2023; Chang et al., 2023; on time series analysis [Cao et al., 2023; Jin et al., 2023b].
Lietal.,2023;Gongetal.,2023]. Thispredictionisinstru- Compared with the complex designs of existing predictive
mentalinoptimizingtrafficmanagement[Miaoetal., 2024; models, LLMs primarily evolve through the expansion of
Zhou et al., 2024] and scheduling public transportation [Jin parameters and pre-training while maintaining their founda-
et al., 2023a; Wang et al., 2020]. For instance, predicting tionalmodelstructure.ExistingLLM-basedpredictionmeth-
vehicle speed aids the transportation department in optimiz- ods tend to focus on the temporal aspect of data in the traf-
ingtrafficmanagement. Similarly, trafficflowforecastingis fic prediction tasks [Zhou et al., 2023; Jin et al., 2023b;
vital for vehicle companies, as it enables them to efficiently Caoetal.,2023]andoftenoverlookthespatialaspect. How-
allocateandschedulevehiclestosatisfyexpecteddemand. ever, in traffic prediction, the spatial variables are strongly
4202
naJ
81
]GL.sc[
1v43101.1042:viXracorrelatedandthespatialdimensionalsoprovestobeimpor- 2 RelatedWork
tant[LablackandShen, 2023;Wenetal., 2023]. Forexam-
ple,acommonproblemsettingistousetrafficdatafromthe 2.1 LargeLanguageModelsforTimeSeries
previoustwelvetimestepstopredicttrafficforthenexttwelve
Analysis
timestepsathundredsofspatiallocations[Lietal.,2018]-in
this case, more spatial data than temporal data can be lever-
Recentlylargelanguagemodels(LLMs)haveshownsuperb
aged. Inourstudy, werefinethetimestepsofaspatialloca-
performance on time series analysis tasks, such as predic-
tion as a token and model the global temporal dependencies
tion [Cao et al., 2023], classification [Jin et al., 2023b],
acrossallthesetokenssoastoemphasizethespatialaspects.
anomaly detection [Zhou et al., 2023], imputation [Chen et
Moreover, LLMs are notable for their ability to transfer al., 2023], few-shot learning [Sun et al., 2023], and zero-
knowledge across domains [Rasul et al., 2023; Nate Gruver shotlearning[NateGruverandWilson,2023]. Forinstance,
andWilson,2023],suchasthepretrainedtransformer(FPT)
TEMPO-GPT combines prompt engineering and seasonal
LLM[Zhouetal.,2023]. WhiletheFPTLLMiseffectivein trenddecompositioninthegenerativepre-trainedtransformer
timeseriesanalysistasks,itshowslessoptimalperformance (GPT) [Cao et al., 2023]. TIME-LLM reprograms an LLM
inlong-termpredictiontasksliketrafficprediction. Thepos- fortimeseriesforecasting,andthebackbonelanguagemodel
sible reason is that FPT struggles to bridge the domain gap remains intact [Jin et al., 2023b]. OFA employs a frozen
between language data and traffic data. To fill this gap, we GPT2 model across various key tasks in time series analy-
proposeapartiallyfrozenattention(PFA)LLM,specifically sis [Zhou et al., 2023], the authors conclude that the LLM
designedtoenhancepredictionaccuracyintrafficprediction. performsbetterontasksoftimeseriesanalysis. TESTgener-
By partially freezing the multi-head attention, the LLM can ates various embedding for time series tokens and executes
adapt to traffic prediction while preserving the foundational time series forecasting and classification tasks [Sun et al.,
knowledgeacquiredduringpre-training. 2023]. The above model only models the temporal dimen-
Insummary,weproposeanovelframeworkfortrafficpre- sionofthedataandignoresthespatialdimension. GATGPT
dictiontermedSpatial-TemporalLargeLanguageModel(ST- integrates the graph attention network and GPT for spatial-
LLM). In this model, we define timesteps at a location as a temporalimputation[Chenetal.,2023]. However,itdirectly
token. Thesetokenstransformaspecializedspatial-temporal overlooks the temporal representation. As of now, the tech-
embeddinglayer,whichisdesignedtoemphasizespatialand niqueforeffectivelyembeddingtimeseriesdatathatencom-
temporalcorrelationssuchasspatialcorrelationsandtempo- passes both spatial and temporal representations before in-
ral patterns. Furthermore, we fuse the spatial-temporal em- puttingitintoLLMsisnotwell-defined.
beddings of each token. This fusion process plays a piv-
otalroleinaggregatingspatialandtemporalembeddingsinto
2.2 TrafficPrediction
a unified representation. Following this, we introduce the
partially frozen attention LLM, a novel strategy tailored for
LLMs to effectively capture the spatial-temporal dependen- Trafficpredictionaimstopredictfuturetrafficfloworspeed
ciesintrafficprediction.Extensiveexperimentsonreal-world based on historical traffic data, which is crucial for intelli-
traffic datasets have validated the efficacy of ST-LLM. The
genttransportationsystems[Yeetal.,2021;Shaoetal.,2022;
keycontributionsofthispaperaresummarizedasfollows: Miaoetal.,2023]. Traditionally,thetrafficpredictionmodel
isbasedonstatistics,suchasARIMAandKalmanfilter[Ku-
mar and Vanajakshi, 2015; Chang et al., 2023]. However,
â€¢ We propose a novel Spatial-Temporal Large Language
thesemodelsdonotperformwellduetotheinherentspatial-
Model (ST-LLM) for traffic prediction, which defines
temporal dependencies of traffic data. Later, numerous ef-
timesteps at a location as a token and embeds each to-
fortshavebeendedicatedtoadvancingtrafficpredictiontech-
ken by a spatial-temporal embedding layer. We fuse
niques through the development of various neural network-
the spatial-temporal embeddings of these tokens uni-
based models. First, convolutional neural networks (CNNs)
formlyandadapttheLLMsforcapturingglobalspatial-
were applied to traffic data to capture spatial dependencies
temporaldependencies.
in the data [Shen et al., 2018; Yuan et al., 2018]. Given
â€¢ AnovelstrategywithintheLLM,namedpartiallyfrozen that CNNs are primarily designed for regular, grid-like ur-
attention, is proposed to enhance the model in traffic ban areas, they encounter challenges when dealing with the
prediction. By partially freezing the multi-head atten- non-Euclidean spatial structure of traffic data. Then, graph
tion, the ST-LLM is adapted to capture global spatial- convolutionalnetwork(GCN)basedmodelsbecamepopular
temporaldependenciesbetweentokensfordifferenttraf- due to their permutation-invariance, local connectivity, and
ficpredictiontasks. compositionally[Wuetal.,2019;Baietal.,2020]. Morere-
cently, attention-based models have emerged as a dominant
â€¢ Extensive experiments are conducted on real traffic trend[Guoetal.,2019;Zhengetal.,2020;Guoetal.,2022;
datasets to show the superior performance achieved by Jiangetal.,2023]. Withouttakingtheadjacencymatrixinto
our ST-LLM across various settings. Moreover, the account,attention-basedmodelscanstillmodeldynamicspa-
few-shot and zero-shot prediction results highlight the tialcorrelationmoreeffectivelythanGCN-basedmodels[Liu
ST-LLMâ€™scapabilityforintra-domainandinter-domain etal.,2023a].However,thestructuresofthesemodelsarebe-
knowledgetransfer. comingincreasinglysophisticated.ğ‘»ğ’“ğ’‚ğ’‡ğ’‡ğ’Šğ’„ ğ‘­ğ’†ğ’‚ğ’•ğ’–ğ’“ğ’† ğ‘¿ğ‘· ğ â„ğ‘·Ã—ğ‘µÃ—ğ‘ª Spatial-Temporal Embedding Embedding Fusion PFA LLM Traffic Prediction ğ’€+ ğ‘º ğ â„ğ‘ºÃ—ğ‘µÃ—ğ‘ª
ğ¿ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›! ğ‘·ğ‘ªğ’ğ’ğ’— ğ‘¬ğ‘· ğ â„ğ‘µÃ—ğ‘« ğ‘¬ğ‘·ğ‘¬ğ‘»ğ‘¬ğ‘º ğ‘¯ P Eo ns ci oti do in na gl ğŸ”¥ ğ¿ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›,
ğ¿ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›) ğ‘¾ğ’…ğ’‚ğ’š ğ‘¬ğ‘»ğ’… ğ‘¬ğ‘· ğ â„ğ‘µÃ—ğ‘« ğ‘­ğ‘ªğ’ğ’ğ’— ğ‘¹ğ‘ªğ’ğ’ğ’— ğ¿ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›)
ğ‘¾ğ’˜ğ’†ğ’†ğ’Œ ğ‘¬ğ‘»ğ’˜
ğ¿ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›* Transformer ğ¿ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›*
ğ‘¡âˆ’ğ‘ƒâˆ’1 t
ğ‘¾ğ’”
ğ‘¬ğ‘º ğ â„ğ‘µÃ—ğ‘« ğ‘¬ğ‘­ ğ â„ğ‘µÃ—ğŸ‘ğ‘« ğ‘¯ ğ â„ğ‘µÃ—ğŸ‘ğ‘«
â„
Block Ã—
ğ‘³ğ‘­"ğ‘¼ğŸ”¥
ğ‘¡+1 ğ‘¡+ğ‘†
Transformer Block ğŸ,â€¦,ğ‘³ğ‘­ Transformer Block ğ‘³ğ‘­"ğŸ,â€¦,ğ‘³ğ‘­"ğ‘¼
Q â„ ğŸ”¥ â„ ğŸ”¥ Q ğŸ”¥ ğŸ”¥ â„ ğŸ”¥
ğ‘¯ğŸ Layer Feed Layer ğ‘¯ğ‘­ Layer Feed Layer ğ‘¯ğ‘­"ğ‘¼ K Attention K Attention
Norm Forward Norm Norm Forward Norm
V V
Figure1: ST-LLMframework. Givenaninputtrafficfeature, wefirstembeditviaaSpatial-TemporalEmbedding. Theseembeddings
arethenintegrateduniformlybyanEmbeddingFusionlayer. ThePFA(partiallyfrozenattention)LLMhasF +U layers,whichare
dividedintothefirstF layersandthelastU layers.Themulti-headattentionandfeed-forwardlayersinthefirstF layersarefrozen,andthe
multi-headattentioninthelastU layersareunfrozen.TheoutputfromPFALLMisregressedtothepredictionresults.
3 ProblemDefinition thelastU layersareunfrozentoenhancethemodelâ€™sfocuson
capturingthespatial-temporaldependenciesbetweentokens,
Definition3.1(TrafficFeature). Wedenotethetrafficdataas
atensorXâˆˆRTÃ—NÃ—C,whereT isthenumberoftimesteps, resultingintheoutputHL âˆˆRNÃ—3D. Finally,theregression
convolutionlayertakesHL andpredictsthefollowingtraffic
N isthenumberofspatialstations,andC isthefeature. For
example,C =1representsthetrafficfloworspeed. data,denotedasY(cid:98)S âˆˆRSÃ—NÃ—C.
Definition 3.2 (Traffic Prediction). Given the
4.2 Spatial-TemporalEmbeddingandFusion
historical traffic feature of P timesteps X =
P
{X ,X ,...,X } âˆˆ RPÃ—NÃ—C, the objec- We aim to modify LLMs that have already been trained for
tâˆ’P+1 tâˆ’P+2 t
tive is to learn a function f(Â·) with parameter Î¸ to trafficpredictiontasks. Werefinethetimestampsateachlo-
predict traffic feature of on the following S timesteps cationoftrafficdataastokens. Thespatial-temporalembed-
Y ={Y ,Y ,...,Y }âˆˆRSÃ—NÃ—C. Thatis, ding layer is designed to transform the tokens into spatial-
S t+1 t+2 t+S
temporal representations that align with the LLMs. These
[X ,X ,...,X ]âˆ’f âˆ’( â†’Â·) [Y ,Y ,...,Y ], representations including spatial correlations, hour-of-day,
tâˆ’P+1 tâˆ’P+2 t t+1 t+2 t+S
Î¸ day-of-weekpatterns,andtokeninformation.
(1)
whereX âˆˆRNÃ—C. We embed the tokens through a pointwise convolution,
i where the input data X is transformed into the embedding
P
E âˆˆRNÃ—D:
4 Methodology P
E =PConv(X ;Î¸ ), (2)
Inthissection, weprovideadetailedelaborationofthepro- P P p
posedST-LLManditscomponents. whereE P representsthetokenembedding. PConv denotes
the pointwise convolution operation using filters that have a
4.1 Overview 1Ã—1kernelsize.X istheinputdata,Disthehiddendimen-
P
The Spatial-Temporal Large Language Model (ST-LLM) sion. Î¸ p representsthelearnableparametersofthepointwise
framework, as depicted in Figure 1, integrates a spatial- convolution.
temporal embedding layer, a fusion convolution layer, an To preserve the temporal information in the tokens, we
LLM layer, and aregression convolution layer. Initially, the utilize a linear layer to encode the input data into separate
historicaltrafficdataisdenotedasX ,whichcontainsN to- embeddings for the hour-of-day and day-of-week temporal
P
kens of spatial locations. The X is processed through the embeddings. We perform absolute positional encoding for
P
spatial-temporal embedding layer, which extracts the token each traffic data at the â€œdayâ€ and â€œweekâ€ resolutions, and
embedding of historical P timestamps, spatial embedding, the generated positional encodings are X day âˆˆ RNÃ—Td and
andtemporalembedding,asE
T
âˆˆRNÃ—D,E
S
âˆˆRNÃ—D,and X
week
âˆˆRNÃ—Tw.Thehour-of-dayembeddingEd
T
âˆˆRNÃ—D
E âˆˆ RNÃ—D, respectively. A fusion convolution then inte- andday-of-weekembeddingEw âˆˆ RNÃ—D arecalculatedas
P T
gratestheserepresentationsintoaunifiedwayE âˆˆRNÃ—3D. follows:
F
Subsequently, the E is input into a PFA LLM that encom-
F Ed =W (X ), Ew =W (X ), (3)
passesL+U layers,wherethemulti-headattentionandfeed T day day T week week
forwardlayersinthefirstF layersarefrozentopreservethe W
day
âˆˆ RTdÃ—D and W
week
âˆˆ RTwÃ—D are the learnable
pretrained knowledge and the multi-head attention layers in parameterembeddingsforthehour-of-dayandday-of-week,
â• â•
ğŸŸ°
â•
â•
â•respectively. Byaddingthesetwoembeddings,weobtainthe where the range of i is from 1 to F, and H1 =
temporalrepresentationE âˆˆRNÃ—D. (cid:2) H1+PE ,H2+PE ,...,HN +PE (cid:3) . PE represent
T 1 2 N i
Torepresentspatialcorrelationsamongtokenpairs,wede- thepositionalencoding. HÂ¯i representstheintermediaterep-
signanadaptiveembeddingoftokens,E S âˆˆRNÃ—D: resentation of the L i layer after applying the frozen multi-
E =Ïƒ(W Â·X +b ), (4) headattentionmechanism(MHA)andthefirstunfrozenlayer
S s P s
normalization(LN).Hisymbolizesthefinalrepresentationof
where Ïƒ denotes the activation function, W âˆˆ RDÃ—D and
s theL layerafterapplyingboththeunfrozenLNandfrozen
b âˆˆRD arethelearnableparameterembeddings. i
s feed-forwardnetwork(FFN).
Subsequently, we introduce a fusion convolution (FConv)
InthelastU layersoftheLLM,weunfreezetheMHAto
to integrate the token, spatial, and temporal embeddings to adapt the ST-LLM for capturing spatial-temporal dependen-
representthespatial-temporalembeddingofeachtoken: ciesoftrafficdata:
H F =FConv(E P||E S||E T;Î¸ f), (5) HÂ¯F+Uâˆ’1 =LN(cid:16) HF+Uâˆ’1+MHA(cid:16) HF+Uâˆ’1,HF+Uâˆ’1,HF+Uâˆ’1(cid:17)(cid:17) ,
w reh see nre tsH thF elâˆˆ earR nN abÃ— le3D p, ar| a| mde en teo rt ses ofco thn eca Ft Cen oa nt vio .n, andÎ¸ f rep- HF+U =LN(cid:16) HÂ¯F+Uâˆ’1+FFN(cid:16) HÂ¯F+Uâˆ’1(cid:17)(cid:17) , (7)
4.3 PartiallyFrozenAttention(PFA)LLM where HÂ¯F+U represents the intermediate representation of
theL layerafterapplyingtheunfrozenMHAandthe
The frozen pretrained transformer (FPT) has demonstrated F+Uâˆ’1
second frozen LN. HF+U denotes the final output of the
effectiveness in a variety of downstream tasks across non-
L layerafterapplyingboththeunfrozenLNandfrozen
language modalities [Lu et al., 2022]. However, its perfor- F+U
FFN,withtheMHAbeingunfrozen.
mance is less optimal in tasks requiring both short-term and
After the PFA LLM, we design a regression convolution
long-termpredictions,suchastrafficprediction[Zhouetal.,
(RConv) to predict the traffic features on the following S
2023]. To address this limitation, we propose a partially
timesteps:
frozenattention(PFA)LLM,specificallydesignedtoenhance
predictionaccuracyintrafficprediction. YË† S =RConv(HF+U;Î¸ r), (8)
ThedifferencebetweentheFPTandourPFAprimarilylies where Y(cid:98)S âˆˆ RSÃ—NÃ—C, and Î¸
r
represents the learnable pa-
in the configuration of frozen attention layers. In the FPT rametersoftheregressionconvolution.
framework, both the multi-head attention and feed-forward ThelossfunctionofST-LLMisestablishedasfollows:
layers are frozen during training, as these layers contain the (cid:13) (cid:13)
mostsignificantportionofthelearnedknowledgewithinthe L=(cid:13) (cid:13)Y(cid:98)S âˆ’Y S(cid:13) (cid:13)+Î»Â·Lreg, (9)
LLM. In the PFA, we maintain the first F layers identical
to the FPT, but crucially, we unfreeze the last U multi-head where Y(cid:98)S is the predicted traffic feature. Y
S
is the ground
attention layers since the attentions offer effective handling truth.LregrepresentstheL2regularizationterm,whichhelps
ofspatial-temporaldependenciesindata. Consequently, our controloverfitting. Î»isahyperparameter.
ST-LLM can adapt to traffic prediction while preserving the
foundationalknowledgeacquiredduringpre-training. Table1:StatisticsofTrafficData
Furthermore, our ST-LLM inverts the traditional calcula- Type Dataset #Station(N) #Timestep Interval
tiondimensionfromtemporaltospatial. Thisinversionisin- NYCTaxi 266 4,368 30min
Flow
tentionalandalignswiththeoperationofthepartiallyfrozen CHBike 250 4,368 30min
layers. By focusing on spatial dimensions, our model cap- METR-LA 207 34,272 5min
Speed
PEMS-BAY 325 52,116 5min
tures global correlations more effectively than if we were to
concentratesolelyontemporalaspects. Thisshiftisparticu-
larlyrelevantinthecontextoftrafficprediction,wherespatial 5 Experiments
dynamicsplayacriticalroleindeterminingflowpatterns.
5.1 Datasets
To elaborate, the PFA LLM generates a probabilis-
This section details the datasets employed to examine the
tic distribution of the new representations based on the
predictive performance of the ST-LLM and baselines, with
sequence of existing embedded representations H =
{H1,H2,...,HN}. InthefirstF layersoftheLLM: real-world traffic data from NYCtaxi [Ye et al., 2021],
CHBike [Jiang et al., 2023], METR-LA, and PEMS-
HÂ¯i =LN (cid:0) Hi+MHA(cid:0) Hi,Hi,Hi(cid:1)(cid:1) , BAY [Shao et al., 2022]. Notably, NYCtaxi and CHBike
Hi+1 =LN (cid:0) HÂ¯i+FFN (cid:0) HÂ¯i(cid:1)(cid:1) , datasetsencompasstwotypesoftrafficflowdata:pick-upand
drop-off. Meanwhile, METR-LA and PEMS-BAY datasets
MHA(Hi,Hi,Hi)=WO(head 1||Â·Â·Â·||head h), focusontrafficspeed. Detailsareasfollowsandthestatistics
head =Attention(WQHi,WKHi,WVHi), ofthesedatasetsareprovidedinTable1.
i i i i
(cid:18) HiHiT(cid:19) 5.2 Baselines
Attention(Hi,Hi,Hi)=softmax âˆš Hi,
d We compare our ST-LLM with the following baseline mod-
k
FFN(HÂ¯i)=max(0,W HÂ¯i+1+b )W +b , els, which cover different types of methods and have been
1 P 1 2 2 widely used for spatio-temporal prediction tasks. We com-
(6)
pare ST-LLM with the following 10 baselines belonging toTable2:ModelcomparisononNYCTaxiandCHBikedatasetsintermsofMAE,RMSE,MAPE(%),andWAPE(%).
Dataset NYCTaxiPick-up NYCTaxiDrop-off CHBikePick-up CHBikeDrop-off
Metric MAE RMSE MAPE WAPE MAE RMSE MAPE WAPE MAE RMSE MAPE WAPE MAE RMSE MAPE WAPE
DCRNN 5.40 9.71 35.09% 20.43% 5.19 9.63 37.78% 19.82% 2.09 3.30 54.22% 42.26% 1.96 2.94 51.42% 39.61%
STGCN 5.71 10.22 36.51% 21.62% 5.38 9.60 39.12% 20.55% 2.08 3.31 53.63% 42.08% 2.01 3.07 50.45% 40.62%
ASTGCN 7.43 13.84 47.96% 28.04% 6.98 14.70 45.48% 26.60% 2.76 4.45 64.23% 55.71% 2.79 4.20 69.88% 56.49%
GWN 5.43 9.39 37.79% 20.55% 5.03 8.78 35.63% 19.21% 2.04 3.20 53.08% 40.95% 1.95 2.98 50.30% 39.43%
AGCRN 5.79 10.11 40.40% 21.93% 5.45 9.56 40.67% 20.81% 2.16 3.46 56.35% 43.69% 2.06 3.19 51.91% 41.78%
GMAN 5.43 9.47 34.39% 20.42% 5.09 8.95 35.00% 19.33% 2.20 3.35 57.34% 44.06% 2.09 3.00 54.82% 42.00%
ASTGNN 5.90 10.71 40.15% 22.32% 6.28 12.00 49.78% 23.97% 2.37 3.67 60.08% 47.81% 2.24 3.35 57.21% 45.27%
STG-NCDE 6.24 11.25 43.20% 23.46% 5.38 9.74 40.45% 21.37% 2.15 3.97 55.49% 61.38% 2.28 3.42 60.96% 46.06%
DGCRN 5.44 9.82 35.78% 20.58% 5.14 9.39 35.09% 19.64% 2.06 3.21 54.06% 41.51% 1.96 2.93 51.99% 39.70%
OFA 5.82 10.42 36.67% 22.00% 5.60 10.14 37.39% 21.36% 2.06 3.21 53.55% 41.70% 1.96 2.97 49.64% 39.68%
GATGPT 5.92 10.55 37.83% 22.39% 5.66 10.39 37.36% 21.60% 2.07 3.23 52.54% 41.70% 1.95 2.94 49.26% 39.43%
GCNGPT 6.58 12.23 40.19% 24.88% 6.64 12.24 42.46% 25.32% 2.37 3.80 56.24% 47.66% 2.24 3.48 51.05% 45.37%
LLAMA2 5.35 9.48 41.32% 20.27% 5.66 10.74 47.47% 21.63% 2.10 3.37 56.63% 42.49% 1.99 3.03 55.23% 40.28%
ST-LLM 5.29 9.42 33.55% 20.03% 5.07 9.07 33.34% 19.18% 1.99 3.08 53.54% 40.19% 1.89 2.81 49.50% 38.27%
Table3:ModelcomparisononMETR-LAandPEMS-BAYdatasets. 5.4 EvaluationMetrics
Method METR-LA PEMS-BAY Fourmetricswereusedforevaluatingthemodels: meanab-
Metric MAE RMSE MAPE MAE RMSE MAPE
soluteerror(MAE),meanabsolutepercentageerror(MAPE),
DCRNN 3.60 7.59 10.50% 2.07 4.74 4.90%
STGCN 4.59 9.40 12.70% 2.49 5.69 5.79% rootmeansquarederror(RMSE),andweightedabsoluteper-
ASTGCN 6.51 12.52 11.64% 2.61 5.42 6.00% centage error (WAPE). MAE and RMSE quantify absolute
GWN 3.53 7.37 10.01% 1.95 4.52 4.63%
errors, while MAPE and WAPE assess relative errors. In
AGCRN 3.59 7.45 10.47% 1.94 4.50 4.55%
GMAN 3.44 7.35 10.07% 1.86 4.32 4.37% all metrics, lower values indicate superior prediction perfor-
ASTGNN 6.14 7.24 8.76% 2.28 2.97 3.68% mance:
STG-NCDE 6.35 14.94 13.49% 2.03 4.58 4.82%
GD AG
O
TC
F
GAR PN
T
3
3
3.
.
.4
1
14
7
8
67 6.. .41 539
8
889 ... 557 253 %%% 1
1
1.
.
.8
6
69
8
6
4
3
3.
.
.4
7
72
1
5
4
3
3.
.
.4
6
63
9
7%
%
%
MAE=
m1 (cid:88)m (cid:12)
(cid:12) (cid:12)Y(cid:98)iâˆ’Y
i(cid:12)
(cid:12) (cid:12), MAPE=
10 m0%(cid:88)m (cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)Y(cid:98)i Yâˆ’ iY i(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12),
GCNGPT 3.26 6.51 8.81% 1.67 3.83 3.78% i=1 i=1 (10)
L SL TA -LM LA M2 3 3. .3 07 9 6 6. .5 30 0 89 .. 41 12 %% 1 1. .7 62 1 3 3. .6 69 1 33 .. 58 94 %% (cid:118) (cid:117) (cid:117)1 (cid:88)m (cid:16) (cid:17)2 (cid:80)m i=1(cid:12) (cid:12) (cid:12)Y(cid:98)iâˆ’Y i(cid:12) (cid:12) (cid:12)
RMSE=(cid:116) m Y(cid:98)iâˆ’Y i , WAPE= (cid:80)m |Y | Ã—100%,
i=1 i=1 i
two classes. (1) GNN-based models: DCRNN [Li et al., (11)
2018], STGCN [Yu et al., 2018], GWN [Wu et al., 2019], wheremisthenumberofpredictedvalues.
AGCRN [Bai et al., 2020], STG-NCDE [Choi et al., 2022],
DGCRN[Lietal.,2023]. (2)Attention-basedmodels: AST- 5.5 ResultsandAnalysis
GCN[Guoetal., 2019], GMAN[Zhengetal., 2020], AST- ThecomparisonresultswithbaselinesareshowninTables2
GNN[Guoetal.,2022].(3)LLM-basedmodels:OFA[Zhou
and 3. We can make the following observations. (1) LLM-
et al., 2023], GATGPT [Chen et al., 2023], GCNGPT, and basedmethodsyieldsuperiorpredictionresults,especiallyin
LLAMA2. traffic speed prediction scenarios with larger data volumes,
withST-LLMexhibitingthemosteffectiveperformance.The
5.3 Implementations
ST-LLM outperforms other LLMs in six traffic prediction
Aligningwithcontemporarypractices,wedividedtheNYC- scenarios, demonstrating its superior accuracy in handling
Taxi and CHBike datasets into training, validation, and test diverse traffic data across various datasets. (2) OFA and
setsusinga6:2:2ratio, andthePEMS-BAYandMETR-LA LLAMA2 are competent but surpassed by ST-LLM which
datasetsintoa7:1:2ratio. WesetthehistoricaltimestepsP achievesa22.5%averageMAEimprovementoverOFAand
and the future timesteps S to 12 each, enabling multi-step 20.8% over LLAMA2. This may be due to OFAâ€™s ineffec-
traffic prediction. T is set at 7 to represent a weekâ€™s seven tive traffic data embedding, making it difficult for LLMs to
w
days. Forthetrafficflowdataset,T is48,witheachtimestep understand the spatial-temporal dependencies between data.
d
spanning 30 minutes, while for the traffic speed dataset, T DespiteLLAMA2â€™slargersizeandcomplexity,itdoesnâ€™tdi-
d
is 288, corresponding to 5-minute intervals. The experi- rectlytranslatetobettertrafficpredictionthanST-LLM.GAT-
ments were carried out on a system incorporating NVIDIA GPTandGCNGPTdonotextracttemporalrepresentationsof
A100GPUs,eachwith40GBofmemory. FortrainingLLM- trafficdatatoinfluencetheLLMtocapturespatial-temporal
basedmodels,weusedtheRangeroptimizerwithalearning dependencies.(3)Attention-basedmodelslikeASTGNNand
rate of 0.001, while GCN and attention-based models em- GMANexhibitvariedperformanceacrossdifferentdatasets.
ployedtheAdamoptimizer,alsosetata0.001learningrate. They performed quite well in some cases but were always
TheLLMsusedareGPT2andLLAMA27B.Weconfigured inferior to ST-LLM. This variability could be attributed to
GPT2withsixlayers[Zhouetal.,2023],andLLAMA2with the limitations of traditional attention mechanisms in han-
eight layers [Jin et al., 2023b]. The experiments were con- dlingcomplexspatial-temporalembeddings,especiallywhen
ductedwithabatchsizeof64over200trainingepochs. Note comparedtolargelanguagemodels. (4)GNN-basedModels
that the experimental results are averaged across all predic- suchasGWNandDGCRNdemonstratecompetitiveperfor-
tiontimesteps. mance, particularly in specific metrics, but still cannot out-Table4:AblationStudyofPartiallyFrozenAttentionLLM.
LLM NoPretrain FullLayer FullTuning FPT PFA
Metric MAE RMSE WAPE MAE RMSE WAPE MAE RMSE WAPE MAE RMSE WAPE MAE RMSE WAPE
CHBikeDrop-off 1.92 2.86 38.84% 1.91 2.83 38.63% 1.90 2.82 38.28% 1.92 2.86 38.90% 1.89 2.81 38.27%
CHBikePick-up 2.03 3.14 40.87% 2.02 3.12 40.62% 2.01 3.11 40.43% 2.07 3.25 41.65% 1.99 3.08 40.19%
Table5:Few-shotpredictionresultson10%dataofLLMs.
NYCTaxiPick-up NYCTaxiDrop-off CHBikePick-up CHBikeDrop-off
LLM
MAE RMSE MAPE WAPE MAE RMSE MAPE WAPE MAE RMSE MAPE WAPE MAE RMSE MAPE WAPE
OFA 6.49 12.12 46.74% 24.54% 6.27 12.10 45.23% 23.92% 2.20 3.59 57.52% 44.40% 2.06 3.17 55.96% 41.63%
GATGPT 7.02 13.09 50.19% 26.54% 6.84 13.27 56.15% 26.09% 2.59 4.41 56.23% 52.20% 2.50 4.07 56.36% 50.64%
GCNGPT 10.31 18.82 59.41% 39.02% 9.25 19.50 56.77% 35.28% 2.73 4.44 56.93% 55.20% 2.79 4.65 61.85% 56.28%
LLAMA2 5.81 10.16 41.82% 21.99% 5.59 9.90 40.58% 21.35% 2.24 3.58 59.47% 45.20% 2.11 3.23 54.44% 42.75%
ST-LLM 5.40 9.63 33.36% 20.45% 5.54 9.84 39.56% 21.14% 2.07 3.23 55.68% 41.85% 1.93 2.88 52.75% 39.21%
ST-LLM 2.30 ST-LLM 44 WM WAA APP PEE E 24 6.4 M RMAE SE 12
6.50 G GA CT NG GP PT T 2.20 G GA CT NG GP PT T 33 18 4.8 9
OFA OFA
6.00 2.10
22 12 3.2 6
5.50 2.00
5.00 1.90 11 6 1.6 3
1.0 2.0 3.0 4.0 5.0 1.0 2.0 3.0 4.0
Average Prediction Time (s/epoch) Average Prediction Time (s/epoch) 0 w/o LLM w/o ST w/o T w/o S ST-LLM 0 0.0 w/o LLM w/o ST w/o T w/o S ST-LLM 0
(a) NYCTaxiDrop-off. (b) CHBikeDrop-off. (a) MAPEandWAPE. (b) MAEandRMSE.
Figure2:InferencetimeofLLMs. Figure3:AblationstudyofST-LLMonNYCTaxiDrop-offdataset.
performST-LLM.ThissuggeststhatwhileGNNseffectively studyontheNYCTaxidatasets. Thew/oLLMvariantshows
capturespatialdependencies,theirtemporalanalysiscapabil- aconsiderableincreaseinerroracrossallmetrics.Itsremoval
ities might not be as advanced as the ST-LLM, which limits leadstoadegradationinperformance,demonstratingthatthe
theiroverallperformance. prediction capabilities of the ST-LLM are heavily reliant on
Figure 2 illustrates the trade-off between inference time theLLMâ€™sabilitytolearncomplexdependenciesfromtraffic
andMAEforvariousLLMsonNYCTaxiandCHBikeDrop- data. The exclusion of ST results in a notable performance
off datasets. Notably, LLAMA2 is excluded from this com- drop. Thishighlightstheimportanceofspatial-temporalem-
parisonduetoitssubstantiallylongerinferencetimerelative bedding in understanding both spatial and temporal depen-
tootherLLMs. OntheNYCTaxiDrop-offdataset,ST-LLM dencies in traffic data. The experimental results reveal that
achieves the lowest MAE while maintaining competitive in- removing either temporal (w/o T) or spatial (w/o S) compo-
ference times. Following closely is OFA, whose inference nentssimilarlyaffectsthemodelâ€™spredictionerror. Ablating
timeissimilartoST-LLM,albeitwithaslightlyhigherMAE. eitheroneoftheseembeddingsresultsinanincreasederror.
This suggests that spatial-temporal embedding and PFA do Notably, the model incurs a larger prediction error without
notslowdowntheinferencespeedoftheLLM,yetenhance thetemporalcomponent,underscoringthesignificanceofour
prediction accuracy. GATGPT and GCNGPT exhibit longer thoughtfullydesignedtemporalembeddings.
inference times and higher MAEs than ST-LLM, with GC-
Ablation Study of Partially Frozen Attention LLM. In
NGPTbeingtheslowest. Thiscouldbeattributedtothefact
this subsection, we conducted an ablation study to evalu-
thatwhileGCNgenerallyhasasimplerstructure,theircom-
ate the efficacy of our proposed Partially Frozen Attention
binationwithGPTmightintroduceextracomputationalcom-
(PFA) LLM, as presented in Table 4. The PFA is compared
plexity. This also implies that GATâ€™s attention mechanism
against several variations: Frozen Pretrained Transformer
could be more efficient when combined with GPT. Similar
(FPT),modelswithoutpretraining(NoPretrain),modelsuti-
trendsareobservedintheCHBikeDrop-offdataset. Insum-
lizingthefull12layersofGPT-2(FullLayer),andfullytuned
mary,ST-LLMstandsoutasthemodelprovidingthebestbal-
models without any frozen layers (Full Tuning). The PFA
ancebetweeninferencespeedandpredictiveaccuracyacross
LLM demonstrates superior performance across all metrics
bothdatasets.
on all datasets, which suggests that partially freezing the at-
tention significantly enhances the predictive accuracy. FPT
5.6 AblationStudies
shows commendable performance, e.g., it is slightly outper-
With or Without Different Components. The ST-LLMâ€™s formedbyPFA.Thisindicatesthatthepartialfreezingstrat-
effectiveness in traffic prediction stems from key compo- egy strikes a better balance between leveraging prelearned
nents, explored through variant comparisons: w/o LLM knowledgeandadaptingtonewdata.TheFullLayerandFull
(LLM removed), w/o ST (spatial-temporal embedding re- Tuning models exhibit competitive performance. However,
moved), w/o T (temporal embedding removed), and w/o S theystillfallshortoftheaccuracydemonstratedbythePFA
(spatial embedding removed). This analysis investigates the model. The comparison with the No Pretrain model high-
impact of each component. Figure 3 presents the ablation lights the role of pretraining in model performance. While
EAM EAM )%(
EPAM
)%(
EPAW EAM ESMRTable6:Zero-shotpredictionresultsofLLMs.
OFA GATGPT GCNGPT LLMAM2 ST-LLM
LLM
MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE
NYCTaxiPick-upâ†’CHBikeDrop-off 3.57 5.72 59.26% 3.25 5.34 59.35% 3.49 5.64 59.06% 3.23 5.74 72.14% 3.12 5.01 55.12%
NYCTaxiPick-upâ†’CHBikePick-up 3.61 5.98 59.55% 3.29 5.60 59.71% 3.53 5.91 59.14% 3.25 5.15 88.52% 3.06 5.40 50.94%
NYCTaxiPick-upâ†’NYCTaxiDrop-off 9.99 20.22 75.14% 10.00 21.16 68.03% 11.03 21.86 70.32% 11.02 22.34 94.31% 9.31 18.68 66.42%
NYCTaxiDrop-offâ†’CHBikeDrop-off 3.58 5.72 59.33% 3.19 4.99 76.75% 3.35 5.19 69.36% 3.29 4.99 80.87% 3.09 4.65 52.73%
NYCTaxiDrop-offâ†’CHBikePick-up 3.62 5.99 59.55% 3.26 5.27 79.41% 3.43 5.49 71.76% 3.33 5.32 82.60% 3.02 5.18 68.27%
NYCTaxiDrop-offâ†’NYCTaxiPick-up 10.04 17.72 88.10% 9.67 17.76 73.46% 8.09 14.58 50.99% 11.14 20.57 94.03% 8.02 13.21 46.16%
Other models, such as OFA, GATGPT, and GCNGPT, al-
though commendable in their performances, do not match
22.0 2.070
thesuperiorresultsofST-LLM.Notably, despiteOFAâ€™sbet-
21.6 2.055
ter performance on the CHBike Drop-off dataset, ST-LLM
21.2 2.040 stilloutperformsitwitha9.15%improvementinMAE.Com-
20.8 2.025
paredwithGATGPTandGCNGPT,ST-LLMshowsremark-
2.010
20.4 able average improvements of over 39.21% and 7.80% in
1.995
20.0 0 1 2 3 4 5 6 0 1 2 3 4 5 6 MAEacrossalldatasets,respectively. Thissignificantdiffer-
U U
(a) U onNYCTaxiPick-up. (b) U onCHBikePick-up. encehighlightstherobustnessofST-LLMinefficientlyhan-
dlingscenarioswithlimiteddata.
Figure4:PerformancestudyofunfreezinglastU layers.
5.9 Zero-ShotPrediction
the No Pretrain model performs reasonably well, it is evi- This zero-shot prediction is designed to assess the cross-
dent that pretraining, especially when combined with strate- dataset adaptation capability of LLMs. This task evaluates
gieslikepartialfrozenisbeneficialtoachievinghigheraccu- themodelâ€™sabilitytogeneralizetotheCHBikedatasetafter
racy. beingtrainedexclusivelyontheNYCtaxidataset,withoutany
priorexposuretodatafromtheCHBikedataset. Theresults
5.7 ParameterAnalysis of this evaluation are shown in Table 6. In terms of intra-
domain transfer (e.g., from NYCTaxi Pick-up to NYCTaxi
Within the ST-LLM framework shown in Figure 4, U rep-
Drop-off),ST-LLMdemonstratesitsabilitytomaintainhigh
resents the hyperparameter that determines the number of
accuracy, underlining its effectiveness in handling dataset-
multi-head attention layers to unfreeze during the training
specific dependencies. The results also show that ST-LLM
process. AsdepictedinFigure4(a),ananalysisoftheNYC-
exhibits exceptional performance in inter-domain scenarios,
Taxi Pick-up dataset reveals that with an increase in U, the
such as transferring from NYCTaxi to CHBike. This is evi-
WAPEinitiallydecreases. Thetrendsuggeststhatunfreezing
dentinallmetrics,whereST-LLMconsistentlyachievesthe
additional layers up to a certain threshold can enhance the
lowesterrorrates,indicatingarobustabilitytoadapttonew
performanceoftheST-LLM.However,thispositiveeffectin-
domainswithoutretraining.WhenanalyzingotherLLMslike
verts when U exceeds 2, at which point the modelâ€™s perfor-
OFA,GATGPT,GCNGPT,andLLAMA2,weobservevaried
mance starts to degrade, hinting at the diminishing benefits
performances. For instance, OFA performs well in certain
associated with unfreezing more layers. In a nutshell, the
scenarios but is generally outperformed by ST-LLM. GAT-
optimal U of taxi flow prediction is set to 2. Traffic speed
GPT and GCNGPT, while showing competent adaptability,
predictionalsohasthesameparametersettings. Figure4(b)
still fall short compared to ST-LLMâ€™s performance, partic-
illustrates a similar pattern on the CHBike Pick-up dataset.
ularly in challenging inter-domain transfers. In conclusion,
The MAE indicates an increase in performance as U is set
thezero-shotpredictionresultsreinforcetheadaptabilityand
to 1, with the lowest MAE observed at this value. This re-
predictivestrengthofST-LLM,makingitahighlypromising
inforcestheobservationthatasingleunfrozenmulti-headat-
modelfortrafficflowpredictiontasksacrossdiversedatasets.
tention layer is optimal for minimizing absolute errors, and
themodelachievesthebalancebetweencomplexityandper-
6 Conclusion
formance. Thisoptimalpointsuggeststhatunfreezingmore
layers does not contribute to improved accuracy and might ST-LLM shows promise in adapting large language models
evendegradetheperformanceofST-LLM. for traffic prediction by embedding traffic data into spatial-
temporal representations for LLMs. A partially frozen at-
5.8 Few-ShotPrediction tention strategy within ST-LLM is proposed to enhance the
modelintrafficprediction.Ourresearchfindingsdemonstrate
In the few-shot prediction experiment, models trained with
thattheST-LLMoutperformsstate-of-the-artmodelsintraf-
just10%ofdatashowedtheST-LLMmodelâ€™ssuperiorabil-
fic prediction. Looking forward, future work will explore a
ity to recognize complex patterns from limited data, as de-
rangeofadditionaltasks,suchastrafficimputation,andwill
tailed in Table 5. While the LLAMA2 model presents com-
focusondevelopingmemory-efficientLLMstailoredfortraf-
petitive results, especially on the NYCTaxi datasets, it does
ficdataanalysis.
notconsistentlysurpasstheperformanceofST-LLM.Forin-
stance,ontheNYCTaxipick-updataset,ST-LLMachievesa
noteworthy7.06%reductioninMAEcomparedtoLLAMA2.
)%( EPAW EAMReferences Neville, editors, Thirty-Seventh Conference on Artificial
[Baietal.,2020] LeiBai,LinaYao,CanLi,XianzhiWang, Intelligence,Washington,DC,USA,February7-14,pages
4365â€“4373,2023.
and Can Wang. Adaptive graph convolutional recurrent
networkfortrafficforecasting. Advancesinneuralinfor- [Jinetal.,2023a] Guangyin Jin, Yuxuan Liang, Yuchen
mationprocessingsystems,33:17804â€“17815,2020. Fang, Zezhi Shao, Jincai Huang, Junbo Zhang, and
[Camposetal.,2023] David Campos, Miao Zhang, Bin Yu Zheng. Spatio-temporal graph neural networks for
Yang,TungKieu,ChenjuanGuo,andChristianS.Jensen. predictive learning in urban computing: A survey. IEEE
LightTS:Lightweighttimeseriesclassificationwithadap-
TransactionsonKnowledgeandDataEngineering,pages
tive ensemble distillation. Proceedings of the ACM on 1â€“20,2023.
ManagementofData,1(2):171:1â€“171:27,2023. [Jinetal.,2023b] Ming Jin, Shiyu Wang, Lintao Ma, Zhix-
[Caoetal.,2023] Defu Cao, Furong Jia, Sercan O Arik, uan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen,
Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan YuxuanLiang,Yuan-FangLi,ShiruiPan,etal. Time-llm:
Liu. Tempo: Prompt-based generative pre-trained trans- Timeseriesforecastingbyreprogramminglargelanguage
former for time series forecasting. arXiv preprint models. InternationalConferenceonLearningRepresen-
arXiv:2310.04948,2023. tations,2023.
[Changetal.,2023] Shih Yu Chang, Hsiao-Chun Wu, and [Jinetal.,2023c] Ming Jin, Qingsong Wen, Yuxuan Liang,
Yi-ChihKao. Tensorextendedkalmanfilteranditsappli- Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang,
cationtotrafficprediction. IEEETransactionsonIntelli- YiWang,HaifengChen,XiaoliLi,etal. Largemodelsfor
gentTransportationSystems,24(12):13813â€“13829,2023. time series and spatio-temporal data: A survey and out-
look. arXivpreprintarXiv:2310.10196,2023.
[Chenetal.,2023] Yakun Chen, Xianzhi Wang, and Guan-
dong Xu. Gatgpt: A pre-trained large language model [Koetal.,2023] Dohwan Ko, Joonmyung Choi,
with graph attention network for spatiotemporal imputa- Hyeong Kyu Choi, Kyoung-Woon On, Byungseok
tion. arXivpreprintarXiv:2311.14332,2023. Roh, and Hyunwoo J. Kim. MELTR: meta loss trans-
formerforlearningtofine-tunevideofoundationmodels.
[Choietal.,2022] JeongwhanChoi,HwangyongChoi,Jee-
InIEEE/CVFConferenceonComputerVisionandPattern
hyunHwang,andNoseongPark. Graphneuralcontrolled
Recognition, Vancouver, BC, Canada, June 17-24, pages
differential equations for traffic forecasting. In Thirty-
20105â€“20115,2023.
Sixth Conference on Artificial Intelligence, Virtual Event,
February22-March1,2022. [KumarandVanajakshi,2015] S Vasantha Kumar and
Lelitha Vanajakshi. Short-term traffic flow prediction
[Gongetal.,2023] Jiahui Gong, Yu Liu, Tong Li, Haoye
using seasonal arima model with limited input data.
Chai, Xing Wang, Junlan Feng, Chao Deng, Depeng Jin,
EuropeanTransportResearchReview,7(3):1â€“9,2015.
and Yong Li. Empowering spatial knowledge graph for
mobiletrafficprediction. InProceedingsofthe31stACM [LablackandShen,2023] Mourad Lablack and Yanming
International Conference on Advances in Geographic In- Shen. Spatio-temporal graph mixformer for traffic fore-
formationSystems,pages1â€“11,2023. casting. Expert Systems with Applications, 228:120281,
2023.
[Guoetal.,2019] Shengnan Guo, Youfang Lin, Ning Feng,
Chao Song, and Huaiyu Wan. Attention based spatial- [Lietal.,2018] Yaguang Li, Rose Yu, Cyrus Shahabi, and
temporal graph convolutional networks for traffic flow Yan Liu. Diffusion convolutional recurrent neural net-
forecasting. In The Thirty-Third Conference on Artificial work: Data-driven traffic forecasting. In International
Intelligence,Honolulu,Hawaii,USA,January27-Febru- Conference on Learning Representations, pages 1â€“16,
ary1,pages922â€“929,2019. 2018.
[Guoetal.,2022] Shengnan Guo, Youfang Lin, Huaiyu [Lietal.,2023] Fuxian Li, Jie Feng, Huan Yan, Guangyin
Wan, Xiucheng Li, and Gao Cong. Learning dynamics Jin, Fan Yang, Funing Sun, Depeng Jin, and Yong Li.
andheterogeneityofspatial-temporalgraphdatafortraffic Dynamic graph convolutional recurrent network for traf-
forecasting. IEEE Transactions on Knowledge and Data fic prediction: Benchmark and solution. ACM Transac-
Engineering,34(11):5415â€“5428,2022. tionsonKnowledgeDiscoveryfromData,17(1):9:1â€“9:21,
[Jietal.,2023] Jiahao Ji, Jingyuan Wang, Chao Huang, 2023.
Junjie Wu, Boren Xu, Zhenhe Wu, Junbo Zhang, and [Linetal.,2020] Zhihui Lin, Maomao Li, Zhuobin Zheng,
Yu Zheng. Spatio-temporal self-supervised learning for YangyangCheng,andChunYuan.Self-attentionconvlstm
traffic flow prediction. In Brian Williams, Yiling Chen, for spatiotemporal prediction. In Thirty-Fourth Confer-
and Jennifer Neville, editors, Thirty-Seventh Conference enceonArtificialIntelligence,NewYork,NY,USA,Febru-
onArtificialIntelligence,Washington,DC,USA,February ary7-12,pages11531â€“11538,2020.
7-14,pages4356â€“4364,2023. [Liuetal.,2022a] ChenxiLiu,ZhuXiao,DongWang,Min-
[Jiangetal.,2023] JiaweiJiang,ChengkaiHan,WayneXin hao Cheng, Hongyang Chen, and Jiawei Cai. Foreseeing
Zhao,andJingyuanWang. Pdformer: Propagationdelay- private car transfer between urban regions with multiple
awaredynamiclong-rangetransformerfortrafficflowpre- graph-basedgenerativeadversarialnetworks. WorldWide
diction. In Brian Williams, Yiling Chen, and Jennifer Web,25(6):2515â€“2534,2022.[Liuetal.,2022b] Chenxi Liu, Zhu Xiao, Dong Wang, Lei series forecasting. arXiv preprint arXiv:2310.08278,
Wang, Hongbo Jiang, Hongyang Chen, and Jiangxia Yu. 2023.
Exploitingspatiotemporalcorrelationsofarrive-stay-leave [Shaoetal.,2022] Zezhi Shao, Zhao Zhang, Wei Wei, Fei
behaviors for private car flow prediction. IEEE Transac-
Wang, Yongjun Xu, Xin Cao, and Christian S. Jensen.
tionsonNetworkScienceandEngineering,9(2):834â€“847,
Decoupled dynamic spatial-temporal graph neural net-
2022.
work for traffic forecasting. Proc. VLDB Endow.,
[Liuetal.,2023a] HangchenLiu,ZhengDong,RenheJiang, 15(11):2733â€“2746,jul2022.
Jiewen Deng, Jinliang Deng, Quanjun Chen, and Xuan [Shenetal.,2018] Bilong Shen, Xiaodan Liang, Yufeng
Song. Spatio-temporaladaptiveembeddingmakesvanilla
Ouyang, Miaofeng Liu, Weimin Zheng, and Kathleen M
transformersotafortrafficforecasting. InProceedingsof
Carley. Stepdeep:Anovelspatial-temporalmobilityevent
the 32nd ACM International Conference on Information
prediction framework based on deep neural network. In
andKnowledgeManagement,pages4125â€“4129,2023.
Proceedingsofthe24thACMSIGKDDinternationalcon-
[Liuetal.,2023b] Yong Liu, Tengge Hu, Haoran Zhang, ference on knowledge discovery & data mining, pages
Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng 724â€“733,2018.
Long. itransformer:Invertedtransformersareeffectivefor [Songetal.,2020] ChaoSong,YoufangLin,ShengnanGuo,
timeseriesforecasting. arXivpreprintarXiv:2310.06625, and Huaiyu Wan. Spatial-temporal synchronous graph
2023. convolutional networks: A new framework for spatial-
[Luetal.,2022] Kevin Lu, Aditya Grover, Pieter Abbeel, temporal network data forecasting. In Proceedings of
andIgorMordatch. Frozenpretrainedtransformersasuni- theAAAIconferenceonartificialintelligence,volume34,
versalcomputationengines.InThirty-SixthConferenceon pages914â€“921,2020.
ArtificialIntelligence,VirtualEvent,February22-March [Sunetal.,2023] ChenxiSun,YaliangLi,HongyanLi,and
1,pages7628â€“7636,2022. Shenda Hong. Test: Text prototype aligned embedding
[Maynezetal.,2023] Joshua Maynez, Priyanka Agrawal, to activate llmâ€™s ability for time series. arXiv preprint
and Sebastian Gehrmann. Benchmarking large language arXiv:2308.08241,2023.
model capabilities for conditional generation. In Anna [Wangetal.,2020] Senzhang Wang, Hao Miao, Hao Chen,
Rogers,JordanL.Boyd-Graber,andNaoakiOkazaki,ed- andZhiqiuHuang.Multi-taskadversarialspatial-temporal
itors,Proceedingsofthe61stAnnualMeetingoftheAsso- networks for crowd flow prediction. In Proceedings of
ciation for Computational Linguistics, Toronto, Canada, the29thACMInternationalConferenceonInformation&
July9-14,pages9194â€“9213,2023. KnowledgeManagement,page1555â€“1564,2020.
[Miaoetal.,2023] Hao Miao, Jiaxing Shen, Jiannong Cao, [Wenetal.,2023] Haomin Wen, Youfang Lin, Yutong Xia,
Jiangnan Xia, and Senzhang Wang. Mba-stnet: Bayes- Huaiyu Wan, Qingsong Wen, Roger Zimmermann, and
enhanced discriminative multi-task learning for flow pre- Yuxuan Liang. Diffstg: Probabilistic spatio-temporal
diction.IEEETrans.Knowl.DataEng.,35(7):7164â€“7177, graphforecastingwithdenoisingdiffusionmodels.InPro-
2023. ceedingsofthe31stACMInternationalConferenceonAd-
[Miaoetal.,2024] Hao Miao, Yan Zhao, Chenjuan Guo, vances in Geographic Information Systems, pages 60:1â€“
Bin Yang, Zheng Kai, Feiteng Huang, Jiandong Xie, and 60:12,2023.
Christian S. Jensen. A unified replay-based continu- [Wuetal.,2019] Zonghan Wu, Shirui Pan, Guodong Long,
ouslearningframeworkforspatio-temporalpredictionon Jing Jiang, and Chengqi Zhang. Graph wavenet for deep
streaming data. IEEE International Conference on Data spatial-temporal graph modeling. In Sarit Kraus, editor,
Engineering,2024. ProceedingsoftheTwenty-EightInternationalJointCon-
[NateGruverandWilson,2023] Shikai Qiu Nate Gruver, ference on Artificial Intelligence, Macao, China, August
Marc Finzi and Andrew Gordon Wilson. Large language 10-16,pages1907â€“1913,2019.
modelsarezeroshottimeseriesforecasters. InAdvances [Wuetal.,2023] Xinle Wu, Dalin Zhang, Miao Zhang,
in Neural Information Processing Systems, pages 1â€“29, Chenjuan Guo, Bin Yang, and Christian S. Jensen. Au-
2023. toCTS+: Joint neural architecture and hyperparameter
[RamezaniandXu,2023] Aida Ramezani and Yang Xu. searchforcorrelatedtimeseriesforecasting. Proceedings
of the ACM on Management of Data, 1(1):97:1â€“97:26,
Knowledge of cultural moral norms in large language
2023.
models. In Anna Rogers, Jordan L. Boyd-Graber, and
Naoaki Okazaki, editors, Proceedings of the 61st Annual [Xueetal.,2022] HaoXue,BhanuPrakashVoutharoja,and
MeetingoftheAssociationforComputationalLinguistics, Flora D. Salim. Leveraging language foundation mod-
Toronto,Canada,July9-14,pages428â€“446,2023. elsforhumanmobilityforecasting. InProceedingsofthe
[Rasuletal.,2023] Kashif Rasul, Arjun Ashok, An- 30thInternationalConferenceonAdvancesinGeographic
InformationSystems,Seattle,Washington,November1-4,
drew Robert Williams, Arian Khorasani, George
pages90:1â€“90:9,2022.
Adamopoulos, Rishika Bhagwatkar, Marin BilosË‡, Hena
Ghonia, Nadhir Vincent Hassen, Anderson Schneider, [Yeetal.,2021] JunchenYe, LeileiSun, BowenDu, Yanjie
et al. Lag-llama: Towards foundation models for time Fu,andHuiXiong. Coupledlayer-wisegraphconvolutionfortransportationdemandprediction. InThirty-FifthCon-
ferenceonArtificialIntelligence, VirtualEvent, February
2-9,pages4617â€“4625,2021.
[Yinetal.,2021] Xueyan Yin, Genze Wu, Jinze Wei, Yan-
ming Shen, Heng Qi, and Baocai Yin. Deep learning on
trafficprediction:Methods,analysis,andfuturedirections.
IEEETransactionsonIntelligentTransportationSystems,
23(6):4927â€“4943,2021.
[Yuetal.,2018] Bing Yu, Haoteng Yin, and Zhanxing Zhu.
Spatio-temporal graph convolutional networks: A deep
learningframeworkfortrafficforecasting. InProceedings
oftheTwenty-SevenInternationalJointConferenceonAr-
tificialIntelligence,page3634â€“3640,2018.
[Yuetal.,2023] Zhongzhi Yu, Shang Wu, Yonggan Fu,
ShunyaoZhang,andYingyanCelineLin.Hint-aug:Draw-
ing hints from foundation vision transformers towards
boostedfew-shotparameter-efficienttuning.InIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,
Vancouver,BC,Canada,June17-24,pages11102â€“11112,
2023.
[Yuanetal.,2018] ZhuoningYuan,XunZhou,andTianbao
Yang. Hetero-convlstm: Adeeplearningapproachtotraf-
fic accident prediction on heterogeneous spatio-temporal
data. In Proceedings of the 24th ACM SIGKDD interna-
tionalconferenceonknowledgediscovery&datamining,
pages984â€“992,2018.
[Zhengetal.,2020] Chuanpan Zheng, Xiaoliang Fan,
Cheng Wang, and Jianzhong Qi. Gman: A graph multi-
attentionnetworkfortrafficprediction. InProceedingsof
theAAAIconferenceonartificialintelligence,volume34,
pages1234â€“1241,2020.
[Zhouetal.,2023] Tian Zhou, Peisong Niu, Xue Wang,
Liang Sun, and Rong Jin. One Fits All: Power general
timeseriesanalysisbypretrainedlm. InAdvancesinNeu-
ralInformationProcessingSystems,pages1â€“34,2023.
[Zhouetal.,2024] Zhengyang Zhou, Jiahao Shi, Hongbo
Zhang, Qiongyu Chen, Xu Wang, Hongyang Chen, and
Yang Wang. Crest: A credible spatiotemporal learning
framework for uncertainty-aware traffic forecasting. In
The 17th ACM International Conference on Web Search
andDataMining,pages1â€“10,2024.