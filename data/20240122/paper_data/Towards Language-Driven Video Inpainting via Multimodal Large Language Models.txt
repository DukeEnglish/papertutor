Towards Language-Driven Video Inpainting
via Multimodal Large Language Models
JianzongWu1,3* XiangtaiLi2,3 ChenyangSi2 ShangchenZhou2 JingkangYang2 JiangningZhang4
YiningLi3 KaiChen3 YunhaiTong1 ZiweiLiu2 ChenChangeLoy2
1PekingUniversity 2S-Lab,NanyangTechnologicalUniversity 3ShanghaiAILaboratory 4ZhejiangUniversity
ProjectPage: https://jianzongwu.github.io/projects/rovi
Abstract
We introduce a new task ‚Äì language-driven video in-
(a) Referring Video Inpainting
painting,whichusesnaturallanguageinstructionstoguide
Remove the woman on the left Running‚Ä¶
the inpainting process. This approach overcomes the limi-
(b) Interactive Video Inpainting
tationsoftraditionalvideoinpaintingmethodsthatdepend
Hello, I took a photo of my dog training session in my Certainly! The video shows two individuals and two dogs. I guess onmanuallylabeledbinarymasks, aprocessoftentedious room! Could you make the video more focus on me and you are the woman in the center, and you want to remove the
my dogswithout any distractions in the background? woman in the background. Here is the new video:
and labor-intensive. We present the Remove Objects from Implicit Requests Model Reasoning
Videos by Instructions (ROVI) dataset, containing 5,650
videosand9,091inpaintingresults,tosupporttrainingand
evaluationforthistask. Wealsoproposeanoveldiffusion-
basedlanguage-drivenvideoinpaintingframework,thefirst
Figure 1. Language-driven video inpainting. It contains two
end-to-end baseline for this task, integrating Multimodal
sub-tasksbasedontheexpressiontypes. Thereferringvideoin-
LargeLanguageModelstounderstandandexecutecomplex paintingtasktakessimplereferringexpressionsasinput,whilein-
language-based inpainting requests effectively. Our com- teractive video inpainting receives chat-style conversations. The
prehensive results showcase the dataset‚Äôs versatility and conversationmayencounterimplicitrequests,andthemodelneeds
themodel‚Äôseffectivenessinvariouslanguage-instructedin- toreasonforacorrectunderstanding.
paintingscenarios. Wewillmakedatasets,code,andmod-
task would become much easier if we could leverage nat-
elspubliclyavailable.
ural language descriptions to specify the inpainting areas,
like ‚Äúwoman on the left,‚Äù thereby preventing the need for
pixel-levelmanualannotations. Importantly, thelanguage-
1.Introduction
drivensettingcanbenefitfromtheflexibilityofnaturallan-
Video inpainting, a technique for restoring missing or cor- guage. For example, with richer sentences, one can easily
rupted segments in video frames, finds extensive applica- refer to multiple or abstract objects, which is much more
tion in areas such as video completion [4], video restora- effective than labeling masks. Extending from this notion,
tion [16], and object removal [6]. Despite continuous ad- onecoulddividethetaskintotwosubtasks,namely‚ÄúRefer-
vancements in enhancing image quality and temporal co- ring Video Inpainting‚Äù and ‚ÄúInteractive Video Inpainting.‚Äù
herenceofinpaintingresults[11,22,46,66],currentmeth- Theformertakessimplereferringexpressionsasinputs,and
ods predominantly depend on manually annotated binary thelatterconsidersmorecomplexconversation-likeinterac-
masks to identify restoration areas. This manual process tionstoaccomplishtheinpaintingtask.
is time-consuming and impractical for long videos. While To establish a baseline model for the proposed tasks, it
automatic labeling tools, such as segmentation and object is essential to have an appropriate dataset for both train-
tracking models [55, 59, 61], offer some relief, they often ingandevaluation. Currently,nopubliclyavailabledataset
necessitatemanualrefinementduetoimperfectlabeling. comprises the triplets of original videos, removal expres-
Perhaps a more natural way to perform video inpaint- sions, and inpainted videos. In response to this gap,
ing is through natural language, as shown in Fig. 1. The we build a new dataset named the Remove Objects from
VideosbyInstructions(ROVI)dataset.Specifically,weem-
*TheworkisdoneduringtheinternshipatShanghaiAILaboratory. ployreferringobjectsegmentationdatasets,whicharepre-
1
4202
naJ
81
]VC.sc[
1v62201.1042:viXra‚ÄúRemove the guy in black shorts jumping on the stairs.‚Äù ‚Ä¢ We propose a dataset to facilitate training and evalua-
tion for the proposed tasks. This dataset is the first of
its kind, containing triplets of original videos, removal
expressions, and inpainted videos, offering a unique re-
sourceforresearchinthisdomain.
‚ÄúRemove the lizard looking back at the water bowl its tail touching.‚Äù
‚Ä¢ We present a diffusion-based architecture, LGVI, as a
baselinemodelfortheproposedtask. Weshowhowone
couldleverageMLLMstoimprovelanguageguidancefor
interactive video inpainting. To our knowledge, it is the
Input InstructPix2Pix MagicBrush Ours
firstmodeltoperformend-to-endlanguage-drivenvideo
Figure2. Comparisonwithgeneralimageeditingmodels. In- inpainting.
structPix2Pix[1]andMagicBrush[63]aregeneralimageediting
methodsbasedondiffusionmodels.Theyproduceinferiorresults 2.RelatedWork
wheninstructedtoremoveobjectsinvideos.
Video inpainting. Video inpainting is a technique aimed
annotated with object masks and descriptive expressions. at restoring or filling missing or corrupted parts in a video
These datasets are further augmented with corresponding plausibly. While related to image inpainting methods [19,
inpaintedvideosgeneratedusingastate-of-the-artvideoin- 20,23,24,35,58], videoinpaintingtechniques[4,11,17,
painting model. We find existing referring expressions for 22, 26, 46, 62, 66] extend the problem to the more com-
interactive video inpainting tasks too simplistic. To ad- plexdomainofmovingvisuals. Thistechniquecanbeap-
dress this limitation, we employ Multimodal Large Lan- plied for various applications, such as object removal, vi-
guageModels(MLLMs)[3,60,67]tocreateconversation- sual restoration, and completion. With the advent of deep
like dialogues. These dialogues are designed to simulate learning,visualinpaintingnetworksusuallyemployconvo-
real-world scenarios, encompassing user requests and cor- lutional neural networks (CNNs) [11, 46, 58] and genera-
respondingmachineresponses. Thisapproachenrichesthe tiveadversarialnetworks(GANs)[19,23,24,35]. Recent
dataset, making it more representative of the complexity worksalsoapplyvisionTransformers[5,29,32,39,57]to
and variability found in practical video inpainting applica- enhance the global interaction among visual features [17,
tions. 20,22,62,66]. State-of-the-artmethodsshowstrongabil-
In addition to the dataset, we introduce the first end- itiesinrestoringmissingpartsandremovingobjects. Most
to-end baseline model, Language-Driven Video Inpainting oftheseworksrequiretheinputofabinarymasktodefine
(LGVI), for the proposed tasks. Our model is built upon therestoringarea[4,20,22,23,26,62]. However,thegen-
diffusion-basedgenerativemodels. Inparticular,weinflate erationofobject-likemasks,particularlyforlengthyvideos,
the text-to-image (T2I) model to become a text-to-video posesasignificantandlabor-intensivechallenge,
(T2V)architecturebyextendingtheparameterswithanad- Language-driven visual editing. Diffusion-based text-to-
ditionaltemporaldimension.Weproposeanefficientvisual image generation models [33, 38, 40, 41, 43] show ex-
conditioning approach that minimally increases the num- cellent abilities in generating images and videos follow-
ber of parameters. To further enhance our model‚Äôs capa- ing text guidance. Recent studies also achieve image
bilitiesfortheinteractivetask,weextendtheLGVIframe- editing [1, 9, 31, 44, 63], image segmentation [21, 49‚Äì
work to LGVI-I (Interactive). This extension incorporates 51] and video editing [13, 52] with DMs. Among these,
anMLLMspecificallydesignedtoprocessandunderstand Prompt2Prompt [9] manipulates the cross-attention maps
user requests phrased in a conversation-like format. The withinthemodeltoenablevariouseditingoperationssuch
LGVI-Imodelistrainedinanend-to-endmanner. Thisin- as object modification, addition, and style transfer. In-
teractive architecture enables the system to interpret com- structPix2Pix [1] leverages this approach to create an im-
plexinstructionsaccurately. Asaresult,itcanproduceap- ageeditingdataset. Similarly,Tune-A-Video[52]proposes
propriate inpainting results and relevant responses within atraining-freearchitecturetoeditvideosbylanguagerefer-
theconversationalcontext,thuspavingthewayformorein- ences.However,theseworksareintendedforgeneralvisual
tuitive and flexible user interactions with interactive video editing. Whenappliedtomorespecificchallenges,suchas
inpaintingsystems. language-drivenvideoinpainting,theytendtoyieldsubop-
Insummary,ourkeycontributionsareasfollows: timal results. Figure 2 shows two examples where these
‚Ä¢ We introduce a novel language-driven video inpainting models produce inferior results when instructed to remove
task, significantly reducing reliance on human-labeled objects. A few works have explored the image inpainting
masksinvideoinpaintingapplications.Thistaskincludes taskusingDMs.Repaint[30]stilltakestheimageandmask
twodistinctsub-tasks: referringvideoinpaintingandin- asinputandletstheDMrestoretheoriginalimage. Smart-
teractivevideoinpainting. Brush [53] takes mask and text as input to guide a region-
2Table1. ComparisonbetweentheROVIdatasetandrelateddatasets. Wechoosetwocommonlyusedimageinpainting(II)datasets,
twovideoinpainting(VI)datasets,andonelanguage-guidedimageinpainting(LII)dataset. OurROVIdatasetisthefirstforlanguage-
guidedvideoinpainting(LVI)andinteractivevideoinpainting(IVI)tasks.
Annotations
Dataset Task Scene #Images #Videos #Frames #Objects #Exprs #Chats Source
mask expr inpaint chat
Places[65] II Buildings&Places 10,624,928 - - √ó √ó √ó √ó - - - -
CelebA[28] II HumanFace 202,599 - - √ó √ó √ó √ó - - - -
YouTube-VOS[54] VI General - 4,453 197,272 ‚úì √ó √ó √ó 7,755 - - YouTube
DAVIS[36] VI General - 50 3,455 √ó √ó √ó √ó 3,455 - - -
GQA-Inpaint[56] LII General 49,311 - - ‚úì ‚úì ‚úì √ó 97,854 107,252 - GQA[12]
ROVI LVI+IVI General - 5,650 247,018 ‚úì ‚úì ‚úì ‚úì 9,091 12,534 9,091 Refer-YouTube-VOS[42]+A2D-Sentences[7]
A2D-Sentences Requests Responses
train 2967 I took a picture of a parked bus with a surfing event advertisement Sure, I can help you with that. This image shows a parked bus with a surfing
1 on its side. I want to remove a specific car in the background that event advertisement on its side. There is a black car in the background that
seems out of place. Could you help me by getting rid of the black seems out of place. I will remove the black car behind another black car for you.
car behind another black car? {{prompt}} remove the black car behind the other black car {{/prompt}}
2715
I love the photo of the panda in the natural setting, but there's Sure, I can modify the image to show the panda sitting in the natural setting
252 test 2 s po am nde ath juin sg t so if tf s a thb eo ru et ait n e da rt ein lag x. eC so wul id th y oo uu t p el ae ta inse g m ana yk te h ii nt ga ?s if the without eating anything. Here's the updated prompt:\n\n{{prompt}
226 test
3 Can you modify this image? I want an empty enclosure but with the Sure, here is the new image: {{prompt}} remove the giant gorilla sitting in a
same attractive background. caged area chewing on a stick and breaking it {{/prompt}}
2457
4 The frisbee seems to interrupt the harmony of the beach scene. Can Sure, here is the new image: {{prompt}} remove the frisbee being
train you make it look like a harmonious gathering of people on the beach? thrown {{/prompt}}
Refer-YouTube-VOS 5 This picture would probably look better without that person walking Sure, I can help you remove the person walking in black. They are located in
2683 in black. Could you help me remove them please? t wh ae l km inid g d inle b o laf ct kh e {{ i /m pra og me. p T th }}e new video will be: {{prompt}} remove the person
(a)Numberofvideosandsplit. (b)Wordcloudofexpressions. (c)Examplesofinteractiverequestsandresponses.
   
   
   
 U H I H U U L Q J  H [ S U H V V L R Q
         L Q W H U D F W L Y H  U H T X H V W
   
   
     L Q W H U D F W L Y H  U H V S R Q V H
   
   
                             
   
                      
   
                         
 5 H O D W L Y H  V H J P H Q W D W L R Q  P D V N  V L ] H  5 H O D W L Y H  P R W L R Q  1 X P E H U  R I  Z R U G V  L Q  V H Q W H Q F H V
(d)Relativesegmentationmasksize. (e)Relativeobjectmotiondistribution. (f)Sentencelengthsdistribution.
Figure3.TheROVIdatasetstatistics.Bestviewedincolor.
controlled generation on the masked area, which aims to 3.ROVIDataset
generate new concepts rather than remove the object. Re-
3.1.ComparingwithExistingDatasets
cently, Inst-Inpaint[56]proposesamethodto performob-
jectremovalonimagesbasedonthelanguagedescriptions.
Table1summarizesthedifferencesbetweenROVIandre-
Despiteitsinnovativeapproach,Inst-Inpaint‚Äôstrainingsam-
lated datasets. In image and video inpainting research,
plesareconstrainedbyalackofinteractiveexpressionsand
prevalent training and evaluation datasets mainly include
video resources, which limits its practical effectiveness in
vision-centriconeslikePlaces[65],CelebA[28],YouTube-
complexscenarios.
VOS [54], and DAVIS [36], without human annotations.
Multi-Modal Large Language Models. Large language These datasets typically employ random masking in train-
models (LLMs) have demonstrated exceptional perfor- ing to simulate missing areas for inpainting. However, for
mance across a variety of text-based tasks and applica- objectremovaltasks, specificallylabeledmasksareessen-
tions [2, 3, 8, 18, 34, 45]. Recent works extend the capa- tial. While YouTube-VOS provides object masks, it lacks
bilitiesofLLMstoincludeimageprocessingandcomputer corresponding inpainting ground truths. The GQA-Inpaint
vision. AnotableexampleisLLaVA[25],whichtranslates dataset[56],althoughrichinobjectexpressionsandinpaint-
image tokens into a language feature space, thereby trans- ingresults,islimitedtoimagedataanddoesnotaccommo-
forming the fine-tuned model into an MLLM. This adap- date video or interactive contexts. Our ROVI dataset ad-
tation enables MLLMs to interpret and understand visual dresses these limitations with comprehensive annotations
content. Subsequent research uses MLLMs in diverse ap- covering a wide array of regions, including object masks,
plications,includingvisualreasoning,objectdetection,and referring expressions, inpainting results, and conversation-
segmentation [8, 18, 25, 37, 60, 67]. To the best of our likedialogues. UnlikePlacesandCelebA,whichfocuson
knowledge, this study is the first to integrate MLLMs into specificimagecategorieslikebuildingsorfaces,ROVIen-
thedomainoflanguage-drivenvideoinpainting. compassesabroaderspectrumofgeneralscenes,makingit
3
 V W F H M E R  I R  W Q H F U H 3  V W F H M E R  I R  U H E P X 1  V H F Q H W Q H V  I R  H J D W Q H F U H 3‚ûäSample Video Data ‚ûãPropagate Mask to Every Frame Object Tracking Model ‚ûéGet Video Description with MLLM
<one frame from video> describe this image
Referring Video
Segmentation The image features a young man skillfully
Dataset riding a skateboard on a concrete floor. He is
wearing a white shirt and black shorts, and he
appears to be enjoying his time skateboarding.
ÔÅ∞ mask The skateboard is positioned underneath him,
ÔÅ∞ video with its wheels visible on the ground
ÔÅ∞ expression man riding a skateboard Video Inpainting Model ‚ûèGet User Request with LLM
‚ûåProduce Inpainting Results with different hyper-parameters ùëë <description> <object to be removed>
Please generate a user request
ùëë=0 ùëë=3 ùëë=5 ùëë=15
Could you provide an image of just an empty
concrete floor and skateboard without the
¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑
skateboarder? I want to use it as a background
for a project.
‚ûêGet Response with MLLM
‚ûçSelect and Revise Results by Humans Human Selection
<request> <one frame from video>
Please generate a response with prompt
Sure, the video depicts a skateboarder boarding
on the floor. I will remove the skateboarder for
you. Here is the new image: {{prompt}} remove
remove the man riding a skateboard Correct Inaccurate Expressions the man riding a skateboard {{/prompt}}"
Figure4.ROVIdatasetannotationpipeline.ThebuildingprocessoftheROVIdatasetinvolvestwodistinctphases:inpaintingannotation
and interactive annotation. In the inpainting annotation phase, the primary objective is to incorporate inpainting results into existing
referringvideosegmentationdatasets,whichinitiallycontainobjectmasksandexpressions.Duringtheinteractiveannotationpipeline,we
followamulti-stepapproachincorporatingLLMsandMLLMs.Bestviewedincolor.
moreadaptablefordiverseinpaintingapplications. choose E2FGVI [22], a state-of-the-art video inpainting
model, to produce the inpainting results. This model,
3.2.DatasetStatistics
trained on video data, guarantees temporal consistency in
Figure3presentsacomprehensivestatisticalanalysisofthe theinpaintingresults.
ROVIdataset. Thedatasetencompasses2,967videosfrom
Tofurtherensurethegroundtruthisofhighquality,we
A2D-Sentences and 2,683 videos from Refer-YouTube-
incorporate a human selection process on the hyperparam-
VOS,dividedintotrainandtestsplits,asshowninFig.3a.
eteroftheinpaintingmethod. Specifically, theinputmask
Figure 3b illustrates the diversity of referring expressions
can be expanded with different pixel sizes, denoted as d.
withwordclouds. Figure3cshowsseveralexamplesofour
Thebiggerthedis,thelargertheinputmaskisdevelopedso
dataset‚Äôsinteractiverequestsandresponses,showingthedi-
thatitmaycoverthewholeobject. Thebestdvaluevaries
versity and complexity of the dialogues. Figure 3d details
throughobjects,causinganunstableperformanceinthein-
therelativesizesofsegmentationmasks(maskareadivided
painted videos if set to a fixed value. Therefore, through-
byimagearea). Wedropobjectswitharelativesizelarger
outthedatagenerationprocess,weexperimentwithvarious
than 0.25 because the inpainting results for large objects
hyperparameterstogeneratemultipleresultsforeachobject
usually have worse qualities. Figure 3e analyzes the dis-
andinvolvehumanannotatorstoselectthebestresult.More
tributionofobjectmotion.Figure3fdeliversahistogramof
detailsareprovidedinAppendixA.
sentencelengthswithinthedataset.
For interactive annotations, we need to collect expres-
3.3.DatasetConstructionPipeline
sionsthroughchat-styleconversations. Unlikethestraight-
Videodataselection.AsdepictedinFig.4,wehavechosen forward ‚Äúremove‚Äù sentences, these interactive requests
referringvideoobjectsegmentationdatasetsforthesource should be implicit, necessitating the model to discern the
of video data. Referring video object segmentation aims user‚Äôs underlyingintent. Rather than relyingsolely on hu-
to segment an object referred to by a given language de- man annotators to articulate these requests, we explore a
scription. These datasets have pre-annotated object masks moreautomatedapproach: weemployLLMsandMLLMs
and descriptive expressions, making them well-suited for to simulate a human user and generate potential requests
the proposed task. Specifically, we select Refer-YouTube- andresponses. Weproposeamulti-stepapproachwithde-
VOS[42]andA2D-Sentences[7]asourdatasources. tails illustrated in Fig. 4. By employing this dual-faceted
Annotation pipeline. We use a video inpainting model annotation pipeline, the ROVI dataset is enabled to handle
to generate the inpainting ground truth. Specifically, we complexuserrequests.
4ùêò Inflated U-Net
‚Ñ∞ d d As io Ne v n o
C
dem ca os dk er
‚Ñí ‚Ñí
ùëëùëñùëìùëì ùëöùëéùë†ùëò
ùêó
v ùíü
‚Ñ∞ n
o
C
Referring Expression
remove thedolphin on the left Spatial Cross Temporal
FFN
Attention Attention Attention
LGVI-I CLIP Text Encoder
IMG Embed
Multimodal Large LM MM FPN-like Mask Decoder
‚Ñí
Language Model Head ùëôùëö Head
Temporal Up
Embed Conv Conv
Conv Sample
Interactive Chat Response
I have a video of two keepers feeding Certainly! There are two persons and one
a dolphin. I want to keep the keepers dolphin, I will remove the dolphin. The new Concatenate with Next Feature Map
only, could you help me with that? video is:<PROMPT> </PROMPT> Sharing Weights Frozen Tuning
Figure 5. The training process of LGVI and LGVI-I. We inflate the U-Net with a temporal dimension to allow video input. To
ensuretemporalconsistencyinthegeneratedvideos,weintroduceatemporalattentionmodulebetweencross-attentionandFFNlayers.
Additionally,weproposeamaskdecodermoduleforexplicitguidanceininpaintingtasks. WeaugmentLGVIwithMLLMjointtraining
forinteractivevideoinpainting,resultinginLGVI-Iasthebaseline. TheoutputofMLLMincludesasetofprompttokens,whichisfed
intothecrossattentionoftheU-Net.
4.Methodology TheTemporalAttentionmoduleisformulatedasfollows:
In this section, we introduce our Language-Driven Video
Inpainting framework (LGVI) and the MLLM-enhanced Q=W qv, K=W kv, V=W vv,
LGVI-I(Interactive)architecture. QKT (1)
Attention(Q,K,V)=Softmax( ‚àö )¬∑V,
C
4.1.LGVI
whereW ,W ,andW arelearnablematricestoproject
q k v
theinputstoquery,key,andvalue.Thecomputationalcom-
The LGVI framework is shown in Fig. 5, which is built plexityoftheTemporalAttentionmoduleisO(CT2),while
onthearchitectureofStableDiffusion[40]. Toextendthe spatialself-attentionhasacomplexityofO(CD2).Consid-
framework to video inputs, we perform temporal inflation
ering T ‚â™ D. The Temporal Attention module is a time-
byreorganizingthenetwork‚Äôsstructurefollowing[43,52].
efficient tool to ensure the consistency of output video se-
For a batch video input with T frames, denoted as X ‚àà
quences.
RB√óT√óH√óW√ó3,whereBisthebatchsize,andH√óW are
thesize,wetransposethetensortoX‚ààR(B√óT)√óH√óW√ó3. Diffusion models learn to gradually remove noises in a
Thistransformationconvertstheinputintoa4-dimensional noisedvideo. Duringtraining, thetargetvideoY isadded
image batch input format. The pre-trained 2D networks withnoisestobethestartpointofthenoisedvideo.Besides
can process video clips as they are separate images. Ad- thenoisedtargetvideoinput, LGVIalsotakestheoriginal
ditionally,weintroduceaparameter-efficientTemporalAt- video X as a control signal input. Concretely, we encode
tention module positioned between the cross-attention and theoriginalvideoXtothelatentspaceandconcatenateits
FFN network. Given latent feature v ‚àà R(B√óT)√óD√óC, feature with the noised target video in the channel dimen-
where D is the length of patched visual tokens, and C is sion. Note that the noise is added only to the target video
the channel size, we transpose it to v‚Ä≤ ‚àà R(B√óD)√óT√óC. latent, and during testing, the noised target video is a ran-
5domlygeneratednoise. Table2. Quantitativeresultsonthereferringvideoinpainting
task.E‚àó denotesE (√ó10‚àí2).
warp warp
z =E(Y), c =E(X),
0 x
Method PSNR‚Üë SSIM‚Üë VFID‚Üì E‚àó ‚Üì
z =AddNoise(z ), (2) warp
t 0 ImageModels
v t =Conv v(z t)+Conv x(c x), InstructPix2Pix[1] 18.12 0.600 0.361 1.343
Inst-Inpaint[56] 19.00 0.896 0.310 1.206
where E is the pre-trained VAE encoder, t is the sampled MagicBrush[63] 20.39 0.725 0.310 0.934
Multi-StageVideoModel
timestamp,Conv andConv areconvolutionallayerswith
v x InpaintAnything*[59] 22.84 0.728 0.283 0.874
3 √ó 3 kernels to transfer the latent codes into U-Net fea-
One-StageVideoModel
ture dimensions. The initial weights of Conv x are set to LGVI(Ours) 22.85 0.756 0.308 0.901
all-zero. Thistechniqueallowsthemodeltoaddvideocon-
ditionguidanceduringtraining.Duetomaskannotationsin
the ROVI dataset, we can leverage masks as an additional canbesummarizedasfollows:
supervisionsignalinourLGVIframework. Concretely,we
e =f(s), e =W ¬∑g(X ),
implement a mask decoder to predict the object‚Äôs mask in l i trans 0
the video that needs to be inpainted or removed. This de- h=MLLM([e l,e v]),
(4)
coderusestheoutputsfromU-Netup-blocksandconsistsof wÀÜ =W ¬∑h,
lm
convolutionalandtemporalconvolutionallayers.Theuseof
h =W ¬∑find prompt(wÀÜ,h),
p mm
masksupervisionenablesthemodeltofocusontheregion
describedinthenaturallanguageinput,therebyfacilitating where f is the language token embedding and g is a pre-
precise andtargeted inpainting. Theeffectiveness of mask trainedimagebackbonetoextractimagefeatures. W
trans
supervisioncanbeseeninSec.5. Thetrainingobjectiveof isalinearlayerthattransformsimagefeaturesintolanguage
LGVIis: tokenspace.histhelastlayer‚ÄôshiddenstatesoftheMLLM.
wÀÜ is the predicted language token distribution through the
L diff =E X,œµ‚àºN(0,I),t(cid:2) ||œµ‚àíœµ Œ∏(v t,c,t)||2 2(cid:3) , LMhead. W lm istheweightsoftheLMhead. Amongthe
L =CrossEntropyLoss(MÀÜ,M), (3) predicted words, we use find prompt function to find the
mask
<PROMPT> and </PROMPT> indicator and extract the
L=Œª L +Œª L ,
1 diff 2 mask hidden states that lie between these two indicators. W
lm
is the weights of MM head. The MM head transfers the
whereL andL arethediffusionmodeltrainingob-
diff mask selected tokens into h , which is then fed into the U-Net
jectiveandmaskloss,respectively;cisthelanguageguid- p
ancefeaturesfromthereferringexpressions;MÀÜ isthemask cross-attentionmodule. Inthisprocess,h pservesasvision-
aware language guidance for the inpainting process. The
predictionandMisthegroundtruthmask. Theparameters
trainingobjectiveofLGVI-Iis:
Œª andŒª arelossweightstobalancetraining.
1 2
L =E (cid:2) ||œµ‚àíœµ (v ,h ,t)||2(cid:3) ,
4.2.LGVI-IwithMLLM diff X,œµ‚àºN(0,I),t Œ∏ t p 2
L =CrossEntropyLoss(wÀÜ,w), (5)
lm
Intheinteractivevideoinpaintingtask,modelsareexpected L=Œª L +Œª L +Œª L ,
1 diff 2 mask 3 lm
to extract valuable information from complex conversa-
tions. Toovercomethisproblem,weproposeincorporating whereh istheMLLM-enhancedlanguagecondition,L
p lm
MLLMs to extend the LGVI from work to LGVI-I (Inter- is language modeling loss, implemented as the Cross-
active). MLLMs have demonstrated strong capabilities in Entropy Loss, w is the ground truth sentence, and Œª is
3
visual comprehension and multimodal reasoning, making thelossweightforlanguagemodelingloss. Byintegrating
themwell-suitedforourproposedinteractivevideoinpaint- anMLLMintotheLGVIframework,thesystemachievesa
ing task. As shown in Fig. 5, the MLLM takes both the higherlevelofuserinteractivity.Thisenablesuserstoguide
imageframeandthechat-styleuserrequestasinputs,gen- the visual inpainting process with interactive language in-
erating the language response and a pair of special indica- structions,thusestablishingamoreuser-friendlyandacces-
tors:<PROMPT>and</PROMPT>.Thehiddenstatesof sibleapproachfortheinteractivevideoinpaintingtask.
thelastlayerbetweenthesetwoindicatorsarethenpassed
through an MM head, implemented as a two-layer linear 5.Experiments
layer with activation functions. The transformed features
5.1.Settings
arefedtothecross-attentionmoduletoguidetheU-Netin-
paintingprocess. Mathematically,giventheinputvideoX Datasets and metrics. We use the ROVI dataset test set
anduserrequests,thecomputationpipelineoftheMLLM forboththereferringvideoinpaintingandinteractivevideo
6Table 3. Results on interactive video inpainting task. E‚àó
warp
denotes E (√ó10‚àí2). MB represents MagicBrush, and IA* inpainting tasks. The test set contains 478 videos and 758
warp
represents Inpaint Anything*. The small numbers on the top 5 objects, each equipped with one referring expression and
rowsarecomparedwiththereferringvideoinpaintingresults.The oneinteractiveexpression. Duringthetrainingofourmod-
smallnumbersonthelastrowarecomparedwithLGVI. els, we incorporate a referring image inpainting dataset,
GQA-Inpaint [56], to enrich the data vocabulary. We fol-
Method PSNR‚Üë SSIM‚Üë VFID‚Üì E‚àó ‚Üì low video inpainting works [22, 26, 62, 66] to use PSNR
warp
ImageModels and SSIM [48] to assess the statistical similarity between
InstructPix2Pix[1] 16.53(-1.59) 0.558(-0.042) 0.391(-0.003) 1.789(-0.446)
predicted results and ground truth. Additionally, we use
Inst-Inpaint[56] 18.96(-0.04) 0.702 0.314(-0.004) 1.047
MagicBrush[63] 20.46 0.728 0.311(-0.001) 0.901 VIFD[47]tomeasuretheperceptualsimilarities. Toassess
Multi-StageVideoModel the temporal consistency and smoothness of the generated
IA*[59] 20.64(-2.20) 0.664(-0.064) 0.312(-0.029) 1.182(-0.308)
videos,wealsoapplytheE metric[15].
One-StageVideoModel warp
LGVI(Ours) 20.70(-2.15) 0.707(-0.049) 0.332(-0.024) 1.191(-0.290) Baselines. For the baselines, we select three language-
MLLM-EnhancedTwo-StageModel driven image editing methods: InstructPix2Pix [1], Inst-
MB+MLLM 20.37 0.726 0.313 1.004
Inpaint [56], and MagicBrush [63]. It is worth noting that
IA*+MLLM 21.37 0.722 0.291 0.875
LGVI+MLLM 21.45 0.738 0.311 0.923 InstructPix2Pix and MagicBrush are pre-trained on exten-
MLLM-EnhancedEnd-to-EndModel siveimageeditingdatasets. Inst-Inpaintisproposedtoper-
LGVI-I(Ours) 22.24(+1.54) 0.732(+0.025) 0.299(+0.033) 0.867(+0.324)
form referring image inpainting on images. We also com-
pare with Inpaint Anything, a multi-stage method for one-
‚ÄúRemove the cat on the right.‚Äù
clickvideoinpainting. ItusesSegmentAnything[14]and
tu OSTrack[55]toproducesegmentationmasksbasedonuser
p
n I click,followedbyinpaintingthemaskedareausinginpaint-
h ingmodels[61]. WeimplementInpaintAnything*, which
su
rB facilitiesInpaintAnything[59]withGroundingDINO[27],
c
ig a enablingittoprocesslanguageinputs.
M
Implementation details. We initialize the U-Net weights
sru fromMagicBrush[63]. Thenewlyintroducedmodulesare
O
trainedfromscratch. Duringtraining,wesamplevideoand
‚Äúremove the man in the white outfit hopping over some exercise balls‚Äù image data at a ratio of 3 : 1. For the MLLM, we adopt
LLaVA-7B[25]. Thelearningratesare3e-5,1e-4,and1e-4
tu
p forU-Net,maskdecoder,andtunedparametersinMLLM,
n
I respectively. The loss weights are set to Œª = 1, Œª =
1 2
h su 0.001, Œª
3
= 0.1. Theseweightsdiffersignificantlydueto
rB
c the different types of losses they represent. The input and
ig
a M outputvideosizesaresetto512√ó320,andthevideolength
is 24. For LGVI, we train 50 epochs on the ROVI dataset
sru
O withabatchsizeof32forvideosand768forimages. For
LGVI-I, we load the LGVI checkpoint and fine-tune it for
50 epochs under the same batch size. All experiments are
Figure 6. Qualitative comparison between LGVI and Mag-
carriedouton8NVIDIAA100GPUs.
icBrush[63]onthereferringvideoinpaintingtask.
Referring multiple objects: ‚ÄúRemove the dog andthe girl.‚Äù
5.2.ReferringVideoInpainting
Quatitative results. We report quantitative results on the
referring video inpainting task. Compared with baseline
models, our model is the first one-stage language-driven
Referring nonexistent object: ‚ÄúRemove the giraffeon the snow.‚Äù
video inpainting model. As shown in Tab. 2, our model
outperforms MagicBrush [63] in all metrics and achieves
on-parresultswithInpaintAnything*[59], evenifInpaint
Anything* uses a mask-based inpainting model [61]. The
Input InpaintAnything* Ours resultsdemonstratetheeffectivenessofourmodel.
Qualitative results. Figure 6 shows qualitative results.
Figure7.Examplesofreferringtomultipleobjectsinonesentence We compare with MagicBrush [63], a robust generalized
andreferringtononexistenceobjects.
language-driven image editing model. In the first exam-
ple, where the language refers to the cat on the right, the
7‚ÄúCan you make it seem like the bird on the right was alone on the blinds?‚Äù
I have a perfect action shot of a tennis player, but my friend was also caught in the frame. Could you help me to create an image where only the player who is about to hit the ball is visible, please?
Can you help me edit this picture? I'd prefer the image if it showed just the man preparing to kick but without the ball in the frame.
Could you make the highway appear less crowded? I'd love to see it without the white sports car in the middle.
Input Frame InpaintAnything* (Mask) Inst-Inpaint InstructPix2Pix MagicBrush LGVI-I (Ours)
Figure 8. Qualitative comparison between LGVI-I and baseline models on the interactive video inpainting task. The chat-style
conversationinputsarelistedaboveeachrow. Columns2and3aretheresultsandpredictedmasksfromInpaintAnything*. Itremoves
theinaccurateobjectsaccordingtothewronglypredictedmasksduetothedifficultinteractivelanguageinputs.
‚Äúremove the man running on the track‚Äù tences referring to multiple objects or nonexistent objects.
Inpaint Anything is driven by a simple combination of re-
tupnI
ferringsegmentationandvideoinpaintingmodels. Thus,it
isfixedtoproduceonemaskforeachsentence.Whenrefer-
egam ringtomultipleornonexistentobjects,itoutputsinaccurate
I o/w results, while our model produces the correct output. This
demonstratestherobustnessofthelanguage-drivensetting.
ksaM
o/w
5.3.InteractiveVideoInpainting
enuT
lluF
o/w
Q
p wa
hu
in
ea ntt ii nt ta
g
ht eti av mse
k
or
r
de
e
es
s
lu
u
sl lt
t
ass r.
ein
tW
T
raae
ib
n.r ee
3
dp .o uAr st
s
int sh ghe
o rw
ein
fn
ete
ri
rr
n
ia nc
t
ghti ev ee
t xo
pv
p
ri ed
5
se sro
io ow
nin
s
s-
,
,
theirperformancedropscorrespondinglyinthistask. This
sruO isintuitivebecauseinteractiveexpressionsaremuchlonger
and more implicit. For the MLLM-Enhanced Two-Stage
Models, we combine the baseline models with an MLLM
Figure 9. Ablations. We ablate makes supervision, image joint inazero-shotmanner.Theinteractiveinputsaretransferred
training, and fine-tuning the whole U-Net. Our model produces into shorter referring expressions by simply prompting the
resultswiththehighestvisionandlanguagecoherence.
MLLM.Thesemodelsexhibitimprovedperformances.Our
LGVI-I model achieves the highest performance, demon-
MagicBrushmodelremovesallthecatsinthescene,while stratingtheeffectivenessoftheproposedarchitecture.
our model successfully inpaints the right cat. In the sec- Qualitativeresults. Figure 8presentsexamples ofthe in-
ondexample,thereferringexpressionbecomesmorecom- teractive video inpainting task. The user requests pose a
plex. MagicBrush struggles to identify the object requir- significantchallengeandcomplexityforthebaselinemod-
ing inpainting and removes the wrong object (the balls) in els to comprehend. In particular, Inpaint Anything* pre-
the last frame. In contrast, our model generates a plausi- dicts incorrect masks, leading to inaccurate results. Simi-
ble output, demonstrating its superior performance in han- larly,otherdiffusion-basedmodelsstruggletointerpretthe
dling complex language-driven inpainting tasks. Further- users‚Äô intentions accurately, resulting in less satisfactory
more,inFig.7,wecomparewithInpaintAnything*onsen- outcomes. Incontrast,ourLGVI-Imodel,whichharnesses
8the power of MLLM, consistently delivers the best perfor-
manceinthesechallengingscenarios. Thisunderscoresthe
superiorityofourproposedapproach.
5.4.AblationStudy
We conduct three ablations involving mask supervision,
fine-tuningtheentireU-Net,andjointtrainingwithimages.
The benefit of mask supervision. The models without
mask supervision rely solely on the inpainting ground for
guidance,lackinganexplicitsignaltodirecttheinpainting
area. AsshowninFig.9,therunningmanremainspresent
inallframes. Notably,althoughweutilizemaskannotation
intheROVIdatasetfortraining,theLGVIframeworkdoes
notneedmaskinputduringinference.
Thebenefitoffine-tuningthewholeU-Net. Wefine-tune Figure10. Thehumanannotationinterface. Humanannotators
thewholeU-Netinsteadofonlyadjustingtheparametersof clickthebestamongthesixresultstoselecttheresultsorchoose
newmodules. AsshowninFig.9, limitingthefine-tuning ‚ÄúNo one is qualified‚Äù to drop them all. The dialogue box in the
toonlythenewmoduleshinderstrainingconvergence,and middleisusedtocorrectmis-annotatedexpressions.
themodelstrugglestooutputexpectedresults.Thisdemon-
stratesthenecessityoftuningthewholemodel. Futurework. Apromisingdirectionistoresolveambigu-
The benefit of joint training with images. As shown itiesinlanguageinputs,possiblybyusingcontextualclues
inFig.9,themodelwithoutjointtrainingproducesoutputs fromthevideoorpreviouslanguageinputs. Inaddition,re-
with heavier artifacts than ours. This is because enlarging searching methods to optimize these models for real-time
thedatasetbringsmorediversityofobjectsandscenestothe videoinpainting,couldbevaluableforlivebroadcastingor
model. Itdemonstratestheeffectivenessofjointtraining. interactivemedia. Anotherimportantfutureworkisincor-
porating interactive user feedback mechanisms that allow
6.Conclusion,Challenges,andOutlook the system to learn from corrections or preferences indi-
cated by users, thereby improving the accuracy and rele-
In this paper, we propose a novel language-driven video
vance of the inpainting results over time. See supplemen-
inpainting task that uses language to guide inpainting ar-
taryformorediscussions.
eas. Fortrainingandevaluation,wecollectavideodataset,
Potential social impacts. This technology could poten-
namely ROVI. Comprehensive statistics demonstrate the
tiallyboostcreativefieldssuchasfilm-making,advertising,
uniquenessanddiversityofourdataset,especiallythechat-
and content creation. It allows for more seamless editing
styleinteractiveconversationsgeneratedbypowerfulLLMs
and creative storytelling, enabling creators to modify and
andMLLMs.Wefurtherproposeadiffusion-basedbaseline
enhance their visual narratives easily. It also comes with
model,LGVI.Quantitativeandqualitativeexperimentalre-
negative impacts. For example, it can be used in creating
sultsshowtheeffectivenessandrobustnessofourmodel.
misleadingorfalsemediaandethicalormoralissues.
Challenges. (1) Handling Ambiguity in Language De-
scriptions. Language-driven video inpainting relies heav-
ily on the accuracy and clarity of language inputs. Ambi-
7.Appendix
guities or vagueness in language descriptions can lead to
inaccuracies in inpainting results. Developing models that
Overview. Oursupplementary includesthe followingsec-
can intelligently handle or clarify ambiguous language in-
tions:
putsisasignificantchallenge. (2)Real-TimeProcessing.
‚Ä¢ AppendixA.Detailsforourdatasetannotationprocess.
Videoinpaintinginareal-timesetting,especiallywithcom-
‚Ä¢ AppendixB.Implementationdetailsofthebaselinemod-
plexlanguage-driveninputs,iscomputationallydemanding.
els.
Diffusion-basedmodelsalsoexperiencetheslowinference
‚Ä¢ AppendixC.Quantitativeablationstudyresults.
problem due to the Markov denoising process. Improv-
‚Ä¢ AppendixD.Morequalitativeresultsofdifferentmodels.
ingthespeedandefficiencyofthesemodelswithoutcom-
‚Ä¢ AppendixE.ThefoundationsofLatentDiffusionModels
promising accuracy is a crucial challenge. (3) Scalability
andcorrelationwithourmodel.
andGeneralization.Anotherchallengeisensuringthatthe
‚Ä¢ AppendixF.Discussionsoflimitationsandchallenges.
modelgeneralizeswellacrossvariouslanguagesandvideo
types. Modelsmightperformwellonthedatasettheywere VideoDemo. Wealsoincludethevideointroductionofour
trainedonbutstrugglewithnew,unseendata. work,whichshowsthevisualizationdemo.
9System Prompt: A chat between a curious user and an artificial System Prompt: You are InpaintQuest Data Generation Helper, an
intelligence assistant. The assistant gives helpful, detailed, and intelligent AI assistant that can generate prompts to simulate user's
polite answers to the user's questions. requests. The requests are intended to remove a specific object in an
image. But the object to be removed may not be inferred explicitly in
the request.
User: <a frame from video>Please describe this image.
According to the description of the image and the given object to be
removed, you should generate a possible user request in the format:
<request> a request user may ask </request>
Here are some examples:
MLLM 1.
Input:
image: The image features a white dog and a grey dog standing on a
wooden floor, both of them eating food off the ground. The white dog
is positioned towards the left side of the image, while the grey dog is on
The image features a man and a woman standing under an the right side. The dogs appear to be enjoying their meal together in a
umbrella, likely at a street fair or outdoor event. The man is cozy setting.
object: the white dog
holding a cup, possibly filled with a beverage, while the woman is
AI:
standing nearby. They are both dressed in striped shirts, adding a
<request> I take a photo of my two dogs eating food. I want to split this
touch of color to the scene. photo to show each of them. Could you give me the photo with only
the white dog? </request>
In the background, there are several potted plants placed around 2.
the area, adding to the festive atmosphere. A vase can also be Input:
seen, possibly as part of the decoration or display at the event. image: The image shows a man standing in a chair, putting his hands up.
The background is grassland and blue sky.
object: the chair the man is standing on
Figure 11. The MLLM prompts in generating image de-
AI:
scriptions. Systemprompthereisthedefaultsystempromptof <request> Can you let the person fly in the air? </request>
LLaVA[25] 3.
Input:
image: The image features a man lying on the ground in a park, possibly
playing or relaxing. He is wearing blue pants and is positioned in the
A.DatasetAnnotationDetails grass. The park is surrounded by trees, providing a natural and serene
atmosphere. In the background, there are several people scattered
throughout the scene, some standing and others walking. A bicycle can
A.1.HumanAnnotation be seen parked near the middle of the park. Additionally, there are two
cars parked in the background, one closer to the left side and the other
towards the right side of the image.
Whengeneratinginpaintingresultsusingavideoinpainting object: the crawling man
AI:
network[22],theinputmaskcanbetrickilyexpandedwith
<request> What is strange in this image? Can you help me remove it for
differentpixelsizes,denotedasd.Thebiggerdis,thelarger me? </request>
theinputmaskisdevelopedsothatitmaycoverthewhole
User: <frame description> the white bucket being handed to someone
object. Thebestdvaluevariesthroughobjects,causingan
unstableperformanceintheinpaintedvideosifsettoafixed
LLM
value.Therefore,throughoutthegenerationprocess,weex-
perimentwithvarioushyperparameterstogeneratemultiple Can you help me with this photo? I bothered that there is
resultsforeachobjectandinvolvehumanannotatorstose- a white bucket being handed over in this picture. I don't
want it to be seen. Can you remove it?
lect the best result. In particular, we generate six samples
withd ‚àà [0,3,5,7,10,15]foreachobject. Humanannota-
Figure12.TheLLMpromptsingeneratinguserequests.
torsareexpectedtochoosethebest-lookingresultinthese
examples. TheobjectdoesnotentertheROVIdatasetifall
examples are evaluated as unqualified. This human label-
ingprocessguaranteesthehigh-qualitygroundtruthofthe A.2.InteractiveAnnotationDetails
ROVI dataset. Fig. 10 shows an illustration of the human
annotationinterface.
In the interactive annotation pipeline, all the generating
Additionally,wefindthereareseveralmisannotationsin processesarecompletedbypromptingMLLMsandLLMs
theRefer-YouTube-VOSdataset.Insomevideos,theobject withoutfine-tuning. Firstly,weletanMLLMmodelgener-
labelmatcheswronglywithobjectmasksandexpressions. ate a detailed description of a given frame. Then, we give
For example, a person may correspond to ‚Äúa dog walking the descriptions to an LLM and let it generate a possible
byaperson‚Äùaccordingtotheobjectlabels. So,humanan- user request. Finally, the MLLM generates AI responses
notators also make necessary revisions on the expressions accordingtotherequestandframecontent. Thepipelineis
incaseofmis-annotations. showninFigs.11to13.
10C.MoreAblationStudies
System Prompt: You are InpaintHelper, an intelligent AI assistant that can read
images and generate prompts to inform diffusion models which object to
remove according to the user request and the input image contents. WeshowthequantitativeablationresultsinTab.4. Theob-
You should include the removal prompt with the format "{{prompt}}HERE IS servationisconsistentwiththeablationsinthemainpaper.
THE PROMPT {{/prompt}}"
Here are a few examples: The performance without joint training with images drops
1. slightly, except for a slight increase of 0.011 in the VFID
User: I take a photo of my two dogs eating food. I want to split this photo to
show each of them. Could you give me the photo with only the white dog? metric. This can be attributed to the enhanced diversity of
AI: Sure, this image contains two dogs eating food. One is white with curly hair. visual sources provided by additional image data. while it
The other back to it is brown and white. I will remove the latter dog for you.
bringsacompromisebetweenthequalityoftheresultsand
{{prompt}}remove the brown dog behind the white dog {{/prompt}}
User: Great! What about the picture with the other dog? temporalconsistency. Theresultsalsodemonstratethene-
AI: Sure. {{prompt}}remove the white dog {{/prompt}}
cessityoftheU-Netinflationandmasksupervisionmodifi-
2.
User: Hey, here's a picture! cationsofLGVI.Theabsenceofthesemodificationsleads
AI: This picture shows a young man standing on a chair in a grassland. toanoticeablereductioninperformance. Themostsignif-
User: Can you let the person float in the air?
AI: Of course, I can do it by removing the chair. Thus, the man looks like he is icantperformancedegradationisobservedwhentheU-Net
flying in the air. {{prompt}}remove the chair the man is standing on {{/prompt}} is not fully fine-tuned, but only the newly added parame-
3.
User: Who is strange in this image? ters are trained. This decline can be attributed to the in-
AI: The image shows a scene of an ordinary park. There are sidewalks, trees, trinsicdifferencesbetweentheinpaintingtaskandthepre-
and grass. The strange person is a man crawling in the bottom right.
trained image generation task. In the latter, the language
User: So, can you remove him for me?
AI: Sure, {{prompt}}remove the crawling man {{/prompt}} input guides the model on what to create, but it does not
specifywhatneedstoberemoved.
User: Can you help me with this photo? I bothered that there is a white bucket
being handed over in this picture. I don't want it to be seen. Can you remove it?
D.MoreQualitativeResults
Fig. 14 compares the referring video inpainting task.
MLLM
It demonstrates the effectiveness of the proposed LGVI
model. Fig.15showsthequalitativeresultsoftheinterac-
tivevideoinpaintingtask. OurLGVI-Imodeloutputsboth
Sure, I can help you with that. There is a white bucket in the image that is being
handed over. I will remove it for you. {{prompt}}remove the white bucket being inpaintingresultsandcomprehensivetextresponses.
handed over {{/prompt}}
E.BasicsofDiffusionModels
Figure13.TheMLLMpromptsingeneratingAIresponses.
DenoisingDiffusionProbabilisticModels(DDPMs).The
coreofDDPMs[10]involvesiterativelyaddingnoisetothe
Table4. QuantitativeablationsonU-Netvideoinflation(VF),
datauntilitbecomesasamplefromasimpleGaussiandis-
masksupervision(MS),imagejointtraining(IJ),andfine-tuning
thewholeU-Net(FW).E‚àó denotesE (√ó10‚àí2). tribution. The reverse process, which generates data from
warp warp
the noise, is learned by the model. The forward process,
Method PSNR‚Üë SSIM‚Üë VFID‚Üì E‚àó ‚Üì also known as the ‚Äúnoising‚Äù process, is typically modeled
warp
asaMarkovchainthatgraduallyaddsGaussiannoisetothe
w/oFW 20.53 0.607 0.370 1.101
dataoverasequenceoftimestepsT:
w/oMS 21.80 0.631 0.358 1.059
‚àö ‚àö
w/oVF 22.08 0.754 0.356 1.017 x ‚àºq(x ),x = Œ± x + 1‚àíŒ± œµ, (6)
0 0 t t t‚àí1 t
w/oIJ 22.39 0.728 0.297 0.987
LGVI(Ours) 22.85 0.756 0.308 0.901 where x 0 is a sample from the data distribution q(x 0), x t
representsthedataattimestept,andŒ± isavariancesched-
t
ulethatdeterminestheamountofnoisetoaddateachstep.
œµisthenoisesampledfromastandardGaussiandistribution
B.BaselineDetails
N(0,I). Thereverseprocess, whichisthegenerativepro-
FortheInst-Inpaint[56]baseline,wefine-tunethereleased cess, aims to learn the distribution of the original data by
checkpoint on the ROVI dataset with the same hyperpa- reversingthenoisingprocess. Thisinvolveslearningapa-
rameters of LGVI. Specifically, we train 50 epochs on 8 rameterized function Œ∏ that models the reverse conditional
√ó80GBNVIDIAA100GPUswithvideoandimagebatch probability p (x |x ). The reverse process is described
Œ∏ t‚àí1 t
sizes of 32 and 768. We resize the input image to 512 √ó by:
320, keeping the same as LGVI. For InstructPix2Pix [1],
p (x |x )=N(x ;¬µ (x ,t),Œ£ (x ,t)), (7)
MagicBrush [63], and Inpaint Anything* [59], we directly Œ∏ t‚àí1 t t‚àí1 Œ∏ t Œ∏ t
usetheirreleasedcheckpointsduetotheyarepre-trainedon where¬µ (x ,t)andŒ£ (x ,t)arelearnedfunctionsthatpre-
Œ∏ t Œ∏ t
largescaleimageediting/inpaintingdatasets. dict the mean and covariance of the distribution for x ,
t‚àí1
11Remove the bird jumping from one stick to another
Remove the small snake being touched by a person wearing a wedding ring.
Remove the man walking with a woman on the beach.
Remove the airplane on a runway.
Input Frame InpaintAnything* Inst-Inpaint InstructPix2Pix MagicBrush LGVI (Ours)
Figure14.Morequalitativeresultsforthereferringvideoinpaintingtask.
I took a photo of a desert scene with a camel, but I would like to use Sure, I can help you with that. The image shows a camel in a desert
only the background for a project. Can you remove the camel in the scene. I will remove the camel in the foreground for you. {{prompt}}
foreground, please? remove the camel in the foreground {{/prompt}}
This image captures the hustle and bustle of a busy train station, but I feel Sure, I can remove the yellow train from the scene. Here's the prompt:
the yellow train on the left is a bit distracting. Could you please remove it {{prompt}} remove the yellow train from the image {{/prompt}}
from the scene to help focus on the red and white train instead?
Figure15.Morequalitativeresultsfortheinteractivevideoinpaintingtask.
12remove the second duck nearest the block at the extreme end of the bath
Input
GT
Ours
Sure, I can remove the person wearing a yellow vest and a blue helmet from
Could you erase the person wearing a yellow vest and a blue helmet from this
the image. They are located in the middle of the group of people. {{prompt}}
picture? I want to keep the focus on the boats and the rest of the people
remove the person wearing a yellow vest and a blue helmet {{/prompt}}
Input
GT
Ours
Figure16. Failurecases. Inthefirstexample,wherethereferringexpressionisvagueandimplicit,ourmodelfailsatremovingtheright
duck.Inthesecondexample,wheremanyinstancesinthesamecategoryoccurtogether,ourmodelcanhardlyrecognizetheuser-intended
one.However,thelanguageoutputdescribestheapproximatepositionofthereferredperson.
givenx andtimestept. ThelearningofŒ∏istypicallydone where c is guidance to control the generation result. We
t
viaavariationalapproach,minimizingalossfunctionthatis extend the condition input with a video input X to control
amodifiedversionoftheEvidenceLowerBOund(ELBO). theinpaintingresults.
This loss function ensures that the learned reverse process Latent Diffusion Models (LDMs). Latent Diffusion
closelyapproximatesthetruedistributionofthedata,which Model(LDM)[40]isatypeofgenerativemodelthatoper-
canbesimplifiedas: atesonalatentspaceratherthandirectlyonthedataspace.
The primary idea is to first encode high-dimensional data,
L(Œ∏)=E (cid:2) ||œµ‚àíœµ (x ,t)||2(cid:3) . (8)
x,œµ‚àºN(0,1),t Œ∏ t 2 likeimages, intoalower-dimensionallatentrepresentation
z = E(x) and then apply the diffusion process within this
The denoising process can incorporate extra guidance,
latent space. A decoder reconstructs the latent back to the
wherethemodelistrainedtogeneratesamplesconditioned
pixeldistributionx=D(z).
onasetoflabelsorattributesc. Typicalguidancesarelan-
guage and images [40, 64]. The loss function can be up- LGVI Architecture. The core of our model is to produce
datedasfollows: inpaintingresultsYÀÜ drivenbylanguageguidancecandvi-
sion input X. This core concept is versatile and can be
L(Œ∏)=E (cid:2) ||œµ‚àíœµ (x ,t,c)||2(cid:3) , (9) integrated into a variety of existing architectures, includ-
x,œµ‚àºN(0,1),t,c Œ∏ t 2
13ing those based on diffusion or transformer paradigms, as [9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
longasthemodelcanfuselanguageandvisioninputs. We Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
choosetheLDMarchitecturebecauseofitsflexibilityinin- age editing with cross attention control. arXiv preprint
flatingtovideomodalityandimprovedsamplequalitydue arXiv:2208.01626,2022. 2
tothereduceddimensionalityoftheproblem. [10] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
sionprobabilisticmodels. NeurIPS,2020. 11
[11] Yuan-Ting Hu, Heng Wang, Nicolas Ballas, Kristen Grau-
F.FutureWorkDiscussions
man,andAlexanderGSchwing.Proposal-basedvideocom-
Fig. 16 shows two LGVI and LGVI-I failure cases. The pletion. InECCV,2020. 1,2
[12] DrewAHudsonandChristopherDManning. Gqa: Anew
models still face the core challenge of implicit language
dataset for real-world visual reasoning and compositional
or description. In the first example, the referring expres-
questionanswering. CVPR,2019. 3
sion is comparatively long and hard to understand, lead-
[13] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
ing to poor performance. In the second example for the
vosyan, Roberto Henschel, Zhangyang Wang, Shant
interactivetask,eveniftheMLLMoutputsareasonablere-
Navasardyan,andHumphreyShi. Text2video-zero:Text-to-
sponse and correctly predicts the position of the removed imagediffusionmodelsarezero-shotvideogenerators.arXiv
person,thediffusionmodeldoesnotprovidetherightout- preprintarXiv:2303.13439,2023. 2
put. That is because the current diffusion-based baseline [14] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
model is a preliminary modification of a pre-trained text- ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
to-image model, which lacks the understanding of precise head,AlexanderCBerg,Wan-YenLo,etal. Segmentany-
locations. The failure cases demonstrate that despite the thing. arXivpreprintarXiv:2304.02643,2023. 7
comparatively stronger performance of previous methods, [15] Wei-ShengLai,Jia-BinHuang,OliverWang,EliShechtman,
ErsinYumer,andMing-HsuanYang. Learningblindvideo
ourproposedmodelisstillabaselineinthelanguage-driven
temporalconsistency. InECCV,2018. 7
video inpainting field. Future work is expected to develop
[16] SunghoLee,SeoungWugOh,DaeYeunWon,andSeonJoo
moreadvancedmethodstoovercomethechallenges.
Kim.Copy-and-pastenetworksfordeepvideoinpainting.In
ICCV,2019. 1
References
[17] Ang Li, Shanshan Zhao, Xingjun Ma, Mingming Gong,
Jianzhong Qi, Rui Zhang, Dacheng Tao, and Ramamoha-
[1] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In-
naraoKotagiri. Short-termandlong-termcontextaggrega-
structpix2pix:Learningtofollowimageeditinginstructions.
tionnetworkforvideoinpainting. InECCV,2020. 2
InCVPR,2023. 2,6,7,11
[18] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,Fanyi
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
it:Multi-modalin-contextinstructiontuning. arXivpreprint
tan,PranavShyam,GirishSastry,AmandaAskell,etal.Lan-
arXiv:2306.05425,2023. 3
guagemodelsarefew-shotlearners. NeurIPS,2020. 3
[19] JingyuanLi,NingWang,LefeiZhang,BoDu,andDacheng
[3] Se¬¥bastienBubeck,VarunChandrasekaran,RonenEldan,Jo-
Tao. Recurrent feature reasoning for image inpainting. In
hannesGehrke,EricHorvitz,EceKamar,PeterLee,YinTat
CVPR,2020. 2
Lee, YuanzhiLi, ScottLundberg, etal. Sparksofartificial
[20] WenboLi,ZheLin,KunZhou,LuQi,YiWang,andJiaya
general intelligence: Early experiments with gpt-4. arXiv
Jia. MAT:Mask-awaretransformerforlargeholeimagein-
preprintarXiv:2303.12712,2023. 2,3
painting. InCVPR,2022. 2
[4] Ya-LiangChang,ZheYuLiu,Kuan-YingLee,andWinston
[21] Xiangtai Li, Henghui Ding, Wenwei Zhang, Haobo Yuan,
Hsu. Free-formvideoinpaintingwith3dgatedconvolution
Guangliang Cheng, Pang Jiangmiao, Kai Chen, Ziwei Liu,
andtemporalpatchgan. InICCV,2019. 1,2
andChenChangeLoy. Transformer-basedvisualsegmenta-
[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, tion:Asurvey. arXivpre-print,2023. 2
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, [22] Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- Ming-Ming Cheng. Towards an end-to-end framework for
vain Gelly, et al. An image is worth 16x16 words: Trans- flow-guidedvideoinpainting. InCVPR,2022. 1,2,4,7,10
formersforimagerecognitionatscale. ICLR,2021. 2
[23] GuilinLiu,FitsumAReda,KevinJShih,Ting-ChunWang,
[6] MouniraEbdelli,OlivierLeMeur,andChristineGuillemot. AndrewTao,andBryanCatanzaro. Imageinpaintingforir-
Video inpainting with short-term windows: application to regularholesusingpartialconvolutions. InECCV,2018. 2
objectremovalanderrorconcealment. TIP,2015. 1 [24] GuilinLiu,AysegulDundar,KevinJShih,Ting-ChunWang,
[7] KirillGavrilyuk,AmirGhodrati,ZhenyangLi,andCeesGM FitsumAReda,KaranSapra,ZhidingYu,XiaodongYang,
Snoek.Actorandactionvideosegmentationfromasentence. AndrewTao, andBryanCatanzaro. Partialconvolutionfor
InCVPR,2018. 3,4 padding,inpainting,andimagesynthesis. TPAMI,2022. 2
[8] YuyingGe,YixiaoGe,ZiyunZeng,XintaoWang,andYing [25] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
Shan. Planting a seed of vision in large language model. Visualinstructiontuning. arXivpreprintarXiv:2304.08485,
arXivpreprintarXiv:2307.08041,2023. 3 2023. 3,7,10
14[26] RuiLiu,HanmingDeng,YangyiHuang,XiaoyuShi,Lewei [41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Lu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hong- Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
sheng Li. Fuseformer: Fusing fine-grained information in RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
transformersforvideoinpainting. InICCV,2021. 2,7 etal.Photorealistictext-to-imagediffusionmodelswithdeep
[27] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao languageunderstanding. NeurIPS,2022. 2
Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun [42] SeongukSeo, Joon-YoungLee, andBohyungHan. Urvos:
Zhu, etal. Groundingdino: Marryingdinowithgrounded Unifiedreferringvideoobjectsegmentationnetworkwitha
pre-training for open-set object detection. arXiv preprint large-scalebenchmark. InECCV,2020. 3,4
arXiv:2303.05499,2023. 7 [43] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn,
[28] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Deep learning face attributes in the wild. In ICCV, 2015. OranGafni, etal. Make-a-video: Text-to-videogeneration
3 without text-video data. arXiv preprint arXiv:2209.14792,
2022. 2,5
[29] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng
Zhang, StephenLin, andBainingGuo. SwinTransformer: [44] Jaskirat Singh, Stephen Gould, and Liang Zheng. High-
Hierarchical vision transformer using shifted windows. In fidelityguidedimagesynthesiswithlatentdiffusionmodels.
ICCV,2021. 2 InCVPR,2023. 2
[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
[30] AndreasLugmayr,MartinDanelljan,AndresRomero,Fisher
Martinet,Marie-AnneLachaux,Timothe¬¥eLacroix,Baptiste
Yu,RaduTimofte,andLucVanGool. Repaint: Inpainting
using denoising diffusion probabilistic models. In CVPR, Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
2022. 2
arXivpreprintarXiv:2302.13971,2023. 3
[31] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
[46] ChuanWang,HaibinHuang,XiaoguangHan,andJueWang.
junWu,Jun-YanZhu,andStefanoErmon. Sdedit: Guided
Videoinpaintingbyjointlylearningtemporalstructureand
imagesynthesisandeditingwithstochasticdifferentialequa-
spatialdetails. InAAAI,2019. 1,2
tions. arXivpreprintarXiv:2108.01073,2021. 2
[47] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,
[32] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan,
Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-
Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. AdaViT:
videosynthesis. NeurIPS,2018. 7
Adaptivevisiontransformersforefficientimagerecognition.
[48] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P
InCVPR,2022. 2
Simoncelli. Imagequalityassessment: fromerrorvisibility
[33] AlexanderQuinnNichol,PrafullaDhariwal,AdityaRamesh,
tostructuralsimilarity. TIP,2004. 7
Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya
[49] Jianzong Wu, Xiangtai Li, Xia Li, Henghui Ding, Yunhai
Sutskever, and Mark Chen. Glide: Towards photorealis-
Tong,andDachengTao.Towardsrobustreferringimageseg-
ticimagegenerationandeditingwithtext-guideddiffusion
mentation. arXivpreprintarXiv:2209.09554,2022. 2
models. InICML,2022. 2
[50] Jianzong Wu, Xiangtai Li, Henghui Ding, Xia Li, Guan-
[34] OpenAI. Gpt-4technicalreport,2023. 3
gliangCheng,YunhaiTong,andChenChangeLoy.Betrayed
[35] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
bycaptions:Jointcaptiongroundingandgenerationforopen
Darrell, and Alexei A Efros. Context encoders: Feature
vocabularyinstancesegmentation. ICCV,2023.
learningbyinpainting. InCVPR,2016. 2
[51] JianzongWu,XiangtaiLi,ShilinXu,HaoboYuan,Henghui
[36] FedericoPerazzi,JordiPont-Tuset,BrianMcWilliams,Luc
Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong,
VanGool,MarkusGross,andAlexanderSorkine-Hornung.
XudongJiang,BernardGhanem,etal.Towardsopenvocab-
Abenchmarkdatasetandevaluationmethodologyforvideo ularylearning: Asurvey. arXivpreprintarXiv:2306.15880,
objectsegmentation. InCVPR,2016. 3
2023. 2
[37] RenjiePi, JiahuiGao, ShizheDiao, RuiPan, HanzeDong, [52] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
JipengZhang,LeweiYao,JianhuaHan,HangXu,andLing- Lei,YuchaoGu,YufeiShi,WynneHsu,YingShan,Xiaohu
pengKongTongZhang. Detgpt: Detectwhatyouneedvia Qie,andMikeZhengShou. Tune-a-video: One-shottuning
reasoning. arXivpreprintarXiv:2305.14167,2023. 3 of image diffusion models for text-to-video generation. In
[38] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu, ICCV,2023. 2,5
and Mark Chen. Hierarchical text-conditional image gen- [53] ShaoanXie, ZhifeiZhang, ZheLin, TobiasHinz, andKun
erationwithcliplatents. arXivpreprintarXiv:2204.06125, Zhang.Smartbrush:Textandshapeguidedobjectinpainting
2022. 2 withdiffusionmodel. InCVPR,2023. 2
[39] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie [54] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang,
Zhou, and Cho-Jui Hsieh. DynamicViT: Efficient vision Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen,
transformerswithdynamictokensparsification. InNeurIPS, andThomasHuang. YouTube-VOS:Sequence-to-sequence
2021. 2 videoobjectsegmentation. InECCV,2018. 3
[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, [55] BotaoYe,HongChang,BingpengMa,ShiguangShan,and
PatrickEsser,andBjo¬®rnOmmer.High-resolutionimagesyn- XilinChen. Jointfeaturelearningandrelationmodelingfor
thesiswithlatentdiffusionmodels. InCVPR,2022. 2,5,13 tracking:Aone-streamframework. InECCV,2022. 1,7
15[56] Ahmet Burak Yildirim, Vedat Baday, Erkut Erdem, Aykut
Erdem, and Aysegul Dundar. Inst-inpaint: Instructing
to remove objects with diffusion models. arXiv preprint
arXiv:2304.03246,2023. 3,6,7,11
[57] HongxuYin, ArashVahdat, JoseMAlvarez, ArunMallya,
JanKautz, andPavloMolchanov. A-ViT:Adaptivetokens
forefficientvisiontransformer. InCVPR,2022. 2
[58] JiahuiYu,ZheLin,JimeiYang,XiaohuiShen,XinLu,and
Thomas S Huang. Free-form image inpainting with gated
convolution. InICCV,2019. 2
[59] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin
Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything:
Segment anything meets image inpainting. arXiv preprint
arXiv:2304.06790,2023. 1,6,7,11
[60] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and
Chen Change Loy. Contextual object detection with
multimodal large language models. arXiv preprint
arXiv:2305.18279,2023. 2,3
[61] YanhongZeng,JianlongFu,andHongyangChao. Learning
joint spatial-temporal transformations for video inpainting.
InECCV,2020. 1,7
[62] Kaidong Zhang, Jingjing Fu, and Dong Liu. Flow-guided
transformerforvideoinpainting. InECCV,2022. 2,7
[63] KaiZhang,LingboMo,WenhuChen,HuanSun,andYuSu.
Magicbrush: A manually annotated dataset for instruction-
guidedimageediting. InNeurIPS,2023. 2,6,7,11
[64] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV,2023. 13
[65] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
andAntonioTorralba. Places: A10millionimagedatabase
forscenerecognition. TPAMI,2017. 3
[66] Shangchen Zhou, Chongyi Li, Kelvin C.K Chan, and
ChenChangeLoy. ProPainter: Improvingpropagationand
transformerforvideoinpainting. InICCV,2023. 1,2,7
[67] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understandingwithadvancedlargelanguagemodels. arXiv
preprintarXiv:2304.10592,2023. 2,3
16