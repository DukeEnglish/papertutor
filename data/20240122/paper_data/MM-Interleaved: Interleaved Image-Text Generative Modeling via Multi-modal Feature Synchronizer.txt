MM-Interleaved: Interleaved Image-Text Generative Modeling
via Multi-modal Feature Synchronizer
ChangyaoTian2,1‚àó‚Ä†,XizhouZhu3,4,1‚àó,YuwenXiong5,1‚àó,WeiyunWang6,1‚Ä†,ZheChen7,1‚Ä†,WenhaiWang2,1,
YuntaoChen8,LeweiLu4,TongLu7,JieZhou3,HongshengLi2,YuQiao1,JifengDai3,1(cid:66)
1OpenGVLab,ShanghaiAILaboratory 2MMLab,TheChineseUniversityofHongKong
3TsinghuaUniversity 4SenseTimeResearch 5UniversityofToronto 6FudanUniversity
7NanjingUniversity 8CAIR,HKISI,CAS
https://github.com/OpenGVLab/MM-Interleaved
Abstract
Multi-scale
Feature Maps
Developinggenerativemodelsforinterleavedimage-text
data has both research and practical value. It requires Input Tokens ‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶
models to understand the interleaved sequences and sub- Im Tea xg te T oTo keke nn C Er do tn d og y L hP oao ovr po e y r c 'o so hP m oo e ub sy o eva .en rd Loo fp ry ie i nn dv sit ie ns . her ‚Ä¶‚Ä¶
sequently generate images and text. However, existing at-
SynF ce ha rt ou nre iz er Input Interleaved Sequence Generated Interleaved Sequence
temptsarelimitedbytheissuethatthefixednumberofvi- Auto-regressively generate either a text token or an image
sual tokens cannot efficiently capture image details, which
Figure 1. Illustration of multi-modal feature synchronizer.
isparticularlyproblematicinthemulti-imagescenarios.To
In the auto-regressive generation of interleaved image-text se-
address this, this paper presents MM-Interleaved, an end-
quences, besides interacting with low-resolution image tokens
to-end generative model for interleaved image-text data.
with self-attention, tokens in MM-Interleaved could also use the
It introduces a multi-scale and multi-image feature syn-
multi-modalfeaturesynchronizertocross-attendmulti-scalehigh-
chronizer module, allowing direct access to fine-grained resolutionimagefeatures. Themulti-modalfeaturesynchronizer
image features in the previous context during the gener- ensuresthecrossattentionwillhaveexactlythesamecausalrela-
ation process. MM-Interleaved is end-to-end pre-trained tionbetweenimageandtexttokens.
on both paired and interleaved image-text corpora. It is
further enhanced through a supervised fine-tuning phase, the interleaved format not only covers a broader range of
wherein the model improves its ability to follow complex databutalsopresentsmorecomplexinformationstructures.
multi-modalinstructions.Experimentsdemonstratethever- Developing multi-modal models that can simultaneously
satilityofMM-Interleavedinrecognizingvisualdetailsfol- understand and generate such interleaved image-text data
lowing multi-modal instructions and generating consistent hassignificantresearchvalueandpracticalapplicationpo-
imagesfollowingbothtextualandvisualconditions. Code tential. Itbroadenstheapplicationscopeofpreviousmulti-
and models are available at https://github.com/ modal models [83] and enables more unified processing
OpenGVLab/MM-Interleaved. of multi-modal data, bringing previously isolated research
fields such as text-to-image generation [71, 74] and visual
question-answering[48,53]together.
1.Introduction With recent progress in multi-modal modeling with
Large Language Models (LLMs) [7, 64], exploring how
Interleavedimage-text data, where thetext andimages are LLMs can be leveraged for interleaved image-text model-
relatedincontentandintertwinedinlayout,isverycommon inghasbecomeincreasinglyattractive. Acorechallengeof
ontheinternet. Asanextensionofimage-textpairswidely interleaved multi-modal LLMs is how to effectively han-
used in previous multi-modal models [21, 22, 37, 70, 82], dle multiple images within the limited context length of
LLMs. In most multi-modal LLMs, images are encoded
*Equalcontribution.
as visual tokens from an image tokenizer (also known as
‚Ä†ThisworkisdonewhenChangyaoTian, WeiyunWang, andZhe
image encoder) and fed into LLMs together with text to-
ChenareinternsatShanghaiAILaboratory.
(cid:66)Correspondingauthor(daijifeng@tsinghua.edu.cn). kens [18, 40, 43, 83]. To reduce computational demands
1
4202
naJ
81
]VC.sc[
1v80201.1042:viXraLLM-based
Multimodal Decoder
andtherequiredcontextlengthofLLMs,PerceiverResam- text image connectionto input layer connectionto intermediate layers
plers [3] are commonly used to mRaespampelearch image from up
to1024tokenstoafixednumberofvisualtokens(e.g., 32 Image Decoder Image Decoder
or 64 tokens) as in Fig. 2. Due Vtoisuatl Thokeenirzeerlatively small to- Multi-modal LLM Multi-modal LLM Multi-modal LLM Multi-modal LLM
kennumber,criticalimagedetailsmaybeoverlooked,espe-
Resampler Resampler Resampler Resampler
cially in tasks requiring fine-grained observation. Increas-
Image Encoder Image Encoder Image Encoder Image Encoder
ing the number of visual tokens [55, 99] can address this
issue to some extent. While the context length of LLMs (a) BLIP-like (b) Flamingo-like (c) GILL/Emu-like (d) MM-Interleaved (Ours)
is typically limited, allocating a large number (e.g., 2890
Image Decoder Image Decoder Figure2. Comparisonofdifferenttypesofimage-textgenera-
in[84])ofvisualtokensperimagewouldposeasignificant
tivemodeling. (a)and(b)canonlygeneratetext. (c)and(d)can
limitationinmulti-imagescenarios.
generatebothimagesandtext. Alltypesexcept(d)arelimitedby
LLM-based LLM-based
WeMunltiomotdeal DtehcoadterthemainproMbultlimeomdal DiescodtehratthePerceiverRe- thefixednumberofvisualtokensextractedbycontext-insensitive
sampler only takes image features as input and is context- Resampler,whichcannotefficientlycaptureimagedetailsandis
insensitive. Itcompresseseachinputimagetoafixednum- problematicinmulti-imagescenarios.
Resampler Resampler
beroftokens,whichmakesitharderorevenimpossibleto
pabilities. Thankstoend-to-endgenerativemodeling,fine-
preserve all information required and fulfill all needs for
Visual Tokenizer Visual Tokenizer tuning can be applied to both text and image generation.
subsequent generation. In contrast, allowing intermediate
Our model is fine-tuned on several tasks, including vi-
layers in decoder networks to dynamically extract a small
sual question-answering, image caption, referring expres-
amount of relevant information from the images can com-
sion grounding, text-to-image generation, segmentation-
pens(ca)t GeIfLoLr/Etmhius-liikneformatio(dn) lMoMss-I.nSterulecahvead (dOyunrsa)micextraction
to-image translation, and visual storytelling. Compared
mechanismshouldalsoefficientlyhandlescenariosrequir-
withpreviousimage-to-textandtext-to-imagemethods,our
ingmultipleimagesandhigh-resolutionfeaturemaps.
model achieves competitive results and shows higher to-
Motivated by this, we designed a fine-grained multi-
kenefficiency. Comparedwithjointimageandtextgener-
modalfeaturesynchronizerbasedonthedeformablesparse
ationmodels,wesetthenewSotAresultsforawiderange
attention [110]. As shown in Fig. 1, it enables the multi-
of tasks. Overall, MM-Interleaved can handle interleaved
modal LLMs to observe multi-scale high-resolution image
image-textinputsandrecognizefine-graineddetailsacross
featuresfrommultipleimagesefficiently. Thismechanism
multiple images to produce accurate textual descriptions
facilitates the extraction of fine-grained image details with
and visually consistent images, demonstrating the poten-
only a small number of input visual tokens, which is par-
tial of high-performance multi-modal models that can un-
ticularlyapplicabletointerleavedimage-textscenarioswith
derstandandgenerateinterleavedimage-textcontent.
multipleimages.
Based on this module, we propose MM-Interleaved, a
2.RelatedWork
novel end-to-end generation model for processing inter-
leavedimage-textdata,asshowninFig.3. Imagesandtext
Modeling of Paired Image-Text Data plays an impor-
are first mapped into tokens through their respective tok-
tant role in the progress of multi-modal research in recent
enizers(i.e.,imageencoderandwordembedding)andthen
years. [24, 37, 76] has released a series of public large-
fedtotheLLM,arrangedintheiroriginalorder. Aspecial
scaleimage-textpairsdatasets.Basedonthesedata,models
token<BoI>isintroducedtorepresent‚ÄúBeginofImage‚Äù.
trained by image-text contrastive learning [39, 50, 70, 82]
When the LLM processes image and text sequences, each
have been proven able to recognize and understand open-
token in the intermediate layers directly observes multi-
worldsemantics. Subsequentwork[46,93,97,111]further
imageandmulti-scalefeaturemapsfromthepreviouscon-
incorporates the text generation tasks such as image cap-
textthroughtheproposedfeaturesynchronizermodule. Af-
tioning. Someotherworks[71,74]wereproposedtogen-
ter being processed by the LLM, the features of the text
erateimagesbasedontextprompts. Thelatestprogressof
tokenspredictthenexttextword. Whenthe<BoI>token
LLMs[64,65]hascreatedanewera, leadingtotheemer-
is predicted, an image decoder based on diffusion models
genceofmanyLLM-basedmulti-modalmodelstrainedus-
isusedtopredictthenextimage. AllpreviousLLMoutput
ing image-text pairs [48, 53, 90, 91]. For example, [53]
features are passed into the image decoder to generate im-
connect pre-trained LLMs and vision foundation models
ages. Each pixel in the intermediate layers of the decoder
withlinearprojectionstobuildpowerfulmulti-modaltext-
can also extract details of the previous image through the
generationmodels.
proposedsynchronizermodule.
Our models are pre-trained on a mixture of image-text Modeling of Interleaved Image-Text Data has received
pairsandinterleavedimage-textsequences. Wedonotuse increasingattentionrecently. [3,33]werethefirsttofocus
anyin-housedata. Similartopreviousmulti-modalLLMs, onunderstandingsuchdatarelyingonnon-publicdatasets.
supervised fine-tuning can further enhance our model ca- To promote the development of this field, [45, 109] re-
2Next-Image-Prediction Loss Next-Token-Prediction Loss
Denoised Text Token Classifier
Image Conditions for
Image Generation Output Token
Embeddings
Image Decoder
Large Language Model
Feature Synchronizer
N FFN
√ó UpsamplingBlock Feature Synchronizer N
√ó
Self-Attention
N DownsamplingBlock
Input Token
√ó
Embeddings
Noised
Image
Image Word Image Word Image
Encoder Embedding Encoder Embedding Encoder
FM eM au u tl ult ti ri- e-I Sm Mca a ag le pe s C Er P do don ybg y aP rao enr do . .r .o L heo ro p fry i ei nn dv sit e ‚Ä¶s I In m St ee a qr g ul ee e-a nTv e ce x ed t
Figure3.ArchitectureoftheproposedMM-InterleavedModel.Theredlinesrepresenthowthemulti-scaleimagefeaturesaregenerated
andutilized. MM-Interleavedincorporatesimageencodertobothextracthigh-resolutionmulti-scaleimagefeaturesandmapeachimage
intoafixednumberoflow-resolutionvisualtokens. Thesevisualtokensarefedintothemulti-modalLLMalongwithtexttokens. LLM
usesthefeaturesynchronizationmoduletoextracthighresolutionimagedetails,andauto-regressivelygeneratestexttokens. Afterthat,a
DM-basedimagedecodergeneratesnextimageconditionedonthepreviouscontextfeaturesfromLLMaswellasthemulti-scaleimage
featuresfromthefeaturesynchronizationmodule.
leasepublicandlarge-scaleinterleavedimage-textdatasets. [6, 10, 53, 55] increase the number of input visual tokens
Moreworks[101,103]havebeguntofocusonunderstand- per image to hundreds. [84] further increases this number
ing the interleaved data. Nevertheless, their generative ca- to 2,890. Although it mitigates information loss, the com-
pabilities remain limited to text. [18, 43, 83, 99] have ini- putationalandmemorydemandsofLLMsaresignificantly
tially attempted to generate images and text in the inter- increased.Increasingthenumberofvisualtokensperimage
leaved context concurrently. [43, 83] involve a two-stage isparticularlyproblematicinmulti-imagescenarios,where
generation process, where text and image are only gener- multipleimagesnaturallyrequiremorevisualtokens,mak-
ated in the first and second stages, respectively. [99] uses ingithardtouseforinterleavedimage-textdata.
VQ-VAE[86]toconvertimagesintodiscretetokens,facil-
itating token-level auto-regressive modeling as traditional
languagemodeling.However,itprimarilyfocusesonimage
generation capabilities and exhibits notable weaknesses in
imageunderstanding. DreamLLM[18],aconcurrentwork 3.Method
to ours, also focuses on single-stage end-to-end modeling
usingrawimagepixelsasinput. Despitetheseefforts,they
3.1.TaskFormulation
only feed image information at the input of LLMs, which
are limited by the problem that fixed number of visual to-
kenscannotefficientlydescribeimagedetails. To build an end-to-end generative model for interleaved
image-textdata. Wefirstconsideraninterleavedimage-text
Integrating Image Details into LLMs is important for sequenceX ={x ,x ,x ,...},whereeachelementx is
1 2 3 n
multi-modal LLMs. Most works use Perceiver Resam- either a text token (denoted as xL) or a whole image (de-
n
plers [3, 48, 107] to extract image information via cross- notedasxV). Textandimagesarearrangedintheorderin
n
attention,mappingeachimageintoafixednumberofvisual which they appear in the original content. In multi-modal
tokens. For example, [3] equips Resamplers in the inter- LLMs,acommonpracticeisfirstextractingembeddingfor
mediatelayersofLLMs,injectingextractedimagefeatures eachtexttokenandeachimagebeforebeingfedintoLLMs,
into LLMs through gated residual connections. [48, 107] i.e., eL = E (xL) and eV = E (xV), where E denotes
n L n n V n L
introduces Resamplers at the bottom of LLM to insert the wordembeddingfollowingstandardpracticesinNLP.E is
V
extractedvisualtokensintotheinputtextsequences. While typically an image encoder (e.g., ViTs [19]) followed by a
these methods achieve good performance, image details PerceiverResampler[3]tomapeachimagetoafixednum-
may be overlooked due to the small number (e.g., 32 or ber of tokens. Then, the generative modeling is trained to
64) of visual tokens. To preserve more image details, maximize the log-likelihood of the multi-modal sequence,
3
‚Ä¶givenby: Model(DM).Thisintegrationaimstoexcelinbothunder-
standing and generation tasks of text and images by lever-
(cid:88)
logp(X)= logp(x n|x 1,x 2,...,x n‚àí1) aging the strengths of each model type. As illustrated in
n Fig.3,ourarchitecturecomprisesthreekeycomponents:
(cid:88)
= logp(x |e ,e ,...,e )
n 1 2 n‚àí1 (1)
n (1) VFM-based Image Tokenizer E that maps each
V
= (cid:88) logp(xL|e )+ (cid:88) logp(xV|e ), imagexV ‚àà RH√óW√ó3 (e.g., H = W = 224)intoafixed
n <n n <n
n‚ààIL n‚ààIV numberofvisualtokenseV ‚ààRN√óC (N =32bydefault).
C isthechanneldimension. Itconsistsofapre-trainedvi-
where I and I represent the index sets for text tokens
L V
sionfoundationmodel(e.g.,CLIP-ViT[70])forfeatureex-
and images within the sequence, respectively. < n in the
tractionandaPerceiverResampler[3]toreducethenumber
subscript represents the abbreviation of {1,2,...,n‚àí1}.
ofvisualtokens. WealsouseaViT-Adapter[12]toextract
ThefollowingparagraphsprovideexplanationsofEq.(1).
multi-scale image features FV ‚àà R((cid:80)L i=1Hi√óWi)√óC for
Text Generation with Multi-modal Condition. The log-
fine-grained feature fusion in subsequent networks, where
likelihoodlogp(xL n|e <n)issimilartotraditionalcausallan- L=4andH = H ,W = W bydefault.
guagemodeling,exceptthattheconditionalsoincludespre- i 2i+1 i 2i+1
vious images. Recent works [48, 53, 90, 91] have demon-
(2)LLM-basedMulti-modalModelD thatextracts
LM
strated the effectiveness of using LLMs for processing ad-
contextfeaturesfromtheinterleavedimage-textsequences.
ditionalvisualinputs. Theyapplyalinearclassifierontop
A pre-trained LLM (e.g., Vicuna [105]) is utilized. Its
oftheLLMs. Thelossfunctionfortextgenerationis input sequence E ‚àà RK√óC is a concanation of embed-
L (xL|e )=‚àíx¬ØL¬∑logsoftmax(cid:0) W ¬∑D (e )(cid:1) , dings (e 1,e 2,...), where e n is either a word embedding
NTP n <n n LM <n eL ‚àà R1√óC or an image embedding eV ‚àà RN√óC. K is
(2) n n
thetotalnumberofinputtokens. Wealsointroduceafine-
where W is the linear classification weight, D denotes
LM grainedfeaturesynchronizermoduleforlettingtheinterme-
the LLM network (e.g., LLaMA [85]), x¬ØL is the one-hot
n diate layers in D directly access and extract multi-scale
vectorindicatingtheground-truthtexttoken. LM
imagefeaturesondemand.
Image Generation with Multi-modal Condition. Max-
imizing the log-likelihood logp(xV|e ) aligns with the (3) DM-based Image Decoder D that generates the
n <n DM
denoising-diffusion process [54], which recently achieves image based on previous image-text sequences. A pre-
widespreadsuccessinimagegeneration. Whenconditional trained diffusion model (e.g., Stable Diffusion [74]) is uti-
inputsx arealltext,thisisidenticaltotext-to-imagedif- lized. To provide the conditional inputs for D , Resam-
<n DM
fusionmodels,wheremaximizingthelog-likelihoodisde- pler[3]isemployedtomaptheoutputfeaturesfromLLM
rivedasminimizingthediffusionmodelinglossas to a fixed number of conditional tokens. The fine-grained
featuresynchronizermoduleisalsousedhereforproviding
L (xV|e )=E ||œµ‚àíD (cid:0) xV ,t,D (e )(cid:1) ||2,
NIP n <n œµ,t DM n,t LM <n detailedvisualconditions,whichisveryusefulfortasksre-
(3)
quiringvisualalignment(e.g.,imagetranslation).
where D is the language model encoding text inputs,
LM
D is the diffusion model for image denoisiong. xV
DM n,t Thenweintroducethedetailsoftheproposedarchitecture.
is the noisy version of the original image at the denoising
stept,andthedenoisingnetworkD istrainedtopredict
DM Multi-image and Multi-scale Feature Synchronizer
thenoiseœµ. Wefoundthatsuchmodelingisalsoapplicable
(MMFS). The MMFS module is designed to dynamically
whenconditionalinputisinterleavedimage-textdata.
capture image details from multiple images in an inter-
Notethatourmodelingallowsfortheflexiblecombination leavedcontext. ItleveragestheDeformableAttention[110]
of different language models, image encoders, and image mechanismtoachieveefficientandsparseimageattention.
decoderstofullyleverageavarietyofpre-trainedmodels.It As is shown in Fig. 4, given a query token that requires
doesnotneedtoinsertadditionalspecialtokensspecifically detailed image features, the MMFS module only attends
forimagegenerationintoLLMs.Thewholeframeworkcan to a small set of sampling points around a reference point
beoptimizedend-to-end. on the reference images. Let f ‚àà RC represent the fea-
q
tures of the query token. pÀÜ ‚àà [0,1]2 denotes the rela-
3.2.Architecture q
tive image coordinate of its reference point. By default,
BuildinguponthetaskformulationinSec.3.1,wepropose pÀÜ = (0.5,0.5) corresponds to the center of the images.
q
anovelmulti-modalarchitectureforprocessinginterleaved {FV}M are the multi-image multi-scale feature maps,
m m=1
image-text data. It integrates a Visual Foundation Model where FV is the multi-scale feature extracted by the im-
(VFM), a Large Language Model (LLM), and a Diffusion age tokenizer, M is the number of reference images. The
4Image Output ùëì " Multi-modalLLMwithMMFS.Theinputofmulti-modal
Feature Maps LLMs is an interleaved sequence of image and text token
Weighted embeddings,whichalwaysstartswithaspecialtoken<s>
Sum
and ends with another special token </s>. Image token
ùêπ$ embeddings are inserted at the corresponding positions in
#
the original sequence. Another special token <BoI> is
ùê¥ addedinfrontofeachimagetorepresent‚ÄúBeginofImage‚Äù.
Sampled !
Spatial Attention MMFS modules are inserted between the self-attention
Feature Weights
layer and the feed-forward layer of the LLM every fixed
number of blocks. Query token f iterates over each to-
ùêπ$ q
%
ken in the LLM, which can only access the previous im-
ages. pÀÜ = (0.5,0.5). The output of MMFS is multiplied
ùëùÃÇ q
! Softmax bytanh(Œ±)beforebeingaddedbacktof ,whereŒ±isazero-
Reference q
Point ùëä ‚Äô Linear Linear ùëä & initializedlearnablescalar.
Image Decoder with MMFS. The output features from
Image Index
Embedding themulti-modalLLMarefurtherprocessedbyanotherPer-
FQ eau te ur ry e ùëì ! Linear ceiverResamplerbeforebeingfedintotheimagedecoder.
ùëä ! For each image to be generated, the Resampler maps pre-
viouscontextfeaturestoafixednumberoftokens(e.g.,77
Figure4.ArchitectureoftheMMFSmodule.Thequeryfeature
tokens)tomatchthepre-traineddiffusionmodel.
ispassedthroughalinearlayerandaddedtotheimageindexem-
MMFS modules are inserted after each upsampling
bedding.Twolinearlayersareusedtopredictthesamplingoffsets
blockintheU-Netstructureofthediffusionmodel. Query
and unnormalized attention weights of each image respectively.
token f iterates over each pixel in the feature map. pÀÜ is
Thesamplingoffsetsareaddedwiththequery‚Äôsreferencepointto q q
formthecorrespondingsamplinglocations,whicharesharedand setasthespatialcoordinateofthequerypixel. Theoutput
rescaled across all feature maps of the same image. The output of MMFS is further processed by a zero-initialized convo-
featureistheweightedsumofthesampledspatialfeaturesfrom lutionlayerbeforebeingaddedbacktof .
q
thesepoints.
TrainingTargetandInferencePipeline. Thetrainingob-
jectiveisdefinedasthesumofNext-Text-TokenPrediction
outputfeaturef ‚ààRC isgivenby
o lossinEq.(2)andNext-ImagePredictionlossinEq.(3),
q(m) =W ¬∑f +PosEmbed(m),
q q L=L +ŒªL (5)
NTP NIP
p(m) =W ¬∑q(m)+pÀÜ , A(m) =W ¬∑q(m),
q p q q A
whereŒªisahyperparameterusedtodeterminetherelative
p q =Concat(p( q1),¬∑¬∑¬∑ ,p( qM)). (4) lossweightbetweentheimageandtextdecodingbranches.
(cid:16) (cid:17)
A =softmax Concat(A(1),¬∑¬∑¬∑ ,A(M)) . Thewholeframeworkcanbeoptimizedend-to-end.
q q q
During inference, the images and texts are generated in
f =DeformAttn({FV}M ,A ,p ),
o m=1 q q an auto-regressive manner. Text tokens are sampled from
the distribution predicted by the multi-modal LLM. When
where PosEmbed ‚àà RM¬Ø√óC is a learnable positional em-
thegeneratedtokenis<BoI>,thediffusionmodeliscalled
bedding,mindexesfromthemaximumofM¬Ø referenceim-
forgeneratingthenextimage.
ages. W ,W ,W arelearnablelinearprojectionweights.
q p A
Thecoordinateofsamplingpointsp(m)andthecorrespond- 4.Experiment
q
ingattentionweightsA(m) arefirstcalculatedforeachim-
q 4.1.ImplementationDetails
age separately. Then, the attention weights are normal-
izedamongdifferentimagesviathesoftmaxfunction. The Network. We adopt CLIP-ViT-L/14 [70], Vicuna-
DeformAttn operator extracts feature at coordinates p ‚àà 13B[105]andStableDiffusionv2.1[74]astheimageen-
q
RM√óL√óK√ó2 fromthecorrespondingfeaturemap,andper- coder, large language model, and image decoder, respec-
formsaweightedsummationaccordingtoA ‚ààRM√óL√óK. tively. For the multi-modal LLM, a Perceiver Resampler
q
Here,LandKdenotethenumberofmulti-scalefeaturelev- with 12 blocks is used to reduce the number of visual to-
elsandsamplingpointsperfeaturemap,respectively. kensperimageto64. TheMMFSmoduleisinsertedevery
MMFScanbeappliedtobothimageandtextdecoding, 4blocksintheLLM.Fortheimagedecoder,aPerceiverRe-
avoidinginformationbottleneckscausedbythewidelyused samplerwithonly1blockisusedtoreducethenumberof
Resamplersinmulti-modalLLMs. Itisespeciallyefficient conditionaltokensperimageto77. TheMMFSmoduleis
forprocessingmultiplehigh-resolutionimagesincontext. insertedaftereachup-samplingblockintheimagedecoder.
5
‚Ä¶‚Ä¶.
‚Ä¶. ‚Ä¶.Method LLM H I A COCO Flickr NoCaps I2Para. VQAv2 OKVQA GQA VizWiz TextVQA VisDial
ModelsforText-GenerationOnly
MetaLM[30] MetaLM - - - 82.2 43.3 58.7 ‚Äì 41.1 11.4 ‚Äì 41.1 11.4
OF-9B[5] MPT-7B - - - 79.5 59.5 ‚Äì ‚Äì 52.7 37.8 ‚Äì 27.5 24.2 ‚Äì
IDEFICS-80B[36] LLaMA-65B - - - 91.8 53.7 65.0 60.0 ‚Äì 45.2 36.0 30.9 ‚Äì
KOSMOS-1[33] MetaLM H - - ‚Äì 65.2 ‚Äì ‚Äì 46.7 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
KOSMOS-2[67] KOSMOS-1 H - - ‚Äì 66.7 ‚Äì ‚Äì 45.6 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
Flamingo-9B[3] Chinchilla-7B H - - 79.4 61.5 ‚Äì ‚Äì 51.8 44.7 ‚Äì 28.8 31.8 48.0
Flamingo-80B[3] Chinchilla-70B H - - 84.3 67.2 ‚Äì ‚Äì 56.3 50.6 ‚Äì 31.6 35.0 52.0
IDEFICS-80B-I[36] LLaMA-65B - I - 117.2 65.3 104.5 37.4 ‚Äì ‚Äì 26.0 ‚Äì
mPLUG-DocOwl[95] LLaMA-7B - I A 52.6 62.2 57.4 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
BLIP-2[48] Vicuna-7B - I A ‚Äì 74.9 107.5 ‚Äì ‚Äì ‚Äì 38.6 25.3 40.1 ‚Äì
BLIP-2[48] Vicuna-13B - I A ‚Äì 71.6 103.9 ‚Äì 41.0 ‚Äì 41.0 19.6 42.5 ‚Äì
InstructBLIP[14] Vicuna-7B - I A ‚Äì 82.4 123.1 ‚Äì ‚Äì ‚Äì 49.2 34.5 50.1 ‚Äì
InstructBLIP[14] Vicuna-13B - I A ‚Äì 82.8 121.9 ‚Äì ‚Äì ‚Äì 49.5 33.4 50.7 ‚Äì
Shikra[10] Vicuna-13B - I A 117.5 73.9 ‚Äì ‚Äì 77.4 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì
LLaVA-1.5[52] Vicuna-7B - I A ‚Äì ‚Äì ‚Äì ‚Äì 78.5 ‚Äì 62.0 50.0 58.2 ‚Äì
LLaVA-1.5[52] Vicuna-13B - I A ‚Äì ‚Äì ‚Äì ‚Äì 80.0 ‚Äì 63.3 53.6 61.3 ‚Äì
VL-GPT-I[108] LLaMA-7B - I A 133.7 ‚Äì ‚Äì ‚Äì 67.2 50.3 51.5 38.9 ‚Äì 51.8
Qwen-VL[6] Qwen-7B H I A ‚Äì 85.8 121.4 ‚Äì 78.8 ‚Äì 59.3 35.2 63.8 ‚Äì
Qwen-VL-Chat[6] Qwen-7B H I A ‚Äì 81.0 120.2 ‚Äì 78.2 ‚Äì 57.5 38.9 61.5 ‚Äì
ModelsforbothImageandTextGeneration
CM3Leon[99] ‚Äì H - - 61.6 ‚Äì ‚Äì 10.5 47.6 23.8 ‚Äì 37.6 ‚Äì 22.6
Emu[83] Vicuna-13B H - - 112.4 ‚Äì ‚Äì ‚Äì 52.0 38.2 ‚Äì 34.2 ‚Äì 47.4
Emu-I[83] Vicuna-13B H - - 117.7 ‚Äì ‚Äì ‚Äì 40.0 34.7 ‚Äì 35.4 ‚Äì 48.0
Emu2[81] LLaMA-33B H - - ‚Äì ‚Äì ‚Äì ‚Äì 33.3 26.7 ‚Äì 40.4 26.2 ‚Äì
DreamLLM[18] Vicuna-7B - I - 115.4 ‚Äì ‚Äì 17.4 56.6 44.3 ‚Äì 38.1 34.9 ‚Äì
VL-GPT[108] LLaMA-7B - - - 116.4 ‚Äì ‚Äì ‚Äì 51.7 35.8 34.6 34.7 ‚Äì 49.9
MM-Interleaved Vicuna-13B - - - 129.0 85.8 106.4 23.5 57.0 40.0 ‚Äì 40.8 37.2 48.7
MM-Interleaved-SFT Vicuna-13B - I A 140.5 93.0 123.2 30.3 80.2 51.7 60.5 54.9 61.0 53.7
Table1. Multi-modalcomprehensionevaluation. ‚ÄòH‚Äôdenotesusingin-housedata,‚ÄòI‚Äômeansthetrainingimagesofsomebenchmarks
areincludedinthetraining,‚ÄòA‚Äômeansthetrainingannotationsofsomebenchmarksarevisibleintraining. Somebenchmarknamesare
abbreviatedduetospacelimits. COCO[11];Flickr: Flickr30k[68];NoCaps[2];I2Para.: Image2Paragraph[44];VQAv2: VQAv2[27];
OKVQA[59];GQA[35];VizWiz[29];TextVQA[80];VisDial[15].
Pre-training. Our models are pre-trained on a mix- tuned on four types of downstream tasks, which are 1) vi-
ture of interleaved image-text sequences and image-text sualquestion-answeringandimagecaption,2)referringex-
pairs, including MMC4 [109], LAION-2B [76], LAION- pressioncomprehension,3)segmentation-to-imagetransla-
COCO [77], CC-12M [8] and Objects365 [78]. For CC- tion,and4)visualstorytelling.
12M[8]andObjects365[78],insteadofutilizingtheorig- Moredetailsofdatasetsandhyper-parametersusedforpre-
inalannotations,weusethepre-trainedBLIP-2model[48] trainingandfine-tuningcouldbefoundintheAppendix.
tocaptiontheimages. ThesamplingprobabilityofMMC4
istwicethatofotherimage-textpairdatasets. Noin-house
4.2.Evaluation
dataisused.
The images are inserted before or after the correspond- Weevaluatethezero-shotvisual-languagecapabilityofthe
ingtextsentencewithequalprobability. Tooptimizetrain- pre-trained MM-Interleaved as well as its performance on
ing efficiency and data utility, multiple image-text pairs or various downstream tasks after fine-tuning and report the
interleavedimage-textsequencesareconcatenatedintoex- resultsinTab.1.
tended sequences with the maximum context length (i.e.,
Zero-shot Results after Pre-training. Our method
2,048tokens).
demonstrates strong multi-modal zero-shot comprehen-
Themodelispre-trainedfor15,000stepswith11billion
sion on various benchmarks, including image caption-
tokens visited. The image encoder and LLM are frozen.
ing on COCO [11], Flicker30K [68], NoCaps [2] and
Thelearningrateissettobe10‚àí5fortheimagedecoderand
Image2Paragraph [44]; general visual question answer-
10‚àí4fortheresttrainableparameters.Theinputandoutput
ing on VQAv2 [27], OKVQA [59], VizWiz [29] and
imageresolutionsare224√ó224and512√ó512,respectively.
TextVQA[80]. WealsoevaluateontheVisDial[15]visual
Supervised Fine-tuning. After pre-training, like multi- dialoguetask.Inthefullydecontaminatedsettingwherethe
modal LLMs, our model capabilities can be further en- imageandtextindownstreamtasksareunseenduringpre-
hanced using supervised fine-tuning. The model is fine- training,ourmodelsignificantlyoutperformsMetaLM[30],
6RefCOCO[42] RefCOCO+[58] RefCOCOg[58]
Model
Val Test-A Test-B Val Test-A Test-B Val Test
OFA-L[88] 79.96 83.67 76.39 68.29 76.00 61.75 67.57 67.50
VisionLLM-H[90] - 86.70 - - - - - -
Shikra[10] 87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19
MiniGPT-V2[9] 88.69 91.65 85.33 79.97 85.12 74.45 84.44 84.66
Ferret[96] 89.48 92.41 84.36 82.81 88.14 75.17 85.83 86.34
*Qwen-VL[6] 89.36 92.26 85.34 83.12 88.25 77.21 85.58 85.48
MM-Interleaved(ours) 89.92 92.59 86.54 82.99 88.57 77.07 85.21 84.92
Table2. Supervisedfine-tuningresultsonreferringexpressioncomprehensiontask. ‚Äú*‚Äùdenotesusinganadditionalself-constructed
groundingdatasetandtrainedwithanimageresolutionlargerthan224.
Model MS-COCO LN-COCO tages 1) our method is capable of generating both im-
ages and text, while LLaVA-1.5 can only generate text; 2)
Text-to-ImageSpecialists
LLaVA-1.5 uses 576 visual tokens as LM inputs, whereas
RetrievalResult 17.97 33.59
DALL-E[71] ‚àº28 - weonlyrequire64. Ourmodelachievescompetitiveimage
CogView[16] 27.10 - understandingwithfarfewervisualtokens,makingitbetter
CogView2[17] 24.00 -
suitedformulti-imagescenarios.
StableDiffusion[74] 12.43 34.26
GLIDE[63] 12.24 - Fine-tuning Results of Referring Expression Compre-
Make-A-Scene[25] 11.84 -
hension. Our model demonstrates state-of-the-art per-
DALL-E2[72] 10.39 -
Muse-3B[94] 7.88 - formance on various referring expression comprehension
Imagen-3.4B[75] 7.27 - (REC) benchmarks, as shown in Tab. 2. Compared to re-
Parti-20B[98] 7.23 15.97
centRECmodelslikeVisionLLM[90]andShikra[10],our
ModelsforbothImageandTextGeneration model outperforms on most datasets. Remarkably, even
CM3-13B[1] 29.56 - though we only use public REC data [42, 58] for fine-
VL-GPT[108] 12.25 -
tuning, our model matches QWen-VL [6] which utilizes a
GILL[43] 12.20 -
Emu-13B[83] 11.66 - extra 22M in-house grounding dataset and trains at higher
CM3Leon-7B[99] 10.82 - resolution(448vs224pixels). Thisshowstheeffectiveness
DreamLLM-7B-Stage1[18] 8.76 22.42
of synchronized fine-grained image features for enhancing
DreamLLM-7B[18] 8.46 20.53
MM-Interleaved-13B(ours) 7.90 23.88 theRECcapabilityofourmodel.
Fine-tuningResultsofSegmentation-to-ImageTransla-
Table 3. Zero-shot text-to-image generation results on MS-
tion. WefollowtheprotocolproposedinControlNet[100]
COCO[51]andLN-COCO[69].FID[31]isreported.
bygeneratingimagesfromADE20Kgroundtruthsemantic
masks and use OneFormer [38] for segmenting the gener-
OF-9B[5]andIDEFICS-80B[45]onallcaptioning,VQA,
atedimages.StandardmIoUisreportedasametricformea-
andmulti-modaltasks. Thisdemonstratestheeffectiveness
suring the semantic and structural fidelity of the generated
of the proposed MM-Interleaved approach. Furthermore,
images. We show the results in Tab. 4a. MM-Interleaved
ourmodelevenexceedsmosttrainedonvastin-housedata
clearly outperforms other baselines including ControlNet
like Flamingo-9B [3] and Emu [83], showing the impor-
by a large margin. Compared to ControlNet, our model
tanceofproperarchitectureforimage-textinteraction.
couldleveragethebetterrepresentationslearnedbyourgen-
Additionally, we evaluate text-conditional image gener- erative modeling from large-scale pre-training data, which
ation on MS-COCO [51] and LN-COCO [69]. On MS- benefits the image generation tasks. Moreover, thanks to
COCO, we sample 8 images per text condition and use MMFS,ourmodeliscapableofgeneratingrealisticimages
CLIP [70] to rerank based on text-image similarity. CLIP with pixel-level precise alignment from a semantic label
reranking is not used for LN-COCO. FID [31] is used to map, where simply relying on coarse Perceiver Resampler
evaluatebothdatasets. AsshowinTab.3,ourmodelshows featuresfailsonthistask, whichwewillshowintheabla-
competitive text-to-image generation compared to existing tionstudies.
imageandtextgenerationmodels.
Fine-tuning Results of Visual Storytelling. Besides
Fine-tuning Results of VQA and Captioning. Our fine- text-to-image and image-to-image generation, our MM-
tuned model achieves state-of-the-art performance on im- Interleaved could also condition on interleaved image-text
agecaptioningbenchmarks. Onvisualquestionanswering as context for generating new images. We evaluate our
tasks, itmatchesthepreviousbestLLaVA-1.5model[52]. model‚Äôsperformancebothforgeneratingthelastimageon
Compared to LLaVA-1.5, our model has two key advan- theVisualStoryTelling(VIST)dataset[34]andforgenerat-
7Model Pororo Flintstones
ADE20K(GT)[38] VQGAN[20] LDM[73] Method CLIPSim.‚Üë FID‚Üì
0.58 0.21 0.31 StoryDALL-E[57] 25.9 26.5
GILL[43] 0.64 - AR-LDM[66] 17.4 19.3
PIPT[89] ControlNet[100] Ours MiniGPT-5[104] 0.70 59.5
ACM-VSG[23] 15.4 18.4
0.26 0.35 0.44 MM-Interleaved 0.70 39.7
MM-Interleaved 14.7 18.7
(a) Segmentation-to-image generation results on (b)ImagegenerationresultsonVIST[34]. (c) Autoregressive image generation results on
ADE20K[106]mIoU(‚Üë)isreported. Pororo[49]andFlintStones[28].FIDs(‚Üì)arereported.
Table4.ComparativeanalysisofimagegenerationabilityofMM-Interleaved.
ingmultipleimagesautoregressivelyonthePororo[49]and when further increasing the input image resolution from
Flintstones[28]datasets.AsisshowninTab.4band 4c,our 224 to 448, as shown in Tab. 5b. Such results indicate
modelachievesSoTAperformanceonbothCLIP-Similarity our method could better exploit the additional information
andFIDmetricscomparedtopreviousworksonVISTand gainedfromhighresolution.
achievescompetitiveperformancecomparedwithprevious
Comparison between Different Cross Attention Mech-
specialistmethodsonPororoandFlinestones. Morequali-
anisms. The ablation of adopting different cross-attention
tativeresultsandimplementationdetailscanbefoundinthe
mechanismsforLLMisshowninTab.6. Theoverallper-
appendix.
formance drops when directly replacing MMFS with the
vanilladensecrossattention,possiblyduetoitsslowercon-
4.3.AblationStudy
vergencespeedon2Ddata,andusingaresamplerperforms
In the ablation study, we use CLIP-ViT-L/14 [70] as the equallywell. Wehighlightthatthemodelwithdeformable
vision foundation model, OpenLLaMA-3B v2 [26] as the attention performs significantly better than other attention
multi-modalLLMandminiSD1 astheimagedecoder. The mechanismsonTextVQA,indicatingthatdeformableatten-
outputresolutionofeachimageis256. tioncouldeffectivelyandefficientlycapturefine-grainedin-
Forsimplicity,inthissubsection,westudytheeffectof formationliketextneededforthetask,suchasvisualques-
token efficiency, image resolution, cross-attention mecha- tionansweringinthiscase.
nisms, and the role of MMFS in image generation in a
MMFSforImageGeneration. Westudywhetheradding
single-image setting, where each token only attends to the
MMFSiscriticalforimagegenerationonthesegmentation-
nearest preceding image. Please refer to the appendix for
to-imagetask.Thistaskishardasitrequirespreciouspixel-
moredetailedablationsstudiesinthemulti-imagesetting.
levelinformationtoalignthegivensegmentationcondition
The model is pre-trained for 10k steps on 80 NVIDIA
and image output properly. From the results in Tab. 5c, it
80G-A100 GPUs. The mini batch size of LAION-
is clear that the model without MMFS failed to generate
COCO[77],LAION-2B[76],andMMC4[109]onasingle
imagesspatiallyalignedwiththesegmentationimagecon-
GPUis2,2,and4respectively. Fortokenefficiency,image
dition,showingextremelylowmIoUresults.Weshowvisu-
resolution,andcross-attentionmechanism,weevaluatethe
alizationsofthegeneratedresultsinthesupplementaryma-
model‚Äôszero-shotperformanceonthreerepresentativetasks
terialtoprovideavisualcomparison. Itisworthnotingthat
andfourdatasets,i.e.,imagecaptionontheCOCOKarpa-
thequalityofthegeneratedimagefromthemodelwithout
thytestset,text-to-imagegenerationontheCOCOKarpa-
MMFS is still high and properly aligned with the text de-
thytestset,visualquestionansweringonOKVQA[59],and
scriptioninput;however,withoutMMFS,themodelcannot
TextVQA validation set. We use CIDEr, FID-5k, and top-
preserve all spatial information, and the spatial alignment
1accuracyastheevaluationmetrics. Forcross-attentionin
forthegeneratedresultsispoor.
imagegeneration,weperformsupervisedfine-tuningonthe
ADE20K[106]forthelabel-to-imagegenerationtask.
5.Conclusion
Token Efficiency. As shown in Tab. 5a, our method
achievesbetteroverallperformancewhileonlyusing32vi- In conclusion, this paper presented MM-Interleaved, an
sualtokensforeachinputimage. Suchresultsdemonstrate end-to-endgenerativemodelspecificallydesignedforinter-
the effectiveness of our method when the context length leaved image-text data. The unique contribution of MM-
is limited. To avoid the potential information bottleneck Interleaved lies in the novel multi-scale and multi-image
caused by the Resampler, we further increase the number featuresynchronizermodule, whichsignificantlyenhances
of visual tokens to 256. Our method still outperforms the the model‚Äôs capability to access and integrate fine-grained
baselineinthissetting. imagefeatures.Thisdesignaddressesthelongstandinglim-
itation in existing models where a fixed number of visual
Scaling up Input Image Resolutions. The performance
tokens inadequately capture intricate image details, espe-
gap between our method and the baseline becomes larger
cially in scenarios involving multiple images. Pre-trained
1https://huggingface.co/justinpinkney/miniSD on diverse image-text datasets and further refined through
8#Token w/MMFS Caption‚Üë Generation‚Üì OK-VQA‚Üë TextVQA‚Üë
w/MMFS Caption‚Üë Generation‚Üì OK-VQA‚Üë TextVQA‚Üë w/MMFS ADE20k‚Üë
32 107.0 32.2 28.7 22.5
32 ‚úì 110.6 30.0 29.8 27.7 110.5 30.3 29.9 24.9 5.3
256 110.7 32.7 29.2 23.6 ‚úì 115.2 30.5 30.6 30.8 ‚úì 35.9
256 ‚úì 110.9 29.5 29.6 27.8
(c)fine-tuningforim-
(a)pre-trainingwith224inputresolution (b)fine-tuningwith448inputresolution agetranslation
Table5.AblationontheusageofMMFSmodule.‚ÄúGenetation‚Äùisthetext-to-imagegenerationtask.‚ÄúADE20k‚Äùisthesegmentation-to-
imagetranslationtask.Othersaretextgenerationtasks.‚Äú#Token‚ÄùindicatesthenumberofinputvisualtokensforLLM(32bydefault).
Cross-Attn Transition AttnInput COCOCap.‚Üë COCOGen.‚Üì OK-VQA‚Üë TextVQA‚Üë w/Resampler w/MMFS Caption‚Üë Generation‚Üì OK-VQA‚Üë TextVQA‚Üë
Deformable None 16√ó16 110.6 30.0 29.8 27.7 ‚úì ‚úì 110.6 30.0 29.8 27.7
Dense None 16√ó16 108.5 30.6 28.4 23.6
Dense Resampler 32tokens 107.2 30.7 28.9 24.0 ‚úì 107.0 32.2 28.7 22.5
‚úì 102.7 32.0 27.3 22.0
Table6. AblationonthedesignchoiceofMMFSmodule. Dif-
ferent attention modules can be used in MMFS. We also ablate Table 8. The complementary relationship between MMFS and
whether to add additional transition layer before feeding image Resampler. When not using Resampler, we directly feed 32
featuresintoMMFS.TransformerattentionwithResamplertran- randomly-initializedlearnableembeddingsasinputvisualtokens
sitiononsinglescaleandsingleimageissimilartothecrossatten- intotheLLM.
tionusedinFlamingo. ‚ÄúSelf-Attention‚Äùhasthesameparameters
with‚ÄúResampler‚Äù.
MMFSwithMulti-ImageandMulti-Scale. Asshownin
supervised fine-tuning, MM-Interleaved shows a respect- Tab.9,addingmulti-imageandmulti-scaleforMMFSim-
fulproficiencyinprocessingcomplexmulti-modalinstruc- proves the model‚Äôs performance on VQA tasks. To better
tions. The empirical results modestly highlight its ability take advantage of the multi-image mechanism, we follow
to discern intricate visual details and generate images that [3,83]tofurtherevaluateourmodelwithfew-shotprompts.
alignaccuratelywithbothtextualandvisualinputs. Twotrainingsamplesincludingbothimageandtextarese-
lectedastheinputprompts. TheresultsinTab.10demon-
A.AdditionalAblationStudies stratesthatMMFSwithmulti-imageandmulti-scalebene-
fitsthemostfromthefew-shotsetting,indicatingitspoten-
This section provides more ablation studies for MM-
tialcapabilityforinterleavedimage-textmodeling.
Interleaved, all of which share the same settings as those
inthemaintextbydefault. Notethatthesemodelsarenot
fine-tunedondownstreamtasks. multi-scale multi-image Caption‚Üë Generation‚Üì OK-VQA‚Üë TextVQA‚Üë
110.6 30.0 29.8 27.7
Pre-training with Different Loss Terms. As showed in ‚úì 111.2 29.5 30.3 28.1
Tab.7, MM-Interleavedachievesbetterperformancewhen ‚úì ‚úì 111.2 29.9 31.1 28.2
jointly trained with both L and L on comprehen-
NTP NIP finetuningwith448inputresolution
sion and generation tasks, demonstrating the mutual bene-
115.2 30.5 30.6 30.8
fitsbetweenthetwolossterms. Moreover, settingŒª = 10 ‚úì 115.4 30.1 31.0 31.3
achievesabetterbalancebetweenL NTP andL NIP empir- ‚úì ‚úì 115.8 30.0 31.7 32.0
ically.
Table9.Zero-shotevaluationresultsofMMFS.
LossTerm Caption‚Üë Generation‚Üì OK-VQA‚Üë TextVQA‚Üë
LNTP +100LNIP 106.2 31.1 29.8 24.5
LNTP +10LNIP 110.6 30.0 29.8 27.7
multi-scale multi-image OK-VQA‚Üë TextVQA‚Üë
LNTP +LNIP 110.0 31.4 29.3 26.0
LNTP only 105.7 ‚Äì 29.9 27.6 36.5 30.7
LNIP only ‚Äì 34.2 ‚Äì ‚Äì ‚úì 38.5 31.1
‚úì ‚úì 38.7 33.1
Table7.Pre-trainingwithdifferentlossterms. finetuningwith448inputresolution
38.3 36.6
The Relationship between MMFS and resampler. The ‚úì 38.7 37.7
results in Tab. 8 validate the mutual benefits between our ‚úì ‚úì 39.1 37.9
proposedMMFSmoduleandtheResamplerusedinimage
Table10.2-shotevaluationresultsofMMFSonVQAtasks.
tokenizer, as removing either of them leads to an overall
performancedegradation.
9B.ImplementationDetails B.2.SupervisedFine-tuning
B.1.Pre-training VQA and Image Captioning. For this task, we
train our model in the form of question answer-
Dataset Details. We use MMC4 [109], LAION-2B,
ing, i.e., using Based on the image, please answer the
LAION-COCO [77], CC-12M [8] and Objects365 [78] as
question. {image}{question}. The answer is: {answer}
thepre-trainingdataset.LAION-2BistheEnglishsubsetof
as the instruction prompt. We utilize public available
LAION-5B[76],andwefurtherfilteritbasedonadditional
datasets for supervised fine-tuning, including LLaVA-
metricsincludingaestheticsscores. LAION-COCO [77]is
Mix-665K [52], COCO Caption [11], VQAv2 [27],
a 600M subset of LAION-2B, which is captioned by pre-
ChartQA [60], DocVQA [13], EST-VQA [92], In-
trainedBLIP[47]models. Textpromptswithlengthshorter
foVQA[61],STVQA[92],TextCaps[79],LLaVAR[102],
than 10 are also filtered out. For CC-12M [8] and Ob-
OCR-VQA [62], and DVQA [41]. See Tab. 12 for more
jects365 [78], instead of utilizing the original annotations,
trainingdetails.
we use the pre-trained BLIP-2 model [48] to caption the
images. Following previous works [18, 83], additional fil-
teringrulesareappledtotheMMC4dataset[109]. Specif- Hyper-parameters Value
ically,imageswithaCLIPsimilarityscorebelow0.24will
Inputimageresolution 448√ó448
be discarded, and only 6 images at most will be kept for
1e-6(languagemodel)
Learningrate
each document. We also exclude 100% of all documents 1e-5(others)
thatdonotcontainanyimages,and50%ofdocumentsthat Weightdecay 0.05
containonly1image. Warmupsteps 500
Data Concatenation Strategy. For image-text-pair Learningrateschedule cosine
datasets (e.g., LAION-2B, LAION-COCO [77]), we ran- Trainingiterations 10k
domly sample multiple image-text pairs from the same Optimizer AdamW
Optimizerhyper-parameters Œ≤ ,Œ≤ ,œµ=0.9,0.999,1e-8
dataset and concatenate them to the maximum context 1 2
Batchsize 256
length (i.e., 2048) during pre-training. For interleaved im-
ageandtextdatasets(i.e.,MMC4[109]),wealsosplitand
Table12.Hyper-parametersforVQAandimagecaptioning.
concatenate the documents to form the training samples.
Suchconcatenationstrategycanutilizethefullcontextwin-
dowofLargeLanguageModelsandthusachievehighdata Referring Expression Comprehension. Following
efficiency. previous works [6, 10], we train our model in the form
MoreTrainingDetails. Thedetailedhyper-parametersof of VQA, i.e., the prompt being Question: Provide the
pre-training are listed in Tab. 11. Besides that, for image bounding box coordinate of the region this sentence
generation,weignorethetraininglossofimageswhichare describes: {object}, Answer: (x1,y1)(x2,y2). The gen-
thefirstelementinthesequence. Thetextconditionofthe eratedboundingboxisconsideredcorrectifitsintersection
restimagesaredroppedwitha10%probabilitytoimprove over union (IoU) with the GT box is greater than 0.5.
classifier-freeguidancesampling[32]. Only public available datasets, including datasets from
RefCOCO [42], RefCOCO+ [58], and RefCOCOg [58]
Hyper-parameters Value are utilized to train MM-Interleaved. See Tab. 13 for
Inputimageresolution 224√ó224 fine-tuninghyper-parameters.
Outputimageresolution 512√ó512
VFM CLIP-ViT-L/14(frozen)
LLM Vicuna-13Bv1.3(frozen)
DM StableDiffusionv2.1 Hyper-parameters Value
Œª 10
Cross-attentionfrequency 4 Inputimageresolution 224√ó224
LLMMulti-scalefeaturemaps i=2,3,4 Learningrate 2e-5
1e-5(imagedecoder)
Learningrate 1e-4(others) Weightdecay 0.05
Weightdecay 0.05 Warmupsteps 500
Warmupsteps 1k
Learningrateschedule cosine
Learningrateschedule cosinewithwarmup
Trainingiterations 15k Trainingiterations 10k
Contextlength 2048 Optimizer AdamW
Optimizer AdamW
Optimizerhyper-parameters Œ≤1,Œ≤2,œµ=0.9,0.995,1e-6 Optimizerhyper-parameters Œ≤ 1,Œ≤ 2,œµ=0.9,0.999,1e-8
Data MMC4,LAION-2B,LAION-COCO,CC-12M,Objects365 Augmentation RandomHorizontalFlip
Augmentation CenterCrop
Batchsize 256
Batchsize(perGPU) 2,2,4
Table11.Hyper-parametersforpre-training. Table13. Hyper-parametersforreferringexpressioncomprehen-
sion.
10Segmentation-to-Image Translation. Following Con- Hyper-parameters VIST/PororoSV/FlintstonesSV
trolNet [100], we use BLIP [47] to generate text cap-
Inputimageresolution 224√ó224
tions for each image in the ADE20K dataset. We
Outputimageresolution 512√ó512
train our model on the training set and evaluate on
1e-4(imagedecoder)
the validation set. The input sequence is formulated Learningrate
1e-5(others)
as {segmentation image}{caption}{ground truth image} for Weightdecay 0.05
eachsegmentation-imagepair. SeeTab.14formoretrain- Warmupsteps 200
ingdetails. Learningrateschedule cosine
Trainingiterations 4k
Optimizer AdamW
Optimizerhyper-parameters Œ≤ ,Œ≤ ,œµ=0.9,0.98,1e-8
1 2
Hyper-parameters ADE20K
Augmentation CenterCrop
Inputimageresolution 224√ó224 Batchsize 128
Outputimageresolution 512√ó512
1e-5(imagedecoder) Table15.Hyper-parametersforvisualstorytelling.
Learningrate
1e-4(others)
Weightdecay 0.05
Warmupsteps 100 B.3.Evaluation
Learningrateschedule cosine
Benchmarks. Evaluating MM-Interleaved comprehen-
Trainingiterations 4k
Optimizer AdamW sively requires various benchmarks and datasets, such as
Optimizerhyper-parameters Œ≤ ,Œ≤ ,œµ=0.9,0.98,1e-5 image caption, visual question answering, text-to-image
1 2
Augmentation RandomHorizontalFlip generation and so on. All these evaluation tasks and met-
Batchsize 512 ricsarelistedinTab.16.
Image Generation. For all image generation tasks, the
Table14. Hyper-parametersforSegmentation-to-ImageTransla- scaleofclassifier-freeguidance[32]andthetotalinference
tion. stepissetas3.5and250bydefault.
TextGeneration. Theprompttemplatesforeachtextgen-
eration tasks are listed in Tab. 17. The ‚ÄòImage Caption
Visual Storytelling. Following previous works [43, 66, (short)‚ÄôtaskincludesCOCOCaption[11],Flicker30k[68]
104], MM-Interleaved is finetuned on the following three andNoCaps[2], whilethe‚ÄòImageCaption(long)‚Äôtaskin-
visualstorytellingdatasetsrespectively: cludesImage2Paragraph[44].
‚Ä¢ VIST [34] is a real-world vision-language dataset, con- C.QualitativeResults
taining 34k and 5k samples for training and evaluation.
Each sample is a sequence consisting of 5 text captions C.1.VLComprehension
andimages. Duringtraining,weconcatenateallthetexts
TextReadingQA.AsisshowninFig.5,MM-Interleaved
and images sequentially and the model is trained to pre-
with MMFS provides more accurate answers when requir-
dict all images. During inference, we test the model on
ingfine-graineddetailsforgeneratingtextoutputsgivenVL
generating the last image in the sequence, conditioned
inputs.
on all preceding images and texts following [43, 104].
ReferringExpressionComprehension. Thevisualization
The evaluation metrics are FID [31] and the CLIP sim-
of MM-Interleaved on REC tasks is shown in Fig. 6. Our
ilarity [70] between the generated images and the corre-
model with MMFS is also capable of generating more ac-
spondingrealimages.
curate coordinates given the referring expression and the
‚Ä¢ PororoSV [49] and FlintstonesSV [56] are two cartoon
queryimage.
storytelling datasets, containing 10191/2334/2208 and
20132/2071/2309samplesofthetrain,validation,andtest C.2.ImageGeneration
set,respectively. Eachsampleisasequenceconsistingof
Segmentation-to-imageTranslation. Fig.7showsthevi-
5textcaptionsandframeimages. Duringtraining,allthe
sualizationresultsofMM-Interleavedforsegmentation-to-
texts and images are concatenated sequentially and the
imagetranslation. Giventhetextpromptandsegmentation
modelistrainedtopredictallimages. Duringinference,
map,thespatiallayoutofimagesgeneratedwithMMFSis
the last 4 images are generated auto-regressively given
significantlyclosertotheoriginalground-truthimage,com-
the first image and all preceding captions as condition.
paredtothebaselineresultswithoutMMFS.
FID[31]isusedastheevaluationmetric.
Multi-imageGeneration. InFig.8,wecomparethemulti-
Thefinetuninghyper-parametersarelistedinTab.15. pleimagessequentiallygeneratedbyMM-Interleavedwith
11Dataset Task Split Metric
COCO[11] Scenedescription test CIDEr(‚Üë)[87]
Flickr30k[68] Scenedescription test CIDEr(‚Üë)[87]
NoCaps[2] Scenedescription test CIDEr(‚Üë)[87]
Image2Paragraph[44] Scenedescription test CIDEr(‚Üë)[87]
VQAv2[27] SceneunderstandingQA test-dev VQAAcc(‚Üë)[4]
OKVQA[59] ExternalknowledgeQA val VQAAcc(‚Üë)[4]
GQA[35] SceneunderstandingQA test-dev VQAAcc(‚Üë)[4]
VizWiz[29] SceneunderstandingQA test-dev VQAAcc(‚Üë)[4]
TextVQA[80] TextreadingQA val VQAAcc(‚Üë)[4]
VisDial[15] Imagedialogue val NDCG(‚Üë)
RefCOCO[42] Referringexperssioncomprehension - IoUAcc(‚Üë)
RefCOCO+[58] Referringexperssioncomprehension - IoUAcc(‚Üë)
RefCOCOg[58] Referringexperssioncomprehension - IoUAcc(‚Üë)
MS-COCO[51] Text-to-imagegeneration val-30K FID(‚Üì)[31]
LN-COCO[69] Text-to-imagegeneration val-30k FID(‚Üì)[31]
ADE20k[106] Segmentation-to-imagegeneration val mIoU(‚Üë)
VIST[34] Interleaved-contextimagegeneration val CLIP-Sim(‚Üë)[70],FID(‚Üì)[31]
PororoSV[49] Interleaved-contextmulti-imagegeneration test FID(‚Üì)[31]
FlintstonesSV[28] Interleaved-contextmulti-imagegeneration test FID(‚Üì)[31]
Table16. Summaryofevaluationbenchmarks,includingimagecaption,visualquestionanswering,referringexperssioncomprehension,
andimagegeneration.
Task PromptTemplate
ImageCaption(short) {image} a photo of
ImageCaption(long) {image} Please describe the image in detail. The image depicts
VQA(exceptVizWiz) Based on the image, please answer the question. {image}{question}
Please provide an accurate answer within one word. The answer is:
Zero-shot
Based on the image, please answer the question. {image}{question}
VizWizQA When the provided information is insufficient, respond with
‚ÄôUnanswerable‚Äô. Please provide an accurate answer within one word.
The answer is:
VisualDialog {image} caption: {caption} question: {history question}? answer:
{history answer}. ¬∑¬∑¬∑ question: {question}? answer:
ImageCaption(short) {image} Provide a one-sentence caption for the provided image.
ImageCaption(long) {image} Please describe the image in detail.
Based on the image, please answer the question. {image}{question}
VQA(exceptVizWiz)
Supervised Please provide an accurate answer within one word. The answer is:
Fine-tuning
Based on the image, please answer the question. {image}{question}
When the provided information is insufficient, respond with
VizWizVQA
‚ÄôUnanswerable‚Äô. Please provide an accurate answer within one word.
The answer is:
{image} caption: {caption} question: {history question}? answer:
VisualDialog
{history answer}. ¬∑¬∑¬∑ question: {question}? answer:
Table17.Prompttemplatesfortextgeneration.
12
.noitpaC
.AQV
.CER
.noitareneGUser: what year is written on the bottom? User: what is in the bottles?
MM-Interleaved(w. MMFS): 2011. MM-Interleaved(w. MMFS): Corona.
MM-Interleaved (w/o MMFS): 2022. MM-Interleaved (w/o MMFS): Tequila.
User: what is the number of the player at bat? User: what brand is on the middle shelf?
MM-Interleaved (w. MMFS): 79. MM-Interleaved(w. MMFS): Coke.
MM-Interleaved (w/o MMFS): 9. MM-Interleaved (w/o MMFS): Pepsi.
User: how many stars can you see on the white board? User: what brand is represented here?
MM-Interleaved(w. MMFS): 3. MM-Interleaved(w. MMFS): Intel.
MM-Interleaved (w/o MMFS): 5. MM-Interleaved (w/o MMFS): Dell.
Figure 5. Qualitative results on TextVQA [80]. Each example consists of the user query, the answer given by MM-Interleaved with
MMFS,andtheanswergivenbyMM-InterleavedwithoutMMFS.Theimageshapesarenormalizedforvisualization.
User: a sweet foo in an orange bowl User: a laptop
MM-Interleaved(w. MMFS): (595,090)(855,520) MM-Interleaved(w. MMFS): (581,001)(999,989)
MM-Interleaved(w/o MMFS): (595,507)(892,981) MM-Interleaved(w/o MMFS): (336,170)(619,754)
User: an orange bus in between two other buses User: part of a small white airplane with three windows
MM-Interleaved(w. MMFS): (619,507)(892,792) MM-Interleaved(w. MMFS): (000,316)(418,599)
MM-Interleaved(w/o MMFS): (482,514)(634,790) MM-Interleaved(w/o MMFS): (000,000)(997,707)
User: a head of broccoli sits on the table User: zebra with its head behind the other zebra
MM-Interleaved(w. MMFS): (000,569)(242,984) MM-Interleaved(w. MMFS): (276,406)(563,727)
MM-Interleaved(w/o MMFS): (000,000)(402,627) MM-Interleaved(w/o MMFS): (446,422)(684,709)
Figure6. ReferringExpressionComprehensiononRefCOCOg[58]. Eachexampleconsistsoftheuserquery,theboxpredictedwith
MMFS,andtheboxpredictedwithoutMMFS.Theimageshapesarenormalizedforvisualization.
and without MMFS. The images generated with MMFS modelisfurtherfinetunedtogeneratebothimagesandtexts
achievesbetterspatialconsistency(e.g.thebackgrounden- simultaneously for visual storytelling tasks. As is shown
vironment,changeofviewpoint,characterpositionrelation- in Fig. 9, given the first frame image and caption as con-
ship etc.) and closer semantic alignment with the inter- text, MM-Interleaved with MMFS generates the following
leavedimage-textcontext. interleaved images and texts coherently, achieving balance
GeneratingInterleavedImageandTexts. Moreover, the betweenthegenerationdiversityandspatialsemanticcon-
13Segmentation GT Generated Generated
map image w. MMFS w/oMMFS
a large brick
building with a red
roof
a large building
with a clock tower
in the middle of it
a small plane on a
runway with a man
on a lawn mower
a lobby with a
couch and chairs
and a table
a bedroom with a
bed, a bookcase,
and a potted plant
a store with a
display of sports
apparel
a dining room
table with a china
cabinet
Figure7.Segmentation-to-ImageGenerationonADE20k[106].Eachrowisanexampleconsistingoffourimages,whicharetheinput
segmentationmap,thegroundtruthimage,thegeneratedimagewithMMFS,andthegeneratedimagewithoutMMFSrespectively. The
shapeofground-truthimagesandsegmentationmapsisnormalizedforvisualization. WhenwithoutMMFS,thegeneratedresultslack
spatialalignmentwiththeinputsegmentationmaps.
sistency. image-textmodeling.
References
D.FutureWork
[1] Armen Aghajanyan, Bernie Huang, Candace Ross,
Future works of MM-Interleaved aims to further scale up
Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro
both the model and data size, enabling end-to-end train- Okhonko, MandarJoshi, GargiGhosh, MikeLewis, etal.
ing with full-parameters from scratch. In addition, we be- Cm3: Acausalmaskedmultimodalmodeloftheinternet.
lievethatitisalsoworthexploringtobuildacomprehensive arXivpreprintarXiv:2201.07520,2022. 7
evaluationbenchmarkspecificallydesignedforinterleaved [2] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,
14[frame:0] Wilma is on a surfboard on a huge wave in the ocean,
[frame:0] Loopy wants to be skinny. Eddy Crong Pororo and Poby and then Fred falls onto her shoulders.
looks at Loopy. [frame:1] Barney is in a car with Fred and talking to Fred as he
Input [frame:1] Eddy Crong Pororo and Poby encourage her. Poby stands drives.
context up. [frame:2] Barney is in the car. he talks while Fred sits next to him.
[frame:2] Poby stands up and give an advice. [frame:3] Fred talks to Barney while driving his car. Barney rides in
[frame:3] Poby moves his arms. Poby thinks exercise is important. the passenger seat.
[frame:4] Loopy thinks about doing exercise. [frame:4] the man in blue shirt is in a dressing room. he is leaning
on the vanity table next to a guitar. he is talking.
GT
image
Generated
w. MMFS
Generated
w/o MMFS
[frame:0] Wilma is standing in a room. she is holding a perfume
[frame:0] Pororo and his friends are angry. Eddy is surprised. bottle and speaking.
[frame:1] Petty blames Eddy. Pororo and his friends look at [frame:1] Wilma is laying on the couch in the living room. she has
Input Eddy. her arms behind her head and she is talking.
context [frame:2] Loopy says angrily. Pororo shakes his head. [frame:2] Wilma is lying on the living room couch. Fred walks
[frame:3] Rody says to Eddy. Eddy looks discouraged. toward her.
[frame:4] Eddy looks down and scratches his head. [frame:3] Wilma yells at Fred while lying on the couch in the living
room.
[frame:4] Fred is in the room and puts on sunglasses.
GT
image
Generated
w. MMFS
Generated
w/o MMFS
Figure8. Multi-imageGenerationonPororoSV[49]andFlintstonesSV[28]. Eachexampleconsistsoffourrows. Thefirstrowisthe
firstframeimageandallcorrespondingcaptions.Thesecondrowcontainsthegroundtruthimagesoffollowingframes;Thethirdroware
thegeneratedresultswithMMFS;AndthelastrowaretheresultsgeneratedwithoutMMFS.WhenwithoutMMFS,thegeneratedmultiple
imageslackcontentconsistencyintermsofcharacters,backgrounds,objects,etc.
Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Bitton,SamirGadre,ShioriSagawa,etal. Openflamingo:
StefanLee,andPeterAnderson.Nocaps:Novelobjectcap- Anopen-sourceframeworkfortraininglargeautoregressive
tioningatscale. InICCV,2019. 6,11,12 vision-languagemodels. arXivpreprintarXiv:2308.01390,
[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An- 2023. 6,7
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur [6] JinzeBai, ShuaiBai, ShushengYang, ShijieWang, Sinan
Mensch, Katherine Millican, Malcolm Reynolds, et al. Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Flamingo: avisuallanguagemodelforfew-shotlearning. Zhou. Qwen-vl: A frontier large vision-language model
NeurIPS,2022. 2,3,4,6,7,9 with versatile abilities. arXiv preprint arXiv:2308.12966,
[4] StanislawAntol,AishwaryaAgrawal,JiasenLu,Margaret 2023. 3,6,7,10
Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
Parikh. VQA:visualquestionanswering. InICCV,2015. biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
12 tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
[5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Languagemodelsarefew-shotlearners. NeurIPS,2020. 1
Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan [8] SoravitChangpinyo,PiyushSharma,NanDing,andRadu
15Ground Generated Generated Ground Generated Generated
Truth w. MMFS w/o MMFS Truth w. MMFS w/o MMFS
Eddy is determined Eddy is determined Eddy is determined
[frame 0] to hit the ball well. to hit the ball well. to hit the ball well. Wilma is talking to Wilma is talking to Wilma is talking to
Eddy is holding the Eddy is holding the Eddy is holding the Betty. they are in a Betty. they are in a Betty. they are in a
text
ball and talking to ball and talking to ball and talking to room together. room together. room together.
Pororo. Pororo. Pororo.
[frame 0]
image
Wilma and Betty are
Eddy seems very Wilma and Betty are standing in a room. Wilma and Betty are
[frame 1] d the rt ee wrm thin ee bd a. lE l d bd ay c k. Pororo smiles and E bd ad ll y b t ur ti e ts h eto b h ai lt l the i fn a ca in r go o em ac. h t h oe ty h ea rr e W toi l Bm eta t yis . Bsp ee tta yk li ong o ks s Wta iln md ain sg p ein a ka s r o too m.
text Eddy is getting ready to play. cannot be go over while Betty does all at Wilma while Betty and Betty
ready to hit the ball. the net. the talking to Wilma. touching her hand responds.
to her chin.
[frame 1]
image
Pororo is getting Wilma and Betty are
Pororo says he is Betty and Wilma are
ready to catch the standing in a room.
[frame 2] ball. Eddy is getting going to play with Petty jumps and Betty is talking to Wilma and Betty are having a
the ball first. Pororo Loopy spins due to standing in the conversation while
text ready to hit the ball. doesn't give the ball Crong's mistake. Wilma. Wilma has room talking. standing in the
Rody is holding the her head turned
to Eddy. living room.
ball. toward Betty.
[frame 2]
image
Pororo is playing
Rody is thinking with the ball. Eddy is
Betty is standing in Wilma and Betty are
[frame 3] about how to throw watching him Eddy and Rody look the room talking. standing in a room Wilma and Betty are
the ball. Rody is playing. Pororo is disappointed. Crong talking to each
text going to throw the throwing the ball at picks up the ball. Wilma is standing speaking to each other in a room.
next to Betty. other.
ball. the wall and
catching it again.
[frame 3]
image
Eddy is getting Wilma and Betty
ready to throw the Pororo is happy that Wilma and Betty are are in the living
ball. Eddy is Pororo got the ball. standing in a room. Wilma is in the room. Wilma is
[frame 4] swinging the bat. Eddy seems angry. Eddy suggests Betty is holding her room. she is saying speaking to
text Eddy looks very Eddy also wants to Crong to give the arms up while something she someone off screen.
confident. Pororo is play with the ball. ball to Eddy. talking and Wilma seems certain about. Betty looks at
preparing to catch turns to look at her. Wilma and then
the ball. speaks to someone.
[frame 4]
image
Figure9.InterleavedImage-TextGenerationonPororoSV[49]andFlintstonesSV[28].Eachexampleconsistsofthreecolumns.The
firstcolumnistheground-truthimagesandcaptionsofallframes. ThesecondcolumnarethegeneratedresultswithMMFS;Andthelast
columnaretheresultsgeneratedwithoutMMFS.Onlythecaptionandimageofthefirstframeisgivenasconditionduringgeneration.
16
Interleaved
generating
images
and
textsSoricut. Conceptual 12m: Pushing web-scale image-text [22] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang,
pre-training to recognize long-tail visual concepts. In XinlongWang, andYueCao. Eva-02: Avisualrepresen-
CVPR,2021. 6,10 tationforneongenesis. arXivpreprintarXiv:2303.11331,
[9] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun 2023. 1
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, [23] Zhangyin Feng, Yuchen Ren, Xinmiao Yu, Xiaocheng
VikasChandra,YunyangXiong,andMohamedElhoseiny. Feng,DuyuTang,ShumingShi,andBingQin. Improved
Minigpt-v2: large language model as a unified interface visual story generation with adaptive context modeling.
for vision-language multi-task learning. arXiv preprint arXivpreprintarXiv:2305.16811,2023. 8
arXiv:2310.09478,2023. 7 [24] SamirYitzhakGadre,GabrielIlharco,AlexFang,Jonathan
[10] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,
Feng Zhu, and Rui Zhao. Shikra: Unleashing multi- MitchellWortsman,DhrubaGhosh,JieyuZhang,etal.Dat-
modal llm‚Äôs referential dialogue magic. arXiv preprint acomp: In search of the next generation of multimodal
arXiv:2306.15195,2023. 3,6,7,10 datasets. arXivpreprintarXiv:2304.14108,2023. 2
[11] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna [25] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Vedantam, Saurabh Gupta, Piotr Dolla¬¥r, and C Lawrence Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-
Zitnick.Microsoftcococaptions:Datacollectionandeval- based text-to-image generation with human priors. In
uationserver. arXivpreprintarXiv:1504.00325, 2015. 6, ECCV,2022. 7
10,11,12 [26] XinyangGengandHaoLiu. Openllama: Anopenrepro-
[12] ZheChen,YuchenDuan,WenhaiWang,JunjunHe,Tong ductionofllama,2023. 8
Lu,JifengDai,andYuQiao.Visiontransformeradapterfor [27] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv
densepredictions. InICLR,2022. 4 Batra, and Devi Parikh. Making the v in vqa matter: El-
[13] ChristopherClarkandMattGardner. Simpleandeffective evatingtheroleofimageunderstandinginvisualquestion
multi-paragraph reading comprehension. arXiv preprint answering. InCVPR,2017. 6,10,12
arXiv:1710.10723,2017. 10 [28] Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek
[14] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat, Hoiem, and Aniruddha Kembhavi. Imagine this! scripts
JunqiZhao,WeishengWang,BoyangLi,PascaleFung,and tocompositionstovideos. InECCV,2018. 8,12,15,16
StevenHoi. Instructblip: Towardsgeneral-purposevision- [29] DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,Chi
language models with instruction tuning. arXiv preprint Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.
arXiv:2305.06500,2023. 6 Vizwizgrandchallenge: Answeringvisualquestionsfrom
[15] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, blindpeople. InCVPR,2018. 6,12
Deshraj Yadav, Jose¬¥ MF Moura, Devi Parikh, and Dhruv [30] YaruHao,HaoyuSong,LiDong,ShaohanHuang,Zewen
Batra. Visualdialog. InCVPR,2017. 6,12 Chi,WenhuiWang,ShumingMa,andFuruWei.Language
[16] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, models are general-purpose interfaces. arXiv preprint
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, arXiv:2206.06336,2022. 6
Hongxia Yang, et al. Cogview: Mastering text-to-image [31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
generationviatransformers. NeurIPS,2021. 7 Bernhard Nessler, and Sepp Hochreiter. Gans trained by
[17] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. atwotime-scaleupdateruleconvergetoalocalnashequi-
Cogview2: Faster and better text-to-image generation via librium. NeurIPS,2017. 7,11,12
hierarchicaltransformers. NeurIPS,2022. 7 [32] Jonathan Ho and Tim Salimans. Classifier-free diffusion
[18] RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,Zheng guidance. arXivpreprintarXiv:2207.12598,2022. 10,11
Ge,JinrongYang,LiangZhao,JianjianSun,HongyuZhou, [33] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
HaoranWei,etal.Dreamllm:Synergisticmultimodalcom- Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,
prehensionandcreation. arXivpreprintarXiv:2309.11499, Owais Khan Mohammed, Qiang Liu, et al. Language is
2023. 1,3,6,7,10 notallyouneed: Aligningperceptionwithlanguagemod-
[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, els. arXivpreprintarXiv:2302.14045,2023. 2,6
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, [34] Ting-HaoK.Huang,FrancisFerraro,NasrinMostafazadeh,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, IshanMisra,JacobDevlin,AishwaryaAgrawal,RossGir-
SylvainGelly,etal.Animageisworth16x16words:Trans- shick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al.
formersforimagerecognitionatscale. InICLR,2020. 3 Visualstorytelling. InNAACL,2016. 7,8,11,12
[20] PatrickEsser,RobinRombach,andBjornOmmer. Taming [35] DrewAHudsonandChristopherDManning. Gqa:Anew
transformersforhigh-resolutionimagesynthesis.InCVPR, dataset for real-world visual reasoning and compositional
2021. 8 questionanswering. InCVPR,2019. 6,12
[21] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu, [36] IDEFICS. Introducing idefics: An open reproduction
Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue of state-of-the-art visual language model. https://
Cao. Eva: Exploringthelimitsofmaskedvisualrepresen- huggingface.co/blog/idefics,2023. 6
tationlearningatscale. arXivpreprintarXiv:2211.07636, [37] GabrielIlharco,MitchellWortsman,RossWightman,Cade
2022. 1 Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
17VaishaalShankar,HongseokNamkoong,JohnMiller,Han- [51] Tsung-Yi Lin, Michael Maire, Serge Belongie, James
nanehHajishirzi,AliFarhadi,andLudwigSchmidt. Open- Hays, Pietro Perona, Deva Ramanan, Piotr Dolla¬¥r, and
clip,2021. 1,2 CLawrenceZitnick. Microsoftcoco: Commonobjectsin
[38] JiteshJain,JiachenLi,MangTikChiu,AliHassani,Nikita context. InECCV,2014. 7,12
Orlov,andHumphreyShi. Oneformer: Onetransformerto [52] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.
ruleuniversalimagesegmentation. InCVPR,2023. 7,8 Improved baselines with visual instruction tuning. arXiv
[39] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana preprintarXiv:2310.03744,2023. 6,7,10
Parekh, HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, [53] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
and Tom Duerig. Scaling up visual and vision-language Lee. Visual instruction tuning. arXiv preprint
representation learning with noisy text supervision. In arXiv:2304.08485,2023. 1,2,3,4
ICML,2021. 2 [54] Calvin Luo. Understanding diffusion models: A unified
[40] YangJin,KunXu,KunXu,LiweiChen,ChaoLiao,Jian- perspective. arXivpreprintarXiv:2208.11970,2022. 4
chao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, [55] TengchaoLv,YupanHuang,JingyeChen,LeiCui,Shum-
ChengruSong,XiaoqiangLei,DiZhang,WenwuOu,Kun ingMa,YaoyaoChang,ShaohanHuang,WenhuiWang,Li
Gai, and Yadong Mu. Unified language-vision pretrain- Dong, WeiyaoLuo, etal. Kosmos-2.5: Amultimodallit-
inginllmwithdynamicdiscretevisualtokenization. arXiv erate model. arXiv preprint arXiv:2309.11419, 2023. 2,
preprintarXiv:2309.04669,2023. 1 3
[56] Adyasha Maharana and Mohit Bansal. Integrating visu-
[41] Kushal Kafle, Brian Price, Scott Cohen, and Christopher
ospatial, linguistic and commonsense structure into story
Kanan. Dvqa: Understandingdatavisualizationsviaques-
visualization. arXivpreprintarXiv:2110.10834,2021. 11
tionanswering. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pages5648‚Äì5656, [57] Adyasha Maharana, Darryl Hannan, and Mohit Bansal.
2018. 10 Storydall-e: Adapting pretrained text-to-image transform-
ersforstorycontinuation. InECCV,2022. 8
[42] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
[58] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Tamara Berg. Referitgame: Referring to objects in pho-
Camburu, Alan L Yuille, and Kevin Murphy. Generation
tographsofnaturalscenes. InEMNLP,2014. 7,10,12
andcomprehensionofunambiguousobjectdescriptions.In
[43] JingYuKoh,DanielFried,andRuslanSalakhutdinov.Gen-
CVPR,2016. 7,10,12,13
erating images with multimodal language models. arXiv
[59] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
preprintarXiv:2305.17216,2023. 1,3,7,8,11
RoozbehMottaghi. Ok-vqa: Avisualquestionanswering
[44] Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li
benchmarkrequiringexternalknowledge. InCVPR,2019.
Fei-Fei. Ahierarchicalapproachforgeneratingdescriptive
6,8,12
imageparagraphs. InCVPR,2017. 6,11,12
[60] AhmedMasry, DoXuanLong, JiaQingTan, ShafiqJoty,
[45] HugoLaurenc¬∏on,LucileSaulnier,Le¬¥oTronchon,StasBek-
and Enamul Hoque. Chartqa: A benchmark for question
man, Amanpreet Singh, Anton Lozhkov, Thomas Wang,
answering about charts with visual and logical reasoning.
Siddharth Karamcheti, Alexander M Rush, Douwe Kiela,
arXivpreprintarXiv:2203.10244,2022. 10
et al. Obelics: An open web-scale filtered dataset of in-
[61] Minesh Mathew, Viraj Bagal, Rube`n Tito, Dimosthenis
terleavedimage-textdocuments. InThirty-seventhConfer-
Karatzas,ErnestValveny,andCVJawahar.Infographicvqa.
ence on Neural Information Processing Systems Datasets
InProceedingsoftheIEEE/CVFWinterConferenceonAp-
andBenchmarksTrack,2023. 2,7
plicationsofComputerVision,pages1697‚Äì1706,2022. 10
[46] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
[62] AnandMishra,ShashankShekhar,AjeetKumarSingh,and
Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.
AnirbanChakraborty. Ocr-vqa: Visualquestionanswering
Align before fuse: Vision and language representation
byreadingtextinimages.InICDAR,pages947‚Äì952.IEEE,
learningwithmomentumdistillation. NeurIPS,2021. 2
2019. 10
[47] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. [63] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Blip: Bootstrapping language-image pre-training for uni- Shyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and
fied vision-language understanding and generation. In MarkChen. Glide: Towardsphotorealisticimagegenera-
ICML,2022. 10,11 tionandeditingwithtext-guideddiffusionmodels. arXiv
[48] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. preprintarXiv:2112.10741,2021. 7
Blip-2: Bootstrapping language-image pre-training with [64] OpenAI. Gpt-4 technical report. arXiv preprint
frozen image encoders and large language models. arXiv arXiv:2303.08774,2023. 1,2
preprintarXiv:2301.12597,2023. 1,2,3,4,6,10 [65] TBOpenAI. Chatgpt: Optimizinglanguagemodelsfordi-
[49] YitongLi,ZheGan,YelongShen,JingjingLiu,YuCheng, alogue. OpenAI,2022. 2
YuexinWu,LawrenceCarin,DavidCarlson,andJianfeng [66] XichenPan,PengdaQin,YuhongLi,HuiXue,andWenhu
Gao. Storygan: Asequentialconditionalganforstoryvi- Chen. Synthesizingcoherentstorywithauto-regressivela-
sualization. InCVPR,2019. 8,11,12,15,16 tent diffusion models. arXiv preprint arXiv:2211.10950,
[50] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feicht- 2022. 8,11
enhofer, and Kaiming He. Scaling language-image pre- [67] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan
trainingviamasking. InCVPR,2023. 2 Huang,ShumingMa,andFuruWei. Kosmos-2: Ground-
18ingmultimodallargelanguagemodelstotheworld. arXiv [81] QuanSun,YufengCui,XiaosongZhang,FanZhang,Qiy-
preprintarXiv:2306.14824,2023. 6 ing Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao,
[68] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Jingjing Liu, Tiejun Huang, et al. Generative multi-
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb- modal models are in-context learners. arXiv preprint
nik. Flickr30kentities: Collectingregion-to-phrasecorre- arXiv:2312.13286,2023. 6
spondencesforricherimage-to-sentencemodels. InICCV, [82] QuanSun,YuxinFang,LedellWu,XinlongWang,andYue
2015. 6,11,12 Cao. Eva-clip: Improved training techniques for clip at
[69] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, scale. arXivpreprintarXiv:2303.15389,2023. 1,2
RaduSoricut,andVittorioFerrari. Connectingvisionand [83] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong
languagewithlocalizednarratives. InECCV,2020. 7,12 Zhang,YuezeWang,HongchengGao,JingjingLiu,Tiejun
Huang,andXinlongWang. Generativepretraininginmul-
[70] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
timodality.arXivpreprintarXiv:2307.05222,2023.1,3,6,
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
7,9,10
AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
ingtransferablevisualmodelsfromnaturallanguagesuper- [84] Han Tian, Chaoliang Zeng, Zhenghang Ren, Di Chai,
vision. InICML,2021. 1,2,4,5,7,8,11,12 Junxue Zhang, Kai Chen, and Qiang Yang. Sphinx: En-
ablingprivacy-preservingonlinelearningoverthecloud.In
[71] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
2022IEEESymposiumonSecurityandPrivacy(SP),2022.
Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
2,3
Sutskever. Zero-shot text-to-image generation. In ICML,
[85] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
2021. 1,2,7
Martinet, Marie-Anne Lachaux, Timothe¬¥e Lacroix, Bap-
[72] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
tiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar,
Chu, and Mark Chen. Hierarchical text-conditional
etal. Llama:Openandefficientfoundationlanguagemod-
image generation with clip latents. arXiv preprint
els. arXivpreprintarXiv:2302.13971,2023. 4
arXiv:2204.06125,2022. 7
[86] AaronVanDenOord,OriolVinyals,etal. Neuraldiscrete
[73] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
representationlearning. NeurIPS,2017. 3
Patrick Esser, and Bjo¬®rn Ommer. High-resolution image
[87] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi
synthesiswithlatentdiffusionmodels. InCVPR,2022. 8
Parikh. Cider: Consensus-basedimagedescriptionevalu-
[74] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
ation. InCVPR,2015. 12
Patrick Esser, and Bjo¬®rn Ommer. High-resolution image
[88] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
synthesiswithlatentdiffusionmodels. InCVPR,2022. 1,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
2,4,5,7
Hongxia Yang. Ofa: Unifying architectures, tasks, and
[75] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
modalitiesthroughasimplesequence-to-sequencelearning
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
framework. InICML,2022. 7
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-
[89] TengfeiWang,TingZhang,BoZhang,HaoOuyang,Dong
mans, etal. Photorealistictext-to-imagediffusionmodels
Chen, Qifeng Chen, and Fang Wen. Pretraining is all
withdeeplanguageunderstanding. NeurIPS,2022. 7
you need for image-to-image translation. arXiv preprint
[76] ChristophSchuhmann,RomainBeaumont,RichardVencu, arXiv:2205.12952,2022. 8
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
[90] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
XizhouZhu,GangZeng,PingLuo,TongLu,JieZhou,Yu
man,etal. Laion-5b:Anopenlarge-scaledatasetfortrain-
Qiao, et al. Visionllm: Large language model is also an
ingnextgenerationimage-textmodels. NeurIPS,2022. 2,
open-endeddecoderforvision-centrictasks.arXivpreprint
6,8,10
arXiv:2305.11175,2023. 2,4,7
[77] Christoph Schuhmann, Andreas Ko¬®pf, Richard Vencu, [91] WeiyunWang,MinShi,QingyunLi,WenhaiWang,Zhen-
Theo Coombes, and Romain Beaumont. Laion hangHuang,LinjieXing,ZheChen,HaoLi,XizhouZhu,
coco: 600m synthetic captions from laion2b-en. ZhiguoCao,etal. Theall-seeingproject: Towardspanop-
https://laion.ai/blog/laion-coco/,2022. 6,8,10 ticvisualrecognitionandunderstandingoftheopenworld.
[78] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,Gang arXivpreprintarXiv:2308.01907,2023. 2,4
Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: [92] XinyuWang,YuliangLiu,ChunhuaShen,ChunChetNg,
Alarge-scale,high-qualitydatasetforobjectdetection. In CanjieLuo,LianwenJin,CheeSengChan,Antonvanden
ICCV,2019. 6,10 Hengel,andLiangweiWang. Onthegeneralvalueofevi-
[79] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and dence,andbilingualscene-textvisualquestionanswering.
AmanpreetSingh. Textcaps: adatasetforimagecaption- InCVPR,pages10126‚Äì10135,2020. 10
ingwithreadingcomprehension.InECCV,pages742‚Äì758. [93] ZiruiWang,JiahuiYu,AdamsWeiYu,ZihangDai,Yulia
Springer,2020. 10 Tsvetkov,andYuanCao. Simvlm: Simplevisuallanguage
[80] AmanpreetSingh, VivekNatarajan, MeetShah, YuJiang, model pretraining with weak supervision. arXiv preprint
Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus arXiv:2108.10904,2021. 2
Rohrbach. Towards vqa models that can read. In CVPR, [94] Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax
2019. 6,12,13 Law, Noah Constant, Gustavo Hernandez Abrego, Steve
19Yuan,ChrisTar,Yun-HsuanSung,etal. Multilingualuni- [107] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMo-
versalsentenceencoderforsemanticretrieval. InProceed- hamed Elhoseiny. Minigpt-4: Enhancing vision-language
ingsofthe58thAnnualMeetingoftheAssociationforCom- understandingwithadvancedlargelanguagemodels.arXiv
putationalLinguistics:SystemDemonstrations,2020. 7 preprintarXiv:2304.10592,2023. 3
[95] JiaboYe,AnwenHu,HaiyangXu,QinghaoYe,MingYan, [108] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie
YuhaoDan,ChenlinZhao,GuohaiXu,ChenliangLi,Jun- Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan.
feng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug- Vl-gpt:Agenerativepre-trainedtransformerforvisionand
docowl:Modularizedmultimodallargelanguagemodelfor language understanding and generation. arXiv preprint
documentunderstanding,2023. 6 arXiv:2312.09251,2023. 6,7
[96] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, [109] WanrongZhu,JackHessel,AnasAwadalla,SamirYitzhak
Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig
Chang, and Yinfei Yang. Ferret: Refer and ground Schmidt,WilliamYangWang,andYejinChoi.Multimodal
anything anywhere at any granularity. arXiv preprint c4: An open, billion-scale corpus of images interleaved
arXiv:2310.07704,2023. 7 with text. arXiv preprint arXiv:2304.06939, 2023. 2, 6,
8,10
[97] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung,
Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Con- [110] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,
trastivecaptionersareimage-textfoundationmodels.arXiv andJifengDai. Deformabledetr:Deformabletransformers
preprintarXiv:2205.01917,2022. 2 forend-to-endobjectdetection. InICLR,2020. 2,4
[111] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Xiaogang
[98] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,
Wang,HongshengLi,XiaohuaWang,andJifengDai. Uni-
GunjanBaid,ZiruiWang,VijayVasudevan,AlexanderKu,
perceiver: Pre-trainingunifiedarchitectureforgenericper-
Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autore-
ception for zero-shot and few-shot tasks. arXiv preprint
gressivemodelsforcontent-richtext-to-imagegeneration.
arXivpreprintarXiv:2206.10789,2022. 7 arXiv:2112.01522,2021. 2
[99] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin
Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh
Tang,BrianKarrer,ShellySheynin,etal. Scalingautore-
gressive multi-modal models: Pretraining and instruction
tuning. arXivpreprintarXiv:2309.02591,2023. 2,3,6,7
[100] LvminZhang,AnyiRao,andManeeshAgrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV,2023. 7,8,11
[101] PanZhang,XiaoyiDongBinWang,YuhangCao,ChaoXu,
LinkeOuyang,ZhiyuanZhao,ShuangruiDing,Songyang
Zhang, Haodong Duan, Hang Yan, et al. Internlm-
xcomposer: A vision-language large model for advanced
text-imagecomprehensionandcomposition.arXivpreprint
arXiv:2309.15112,2023. 3
[102] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,
NedimLipka,DiyiYang,andTongSun. Llavar:Enhanced
visualinstructiontuningfortext-richimageunderstanding.
arXivpreprintarXiv:2306.17107,2023. 10
[103] HaozheZhao,ZefanCai,ShuzhengSi,XiaojianMa,Kaikai
An,LiangChen,ZixuanLiu,ShengWang,WenjuanHan,
andBaobaoChang. Mmicl: Empoweringvision-language
modelwithmulti-modalin-contextlearning.arXivpreprint
arXiv:2309.07915,2023. 3
[104] KaizhiZheng,XuehaiHe,andXinEricWang. Minigpt-5:
Interleaved vision-and-language generation via generative
vokens. arXivpreprintarXiv:2310.02239,2023. 8,11
[105] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang,ZhanghaoWu,YonghaoZhuang,ZiLin,Zhuohan
Li,DachengLi,Eric.PXing,HaoZhang,JosephE.Gonza-
lez,andIonStoica. Judgingllm-as-a-judgewithmt-bench
andchatbotarena,2023. 4,5
[106] BoleiZhou, HangZhao, XavierPuig, SanjaFidler, Adela
Barriuso, and Antonio Torralba. Scene parsing through
ade20kdataset. InCVPR,2017. 8,12,14
20