Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion
for Zero-shot Motion Transfer
SigalRaab1, InbarGat1, NathanSala1, GuyTevet1, RotemShalev-Arkushin1,
OhadFried2, AmitH.Bermano1, DanielCohen-Or1
TelAvivUniversity,Israel1 ReichmanUniversity,Israel2
sigal.raab@gmail.com
Giventheremarkableresultsofmotionsynthesiswithdiffusionmodels,
anaturalquestionarises:howcanweeffectivelyleveragethesemodels
formotionediting?Existingdiffusion-basedmotioneditingmethodsover-
Leader
looktheprofoundpotentialofthepriorembeddedwithintheweightsof
pre-trainedmodels,whichenablesmanipulatingthelatentfeaturespace; Followers
hence,theyprimarilycenteronhandlingthemotionspace.Inthiswork,we
exploretheattentionmechanismofpre-trainedmotiondiffusionmodels.
Weuncovertherolesandinteractionsofattentionelementsincapturingand
representingintricatehumanmotionpatterns,andcarefullyintegratethese
elementstotransferaleadermotiontoafolloweronewhilemaintaining
thenuancedcharacteristicsofthefollower,resultinginzero-shotmotion
transfer.Manipulatingfeaturesassociatedwithselectedmotionsallowsusto
confrontachallengeobservedinpriormotiondiffusionapproaches,which
usegeneraldirectives(e.g.,text,music)forediting,ultimatelyfailingtocon-
veysubtlenuanceseffectively.Ourworkisinspiredbyhowamonkeyclosely
imitateswhatitseeswhilemaintainingitsuniquemotionpatterns;hence
wecallitMonkeySee,MonkeyDo,anddubitMoMo.Employingourtech-
niqueenablesaccomplishingtaskssuchassynthesizingout-of-distribution
motions,styletransfer,andspatialediting.Furthermore,diffusioninversion
isseldomemployedformotions;asaresult,editingeffortsfocusongener-
atedmotions,limitingtheeditabilityofrealones.MoMoharnessesmotion
inversion,extendingitsapplicationtobothrealandgeneratedmotions.
Experimentalresultsshowtheadvantageofourapproachoverthecurrent
art.Inparticular,unlikemethodstailoredforspecificapplicationsthrough time
training,ourapproachisappliedatinferencetime,requiringnotraining. Fig.1. Motiontransfer.Thetoprowdisplaysaleaderperformingawalk-
Ourwebpage,whichincludeslinkstovideosandcode,canbefoundat ingmotion.Theleftcolumnshowcasessampleframesoffourfollowers,
https://monkeyseedocg.github.io. eachengagedinadifferentmotion.Thecentralblockpresentstheoutput
motion,wheretheoutlineoftheleader(e.g.,leadingleg)istransferredto
1 INTRODUCTION
thefollowersandintegratedwiththeirdistinctmotifs.Notethealignment
Humanmotionsynthesisisafundamentaltask,usefulforvarious ofthestepsfortheleaderandoutputmotions.Ourmotiontransferiscon-
ductedbymanipulatingself-attentionlatentfeaturesinazero-shotfashion.
fieldsincludingrobotics,autonomousdriving,healthcare,gaming
andanimation.Diffusionmodels[Hoetal.2020;Sohl-Dicksteinetal.
2015]standoutastheprevailingsynthesisparadigmacrossdifferent significantlyfewerDoF.Therefore,insightsregardingpre-trained
modalities,suchasimaging[Sahariaetal.2022b],video[Hoetal. imagingmodelsdonotdirectlyapplytomotion.
2022],3Dpointclouds[LuoandHu2021],andalsomotion[Dabral Inthefollowing,thetermmotionoutlinedenotesastructured
etal.2023;Tevetetal.2023].Withtheemergenceoffoundationmod- planthatorganizesthekeyblocksinvolvedinaspecificmotion
els[Bommasanietal.2021],ithasbeennaturalforsomemodalities, (e.g.,locomotionrhythm),andthetermmotionmotifsdenotesges-
likeimagingandvideo,toevolvetowardszero-shotediting[Geyer turesorpatterns(e.g.,characteristicpose).Thesetermsarefurther
etal.2024;Hertzetal.2023].Suchworkstypicallydependonthe elaboratedinSec.3.
priorinformationencodedintheweightsofpre-trainedmodels, Humansubjectsexhibithighlyexpressivemotions,containinga
whichfacilitatelatentfeaturespacemanipulation.Thiscapitalizes wealthofinformation.Forexample,ajumpingmotioncanbecom-
onadeepunderstandingoftheirintricacies,withattentionlayers binedwiththemessuchasraisingthearmsorclappingthehands.
playingadominantroleinthesemethods. Asubtlechangeineachmotionpatternwouldleadtheviewerto
However,thepriorinformationencodedinpre-trainedmotion acompletelydifferentimpression.Similarly,discerninganindivid-
diffusionmodelsremainslargelyunexplored.Furthermore,ano- ualâ€™smoodoragebecomespossiblebyobservingashortdurationof
tabledisparityexistsbetweentheimagingandmotiondomains. theirwalkingpattern.However,accuratelyconveyingthesesubtle
Imagespossessaregularized2Dspatialstructurewithaconsider- nuancesthrougheitherhigh-levelcontrolssuchastextorlow-level
ablenumberofdegreesoffreedom(DoF),whereasourmotionis controlssuchasjointtrajectoryposesachallenge.Moreover,mo-
definedovera3Dhumanskeletonwitha1Dtemporalaxis,and tiondatasetsarelimited,andacquiringreal-worldhumanmotions
4202
nuJ
01
]VC.sc[
1v80560.6042:viXra2 â€¢ Raab,S.etal.
representingvariousmotifssolelythroughmotioncapture(MoCap) Weintroduceacomprehensivebenchmarktoevaluateourwork.
isexpensive,slow,andunscalable. Ourbenchmark,namedMTB,willbemadepubliclyavailable.It
Diffusionworksthatfacilitatemotionediting[Goeletal.2023; comprisesselectedmotionpairsfromtheHumanML3D[Guoetal.
Tevet et al. 2023] modify motion features (e.g., rotation angles), 2022]testsetandisdescribedinSec.5.1.UsingMTB,wecompare
hencearelimitedtofixedsetsofjointsoroperations.Furthermore, our model to current state-of-the-art methods and demonstrate
theyrelyontextualcontrolandthuscannotconveysubtlemotifs. thatnoneofferthesamebreadthoffunctionalityasMoMo,which
Worksthatdomanipulatelatentfeatures[Raabetal.2023;Tevet consistentlyoutperformsthem.
etal.2022]precedethediffusionmodelsera,lackingaccesstotheir
2 RELATEDWORK
richpriors.
Inthiswork,weexaminetheself-attentionmechanismwithinthe 2.1 MotionSynthesis
motiondomain(Sec.4)andsuggestanunpairededitingframework
Multimodalsynthesis. Petrovichetal.[2021,2022]incorporate
thattransfersaleadermotiontoafolloweronewhilepreservingthe
aTransformer[Vaswanietal.2017]architectureforthetasksof
subtlemotifsofthefollower,therebyreducingrelianceoncostly
action-to-motionandtext-to-motion.T2M[Guoetal.2022],T2M-
MoCapsystemsoroverlygeneraltextualdescriptions.Wenameour
GPT[Zhangetal.2023b]andMotionGPT[Jiangetal.2024]use
workâ€œMonkeySee,MonkeyDoâ€(dubbedMoMo),asitencapsulates
VQ-VAE[VanDenOordetal.2017]toquantizemotion,thense-
theconceptofdirectlytransferringamotionoutlinefromaleader
quentiallysynthesizeitinthequantizedspaceconditionedontext.
toafollowerwhileretainingthefollowerâ€™suniquemotifs,muchlike
Morerecently,MDM[Tevetetal.2023]andMofusion[Dabraletal.
amonkeymimickingobservedbehaviorinamonkeyishmanner.
2023]adaptedthedenoisingdiffusionframework[Hoetal.2020]for
MoMooffersaversatilemotiontransfertechnique,facilitating
motionsynthesisandshoweditsmeritsformultimodaltaskssuchas
tasksunifiedbythecoreconceptoftransferringmotifsfromonemo-
action-to-motion,text-to-motion,andmusic-to-motion[Tsengetal.
tiontoanother.Specialcasesincludestyletransfer(e.g.,transferring
2023].TheMAS[Kaponetal.2023]algorithmextendeddiffusionto
awalkingmotiontozombiewalking),spatialediting(e.g.,transform-
out-of-domainmotionsbyleveragingvideodata.MoMoexcelsby
ingajumpingmotionintoâ€œjumpingwithhandsupâ€),actiontransfer
usingafollowerreferenceinsteadofanoverlygeneralizedtext.
(e.g.,transitioningfromwalkingtorunning),andout-of-distribution
synthesis(e.g.,generatingadancinggorilla).SeeFigs.1and7for Spatialcontrolandediting. MotionGraphs[ArikanandForsyth
moreexamples.Ourmodeloperatesatinferencetimewithoutre- 2002;Kovaretal.2002;Leeetal.2002]areapopulardatastructure
quiringoptimizationortrainingandcanfunctionseamlesslywith fortraversingtheposespaceofagivenmotiondatasettosynthesize
anyunderlyingmotionsynthesisbackbone(i.e.,foundationmodel) newmotionvariations,oftenaccordingtoaninputtrajectory.GAN-
thatutilizesself-attention,regardlessofitsspecificarchitecture. imator[Lietal.2022]andSinMDM[Raabetal.2024]showthat,
Ourstudyhypothesizesthatself-attentionelementscancapture analogouslytoimages[Nikankinetal.2023;Shahametal.2019],
complexmotionpatterns,delvesintothedistinctfunctionalities overfittingasinglemotionallowslearningitsinternaldistribution
of attention elements and examines their interplay. Inspired by andsynthesizingnewvariationsofitwithspatialandtemporal
researchintheimagingdomain[Alalufetal.2023;Caoetal.2023; variations.Usingdiffusion,MDMenablesbothjointsandtemporal
Huetal.2023],wedeviseamethodwherethecapabilityofaqueryğ‘„ editingbyadaptingimagediffusioninpainting[Sahariaetal.2022a;
inaleadermotion,isutilizedtodetectthemostrelevantkeysğ¾from Songetal.2020b].Followupworksdemonstratethemeritsofprop-
afollowermotion.Specifically,wecalculateanattentionscoreusing agatingcontrolsignalsthroughthegradualdenoisingprocessfor
theğ‘„fromtheleadermotionandtheğ¾ fromthefollowermotion. variousapplicationssuchaslongmotionsynthesis[Petrovichetal.
Thisscoreisutilizedtoextractaweightedcombinationofvalues 2024;Zhangetal.2023a],motionin-betweening[Cohanetal.2024;
ğ‘‰ fromthesamefollowermotion.Consequently,anewmotionis Xieetal.2023],andsinglejointcontrol[Karunratanakuletal.2023;
synthesized,incorporatingtheoutlinefromtheleadermotion,and Shafiretal.2024].ComMDM[Shafiretal.2024]usesdiffusionto
themotifsfromthefollowermotion,maintainingfidelitytoboth.In synthesizetwoactors.InterGen[Liangetal.2024]followsitand
essence,theleadermotiondeterminestheâ€œwhatâ€ andâ€œwhenâ€,and showstheeffectivenessofcross-attentioninthetrainingprocess.
thefollowermotiondeterminestheâ€œhowâ€.Forexample,aninstance SINC[Athanasiouetal.2023]andFineMoGen[Zhangetal.2024]
oftheâ€œwhatâ€couldbeâ€œastepforwardwiththerightlegâ€,whilean useLargeLanguageModels(LLMs)tobreakthetextpromptto
instanceoftheâ€œhowâ€canbeâ€œrunningâ€,â€œtiredlyâ€,orâ€œwithhands instructeachbodypartseparately.Goeletal.[2023]usethecoding
raisedâ€.Figure5illustratestheimplicitsemanticcorrespondence skillsofLLMswithpredefinedposemodifiersforframeediting,then
betweentheleaderandfollowermotions,whichdoesnotrequire blendthemusingdiffusion.Ourapproachenablesspatialeditingof
additionalsupervision. theleadermotionasaspecialcaseofmotiontransfer.
Ourworkstandsasthesoleapproachcapableofutilizingmotion Styletransfer. OneofthespecialcasesenabledbyMoMoisstyle
DDIMinversionwithindiffusionmodels[DhariwalandNichol2021; transfer.Holdenetal.[2017a,2016]havesuggestedlearningthe
Songetal.2020a].Inthefieldofimaging,theintegrationofinver- motionmanifoldusinganauto-encoderneuralnetwork.Thelatent
sionwithdiffusioniswidelyused,facilitatingthemanipulationof spaceoftheauto-encoderexposessemanticfeaturesofthemotion,
realimages[Garibietal.2024;Huberman-Spiegelglasetal.2023; whichenablesmotionstylizationusingtheGrammatrixheuristic
Mokadyetal.2023].However,inthemotiondomain,diffusionmod- assuggestedbyGatysetal.[2015].Abermanetal.[2020]usethe
elstypicallyavoidemployinginversion.MoMoutilizesinversion, AdaINheuristictodisentanglecontentandstyleaspresentedin
therebyenablingeditingofbothrealandgeneratedmotions. StyleGAN[Karrasetal.2019],followedbyGuoetal.[2024]andKimMonkeySee,MonkeyDo â€¢ 3
etal.[2024].UnlikeMoMo,thesemodelsaretrainedonpredefined Xout
stylesandstruggletogeneralize.
Xl 0dr
0
Xf 0lw
2.2 AttentionControlintheImagingDomain Denoising Net Denoising Net Denoising Net
Thelatentinformationencapsulatedintheattentionlayersofthe
popularUNet[Ronnebergeretal.2015]architectureisextensively Self Attention Mixed Attention Self Attention
usedintheimagedomaintoguideandcontrolthedenoisingdif-
Qldr Kldr Vldr Qldr Kflw Vflw Qflw Kflw Vflw
fusionprocess.PnP[Tumanyanetal.2023],MasaCtrl[Caoetal. t t t t t t t t t
2023]andCIA[Alalufetal.2023]showthattheself-attentionlayers
encodestructuralinformationthatcanbeusedtoeditanimage
Xldr Xout Xflw
withoutlosingitsoriginalcomposition.Prompt-to-Prompt[Hertz t t t
etal.2023]andAttend-and-Excite[Cheferetal.2023]showthat
certainaspectsoftheimagecanbeeditedbymanipulatingthecross- Xout cout
T
attentionwiththeinputtext,withoutaffectingtherest.Patashnik
etal.[2023]andDaharyetal.[2024]aremanipulatingtheself-and cldr Xldr copy copy cflw Xflw
T T
cross-attentionlayerstocontrolthelayoutoftheimageandavoid
semanticleakagebetweenitsdifferentparts.Tune-A-Video[Wu
etal.2023],TokenFlow[Geyeretal.2024]andQ-NeRF[Patashnik Inversion Inversion
Optional
etal.2024]observethattheattentionquery,ğ‘„,encodesthestructure
whilethekeyandvalue,ğ¾ andğ‘‰,encodetheappearance,anduse
Xldr Xflw
itformutualeditingofimagespreservingtemporalandstructural
consistencies.Ourworkfollowsthelatter,leveragingself-attention
layersformotionediting.Unlikeimagingworks,ourworkuses
Fig.2. TheMoMoPipeline.Theinputtoourmodelistwonoisytensors,
layersfromatransformerandnotfromaUNet. ğ‘‹ldr andğ‘‹flw,producedbyeitherinvertingrealmotionsorsamplinga
ğ‘‡ ğ‘‡
Gaussiannoise.Thetwotensorsrepresentleaderandfollowermotions,and
3 MODEL aregivenalongwiththeirassociatedtextprompts.Weinitializeouroutput
motion,ğ‘‹out,usingtheinitialnoisefromtheleadermotionandpairitwith
Thissectionsuggestsaneditingframeworkthattransferstheoutline ğ‘‡
thetextpromptfromthefollowermotion.Thethreenoisedmotionsğ‘‹ldr,
ofaleadermotiontoafolloweronewhilepreservingthemotion ğ‘‹flwandğ‘‹out,arepassedtothefrozendenoisingnetworkateachtimestğ‘¡
ep
motifsofthefollower.Ourunpairedframeworkoperatesatzero- ğ‘¡,ğ‘¡ alongwitğ‘¡ htheirpromptsandwithğ‘¡.Withinthedenoisingnetwork,ğ‘‹out
ğ‘¡
shot,withoutrequiringoptimizationormodeltraining. undergoesmixed-attentionbycombiningthequeryfromtheleadermotion
A motion outline denotes a structured plan that arranges the withthekeyandvaluefromthefollowermotion.Meanwhile,ğ‘‹ldrandğ‘‹flw
ğ‘¡ ğ‘¡
essentialmovementsnecessaryforaspecificmotion.Itprovides followastandarddiffusionprocess.
avisualblueprintforcomprehendingthesequenceofactionsand
transitionsneededtoexecutethemotioneffectively.Anexampleof
asingleframe,alsoknownaspose.Finally,letğ½ denotethenum-
amotionoutlinewouldbeâ€œstandstillonframes10-20,stepwith
berofskeletaljoints.Weadheretotherepresentationusedinthe
rightlegonframes21-25â€,etc.Notethattheoutlineisasgeneralas
HumanML3Ddataset[Guoetal.2022],wherethefeaturesfromall
possible;forexample,thetypeofstep(walk,run,hop)belongstothe
thejointsareconcatenatedintoasinglelargefeature,resultingin
motifs.Motionmotifsincludesubtlenuances,gestures,orpatterns
amotionrepresentationğ‘‹ âˆˆRğ‘Ã—ğ¹ .Detailsregardingtheinternal
thatconveymeaningandemotion.Thesemotifsmayrepeatand
representationofthefeaturescanbefoundintheAppendix.
vary,formingexpressivemotionsequences,andaidinginestablish-
ingvisualthemesandnarratives.Considervariousrunningmotions, SelfAttention. Werecapself-attention[Vaswanietal.2017],as
eachwithdistinctmotifs.Thesemotionsconveypersonalstyles itplaysakeyroleinourframework.Letğ¼ğ» (â€œinputhiddenâ€)bea
expressedbybodyangle,footpositioning,handgestures,airborne latenttensoroffeaturesfedasinputtoaself-attentionlayer,and
duration,etc.Evenwithextensivepromptengineering,capturing letğ»Ë† betheoutputofthislayer.Theelementsquery,key,andvalue,
everysubtlemotifremainsunattainable.Conversely,incorporating arecalculatedrespectivelyby
motifsfromagivenmotionensurescompletefidelitytothatmotion. ğ‘„ =ğ¼ğ» Â·ğ‘Š ğ‘ğ‘‡ +ğ‘ ğ‘, ğ¾ =ğ¼ğ» Â·ğ‘Š ğ‘˜ğ‘‡ +ğ‘ ğ‘˜, ğ‘‰ =ğ¼ğ» Â·ğ‘Š ğ‘£ğ‘‡ +ğ‘ ğ‘£, (1)
Ourframeworkenablestransitionsacrossmotionsofdifferent
temporallengths.Theoutlinesofaleadermotioncanbetransferred
where (ğ‘Š ğ‘,ğ‘ ğ‘), (ğ‘Š ğ‘˜,ğ‘ ğ‘˜), and (ğ‘Š ğ‘£,ğ‘ ğ‘£) are learned linear projec-
tions.
tomultiplefollowers,asshowninFig.1,andmultipleoutlinescan
beseparatelyappliedtoasinglefollower.
Foreachqueryvectorğ‘
ğ‘›
âˆˆğ‘„attemporallocation(i.e.,frame)ğ‘›,
anattentionscoreiscomputedbasedonallkeysinğ¾.Thisscore
indicatestherelevanceofeachkeytothequeryğ‘ ğ‘›,assessingtheir
3.1 Preliminaries
similarity.Theattentionscoresarenormalizedthroughasoftmax
MotionRepresentation. Letğ‘ denotethenumberofframesina operation,whichdeterminestheweightingofeachvalueinğ‘‰,to
motionsequence,andğ¹ denotethelengthofthefeaturesdescribing beusedforupdatingthefeaturesatlocationğ‘›.Theweightedvalues4 â€¢ Raab,S.etal.
Q K
Walk
Walk then turn
Turn while
crouching
Locomotion phases Motion start/end Bend down Stand Walk Crouch Turn
Fig.3. DominantfeaturesinQvs.K.Eachrowdepictstwocopiesofthesamemotion,showcasingtheK-Meansclusteringofitsğ‘„andğ¾featuresinthe
leftandrightcolumns,respectively.Notehowthefeaturesinğ‘„aredominatedbytheoutline,whilethoseinğ¾aredominatedbythemotionmotifs.Intheğ‘„
column,periodicstepsshareclusters,ignoringuniquepatterns.Intheğ¾column,clustersarerelatedtomotionmotifs;thus,walking,turningwhilewalking,
andcrouchingwhilewalkinghavedistinctclusters.Temporalinformationisevidentintheclustersofğ‘„butnotinthoseofğ¾.Intheğ‘„column,thebeginnings
ofthefirsttwomotionsandtheendofallthreearehighlightedbythecolorsoflowandhighframenumbers,respectively.
arethenaggregatedtoproducetheoutputateachquerylocation, standardDDIMdenoising,whileğ‘‹ ğ‘¡outisdenoisedusingourmixed-
via attentionblock,describednext.Finally,MoMoproducestheoutput
ğ´ ğ‘› =softmax(cid:32) ğ‘ âˆšï¸ğ‘› |ğ¼Â· ğ»ğ¾ ğ‘›ğ‘‡ |(cid:33) , â„Ë† ğ‘› =ğ´ ğ‘›Â·ğ‘‰, (2) m moo T dt hi eo e ln d thğ‘‹ e an0o tou uit s. ti in lig zen se st ew lfo -r ak tti en no tiu or np li ap ye eli rn se .Ic nan oub re ea xn py em rio mti eo nn ts-d ,e wn eo eis min -g
ploy a variant of MDM as a backbone. Note that, while related
whereğ´ ğ‘›isthenormalizedattentionscoreatframeğ‘›,â„Ë† ğ‘› âˆˆğ»Ë† isthe worksintheimagingdomainutilizetheself-attentionlayersofa
self-attentionresultatframeğ‘›,and|ğ¼ğ» ğ‘›|isthefeaturesnumberin
UNet[Ronnebergeretal.2015],weleveragetheself-attentionlayers
frameğ‘›.Finally,ğ»Ë† isusedasaresidualandaddedtotheinputğ¼ğ»,
ofaTransformer[Vaswanietal.2017]duetotheusageofMDM.
ğ‘‚ğ» =ğ¼ğ»+ğ»Ë†, (3) 3.3 LeveragingSelf-attention
whereğ‘‚ğ» (â€œoutputhiddenâ€)isthetensorpassedtothenextlayer. Ourproposedframeworkintegratesself-attentioncomponentsfrom
Weusemulti-headattentionandomititsnotationsforbrevity. boththeleaderandfollowermotionsintoasingleoutputmotion.In
theimagingdomain,Caoetal.[2023]havestudiedtheself-attention
MDM,DDPMandDDIM. MotionDiffusionModel(MDM)[Tevet
layersoftext-to-imagedenoisingnetworks.Theydemonstratethat
etal.2023]isawidespreadmodelforhumanmotionsynthesisand
keepingthekeysandvaluesoftheselayershelpspreservethevisual
editing.Inourworkweuseavariationofit,hencewerecapithere.
characteristicsofobjectswhenperformingnon-rigidmanipulations
MDMusesDenoisingDiffusionProbabilisticModels(DDPM)[Ho
on a given image. Alaluf et al. [2023] made further progress by
etal.2020],whicharetrainedtotransformunstructurednoiseinto
combiningstructureandappearancefromtwoimages.
samplesfromaspecifieddistribution.Thisisachievedthroughan
Inspiredbytheseinsights,thisworkillustratesthecrucialfunc-
iterativeprocessinvolvingthegradualremovalofsmallamountsof
tionsofqueries,keys,andvaluesinencodingsemanticmotioninfor-
Gaussiannoise.UnlikeMDM,inthiswork,weemployDDIM[Song
mation.Wefindthatleveragingthequeries,keys,andvaluesfrom
et al. 2021] for inversion and deterministic inference, aiming to
self-attentionlayersenablesthetransferofsemanticinformation
reconstruct inverted motions precisely to their original form. A
acrossdifferentmotions.
recapregardingDDPMandDDIMcanbefoundintheAppendix.
InSec.4weshowthatthisapproachenablestheimplicittransfer
ofmotionpatternsbetweensemanticallysimilarframes.Morepre-
3.2 Pipeline
cisely,ateachdenoisingstepğ‘¡,weuseourmixed-attentionblock
MoMoemploysapre-trainedandfixedmotiondiffusionmodelto toinjectthequeriesfromtheleadermotionğ‘‹ldr,andthekeysand
synthesizetheoutputmotionğ‘‹outbyapplyingthemotionoutlineof valuesfromthefollowermotionğ‘‹flwtotheself-attentionblockof
ğ‘‹ldrontoğ‘‹flw,whereğ‘‹ldrandğ‘‹flwareeithergiven(real)motions theoutputmotionğ‘‹out,via
orgeneratedones(SeeFig.2).
Theinputtoourframeworkistwoinputnoises,ğ‘‹ ğ‘‡ldrandğ‘‹ ğ‘‡flwand
ğ‘‚ğ»out=ğ¼ğ»out+softmax(cid:32) ğ‘„ldrÂ·ğ¾flwğ‘‡ (cid:33)
ğ‘‰flw. (4)
theircorrespondingtextpromptsğ‘ldrandğ‘flw,respectively.Thein- âˆšï¸ |ğ¼ğ» ğ‘›|
putnoisesareeitherinvertedfromrealmotionsusingDDIM[Song
4 UNDERSTANDINGSELF-ATTENTIONFEATURES
etal.2021]orsampledfromaGaussiandistribution,inwhichcase
MoMorunsongeneratedmotions.Thetextpromptsassistincon- Inthissection,weexploresomeofthepriorinformationencodedin
trollingsynthesisfromnoiseandinvertingthemotionsifinversion pre-trainedmotiondiffusionmodelsandidentifyusefulattributes
isused.Ateachtimestepğ‘¡,wepassthethreenoisedmotionsğ‘‹ ğ‘¡ldr, withinit.Inthefollowing,wedemonstratethat(i)thequeriesğ‘„
ğ‘‹ ğ‘¡flwandğ‘‹ ğ‘¡outtothedenoisingnetwork.ğ‘‹ ğ‘¡ldrandğ‘‹ ğ‘¡flwgothrougha establishafocalpointforcontextualdetermination,(ii)thekeysğ¾MonkeySee,MonkeyDo â€¢ 5
er <Jumping forward= followermotionthatelicitsthegreatestactivationintheattention
ollow mapğ‘ ğ‘›ldrÂ·ğ¾flwğ‘‡ .Thesepairingsportraythetransitionfromtheleader
F tothefollowermotion,highlightingthemostsignificantactivations.
Asdepicted,thesepairingsexemplifyalignmentintheoutline,such
as the synchronization of body movement (â€œupâ€/â€œdownâ€) across
frames.Theattentionmapsinthissectionarecomputedforlayer
<Jumping jacks= 10anddiffusionstep50.
4.3 AttentionMaps
After computing attention maps in the previous section, which
Each leader frame is replaced by its nearest neighbor from the follower
revealedthehighestactivationforeachframe,wenowdelveinto
Fig.4. Correspondenceviaattention.Followerframesarecolor-coded
thecompleteattentionmapsfortwoselectedframes.Withineach
accordingtoconsecutiveindices(toprow).Nearestneighborfollowerframes
(bottom)aretheonesthatachievethehighestmixed-attention(ğ‘„ldrÂ·ğ¾flwğ‘‡) rowinFig.5,wefocusonatemporalregionidentifiedbythenumber
ofitscenterframe.Ineachcolumn,wepresenttheattentionmaps
activation,shownrespectivelytoleaderâ€™sframes(middlerow).Thesecor-
respondencesaresemanticallyaligned,e.g.,movingâ€œupâ€andâ€œdownâ€sub- foreachquerylocationusingvariouscombinationsofkeysand
motionsareconsistentlyassignedwithfollowermovingâ€œupâ€andâ€œdownâ€ queries.Naturally,whenkeysandqueriesfromthesamemotion
frames.Someofthenearestneighborsarehighlightedwitharrows.
aremultiplied(ğ‘„ldrÂ·ğ¾ldrğ‘‡
),theyproducehighsimilarityscoresfor
regionsresemblingthequeriedpose,especiallythequeriedframe
serveasalearnedframedescriptor,enablingthemodeltoassessthe itself.Incontrast,inthefollowermotion,theposeatthesameframe
importanceofdifferentframesinthemotionrelativetoaspecific numberasthequerymaynotbesimilar,makingitunhelpfulfor
query,and(iii),thevaluesğ‘‰ denotethecontextualrepresentation retargetingtheleadermotion.Intherightcolumn,weuseMoMoto
weseektogenerate,guidingthemodelinshapingthefeaturesat
computeğ‘„ldrÂ·ğ¾flwğ‘‡
.Asillustrated,thequeriesfocusonsemantically
eachqueryâ€™stemporallocation.Inparticular,weshowthatwhile similarregionsinthefollowermotion.
informationregardingtheoutlineandthemotifsofthemotionis Notethatthesecorrespondencesaremadedespitethetwomo-
containedinbothğ‘„andğ¾,theoutlineinformationismoredominant tionshavingdifferentsequencesofposes.Asaresult,multiplying
inğ‘„ andthemotifsinformationismoredominantinğ¾.Thiskey theattentionmapswiththefollowervaluesğ‘‰flw,enablesthetrans-
insightispresentedinSec.4.1andservesasthefoundationonwhich feroftheoutlineoftheleadermotionontothefollowerâ€™smotifs.
ourmixed-attentionblockwasbuilt;tothebestofourknowledge, Finally,Fig.6depictsattentionmapsforthefullmotionlength,
thisinsighthasnotbeenexploredintheimagingormotiondomains. providingvisualinsightintoleader-followercorrespondence.
4.1 DistinguishingBetweenğ‘„ andğ¾ 5 EXPERIMENTS
Tounderstandthedifferentrolesofqueriesandkeys,weextract Theresultsinthisworkarecomputedusingthetransformerdecoder
theirfeaturesfromachosenself-attentionlayerâ„“atdiffusionstep versionofMDM.Theexacthyperparametervaluesaredetailedin
ğ‘¡,reducethedimensiontoğ‘‘outputchannelsbyapplyingPCA,and theAppendix.Inpractice,eq.(4)isappliedindiffusionsteps90to
grouptheframesintoğ‘šclustersusingK-Means.Whileğ‘„ andğ¾ 10(outof100)andinlayers2to12(outof12).
containbothoutlineandmotifsinformation,applyingK-Means
5.1 Benchmark
emphasizesthemoredominantfeatures.Figure3visualizeseach
clusterusingadifferentcolor.Clusteringğ¾ showsthatitisdomi- ToevaluateMoMo,weintroduceMotionTransferBenchmark,dubbed
natedbythemotionmotifsandthatsimilarmotifsaregroupedinto MTB,ofleaderandfollowermotionpairs,whichwillbemadeavail-
thesameclusters.Clusteringğ‘„depictsthatitisdominatedbythe
able.MTBisasubsetoftheHumanML3D[Guoetal.2022]testset
motionoutline.Inparticular,(i)periodicsub-motions,likesteps,are andisstraightforwardlyfilteredaccordingtothetextualprompts
groupedintothesamecluster,dominatingovermotionmotifs,and attachedtothemotions.Fortheleadermotions,weincludemotions
(ii)thetemporalsignalisdominant.From(i)weconcludethatthe thatcontainlocomotionverbssuchasâ€œrunâ€orâ€œwalkâ€.Forthefol-
ğ‘„ğ‘ indifferentperiodicmotionssharesimilarfeatures,thustheycan
lowermotions,weincludemotionswithtextthatcontainswords
beâ€œunderstoodâ€byğ¾ğ‘ fromothermotions.Thisfindingexplains
suggestingmotionmotifs,suchasâ€œchickenâ€orâ€œclapâ€.Theword
whyourmotiontransferworkswell.Interestingly,ğ‘„encodesthelo-
choices,sharedinourAppendix,aremadestraightforwardlyand
comotionphaseswithoutexplicitlylearningitasinPFNNs[Holden donotinvolvelargelanguagemodels.
etal.2017b;Starkeetal.2022].OurPCAisappliedon256generated Wecreatepairsofleaderandfollowermotionsbycombining
motionswith128textprompts.Weuseâ„“=11,ğ‘¡=30,ğ‘‘=10,andğ‘š=10.
sentencesfromtherespectivegroupsinaCartesianproduct.How-
ever,suchacombinationinducesapproximately46ğ¾ pairs,which
4.2 Correspondenceviaattention
ismorethanweneed.Todecreasethenumberofpairs,insteadof
Given two entirely different motions, we show that the nearest usingallthecrosscombinationsofleaderandfollower,weallow
neighborfromafollowermotion,foraqueryğ‘„fromaleadermotion, upto20repetitionsofeachfollowersampleandnorepetitionsfor
wouldbetheframethatresemblesitmostinitsoutline.InFig.4,for theleadersamples.Inpractice,weuse4leadersearchwords,re-
everyframeğ‘›withintheleadermotion,wediscerntheframeinthe sultingin842motions,and17followersearchwords,resultingin
tseraeN
redaeL
srobhgieN6 â€¢ Raab,S.etal.
Leader Follower
A person does jumping jacks A person raises his hands then lowers them down
24 24
60 60
Similarity
low high
Fig.5. Attentionmapperquery.Intheleftcolumn,wedisplaythreecopiesoftheleader;intherightcolumn,weshowcopiesofthefollower.Thetopcopies
depictthemotionsastheyare,whiletheonesbelowhighlightattentionscores.Wedefinetwoqueriescorrespondingtodifferentsemantictemporalregionsin
theleadermotion.Eachquerycorrespondstoadifferentpose,withvariedarmdirectionorbodystretch.Eachmotioncolumndisplaysattentionmapsfroma
singlelayer,computedindifferentways.Intheleftcolumn,wepresentself-attentionmapsderivedfromqueriesandkeysfromtheleadermotion,causingeach
querytoconcentrateonsemanticallysimilarregionswithinthatmotion.Theframenumberrelatedtoeachqueryisindicatedwithanarrow.Forexample,the
queryinframe24focusesonaposeofâ€œstandinglowinanAposeâ€,intheleadermotion.However,framenumber24correspondstoanentirelydifferentpose
inthefollowermotionintherightcolumn.Intherightcolumn,weapplyMoMo,aligningleaderqueriesğ‘„ldrwithfollowerkeysğ¾flw.Thiswayweensurethat
eachqueryfromtheleadermotionalignswithsemanticallysimilarregionsofthefollowermotion.Forinstance,inframe60,thequeryhighlightstheregion
wherethecharacterraisestheirarms.Theframeswithhighercorrespondence(red)intherightcolumnalsobelongtocharactersraisingtheirarms.
55motions(somesearchtermsresultinlessthanthreesentences). secondcomputesasoftmaxontopofthesimilarityscores,extract-
Altogether,MTBincludes842(leader,follower)motionpairs. ingfollowerâ€™sframesaccordingtothesoftmaxweights.Thethird
computesthesimilaritybetweentheleaderâ€™sğ‘„andthefollowerâ€™s
5.2 Metrics ğ¾ latentfeatures,andextractsthefollowerâ€™svalueğ‘‰ accordingto
themostsimilar(i.e.,nearestneighbor)frame.
Weaimtoassesstheresultsbasedonthreecriteria:(a)thequalityof
StyleTransfer. StyleTransferisaspecialcaseofmotiontransfer.
theoutputmotions,(b)howcloselytheoutputmotionsfollowthe
Itreferstodoingagivenactioninadifferentwaythatrepresents
outlineoftheleadermotion,and(c)howwelltheoutputmotions
anemotionoraphysicalstate,suchasâ€œhappilyâ€orâ€œlikeamon-
alignwiththemotifsoftheirassociatedfollowermotions.Toevalu-
keyâ€.Typically,itretainstheoriginalaction(e.g.,walk),whilemo-
atetheseaspects,wehavechosenspecificmetrics.Criterion(a)is
tiontransferencompassesanychangeinmotionmotifs,including
evaluatedusingFID,(b)isassessedbyfootcontactsimilarity,and
changingtheaction(actiontransfer)ormodifyingasubsetofjoints
criterion(c)isevaluatedbyR-precisionandsimilaritytofollower
withoutalteringthestyle(spatialediting).Whilemostmotionstyle-
locationsandrotations.Detailsaboutthesemetricsareprovidedin
transferapproaches[Abermanetal.2020;Guoetal.2024;Jangetal.
theAppendix.
2022]excelwithpredefinedstyleclassesbutstrugglewithunseen
5.3 Baselines styles,ourmethodseamlesslyhandlesanygivenstylemotion.Exist-
ingstyletransfermethodsrequiretrainingandnoneofthemutilizes
Noexistingbaselineencompassesallaspectsofmotiontransfer
diffusionmodels.Wecomparewithstate-of-the-artMoST[Kimetal.
addressedbyMoMo.Hence,wecompareMoMowithseveralworks,
2024],afterwetrainitontheHumanML3Ddataset.
each representing a different special case. We demonstrate that
Spatial Editing. Spatial editing adjusts specific joints, like the
MoMoofferssuperiorgeneralizationcapabilitiescomparedtothese
arms,whilepreservingtheoverallmotion.Amongspatialediting
works.Additionally,mostpriorartistailoredforspecificapplica-
motiondiffusionmethods,MDMinpainting[Tevetetal.2023],as
tionsthroughtraining,andoftenrequirespaireddata.Incontrast,
wellasMEO[Goeletal.2023],usetextualcontrolandmodifyend
MoMoisunpairedandaccomplishesitsfunctionalitiesduringinfer-
featuresofamotion(e.g.,rotationangles)withoututilizinglatent
encewithoutnecessitatingspecifictrainingoroptimization.
space.MDMinpaintingexcelsateditingbroadsetsofjoints,but
MotiontransferUsingNaÃ¯veNearestNeighbor. Toillustratethe needsrefinementforindividualjointedits[Shafiretal.2024].It
necessityoflatentspaceediting,wecompareMoMowithitsnaÃ¯ve reliesonhard-codedjointmasks,limitingeditstopredefinedbody
nearestneighbor(NN)equivalent,andexaminethreeapproaches. parts.MEO[Goeletal.2023]usesafinitehard-codedsetofediting
Thefirstapproachselectseachframeâ€™snearestcounterpartinthe operations.Incontrast,ourapproachisnotlimitedtoeditingalarge
followermotionandincorporatesitintotheoutputmotion.The setofjoints,hard-codedmasks,orafinitesetofeditingoperations.MonkeySee,MonkeyDo â€¢ 7
ã—„     ã—ƒ     ã—„     ã—ƒ     ã—„     ã—ƒ     results.MoMoâ€™sversionoperatingongeneratedmotionsoutper-
high
formstheoneoninvertedmotions.
5.5 QualitativeResults
Oursupplementaryvideoreflectsthequalityofourresults.Itpresents
leader multipletransferredmotionsandcomparisonstootherbaselines.
low frame 37 Itshowsthatthefirstnearestneighborapproachresultsinjittery
Key Key Key
outcomes,thesecondproducesunnaturaloutcomeswithfootslid-
ing,MDMInpaintingcannotgeneralizebeyonditsspatialediting
expertise,andMoSTstrugglestogeneralizeonunseenstyles.MoMo
output follower offersaversatilemotiontransfertechniqueexpressedinFigs.1and7
frame 63 frame 63
whereweseehowitfacilitatestasksunifiedbythecoreconcept
Fig.6. Attentionmapsforfullmotions.Theleftandmiddleself-attention
mapsrelatetoaleaderandafollower,performingâ€œwalkingâ€andâ€œwalking oftransferringamotionoutlinefromaleadertoafollowerwhile
likeagorillaâ€motions,respectively.Totherightistheirmixed-attention retainingthefollowerâ€™suniquemotifs.Arrangedfromtoptobottom,
map.Forself-attention,themaindiagonalindicatesself-correlationand Fig.1demonstratesspecialcasesofspatialediting,styletransfer,and
thesecondarydiagonalsindicatetheperiodicityinthewalks.Formixed- actiontransfer.Fig.7illustratesactiontransfer,out-of-distribution
attention,thehighattentiondiagonalsrelatetothecorrelationbetween synthesis,andanotheractiontransfer.
theleaderandthefollower.Intheframesattributedtothecircledhigh
attentionscorewithinthemixed-attentionmap,bothleaderandfollower 5.6 Ablation
depictasimilarsteppingpose,justifyingthehighactivation.Theresulting
Ourinitialstudyfocusesonidentifyingtheappropriatelayersand
outputframeissimilar,butnotidentical,tothefollowerâ€™s,attributedtothe
diffusionstepsforwhichmixed-attentionshouldbeapplied.We
weightedsumthatcombinesmultipleinputs.Allmapsrelatetolayer6and
diffusionstep70inourbackbone. denotetherangeoflayersanddiffusionstepswheremixed-attention
isappliedas[ğ‘ -ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ,ğ‘’-ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ]and[ğ‘ -ğ‘ ğ‘¡ğ‘’ğ‘,ğ‘’-ğ‘ ğ‘¡ğ‘’ğ‘],respectively.We
havetestedapproximately200configurations,withvaryingvalues
ofğ‘ -ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ,ğ‘’-ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ,ğ‘ -ğ‘ ğ‘¡ğ‘’ğ‘,andğ‘’-ğ‘ ğ‘¡ğ‘’ğ‘.Table2displaysrepresentative
MDMInpaintingistheclosestdiffusionspatialeditingworkthat resultsofthevariationsweexperimentedwith.
providescode.Itexpectsaninputmotionandatext,soforeachpair Anotherablationtestexaminesthetextualpromptusedduring
ofbenchmarkmotions,theleaderisusedasis,andthefolloweris thesynthesisoftheoutputmotion.ResultsinTab.3confirmthat
givenbyitstextprompt. ourchoicetoreplicatethefollowerâ€™spromptisoptimal.
OODSynthesis. Out-of-distribution(OOD)synthesisentailsgener-
6 CONCLUSION,LIMITATIONSANDFUTUREWORK
atingmotionsthatwerenotencounteredduringthetrainingphase,
posingachallengetothenetworkâ€™sgeneralizationcapabilities.For Wehaveexploredandleveragedthepowerfulmechanismofself-
instance,whenattemptingtogenerateadancinggorilla,thenetwork attention,withinpre-trainedmotiondiffusionmodels.Ourstudy
strugglestogeneralizetothiscombinationdespitebeingtrainedon hasresultedinanovelapproachformotioneditingindiffusion
individualinstancesofdancinghumansandofnon-dancinggorillas. models,whichwehavedemonstratedthroughmotiontransfer.
Thisdifficultyarisesfromthesparserepresentationofsuchcom-
binationsinthelatentspace.Byapplyingourproposedtechnique,
Table1. ComparisonwithbaselinesontheMTBbenchmark.MoMo
wegeneratelatentfeatureslocatedsparselyawayfromallother exhibitsthebestFIDandR-precisionresultsandcomparablesimilarity
features,andyet,theyaredenoisedintoanaturalmotion.Existing scores.MoMoâ€™sversionthatrunsongeneratedmotions(abbreviatedâ€œGen.â€)
motiondiffusionmethodsdonottransferagivenmotionintoan hasslightlybetterresultsthantheonethatrunsoninvertedones(â€œInv.â€).
OODone.Hence,weevaluatetheOODsamplesinourbenchmark Baselinesthatexhibitthehighestsimilaritymetricsscores,showpoorFID
usingstyletransferandspatialeditingbaselines. andR-precisionresults.NaÃ¯venearestneighborvariationsattaingoodsimi-
laritytothefollower,astheyarebuiltbyfeaturescopiedfromit.However,
ActionTransfer. Actiontransferistheapplicationoftransferring
theyexhibitpoorFIDduetoajitteryoutput.MoSTresultsaretoosimilar
theoutlineofoneactionintoanother,say,â€œrunningtowalkingâ€ totheleader,hencefailinsimilaritytothefollower,andMDMInpainting
or â€œwalking to crawlingâ€, where the output would be doing the workswellonlywhentheupperbodypartneedstobeedited,duetoits
followerâ€™sactioninthesamerhythmandlegorderastheleader. fixedmask.Boldandunderlineindicatebestandsecond-bestresults,re-
AsforOODsynthesis,nocurrentmethoddiffusesmotionintoa spectively.
differentaction.Weassessactiontransfersamplesinourbenchmark
Metric RPrecision LeaderFoot Follower Follower
usingstyletransferandspatialeditingbaselines. FIDâ†“ â†‘ â†‘ â†‘ â†‘
Model (top3) ContactSim. Rot.Sim. Loc.Sim.
NNmotionspace 6.17 0.385 0.830 1.00 1.00
5.4 QuantitativeResults
+softmax 11.9 0.312 0.756 0.994 0.986
NNlatentspace 3.63 0.384 0.798 0.981 0.966
InTab.1wecompareMoMowiththebaselinesmentionedabove,
MoST[2024] 15.2 0.240 0.824 0.207 0.227
usingtheMTBbenchmark.OurworkexcelsinFIDandR-precision MDMInp.[2023] 3.51 0.213 1.00 0.244 0.329
whileachievingcomparablesimilarityscores.However,baselines MoMoGen.(Ours) 2.33 0.439 0.816 0.993 0.972
MoMoInv.(Ours) 2.50 0.490 0.793 0.933 0.856
withthehighestsimilarityscoresshowinferiorFIDandR-precision
yreuQ8 â€¢ Raab,S.etal.
Follower Leader Out
A person dancing A person walks in a circle
A person is dancing like a gorilla A person is dancing
A person is crouching A person is walking forward
time time time
Fig.7. QualitativeResults.Theresultsheredemonstratethetransferoftheleadermotionâ€™soutline,suchasthesequenceandpaceofsteps,tothefollower
whilepreservingthesubtlenuancesofthefollowerâ€™smotion.Thefirstandthirdrowsrelatetothespecialcaseofanactiontransfer.Inthesecondrow,we
illustrateanout-of-distributionsynthesisscenario;initially,thefollowerlacksadancingmotion,despitethetextprompt.However,afterthetransferprocess,
thesynthesizedmotionimitatesagorilladancing,asspecified.
Ourzero-shotandunpairedmethodology,namedMoMo(Monkey transfer,actiontransfer,andspatialediting.Byharnessingmotion
See, Monkey Do), demonstrates promising capabilities in trans- inversion,MoMoextendsitsapplicabilitytobothrealandgenerated
ferringtheoutlineofaleadermotionontoafolloweronewhile motions.
preservingthesubtlemotifsofthefollower.Thisenablesavari- Experimental results demonstrate the merits of our approach
etyofsub-taskssuchasout-of-distributionmotionsynthesis,style comparedtothebaselines.Notably,itexcelsinapplyingfunction-
alitiesatinferencetimewithoutadditionaltraining.Ourfindings
shedlightontheimportanceofattentioninhumanmotionandlay
Table2. Layersandstepsablation.Thetabledisplaysrepresentative
thegroundworkforfutureadvancementsinthefield.
resultsofthevariationsweexperimentedwithdiffusionstepsandlayers
Theprimarylimitationofourworkisthedifficultyintransferring
inwhichtheself-attentionfeaturesaremanipulated.Thetoprowdisplays
amotionwhenbasicoutlineelementsoftheleadermotionarelack-
theconfigurationofourselectedmodel,whereweapplymixed-attention
forself-attentionlayers2-12andfordiffusionsteps90to10(outof100). inginthefollower.Forexample,iftheleadermotiondepictswalking
Inthemiddleblock,wemaintainourbestlayerconfigurationandtest whilethefollowerremainsstationary,thentheoutputmotionwill
variousdiffusionstepranges.Inthebottomblock,wemaintainourbest alsobestationary,receivingnoinputfromtheleader.However,
diffusionstepsconfigurationandexperimentwiththerangeoflayers.To thislimitationmaybeinherenttothetaskdescriptionitself,asour
selectthebestconfiguration,weprioritizedFIDandR-Precisionoverthe objectiveisprimarilytotransfermotionsthatsharesomedegree
othermetrics. ofcommonalityintheiroutlines.Anothercurrentlimitationisthat
oncealeaderandafollowermotionsaredetermined,thereisno
Diffusion RPrecision LeaderFoot Follower Follower
Layers Steps FIDâ†“ (top3) â†‘ ContactSim.â†‘ Rot.Sim.â†‘ Loc.Sim.â†‘ diversityintheoutputmotionsobtainedfromthem.Thislimitation
2-12 10-90 2.334 0.439 0.816 0.993 0.972 arisesfromthedeterministicnatureoftheDDIM[Songetal.2020a]
diffusionmodelweareusing.Inthefuture,outputdiversitycould
2-12 20-80 3.028 0.412 0.820 0.993 0.973
2-12 15-90 2.833 0.406 0.817 0.992 0.975 beachievedusingnon-deterministicmodels(e.g.,DDPM).
2-12 20-70 2.867 0.416 0.821 0.991 0.973 A possible future direction we offer here is to explore latent
4-9 10-90 4.063 0.373 0.795 0.978 0.971 featurelayersotherthanself-attention,e.g.,cross-attentionandfeed-
5-11 10-90 2.971 0.393 0.837 0.989 0.967
forward.Additionally,ourfindingsonmotionmotifspreservation
4-10 10-90 3.098 0.404 0.821 0.991 0.974
couldbeappliedtopersonalizationapplications,adomainpopular
inimaging[Galetal.2023]butcurrentlylackinginthemotion
Table3. Textpromptablation.Ineachrow,adifferenttextualpromptis
domain.
utilizedfortheoutputmotion.Resultsindicatethatusingthesametextas
thefolloweryieldsthebestoutcomes. 7 ACKNOWLEDGMENTS
WethankRinonGalandOrPatashnikforfruitfuldiscussions.This
Metric RPrecision LeaderFoot Follower Follower
FIDâ†“ â†‘ â†‘ â†‘ â†‘
Prompt (top3) ContactSim. Rot.Sim. Loc.Sim. researchwassupportedinpartbytheIsraelScienceFoundation
Sameasfollower 2.572 0.434 0.814 0.994 0.975 (grantsno.2492/20and3441/21),LenBlavatnikandtheBlavatnik
None 3.182 0.410 0.817 0.986 0.948 familyfoundation,andtheTelAvivUniversityInnovationLabora-
â€œApersonâ€ 3.282 0.391 0.824 0.986 0.947
tories(TILabs).MonkeySee,MonkeyDo â€¢ 9
REFERENCES
LiHu,XinGao,PengZhang,KeSun,BangZhang,andLiefengBo.2023. Animate
KfirAberman,YijiaWeng,DaniLischinski,DanielCohen-Or,andBaoquanChen.2020. Anyone:ConsistentandControllableImage-to-VideoSynthesisforCharacterAni-
Unpairedmotionstyletransferfromvideotoanimation. ACMTransactionson mation.
Graphics(TOG)39,4(2020),64â€“1. InbarHuberman-Spiegelglas,VladimirKulikov,andTomerMichaeli.2023. AnEdit
Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel FriendlyDDPMNoiseSpace:InversionandManipulations.
Cohen-Or. 2023. Cross-Image Attention for Zero-Shot Appearance Transfer. Deok-KyeongJang,SoominPark,andSung-HeeLee.2022.Motionpuzzle:Arbitrary
arXiv:2311.03335[cs.CV] motionstyletransferbybodypart.ACMTransactionsonGraphics(TOG)41,3(2022),
OkanArikanandDavidAForsyth.2002.Interactivemotiongenerationfromexamples. 1â€“16.
ACMTransactionsonGraphics(TOG)21,3(2002),483â€“490. BiaoJiang,XinChen,WenLiu,JingyiYu,GangYu,andTaoChen.2024.MotionGPT:
NikosAthanasiou,MathisPetrovich,MichaelJBlack,andGÃ¼lVarol.2023. SINC: HumanMotionasaForeignLanguage.AdvancesinNeuralInformationProcessing
Spatialcompositionof3Dhumanmotionsforsimultaneousactiongeneration. Systems36(2024).
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.IEEE Roy Kapon, Guy Tevet, Daniel Cohen-Or, and Amit H. Bermano. 2023. MAS:
ComputerSociety,Washington,DC,USA,9984â€“9995. Multi-viewAncestralSamplingfor3Dmotiongenerationusing2Ddiffusion.
RishiBommasani,DrewAHudson,EhsanAdeli,RussAltman,SimranArora,Sydney arXiv:2310.14729[cs.CV]
vonArx,MichaelSBernstein,JeannetteBohg,AntoineBosselut,EmmaBrunskill, TeroKarras,SamuliLaine,andTimoAila.2019.Astyle-basedgeneratorarchitecture
etal.2021.Ontheopportunitiesandrisksoffoundationmodels. forgenerativeadversarialnetworks.InProceedingsoftheIEEE/CVFconferenceon
MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiang computervisionandpatternrecognition(CVPR).IEEEComputerSociety,Washington,
Zheng.2023.MasaCtrl:Tuning-FreeMutualSelf-AttentionControlforConsistent DC,USA,4401â€“4410.
ImageSynthesisandEditing. KorraweKarunratanakul,KonpatPreechakul,SupasornSuwajanakorn,andSiyuTang.
HilaChefer,YuvalAlaluf,YaelVinker,LiorWolf,andDanielCohen-Or.2023.Attend- 2023. GuidedMotionDiffusionforControllableHumanMotionSynthesis.In
and-excite:Attention-basedsemanticguidancefortext-to-imagediffusionmodels. ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.IEEE
ACMTransactionsonGraphics(TOG)42,4(2023),1â€“10. ComputerSociety,Washington,DC,USA,2151â€“2162.
SetarehCohan,GuyTevet,DanieleReda,XueBinPeng,andMichielvandePanne. BoeunKim,JunghoKim,HyungJinChang,andJinYoungChoi.2024.MoST:Motion
2024.FlexibleMotionIn-betweeningwithDiffusionModels. StyleTransformerbetweenDiverseActionContents.
Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian LucasKovar,MichaelGleicher,andFrÃ©dÃ©ricPighin.2002.Motiongraphs.InProceed-
Theobalt.2023.MoFusion:AFrameworkforDenoising-Diffusion-basedMotionSyn- ingsofthe29thannualconferenceonComputergraphicsandinteractivetechniques
thesis.InComputerVisionandPatternRecognition(CVPR).IEEEComputerSociety, (SIGGRAPH2002).ACM,NewYork,NY,USA,473â€“â€“482.
Washington,DC,USA. JeheeLee,JinxiangChai,PaulSAReitsma,JessicaKHodgins,andNancySPollard.2002.
OmerDahary,OrPatashnik,KfirAberman,andDanielCohen-Or.2024.BeYourself: Interactivecontrolofavatarsanimatedwithhumanmotiondata.InProceedingsof
BoundedAttentionforMulti-SubjectText-to-ImageGeneration. the29thannualconferenceonComputergraphicsandinteractivetechniques.ACM
PrafullaDhariwalandAlexanderNichol.2021.Diffusionmodelsbeatgansonimage NewYork,NY,USA,NewYork,NY,USA,491â€“500.
synthesis.AdvancesinNeuralInformationProcessingSystems34(2021),8780â€“8794. PeizhuoLi,KfirAberman,ZihanZhang,RanaHanocka,andOlgaSorkine-Hornung.
RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHaimBermano,GalChechik, 2022.GANimator:NeuralMotionSynthesisfromaSingleSequence.ACMTransac-
andDanielCohen-or.2023.AnImageisWorthOneWord:PersonalizingText-to- tionsonGraphics(TOG)41,4(2022),138.
ImageGenerationusingTextualInversion.InTheEleventhInternationalConference HanLiang,WenqianZhang,WenxuanLi,JingyiYu,andLanXu.2024. Intergen:
onLearningRepresentations.OpenReview.net,OpenReview.net. https://openreview. Diffusion-basedmulti-humanmotiongenerationundercomplexinteractions.In
net/forum?id=NAQvF08TcyG InternationalJournalofComputerVision.Springer,Berlin/Heidelberg,Germany,
DanielGaribi,OrPatashnik,AndreyVoynov,HadarAverbuch-Elor,andDanielCohen- 1â€“21.
Or.2024.ReNoise:RealImageInversionThroughIterativeNoising. ShitongLuoandWeiHu.2021. Diffusionprobabilisticmodelsfor3dpointcloud
LeonA.Gatys,AlexanderS.Ecker,andMatthiasBethge.2015.ANeuralAlgorithmof generation.InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
ArtisticStyle. arXiv:1508.06576[cs.CV] PatternRecognition.IEEEComputerSociety,Washington,DC,USA,2837â€“2845.
MichalGeyer,OmerBar-Tal,ShaiBagon,andTaliDekel.2024. TokenFlow:Con- RonMokady,AmirHertz,KfirAberman,YaelPritch,andDanielCohen-Or.2023.Null-
sistentDiffusionFeaturesforConsistentVideoEditing.InTheTwelfthInterna- textinversionforeditingrealimagesusingguideddiffusionmodels.InProceedingsof
tionalConferenceonLearningRepresentations.OpenReview.net,OpenReview.net. theIEEE/CVFConferenceonComputerVisionandPatternRecognition.IEEEComputer
https://openreview.net/forum?id=daEqXJ0yZo Society,Washington,DC,USA,6038â€“6047.
PurviGoel,Kuan-ChiehWang,CKarenLiu,andKayvonFatahalian.2023.Iterative YanivNikankin,NivHaim,andMichalIrani.2023. SinFusion:TrainingDiffusion
MotionEditingwithNaturalLanguage. ModelsonaSingleImageorVideo.InInternationalConferenceonMachineLearning.
ChuanGuo,YuxuanMu,XinxinZuo,PengDai,YouliangYan,JuweiLu,andLiCheng. PMLR,PMLR,PMLR,26199â€“26214.
2024.GenerativeHumanMotionStylizationinLatentSpace.InTheTwelfthInterna- OrPatashnik,RinonGal,DanielCohen-Or,Jun-YanZhu,andFernandoDelaTorre.
tionalConferenceonLearningRepresentations.OpenReview.net,OpenReview.net. 2024.ConsolidatingAttentionFeaturesforMulti-viewImageEditing.
https://openreview.net/forum?id=daEqXJ0yZo OrPatashnik,DanielGaribi,IdanAzuri,HadarAverbuch-Elor,andDanielCohen-Or.
ChuanGuo,ShihaoZou,XinxinZuo,SenWang,WeiJi,XingyuLi,andLiCheng.2022. 2023.LocalizingObject-LevelShapeVariationswithText-to-ImageDiffusionModels.
GeneratingDiverseandNatural3DHumanMotionsFromText.InProceedingsofthe InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision(ICCV).
IEEE/CVFConferenceonComputerVisionandPatternRecognition.IEEEComputer SpringerInternationalPublishing,Berlin/Heidelberg,Germany,23051â€“23061.
Society,Washington,DC,USA,5152â€“5161. MathisPetrovich,MichaelJ.Black,andGÃ¼lVarol.2021.Action-Conditioned3DHuman
AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDaniel MotionSynthesiswithTransformerVAE.InInternationalConferenceonComputer
Cohen-Or.2023.Prompt-to-promptimageeditingwithcrossattentioncontrol.In Vision(ICCV).IEEEComputerSociety,Washington,DC,USA,10985â€“10995.
TheEleventhInternationalConferenceonLearningRepresentations(ICLR).OpenRe- MathisPetrovich,MichaelJ.Black,andGÃ¼lVarol.2022.TEMOS:Generatingdiverse
view.net,OpenReview.net. humanmotionsfromtextualdescriptions.InEuropeanConferenceonComputer
MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSepp Vision(ECCV).SpringerInternationalPublishing,Berlin/Heidelberg,Germany.
Hochreiter.2017.Ganstrainedbyatwotime-scaleupdateruleconvergetoalocal MathisPetrovich,OrLitany,UmarIqbal,MichaelJ.Black,GÃ¼lVarol,XueBinPeng,
nashequilibrium.Advancesinneuralinformationprocessingsystems30(2017). andDavisRempe.2024.STMC:Multi-TrackTimelineControlforText-Driven3D
JonathanHo,AjayJain,andPieterAbbeel.2020. Denoisingdiffusionprobabilistic HumanMotionGeneration.
models.AdvancesinNeuralInformationProcessingSystems33(2020),6840â€“6851. SigalRaab,InbalLeibovitch,PeizhuoLi,KfirAberman,OlgaSorkine-Hornung,and
JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,MohammadNorouzi, DanielCohen-Or.2023.MoDi:UnconditionalMotionSynthesisfromDiverseData.
andDavidJFleet.2022.Videodiffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition
DanielHolden,IkhsanulHabibie,IkuoKusajima,andTakuKomura.2017a.Fastneural (CVPR).IEEEComputerSociety,Washington,DC,USA,13873â€“13883.
styletransferformotiondata.IEEEcomputergraphicsandapplications37,4(2017), SigalRaab,InbalLeibovitch,GuyTevet,MoabArar,AmitHBermano,andDaniel
42â€“49. Cohen-Or.2024.SingleMotionDiffusion.InTheTwelfthInternationalConference
DanielHolden,TakuKomura,andJunSaito.2017b.Phase-functionedneuralnetworks onLearningRepresentations(ICLR).OpenReview.net,OpenReview.net.
forcharactercontrol.ACMTransactionsonGraphics(TOG)36,4(2017),1â€“13. OlafRonneberger,PhilippFischer,andThomasBrox.2015. U-net:Convolutional
DanielHolden,JunSaito,andTakuKomura.2016. Adeeplearningframeworkfor networksforbiomedicalimagesegmentation.InInternationalConferenceonMedical
charactermotionsynthesisandediting.ACMTransactionsonGraphics(TOG)35,4 imagecomputingandcomputer-assistedintervention.Springer,SpringerInternational
(2016),1â€“11. Publishing,Berlin/Heidelberg,Germany,234â€“241.
ChitwanSaharia,WilliamChan,HuiwenChang,ChrisLee,JonathanHo,TimSalimans,
DavidFleet,andMohammadNorouzi.2022a. Palette:Image-to-imagediffusion10 â€¢ Raab,S.etal.
models.InACMSIGGRAPH2022ConferenceProceedings.ACM,NewYork,NY,USA, APPENDIX
1â€“10.
ThisAppendixaddsdetailsontopoftheonesgiveninthemain
ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDen-
ton,SeyedKamyarSeyedGhasemipour,RaphaelGontijo-Lopes,BurcuKaragol paper.Whilethemainpaperstandsonitsown,thedetailsgiven
Ayan,TimSalimans,JonathanHo,DavidJ.Fleet,andMohammadNorouzi.2022b. heremayshedmorelight.
PhotorealisticText-to-ImageDiffusionModelswithDeepLanguageUnderstand-
ing. AdvancesinNeuralInformationProcessingSystems35(2022),36479â€“36494. InappendixAweprovidemoredetailsregardingtheprelimi-
https://openreview.net/forum?id=08Yk-n5l2Al nariesofourwork:motionrepresentation,andthemodelsMDM,
YoniShafir,GuyTevet,RoyKapon,andAmitHaimBermano.2024.HumanMotion
DDPM,andDDIM.AppendixBelaboratesonourexperiments,in
DiffusionasaGenerativePrior.InTheTwelfthInternationalConferenceonLearning
Representations.OpenReview.net,OpenReview.net. particularthebenchmark,metrics,hypermparameters,andaddi-
TamarRottShaham,TaliDekel,andTomerMichaeli.2019.SinGAN:Learningagenera- tionalimplementationdetails.
tivemodelfromasinglenaturalimage.InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision.IEEEComputerSociety,Washington,DC,USA,
4570â€“4580. A PRELIMINARIES-MOREDETAILS
JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli.2015.
Deepunsupervisedlearningusingnonequilibriumthermodynamics.InInternational Motion Representation. Recall that ğ‘ denotes the number of
ConferenceonMachineLearning.PMLR,PMLR,PMLR,2256â€“2265. framesinamotionsequence,ğ¹ denotesthelengthofthefeatures
JiamingSong,ChenlinMeng,andStefanoErmon.2020a.DenoisingDiffusionImplicit
Models.InInternationalConferenceonLearningRepresentations.OpenReview.net, describingasingleframe,ğ½ denotesthenumberofskeletaljoints,
OpenReview.net. and ğ‘‹ âˆˆ Rğ‘Ã—ğ¹ denotes a motion. Each feature is redundantly
JiamingSong,ChenlinMeng,andStefanoErmon.2021.DenoisingDiffusionImplicit
represented with the joint angles, positions, velocities, and foot
Models.InInternationalConferenceonLearningRepresentations.OpenReview.net,
OpenReview.net. https://openreview.net/forum?id=St1giarCHLP contact[Guoetal.2022].Eachsingleposeisdefinedby
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoEr-
mon,andBenPoole.2020b.Score-BasedGenerativeModelingthroughStochastic (ğ‘Ÿ(cid:164)ğ‘,ğ‘Ÿ(cid:164)ğ‘¥,ğ‘Ÿ(cid:164)ğ‘§,ğ‘Ÿğ‘¦,ğ‘—ğ‘,ğ‘—ğ‘Ÿ,ğ‘—ğ‘£,ğ‘ğ‘“
)
âˆˆRğ¹,
DifferentialEquations.InInternationalConferenceonLearningRepresentations.Open-
Review.net,OpenReview.net.
SebastianStarke,IanMason,andTakuKomura.2022.Deepphase:Periodicautoencoders whereğ‘Ÿ(cid:164)ğ‘ âˆˆRistherootangularvelocityalongtheY-axis.ğ‘Ÿ(cid:164)ğ‘¥,ğ‘Ÿ(cid:164)ğ‘§ âˆˆR
forlearningmotionphasemanifolds.ACMTransactionsonGraphics(TOG)41,4
(2022),1â€“13. arerootlinearvelocitiesontheXZ-plane,andğ‘Ÿğ‘¦ âˆˆ Ristheroot
GuyTevet,BrianGordon,AmirHertz,AmitHBermano,andDanielCohen-Or.2022. height.ğ‘—ğ‘ âˆˆR3(ğ½âˆ’1),ğ‘—ğ‘Ÿ âˆˆR6(ğ½âˆ’1) andğ‘—ğ‘£ âˆˆR3ğ½ arethelocaljoint
M ECo Cti Von 20c 2li 2p :: 1E 7x thp Eo usi rn og peh au nm Ca on nfm ero enti co en ,Tg ee ln Ae vr ia vt ,i Io srn aeto l,Ocl cip tobsp erac 2e 3. â€“I 2n 7,C 2o 0m 22p ,u Pt re or ceV ei ds ii non gsâ€“
,
positions,velocities,androtationsrelativetotheroot,andğ‘ğ‘“ âˆˆR4
PartXXII.Springer,SpringerInternationalPublishing,Berlin/Heidelberg,Germany, arebinaryfeaturesdenotingthefootcontactlabelsforfourfoot
358â€“374.
joints(twoforeachleg).
GuyTevet,SigalRaab,BrianGordon,YoniShafir,DanielCohen-or,andAmitHaim
Bermano.2023. HumanMotionDiffusionModel.InTheEleventhInternational
MDM,DDPMandDDIM. RecallthatMDM[Tevetetal.2023]does
ConferenceonLearningRepresentations(ICLR).OpenReview.net,OpenReview.net.
https://openreview.net/forum?id=SJ1kSyO2jwu humanmotionsynthesisandeditingandthatitusesDDPMs[Ho
JonathanTseng,RodrigoCastellon,andKarenLiu.2023.Edge:Editabledancegeneration etal.2020].Inthefollowing,webrieflydescribethemechanismof
frommusic.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
DDPM.
Recognition(CVPR).IEEEComputerSociety,Washington,DC,USA,448â€“458.
NarekTumanyan,MichalGeyer,ShaiBagon,andTaliDekel.2023. Plug-and-Play Aninputmotionğ‘¥ 0,issubjectedtoaMarkovnoiseprocesscon-
D thi eff Iu Es Eio En /CF Ve Fat Cu or ne fs ef ro enr cT ee ox nt- CD or miv pe un tI em rVag ise i- ot no- aI nm da Pg ae ttT er ra nn Rsl ea ct oi go nn i. tiI on nP (r Co Vce Pe Rd )i .n Ig Es Eo Ef sistingofTsteps,resultinginthesequence{ğ‘¥ ğ‘¡}ğ‘‡ ğ‘¡=0,suchthat
ComputerSociety,Washington,DC,USA,1921â€“1930. âˆš
AaronVanDenOord,OriolVinyals,etal.2017.Neuraldiscreterepresentationlearning. ğ‘(ğ‘¥ ğ‘¡|ğ‘¥ ğ‘¡âˆ’1)=N( ğ›¼ ğ‘¡ğ‘¥ ğ‘¡âˆ’1,(1âˆ’ğ›¼ ğ‘¡)ğ¼), (5)
Advancesinneuralinformationprocessingsystems30(2017).
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN
Gomez,ÅukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.Advances whereğ›¼ ğ‘¡ âˆˆ (0,1)areconstanthyper-parameters.Whenğ›¼ ğ‘¡ issmall
inneuralinformationprocessingsystems30(2017). enough,wecanapproximateğ‘¥
ğ‘‡
âˆ¼N(0,ğ¼).
JayZhangjieWu,YixiaoGe,XintaoWang,StanWeixianLei,YuchaoGu,YufeiShi,
WynneHsu,YingShan,XiaohuQie,andMikeZhengShou.2023.Tune-a-video:One- ğ‘¥ 0canbemodeledviathereverseddiffusionprocessbygradually
shottuningofimagediffusionmodelsfortext-to-videogeneration.InProceedingsof cleaning ğ‘¥ ğ‘‡, using a generative network ğ‘ ğœƒ. MDM [Tevet et al.
Wthe asI hE iE nE g/ tC oV nF ,DIn Cte ,r Un Sa Atio ,n 7a 6l 23C â€“o 7n 6fe 3r 3e .nceonComputerVision.IEEEComputerSociety, 2023]predictstheinputmotion,denotedğ‘¥Ë†0,ratherthanğœ– ğ‘¡,such
YimingXie,VarunJampani,LeiZhong,DeqingSun,andHuaizuJiang.2023.Omni-
thatğ‘¥Ë†0=ğ‘ ğœƒ(ğ‘¥ ğ‘¡,ğ‘¡).Then,thewidespreaddiffusionlossisapplied:
Control:ControlAnyJointatAnyTimeforHumanMotionGeneration.InThe
TwelfthInternationalConferenceonLearningRepresentations.OpenReview.net,Open-
JianR re ov ni gew Zh.n ae nt g.
,YangsongZhang,XiaodongCun,ShaoliHuang,YongZhang,Hongwei
L simple=E ğ‘¡âˆ¼[1,ğ‘‡]âˆ¥ğ‘¥ 0âˆ’ğ‘ ğœƒ(ğ‘¥ ğ‘¡,ğ‘¡)âˆ¥2 2. (6)
Zhao,HongtaoLu,andXiShen.2023b.T2M-GPT:GeneratingHumanMotionfrom
TextualDescriptionswithDiscreteRepresentations.InProceedingsoftheIEEE/CVF Duringinference,synthesisiteratesfrompurenoiseğ‘¥ ğ‘‡.Ineach
C Soo cn if ee tr ye ,n Wce aso hn inC go tm onp ,u Dte Cr ,V Ui Ssi Aon . andPatternRecognition(CVPR).IEEEComputer iteration,thedenoisingnetworkğ‘ ğœƒ predictsacleanversionofthe
MingyuanZhang,HuirongLi,ZhongangCai,JiaweiRen,LeiYang,andZiweiLiu.2024.
currentsampleğ‘¥ ğ‘¡.Thepredictedcleansampleğ‘¥Ë†0isthenâ€œre-noised"
Finemogen:Fine-grainedspatio-temporalmotiongenerationandediting.Advances tocreatethenextsampleğ‘¥ ğ‘¡âˆ’1,repeatedlyuntilğ‘¡ =0.
inNeuralInformationProcessingSystems36(2024).
DenoisingDiffusionImplicitModels(DDIM)[Songetal.2021]
QinshengZhang,JiamingSong,XunHuang,YongxinChen,andMingyuLiu.2023a.
DiffCollage:ParallelGenerationofLargeContentwithDiffusionModels.In2023 enableanonMarkovian,deterministic,versionofDDPMs.DDIM
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).IEEE,IEEE canbeappliedonanetworkpre-trainedwithDDPM,foreither
ComputerSociety,Washington,DC,USA,10188â€“10198.
acceleratedinference,orinversion.Inthiswork,weemployDDIM
forinversionanddeterministicinference,toreconstructinverted
motionspreciselytotheiroriginalform.MonkeySee,MonkeyDo â€¢ 11
B EXPERIMENTS-MOREDETAILS
countedasmisses.Themetricscoreisdeterminedbytherateof
B.1 Benchmark hits.
SimilaritytoFollowerRotations. Thismetricevaluatesthefi-
ThecompletelistoffiltertermsusedtocreatetheMTBbenchmark
delityoftheoutputmotiontothesubtlemotifsfoundinthefol-
isasfollows.Foreachpairinthebenchmark,theleadercontains
lower.Specifically,itassessestheresemblanceofrotationswithin
oneofthefollowinglocomotionkeywords:â€œrunâ€,â€œwalkâ€,â€œjumpâ€,
theoutputmotiontoeithertheleaderorfollowermotions.Given
â€œdancâ€.Thefollowercontainsoneoftheaforementionedlocomotion
thatsubtlenuancesaretypicallyexpressedinmostjointsâ€™rotations,
keywords,plusonestyleoractionkeyword,fromthelistâ€œgorillaâ€,
weanticipateobservingahigherdegreeofsimilaritytothefollower
â€œdrunkâ€,â€œrobotâ€,â€œchickenâ€,â€œfrogâ€,â€œmonkeyâ€,â€œstyleâ€,â€œlikeâ€,â€œoldâ€,
motion.
â€œchildâ€,â€œraiseâ€,â€œclapâ€,â€œwavâ€,â€œkickâ€,â€œpunchâ€,â€œpushâ€,â€œpullâ€.
Tocomputethismetric,theinitialstepistoidentify,foreach
FollowingareseveralexamplesofpairsfromtheMTBbenchmark.
frame,thenearestneighborinboththeleaderandfollowermotions.
Eachexampleisidentifiedbyitstextpromptanditsindexnumber
Thisentailsidentifyingtheclosestmatchforeachframe.
intheHumanML3Ddataset.
Oncethenearestneighborsareidentified,themetriccalculates
â€¢ Leader:â€œapersonwalkstowardstheleftmakingawideâ€™sâ€™ therateofframeswherethesimilaritytothefollowerâ€™snearest
shapeâ€(#009488).Follower:â€œapersonwalksinaclockwise neighbor surpasses that of the leaderâ€™s. Essentially, it evaluates
circleandraisestheirhandtotheirfacetoyawnâ€(#004222). theproportionofrotationframeswheretheoutboundmotionex-
â€¢ Leader:â€œapersonrunsandthenjumpsâ€(#007291).Follower: hibitsgreateralignmentwiththefollowerâ€™smotioncomparedto
â€œthedrunkguystrugglestowalkdownthestreetâ€(#005037). thetrackedone.
â€¢ Leader:â€œitisapersonwalkingbackwardsâ€(#007199).Fol- SimilaritytoFollowerLocations. Thismetricalsoevaluatesthe
lower:â€œpersoniswalkingwithhisarmsoutlikeheisbal- fidelityoftheoutputmotiontofollowerâ€™smotifs.Similartothe
ancingâ€(#010823). previousmetric,itcalculatesthenearestneighborsforeachframe,
but this time for the locations relative to the root. As with the
B.2 Metrics
previous metric, we expect a greater degree of similarity to the
FID[Guoetal.2022;Heuseletal.2017]. Weusethismetricto followermotion.
assess the quality of the output motions. The FrÃ©chet Inception
B.3 ImplementationDetails
Distance (FID) evaluates the similarity between the distribution
ofrealmotionsandsynthesizedonesWeconsiderthebenchmark
datasetasthegroundtruthdistributionandextractfeaturesfrom
Table4. Hyperparametersusedforourbackbone.Ourbackboneisa
variationofMDM,withthehyperparameterslistedhere.
bothrealmotionsinthebenchmarkandgeneratedoutputmotions.
Motionsaredeemedofhighqualityiftheyexistwithinthemanifold Name Value
ofthegroundtruth. Model
R-precision[Guoetal.2022]. Weusethismetrictoassessthe Architecture TransformerDecoder
alignmentbetweeneachoutputmotionandthemotionmotifsof layers 12
theirassociatedfollowermotions.R-precisionusesalatentspace latentdim 512
sharedbetweentextandmotiontomeasurethedistancebetween Diffusion
motionandtextembeddings.Hence,itmeasureswhetheramotion diffusionsteps 100
reflectssometextprompt.Thetextpromptofeachoutputmotion noiseschedule cosine
iscopiedfromthefollowerusedwhengeneratingit,soasuccessful guidancescale 2.5
matchmeanstheoutputmotionadherestothemotionpatternof Training
thefollower. batchsize 32
Foreachgeneratedmotion,wecreateadescriptionpoolconsist- lr 0.0001
ingofitsground-truthtextdescriptionand31randomlychosen, dropout 0.1
unrelateddescriptionsfromthetestset.Wethencalculateandrank numsteps 600000
theEuclideandistancesbetweenthemotionfeatureandthetext warmupsteps 0
featureofeachdescriptioninthepool.Wemeasuresuccessbythe weightdecay 0
averageaccuracyatthetop-1,top-2,andtop-3positions.Asuccess- seed 10
fulretrievaloccurswhenthegroundtruthentryranksinthetop-k
candidates.
Backboneandhyperparameters. Thebackboneweuseforre-
FootContactSimilarity. Thismetricassesseshowwelltheoutput portingexperimentalresultsisthetransformerdecodervariation
motionretainsthelocomotionrhythmoftheleadermotion.Suc- ofMDM[Tevetetal.2023].WeretrainitontheHumanML3D[Guo
cessfulmotiontransfershouldpreservetherhythmfromleaderto etal.2022]dataset,withthehyperparametersshowninTab.4.In
outputmotion,sothefootcontactlabelsofthetwomotionsshould particular,weslightlychangethearchitecturesuchthatthetext
closelyalign.Wemeasurethissimilarityscorebycomparingthe embeddingisgiventwice:onceasanextra-temporaltokenlike
footcontactdatawithintheHumanML3Dmotionfeatures.Match- inthetransformerencodervariation,andonceasseparateword
ingfootcontactlabelsarecountedashitswhiledifferinglabelsare tokensusingcross-attention.12 â€¢ Raab,S.etal.
Table 5. Hyperparameters used during inference when applying butwouldproceedinadifferentdirection.Fortunately,thesolu-
MoMo.
tionisstraightforward.Forgeneratedmotions,wecreatemultiple
Name Value followers,eachgeneratedwithadifferentseed,forthegiventext
Applyingmixed-attention prompt.MoMoisthenappliedtotheconcatenationofallfollowers
together.Ourexperimentsshowthatgeneratingjustafewfollowers
Layers range(1,12)
ensuresthattheoutputmotioncloselyfollowsthedirectionofthe
Diffusionsteps range(10,91)
follower.Forinvertedmotions,usingdifferentseedswouldnothelp
Textprompt
astheinversionbyDDIMisdeterministic.Oursolutionistorotate
Outputmotion Sameasfollower
thegivenmotionaroundtheverticalaxisinseveralrotationan-
gles.Similartothegenerativecase,weapplyourframeworktothe
Additionally,Tab.5provideshyperparametersusedduringinfer- concatenationofallrotatedfollowers.Consequently,eachleaderâ€™s
encewhenapplyingMoMo. frameattendstotheframesofallthefollowersandachievesthe
highestattentionscoreswiththosethathavedirectionssimilarto
Controllingdirectionsduringmotiontransfer. Anaturalchal-
itsown.Thisfacilitatesnear-accuratedirectiontransfer.Inpractice,
lengeariseswhenthelocomotiondirectionoftheleadermotion
wehavefoundthatthissolutionisnearlyequivalenttocopying
differsfromthatofthefollowerâ€™s.Insuchcases,theoutputmotion
therootrotationvaluefromtheleadertotheoutputmotion.For
wouldretaintheoutlineoftheleader,suchasthetimingofsteps,
evaluation,weusedthelatter,toacceleratecomputationtime.