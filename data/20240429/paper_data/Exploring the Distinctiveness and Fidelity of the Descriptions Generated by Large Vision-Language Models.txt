Exploring the Distinctiveness and Fidelity of the Descriptions
Generated by Large Vision-Language Models
YuhangHuang‚àó ZihanWu‚àó ChongyangGao
huangyh723@seu.edu.cn zihanwuseu@outlook.com chongyanggao2026@u.northwestern.edu
SoutheastUniversity SoutheastUniversity NorthwesternUniversity
Nanjing,Jiangsu,China Nanjing,Jiangsu,China Evanston,Illinois,UnitedStates
JiaweiPeng XuYang‚Ä†
pengjiawei@seu.edu.cn xuyang_palm@seu.edu.cn
SoutheastUniversity SoutheastUniversity
Nanjing,Jiangsu,China Nanjing,Jiangsu,China
ABSTRACT Caption:
a small white dog
LargeVision-LanguageModels(LVLMs)aregainingtractionfor Label: Maltese dog
VVVLLLMMM laying on the floor
theirremarkableabilitytoprocessandintegratevisualandtextual
data. Despite their popularity, the capacity of LVLMs to gener- FGVD:
ate precise, fine-grained textual descriptions has not been fully
The image shows a
explored. This study addresses this gap by focusing on distinc- small white dog lying
tiveness andfidelity,assessinghowmodelslikeOpen-Flamingo, on the floor in front of
IDEFICS,andMiniGPT-4candistinguishbetweensimilarobjects Referenced a table. The dog has
LLLVVVLLLMMM
andaccuratelydescribevisualfeatures.WeproposedtheTextual Text: N/A big brown eyes and a
Retrieval-AugmentedClassification(TRAC)framework,which,by fluffy white coat. Its
leveragingitsgenerativecapabilities,allowsustodelvedeeperinto Describe the visual head is resting on its
analyzingfine-grainedvisualdescriptiongeneration.Thisresearch features for this paws, and it looks up
dog in the image at the camera with a
providesvaluableinsightsintothegenerationqualityofLVLMs,
curious expression.
enhancingtheunderstandingofmultimodallanguagemodels.No-
tably,MiniGPT-4standsoutforitsbetterabilitytogeneratefine-
Figure1:ThecaptionproducedbyasmallerVisionLanguage
graineddescriptions,outperformingtheothertwomodelsinthis
Model(VLM)offersabroadoverviewoftheimage.Incon-
aspect.Thecodeisprovidedathttps://anonymous.4open.science/r/
trast,thefine-grainedvisualdescription(FGVD),generated
Explore_FGVDs-E277.
bytheLargeVisionLanguageModel(LVLM)conditionedon
CCSCONCEPTS bothvisualandlinguisticcues,encompassesmorenuanced
details.
‚Ä¢Computingmethodologies‚ÜíSceneunderstanding;Natural
languagegeneration;Visualcontent-basedindexingandretrieval.
KEYWORDS withtextualdescriptions,facilitatingabroadspectrumofVisual-
Languagetasks,includingImageCaptioning(IC)[16,32].However,
LargeVisionLanguageModels,VisualDescription,Distinctiveness,
existingmodelsfocusongenerativemethods[42],andthecurrent
Fidelity,LVLMHallucination
solutionforevaluatingthequalityofthegeneratedtextisrelatively
simplistic,makingitdifficulttoeffectivelydiscernthequalitydiffer-
1 INTRODUCTION encesbetweentexts,particularlyintermsofaccurateanddetailed
Intheevolvinglandscapeofartificialintelligence,Vision-Language descriptionsofimages.Thislimitationhighlightsapromisingarea
Models(VLMs)suchasCLIPandBLIP[22,30]havebeeninstru- forfurtherresearch.
mentalinenhancingmachineinterpretationofvisualcontentby Toaddressthisgap,exploringthedifferencesindetailedgen-
aligningthevisualandtextembeddingspacesthroughcontrastive erationqualityamongLVLMsisrequired.However,generating
learningtechniques.Buildingonthisfoundationandlayingthe accurate,fine-grainedvisualdescriptions(FGVDs)andcapturing
groundworkforgeneratingcoherent,relevantcaptionsacrossa thenuanceddetailsessential[10]continuestopresentsignificant
diverserangeofvisualcontent[6],LargeVision-LanguageModels challenges.Initially,thesechallengesareespeciallypronouncedin
(LVLMs)likeOpen-Flamingo[4],IDEFICS[20],andMiniGPT-4[43] fieldsthatdemandexceptionalspecificity,suchasbiodiversityfor
have advanced the field further. These innovations have signif- precisespeciesidentification[12],andintechnicaldomainsthat
icantlyimprovedmachinecapabilitytointegratevisualcontent requiredifferentiationbetweencloselyrelatedyetdistinctobjects.
Also,traditionalcaptioningdatasetssuchasMicrosoftCOCO[9],
‚àóBothauthorscontributedequallytothisresearch. Flickr30k[29],andVisualGenome[19]withimagescontaining
‚Ä†Correspondingauthor. amultitudeofsubjectsandvariedelements,oftenonlyprovide
4202
rpA
62
]VC.sc[
1v43571.4042:viXraACMMM,28October-1November,2024,Melbourne,Australia YuhangHuang,ZihanWu,ChongyangGao,JiaweiPeng,andXuYang
broadoverviewsintheirground-truthdescriptions.Specifically, amongcloselyrelatedcategories.First,asubsetofgenerateddescrip-
theirefficacyinproducingfine-graineddescriptionsissignificantly tionsisreservedasareferencecorpusagainstwhichtestdescrip-
hamperedbytheirinherentbrevity,simplicity,andmulti-target tionsarecompared.Thedistinctivenessofthesedescriptionsisthen
focus.Furthermore,traditionalevaluationmetricslikeBLEU[28], indirectlyassessedbymeasuringtheaccuracywithwhichtheyare
CIDEr[34],andSPICE[2]areinadequateforassessingfine-grained categorizedunderthecorrectlabels,providingawaytogaugethe
descriptionsbecausetheyrelyheavilyonthesecoarseground-truth uniquenessofthegeneratedFGVDs.Forfidelity,Itincludestwoap-
descriptions,whichlacksthenecessarygranularity. proaches:firstly,applyingtheCLIPembeddingsimilaritytoassess
These limitations have underscored a critical gap in evaluat- thealignmentbetweenimagesandtheirtextualoutputsshowcases
ingthecapabilitiesofLVLMstodeliverdetaileddescriptions.In thedirecttext-imagerelationship;secondly,employingtheStable
responsetotheaforementionedissues,someresearchershavepro- DiffusionmodeltotransformLVLM-generatedFGVDsbackinto
posedtheCLIPscoreasanalternativetotraditionaln-gram-based images,inspiredbytheconceptoftextasaneffectivecross-modal
metrics.Althoughitofferssomeadvantages,theCLIPscorestill interface[37].Thisallowsforfidelityevaluationthroughimage-
cannotcapturefine-grainednuancesbecausethecontrastivelearn- to-imagecomparison.Throughtheevaluationofdistinctiveness
ingprocessoftheCLIPmodelstillreliesonimagesandcoarse andfidelity,thismethodfacilitatesacomprehensiveanalysisofthe
sentencepairs.Consequently,itremainsanimperfectmetricfor qualityoffine-grainedvisualdescriptionsgeneratedbyLVLMs,
evaluatingthequalityofdetailedtextualgeneration.Thisunder- therebycapturingtheirnuancedcapabilitiesandperformancefrom
scorestheneedtoreevaluatemethodsanddevelopinsightsfor severalcriticalperspectives.
fine-grainedtextualgenerationthatovercometheconstraintsof Byexaminingthenuancesofgeneratingdetailedimagedescrip-
traditionalcaptioningdatasets,callingfornovelapproachestobet- tionswithLVLMsfrommultipleperspectives,thisstudyaimsto
terunderstandtheabilityofLVLMtoproducehigh-qualityvisual deepentheunderstandingofhowthesemodelsperformincreating
descriptions. fine-grainedvisualdescriptions.Thiscomprehensiveanalysisex-
Inthispaper,totackletheissuesoutlinedabove,weproposea ploresvariousaspectsofdescriptiongeneration,highlightingthe
frameworktoanalyzethequalityofFGVDsgeneratedbyLVLMs capabilitiesandlimitationsofLVLMsinthiscontext.Contributions
fromtwoperspectives.WefocusondistinctivenessbecauseLVLMs ofthisresearchinclude:
effectively recognize the broad categories to which images be-
(1) WearethefirsttoevaluatethequalityofLVLM-generated
long.Thisaspectisexploredfromafine-grainedperspectiveto
fine-grainedvisualdescriptionsthroughdistinctivenessand
furtherunderstandandenhancetheircapabilities.Thisiscrucial
fidelity,enrichingmultimodallanguagemodelresearchand
becauseofthecomparativeadvantagesofgenerativeLVLMsover
identifyingareasforsystemrefinement.
non-generativemodelslikeCLIP,whichdonotspecificallytailor
(2) WeintroducedanovelmethodTextualRetrieval-Augmented
theiroutputstoaddressthecomplexitiesinherentinfine-grained
Classification(TRAC)thatutilizesLVLMsforclassification,
datasets.Incontrasttoearliermodelsthatheavilyreliedoncap-
inspiredbytheRAGframework.Thisapproachaddressesthe
tioningdatasetssuchasMicrosoftCOCO,currentLVLMsdemon-
uniquechallengesposedbythegenerativenatureofLVLMs,
stratemarkedimprovementsinthegranularityoftheirdescrip-
unlike contrastive models like CLIP. It thereby broadens
tions,whichmakesiturgenttoevaluatetheprecisionandrele-
the tasks of LVLMs and provides new insights into their
vanceofgenerateddescriptionswithfine-graineddatasets.Toover-
generations.
cometheselimitations,thisstudyutilizesfine-grainedclassification
(3) We determined the conditions under which LVLMs pro-
datasetstailoredtospecificcategories[8],whicharedistinctfrom
ducehallucinationsindetailedFGVDs.Byintegratinghigh-
traditionalcaptioningdatasets.Theseresourcesmakeitpossibleto
qualityGPT-4descriptionswithIn-ContextLearning,we
evaluatetheextenttowhichLVLM-generatedtextscaneffectively
significantlyimprovedtheabilityofLVLMstoproducemore
distinguishamongcategoriesatagranularlevel.Weexaminethe
detailedandaccuratetext.
abilityofthesemodelstogeneratedescriptionsthateffectivelydis-
tinguishamongdiversecategoriesbypreciselyidentifyingandar- 2 RELATEDWORKS
ticulatingnuancesbetweencloselyrelatedvisualsubjects.However,
2.1 GeneratingVisualDescriptions
thegenerateddescriptionsmightinaccuratelyreflectthevisualcon-
tent,therebynecessitatingtheinclusionoffidelity.Fidelityassesses Image captioning (IC) is a pivotal task in computer vision that
fromamorecoarse-grainedviewpointtoensurethatthegenerated involves translating images into textual descriptions. Tradition-
textnotonlydifferentiateseffectivelybutalsoremainsaccurate ally,numerouscaptioningmodels[3,13,35]havebeendeveloped,
andfaithfultotheoriginalimages.Thisbalancedapproach‚Äîfrom employingencoder-decoderarchitecturestoachievehigh-quality
distinctivenesstofidelityenhancesourunderstandingofmodel captiongeneration.Inrecentyears,advancementshavebeenmade
performanceintheintricatetaskoffine-grainedtextgeneration, throughtheextensivetrainingofmodelsonlarge-scaleimage-text
emphasizingtheimportanceofbothgranulardetailandoverall pairdatasetsandtheintegrationofvisualandlinguisticmodalities.
accuracyintheevaluationprocess. InnovationssuchasClipCap[27]andBLIP[22]havesignificantly
Toevaluatedistinctiveness,ourapproachdrawsinspirationfrom improvedtheunderstandingofvisualcontent,therebyenhancing
theRetrieval-AugmentedGeneration(RAG)framework[21]within theaccuracyandcontextualrelevanceofgeneratedcaptions.
NLP. We have developed a method named "Textual Retrieval- However,thecaptionsgeneratedbythesemodelsoftenprovide
AugmentedClassification"(TRAC),whichutilizesclassification- onlyabroadoverviewoftheimage,lackingspecificity.Ontheother
drivenmethodstoexplorehowLVLMsdistinguishuniqueattributes hand,elaboratetextualdescriptionsenhancecomprehensionoftheExploringtheDistinctivenessandFidelityoftheDescriptionsGeneratedbyLargeVision-LanguageModels ACMMM,28October-1November,2024,Melbourne,Australia
visualcueswithinthetext,benefitingtasksliketext-to-imagegen- considerablecosts.Consequently,focusespredominantlyoneval-
eration.Thegenerationofthesedetaileddescriptions,pioneered uatingthefine-grainedvisualdescriptionsgeneratedbyLVLMs,
bytheseworks[25,26]usingLargeLanguageModels(LLMs)(e.g., circumventingtheneedforGPT-4andreferencetoground-truth
GPT-3,GPT-4)hascontributedtotheimprovementofzero-shot texts.Tothisend,weleveragepubliclyaccessiblemodels,namely
classificationofCLIPmodel.Nonetheless,despitetheirdetailed CLIP[30]andStableDiffusion[31],toassessthegeneratedvisual
nature,thesedescriptionsmaystillproduceinaccuraciesdueto descriptionsfordistinctivenessandfidelity.
theabsenceofdirectvisualreferences,leadingtodescriptionsthat
arenotvisuallycongruentwiththeactualfeaturesoftheobjects 3 METHOD
depicted.TheemergenceofLargeandVisionLanguageModels
ThissectionoutlinesasystematicapproachforassessingLarge
(LVLMs)(e.g.,Open-Flamingo[4],MiniGPT-4[43],IDEFICS[20])of-
Vision-LanguageModels(LVLMs)intermsoftheirabilitytopro-
fersthepotentialforgeneratingmoreextendedanddetailedtextby
ducedetailedvisualdescriptions.InSection3.1,theprocedurefor
leveragingawealthofexternalvisualandlinguisticknowledge.A
generatingfine-grainedvisualdescriptions(FGVDs)viaLVLMsis
recentstudy[12]hasexploitedthispotential,usingLVLMstogen-
elaborated.Followingthis,adual-strategyframeworkisintroduced
eratefine-grainedvisualdescriptionsforzero-shotwildlifeanimal
forevaluatingthequalityofgeneratedcontents,focusingondistinc-
speciesrecognition.Similarly,ourworkemploysLVLMs,prompted
tiveness(Section3.2.1)andfidelity(Section3.2.2).Fig.2illustrates
withbothvisualandlinguisticcues(i.e.,craftedquestions),topro-
thisevaluativeframework,facilitatinganin-depthexplorationof
ducerichanddetailedvisualdescriptionsatamoregranularlevel.
thequalityofFGVDsproducedbyLVLMs.
Diverging from prior efforts, our work aims to steer LVLMs to
explorevariousperspectivesofvisualcontentviavariouscrafted 3.1 Fine-GrainedVisualDescriptionGeneration
questions,whichisvitalforthesubsequentanalysisandevaluation
Thefine-grainedvisualdescriptionsareproducedbytheLVLMs
ofthesegenerateddescriptions.
(e.g.,Open-Flamingo[4])conditionedonbothvisualandtextual
cues.Contrarytothetaskofcaptiongeneration,whichproduces
2.2 VisionLanguageModels(VLMs) simpleandbroaddescriptions,ourobjectiverequiresthegeneration
ofmorerefinedandnuancedtextualdescriptions.Thisdemandsthe
VisionLanguageModels(VLMs)havebecomeacentralfocuswithin
craftingofspecificpromptingquestionsdesignedtoelicitdetailed
artificialintelligenceresearch.CLIP[30]marksapivotaldevelop-
descriptionsfromthemodels.WithtextualpromptPandimage
mentbyutilizingacontrastivemethodandpre-trainingon400
I,theprocessoffine-grainedvisualdescriptiongenerationcanbe
millionimage-captionpairstoeffectivelysynchronizevisionand
formulatedasfollows:
languagedomains.Paralleladvancements,suchasClipCap[27]
andBLIP[22],haveimprovedthecreationofvisuallycoherentcap- ùëë =ùëì M(I,P) (1)
tions.TheemergenceofLargeLanguageModelslikeGPT-3[5]and
LLaMA[33]hasfacilitatedthedevelopmentofadvancedLVLMs, where ùëì M(¬∑) denotestheforwardprocessofLVLMM andùëë =
whichharnessvastLLMknowledgetodeeplyunderstandimages
{ùë§ÀÜ1,...,ùë§ÀÜùë°}representsthesequenceofgeneratedwords,decoded
throughsufficientalignmentacrossmodalities,employingdiverse auto-regressively.Forzero-shotgeneration,Prepresentsametic-
multimodaldatasetsfortraining.ModelslikeOpen-Flamingo[4] ulouslycraftedpromptingquestionQaimedatelicitingeithera
andIDEFICS[20]achievethisthroughperceivermodulesandcross- salientorglobaldescription.Forthein-contextlearningsetting,I
attentiontechniques,whereasMiniGPT-4[43]integratesaprojec- includesonequeryimageandasetofdemonstratedimages.The
tionlayerandQ-formerwithLLM,trainingonaccessibledatasets textualpromptsParecomposedofafewinterleaveddemonstrated
likeLAION.
promptingquestionsùëÑanddescriptionsùëë.
3.2 Dual-Evaluation
2.3 EvaluationofVision-LanguageModels
3.2.1 Distinctiveness. ToevaluatetheproficiencyofLVLMsin
(VLMs)
differentiatingamongvariouscategories,weimplementaretrieval-
ThetraditionalevaluationofVision-LanguageModels(VLMs)pri- basedapproachnamedTRAC.Itinvolvesutilizingthetraining
marilyfocusesontheirabilitytounderstandandinterpretvisual setofafine-graineddatasettoconstructasupportingsetoffine-
contentwithinimages.Forinstance,theImageCaptioning(IC)task, grainedvisualdescriptions(FGVDs).Thesedescriptions,generated
whichassesseshowaccuratelyanimagecanbetranslatedintotext, byLVLMsforapredeterminedsetofimages,areprocessedthrough
typicallyemploysmetricssuchasBLEU-4[28]andCIDEr[34]for atextencodertoextractembeddings,whicharethenstoredfor
measurement.However,withtheincreasingscaleofVLMs,the subsequentretrievaltasks.
scopeoftheirevaluationhasexpandedtobecomemorecomprehen- TRACProcess.Letùê∑ = {ùëë 1,ùëë 2,...,ùëë ùëõ}denotethesetofFGVD
siveandrobust.Recentstudies[24,39,41]havecontributedtothe generatedbytheLVLMs,whereùëõisthenumberofimagesinthe
developmentofbenchmarksforevaluatingVLMs,coveringaspects supportingset.EachFGVDùëë ùëñ correspondstoanimageIùëñ andis
ofvisualperception,understanding,andreasoning.However,quan- associatedwithalabelùë¶ ùëñ.Theembeddingsextractedfromthese
tifyingtheevaluationcriteriaforVLMsintheabsenceofreference FGVDarerepresentedasE ={e1,e2,...,eùëõ},whereeùëñ istheem-
textsoranswersremainsasignificantchallenge.Althoughboth beddingofFGVDùëë ùëñ.
automatedevaluationusingmodelslikeGPT-4andmanualeval- Forthetestingsetofthedataset,eachimageI testissimilarlypro-
uationbyhumansaretheoreticallyviableapproaches,theyentail cessedtoobtainatextualdescriptionùëë test,whichisthenencodedACMMM,28October-1November,2024,Melbourne,Australia YuhangHuang,ZihanWu,ChongyangGao,JiaweiPeng,andXuYang
(a) FGVD Generation RReeccoonnssttrruucctteedd
FFGGVVDD
TTeesstt Diffusion IImmaaggee
TThhee iimmaaggee sshhoowwss aa rreedd
IImmaaggee
bbiirrdd ssiittttiinngg oonn aa ttrreeee
bbrraanncchh.. TThhee bbiirrdd hhaass aa
LLVVLLMM bbrriigghhtt rreedd bbooddyy wwiitthh aa
yyeellllooww ppooiinnttyy bbeeaakk..
WWhhaatt aarree tthhee mmaaiinn TThhee ttaaiill aappppeeaarrss ttoo hhaavvee
vviissuuaall ffeeaattuurreess ffoorr bbllaacckk ssppoottss..
tthhee iinn tthhiiss iimmaaggee??
TTeexxttuuaall RReettrriieevvaall
TTTeeexxxttt --AAuuggmmeenntteedd CCllaassssiiffiiccaattiioonn
EEEnnncccooodddeeerrr SSiimmiillaarriittyy--
BBaasseedd RReettrriieevvaall
IIImmmaaagggeee IImmaaggee
VVeeccttoorriizzeedd FFVVGGDD Top-k Retrieved
SS TTuu aamm nnaamm ggeeee rrrr DDaattaabbaassee FGVDs EEEnnncccooodddeeerrr EEnnccooddeerr
VVoottee SSuummmmeerr
11.. TTaannaaggeerr TTThhhiiisss bbbiiirrrddd hhhaaasss aaa rrreeeddd bbbeeeaaakkk‚Ä¶‚Ä¶‚Ä¶
SS TTuu aamm nnaamm ggeeee rrrr 22..SS TTuu aamm nnaamm ggeeee rrrr TTThhheee iiimmmaaagggeee ssshhhooowwwsss aaa rrreeeddd bbbiiirrrddd pppeeerrrccchhheeeddd‚Ä¶‚Ä¶‚Ä¶
‚àö‚àöAAVVGG
33.. CCaarrddiinnaall TTThhheee iiimmmaaagggeee ssshhhooowwwsss aaa cccaaarrrdddiiinnnaaalll sssiiittttttiiinnnggg‚Ä¶‚Ä¶‚Ä¶
Dot Product
SSuummmmeerr ...... 26.8% 80.1%
TTaannaaggeerr FFFGGGVVVDDD CCClllaaassssss AAAvvveeerrraaagggeee
‚àö‚àöTToopp--11 EEEmmmbbbeeeddddddiiinnnggg EEEmmmbbbeeeddddddiiinnnggg ((bb)) DDiissttiinnccttiivveenneessss (c) Fidelity
Figure2:Anoverviewofourframeworkforevaluatingthequalityoffine-grainedvisualdescriptions(FGVDs)generatedby
LargeVision-LanguageModels(LVLMs).IntheFGVDGenerationphase(a),FGVDsareproducedbyconditioningonbothvisual
andlinguisticcues.Subsequently,weevaluatethequalityofgeneratedcontentintermsofitsdistinctiveness(b)andfidelity(c).
intoanembeddinge.Thisembeddingisusedtoretrievethemost thisapproachaccentuatesinter-classdifferences,sharpeningthefo-
similarembeddingseÀÜfromEbyleveragingcosinesimilarity.The cusondistinguishingbetweencategoriestoenhanceclassification
labelùë¶ RassociatedwiththemostsimilarembeddingeÀÜisthencom- precision.
paredwiththeoriginallabelùë¶ofI test.Theoveralldistinctiveness TOP-KVoting.ThismethodselectstheTOP-Kclosestcandidates
ofthegeneratedFGVDisquantifiedbycalculatingtheaccuracy ratherthanTOP-1basedonembeddingsimilarityanddetermines
acrossthetestingset,definedastheproportionofinstanceswhere thefinalcategorybasedonamajorityvoteamongthesecandidates.
ùë¶ÀÜ=ùë¶: This approach offers the advantage of mitigating the impact of
outliersandensuresamorerobustanddemocraticdetermination
1 ‚àëÔ∏Å
Accuracy= 1(ùë¶ÀÜ=ùë¶) (2) ofthemostrepresentativecategoryforeachfine-grainedvisual
|D |
test I ‚ààD description.Additionally,thismethodaimstoenhancethediscrim-
test test
inationofconditionswithineachclass,ensuringfinerresolutionof
whereùëá representsthetestset,and1istheindicatorfunction, intra-classvariations.
equalto1whenùë¶ÀÜ=ùë¶and0otherwise.Thisaccuracymetricserves
as an indirect measure of the capability of LVLMs to generate 3.2.2 Fidelity. Inadditiontoevaluatingthedistinctivenessofgen-
FGVDsthataredistinctenoughtoallowforaccuratecategorization eratedFine-GrainedVisualDescriptions(FGVDs),itisimperative
andretrievalbasedoncontentsimilarity. toassesstheirfidelity,i.e.,theextenttowhichthesedescriptions
Wealsoexploredseveralapproachestoassessthediversityofdis- visuallyalignwiththeoriginalimages.Relyingsolelyondistinc-
tinctiveness,initiallyconcentratingonidentifyingthemostsimilar tivenessmayleadtodescriptionsthat,whileunique,incorporate
description,referredtoasTOP-1,aspreviouslymentioned.Subse- externalandsometimesirrelevantinformation,therebydeviating
quently,wedesignedtwoadditionalretrievalmethodstoexpand significantlyfromtheactualvisualcontentofthesourceimages.
ouranalysis: Ontheotherhand,FGVDsserveasconciserepresentationsofvi-
AverageEmbedding.ForeachcategoryinthedatasetD,wecal- sualcontentofanimage,which,composedofthousandsofpixels,
culate the average embedding of its diverse fine-grained visual containsmuchredundantinformation.Textualdescriptionsoffera
descriptions.Thisprocessinvolvesaveragingtheembeddingsfor meanstocompressthisvisualinformationefficiently.Determining
alldescriptionswithinacategory,denotedase¬Øùëê.Theembeddingof thedegreetowhichthesedescriptionspreservetheoriginalvisual
atestdescriptionisthencomparedagainste¬Øùëê toascertaintheclos- informationisessentialforensuringthequalityofthetextualrep-
estmatch.Thismethodologycapitalizesonthecentraltendencyof resentations.Tothisend,weemploytwostrategiestoinvestigate
embeddingswithineachcategory,reducingvariancecausedbyindi- thisfidelity:
vidualdescriptionoutliersandofferingacondensedrepresentation Image-TextFidelity.DrawingonmethodsakintoCLIP-score[14],
ofthesemanticspaceassociatedwitheachcategory.Additionally, weusetheembedding-similarityùë†(I,ùëë)tomeasurethealignment
‚àö‚àöExploringtheDistinctivenessandFidelityoftheDescriptionsGeneratedbyLargeVision-LanguageModels ACMMM,28October-1November,2024,Melbourne,Australia
betweentheinputimageIandthegeneratedfine-grainedvisual RTX 3090 GPU with BF16 acceleration for all tests. For feature
description(FGVD)ùëë. extractionfromfine-grainedvisualdescriptions(FGVDs),wechose
Image-ReconstructedImageFidelity.Fine-grainedvisualde- thetextencodertrainedbyCLIP,comparingitagainstalternatives
scriptions(FGVDs)servenotonlyascompactrepresentationsof likeSentence-BERTandtheTF-IDFtechniqueusedinretrieval
imagesbutalsoaseffectivecross-modalinterfacerepresentations frameworks,asdetailedintheAppendix.Additionally,ourfidelity
formultimodaltasks[37].Hence,weleverageStableDiffusion[31] evaluations used the Stable Diffusion Model version 1.4 [? ] to
tomanifestthesemanticinformationhiddeninthecorresponding convertLVLM-generatedFGVDsbackintoimages.
FGVD.Thetransformationisrepresentedas:
IÀú =Diffusion(I) (3) 4.2 DistinctivenessEvaluation
Subsequently,wemeasurethesimilarityordistanceùë†(I,IÀú) be- In this section, we present the experimental results for the dis-
tweentheoriginimageIandthereconstructedimageIÀú,toquan- tinctiveness aspect of our evaluation framework. We utilized a
zero-shotapproachandusedthefixedpromptformatforinput
tifythepreservationofvisualcontentinformationviathecompres-
intoeachLargeVision-LanguageModel,focusingonlocalvisual
sionofFGVD.
featurestoemphasizethedetailedaspectsofanimage.Theprompt
templatewasstructuredasfollows:
4 EXPERIMENTS
"Whatarethemainvisualfeaturesfor{Category}inthisimage?"
Thissectionoutlinesourstudyimplementationsandexperiments.
Here,weprovidedonlygeneralcategoriestotheprompt,suchas
InSection4.1,wedescribethedatasetsused,includingfine-grained
"bird"forCUB200and"dog"forStanfordDogs,withoutofferingany
classificationsetsandImageNet1k,toevaluateLVLMperformance
specificdemonstrations.Thiszero-shotsettingallowstheLVLMto
acrossvarieddetaillevels.WeexaminedistinctivenessinSection4.2
focusmoreonlocalizedratherthanglobalfeaturesoftheimages.
andfidelityinSection4.3,culminatinginacomprehensiveanalysis
Open-FlamingoOffersSuperiorDistinctivenessButAddsEx-
ofhallucinationissuesinSection5.1.Here,weassessthecapabilities
traKnowledge;MiniGPT-4RemainsStable.AsshowninFig.
ofLVLMsforgeneratingdetailedtextdescriptions,highlighting
3,consistenttrendswereobservedacrossthethreemethodsused
differencesinquality,andofferinginsightsintopotentialsolutions
toevaluatemodeldistinctiveness,withtheOpen-Flamingomodel
forreducinghallucinations.
exhibitingsuperiorperformanceontheaggregatedfivedatasets.
MiniGPT-4displayedmoderateperformance,whereasIDEFICSun-
4.1 DatasetsandImplementationDetails
derperformed.Adetailedanalysisofthegeneratedresultsrevealed
Datasets.Toassessthequalityoffine-grainedvisualdescriptions thatMiniGPT-4andIDEFICStendtoproducewell-structuredfor-
(FGVDs),ourstudyencompassesaseriesofcomprehensiveexperi- mats,withdescriptionscloselymatchingtheoriginalimagesacross
mentsacrossfivedistinctimageclassificationdatasets,whichspan mostdatasets.However,Open-Flamingooftendeviatedtowards
bothfine-grainedandmoregeneralobjectcategories.Weselected directlygeneratinglabelspresentintheimages,avoidingvisual
CUB-200[38],StanfordDogs[17],StanfordCars[18],andOxford descriptions,leadingthelargelanguagemodel(LLM)tointroduce
102Flowersforfine-graineddatasets.Forthegenericcategory,we additional,category-relatedinformationnotstrictlyfaithfultothe
utilizedaspecificallycuratedsubsetofImageNet1k[11],compris- originalimages.Thistendency,whichenhancesdistinctiveness,
ing10imagespercategoryinthetrainingsetand5percategoryin especiallyintheStanfordDogsdataset,isfurtherdetailedintheAp-
thetestingset.Moredetailedinformationaboutthedatasetwillbe pendix.Incontrast,MiniGPT-4adeptlyavoidedthisissue,striving
thoroughlypresentedintheAppendix.Thechosendatasets,such todescribetheactualcontentdepictedintheimages.Wespeculate
asCUB200andtheStanford_Dogsdataset,areselectedfortheirem- thisisduetoitstrainingphaseseffectivelyaligningtextualand
phasisonfine-graineddistinctionsamongcloselyrelatedcategories, visualfeatures.Thisobservationunderscorestheneedtoensurea
requiringhighvisualspecificityandintricacyinmodelpredictions. betteralignmentandemphasizetheimportanceofincorporating
This makes them ideal for assessing the quality of descriptions fidelityinourevaluation.
generatedbyLargeVision-LanguageModels(LVLMs),presenting LVLMsVaryinDataset-SpecificPerformance.Allthreemodels
asignificantchallengeindifferentiatingnuancedcategories. exhibitedpoorperformanceinfine-graineddifferentiationwithin
Itisworthnotingthatforalltheselecteddatasets,weapplya theCUB200dataset,andtheyalsostruggledwiththeStanfordDogs
train/testsplit.Concretely,thetrainingsetisemployedasasup- datasetinIDEFICSandMiniGPT-4.Conversely,thethreeLVLMs
portsetforretrievingFGVDs,andthetestingsetisreservedfor performedcommendablyontheStanfordCarsdataset.Incontrast,
evaluation. animalslikebirdsanddogshavehigherintra-speciessimilarityand
ImplementationDetails.Inourexperiments,weutilizedthree subtlevariationsinfeatures.Specifically,thechallengewithanimals
LargeVision-LanguageModels:Open-Flamingo[4]withtheViT- couldstemfromtheirgreatervariationinposes,environments,and
L/14visionencoder,IDEFICS[20],andMiniGPT-4[43].Toensure overlappingfeatures,complicatingthefine-grainedclassification
consistencyacrossmodels,parameterswerestandardized,setting taskforLVLMs.Wefoundthattheprimarydescriptivefeaturesfor
beamsizeto3andthelengthpenaltyto1.0.Toexploretheim- birdsfocusonaspectssuchasfeathers,beaks,andbellies,while
pactoftextlengthonperformance,maximumgenerationlength cars,distinguishedbylogosandeasilyidentifiablefeaturesofthe
wastreatedasavariable,investigatingitseffectonthequalityof carbodysuchasthehood,windows,andtires,oftenrequireonly
generateddescriptions.Notably,ourexperimentalsetuprequired brandrecognition,bypassingtheneedtoidentifyspecificmodel
noadditionaltrainingorfine-tuning,utilizingasingleNVIDIA years(e.g.,"AcuraTLsedan2012",LVLMscanclassifycorrectlyonlyACMMM,28October-1November,2024,Melbourne,Australia YuhangHuang,ZihanWu,ChongyangGao,JiaweiPeng,andXuYang
Top-1 Average Top-k IDEFICS
Flowers102 Flowers102 Flowers102
MiniGPT-4
OpenFlamingo
Stanford Cars Stanford Cars Stanford Cars
40 40 40
30 30 30
20 20 20
10 10 10
0 0 0
CUB200 CUB200 CUB200
Stanford Dogs Stanford Dogs Stanford Dogs
ImageNet Subset ImageNet Subset ImageNet Subset
Figure3:ResultsofLVLMsunderdifferentdistinctnessmethodsonfivedatasets.
bydescribingitasAcuraTL).Theaveragemethoddemonstrated Table1:Image-textfidelityresultsacrossfivedatasetsutiliz-
improvementsforallmodelsontheImageNetdataset,suggesting ingtheCLIP-Smetric(CLIPembeddingsimilarity).
thatthismethodmoreeffectivelycaptureslargecategorydiffer-
ences in datasets that are not fine-grained. Also, the trends we Dataset IDE MiniGPT4 OF
observedacrossthefivedatasetsaligncloselywiththeclassifica-
CUB200 24.06 26.63 24.81
tionscoresproducedbyCLIP[7,30]onthesamedatasets.This
Flowers102 25.13 27.72 25.31
parallelismunderscorestheefficacyofourapproach,whichsolely
StanfordCars 25.16 26.51 25.22
reliesontextualinputforclassification.
StanfordDogs 22.58 27.27 25.20
Top-kExposesPoorCategorizationDistinctivenessinLVLMs
ImageNetSubset 25.12 27.80 25.78
Forthetop-kmethod,distinctivenesswasassessedatvariousval-
uesofùëò,whereùëò ‚àà (3,ùëò ùëöùëéùë•).ùëò ùëöùëéùë• wassetbasedontheapproxi-
matenumberofimagesperclasswithineachtestdataset.(e.g.,the
CUB200testingsetaverages30imagesperclass,soùëòrangedfrom3 termsofdistinctiveness.Specifically,fortheSTTRapproach,we
to30)Resultsindicate,asshowninFig.4,thatdifferentiationtends generatedatextualcaseusingazero-shotframeworkandtheniden-
toincreaseandthendecreaseasùëòapproachesùëò ùëöùëéùë•,highlighting tifiedsimilardescriptionswithinthesupportingset.Weconstruct
deficienciesinmodelperformanceinaccuratelyclassifyingfine- In-ContextExamples(ICEs)basedonthesesimilarities,whichassist
grainedcategories.Importantly,optimaldifferentiationwasnot themodelingeneratingfine-grainedvisualdescriptions,otherde-
achievedatùëò ùëöùëéùë•,furtherdisplayinglimitationsindistinguishing tailedselectionmethodswillbedemonstratedintheAppendix.As
betweencloselyrelatedcategoriesamongLargeVisionLanguage showninTable3,theuseofIn-ContextLearning(ICL)forauxiliary
Models. generationinmethodssuchasRS,SIIR,andSTTRresultedinpoorer
outcomes.Ouranalysissuggeststhatthisdeclineinperformance
CUB200 Stanford Dogs isattributabletotheuseofmodel-generated,fine-grainedvisual
22.0 I OD pE eF nIC FlS amingo 54 I OD pE eF nIC FlS amingo descriptions that were of low quality. Employing ICL with sub-
21.5 MiniGPT-4 MiniGPT-4
53 pardescriptionsinitiatesadetrimentalcyclethatdegradesoverall
21.0 52 results.Toaddressthis,weincorporatedGPT-4[1]toproducehigh-
20.5
20.0 51 qualityFGVDs,whichsignificantlyimprovedresultsacrossboth
19.5 50 datasetswhenassistinginLVLMgeneration.Ourresultsdemon-
19.0 49 stratethatemployinghigh-quality,long-form,fine-grainedvisual
18.5 48 descriptionsnotonlyboostsperformancebutalsomitigatesissues
3 30 3 40
ofmodelhallucination,furtherdiscussedinSection5.1.
Figure4:Thedistinctivenessresultsatdifferentùëò-values
4.3 FidelityEvaluation
DescriptionsGeneratedbyIn-ContextLearning.Inourexper- Thissectionfocusesonassessingthefidelityoffine-grainedvisual
iments,wealsoexploredseveralconfigurationstoenhancethedis- descriptionsproducedbyLargeVision-LanguageModels.Tothis
tinctivenessoffine-grainedvisualdescriptions(FGVDs),leveraging end,LVLMsarepromptedwithasinglequestionwithoutdemon-
thecapabilitiesofIn-ContextLearning(ICL)ratherthansolelyrely- strations,requiringacomprehensiveunderstandingofthevisual
ingonazero-shotapproach.WeutilizedmethodssuchasRandom contentoftheimage.Thisforcesthemodelstoarticulatevisual
Sampling(RS),Similarity-basedImage-ImageRetrieval(SIIR)[40], informationinatextualformat.Theuniformpromptusedis
andSimilarity-basedText-TextRetrieval(STTR),alongwithhigh- "What are the main elements in this image, and how do they
qualitytextgenerationfromGPT-4,toexaminetheoutcomesin interactorrelatetoeachother?".
)%(
ssenevitcnitsiD
)%(
ssenevitcnitsiDExploringtheDistinctivenessandFidelityoftheDescriptionsGeneratedbyLargeVision-LanguageModels ACMMM,28October-1November,2024,Melbourne,Australia
Table3:UsingdifferentconfigurationsinIn-ContextLearn-
Models IDEFICS MiniGPT-4 OpenFlamingo
ingtohelpLVLMsgenerate.(l30meanslengthof30)
Average Score 2.84 3.77 3.18
CUB200 StanfordDogs
MethodsAndDatasets
0-shot 1-shot 2-shot 3-shot 4-shot 0-shot 1-shot 2-shot 3-shot 4-shot
RS(l30) 21.49 8.78 8.54 9.51 9.22 12.33 7.27 7.87 8.61 9.03
RS(l70) 16.98 10.11 12.08 12.5 10.96 10.41 6.28 8.29 9.36 9.79
SIIR(l30) 21.49 6.71 7.68 9.18 7.8 12.33 6.08 5.78 5.93 6.32
SIIR(l70) 16.98 6.17 8.7 8.66 8.77 10.41 5.22 4.9 5.16 5.54
STTR(l30) 21.49 6.45 7.56 8.03 8.20 12.33 5.05 6.85 6.99 7.07
STTR(l70) 16.98 7.65 9.20 9.22 9.29 10.41 5.02 5.87 6.47 7.02
GPT-4Generated
Description 16.98 18.21 19.00 19.02 22.87 10.41 16.36 16.71 17.96 19.64
imagesforathoroughanalysis.Toassessfidelity,weemployedmet-
ricssuchasStructuralSimilarityIndex(SSIM)[36]andCLIPEmbed-
Figure5:Humanevaluationresultsassessingthefidelityof dingSimilarity(CLIP-S-I),comparingtheoriginalandreconstructed
descriptionsgeneratedbyLVLMs.Top:Averagescoresacross images.Additionally,theFr√©chetInceptionDistance(FID)[15]was
variousmodels.Bottom:Scoredistributionfrom1to5for usedtoevaluatethedistributionaldistancebetweentheoriginal
eachmodel. andreconstructedimages,offeringinsightsintohoweffectively
thetext,servingasanintermediary,preservesvisualinformation.
Table2reaffirmedthesuperiorperformanceofMiniGPT-4inthis
Table2:Experimentalresultsforimage-reconstructedimage
complextask.Throughouttheprocessofimagereconstruction,text
fidelityacrossfivedatasetsutilizingthreemetrics(i.e.,SSIM
actsasacrucialintermediary,adeptlypreservinginformationthat
‚Üë,FID‚Üì,CLIP-S-I(CLIPembeddingsimilarity)‚Üë).
remainsfaithfultotheoriginalimage.However,anydiscrepancies
orelementsunfaithfultotheoriginalweresignificantlyexacerbated
Dataset SSIM‚Üë FID‚Üì CLIP-S-I‚Üë inthereconstructedimages.Thisnotonlyaugmentedourevalua-
IDE MiniGPT4 OF IDE MiniGPT4 OF IDE MiniGPT4 OF
tionbutalsoilluminatedthelimitationsandcapabilitiesofLVLMs.
CUB200 28.25 29.52 28.69 90.98 71.28 93.37 74.83 75.98 73.64
Flowers102 18.39 20.37 19.33 255.57 215.62 318.54 75.49 79.15 76.30 AsshowninFig.6,theFGVDsregeneratedbyOpen-Flamingoand
StanfordCars 16.22 17.38 16.80 63.69 49.61 77.38 73.35 75.87 73.69
StanfordDogs 16.17 17.16 16.96 329.75 218.29 252.75 67.87 73.45 71.55 IDEFICSexhibitednotabledissimilaritiestotheoriginalimages,
ImageNetSubset 17.03 17.55 17.47 78.20 66.84 64.14 71.33 73.54 71.67 highlightingthechallengesofproducingfaithfulvisualrepresenta-
tions.
MiniGPT-4ExhibitsHigherFidelity.Webeganouranalysisby 5 QUALITATIVERESULTS
utilizingtheCLIPEmbeddingSimilarity(CLIP-S)metric,which
5.1 HallucinationAnalysis
measuresthesemanticsimilaritybetweenthegeneratedFGVDs
(Fine-Grained Visual Descriptions) and the original images. As Afterthoroughlycheckingthequalityofthetextsgeneratedbythe
indicatedinTable1,FGVDsproducedbyMiniGPT-4exhibithigher LVLM,wefoundthatallthreemodelshavemoreorlessillusory
fidelity compared to those generated by the other two models. problems.Thedescriptionswillbeinconsistentwiththeoriginal
Despite the limitations imposed by modal discrepancies within images,whichisoneofthereasonswhyweintroducedFidelity,to
theCLIPembeddingspace,asnotedinpreviousstudies[23],the seewhethertheLVLMscanbediscriminativeandatthesametime
descriptionsgeneratedbyMiniGPT-4consistentlyachievedthe havefidelityfromanotherperspective.
highestscoresacrossmultipledatasets. LVLMsExhibitsNon-RelevantInformation.Inourdistinctive-
Tocomplementourquantitativefindingsandenhancethecom- nessexperiment,weobservedthatMiniGPT-4consistentlypro-
prehensivenessofourfidelityassessment,weintegratedhuman ducesaccuratedescriptionsalignedwiththeimages.Incontrast,
evaluationintoourexperiment.Thismethodassessesthealignment IDEFICSandOpen-Flamingooftengeneratetextthat,whileclear,
betweenthegeneratedFGVDsandtheircorrespondingoriginal isunrelatedtotheimagecontent.Theytendtoprematurelyspecify
images,assigningscoresfrom1to5todenoteincreasinglevels thelabelfirstatthebeginningofsentences,leadingtocontentthat
offidelity.Theevaluationalsoconsiderspotentialissuessuchas doesnotalignwiththevisualinformation.Thisapproachresultsin
hallucinationsandtheintroductionofexternalknowledge.Forthis subsequentdescriptionsbecomingcompletelydisordered.Forex-
purpose,werandomlyselected20FGVDsacrossfivedatasets.The ample,thedescriptioncorrespondingtothecategory"Otterhound"
resultsofthishumanevaluationaredisplayedinFigure5,where was"AiredaleTerrier.PhotoCredit:WikimediaCommons."This
MiniGPT-4achievedthehighestscores.Furthermore,thenumber wasparticularlyevidentaftermodifyingthepromptstoenhance
ofhigh-fidelityFGVDsproducedbyMiniGPT-4significantlysur- visualdetailrepresentationbyadding"Ithas"tothedescriptions,
passed those generated by the other models, demonstrating its whichhelpedsteertheLVLMstowardsmorevisuallyrepresentative
superiorperformanceinmaintainingtextual-visualalignment. outputs.AsillustratedinFig.7,thecomparativeperformancebefore
ImageReconstructionAmplifiesDiscrepanciesinLVLMOut- andafterthisguidancehighlightstherelativeweaknessofOpen-
puts.WefurtherscrutinizedthefidelityofLargeVisionLanguage Flamingoinprocessingvisualinformation,explainingitsinitial
Models(LVLMs)inpreservingvisualinformationduringgener- superiorityontheStanfordDogsdataset.Overall,MiniGPT-4pro-
ation. Utilizing Stable Diffusion, we transformed the generated ducesmorestableandhigher-qualitydescriptionsthanIDEFICSand
Fine-GrainedVisualDescriptions(FGVDs)backintoreconstructed Open-Flamingo,attributabletoitssimplifiedarchitecture,targetedACMMM,28October-1November,2024,Melbourne,Australia YuhangHuang,ZihanWu,ChongyangGao,JiaweiPeng,andXuYang
IDEFICS MiniGPT-4 OpenFlamingo
The image shows a large lizard
frI im llea dg -e nN ece kt eS du lb izs ae rt d T i e e as nn lh si v v oe n i i t rrm ae oo nra n na i em mcn lt e eeie n mnnl ge t t e . m i w nsT t e i h a tn i eh ntt r lii teits hz es eat , h r w pde i hs cl ii tcz uha rr e id .s. It o o b a t cr o fr r el o vo eai wg en itu rg nr n ea e dt ten a a h n.a ii e nl dT p ba h ge g n aer rr cdc e el k h eyi sz g ne hca rd a loor e rdo ul p ao n n vh r t d eaa eth ss t e ii e sto aa h n n t .r , du T w n hk i et h T t a ah nnh e de a l n lim az i ymaa r si ad n el. ge tI hl gte a sim t .s ie a sn r ct e oi pn lt d it -lh e b,e l o wi om h dia ecg dhe ii ss
branches.
AuS dt i a Sn 5f o Cr od u C pea r 2s 012 T A c b b s loha u ah 5 or icie n . l kki d iI s m g n lt i i np r gia kogs a i eou n r .a k nn i T te el d thul id h e sex a m e i r nu gn e cdre l oy af an tr w rrt hos ,e p ei in ms nt o t sr g t r ae uoh t .ke ns fe is nc a iA i s a gn u r i. td thT i e h e T w c w h s ata neh ri ir a det te h h d a h s i mlaa am his g als l a iha ral ng pe r t esl ge e o . ed k e s w dT , ,h g gwh,mo r ees i iw so ll t l .ho bds e p o e sa ai dr m n nns yg di o dl i ov sf se r te ws so hr i ln e g e cc etn e ua p ke.r r in T vn ad egh n s e d T t s w S M s th hop ih h ee lo e ve t i tr hs sc aetm i th es r l ul s v a cc i di se asi a a i n r r orr aa e . A le n r lsT s e il ou ge he fh ldm hl iae ee ni t c me sm ycA t .n a se ae5t r n tnt .i hai t n dsI elo t l t s r if lh i c ei i s l gce f v e la o hei e l lm e tcl r o u m, ft ra i rx . vg oe u ene mr , ty i . s
HHaarrCC rriiUU ss BB SSpp22 aa00 rr00 rrooww T T s r i T r pn uen ah hh l bo sae ee t e sw ht ese ebm c rd Ai a ier b na n t nd maoi ed sn c e )i bt ks t rh ihe g i re c iil de r s ne aoo . m t ng ute ah r nre o ea dn Pruc m t it n ob pi edi fn i is r dt.g d t ih I ust w th e ( m t A e hii p -st a nh sh b t ita o zhi t alr t eh usd or doe e s. . T s i a p a s isnt in pih a d n sd pae n ek i e nd wf i wab gim i ren h e iisl nta g i a d htg t g ke. o oe i T on s a t b ss t rn h r e ah mdie c o p lp aob obw ea lol u li dts a ir kc t nd cb ha h i g k no h o s o gd oa em pf y us y o e da tea f nw .l r s f ,l y b . i t ab t r I ohg soi t r r tw a id a h f s n e is t T t b t i b T a oh h s re feh h ee e a cc te e l haab b i rs mm uu iei a o sr s sc pa ad ee t pki r oh. hn egi w reT t o tr te aeh oi s twsl on ue ue a o .ti b nm r b n tejd o oi ee sr tlc n id h ho ts t ht a eo i oo ei r rs kn p f u i e c a nt tt l of hh ef goo meo me fc a c pu pi f etam o os h nl i c s to taa p . iu st ng to os itde oi . h n nI ai ts t t
Figure6:Qualitativeexamplesoffine-grainedvisualdescriptions(FGVDs)generatedbythreeLargeVision-LanguageModels
(LVLMs),alongsidetheircorrespondingreconstructedimages.HallucinationsinthegeneratedFGVDsarehighlightedinredto
indicatediscrepanciesbetweenthegenerateddescriptionsandtheactualimagecontent.
thatOpen-Flamingoisinsensitivetotheselengthchanges,typi-
w/o Instruct
w Instruct IDEFICS OpenFlamingo callygeneratingshortertexts.Incontrast,IDEFICSandMiniGPT-4
Flowers102 Flowers102
aremorelikelytoproducelongertexts,atendencyweattribute
Stanford Cars Stanford Cars todifferencesinthetrainingdatasetsutilized[20].Importantly,
0
10203040
0
10203040 w neh si sle atbo thth emID aE xF imIC uS man led nM gti hni oG fP 7T 0- ,4 ths eho pw ere fd ord me ac nre ca es te rd end dist di in ffc et riv ede-
:
CUB200 CUB200
IDEFICSexhibitedaconsistentdeclineindistinctivenessastext
lengthincreased,whereasMiniGPT-4initiallyshowedanimprove-
Stanford Dogs Stanford Dogs mentindistinctiveness,whichpeakedbeforeeventuallydeclining
as lengths approached 70. The decrease in distinctiveness with
ImageNet Subset ImageNet Subset increasingtextlength,leadstoahigherlikelihoodofproducing
contentthatisirrelevanttotheimage,forexample,thegenerated
Figure7:Comparativeresultswithandwithoutinstructional descriptionoftencontains"Itislookingatthecamera","There
guidancefordescription. isabluesky"attheendofthedescription,however,thisisnot
accurateandirrelevantwiththevisualfeatures.Webelievethese
IDEFICS MiniGPT-4 OpenFlamingo resultsrevealalimitationofLVLMs:theirdiminishingabilityto
22
19.0 32 stayvisuallycoherentastextlengthens.Thisissuesuggestsaneed
20 18.5 30 fortrainingmethodsthatbettermaintaincontextoverlongerout-
18 28 18.0
16 17.5 22 46 puts.Wehypothesizethatrigorouspost-processingandmanual
14 17.0 22 verificationofthetrainingdataarecriticaltoachievingthis.
12 16.5 20
10 30 40 50 60 70 16.0 30 40 50 60 70 18 30 40 50 60 70 6 CONCLUSIONANDLIMITATIONS
Maximum Length Maximum Length Maximum Length
CUB200 Stanford Dogs
Inthispaper,wefocusedonexploringthedistinctivenessandfi-
Figure8:DistinctivenessofLVLMs-Generatedfine-grained delityoftextualdescriptionsgeneratedbyLargeVision-Language
textatdifferentlengths. Models(LVLMs)usingourproposedTRACmethodcoupledwith
variousanalyticaltechniques.OuranalysisrevealedthatMiniGPT-
4excelsingeneratingfine-graineddescriptions,markingthefirst
alignmentdataset,andrefinedtrainingapproach,whichcollectively evaluation of a distinctive and faithful generation of LVLMs in
sharpenaccuracyandreducehallucinatorycontent. thisdomain.Thisworkenrichesmultimodallanguagemodelre-
IncreasedLengthLeadstoLossofFocusandHallucinations searchandidentifiescriticalareasforimprovement,especiallyin
inLVLMs.Uponfurtheranalysis,weobservedanincreaseinthe addressing hallucination issues inherent to these models. How-
probabilityofnonsensicaloutputassentencelengthextended.We ever,relyingonexistingtextencodersforfeatureextractionmay
evaluatedthreemodels‚ÄîOpen-Flamingo,IDEFICS,andMiniGPT-4, limitourstudy,astheCLIPembeddings,alongwithothercurrent
usingmaximumlengthsrangingfrom30to70whilekeepingthe embeddingtechniques,havelimitedcapabilityindistinguishing
lengthpenaltyparameterconstant.OurfindingsinFig.8indicate fine-graineddetails,impactingtheevaluationscore.Additionally,
)%(
ssenevitcnitsiDExploringtheDistinctivenessandFidelityoftheDescriptionsGeneratedbyLargeVision-LanguageModels ACMMM,28October-1November,2024,Melbourne,Australia
ourrelianceonasingleStableDiffusionmodelmayalsolimitthe Kiela,etal.2024. Obelics:Anopenweb-scalefiltereddatasetofinterleaved
generalizationofourfindings;futurestudiescouldbenefitfromtest- image-textdocuments. AdvancesinNeuralInformationProcessingSystems36
(2024).
ingacrossvarioustypesofStableDiffusionmodelstovalidateand
[21] PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,
refineourresultsandweplantoenhanceourmethodsforbetter NamanGoyal,HeinrichK√ºttler,MikeLewis,Wen-tauYih,TimRockt√§schel,
granularityindistinctionandexplorenewclassificationstrategies etal.2020.Retrieval-augmentedgenerationforknowledge-intensivenlptasks.
AdvancesinNeuralInformationProcessingSystems33(2020),9459‚Äì9474.
forLVLMsusingourfindings. [22] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi.2022.Blip:Bootstrapping
language-imagepre-trainingforunifiedvision-languageunderstandingand
REFERENCES generation.InInternationalconferenceonmachinelearning.PMLR,12888‚Äì12900.
[23] VictorWeixinLiang,YuhuiZhang,YongchanKwon,SerenaYeung,andJamesY
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,Floren- Zou.2022.Mindthegap:Understandingthemodalitygapinmulti-modalcon-
ciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,Shyamal trastiverepresentationlearning. AdvancesinNeuralInformationProcessing
Anadkat,etal.2023. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774 Systems35(2022),17612‚Äì17625.
(2023). [24] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,Wangbo
[2] PeterAnderson,BasuraFernando,MarkJohnson,andStephenGould.2016. Zhao,YikeYuan,JiaqiWang,ConghuiHe,ZiweiLiu,etal.2023.Mmbench:Is
Spice:Semanticpropositionalimagecaptionevaluation.InComputerVision‚Äì yourmulti-modalmodelanall-aroundplayer?arXivpreprintarXiv:2307.06281
ECCV2016:14thEuropeanConference,Amsterdam,TheNetherlands,October11-14, (2023).
2016,Proceedings,PartV14.Springer,382‚Äì398. [25] MayugManiparambil,ChrisVorster,DerekMolloy,NoelMurphy,KevinMcGuin-
[3] PeterAnderson,XiaodongHe,ChrisBuehler,DamienTeney,MarkJohnson, ness,andNoelEO‚ÄôConnor.2023.Enhancingclipwithgpt-4:Harnessingvisual
StephenGould,andLeiZhang.2018. Bottom-upandtop-downattentionfor descriptionsasprompts.InProceedingsoftheIEEE/CVFInternationalConference
imagecaptioningandvisualquestionanswering.InProceedingsoftheIEEE onComputerVision.262‚Äì271.
conferenceoncomputervisionandpatternrecognition.6077‚Äì6086. [26] SachitMenonandCarlVondrick.2022.Visualclassificationviadescriptionfrom
[4] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,Wanrong largelanguagemodels.arXivpreprintarXiv:2210.07183(2022).
Zhu,KalyaniMarathe,YonatanBitton,SamirGadre,ShioriSagawa,JeniaJitsev, [27] RonMokady,AmirHertz,andAmitHBermano.2021.Clipcap:Clipprefixfor
SimonKornblith,PangWeiKoh,GabrielIlharco,MitchellWortsman,andLudwig imagecaptioning.arXivpreprintarXiv:2111.09734(2021).
Schmidt.2023.OpenFlamingo:AnOpen-SourceFrameworkforTrainingLarge [28] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. Bleu:a
AutoregressiveVision-LanguageModels. arXiv:2308.01390[cs.CV] methodforautomaticevaluationofmachinetranslation.InProceedingsofthe
[5] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, 40thannualmeetingoftheAssociationforComputationalLinguistics.311‚Äì318.
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda [29] BryanAPlummer,LiweiWang,ChrisMCervantes,JuanCCaicedo,JuliaHock-
Askell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneural enmaier,andSvetlanaLazebnik.2015.Flickr30kentities:Collectingregion-to-
informationprocessingsystems33(2020),1877‚Äì1901. phrasecorrespondencesforricherimage-to-sentencemodels.InProceedingsof
[6] Yan-ShuoChang.2018. Fine-grainedattentionforimagecaptiongeneration. theIEEEinternationalconferenceoncomputervision.2641‚Äì2649.
MultimediaToolsandApplications77,3(2018),2959‚Äì2971. [30] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,
[7] Haokun Chen, Xu Yang, Yuhang Huang, Zihan Wu, Jing Wang, and Xin SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,
Geng. 2023. Manipulating the Label Space for In-Context Classification. etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
arXiv:2312.00351[cs.CV] InInternationalconferenceonmachinelearning.PMLR,8748‚Äì8763.
[8] ShizheChen,QinJin,PengWang,andQiWu.2020.Sayasyouwish:Fine-grained [31] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rn
controlofimagecaptiongenerationwithabstractscenegraphs.InProceedingsof Ommer.2021.High-ResolutionImageSynthesiswithLatentDiffusionModels.
theIEEE/CVFconferenceoncomputervisionandpatternrecognition.9962‚Äì9971. arXiv:2112.10752[cs.CV]
[9] XinleiChen,HaoFang,Tsung-YiLin,RamakrishnaVedantam,SaurabhGupta,Pi- [32] MengyueShao,JieFeng,JieWu,HaixiangZhang,andYayuZheng.2023.Fine-
otrDoll√°r,andCLawrenceZitnick.2015.Microsoftcococaptions:Datacollection GrainedFeaturesforImageCaptioning.CMC-COMPUTERSMATERIALS&CON-
andevaluationserver.arXivpreprintarXiv:1504.00325(2015). TINUA75,3(2023),4697‚Äì4712.
[10] JaeminCho,SeunghyunYoon,AjinkyaKale,FranckDernoncourt,TrungBui, [33] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne
andMohitBansal.2022.Fine-grainedimagecaptioningwithclipreward.arXiv Lachaux,Timoth√©eLacroix,BaptisteRozi√®re,NamanGoyal,EricHambro,Faisal
preprintarXiv:2205.13115(2022). Azhar,etal.2023.Llama:Openandefficientfoundationlanguagemodels.arXiv
[11] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.2009.Imagenet: preprintarXiv:2302.13971(2023).
Alarge-scalehierarchicalimagedatabase.In2009IEEEconferenceoncomputer [34] RamakrishnaVedantam,CLawrenceZitnick,andDeviParikh.2015. Cider:
visionandpatternrecognition.Ieee,248‚Äì255. Consensus-basedimagedescriptionevaluation.InProceedingsoftheIEEEconfer-
[12] ZalanFabian,ZhongqiMiao,ChunyuanLi,YuanhanZhang,ZiweiLiu,Andr√©s enceoncomputervisionandpatternrecognition.4566‚Äì4575.
Hern√°ndez,Andr√©sMontes-Rojas,RafaelEscucha,LauraSiabatto,Andr√©sLink, [35] OriolVinyals,AlexanderToshev,SamyBengio,andDumitruErhan.2015.Show
etal.2023.Multimodalfoundationmodelsforzero-shotanimalspeciesrecogni- andtell:Aneuralimagecaptiongenerator.InProceedingsoftheIEEEconference
tionincameratrapimages.arXivpreprintarXiv:2311.01064(2023). oncomputervisionandpatternrecognition.3156‚Äì3164.
[13] SimaoHerdade,ArminKappeler,KofiBoakye,andJoaoSoares.2019. Image [36] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSimoncelli.2004.Image
captioning:Transformingobjectsintowords. Advancesinneuralinformation qualityassessment:fromerrorvisibilitytostructuralsimilarity.IEEEtransactions
processingsystems32(2019). onimageprocessing13,4(2004),600‚Äì612.
[14] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,andYejinChoi.2021. [37] ChenWei,ChenxiLiu,SiyuanQiao,ZhishuaiZhang,AlanYuille,andJiahuiYu.
Clipscore:Areference-freeevaluationmetricforimagecaptioning.arXivpreprint 2023. De-diffusionmakestextastrongcross-modalinterface. arXivpreprint
arXiv:2104.08718(2021). arXiv:2311.00618(2023).
[15] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,and [38] P.Welinder,S.Branson,T.Mita,C.Wah,F.Schroff,S.Belongie,andP.Perona.
SeppHochreiter.2017.Ganstrainedbyatwotime-scaleupdateruleconvergeto 2010.Caltech-UCSDBirds200.TechnicalReportCNS-TR-2010-001.California
alocalnashequilibrium.Advancesinneuralinformationprocessingsystems30 InstituteofTechnology.
(2017). [39] PengXu,WenqiShao,KaipengZhang,PengGao,ShuoLiu,MengLei,Fanqing
[16] QingbaoHuang,YuLiang,JielongWei,YiCai,HanyuLiang,Ho-fungLeung, Meng,SiyuanHuang,YuQiao,andPingLuo.2023.Lvlm-ehub:Acomprehen-
andQingLi.2021.Imagedifferencecaptioningwithinstance-levelfine-grained siveevaluationbenchmarkforlargevision-languagemodels. arXivpreprint
featurerepresentation.IEEEtransactionsonmultimedia24(2021),2004‚Äì2017. arXiv:2306.09265(2023).
[17] AdityaKhosla,NityanandaJayadevaprakash,BangpengYao,andFei-FeiLi.2011. [40] XuYang,YongliangWu,MingzhuoYang,HaokunChen,andXinGeng.2023.
Noveldatasetforfine-grainedimagecategorization:Stanforddogs.InProc.CVPR ExploringDiverseIn-ContextConfigurationsforImageCaptioning.InThirty-
workshoponfine-grainedvisualcategorization(FGVC),Vol.2.Citeseer. seventhConferenceonNeuralInformationProcessingSystems. https://openreview.
[18] JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei.2013.3dobjectrepre- net/forum?id=czwZnNf60r
sentationsforfine-grainedcategorization.InProceedingsoftheIEEEinternational [41] ShukangYin,ChaoyouFu,SiruiZhao,KeLi,XingSun,TongXu,andEnhong
conferenceoncomputervisionworkshops.554‚Äì561. Chen.2023. Asurveyonmultimodallargelanguagemodels. arXivpreprint
[19] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,Joshua arXiv:2306.13549(2023).
Kravitz,StephanieChen,YannisKalantidis,Li-JiaLi,DavidAShamma,etal. [42] JingyiZhang,JiaxingHuang,ShengJin,andShijianLu.2024.Vision-language
2017.Visualgenome:Connectinglanguageandvisionusingcrowdsourceddense modelsforvisiontasks:Asurvey. IEEETransactionsonPatternAnalysisand
imageannotations.Internationaljournalofcomputervision123(2017),32‚Äì73. MachineIntelligence(2024).
[20] HugoLauren√ßon,LucileSaulnier,L√©oTronchon,StasBekman,AmanpreetSingh,
AntonLozhkov,ThomasWang,SiddharthKaramcheti,AlexanderRush,DouweExploringtheDistinctivenessandFidelityoftheDescriptionsGeneratedbyLargeVision-LanguageModels ACMMM,28October-1November,2024,Melbourne,Australia
[43] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny.2023. "image_id": 6195
Minigpt-4:Enhancingvision-languageunderstandingwithadvancedlargelan- "class_id": 30
guagemodels.arXivpreprintarXiv:2304.10592(2023). "class_name": "Bernese mountain dog"
Strange Generation
A DATASETS
"Openflamingo": Bernese Mountain Dog. Photo Credit: Wikimedia Commons. Question: What is the
Toevaluatethedistinctivenessandfidelityoffine-graineddescrip- name of this dog breed? Answer: Poodle.
"Idefics": What is the dog's breed, and what is its name? What does it look like? Is it a male or a
tionsgeneratedbylargevision-languagemodels(LVLMs),wese- female? How old is it? Does it appear to be healthy? Why or why not? If it is not a purebred dog,"
lectedspecificfine-grainedimagedatasets.TheseincludeCUB-200,
"MiniGPT-4": The image shows a large, black and white dog lying on the ground in a grassy area. The
dog has a long, thick coat and floppy ears. Its eyes are brown and its tail is long and wagging. There
StanfordDogs,StanfordCars,andOxford102Flowers.Toassess are several leaves and acorns scattered around the dog
theperformanceofamoregenericobjectsdataset,wecurateda "image_id": 4699
"class_id": 98
subsetfromImageNet,consistingof10samplesperclassforthe
"class_name": "vizsla"
trainingsetand5samplesperclassforthetestingset.Detailed
Addtional Knowledge
descriptionsofthesedatasetsareprovidedinTable4.
"Openflamingo": Vizsla is a breed of medium-sized hunting dog that originated in Hungary. It is
named after the town of V\u00edzs\u00e9g, where it was bred. The breed is also known as the
Table4:DatasetInformation Hungarian Pointer, the Magyar Ag\u00e1r, or the Carpathian Pointer.
"Idefics": The dog is lying on the floor. It has a long snout. Its eyes are open and its tongue is hanging
out of its mouth. There is a blanket underneath the dog. The floor is carpeted and the walls are
painted a light blue color. In the background
Dataset Classes SupportSet TestSet "MiniGPT-4": The image shows a brown dog lying on the floor with its head resting on its paws. The
dog's eyes are closed and its tongue is hanging out of its mouth. Its ears are perked up and its tail is
CUB200 200 5994 5794 wagging back and forth. Its paws are on
Flowers102 102 4093 2463
"image_id": 2909
StanfordCars 196 8144 8041 "class_id": 88
StanfordDogs 120 12000 8580 "class_name": "Yorkshire terrier"
ImageNetSubset 1000 10000 5000 Hallucination
"Openflamingo": The dog is wearing a witch's hat and is holding a groom.
"Idefics": The dog is wearing a graduation cap and holding a diploma in its
mouth. It also has a bow tie on its collar and is sitting on a table. The table is
covered with a red and white checkered tablecloth, and there is a candle on it.
B COMPARISON "MiniGPT-4": The image shows a small dog wearing a graduation cap and
holding a diploma in its mouth. The dog is sitting on a black background with
Toavoidtheimpactofthedifferencesbetweendifferenttexten- its legs crossed and its tail wagging. The background appears to be a stage or
podium, with a microphone and a lectern
codersonourTRACmethod,wetestedthetextencoderofCLIPand
Sentence-BERT,andweevenutilizedtheTF-IDFstatisticmethodto
observetheresults.AsshowninTable5,aftercarefulconsideration,
Figure9:PoorgenerationofStanfordDogamong3LVLMs.
weusedtheembeddingsgeneratedbythetextencoderofCLIPfor
theexperimentsinthispaper.
rigorouslyevaluatethequalityoftheFGVDsgeneratedbyLVLMs,
C STANFORDDOGPOORGENERATION ensuringacomprehensiveandunbiasedassessmentoftheirper-
OuranalysisoftheStanfordDogsdatasetrevealedthatwhileLarge formanceincreatingvisuallycongruenttextualdescriptions.The
Vision-LanguageModels(LVLMs)generallyperformwellingener- criteriaforscoringareshowninTable6.
atingdistinctivedescriptions,especiallyinOpenFlamingo,certain
issuespersist.Detailedexaminationsindicatethatthemodeloc-
casionallyproducesstrangestatementsandincorporatesexternal
information,asillustratedinFig.9.Notably,OpenFlamingotends
togenerateshortdescriptions,whichmayartificiallyinflateits
performancescores.Incontrast,MiniGPT-4consistentlygenerates
descriptionsthatalignmorecloselywiththevisualcontentofthe
images,asdemonstratedthroughside-by-sidecomparisonswith
thecorrespondingimages.
D HUMANEVALUATIONFORTHEFIDELITY
Tomoreaccuratelyassessthefidelityoffine-grainedvisualdescrip-
tions(FGVDs)generatedbythreedifferentLargeVision-Language
Models(LVLMs),wehaveimplementedahumanevaluationproto-
col.Consideringthecomplexityinvolvedintheevaluationprocess,
werandomlyselected20setsofdatafromfivedatasetsusedin
ourstudyassubjectsforthishumanevaluation.Furthermore,we
developeda5-pointevaluationrubricthataccountsforboththe
presenceofhallucinatoryelementsandtheintroductionofexter-
nalknowledgewithinthedescriptions.ThisrubricisdesignedtoACMMM,28October-1November,2024,Melbourne,Australia YuhangHuang,ZihanWu,ChongyangGao,JiaweiPeng,andXuYang
Table5:Distinctivenessresultsobtainedusingdifferentmethodsormodelsforretrieval.
CLIP-Embeddings SENTENCE-BERT-Embeddings TF-IDF
CUB DOG CAR FLOWER IMAGENET CUB DOG CAR FLOWER IMAGENET CUB DOG CAR FLOWER IMAGENET
IDEFICS
21.49 12.33 52.61 34.63 33.66 22.07 13.00 51.89 28.83 34.90 19.59 12.03 49.02 31.91 27.46
CUB DOG CAR FLOWER IMAGENET CUB DOG CAR FLOWER IMAGENET CUB DOG CAR FLOWER IMAGENET
OpenFlamingo
27.20 32.65 45.60 50.83 46.34 17.47 31.48 48.25 46.62 41.30 15.27 29.63 47.82 46.49 39.50
CUB DOG CAR FLOWER IMAGENET CUB DOG CAR FLOWER IMAGENET CUB DOG CAR FLOWER IMAGENET
MiniGPT-4
19.16 17.11 47.00 52.78 39.94 21.83 14.87 42.46 49.21 41.36 16.76 11.45 43.12 42.79 35.72
Table6:HumanEvaluationRubricforFidelityofFine-GrainedVisualDescriptions
Score Description
1 CompletelyUnfaithful:Thereisnorelationbetweenthedescriptionandtheoriginalimage.
Theelementsmentioneddonotexistintheimage.
2 AlmostUnfaithful:Thereareminimalsimilaritiesbetweenthedescriptionandtheoriginal
image.Thedescriptionlargelyintegratesexternalknowledge.Mostcontentseitherdonot
correspondtoorsignificantlydeviatefromtheactualimage.
3 PartiallyFaithful:Thedescriptionsomewhatmatchestheoriginalimagewithafairproportion
ofaccuracy,albeitwithsomeexternalknowledgetext.Somecontentstillmismatchesoromits
keyelementsoftheimage.
4 MostlyFaithful:Mostofthedescriptionisconsistentwiththeoriginalimage,withminor
discrepanciesoromissions.
5 CompletelyFaithful:Thedescriptionperfectlyalignswiththeoriginalimagewithsufficient
detail.Alldescribedelementscanbeidentifiedintheimagewithoutanyomissionsorinaccu-
racies.