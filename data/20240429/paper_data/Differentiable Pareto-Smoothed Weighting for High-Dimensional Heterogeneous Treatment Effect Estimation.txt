Differentiable Pareto-Smoothed Weighting
for High-Dimensional Heterogeneous Treatment Effect Estimation
YoichiChikahara1 KanseiUshiyama2*
1NTTCommunicationScienceLaboratories,Kyoto,Japan
2TheUniversityofTokyo,Tokyo,Japan
Abstract One of the fundamental difficulties in high-dimensional
heterogeneoustreatmenteffectestimationisthesamplese-
lectionbiasinducedbyconfounders,i.e.,thefeaturesofan
There is a growing interest in estimating hetero- individual that affect their treatment choice and outcome.
geneoustreatmenteffectsacrossindividualsusing
Forinstance, inthecaseof medicaltreatment,apossible
theirhigh-dimensionalfeatureattributes.Achiev- confounderisage:Olderpatientsavoidchoosingsurgery
ing high performance in such high-dimensional duetoitsriskwhiletheyhavehighermortalityingeneral
heterogeneoustreatmenteffectestimationischal-
[Zengetal.,2022].Sincetherearefewerrecordsofolder
lengingbecauseinthissetup,itisusualthatsome patients who received surgery treatment, it is not easy to
featuresinducesampleselectionbiaswhileothers accuratelypredicttheoutcomeofsurgeryonolderpatients,
donotbutarepredictiveofpotentialoutcomes.To thusmakingitdifficulttoestimatethetreatmenteffect.
avoidlosingsuchpredictivefeatureinformation,
Toaddresssuchsampleselectionbias,itiscrucialhowto
existingmethodslearnseparatefeaturerepresen-
breakthedependenceoftreatmentchoiceonconfounders. tationsusingtheinverseofprobabilityweighting
A promising approach for high-dimensional setup is rep-
(IPW).However,duetothenumericallyunstable
IPWweights,theysufferfromestimationbiasun- resentation learning [Johansson et al., 2016, Shalit et al.,
2017], which estimates the potential outcomes under dif-
derafinitesamplesetup.Todevelopanumerically
ferent treatment assignments by extracting the balanced
robustestimatorviaweightedrepresentationlearn-
ing,weproposeadifferentiablePareto-smoothed featurerepresentationthatislearnedsuchthatitsdistribu-
tionisidenticalbetweentreatedanduntreatedindividuals.
weightingframeworkthatreplacesextremeweight
valuesinanend-to-endfashion.Experimentalre- However,sincethisapproachconvertsallinputfeaturesto
sultsshowthatbyeffectivelycorrectingtheweight
a single balanced representation, if some features are ad-
values,ourmethodoutperformstheexistingones, justment variables (a.k.a., risk factors) [Brookhart et al.,
includingtraditionalweightingschemes. 2006],whicharenotrelatedtosampleselectionbiasbutare
usefulforoutcomeprediction,itmayinadvertentlyelimi-
natesuchusefulfeatureinformation,leadingtoinaccurate
1 INTRODUCTION treatmenteffectestimation[Saueretal.,2013].Thisissueis
serious,especiallyinhigh-dimensionalsetupwhereinput
In this paper, we tackle the problem of estimating het- featuresoftencontainadjustmentvariables.Moreover,due
erogeneoustreatmenteffectsacrossindividualsfromhigh- tothelackofpriorknowledgeaboutfeatures,itisdifficult
dimensional observational data. This problem, which we forpractitionerstocorrectlyseparateadjustmentvariables
callhigh-dimensionalheterogeneoustreatmenteffectesti-
fromconfounders.Suchfeatureseparationmightbeimpos-
mation,offerscrucialapplications.Applicationexamples
sibleifoneattemptstoinputthefeatureembeddingsofa
include the evaluation of medical treatment effects from complex object (e.g., texts, images, and graphs) that are
numerousattributes[Curthetal.,2024,Shalit,2020]and constructedfrompre-trainedgenerativemodels,including
theassessmentoftheadvertisingeffectsfromeachuserâ€™s
largelanguagemodels(LLMs)[Keithetal.,2020].
manyattributes[Bottouetal.,2013,Wangetal.,2015].
Tofulfillsuchimportantbutoftenoverlookedneeds,several
data-drivenfeatureseparationmethodshavebeenproposed
*WorkduringsummerinternshipatNTTCommunicationSci- [Kuangetal.,2017,HassanpourandGreiner,2020,Kuang
enceLaboratories.
Acceptedforthe40thConferenceonUncertaintyinArtificialIntelligence (UAI2024).
4202
rpA
62
]LM.tats[
1v38471.4042:viXraet al., 2020, Wang et al., 2023]. Among these methods, 2 PRELIMINARIES
disentangledrepresentationsforcounterfactualregression
(DRCFR)[HassanpourandGreiner,2020]aimtoavoidlos- 2.1 PROBLEMSETUP
ingusefulfeatureinformationforheterogeneoustreatment
effect estimation by learning different representations for Suppose that we have a sample of n individuals, D =
confoundersandadjustmentvariables.Toachievethis,item- {(a,x,y)}n i.âˆ¼i.d. P(A,X,Y), where A âˆˆ {0,1} is a binary
ploysthetechniqueofinverseprobabilityweighting(IPW)
treai tmi enti (Ai=1
=1ifanindividualistreated;otherwiseA=0),
[RosenbaumandRubin,1983],whichperformsweighting X =[X ,...,X ]âŠ¤isd-dimensionalfeatures(a.k.a.,covari-
1 d
basedontheinverseoftheprobabilitycalledapropensity ates),andY isanoutcome.LetY0andY1bepotentialout-
score. However, such weights often take extreme values comesi.e.,theoutcomeswhenuntreated(A=0)andwhen
(especially in high-dimensional setup [Li and Fu, 2017]), treated(A = 1),whicharegivenbyY = AY1+(1âˆ’A)Y0.
andevenaslightpropensityscoreestimationerrormaylead Thenatreatmenteffectforanindividualisdefinedastheir
toalargetreatmenteffectestimationerror.
difference,i.e.,Y1âˆ’Y0[Rubin,1974].
Toresolvethisissue,wedevelopaweightcorrectionframe- Weconsidertheheterogeneoustreatmenteffectestimation
workbyutilizingthetechniqueinextremevaluestatistics,
problem, where we take as input sample D and feature
called Pareto smoothing [Vehtari et al., 2024], which re-
valuesxandoutputtheestimateofaconditionalaverage
placestheextremeweightvalueswiththequantilesofgen- treatmenteffect(CATE)conditionedonX =x:
eralizedParetodistribution(GPD).Indeed,Zhuetal.[2020]
(cid:104) (cid:105)
have already adopted Pareto smoothing and empirically E Y1âˆ’Y0 |X =x . (1)
shownthatitcanconstructanumericallystableestimator
oftheaveragetreatmenteffect(ATE)overallindividuals. CATEisanaveragetreatmenteffectinasubgroupofindi-
vidualswhohaveidenticalfeatureattributesX =x.
Toestimateheterogeneoustreatmenteffectsacrossindivid-
uals,weproposeaPareto-smoothedweightingframework ToestimatetheCATEin(1),wemaketwostandardassump-
thatcanbecombinedwiththeweightedrepresentationlearn- tions.Oneisconditionalignorability,{Y0,Y1}âŠ¥âŠ¥A|X;this
ingapproach.Thisgoal,however,isdifficulttoachievebe- conditionalindependenceissatisfiediffeaturesXcontain
causetheweightcorrectionwithParetosmoothingrequires all confounders and include only pretreatment variables,
the computation of the rank (or position) of each weight
whicharenotaffectedbytreatmentA.Theotherispositiv-
value;thiscomputationisnon-differentiableandprevents ity,0<Ï€(x)<1forallx,whereÏ€(x)(cid:66)P(A=1|X =x)is
gradientbackpropagation.Toovercomethisdifficulty,we theconditionaldistributionmodelcalledapropensityscore.
utilizethetechniqueofdifferentiableranking[Blondeletal.,
OurgoalistoachievehighCATEestimationperformance
2020]andestablishadifferentiableweightcorrectionframe-
inhigh-dimensionalsetup,wherethenumberoffeaturesd
workfoundedonParetosmoothing.Thisideaofcombining
isrelativelylarge.Underthissetup,removingthesample
Pareto smoothing and differentiable ranking, which have
selectionbiasbytransformingallfeaturesX intoasingle
beenstudiedintotallydifferentfields(i.e.,extremevalue
balancedrepresentationmightbeoverlysevere,whichleads
statistics and differentiable programming), allows for an toinaccuratetreatmenteffectestimation.Althoughseveral
effectiveend-to-endlearningforhigh-dimensionalhetero-
representationlearningmethodsaimtoavoidsuchanoverly
geneoustreatmenteffectestimation.
severe balancing [Kuang et al., 2017, Wang et al., 2023],
Ourcontributionsaresummarizedasfollows. mostofthemaredesignedATE,notCATE.Bycontrast,the
DRCFRmethod[HassanpourandGreiner,2020]focuseson
â€¢ WeproposeadifferentiablePareto-smoothedweight- CATEestimationandeffectivelybalanceshigh-dimensional
ingframeworkthatreplacestheextremeIPWweight featuresXviaweightedrepresentationlearning.
valuesinanend-to-endfashion.Tomakethisweightre-
placementproceduredifferentiable,weutilizethetech-
2.2 WEIGHTEDREPRESENTATIONLEARNING
niqueofdifferentiableranking[Blondeletal.,2020].
â€¢ Takingadvantageofthedifferentiability,webuildour DRCFR[HassanpourandGreiner,2020]isfoundedonthe
weight correction framework on the neural-network- graphical model in Figure 1, where treatment A is deter-
based weighted representation learning method (i.e., mined by functions Î“(X) and âˆ†(X), while outcome Y is
DR-CFR[HassanpourandGreiner,2020])toperform given by âˆ†(X) and Î¥(X). Following this model, it formu-
data-driven feature separation for high-dimensional
latesÎ“(X),âˆ†(X),andÎ¥(X)asthreeneuralnetworkencoders,
heterogeneoustreatmenteffectestimation. eachofwhichextractstherepresentationofinstrumental
variables(i.e.,thefeaturesthatinfluenceAbutdonotaffect
â€¢ Weexperimentallyshowthatourmethodeffectively
Y),confounders,andadjustmentvariables,respectively.For
learnsthefeaturerepresentationsandoutperformsthe example,inmedicaltreatmentsetup,apossibleinstrumen-
existingones,includingtraditionalweightingschemes. talvariable,confounder,andadjustmentvariablewouldbe
2First,welearnpropensityscoreÏ€(whilefixingothermodel
Features X
parameters)byminimizingthecrossentropyloss:
1(cid:88)n (cid:16)
minâˆ’ a log(Ï€(Î“(x),âˆ†(x)))
Î“(X) âˆ†(X) Î¥(X) Ï€ n i i i
i=1
(cid:17)
+(1âˆ’a)log(1âˆ’Ï€(Î“(x),âˆ†(x))) +Î» â„¦(Ï€), (2)
i i i Ï€
where â„¦(Â·) is some regularizer that penalizes the model
complexity,andÎ» >0isaregularizationparameter.
Ï€
A Y
Second,welearntheothermodelparameters(withÏ€â€™spa-
rametersfixed)byminimizingtheweightedloss:
Figure1:GraphicalmodelillustrationofDRCFRmethod 1(cid:88)n
Î“,âˆ†m ,Î¥i ,hn
0,h1 n
i=1
w il(y i,hai(âˆ†(x i),Î¥(x i)))
income,age,andsmokinghabits,respectively.
+Î»Î¥MMD(cid:16)(cid:8)Î¥(x i)(cid:9) i:ai=0,(cid:8)Î¥(x i)(cid:9) i:ai=1(cid:17)
(cid:16) (cid:17)
Tolearntherepresentationsofsuchfeatures,DRCFRmini- +Î» âˆ’Ï€â„¦ Î“,âˆ†,Î¥,h0,h1 , (3)
mizestheweightedlossbycomputingtheweightsbasedon
theinverseofthepropensityscore,givenbyÏ€(Î“(X),âˆ†(X)). where w i is the weight that is given by propensity score
Ï€(Î“(x i),âˆ†(x i)),listhepredictionlossforoutcomey i,Î»Î¥ >0
AstrongadvantageofDRCFRisthatitcanavoidlosingthe andÎ» >0areregularizationparameters,1 andMMDde-
âˆ’Ï€
informationofadjustmentvariables,representedbyÎ¥(X),
notesthekernelmaximummeandiscrepancy(MMD)[Gret-
which is useful for outcome prediction. However, since tonetal.,2012],whichmeasuresthediscrepancybetween
theinverseofconditionalprobability,Ï€(Î“(X),âˆ†(X)),often empirical conditional distributions PË†(Î¥(X) | A = 0) and
yieldsextremevalues,underfinitesamplesettings,evena PË†(Î¥(X)| A=1).RegularizingthisMMDtermenforcesÎ¥
slightestimationerrorofÏ€leadstoalargeweightestimation tohavenoinformationabouttreatmentA,thusmakingÎ¥(X)
error.Thisnumericalinstabilityofweightestimationmakes agoodrepresentationofadjustmentvariables.
itdifficulttoachievehighCATEestimationperformance.
ToachievehighCATEestimationperformance,itisessen-
tial how to compute the weight value, i.e., w in (3). In
i
3 PROPOSEDMETHOD theDRCFRmethod[HassanpourandGreiner,2020],the
weightisformulatedusingimportancesampling,whichem-
ploysthedensity-ratio-basedweighttoconstructaweighted
Toimprovetheestimationstabilityofweightedrepresenta-
tionlearning,weproposeadifferentiableweightcorrection estimatorofexpectedvalue.Toestimatetheexpectedout-
comepredictionlossoverobservedindividuals(A=a)and
frameworkthatcanbeusedinanend-to-endfashion. i
theoneoverunobservedindividuals(A=1âˆ’a),DRCFR
i
formulatestheweightasthesumoftwodensityratios:
3.1 OVERVIEW
P(Î“(x),âˆ†(x)| A=a) P(Î“(x),âˆ†(x)| A=1âˆ’a)
w = i i i + i i i
i P(Î“(x),âˆ†(x)| A=a) P(Î“(x),âˆ†(x)| A=a)
To estimate the CATE in (1), following the DRCFR i i i i i i
P(A=1âˆ’a |Î“(x),âˆ†(x)) P(A=a)
method [Hassanpour and Greiner, 2020], we perform =1+ i i i i
weighted representation learning. To achieve this, we P(A=1âˆ’a i) P(A=a i|Î“(x i),âˆ†(x i))
learn the three model components: the feature represen- P(A=a) (cid:32) 1 (cid:33)
=1+ i âˆ’1 (4)
tations(i.e.,Î“(X),âˆ†(X),andÎ¥(X)inFigure1),propensity P(A=1âˆ’a) P(A=a |Î“(x),âˆ†(x))
i i i i
scoremodelÏ€(Î“(X),âˆ†(X)),andoutcomepredictionmodels
1 1
h0(âˆ†(X),Î¥(X)) and h1(âˆ†(X),Î¥(X)), where h0 and h1 are âˆ P(A=a |Î“(x),âˆ†(x)) (cid:66) Ï€ (Î“(x),âˆ†(x)),
usedtopredictpotentialoutcomesY0andY1,respectively. i i i ai i i
where Ï€ (X) = aÏ€(X)+(1âˆ’a)(1âˆ’Ï€(X)). Here weight
DRCFR jointly optimizes these three model components ai i i
w is proportional to the inverse of the propensity score
byminimizingtheweightedloss.However,weempirically i
Ï€ (Î“(x),âˆ†(x)).SincesuchanIPWweightoftentakesan
observedthatsuchajointoptimizationisdifficult.Apos- ai i i
extremevalue,theweightestimationisnumericallyunstable,
siblereasonisthatthelossfunctiondramaticallychanges
leadingtoinaccurateCATEestimation.Thisissueisserious
withtheIPWweightsandhencesubstantiallyvarieswith
inhigh-dimensionalsetupduetothedifficultyofcorrectly
theparametervaluesofpropensityscoreÏ€.Forthisreason,
estimatingthepropensityscore[Assaadetal.,2021].
weseparatelylearnÏ€andperformanalternateoptimization
thatrepeatedlytakesthefollowingtwosteps. 1Subscriptâˆ’Ï€denotestheothermodelcomponentsthanÏ€.
3To resolve this issue, we improve the weight stability by whereâŒŠnâŒ‹denotesafloorfunction,whichreturnsthegreatest
replacinganextremevalueofw in(4).Todoso,weutilize integerthatislessthanorequalton.Lettingw â‰¤ Â·Â·Â· â‰¤
i [1]
theweightstabilizationtechnique,calledParetosmoothing. w betheweightssortedinanascendingorder,theM+1
[n]
largestonesaredenotedbyw ,...,w .
[nâˆ’M] [n]
3.2 WEIGHTCORRECTIONVIAPARETO FollowingVehtarietal.[2024],wesetlocationparameterÂµ
SMOOTHING tothe(M+1)-thlargestIPWweightvalue,i.e.,
ÂµË† =w . (9)
Paretosmoothing[Vehtarietal.,2024]isthetechniquefor [nâˆ’M]
improvingtheweightstabilityofimportancesampling. Bycontrast,weestimateÏƒandÎ¾ usingw [nâˆ’M+1],...,w [n].
Among several estimators, we employ the standard one
AccordingtoVehtarietal.[2024],thistechniquehastwoad-
calledprobabilityweightedmoment(PWM)[Hoskingand
vantages.First,itcanyieldalessbiasedestimator,compared
Wallis,1987],2whichconstructstheestimatorsofÏƒandÎ¾
with weight truncation, which replaces extreme weights
usingthefollowingweightedmomentstatistic:
naivelywithconstants[Crumpetal.,2009,Ionides,2008]:
Î±
=E(cid:2) (1âˆ’F(W))s(Wâˆ’Âµ)(cid:3) sâˆˆ(cid:8) 0,1(cid:9)
. (10)
ï£± s
wT i runc.
(cid:66)ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³ w
UL
i
i
i
if
f
f
w ULi
â‰¤
â‰¤<
w
wL
i <U , (5) R ofo Wugh âˆ’ly Âµs wpe ita hki wn eg i, gs hta tt (i 1st âˆ’ic FÎ± (s Win ))( s1 a0 n) dis isa ew ste ii mgh at te ed da av serage
i
whereL>0andU >0arethetruncationthresholds.Sec- Î±Ë† = 1
(cid:88)n
(cid:0) w âˆ’ÂµË†(cid:1) , (11)
ond, it can be combined with self-normalization, which 0 M [i]
i=nâˆ’M+1
p atr ie vv een tots et ah ce hw oe ti hg eh rts bf yro dm ivib de ii nn gg eto ao chsm wa el il go hr tt vo ao lula erg be yr ie tl s-
Î±Ë† = 1
(cid:88)n
(nâˆ’i)(cid:0) w âˆ’ÂµË†(cid:1) . (12)
1 M [i]
empiricalmeanunderthesametreatmentassignment: i=nâˆ’M+1
wN
i
orm. (cid:66) ww A=i ai, wherewA=ai = (cid:80) (cid:80)n j=
n
j1 =1I( Ia (aj j= =a i a) iw )j . (6) UsingÎ±Ë† 0andÎ±Ë† 1,the ÏƒË†P =WM 2Î±Ë†m 0e Î±Ë†t 1ho ,destimatesÏƒandÎ¾a (s
13)
Î±Ë† âˆ’2Î±Ë†
0 1
Here I(a = a) is the indicator function that takes 1 if Î±Ë†
j i Î¾Ë† =2âˆ’ 0 . (14)
a j =a i;otherwise,0.Weexperimentallyconfirmedthatper- Î±Ë† 0âˆ’2Î±Ë† 1
formingself-normalizationoverPareto-smoothedweights
leadstobetterCATEestimationperformance(Section4.1). 3.2.2 WeightReplacementwithGPDQuantiles
Toconstructaweightedestimatorthatisnumericallyrobust
Second, we replace the M largest weight values, i.e.,
toweightestimationerror,Paretosmoothingreplacesthe
w [nâˆ’M+1],...,w [n],withthequantilesofthefittedGPDwith
extremelylargeweightvalueswiththeGPDquantiles.To parameters(ÂµË†,ÏƒË†,Î¾Ë†)in(9),(13),and(14).
achievethis,ittakestwosteps:theGPDparameterestima-
tionandtheweightreplacement. Since the quantile function is given by the inverse of the
cumulativedistributionfunction,wereplaceweightvalue
3.2.1 GPDParameterFitting
w [nâˆ’M+m](m=1,...,M)with mâˆ’ M1/2-quantileas
(cid:32) (cid:33)
First,wefittheGPDparameterstolargeIPWweightvalues.
w [nâˆ’M+m]
=FË†âˆ’1 mâˆ’ M1/2
, (15)
SupposethatarandomvariableW followstheGPD.Then where FË† denotes the fitted GPD cumulative distribution
itsGPDcumulativedistributionfunctionisdefinedas function.Bycontrast,wedonotchangetheotherweight
F(w)=ï£± ï£´ï£´ï£²
ï£´ï£´ï£³
1 1âˆ’ âˆ’(cid:16) e1 âˆ’w+ Ïƒâˆ’ÂµÎ¾(w Ïƒâˆ’Âµ)(cid:17)âˆ’1 Î¾ (( Î¾Î¾ =(cid:44)0 0)
)
(7) wva elu ce as wn, si. ue =m., m Iw (a[ i1 r â‰¥] i, z. e n.. t âˆ’h, ew M[ wnâˆ’ +eM i 1g]. )hH t FË†r âˆ’e e 1n p (cid:32)c la ie c, âˆ’ele m (ntt ei âˆ’nn tg Mpi r )o= âˆ’cen 1dâˆ’ /u 2rM e (cid:33)a+ s m,
where Âµ âˆˆ R, Ïƒ > 0, and Î¾ âˆˆ R are location, scale, and [i] M
shapeparameters,respectively. +(1âˆ’I(iâ‰¥nâˆ’M+1))w . (16)
[i]
WefittheseGPDparameterstotheM+1largestIPWweight
2Although we empirically observed that using PWM leads
values.HereM(0< M <n)isgivenbyheuristic;following to good CATE estimation performance, GPD parameter fitting
Vehtarietal.[2024],wedetermineitby mightnotbeeasyingeneral.However,accordingtoVehtarietal.
[2024,Section6],onecaneffectivelyevaluatethereliabilityof
(cid:26)(cid:22)n(cid:23) âˆš (cid:27)
M =min ,âŒŠ3 nâŒ‹ , (8) GPDfittingbyemploying theestimatedvalueof GPDâ€™sshape
5 parameter,Î¾Ë†,whichdeterminestheheavinessofdistributiontail.
4IPWweightsby(16),andthenlearnsthefeaturerepresenta-
r
tions.Thisapproach,however,requirestofitapropensity
r Îµ(Îµ = 0.1)
scoremodeldirectlytofeaturesX,nottheirrepresentation.
r Îµ(Îµ = 0.2)
Sinceaccuratelyestimatingapropensityscoremodelfrom
r
1
high-dimensionalfeaturesXisconsiderablydifficult,dueto
themodelmisspecificationerror,suchaseparatelearning
approachwillleadtoCATEestimationbias.Weexperimen-
tallyshowitspoorperformanceinSection4.1.
For this reason, we develop a joint learning approach by
makingthenon-differentiablecomputationinParetosmooth-
w
1
ingdifferentiable.
Figure2:Illustrationofrankfunctionr=r(w)(black)and
differentiablerankfunctionsr=r (w)(orangeandgreen). 3.4 MAKINGPARETOSMOOTHING
Îµ
Herewetakeinputvectorw=[w ,1,2,3]âŠ¤,varyw â€™svalue, DIFFERENTIABLE
1 1
andlookathowitsrankr âˆˆrchanges.Whenregularization
1
parameterÎµâ†’0,r convergestor[Blondeletal.,2020]. 3.4.1 DifferentiableApproximation
Îµ
Theweightreplacementformulain(16)requiresthecompu-
In this paper, we propose to utilize the weight replace- tationofthetwotroublesomepiecewiseconstantfunctions.
ment formula in (16) to improve the estimation stability Oneisrankfunctionr,whichisneededtoobtaintheposi-
ofweightedrepresentationlearning.Unfortunately,wecan- tionofweightw inthesortedvector[w ,...,w ]âŠ¤,and
i [1] [n]
notdirectlyemploythisformulainanend-to-endmanner theotherisindicatorfunctionI(iâ‰¥nâˆ’M+1).
becauseitneedsnon-differentiablecomputations.
Tomakerankfunctionr differentiable,weutilizethedif-
ferentiablerankingtechnique[Cuturietal.,2019,Blondel
3.3 NON-DIFFERENTIABLEPROCEDURES etal.,2020],whichapproximatesrankfunctionr(w)with
a differentiable function. Among the recent methods, we
ThemaindifficultyofusingParetosmoothingforweighted selectacomputationallyefficientone[Blondeletal.,2020],
representationlearningisthatitrequiresthecomputationof which works with O(nlogn) time and O(n) memory com-
therankofeachIPWweight. plexity. With this method, we approximate rank function
Ranking is an operation that takes input vector w = r(w)asthesolutiontotheregularizedlinearprogramming
[w ,...,w ]âŠ¤andoutputsthepositionofeachelementw in (LP) that contains the l2 regularization term with regular-
1 n i
thesortedvector[w ,...,w ]âŠ¤,wherew â‰¤ Â·Â·Â· â‰¤ w . izationparameterÎµ>0.Thesolution,r Îµ(w),isapiecewise
[1] [n] [1] [n]
To illustrate this operation, consider the case with n = 3. linearfunctionthatcanwellapproximaterankfunctionr
Forinstance,ifwsatisfiesw â‰¤ w â‰¤ w ,sincew = w ,
(asillustratedinFigure2)andisdifferentiablealmostevery-
3 1 2 1 [2]
w = w , and w = w hold, the rank of w is given as where,thusgreatlyfacilitatinggradientbackpropagation.
2 [3] 3 [1]
vectorr=[2,3,1]âŠ¤.Formally,suchanoperationcanbeex-
RegardingindicatorfunctionIin(16),weapproximateitas
pressedasr=r(w),usingfunctionr,calledarankfunction
thesigmoidfunctionÏ‚:
(SeeAppendixAforthedefinitionoffunctionr).
1
Unfortunately,thisrankfunctionisnotdifferentiablewith I(iâ‰¥ j)â‰ƒÏ‚(i, j)(cid:66) , (17)
1+eâˆ’Îº(iâˆ’j)
respecttoinputw.Toseethis,considerw = [w ,1,2,3]âŠ¤
1
andobservehowtherankofw varieswhenweincrease whereÎº>0isahyperparameter.
1
w â€™svalue.Inthiscase,itsrank,r ,isgivenasapiecewise
1 1
constantfunction,asillustratedastheblacklineinFigure2.
3.4.2 ReformulationofGPDParameterEstimators
Sincethederivativeofsuchapiecewiseconstantfunction
is always zero or undefined, we cannot perform gradient
Toemploydifferentiablerankr=r (w)forParetosmooth-
backpropagationandhencecannotemploytheweightcor- Îµ
ing,sinceittakescontinuousvalues,weneedtomodifythe
rectiontechniquein(16)inanend-to-endmanner.There-
GPDparameterestimators,i.e.,ÂµË†,ÏƒË†,andÎ¾Ë†.
fore,withsuchanon-differentiablerankfunction,wecannot
useParetosmoothingforweightedrepresentationlearning, As regards location parameter ÂµË† in (9), since this esti-
whichjointlylearnsthepropensityscoremodel,thefeature mator is given as w , i.e., the largest weight among
[nâˆ’M]
representations,andtheoutcomepredictionmodels. w ,...,w ,wereformulateitas
[1] [nâˆ’M]
Onemayconsideraseparatelearningapproachthattrains ÂµËœ =w, wherei=argmax(cid:8) r |r â‰¤nâˆ’M(cid:9) .
i i i
thepropensityscoremodel,computesthePareto-smoothed i
5ToreformulateÏƒË† andÎ¾Ë†in(13)and(14),werephraseestima- Algorithm 1 Differentiable Pareto-Smoothed Weighting
torsÎ±Ë† andÎ±Ë† in(11)and(12).Withnon-differentiablerank (DPSW)
0 1
r=r(w),theseestimatorsareequivalentlyreformulatedby
1: InitializetheparametersofÎ“,âˆ†,Î¥,Ï€,h0,andh1
rewritingthesummationoverw [nâˆ’M+1],...,w [n]in(11)and
2: whilenotconvergeddo
(12)withindicatorfunctionIas
3: whilenotconvergeddo
1 (cid:88)n 4: Samplemini-batchfromD={(a i,x i,y i)}n i=1
Î±Ë† = I(r â‰¥nâˆ’M+1)(w âˆ’ÂµË†) 5: UpdateÏ€byminimizingcrossentropylossin(2)
0 M i i
i=1 6: endwhile
1 (cid:88)n 7: whilenotconvergeddo
Î±Ë† 1 = M I(r i â‰¥nâˆ’M+1)(nâˆ’r i)(w iâˆ’ÂµË†). 8: Samplemini-batchfromD={(a i,x i,y i)}n i=1
i=1 9: forinstanceiinmini-batchdo
Hence,whengivendifferentiablerankr=r (w),byreplac- 10: Computeweightw iby(4)
Îµ
11: endfor
ingindicatorfunctionIwithsigmoidfunctionÏ‚in(17),we
makeÎ±Ë† andÎ±Ë† differentiablewithrespecttor: 12: Computedifferentiablerankr=r Îµ(w)
0 1 13: EstimateGPDparametersasÂµËœ,ÏƒËœ,andÎ¾Ëœ
1
(cid:88)n 14: forinstanceiinmini-batchdo
Î±Ëœ 0 = MËœ Ï‚(r i,nâˆ’M+1)(w iâˆ’ÂµËœ), (18) 15: Replaceeachweightw iwithwËœ iin(20)
i=1 16: endfor
1 (cid:88)n 17: UpdateÎ“,âˆ†,Î¥,h0,andh1byminimizingpredic-
Î±Ëœ = Ï‚(r,nâˆ’M+1)(nâˆ’r)(w âˆ’ÂµËœ), (19)
1 MËœ i i i tionlossin(3)withPareto-smoothedweights{wËœ i}
i=1
18: endwhile
whereMËœ =(cid:80)n Ï‚(r,nâˆ’M+1).BysubstitutingÎ±Ëœ andÎ±Ëœ 19: endwhile
i=1 i 0 1
forÎ±Ë† andÎ±Ë† in(13)and(14),wecomputescaleandshape
0 1
parametersasÏƒËœ andÎ¾Ëœ,respectively.
isabsolutelycontinuous,whichisemployedtoprovethe
asymptoticconsistencyinTheorem1ofVehtarietal.[2024].
3.4.3 OverallAlgorithm This assumption holds if each activation is differentiable
almost everywhere (i.e., differentiable except on a set of
Using the GPD cumulative distribution function, FËœ, with measure zero). However, for instance, using the rectified
parameters(ÂµËœ,ÏƒËœ,Î¾Ëœ),wereplaceeachweightw in(4)with linearunit(ReLU)inpropensityscoremodelÏ€makesthe
i
distributionofIPWweightdiscontinuous,thusviolatingthe
(cid:32) (cid:32) (cid:33)(cid:33)
wËœ
=Ï‚(r,nâˆ’M+1)FËœâˆ’1
Î¶
r iâˆ’(nâˆ’M)âˆ’1/2 assumptionofParetosmoothing.
i i M
+(1âˆ’Ï‚(r i,nâˆ’M+1))w i, (20) 4 EXPERIMENTS
where Î¶(x) (cid:66) min{max{x,0},1}isafunction thatforces
4.1 SEMI-SYNTHETICDATA
input x to lie in [0,1]. Using Pareto-smoothed weight wËœ
i
insteadofw,weminimizetheobjectivefunctionin(3).
i
First,weevaluatedtheCATEestimationperformanceusing
Algorithm1summarizesourmethod.Toalternatelymin- semi-syntheticbenchmarkdatasets,wherethetrueCATE
imize the objective functions in (2) and (3), we perform valuesareavailable,unlikereal-worlddata.
stochastic gradient descent [Kingma and Ba, 2015]. Af-
Data:Weselectedthetwohigh-dimensionaldatasets:the
ter the convergence, we estimate the CATE in (1) by
h1(âˆ†(x),Î¥(x))âˆ’h0(âˆ†(x),Î¥(x)). NewsandtheAtlanticCausalInferenceConference(ACIC)
datasets[Johanssonetal.,2016,Shimonietal.,2018].
ComparedwithDRCFR[HassanpourandGreiner,2020],
The News dataset is constructed from n = 5000 articles,
our method requires additional time to compute Pareto-
randomlysampledfromtheNewYorkTimescorpusinthe
smoothedweights(lines12-16inAlgorithm1).Inpartic-
ular, computing differentiable rank (line 12) requires the
UCIrepository.3Thetaskistoinfertheeffectoftheview-
ing device (desktop (A = 0) or mobile (A = 1)) on the
time complexity O(BlogB) for mini-batch size B, which
readersâ€™experienceY.FeaturesXarethecountofd =3477
is needed to evaluate the objective function in (3) and its
wordsineacharticle.TreatmentAandoutcomeY aresim-
gradientforeachiterationinthetrainingphase.
ulatedusingthelatenttopicvariablesobtainedbyfittinga
Remark:Strictlyspeaking,thechoiceofactivationfunc- topic model on X. The ACIC dataset is derived from the
tionsinpropensityscoreÏ€andfeaturerepresentationsÎ“and clinical measurements of d = 177 features in the Linked
âˆ†isimportanttosatisfytheassumptionofParetosmooth-
ingthatthedistributionoftheimportancesamplingweight 3https://archive.ics.uci.edu/dataset/164/bag+of+words
6Birth and Infant Death Data (LBIDD) [MacDorman and Table1:MeanandstandarddeviationoftestPEHEonsemi-
Atkinson,1998]andisdevelopedforthedataanalysiscom- syntheticdatasets(Lowerisbetter)
petition called ACIC2018. Here, we randomly picked up
n=5000observationsandprepared20datasets.Withboth Method News(d =3477) ACIC(d =177)
semi-syntheticdatasets,werandomlyspliteachsampleinto
LR-1 3.35Â±1.28 0.72Â±0.07
training,validation,andtestdatawitharatioof60/20/20.
LR-2 5.36Â±1.75 3.82Â±0.15
Baselines: To evaluate our method (DPSW) and its vari-
SL 2.83Â±1.11 1.69Â±0.52
ant that performs self-normalization (DPSW Norm.),
TL 2.55Â±0.82 2.23Â±0.50
we consider 15 baselines. With DRCFR [Hassanpour
XL 2.77Â±1.01 1.05Â±0.72
and Greiner, 2020], we tested the four different weight-
DRL 23.9Â±5.96 3.77Â±8.96
ing schemes: no weight modification (DRCFR), self-
normalization(DRCFRNorm.;Eq.(6)),theweighttrun- CF 3.84Â±1.67 3.55Â±0.19
cationwiththethresholdsuggestedbyCrumpetal.[2009] CFDML 2.69Â±1.06 1.18Â±0.32
(DRCFRTrunc.;Eq.(5)),andtheschemethatignoresthe TARNet 4.92Â±1.80 3.31Â±0.51
prediction loss for the individuals with extreme weights GANITE 2.68Â±0.66 3.69Â±0.77
based on the same threshold (DRCFR Ignore). We also
DRCFR 2.38Â±0.66 0.98Â±0.07
tested a separate learning approach (PSW; Section 3.3),
DRCFRNorm. 2.37Â±0.94 0.73Â±0.12
whichtrainspropensityscoreÏ€with{(a,x)}n beforehand
andlearnsonlyâˆ†(X)andÎ¥(X)usingthi ePi ari= e1
to-smoothed
DRCFRTrunc. 2.42Â±0.79 1.06Â±0.06
DRCFRIgnore 2.35Â±0.75 0.84Â±0.06
IPWweights.Otherbaselinesinclude(i)linearregression
PSW 4.03Â±1.35 0.71Â±0.01
methods:asinglemodelwithtreatmentAaddedtoitsinput
(LR-1)andtwoseparatemodelsforeachtreatment(LR-2); DPSW 2.20Â±0.72 0.57Â±0.03
(ii)meta-learnermethods:theS-Learner(SL),theT-Learner DPSWNorm. 2.10Â±0.66 0.52Â±0.16
(TL),theX-Learner(XL),andtheDR-Learner(DRL);(iii)
tree-basedmethods:thecausalforest[Atheyetal.,2019]
(CF)andthevariantcombinedwithdouble/debiasedma-
theself-normalizationofPareto-smoothedweightscanfur-
chinelearning[Chernozhukovetal.,2018](CFDML);and therimprovethestabilityoftheweightestimation.
(iv)neuralnetworkmethods:thetreatment-agnosticregres-
Weighted representation learning methods (DR-CFR and
sionnetwork[Shalitetal.,2017](TARNet)andthegenera-
DPSW)performedbetterthanotherneuralnetworkmethods
tiveadversarialnetwork[Yoonetal.,2018](GANITE).We
(TARNet and GANITE), especially on the ACIC dataset.
detailthesettingsofthesebaselinesinAppendixB.1.
GiventhattreatmentAandoutcomeY ofthisdatasetwere
Settings:RegardingourmethodandDRCFR,weusedthree- simulated using different features in X, these results em-
layeredfeed-forwardneuralnetworks(FNNs)toformulate phasizetheimportanceofperformingdata-drivenfeature
feature representations Î“(X), âˆ†(X), and Î¥(X), propensity separationviaweightedrepresentationinsuchasetup.
scoreÏ€andoutcomepredictionmodelsh0andh1.
PSWworkedmuchworseontheNewsdatasetthanDPSW,
Wetunedthehyperparameters(e.g.,parameterÎµofdifferen-
indicating that fitting a propensity score directly to high-
tiablerankr Îµ(w)inourmethod)byminimizingtheobjective dimensionalfeaturesXleadstoaseveremodelmisspecifi-
functionvalueonvalidationdata;suchhyperparametertun- cationerror,makingthesubsequentweightedrepresentation
ingisstandardforCATEestimation[Shalitetal.,2017]. learningdifficult,evenwiththePareto-smoothedweights.
By contrast, our joint learning approach well performed,
Performancemetric:FollowingHill[2011],weusedthe
precisionintheestimationofheterogeneouseffect(PEHE),
thankstotheuseofdifferentiableParetosmoothing.
(cid:113)
PEHE(cid:66)
1(cid:16)
(y1âˆ’y0)âˆ’Ï„Ë†
(cid:17)2
,wherey0andy1arethetrue
n i i i i i
potential outcomes, and Ï„Ë† denotes the predicted CATE 4.2 SYNTHETICDATA
i
value.Wecomputedthemeanandthestandarddeviationof
testPEHEover50realizationsofpotentialoutcomes(the Next,weinvestigatedhowwellourmethodcanlearnthe
Newsdataset)and20realizations(theACICdataset). featurerepresentationsusingsyntheticdata,wherethedata-
generatingprocessesareentirelyknown.
Results:Table1presentsthetestPEHEsontheNewsand
ACICdatasets. Data:FollowingHassanpourandGreiner[2020],wesimu-
latedsyntheticdata.WerandomlygeneratedfeaturesX =
Ourproposedframework(DPSWandDPSWNorm.)out-
performedallbaselines,demonstratingtheireffectiveness
[XÎ“,Xâˆ†,XÎ¥]âŠ¤ âˆˆ Rd (d = 15,18,...,30).Then,byregard-
ingfeaturesubsetsXÎ“ âˆˆRd/3,Xâˆ† âˆˆRd/3,andXÎ¥ âˆˆRd/3as
in CATE estimation from high-dimensional data. DPSW
instrumentalvariables,confounders,andadjustmentvari-
Norm.achievedlowerPEHEsthanDPSW,implyingthat
ables, respectively, we sampled binary treatment A using
7(a)Relative difference between |Wğšª1| and |Wâˆ’1ğšª| (b)Relative difference between |Wâˆ†1| and |Wâˆ’1 âˆ†| (c)Relative difference between |Wğš¼1| and |Wâˆ’1 ğš¼| (d) Test PEHE
Number of features d Number of features d Number of features d Number of features d
Figure3:LearnedencoderparameterdifferencesandtestPEHEsonsyntheticdata.(a):ValuedifferenceofW1inencoder
Î“(X);(b):ValuedifferenceofW1 inencoderâˆ†(X);(c):ValuedifferenceofW1 inencoderÎ¥(X);(d)TestPEHEs.With
TARNet,sinceitlearnssingleencoder,wecomputedallparametervaluedifferenceswithweightmatrixinsameencoder.
XÎ“ andXâˆ† andoutcomeY byemployingXâˆ† andXÎ¥ (See andDRCFR,demonstratingtheimportanceofdata-driven
AppendixB.2forthedetail).Wespliteachof20datasets featureseparationviaweightedrepresentationlearning.By
(n=20000)witha50/25/25training/validation/testratio. contrast,ourmethodachievedthelowestPEHE,thusindicat-
ingthatourweightcorrectionframeworkcansuccessfully
Performance metric: As with Hassanpour and Greiner
improvetheCATEestimationperformanceofDRCFR.
[2020],weevaluatedthequalityofthelearnedfeaturerep-
resentationsÎ“(X),âˆ†(X),andÎ¥(X),eachofwhichisformu- Performance under high-dimensional setup: We con-
latedasathree-layeredFNNencoder: firmed that our method also worked well with d =
(cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17) 600,1200,...,3000(SeeAppendixCforthedetail).
FNN(X)(cid:66)Î½ W3Î½ W2Î½ W1X ,
whereÎ½isexponentiallinearunits(ELUs)[Clevertetal.,
5 RELATEDWORK
2016],andW1,W2,andW3aretheweightparametermatri-
cesinthefirst,thesecond,andthethirdlayer,respectively.
Data-driven feature separation for CATE estimation:
TodeterminewhetherthelearnedFNNencoderscorrectly
CATEestimationhasgainedincreasingattentionbecause
lookatimportantfeatures,wemeasuredtheattributionof
ofitsgreatimportanceforcausalmechanismunderstanding
featuresXÎ“,Xâˆ†,andXÎ¥byemployingW1,i.e.,thetrained
[Chikaharaetal.,2022,Zhaoetal.,2022]andfordecision
weightmatrixinthefirstlayerofeachencoder.Forinstance,
supportinvariousfields,suchasprecisionmedicine[Gao
wequantifiedXÎ“â€™sattributiononÎ“(X),bytakingtwosteps.
etal.,2021]andonlineadvertising[Sunetal.,2015].There
First, we partitioned its learned weight parameter matrix
hasbeenasurgeofinterestinleveragingflexiblemachine
asW1 = [W1,W1 ],whereW1 isthesubmatrixwiththe
Î“ âˆ’Î“ Î“ learningmodels,includingtree-basedmodels[Atheyetal.,
firstd/3columnsofW1andW1 istheonewiththeother
âˆ’Î“ 2019, Hill, 2011], Gaussian processes [Alaa and van der
columns.ThenwemeasuredhowgreatlyfeaturesXÎ“affects
Schaar,2018,HoriiandChikahara,2024],andneuralnet-
thelearnedrepresentationÎ“(X)bytakingtherelativedif-
works [Hassanpour and Greiner, 2019, Johansson et al.,
ferencebetweentheaverageabsolutevaluesoftheweight
2016,Shalitetal.,2017].However,mostmethodstreatall
parametersubmatrices,i.e., |W1 Î“|âˆ’|W1 âˆ’Î“| .Weevaluatedother inputfeaturesXasconfounders.AspointedoutbyWuetal.
learnedrepresentations,âˆ†(X)an|W d1 âˆ’Î“Î¥|
(X),inthesameway.
[2022],theempiricalperformanceofsuchmethodsvariesa
lotwiththepresenceofadjustmentvariablesinX,whichis
Results:Figure3showsthemeansandstandarddeviations usualinpractice,especiallyinhigh-dimensionalsettings.
of learned parameter differences and test PEHEs over 20
Thisissuemotivatesustodevelopdata-drivenfeaturesepara-
randomlygeneratedsyntheticdatasets.
tionmethodsfortreatmenteffectestimation.Apioneerwork
WithourDPSWmethodandDRCFR,theabsoluteparam- isthedata-drivenvariabledecomposition(D2VD)[Kuang
eter values |W1|, |W1|, and |W1| were sufficiently larger etal.,2017,2020],whichminimizestheweightedprediction
Î“ âˆ† Î“
than |W1 |, |W1 |, and |W1 |, respectively, showing that lossplustheregularizerforfeatureseparation.Therecent
âˆ’Î“ âˆ’âˆ† âˆ’Î“
bothmethodscorrectlylearnÎ“(X),âˆ†(X),andÎ¥(X)thatare methodaddressesamorecomplicatedsetup,wherefeatures
highlydependentoninstrumentalvariablesXÎ“,confounders X includepost-treatmentvariables,whichareaffectedby
Xâˆ†,andadjustmentvariablesXÎ¥,respectively.Theseresults treatment A[Wangetal.,2023].However,theestimation
made a clear contrast to TARNet, which learns a single targetofthesemethodsisATE,notCATE.
representationwithoutperformingfeatureseparation.
By contrast, DRCFR deals with CATE estimation and is
ThesameistruefortheCATEestimationperformance(Fig- founded on weighted representation learning, which is a
ure3(d)).ThetestPEHEofTARNetwaslargerthanDPSW promisingapproachforaddressinghigh-dimensionaldata.
8This is why we adopted it as the inference engine of our References
weight correction framework. Integrating the recent idea
ofenforcingindependencebetweenfeaturerepresentations Ahmed M Alaa and Mihaela van der Schaar. Bayesian
with mutual information [Cheng et al., 2022, Chu et al., nonparametric causal inference: Information rates and
2022,Liuetal.,2024]isleftasourfuturework. learningalgorithms. IEEEJournalofSelectedTopicsin
SignalProcessing,12(5):1031â€“1046,2018.
Weightingschemesfortreatmenteffectestimation:IPW
[RosenbaumandRubin,1983]isacommonweightingtech-
SergeAssaad,ShuxiZeng,ChenyangTao,ShounakDatta,
niquefortreatmenteffectestimation.However,theweighted
Nikhil Mehta, Ricardo Henao, Fan Li, and Lawrence
estimatorbasedonIPWisoftennumericallyunstabledue
Carin. Counterfactualrepresentationlearningwithbal-
to the computation of the inverse of the propensity score.
ancingweights. InAISTATS,pages1972â€“1980,2021.
Oneremedyforthisissueisweighttruncation[Crumpetal.,
2009],which,however,causestheestimationbias,leading Susan Athey, Julie Tibshirani, and Stefan Wager. Gener-
toinaccuratetreatmenteffectestimation.
alizedrandomforests. AnnalsofStatistics,47(2):1148â€“
1178,2019.
Toimprovetheestimationperformance,Zhuetal.[2020]
haveemployedParetosmoothing[Vehtarietal.,2024].Al-
KeithBattocchi,EleanorDillon,MaggieHei,GregLewis,
thoughtheyempiricallyshowthatusingthistechniqueleads
Paul Oka, Miruna Oprescu, and Vasilis Syrgkanis.
tobetterperformancethanweighttruncation,theirmethod
EconML:APythonpackageforML-basedheterogeneous
isdevelopedfortheestimationofATE,notCATE. treatmenteffectsestimation. Version0.14.1,2019.
ApplyingParetosmoothinginweightedrepresentationlearn-
ing for CATE estimation is difficult because it prevents MathieuBlondel,OlivierTeboul,QuentinBerthet,andJosip
gradient backpropagation due to the non-differentiability. Djolonga. Fast differentiable sorting and ranking. In
This difficulty is disappointing, given that previous work ICML,pages950â€“959,2020.
hastheoreticallyshownthattheweightcorrectionschemes,
LÃ©on Bottou, Jonas Peters, Joaquin QuiÃ±onero-Candela,
suchasweighttruncation,helpstoextractpredictivefeature
Denis X Charles, D Max Chickering, Elon Portugaly,
representationsforCATEestimation[Assaadetal.,2021].
DipankarRay,PatriceSimard,andEdSnelson. Counter-
To establish a Pareto-smoothed weighting framework for factualreasoningandlearningsystems:Theexampleof
CATE estimation from high-dimensional data, we have computationaladvertising. JMLR,14(11),2013.
shownthathowdifferentiablerankingtechnique[Blondel
MAlanBrookhart,SebastianSchneeweiss,KennethJRoth-
etal.,2020]canbeusedtosimultaneouslylearnthepropen-
man,RobertJGlynn,JerryAvorn,andTilStÃ¼rmer. Vari-
sityscoremodelandthefeaturerepresentations.
able selection for propensity score models. American
JournalofEpidemiology,163(12):1149â€“1156,2006.
MingyuanCheng,XinruLiao,QuanLiu,BinMa,JianXu,
andBoZheng. Learningdisentangledrepresentationsfor
counterfactual regression via mutual information mini-
6 CONCLUSION mization. InProceedingsofthe45thInternationalACM
SIGIRConferenceonResearchandDevelopmentinIn-
formationRetrieval,pages1802â€“1806,2022.
In this paper, we have established a differentiable Pareto-
smoothedweightingframeworkforCATEestimationfrom Victor Chernozhukov, Denis Chetverikov, Mert Demirer,
high-dimensionaldata.ToconstructaCATEestimatorthat Esther Duflo, Christian Hansen, Whitney Newey, and
isnumericallyrobusttopropensityscoreestimationerror, James Robins. Double/debiased machine learning for
wemaketheweightcorrectionprocedureinParetosmooth-
treatmentandstructuralparameters:Double/debiasedma-
ingdifferentiableandincorporateitintotheweightedrep- chinelearning. EconometricsJournal,21(1),2018.
resentation learning approach for CATE estimation. Ex-
YoichiChikahara,MakotoYamada,andHisashiKashima.
perimental results show that our framework successfully
Featureselectionfordiscoveringdistributionaltreatment
outperformstraditionalweightingschemes,aswellasthe
effectmodifiers. InUAI,pages400â€“410,2022.
existingCATEestimationmethods.
Leveragingtheversatilityofweighting,ourfutureworkwill JiebinChu,YaoyunZhang,FeiHuang,LuoSi,Songfang
investigatehowtoextendourframeworktoestimatetheef- Huang,andZhengxingHuang. Disentangledrepresenta-
fectsofsuchcomplextreatmentashigh-dimensionalbinary tionforsequentialtreatmenteffectestimation. Computer
treatment[Zouetal.,2020],continuous-valuedtreatment Methods and Programs in Biomedicine, 226:107175,
[Wangetal.,2022],timeseriestreatment[Limetal.,2018]. 2022.
9Djork-ArnÃ©Clevert,ThomasUnterthiner,andSeppHochre- KatherineKeith,DavidJensen,andBrendanOâ€™Connor.Text
iter. Fastandaccuratedeepnetworklearningbyexponen- andcausalinference:Areviewofusingtexttoremove
tiallinearunits(ELUs). InICLR,2016. confoundingfromcausalestimates. InACL,pages5332â€“
5344,2020.
RichardKCrump,VJosephHotz,GuidoWImbens,and
Oscar A Mitnik. Dealing with limited overlap in esti- Diederik Kingma and Jimmy Ba. Adam: A method for
mationofaveragetreatmenteffects. Biometrika,96(1): stochasticoptimization. InICLR,2015.
187â€“199,2009.
KunKuang,PengCui,BoLi,MengJiang,ShiqiangYang,
AliciaCurthandMihaelavanderSchaar. Nonparametrices- and Fei Wang. Treatment effect estimation with data-
timationofheterogeneoustreatmenteffects:Fromtheory driven variable decomposition. In AAAI, volume 31,
tolearningalgorithms. InAISTATS,pages1810â€“1818, 2017.
2021.
KunKuang,PengCui,HaoZou,BoLi,JianrongTao,Fei
Alicia Curth, Richard W Peck, Eoin McKinney, James Wu,andShiqiangYang. Data-drivenvariabledecomposi-
Weatherall, and Mihaela van Der Schaar. Using ma- tionfortreatmenteffectestimation. IEEETransactions
chinelearningtoindividualizetreatmenteffectestimation: onKnowledgeandDataEngineering,34(5):2120â€“2134,
Challengesandopportunities. ClinicalPharmacology& 2020.
Therapeutics,2024.
RavinKumar,ColinCarroll,AriHartikainen,andOsvaldo
MarcoCuturi,OlivierTeboul,andJean-PhilippeVert. Dif- Martin. ArviZaunifiedlibraryforexploratoryanalysis
ferentiablerankingandsortingusingoptimaltransport. ofBayesianmodelsinPython. JournalofOpenSource
InNeurIPS,pages6861â€“6871,2019. Software,4(33):1143,2019.
ZijunGao,TrevorHastie,andRobertTibshirani. Assess- ShengLiandYunFu. Matchingonbalancednonlinearrep-
mentofheterogeneoustreatmenteffectestimationaccu- resentationsfortreatmenteffectsestimation. InNeurIPS,
racyviamatching. StatisticsinMedicine,40(17):3990â€“ pages930â€“940,2017.
4013,2021.
BryanLim,AhmedAlaa,andMihaelavanderSchaar. Fore-
ArthurGretton,KarstenM.Borgwardt,MalteJRasch,Bern- casting treatment responses over time using recurrent
hard SchÃ¶lkopf, and Alexander Smola. A kernel two- marginalstructuralnetworks. InNeurIPS,pages7494â€“
sampletest. JMLR,13(1):723â€“773,2012. 7504,2018.
Negar Hassanpour and Russell Greiner. Counterfactual Yu Liu, Jian Wang, and Bing Li. EDVAE: Disentangled
regressionwithimportancesamplingweights. InIJCAI, latentfactorsmodelsincounterfactualreasoningforindi-
pages5880â€“5887,2019. vidualtreatmenteffectsestimation. InformationSciences,
652:119578,2024.
Negar Hassanpour and Russell Greiner. Learning disen-
MarianFMacDormanandJonnaeOAtkinson. Infantmor-
tangledrepresentationsforcounterfactualregression. In
tality statistics from the linked birth/infant death data
ICLR,2020.
setâ€“1995perioddata. 1998.
Jennifer L. Hill. Bayesian nonparametric modeling for
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,
causalinference. JournalofComputationalandGraphi-
JamesBradbury,GregoryChanan,TrevorKilleen,Zem-
calStatistics,20(1):217â€“240,2011.
ingLin,NataliaGimelshein,LucaAntiga,etal. PyTorch:
ShunsukeHoriiandYoichiChikahara. Uncertaintyquantifi- Animperativestyle,high-performancedeeplearningli-
cationinheterogeneoustreatmenteffectestimationwith
brary. InNeurIPS,volume32,2019.
gaussian-process-basedpartiallylinearmodel. InAAAI,
Fabian Pedregosa, GaÃ«l Varoquaux, Alexandre Gramfort,
volume38,pages20420â€“20429,2024.
VincentMichel,BertrandThirion,OlivierGrisel,Mathieu
JonathanRMHoskingandJamesRWallis. Parameterand Blondel,PeterPrettenhofer,RonWeiss,VincentDubourg,
quantileestimationforthegeneralizedparetodistribution. etal. Scikit-learn:MachinelearninginPython. JMLR,
Technometrics,29(3):339â€“349,1987. 12(Oct):2825â€“2830,2011.
EdwardLIonides. Truncatedimportancesampling. Journal PaulR.RosenbaumandDonaldB.Rubin. Thecentralrole
of Computational and Graphical Statistics, 17(2):295â€“ ofthepropensityscoreinobservationalstudiesforcausal
311,2008. effects. Biometrika,70(1):41â€“55,1983.
FredrikJohansson,UriShalit,andDavidSontag. Learning DonaldB.Rubin. Estimatingcausaleffectsoftreatments
representations for counterfactual inference. In ICML, inrandomized andnonrandomizedstudies. Journal of
pages3020â€“3029,2016. EducationalPsychology,66(5):688,1974.
10Brian C Sauer, M Alan Brookhart, Jason Roy, and Tyler FujinZhu,JieLu,AdiLin,andGuangquanZhang.APareto-
VanderWeele. Areviewofcovariateselectionfornon- smoothingmethodforcausalinferenceusinggeneralized
experimentalcomparativeeffectivenessresearch. Phar- paretodistribution. Neurocomputing,378:142â€“152,2020.
macoepidemiologyandDrugSafety,22(11):1139â€“1145,
Hao Zou, Peng Cui, Bo Li, Zheyan Shen, Jianxin Ma,
2013.
HongxiaYang,andYueHe. Counterfactualprediction
UriShalit. Canwelearnindividual-leveltreatmentpolicies forbundletreatment. InNeurIPS,pages19705â€“19715,
fromclinicaldata? Biostatistics,21(2):359â€“362,2020. 2020.
UriShalit,FredrikD.Johansson,andDavidSontag. Esti-
matingindividualtreatmenteffect:Generalizationbounds
andalgorithms. InICML,pages3076â€“3085,2017.
Yishai Shimoni, Chen Yanover, Ehud Karavani, and
Yaara Goldschmnidt. Benchmarking framework for
performance-evaluation of causal inference analysis.
arXivpreprintarXiv:1802.05046,2018.
Wei Sun, Pengyuan Wang, Dawei Yin, Jian Yang, and
Yi Chang. Causal inference via sparse additive mod-
elswithapplicationtoonlineadvertising. InAAAI,vol-
ume29,2015.
AkiVehtari,DanielSimpson,AndrewGelman,YulingYao,
andJonahGabry. Paretosmoothedimportancesampling.
JMLR,2024.
Haotian Wang, Kun Kuang, Haoang Chi, Longqi Yang,
Mingyang Geng, Wanrong Huang, and Wenjing Yang.
Treatmenteffectestimationwithadjustmentfeaturese-
lection. InKDD,pages2290â€“2301,2023.
Pengyuan Wang, Wei Sun, Dawei Yin, Jian Yang, and
YiChang. Robusttree-basedcausalinferenceforcom-
plexadeffectivenessanalysis. InWSDM,pages67â€“76,
2015.
Xin Wang, Shengfei Lyu, Xingyu Wu, Tianhao Wu, and
HuanhuanChen. Generalizationboundsforestimating
causaleffectsofcontinuoustreatments.InNeurIPS,pages
8605â€“8617,2022.
AnpengWu,JunkunYuan,KunKuang,BoLi,RunzeWu,
QiangZhu,YuetingZhuang,andFeiWu. Learningde-
composedrepresentationsfortreatmenteffectestimation.
IEEETransactionsonKnowledgeandDataEngineering,
35(5):4989â€“5001,2022.
JinsungYoon,JamesJordon,andMihaelaVanDerSchaar.
GANITE:Estimationofindividualizedtreatmenteffects
usinggenerativeadversarialnets. InICLR,2018.
JiamingZeng,MichaelFGensheimer,DanielLRubin,Su-
sanAthey,andRossDShachter.Uncoveringinterpretable
potentialconfoundersinelectronicmedicalrecords. Na-
tureCommunications,13(1):1014,2022.
QingyuanZhao,DylanS.Small,andAshkanErtefaie. Se-
lective inference for effect modification via the lasso.
Journal of Royal Statistical Society: Series B (Statisti-
calMethodology),84(2):382â€“413,2022.
11Supplementary Materials for
"Differentiable Pareto-Smoothed Weighting
for High-Dimensional Heterogeneous Treatment Effect Estimation"
YoichiChikahara1 KanseiUshiyama2*
1NTTCommunicationScienceLaboratories,Kyoto,Japan
2TheUniversityofTokyo,Tokyo,Japan
A RANKFUNCTIONDEFINITION
InSection3.3,weconsiderrankfunctionr,whichtakesinputvectorw=[w ,...,w ]âŠ¤andoutputstherankofeachelement
1 n
inw.Weformallydefinethisrankfunctionbasedonthesortingoperationandtheconceptofinversepermutation.
Considerasortingoperationoverw ,...,w inwâˆˆRnthatfindsapermutationÏ=[Ï ,...,Ï ]âŠ¤suchthatthevectorvalues
1 n 1 n
permutedaccordingtoÏ,w =[w ,...,w ]âŠ¤,areincreasingasw â‰¤Â·Â·Â·â‰¤w .LetÏâˆ’1betheinverseofpermutationÏ,
Ï Ï1 Ïn Ï1 Ïn
i.e.,apermutationwhoseÏ-thelementisifori=1,...,n.
i
Thentherankingfunctionisdefinedastheinverseofthesortingpermutation:
r(w)=Ïâˆ’1(w). (21)
Throughoutthispaper,weconsiderrankfunctionr(w)thatevaluatesthepositionofeachw basedonthesortinginan
i
ascendingorder.Ifweneedtoconsidertherankingindescendingorder,wecanformulateitasr(âˆ’w).
B EXPERIMENTALDETAILS
B.1 SETTINGSOFBASELINES
Regarding linear regression methods (LR-1 and LR-2), to avoid overfitting due to the large number of input features,
weemployedtheridgeregressionmodelinscikit-learn[Pedregosaetal.,2011].Forthemeta-learnermethods(i.e.,the
S-Learner(SL),theT-Learner(TL),theX-Learner(XL),theDR-Learner(DRL))andthetree-basedmethods(CFand
CFDML)weusedtheEconMLPythonpackage[Battocchietal.,2019].AsthebaselearnersforSL,TL,XL,andDRL,
wechoserandomforestbecauseweempiricallyobservedthatitachievedthebestperformanceamongthethreemodel
candidates,randomforest,gradientboosting,andsupportvectormachine.
Weevaluatedtheperformanceofneuralnetworkmethods(TARNetandGANITE),usingtheexistingimplementations
[CurthandvanderSchaar,2021,Yoonetal.,2018].12AsregardstheDRCFRmethod,weimplementeditusingPyTorch
[Paszkeetal.,2019].
WithPSW,wetrainedpropensityscoremodelÏ€(X)beforehand,usingapairedsample{(a,x)}âŠ‚D.Hereweformulated
i i
Ï€(X)usingthethree-layeredFNN,aswithourmethodandDRCFR.AfterhavingcomputedtheIPWweightsbasedonthe
trainedpropensityscoremodel,weperformedParetosmoothingoverthem,usingpsislw()functioninthePythonpackage
namedArviZ[Kumaretal.,2019].UsingthePareto-smoothedweights,welearnedthetwoencodersâˆ†(X)andÎ¥(X),as
wellasoutcomepredictionmodelsh0andh1.
1https://github.com/AliciaCurth/CATENets
2https://github.com/jsyoon0823/GANITE
Acceptedforthe40thConferenceonUncertaintyinArtificialIntelligence (UAI2024).B.2 SYNTHETICDATA
FollowingHassanpourandGreiner[2020],wepreparedthesyntheticdatasets.3
WefirstdrawnthevaluesoffeaturesX âˆˆRd fromstandardmultivariateGaussiandistribution:
X âˆ¼N(0,I), (22)
whereN denotestheGaussiandistribution.
Then,wesampledthevaluesoftreatment AandoutcomeY usingXÎ“ âˆˆ Rd/3,Xâˆ† âˆˆ Rd/3,andXÎ¥ âˆˆ Rd/3,whicharethe
featuresubsetsinX =[XÎ“,Xâˆ†,XÎ¥]âŠ¤.Inparticular,weemployedÏˆ=[XÎ“,Xâˆ†]âŠ¤ âˆˆR2d/3,togenerateAâ€™svaluesas
(cid:32) (cid:33)
1
Aâˆ¼Ber , (23)
1+exp(âˆ’c Â·(Ïˆ+1))
A
wherec âˆˆR2d/3isthecoefficientvectordrawnfromN(0,1),and1=[1,...,1]âŠ¤isavectorwithlength2d/3.Bycontrast,
A
weusedÏ•=[Xâˆ†,XÎ¥]âŠ¤ âˆˆR2d/3tosimulatepotentialoutcomesY0andY1as
3
Y0 = c Â·Ï•+Ïµ (24)
2d
Y0
3
Y1 = c Â·(Ï•âŠ™Ï•)+Ïµ, (25)
2d
Y1
(26)
wherec âˆˆR2d/3andc âˆˆR2d/3arethecoefficientvectorsdrawnfromN(0,1),symbolâŠ™denoteselement-wiseproduct
Y0 Y1
(a.k.a.,Hadamardproduct),andÏµ âˆ¼N(0,1)isascalarstandardGaussiannoise.UsingthevaluesofpotentialoutcomesY0
andY1,wecomputedthevaluesofoutcomeY byY = AY1+(1âˆ’A)Y0.
C ADDITIONALEXPERIMENTALRESULTS
In Section 4.2, as with Hassanpour and Greiner [2020], we present the performance on the relatively low-dimensional
syntheticdata.Tofurtherinvestigatetheperformanceofourmethod,weperformedadditionalexperimentsusinghigh-
dimensionalsyntheticdata.
Here,wegeneratedthesyntheticdatainthesameway(asdescribedinAppendixB.2)exceptthatthenumberoffeatured
wassettod =600,1200,...,3000.
Figure4showstheresults.Again,ourmethodachievedbetterCATEestimationperformancethanDRCFRandTARNet,
thusshowingthatitcansuccessfullyimprovetheestimationperformanceofDRCFRbyeffectivelycorrectingtheIPW
weightsviadifferentiableParetosmoothing.
Regardingthefeaturerepresentationlearningperformance(Figure4(a)-(c)),bothDRCFRandourmethodworkedbetter
thanTARNet.AsillustratedinFigure4(a),however,withDRCFRandourmethod,thedifferenceamongtheabsolute
parametervaluesinlearnedencoderÎ“(X)decreased,asthenumberoffeaturesdincreased,indicatingthatdistinguishing
therepresentationofinstrumentalvariablesÎ“(X)fromtheoneofconfoundersâˆ†(X)wasdifficultforbothmethods.This
difficultyispartlybecausebinarytreatmentAcanbeaccuratelypredictedsolelyfromtherepresentationofconfounders(i.e.,
âˆ†(X))whenthereareasufficientnumberofconfoundersXâˆ†.
Apossiblesolutionistoenforcetheindependencebetweenfeaturerepresentationsusinganadditionalmutualinformation
regularizer,aswithChengetal.[2022].OurfutureworkconstitutestheevaluationofsuchavariantofourmethodforCATE
estimationperformanceunderhigh-dimensionalsetups.
3Weusedtheirimplementationinhttps://www.dropbox.com/sh/vrux2exqwc9uh7k/AAAR4tlJLScPlkmPruvbrTJQa?dl=0.
13(a) Relative difference between |W1| and |W1 | (b) Relative difference between |W1| and |W1 |
ğšª âˆ’ğšª âˆ† âˆ’âˆ†
Number of features d Number of features d
(c)Relative difference between |W1| and |W1 | (d) Test PEHE
ğš¼ âˆ’ğš¼
Number of features d Number of features d
Figure4:LearnedencoderparameterdifferencesandtestPEHEsonsyntheticdata.(a):ValuedifferenceofW1inencoder
Î“(X);(b):ValuedifferenceofW1 inencoderâˆ†(X);(c):ValuedifferenceofW1 inencoderÎ¥(X);(d)TestPEHEs.With
TARNet,sinceitlearnssingleencoder,wecomputedallparametervaluedifferenceswithweightmatrixinsameencoder.
14