1
DreamWaltz-G: Expressive 3D Gaussian
Avatars from Skeleton-Guided 2D Diffusion
Yukun Huang, Jianan Wang, Ailing Zeng, Member, IEEE, Zheng-Jun Zha, Member, IEEE,
Lei Zhang, Fellow, IEEE, Xihui Liu(cid:0) Member, IEEE
Abstract‚ÄîLeveragingpretrained2Ddiffusionmodelsandscoredistillationsampling(SDS),recentmethodshaveshownpromising
resultsfortext-to-3Davatargeneration.However,generatinghigh-quality3Davatarscapableofexpressiveanimationremains
challenging.Inthiswork,wepresentDreamWaltz-G,anovellearningframeworkforanimatable3Davatargenerationfromtext.The
coreofthisframeworkliesinSkeleton-guidedScoreDistillationandHybrid3DGaussianAvatarrepresentation.Specifically,the
proposedskeleton-guidedscoredistillationintegratesskeletoncontrolsfrom3Dhumantemplatesinto2Ddiffusionmodels,enhancing
theconsistencyofSDSsupervisionintermsofviewandhumanpose.Thisfacilitatesthegenerationofhigh-qualityavatars,mitigating
issuessuchasmultiplefaces,extralimbs,andblurring.Theproposedhybrid3DGaussianavatarrepresentationbuildsontheefficient
3DGaussians,combiningneuralimplicitfieldsandparameterized3Dmeshestoenablereal-timerendering,stableSDSoptimization,
andexpressiveanimation.ExtensiveexperimentsdemonstratethatDreamWaltz-Gishighlyeffectiveingeneratingandanimating3D
avatars,outperformingexistingmethodsinbothvisualqualityandanimationexpressiveness.Ourframeworkfurthersupportsdiverse
applications,includinghumanvideoreenactmentandmulti-subjectscenecomposition.Formorevivid3Davatarandanimationresults,
pleasevisithttps://yukun-huang.github.io/DreamWaltz-G/.
IndexTerms‚Äî3Davatargeneration,3Dhuman,expressiveanimation,diffusionmodel,scoredistillation,3DGaussians.
‚ú¶
1 INTRODUCTION
ANIMATABLE 3D avatar generation is essential for a for data collection. However, creating 3D avatars using a
wide range of applications, such as film and car- 2Ddiffusionmodelremainschallenging.First,staticavatars
toon production, video game design, and immersive me- requirearticulatedstructureswithintricateparts(e.g.,hands
dia such as virtual/augmented reality. Traditional tech- andfaces)anddetailedtextures,whichpretraineddiffusion
niques for creating such intricate 3D avatars are costly and modelsandscoredistillationstruggletogenerate.Secondly,
time-consuming,requiringthousandsofhoursfromskilled dynamic avatars assume various poses in a coordinated and
artists with extensive aesthetics and 3D modeling knowl- constrained manner, where changes in shape and appear-
edge.Meanwhile,theadvancementof3Dreconstruction[1], ance should be realistic without artifacts caused by inac-
[2], [3], [4] has enabled promising methods which can re- curate skeleton rigging. Although previous methods [22],
construct 3D human models from monocular images [5], [23],[24],[25],[26],[27],[28]havedemonstratedimpressive
[6], [7], [8], [9], monocular videos [10], [11], [12], [13], or results on text-driven 3D avatar creation, they still struggle
3D scans [14], [15], [16], [17]. Nonetheless, these methods with producing intricate geometric structures and detailed
relyheavilyonthecollectionofimage/videodatacaptured appearances,letaloneforrealisticanimation.
with a monocular camera or a synchronized camera array. In this paper, we present DreamWaltz-G, a zero-shot
Thismakesthemunsuitableforgenerating3Davatarsfrom learning framework for text-driven 3D avatar generation.
imaginativebutabstractpromptsliketexts. At the core of this framework are Skeleton-guided Score
Recently, integrating pretrained text-to-image diffusion Distillation (SkelSD) and Hybrid 3D Gaussian [4] Avatars
models [18], [19] into 3D modeling with score distillation (H3GA)forstableoptimizationandexpressiveanimation.
sampling(SDS)[20],[21]hasgainedsignificantattentionto For SkelSD, different from previous methods [24], [25],
make 3D digitization more accessible, alleviating the need [26] that only apply human priors to 3D avatar represen-
tations (e.g., 3D mesh [24]), we additionally inject human
priors into diffusion model through skeleton control [29],
‚Ä¢ Y. Huang and X. Liu are with The University of Hong Kong (HKU),
[30], leading to a more stable SDS that conforms to the 3D
HongKongSAR999077,China.
E-mail:yukun@hku.hk,xihuiliu@eee.hku.hk human body structure. This design brings three benefits:
‚Ä¢ J.WangiswithAstribot,Shenzhen518063,China. (1) skeleton guidance from 3D human templates [31], [32]
E-mail:jiananwang@astribot.com
enhancesthe3DconsistencyofSDSandpreventstheJanus
‚Ä¢ A.ZengiswithTencent,Shenzhen518054,China.
E-mail:ailingzengzzz@gmail.com (multi-face) problem; (2) it eliminates pose uncertainty of
‚Ä¢ Z.ZhaiswithUniversityofScienceandTechnologyofChina(USTC), SDSandavoidsdefectssuchasextralimbsandghosting;(3)
Hefei230026,China. randomlyposedskeletonguidanceenablespose-dependent
E-mail:zhazj@ustc.edu.cn
shapeandappearancelearningfrom2Ddiffusionmodel.
‚Ä¢ L. Zhang is with International Digital Economy Academy (IDEA),
Shenzhen518045,China. H3GA is a hybrid 3D representation for animatable
E-mail:leizhang@idea.edu.cn 3D avatars, specifically designed to adapt SDS optimiza-
(cid:0):Correspondingauthor. tion and enable expressive animation. Specifically, H3GA
4202
peS
52
]VC.sc[
1v54171.9042:viXraExpressive Animation Shape Editing 2
Human Video Reenactment Multi-subject Scene Composition
Fig. 1. We present DreamWaltz-G, a text-driven animatable 3D avatar generation framework, which can create high-quality 3D avatars from
imaginativetextpromptsandanimatethemgivenmotionsequenceswithoutmanualriggingandretraining.Ourmethodenablesvariousdownstream
applications,suchasexpressiveanimationproduction,shapeediting,humanvideoreenactment,andmulti-subjectscenecomposition.
combines the efficiency of 3D Gaussian Splatting [4], the ‚Ä¢ We propose H3GA, a hybrid 3D Gaussian avatar
local continuity of neural implicit fields [1], [2], and the representation that enables stable SDS optimization,
geometric accuracy of parameterized meshes [31], [32]. As real-time rendering, and expressive animation with
a result, H3GA supports real-time rendering, is robust to fingermovementsandfacialexpressions.
SDS optimization, and enables expressive animation with ‚Ä¢ Experiments demonstrate that DreamWaltz-G can
fingermovementsandfacialexpressions.Furthermore,con- effectively create animatable 3D avatars, achieving
sideringthedynamiccharacteristicsofdifferentbodyparts, superiorgenerationandanimationqualitycompared
we designed a dual-branch deformation strategy to drive toexistingtext-to-3Davatarmethods.
canonical3DGaussiansforrealisticanimation.
Comparedwiththepreliminaryconferenceversion[28],
BasedontheproposedSkelSDandH3GA,DreamWaltz-
thisworkintroducesseveralnon-trivialimprovements.The
Ggeneratesanimatable3Davatarsintwotrainingstages:
most significant enhancement is the redesign of 3D avatar
(I) Canonical Avatar Generation. For Stage I, we aim
representation. Specifically, DreamWaltz [28] uses Instant-
to create a canonical 3D avatar given text descriptions.
NGP[33]formodeling3Davatars.However,whenapplied
Specifically, we employ Instant-NGP [33] as the canonical
todynamicavatarswithdeformation,high-resolutionsam-
avatarrepresentationandoptimizeitwithSkelSDforshape
pling combined with inverse LBS [31] becomes computa-
and appearance learning, where the skeleton guidance is
tionally expensive and impractical for training. To address
extractedfromSMPL-X[32]inthecanonicalpose.
this, DreamWaltz-G adopts a novel hybrid 3D Gaussian
(II) Animatable Avatar Learning. For Stage II, we aim to
representation, benefiting from efficient deformation and
make the canonical avatar from Stage I rigged to SMPL-
rendering of 3DGS [4] while remaining compatible with
X and accurately animated. We employ H3GA as the ani-
SDSoptimizationandSMPL-Xparameters.Additionally,we
matableavatarrepresentationforefficientdeformationand
replace the used 3D human parametric model SMPL [31]
stable optimization. Similar to Stage I, we use SkelSD for
withSMPL-X[32],introducelocalgeometricconstraintsfor
pose-dependentshapeandappearancelearning,exceptthe
NeRFtraining,andexploremorepotentialapplications.
skeleton guidance is extracted from SMPL-X in randomly
sampledplausibleposes.
Insummary,ourframeworklearnsahybrid3DGaussian 2 RELATED WORK
avatar representation using skeleton-guided score distilla-
We first review the previous methods for 2D diffusion
tion, ready for expressive animation and a wide range of
models and then discuss recent advances in text-driven 3D
applications,asillustratedinFigure1.Thekeycontributions
objectand3Davatargeneration.
ofthisworklieinfourmainaspects:
‚Ä¢ We introduce a text-driven animatable 3D avatar
2.1 Text-drivenImageGeneration
generationframework,i.e.,DreamWaltz-G,readyfor
expressiveanimationandvariousapplications. Recently, there have been significant advancements in text-
‚Ä¢ We propose SkelSD, a novel skeleton-guided score to-image models such as GLIDE [34], unCLIP [18], Ima-
distillation strategy to reduce the view and pose gen [35], and Stable Diffusion [19], which enable the gen-
inconsistencies between the 3D avatar‚Äôs rendering erationofhighlyrealisticandimaginativeimagesbasedon
andthe2Ddiffusionmodel‚Äôssupervision. textprompts.Thesegenerativecapabilitieshavebeenmade3
TABLE1
Comparisonsofdifferenttext-driven3Davatargenerationmethods.Toclarify,ShapeControlreferstospecifyingtheavatar‚Äôsshapeduring
generationinsteadoftheshapeinitialization‚Ä†,whileShapeEditinginvolvesadjustingtheavatar‚Äôsshapeaftergeneration.
Methods 3DModel BodyAnimation HandAnimation FaceAnimation ShapeControl ShapeEditing
DreamHuman[25] NeRF ‚úì ‚úï ‚úï ‚úï ‚úï
DreamWaltz[28] NeRF ‚úì ‚úï ‚úï ‚úì ‚úï
TADA‚Ä†[24] Mesh ‚úì ‚úì ‚úì ‚úï ‚úì
HumanGaussian[27] 3DGS ‚úì ‚úï ‚úï ‚úì ‚úì
GAvatar[26] 3DGS ‚úì ‚úï ‚úï ‚úï ‚úì
DreamWaltz-G(Ours) 3DGS ‚úì ‚úì ‚úì ‚úì ‚úì
possible by advancements in modeling, such as diffusion DreamWaltz[28]andAvatarVerse[62]furtherutilizesCon-
models[36],[37],[38],andtheavailabilityoflarge-scaleweb trolNet[29]andSMPL[31]toprovideview/pose-consistent
data containing billions of image-text pairs [39], [40], [41]. 2D human guidance such as skeleton and DensePose [63].
These datasets encompass a wide range of general objects, Considering the limited 3D awareness of 2D diffusion
with significant variations in color, texture, and camera models, HumanNorm [64] proposes the normal-adapted
viewpoints, providing pre-trained models with a compre- anddepth-adapteddiffusionmodelsforaccurategeometry
hensive understanding of general objects and enabling the generation. In addition, to enable animatable avatar learn-
synthesis of high-quality and diverse objects. Furthermore, ing, DreamHuman [25] employs implicit 3D human model
recent works [29], [30], [42], [43] have explored incorpo- imGHUM[65]as3Davatarrepresentation,whichimproves
rating additional conditioning, such as depth maps and the dynamic visual quality of generated avatars. Recently,
humanskeletonposes,togenerateimageswithmoreprecise 3DGaussianSplatting(3DGS)[4]hasemergedasanexplicit
control. With more advanced network architectures [44], 3D representation enabling real-time deformation [66] and
[45], [46] and larger, higher-quality datasets [47], [48], the rendering. Some works [14], [16], [26], [27], [67], [68] have
capabilities of text-to-image generation models continue to exploredusing3DGStorepresent3Davatars.HumanGaus-
improve. sian [27] proposes a Structure-Aware SDS, which guides
the adaptive density control of 3DGS with intrinsic human
structures.GAvatar[26]introducesaprimitive-based3DGS
2.2 Text-driven3DObjectGeneration
representationwhere3DGaussiansaredefinedinsidepose-
DreamFields[49]andCLIPmesh[50]weregroundbreaking
drivenprimitivestofacilitateanimation.
in their utilization of CLIP [51] to optimize an underlying
To highlight our contributions, we summarize the key
3D representation, aligning its 2D renderings with user-
differencesbetweenourworkandrelatedworksinTable1.
specifiedtextpromptswithoutnecessitatingcostly3Dtrain-
ingdata.However,thisapproachtendstoresultinlessreal-
3 METHOD
istic3DmodelssinceCLIPonlyprovidesdiscriminativesu-
pervisionforhigh-levelsemantics.Incontrast,recentworks We first review some preliminary knowledge in Sec. 3.1,
havedemonstratedremarkabletext-to-3Dgenerationresults then present the proposed Skeleton-guided Score Distillation
byemployingpowerfultext-to-imagediffusionmodelsasa in Sec. 3.2 and Hybrid 3D Gaussian Avatar Representation
robust 2D prior for optimizing a differentiable 3D repre- in Sec. 3.3. Finally, we introduce the text-driven 3D avatar
sentation with Score Distillation Sampling (SDS) [20], [21], generationframeworkDreamWaltz-GinSec.3.4.
[52],[53],[54].Nonetheless,thehighvariationinSDSleads
toblurriness,over-saturatedcolors,and3Dinconsistencies. 3.1 Preliminary
Although a series of subsequent works [55], [56], [57], [58],
Beforedelvingintoourproposedmethod,wefirstintroduce
[59], [60] have introduced fundamental improvements to
someconceptsthatformthebasisofourframework.
SDS optimization, the results remain unsatisfactory when
3DGaussianSplatting(3DGS)[4]representsa3Dscene
applied to generating animatable 3D avatars with intricate
through a set of 3D Gaussians G = {G i | i = 1,...,N}.
details.
The geometry of each 3D Gaussian G i is parameterized by
a position (mean) p i ‚àà R3√ó1 and covariance matrix Œ£ i ‚àà
2.3 Text-driven3DAvatarGeneration R3√ó3 definedinworldspace:
Different from everyday objects, 3D avatars have de- G i(x)=e‚àí 21(x‚àípi)TŒ£‚àí i1(x‚àípi),
tailed textures and intricate geometric structures that can
be driven for realistic animation. Avatar-CLIP [22] em- where x is a 3D point in world coordinates. To maintain
ploys CLIP [51] for shape sculpting and texture genera- the position semi-definite property of Œ£ i, a decomposition
tion but tends to produce less realistic and oversimpli- is used: Œ£ i = R iS iST i RT i , where the scaling matrix S and
fied 3D avatars. Unlike CLIP-based methods, both Avatar- the rotation matrix R are parameterized by a 3D vector s
Craft [23] and DreamAvatar [61] leverage powerful text-to- andaquaternionqforgradientdescent.
image diffusion models to provide 2D image guidance, ef- To render an image, the 3D Gaussians can be projected
fectivelyimprovingthevisualqualityofgeneratedavatars. to 2D using: Œ£‚Ä≤ = JWŒ£WTJT, where W is a viewing4
transformation from world to camera coordinates, and J human motion representation ability, SMPL-X has been
denotes the Jacobian of the affine approximation of the widely used in human motion-driven tasks [22], [72], [73].
projective transformation. We use G‚Ä≤ parameterized by Œ£‚Ä≤ The input parameters for SMPL-X include a 3D body joint
i
to represent the 2D Gaussian projected from G i. Finally, and global rotation Œæ ‚àà R(N j+1)√ó3, a body shape Œ≤ ‚àà R300,
the color c of each pixel x is rendered by alpha blending anda3Dglobaltranslationt‚ààR3.
accordingtothe3DGaussians‚Äôdepthorder1,...,N: Formally, a triangulated mesh T cnl(Œ≤,Œæ) ‚àà RN v√ó3 in
canonical pose is constructed by combining the template
N i‚àí1
c(x)=(cid:88) c Œ± G‚Ä≤(x)(cid:89) (1‚àíŒ± G‚Ä≤(x)), shapeT¬Ø ,theshape-dependentdeformationsB S(Œ≤),andthe
i=1
i i i
j=1
j j pose-dependentdeformationsB P(Œæ)as,
whereŒ± i ‚àà[0,1]istheopacityofG i. T cnl(Œ≤,Œæ)=T¬Ø+B S(Œ≤)+B P(Œæ), (2)
Neural Radiance Field (NeRF) [1], [33] is commonly where B P(Œæ) is used to relieve artifacts in Linear Blend
used as the differentiable 3D representation for text-driven
Skinning (LBS) [74]. Then, the LBS function is employed to
3Dgeneration[20],[52],parameterizedbyatrainableMLP. transform the canonical mesh T (Œ≤,Œæ) into a triangulated
For rendering, a batch of rays r(k) = o+kd are sampled meshT (Œ≤,Œæ)intheobservedpcn ol seas,
based on the camera position o and direction d on a per- obs
pixel basis. The MLP takes r(k) as input and predicts T obs(Œ≤,Œæ)=LBS(T cnl(Œ≤,Œæ),J(Œ≤),Œæ,W lbs), (3)
densityœÑ andcolorc.Thevolumerenderingintegralisthen
whereJ(Œ≤) ‚àà RN j√ó3 denotesthe correspondingjoint posi-
approximatedusingnumericalquadraturetoyieldthefinal tions,andW lbs ‚ààRN v√óN j isasetofblendweights.
coloroftherenderedpixel:
CÀÜ
(r)=(cid:88)Nc
‚Ñ¶ ¬∑(1‚àíexp(‚àíœÑ Œ¥ ))c ,
3.2 SkelSD:Skeleton-GuidedScoreDistillation
c i i i i
Vanilla score distillation methods [20], [21] utilize view-
i=1
dependentpromptaugmentationssuchas‚Äúfrontviewof...‚Äù
where N c is the number of sampled points on a ray, ‚Ñ¶ i = for diffusion model to provide crucial 3D view-consistent
exp(‚àí(cid:80)i j‚àí =1 1œÑ jŒ¥ j) is the accumulated transmittance, and Œ¥ i supervision.However,thispromptingstrategycannotguar-
isthedistancebetweenadjacentsamplepoints.
antee precise view consistency, leaving the disparity be-
Diffusionmodels[38],[69]whichhavebeenpre-trained
tween the viewpoint of the diffusion model‚Äôs supervision
on extensive image-text datasets [18], [35], [70] provide a
image and the 3D avatar‚Äôs rendering image unresolved.
robust image prior for supervising text-to-3D generation.
Such inconsistency causes quality issues for 3D generation,
Diffusion models learn to estimate the denoising score
suchasblurrinessandtheJanus(multi-face)problem.
‚àá xlogp data(x)byaddingnoisetocleandatax‚àºp(x)(for-
wardprocess)andlearningtoreversetheaddednoise(back-
Occlusion-Aware Skeleton Extraction
ward process). Noising the data distribution to isotropic
Gaussian is performed in T timesteps, with a pre-defined
noising schedule Œ± t ‚àà (0,1) and Œ±¬Ø t := (cid:81)t s=1Œ± s, according
to:
‚àö ‚àö
Visible
x
t
= Œ±¬Ø tx+ 1‚àíŒ±¬Ø tœµ, whereœµ‚àºN(0,I).
In the training process, the diffusion models learn to esti-
Occluded
matethenoiseby SMPL-X
(cid:104) (cid:105)
L =E ‚à•œµ (x ,t)‚àíœµ‚à•2 . Condition ùëê
t x,œµ‚àºN(0,I) œï t 2
‚àáùêø (ùë•;ùë¶,ùëê)
cSDS
Once trained, one can estimate x from noisy input and the
correspondingnoiseprediction. ControlNet
Score Distillation (SDS) [20], [52], [71] is a technique
introducedbyDreamFusion[20]andextensivelyemployed
to distill knowledge from a pre-trained diffusion model œµ œï
into a differentiable 3D representation. For a NeRF model Naruto Uzumaki
parameterizedbyŒ∏,itsrenderingxcanbeobtainedbyx= Rendered
g(Œ∏)whereg isadifferentiablerenderer.SDScalculatesthe Image ùë• Noise Text Prompt ùë¶
gradientsofNeRFparametersŒ∏by,
Fig. 2. The proposed skeleton-guided score distillation utilizes 2D
‚àá Œ∏L SDS(œï,x)=E
t,œµ(cid:20)
w(t)(œµ œï(x
t;y,t)‚àíœµ)‚àÇ ‚àÇx xt‚àÇ ‚àÇx Œ∏(cid:21)
, (1)
s 2k Del de it fo fun sii om nag me os dc ele (x wtr ha ec rt eed wf ero am doS pM tP CL o- nX tro[3 lN2] et to [2c 9o ]n ),di wtio hn ichco en nt hro al nla cb ele
s
the view and pose consistencies between the rendered image x and
where w(t) is a weighting function that depends on the theSDSsupervision‚àÜL cSDS.Inaddition,weintroduceocclusionculling
to eliminate keypoints that are invisible from the current viewpoint,
timesteptandy denotesthegiventextprompt.
preventingambiguityforthediffusionmodel.
SMPL-X [32] is a unified parametric 3D human model
that extends SMPL [31] with fully articulated hands and Skeleton-guided Score Distillation (SkelSD). Inspired
an expressive face, containing N = 10,475 vertices and by recent works in controllable image generation [29], [30],
v
N = 54 joints. Benefiting from its efficient and expressive we propose SkelSD, which utilizes additional 3D-aware
j5
Hybrid 3D Gaussian positions colors, opacities
Instant-NGP Trainable Modules
Avatar (H3GA)
Non-Rigid
LBS
Deformation
Body Animation Splat
Mesh-binding Deformation
Parameterized
3D Meshes
Hands & Face
Canonical 3D Gaussians Observed 3D Rendered
Animation
(w/o colors and opacities) Gaussians Image
Fig.3.Theproposedhybrid3DGaussianavatarrepresentationintegratesefficient3DGaussianSplatting[4]withneuralimplicitfield(wherewe
adoptInstant-NGP[33])andparameterized3DmeshesofSMPL-X[32]bodyparts(e.g.,handsandface).Specifically,thecanonical3DGaussian
avatarisjointlyrepresentedbyunconstrained3DGaussiansG u andmesh-binding3DGaussiansG m boundtoparameterized3Dmeshes.The
colorsandopacitiesofbothG uandG marepredictedbytheneuralimplicitfield.Foranimation,G uandG maredeformedseparatelyandmergedto
formobserved3DGaussians,thensplattedtoobtaintherenderedavatarimage.
skeletonimagesfrom3Dhumantemplate[32]tocondition and results in extremely slow rendering and animation at
SDSforview/pose-consistentscoredistillation,asshownin highimageresolutions(e.g.,1024√ó1024).Toachievehigher
Figure 2. Specifically, the skeleton conditioning image c is training and inference efficiency, we adopt 3D Gaussian
injectedtoEquation1forSDSgradients,yielding: Splatting[4]astherepresentationfor3Davatars.
(cid:20) ‚àÇx ‚àÇx(cid:21) Specifically for diffusion-guided 3D avatar creation, we
‚àá Œ∏L cSDS(œï,x)=E t,œµ w(t)(œµ œï(x t;y,t,c)‚àíœµ) ‚àÇxt ‚àÇŒ∏ , review existing 3D Gaussian avatar representations [26],
[27] and propose several effective improvements for better
wheretheconditioningimageccanbeoneoracombination generationandanimationquality:
of skeletons, depth maps, normal maps, etc. In practice,
1) The high variance of score distillation gradients
we opt for skeletons as the conditioning type because they
makes optimizing millions of 3D Gaussians chal-
offer minimal human shape priors, thereby facilitating the
lenging, as illustrated in Figure 10. Thus, we use
generationofcomplexgeometries,asillustratedinFigure8.
pre-trained Instant-NGP [33] to initialize the 3D
In order to acquire 3D-aware skeleton images, we use the
Gaussiansandtopredictthe3DGaussianproperties
parametric 3D human model SMPL-X [32] for skeleton
forstableSDSoptimization.
rendering, where the skeleton image‚Äôs viewpoint is strictly
2) Considering that existing pre-trained 2D diffusion
alignedwiththeavatar‚Äôsrenderingviewpoint.
models struggle to generate intricate hands or con-
Occlusion Culling. The introduction of 3D-aware con-
trol facial expressions, we embed the learnable 3D
ditioning images can enhance the 3D consistency in the
meshesofSMPL-Xbodyparts(i.e.,handsandface)
SDS optimization process. However, the effectiveness is
into3DGaussianstoensureaccurategeometryand
constrained by the adopted diffusion model [29] on its
animationforthesebodyparts.
interpretation of the conditioning images. As shown in
3) To articulate 3D Gaussians for animation, we bind
Fig. 9 (a), we provide a back-view skeleton map as the
each 3D Gaussian to the SMPL-X joints by assign-
conditioning image to ControlNet [29] and perform text-
ing LBS weights and propose a geometry-aware
to-image generation. However, a frontal face still appears
smoothingalgorithmbasedonK-NearestNeighbors
in the generated image. Such defects bring problems such
(KNN)foradaptiveadjustments.
as multiple faces (the Janus problem) and unclear facial
4) We introduce a deformation network conditioned
features to 3D avatar generation. To this end, we propose
onhumanposetopredictthepose-dependentvari-
to use occlusion culling algorithms [75] in computational
ationsof3DGaussianproperties.
graphicstodetectwhetherfacialkeypointsarevisiblefrom
the given viewpoint and subsequently remove them from
These improvements constitute the proposed hybrid 3D
the skeleton map if considered invisible. Body keypoints
Gaussian avatar representation, an overview of which is
remainunalteredbecausetheyresideintheSMPL-Xmesh,
illustratedinFigure3.
and it is difficult to determine whether they are occluded
Formulation. The proposed hybrid 3D Gaussian avatar
withoutintroducingnewpriors.
representation consists of two types of 3D Gaussians:
G = G ‚à™ G , where G denotes unconstrained 3D
avatar u m u
3.3 H3GA:Hybrid3DGaussianAvatars Gaussians,andG denotesmesh-binding3DGaussians.
m
The previous method DreamWaltz [28] utilizes NeRF [1] to Forunconstrained3DGaussiansG ,theinitialpositions
u
represent 3D avatars, which is computationally expensive areextractedfromapre-trainedNeRF.Specifically,wequery6
NeRFtoobtainthedensitydistributionofahigh-resolution 3.4.1 CanonicalAvatarLearning
3Dgrid,andpositionswherethedensityexceedsaconstant
In this stage, we employ a static NeRF (implemented with
threshold are used as the initial positions p u for G u. Then, Instant-NGP[33])asthecanonicalavatarrepresentationand
thecolorsc u andopacitiesŒ± u ofG u arepredictedby: train it using the skeleton-conditioned ControlNet [29] and
the canonical-posed SMPL-X model [32]. In particular, it
c,Œ±=NeRF(p). (4)
leveragestheSMPL-Xmodelinthreeways:(1)pre-training
Thescaless uandrotationsq uofG uareexplicitlyinitialized NeRF,(2)providinggeometryconstraints,and(3)rendering
skeleton images to condition ControlNet for 3D-consistent
following3DGS[4]ratherthanbeingpredictedbyNeRF.
For mesh-binding 3D Gaussians G , we utilize the pre- andpose-alignedscoredistillation.
m
Pre-training with SMPL-X. To speed up the NeRF op-
defined3DmeshesofthehandsandfacefromSMPL-Xand
timization and to provide reasonable initial renderings for
constructmesh-binding3DGaussiansfollowingSuGaR[76]
and GaMeS [77]. Exceptionally, the colors c m and opacities thediffusionmodel,wepre-trainNeRFbasedonanSMPL-
Œ± m of G m are predicted by NeRF following Equation 4. X mesh template. Specifically, we render the silhouette and
depth images of NeRF and SMPL-X given a randomly
Besides, we parameterize the pre-defined 3D meshes using
theshapeparametersŒ≤ ofSMPL-X,whicharelearnable. sampledviewpoint,andminimizetheMSElossbetweenthe
NeRFrenderingsandtheSMPL-Xrenderings.TheNeRFini-
ArticulationandPoseTransformation.SMPL-Xutilizes
tialization from the human template significantly improves
linearblendskinning(LBS)[74]fortheposetransformation
thegeometryandtheconvergenceefficiencyforsubsequent
ofanarticulatedhumanbody.Thistechniquetransformsthe
text-specificavatargeneration.
vertices of 3D meshes by blending multiple joint transfor-
Score Distillation in Canonical Pose. Given the target
mationsbasedonLBSweights.Therefore,formesh-binding
3D Gaussians G bound to SMPL-X body parts, we can text prompt, we optimize the pre-trained NeRF through
m
skeleton-guidedscoredistillationlossLcnl inthecanonical
animatethembytransformingthemeshvertices,following cSDS
Equation 3. For unconstrained 3D Gaussians G , the pose pose space. We adopt the A-pose as the canonical pose
u
transformationinvolvestranslatingthepositionpandrotat- becauseitbestalignswiththediffusionpriorandavoidsleg
ing the quaternion q. We extend the LBS transformation of overlap.UnlikeDreamWaltz[28]usingSMPL[31]skeletons
asconditionimages,weemploythemoreadvancedSMPL-
SMPL-Xverticestounconstrained3DGaussiansasfollows:
X[32]skeletonswithhandjointsandfaciallandmarks.
G (Œæ)=LBS(Gcnl,J,Œæ,W ), (5) Local Geometric Constraints of Body Parts. During
u u lbs
NeRFtraining,weintroducealocalgeometrylossbasedon
where G ucnl denotes unconstrained 3D Gaussians in the pre-definedmeshesofbodyparts,suchashandsandfaces.
canonical pose, J represents SMPL-X joint positions, Œæ is This ensures the trained NeRF is geometrically compatible
the SMPL-X pose, and W lbs is a set of LBS weights for G u. with mesh-binding 3D Gaussians when serving as 3DGS
TheacquisitionofLBSweightsW lbsisgiveninSection3.4.2. initialization in subsequent stages. Specifically, we align
Non-rigid Deformation. Pose-dependent deformations the NeRF densities œÑ of local regions with the pre-defined
(i.e.,B P(Œæ)inEquation2)allowtheSMPL-Xmodeltofinely meshesusingamarginrankingloss:
adjust and deform the body surface during pose changes.
(cid:40)
Still, it struggles to generalize to clothed avatars generated
L =
(max(0,œÑ max‚àíœÑ(p)))2 ifponmesh
from texts. Thus we introduce a MLP-based deformation geo (max(0,œÑ(p)‚àíœÑ ))2 ifpnotonmesh,
min
network [66] to model pose-dependent deformations for
unconstrained3DGaussiansG : whereprepresents3Dpointssampledonandnearthepre-
u
defined meshes, œÑ(p) denotes the densities of 3D points
(Œ¥p,Œ¥s,Œ¥q)=NRDeform(Œæ), (6) p predicted by NeRF, œÑ and œÑ are constant hyperpa-
min max
rameters. Notably, Latent-NeRF [71] also introduces shape
where(Œ¥p,Œ¥s,Œ¥q)representstheoffsetsofpositions,scales,
guidancetoconstrainNeRFgeometrygivenameshsketch.
and quaternions of the unconstrained 3D Gaussians Gcnl in
u Although both methods use pre-defined meshes as geom-
the canonical pose. Note that the deformation network is
etry guidance for NeRF optimization, the difference lies in
subject-specificandtrainedfromthediffusionguidance.
theiraimtoprovideacoarsegeometryalignment,whereas
In addition, for mesh-binding 3D Gaussians G m, we weenforcestrictlyconsistentgeometries.
model pose-dependent deformations following the mesh
Overall Objective.Tolearnacanonical3Davatargiven
transformationsofSMPL-XasdescribedinEquation2.
text prompts, we optimize the NeRF-based static avatar
representationusing:
3.4 DreamWaltz-G: Learning 3D Gaussian Avatars via Lcnl =Lcnl +Œª L ,
total cSDS geo geo
Skeleton-guidedScoreDistillation
whereLcnl denotestheconditionalSDSlosswithcanonical
Based on the proposed Skeleton-guided Score Distillation cSDS
skeletonimagesasconditions,andŒª = 1.0isabalanced
and Hybrid 3D Gaussian Avatar Representation, We fur- geo
weightofthelocalgeometryconstraint.
ther introduce a text-driven avatar generation framework:
DreamWaltz-G. The framework comprises two training
3.4.2 AnimatableAvatarLearning
stages: (I) Static NeRF-based Canonical Avatar Learning
(Sec.3.4.1),(II)Deformable3DGS-basedAnimatableAvatar Inthisstage,weinitializetheproposedhybrid3DGaussians
Learning(Sec.3.4.2),asillustratedinFigure4. G astheanimatableavatarrepresentationandoptimize
avatar7
Canonical render condition render condition
SMPL-X
SMPL-X
Skeleton Image ControlNet Human Template Skeleton Image ControlNet
ùêø
geo
render add noise Random Pose & Expr. render add noise
Instant-NGP H3GA
{ùúÉ ,ùúÉ ,ùõΩ }
body hand expr
ùêø ùêø
NeRF-based cSDS 3DGS-based cSDS
Canonical Avatar Rendered Image Trained Animatable Avatar Rendered Image
Stage I: Canonical Avatar Learning Instant-NGP Stage II: Animatable Avatar Learning
Fig.4.Theproposedanimatable3DavatargenerationframeworkDreamWaltz-Gconsistsoftwotrainingstages:(I)CanonicalAvatarLearning
and(II)AnimatableAvatarLearning.InStageI,WeadoptthestaticInstant-NGP[33]ascanonicalavatarrepresentation.Foreachiteration,we
extractaskeletonimagefromcanonicalSMPL-X[32]toconditionControlNet[29].Skeleton-conditionedscoredistillationlossL isusedasa
cSDS
trainingobjectivetolearnthecanonicalavatar.InStageII,theproposedanimatableavatarrepresentationH3GAisfirstinitializedwiththetrained
Instant-NGPfromStageIandthenoptimizedbyL .UnlikeStageI,whichusesafixedcanonicalpose,inStageII,werandomlysampleplausible
cSDS
humanposesandexpressionsineachiterationtodriveH3GAandSMPL-X,encouragingavatarlearningacrossdifferentmotions.
itinrandomposespaceusingscoredistillationconditioned and facial expressions. To obtain random hand poses and
onSMPL-Xskeletons. facial expressions, we randomly sample PCA coefficients
LBS Weight Initialization with SMPL-X. Assigning from a Gaussian distribution and use the SMPL-X prior to
LBS weights from SMPL-X vertices to each unconstrained computecorrespondingposeandshapeparameters.
3D Gaussian G ‚àà G is necessary for articulation and Overall Objective. To learn an animatable 3D avatar
u
pose transformation. A naive implementation is mapping given text prompts, we optimize the hybrid 3DGS-based
LBS weights based on nearest vertex criteria; however, this dynamicavatarrepresentationusingLarb only.
cSDS
method cannot handle the geometric mismatches between
SMPL-X and the generated avatars, leading to erroneous
4 EXPERIMENTS
skeletal binding and distortions, as demonstrated in Fig-
ure 14. To address this, we propose using a geometry- 4.1 ImplementationDetails
awareKNNsmoothingalgorithmtoadjusttheassignedLBS DreamWaltz-G is implemented in PyTorch and can be
weights of the 3D Gaussians adaptively. Specifically, for a trainedandevaluatedonasingleNVIDIAL40SGPU.
3D Gaussian G ‚àà G , its initial LBS weights W(0) can be For the Canonical Avatar Learning stage, we employ
u lbs
derivedfromthenearestvertexinSMPL-X.Next,weupdate Instant-NGP [33] as the static 3D avatar representation. We
W iterativelybyweightedaggregationoftheLBSweights optimize it for 15,000 iterations, which takes about one
lbs
W lbs,k oftheK lbs nearest3DGaussians: hour. We adopt a progressive resolution sampling strategy
for efficient optimization, where the rendering resolution
K
W(i+1) = (cid:88)lbs Z lbs W(i) , (7) increases from 64√ó64 to 512√ó512 as iterations progress.
lbs k=1d ng,k¬∑d nv,k lbs,k More details on NeRF optimization, such as the optimizer
andlearningrate,areconsistentwithDreamWaltz[28].
where i ‚àà {0,1,...,N lbs} denotes the current iteration For the Animatable Avatar Learning stage, we use the
step, Z lbs represents the normalization constant ensuring proposed H3GA as the dynamic 3D avatar representation,
Z lbs(cid:80)K k=lb 1s (d ng,k¬∑d nv,k)‚àí1 =1,d ng,k isthesquareddistance which is trained for 15,000 iterations, and the rendering
from the k-th nearest 3D Gaussian G k to the current 3D resolution is maintained at 512√ó512. To optimize 3D Gaus-
Gaussian G, and d nv,k is the squared distance from G k to sianattributes,weadheretotheoriginalimplementationof
its nearest vertex in SMPL-X. For clarity,
d‚àí ng1
,k reflects the 3DGS[4].However,wedonotusethedensificationstrategy
contributionofG ktoG,whiled‚àí nv1 ,kindicatestheconfidence for two reasons: (i) The high variance of SDS gradients
oftheinitialLBSweightsofG k. makesgradient-baseddensificationunstable;(ii)Theinitial-
Score Distillation in Arbitrary Poses and Expressions. ization based on a trained NeRF can provide accurate and
Skeleton-guided score distillation Larb in arbitrary poses quantitative3DGaussians.
cSDS
helpstoenhancevisualqualityandmitigatemotionartifacts Diffusion Guidance. We use Stable-Diffusion-v1.5 [19]
innovelposes.ThepreviousworkDreamWaltz[28]samples and ControlNet-v1.1-openpose [29] to provide SDS guidance
randomposesusingtheoff-the-shelfVPoser[32],whichisa for both training stages. We randomly sample the timestep
variationalautoencoderthatlearnsalatentrepresentationof fromauniformdistributionof[0.02,0.98],andtheclassifier-
humanpose.However,optimizingdirectlyinarbitrarypose free guidance scale is set to 50.0. The weight term w(t) for
spaces may be challenging to converge, leading to quality SDSlossissetto1.0.TheconditioningscaleforControlNet
issues such as blurring. Therefore, we adopt a curriculum is set to 1.0 by default. To further improve 3D consistency
learning strategy from simple to difficult tasks, starting and visual quality, both view-dependent text augmenta-
with sampling various canonical poses (such as A-pose, T- tion[20]andnegativepromptsareused.
pose,andY-pose),followedbysamplingrandomposesfrom Camera Sampling. For each iteration, the camera view
VPoser. Note that VPoser does not encompass hand poses is randomly sampled in spherical coordinates, where the8
TABLE2
Userpreferencestudies.Wereportthepreferencepercentages(%)ofourmethodoverexistingstate-of-the-artmethodsintermsofgeometric
quality,appearancequality,andconsistencywiththetextprompts.
Methods GeometryQuality AppearanceQuality TextConsistency
Oursvs.DreamWaltz[28] 84.93 86.30 78.08
Oursvs.DreamHuman[25] 82.61 86.96 84.78
Oursvs.TADA[24] 70.27 77.03 66.22
Oursvs.GAvatar[26] 82.05 76.92 79.49
Oursvs.HumanGaussian[27] 70.31 75.00 76.56
radius,azimuth,elevation,andFoVareuniformlysampled in Table 2, the participants favor 3D avatars generated by
from [1.0,2.0], [0,360], [60,120], and [40,70], respectively. ourmethodacrossallevaluationcriteria.
The camera focus strategy is also employed, with a 0.2
4.3 AblationandAnalysis
probability of focusing on the face of the 3D avatar to
enhancefacialdetails.Additionally,weempiricallyfindthat We perform a comprehensive ablation analysis to demon-
horizontal camera jitter during training helps improve the stratetheeffectivenessoftheproposedimprovements.
visualqualityofthefootregion. Effectiveness of Skeleton Guidance. We visualize the
MotionSequences.Tocreateanimationdemonstrations, SDSgradientsandgeneratedimagesinFigure8toillustrate
we utilize SMPL-X motion sequences from 3DPW [78], theadvantagesofskeletonguidancecomparedtotext-only
AIST++[79],Motion-X[80],andTalkSHOW[81]datasetsto guidance and depth guidance. It is evident that depth and
animate avatars. SMPL-X motion sequences extracted from skeletonimagesfromhumantemplatesoffermoreinforma-
in-the-wildvideosarealsoused. tive guidance than text alone. However, the strong contour
priors in depth images cause the SDS gradientsto conform
tightlytotheavatar‚Äôsskin,leadingtoalackofcomplexap-
4.2 Comparisons
pearances(e.g.,thedisappearanceofSuperman‚Äôscapeinthe
We provide both qualitative and quantitative results of secondrowofFigure8).Ontheotherhand,skeletonimages,
our DreamWaltz-G compared to existing text-driven 3D asadoptedbyDreamWaltz-G,providebothinformativeand
avatar generation methods, including DreamWaltz [28], flexiblesupervision,accuratelycapturingtheavatars‚Äôposes
DreamHuman [25], TADA [24], HumanGaussian [27], and andintricateshapes.
GAvatar[26]. Ablation Studies on Occlusion Culling. Occlusion
Qualitative Results of Canonical Avatars. We present culling is crucial for resolving view ambiguity both for
the results of canonical avatars, as shown in Figure 5. skeleton-conditioned 2D and 3D generation, as shown in
Comparedtoexistingmethods,ourapproachachieveshigh- Figure 9. Limited by the view-aware capability, Control-
definition and realistic appearances, alleviating blurriness Net[29]failstogeneratetheback-viewimageofacharacter
and over-saturation issues. Additionally, our approach can even with view-dependent text and skeleton prompts, as
generate accurate hand and facial shapes by leveraging shown in Figure 9(a). The introduction of occlusion culling
the geometric priors of predefined meshes, addressing the eliminates the ambiguity of skeleton conditions and helps
diffusion model‚Äôs difficulty in generating detailed human ControlNet to generate correct views. Similar effects can
body parts. We provide more examples of canonical 3D be observed in text-to-3D avatar generation. As shown in
avatarsgeneratedbyourmethodinFigure6. Figure 9(b), The Janus (multi-face) problem is solved by
QualitativeResultsofAnimatableAvatars.Wedemon- introducingocclusioncullingtotherenderingprocessfrom
strate the animation results of our method compared to 3DSMPL-Xtothe2Dskeletonimages.
HumanGaussian[27]andTADA[24],asshowninFigure7. Ablation Studies on Hybrid 3D Gaussian Avatars.
The SMPL-X motion sequences from the AIST++ dance Theproposed3Davatarrepresentation,H3GA,incorporates
dataset [79] are used to animate the generated avatars. several improvements to accommodate SDS optimization
Compared to existing competing methods, our approach and enable expressive avatar animation. We analyze the
achievesclearerhandmotionsandhigher-fidelityanimation effects of these improvements individually, as shown in
quality. In comparison to HumanGaussian, which is also Figure 10. Specifically, ‚ÄúNeRF Initialization‚Äù provides a
based on 3DGS [4], we effectively avoid sharp artifacts well-structured point cloud to initialize the 3D Gaussians,
caused by the incorrect driving of 3D Gaussians. More facilitating the capture of complex geometries that differ
examples of avatar animations can be seen in Figure 6 and from SMPL-X templates. ‚ÄúNeRF Encoding‚Äù utilizes multi-
Figure16. resolutionhashgrids[33]andMLPstopredict3DGaussian
User Studies. To quantitatively evaluate the quality of attributes, resulting in more stable SDS optimization and
thegenerated3Davatarscomparedtoexistingmethods,we avoidinghigh-frequencynoiseintextures.
conducted a A/B user preference study based on 24 text For body parts that are challenging to generate and
prompts released by GAvatar [26]. Twenty participants are animate (e.g., hands and face), we adopt a ‚ÄúMesh Bind-
asked to view 3D avatars generated by our method and ing‚Äù strategy. This strategy binds the corresponding 3D
one of the competing methods and then choose the better Gaussians to the meshes of SMPL-X body parts, achieving
methodbasedon(1)geometricquality,(2)appearancequal- sharp and joint-aligned geometries. Note that these mesh-
ity, and (3) consistency with the text prompts. As reported binding body parts are parameterized by SMPL-X shape9
e
t ih.s
t
wr
o
ah
gs DreamHuman TADA DreamWaltz
n ird n
a
a
ep
wo
t
n k
an
ma
t
A
HumanGaussian GAvatar DreamWaltz-G (Ours)
.t
iu
s
e
g
ie
b
a
g
n DreamHuman TADA DreamWaltz
ir
a
e
w
n
a
m
y
lr
e
d
le
n
A
HumanGaussian GAvatar DreamWaltz-G (Ours)
Fig. 5. Qualitative results of canonical avatars compared to existing text-driven 3D avatar generation methods: DreamWaltz [28], DreamHu-
man[25],TADA[24],GAvatar[26],HumanGaussian[27].Thetextpromptsusedarelistedontheleft.10
An elderly woman with a sunhat. A medieval European king. Mulan.
Jane Goodall. A professional boxer. A Bedouin dressed in white. A farmer.
An elderly man wearing A gardener in overalls An Asian woman in a A black female surgeon.
a beige suit. and a wide-brimmed hat. leather jacket.
A karate master. A football player. A clown. A Viking.
Harley Quinn. Rapunzel in Tangled. Goku. Spiderman.
Fig.6.Moreexamplesof3Davatarsandtheiranimationsproducedbyourapproach.Thetextpromptsusedarelistedbelow.11
g
n
ik
iV
A
e
c
n
a
D
+
+
T d
S I e lg
A n
a
T
n
i
le
z
n
u
p
a
R
HumanGaussian TADA DreamWaltz-G (Ours)
Fig.7.Qualitativeresultsofanimatableavatarscomparedtoexisting3davatargenerationandanimationmethods:HumanGaussian[27]and
TADA[24].Comparedtocompetingmethods,ourapproachachievesclearerhandmotionsandhigher-fidelityanimationquality.Incomparisonto
HumanGaussian,whichisalsobasedon3DGS[4],weeffectivelyavoidsharpartifactscausedbytheincorrectdrivingof3DGaussians.
n y
eo ln
lb
a
tSis
u f
f
iD
tx
eO
T
h
tp
e
te D
N
lo
r
tn
o n
C o
te
le
k
S
SDS Gradients One-step Denoised Images Generated Images
Fig.8.VisualizationofSDSgradientsandgeneratedimagesunderdifferentguidanceconditions.Theresultsinthefirstrowareconditioned
onlyontext.Incontrast,thesecondandthirdrowsareconditionedonadditionaldepthandskeletonimages,respectively,asindicatedintheupper
leftcornerofeachvisualization.Theseresultsarebasedonthetextprompt‚Äúsuperman‚Äù.Itisevidentthatskeletonconditions,asadoptedbyour
DreamWaltz-G,providemoreinformativesupervisionthantext-onlyconditions.Skeletonconditionsarealsolessrestrictivethandepthconditions,
successfullyavoidingthelossofcomplexappearances,suchasthedisappearanceofSuperman‚Äôscape.
Text Prompt: backview of Mulan Text Prompt: Hatsune Miku
w/o Occlusion Culling w/ Occlusion Culling w/o Occlusion Culling w/ Occlusion Culling
(a) Text-to-2D results, produced by ControlNet. (b) Text-to-3D results, rendered from side and back views.
Fig. 9. Ablation studies on occlusion culling. We employ occlusion culling to refine skeleton condition images by removing invisible human
keypoints,suchastheeyesandnoseinthebackview.Ithelps(a)ControlNet[29]togeneratethecharacter‚Äôsbackviewcorrectly,and(b)text-to-3D
avatargenerationtoresolvethemulti-faceproblem,ashighlightedbytheboundingboxes.12
Baseline + NeRF Initialization + NeRF Encoding + Mesh Binding (Ours)
Fig.10.AblationstudiesontheproposedHybrid3DGaussianAvatarrepresentation,whichincorporatesseveralimprovementstoaccommo-
dateSDSoptimizationandenableexpressiveavataranimation.Specifically,‚ÄúNeRFInitialization‚Äùprovidesawell-structuredpointcloudtoinitialize
the3DGaussians,facilitatingthecaptureofcomplexgeometries.‚ÄúNeRFEncoding‚ÄùutilizesInstant-NGP[33]topredict3DGaussianattributes,
resultinginmorestableSDSoptimizationandavoidinghigh-frequencynoiseintextures.Forintricatebodypartslikehands,weadopta‚ÄúMesh
Binding‚Äùstrategy,whichbindsthecorresponding3DGaussianstotheSMPL-Xbodyparts,achievingsharpandjoint-alignedgeometries.
w/o Learnable Hand Shapes w/ Learnable Hand Shapes
w/o AAL w/ AAL
Fig.11.Ablationstudiesonlearnableshapeparameters(e.g.,Œ≤
hand
ofSMPL-X[32])formesh-binding3DGaussianbodyparts.Weusethe
handsof‚ÄúPrincessElsainFrozen‚Äùasanexampletodemonstrate.By Fig. 13. Ablation studies on Animatable Avatar Learning (AAL),
optimizingthehandshapeparametersofmesh-binding3DGaussians, whichistheStageIIofDreamWaltz-G.For‚Äúw/oAAL‚Äù,wetrainforthe
slimmerhandsthatmatchElsa‚Äôscharacteristicscanbegenerated. same iterations as ‚Äúw/ AAL‚Äù but use a fixed canonical pose to ensure
afaircomparison.ItcanbeobservedthattheintroductionofAALfixes
textureinformationforareasnotvisibleinthecanonicalpose.Besides,
itreducesanimationartifactscausedbyincorrectskeletonbinding.
parametersandaretrainable.AsshowninFigure11,hands
that conform to the character‚Äôs features can be obtained by
optimizingtheSMPL-Xhandshapeparameters.
Ablation Studies on Local Geometric Constraints.
The local geometric constraints L are introduced during
geo
canonical NeRF training to maintain the geometric struc-
tures of intricate body parts, such as hands and faces. As
shown in Figure 12, without the local geometric loss, the
generated avatar‚Äôs hands appear in a clenched fist state,
exhibitinguncleargeometricstructuresanddifficultieswith
riggingandanimation.Introducingthelocalgeometricloss
ensures that the hand structure is accurately aligned with
w/o Local Geometric Loss w/ Local Geometric Loss
canonical SMPL-X, avoiding erroneous geometries and fa-
cilitatingsubsequenthandanimation.
Fig. 12. Ablation studies on local geometric constraints. Without
thelocalgeometriclossL geo,thegeneratedavatar‚Äôshandsappearin Ablation Studies on DreamWaltz-G. The proposed
aclenchedfiststate(highlightedbydashedboxes),exhibitingunclear avatar generation framework, DreamWaltz-G, consists of
geometric structures. The introduction of L geo ensures that the hand two training stages: Canonical Avatar Learning (CAL), and
structure is accurately aligned with canonical SMPL-X (highlighted by
AnimatableAvatarLearning(AAL).TheCALstageaimsto
dashed boxes), avoiding erroneous geometries and facilitating subse-
quentriggingandhandanimation. provideagoodNeRFinitializationforH3GA,theeffective-
nessofwhichisvalidatedasshowninFigure10.TheAAL
stageaimstolearntheappearanceandgeometryofthe3D13
w/o KNN Smooth w/ KNN Smooth Canonical Avatar w/o KNN Smooth w/ KNN Smooth
Canonical Avatar (Baseline) (Ours) (Baseline) (Ours)
(a)Continuous deformation of complex clothing. (b)Accurate skeleton binding.
Fig.14.AblationstudiesonKNNsmoothingforLBSweightinitialization.Theproposedgeometry-awareKNNSmoothingalgorithmrefinesthe
3DGaussians‚ÄôinitialLBSweights(representingtheassociationofeach3DGaussiantobodyjoints).ComparedtothebaselinethatassignsLBS
weightsbasedsolelyonthenearestneighborcriterion,theproposedalgorithmenables(a)continuousdeformationofcomplexclothing,e.g.,the
stretchingofthechef‚Äôsapron;(b)accurateskeletonbinding,forexample,thehathangingfromWoody‚Äôswaistisnotaffectedbyarmmovements.
Fat SMPL-X Fat ‚ÄúJoker‚Äù Thin SMPL-X Thin ‚ÄúJoker‚Äù
Template Template
(a) Shape Control (b) Shape Editing
Fig.15.Application:ShapeControlandEditing.Ourmethodenables(a)training-timeshapecontrolbymodifyingtheSMPL-Xtemplateand
(b)inference-timeshapeeditingduringinferencebyexplicitlyadjustingthe3DGaussians.Bothshapecontrolandeditingarecompatiblewiththe
SMPL-XshapeparametersŒ≤,allowinguserstosimplyadjustŒ≤toachievethedesired3Dshape.
‚ÄúKobe Bryant‚Äù + TalkShow (Conan) ‚ÄúA chef dressed in white‚Äù + TalkShow (Chemistry)
Fig.16.Application:Talking3DAvatars.BenefitingfromtheproposedexpressiveH3GArepresentation,ourmethodcanlearnanimatable3D
avatarsfrom2Ddiffusionpriorswhilepreservingthefinedetailsofhandsandfaces.Thisallowsustocreatemoreexpressive3Davataranimations
liketalking3Davatars.
avatar in a random pose space. As shown in Figure 13, the solelyonthenearestneighborcriterion.
introduction of AAL fixes texture information for areas not
visibleinthecanonicalposeandreducesanimationartifacts
4.4 Applications
causedbyincorrectskeletonbinding.
Ablation Studies on KNN Smoothing for LBS Weight Weexplorepracticalapplicationsofourmethod,including:
Initialization.Weproposeageometry-awareKNNSmooth- shapecontrolandediting,talking3Davatars,humanvideo
ingalgorithmtorefinetheinitialLBSweights(representing reenactment,andmulti-subject3Dscenecomposition.
theassociationofeach3DGaussiantobodyjoints),bringing Shape Control and Editing. Our method utilizes the
various improvements in avatar rigging and animation. SMPL-X template to provide skeleton guidance for 3D
As shown in Figure 14, the proposed KNN smoothing avatar creation. By adjusting the shape parameters of the
algorithm enables: (a) continuous deformation of complex SMPL-Xtemplate,theshapeofthegenerated3Davatarcan
clothing,e.g.,thestretchingofadress;(b)accurateskeleton becontrolled,asshowninFigure15(a).However,thisshape
binding,whichshouldbegeometry-awareratherthanbased controlrequiresre-training,whichleadstoinefficiencyand14
Original Video Frame SMPL-X Motion Reenacted Video Frame
Fig.17.Application:HumanVideoReenactment.Combinedwith3Dhumanposeestimationandvideoinpaintingtechniques,the3Davatars
generated by our method can be projected onto 2D human videos. This integration allows for seamless blending of animated 3D avatars with
real-worldfootage,enhancingtherealismandinteractivityofthereenactedscenes.
Fig.18.Application:Multi-subjectSceneComposition.Thegenerated3Davatarscanbeseamlesslyintegratedwithexisting3Dassets.The
presented3DenvironmentsarefromtheMip-NeRF360dataset[82]andreconstructedbyvanilla3DGaussianSplatting[4].
appearance randomness. Thanks to the explicit 3D avatar allows for seamless blending of animated 3D avatars with
representation, our method can also achieve shape editing real-world footage, enhancing the realism and interactivity
byadjustingthe3DGaussians.Comparedtoshapecontrol, ofthereenactedscenes.
shape editing is real-time, interactive, and able to maintain Multi-subject Scene Composition. The generated 3D
aconsistentappearance,asshowninFigure15(b). avatars can be integrated with existing 3D assets into the
Talking3DAvatars.TheproposedH3GArepresentation same scene. As shown in Figure 18, we place the animated
enables the modeling of animatable 3D avatars from 2D 3D avatars ‚ÄúKobe Bryant‚Äù and ‚Äúa chef dressed in white‚Äù
diffusion priors while preserving the fine details of hands into 3D scenes, seamlessly integrating the avatars into the
andfaces.Thisallowsustocreatemoreexpressive3Davatar environment.
animations, for example, talking 3D avatars. As shown in
Figure 16, the results exhibit realistic appearances, intricate
5 CONCLUSIONS
geometries,andaccuratehandandfaceanimations.
HumanVideoReenactment.Combinedwith3Dhuman We introduce DreamWaltz-G, a novel learning framework
pose estimation [80] and video inpainting techniques, the foranimatable3Davatargenerationfromtexts.Atthecore
3D avatars generated by our method can be projected onto ofthisframeworkareskeleton-guidedscoredistillationand
2D human videos, as shown in Figure 17. This integration hybrid 3D Gaussian avatar representation. Specifically, we15
leverage the skeleton priors from the human parametric [13] Z.Qian,S.Wang,M.Mihajlovic,A.Geiger,andS.Tang,‚Äú3DGS-
model [32] to guide the score distillation process, provid- Avatar: Animatable Avatars via Deformable 3D Gaussian Splat-
ting,‚ÄùinProceedingsoftheIEEE/CVFConferenceonComputerVision
ing 3D-consistent and pose-aligned supervision for high-
andPatternRecognition,2024. 1
qualityavatargeneration.Thehybrid3DGaussianrepresen-
[14] W. Zielonka, T. Bagautdinov, S. Saito, M. Zollho¬®fer, J. Thies,
tation builds on the efficiency of 3D Gaussian splatting [4], and J. Romero, ‚ÄúDrivable 3D Gaussian Avatars,‚Äù arXiv preprint
combining NeRF [1] and 3D meshes [76] to accommodate arXiv:2311.08581,2023. 1,3
[15] F. Zhao, Y. Jiang, K. Yao, J. Zhang, L. Wang, H. Dai, Y. Zhong,
SDSoptimizationandenableexpressiveanimations.Exten-
Y. Zhang, M. Wu, L. Xu et al., ‚ÄúHuman Performance Modeling
sive experiments demonstrate that DreamWaltz-G is effec- andRenderingviaNeuralAnimatedMesh,‚ÄùACMTransactionson
tive and outperforms existing text-to-3D avatar generation Graphics(TOG),vol.41,no.6,pp.1‚Äì17,2022. 1
methods in both visual quality and animation. Benefiting [16] Y. Jiang, Q. Liao, X. Li, L. Ma, Q. Zhang, C. Zhang, Z. Lu, and
Y.Shan,‚ÄúUVGaussians:JointLearningofMeshDeformationand
fromDreamWaltz-G,wecouldunleashourimaginationand
Gaussian Textures for Human Avatar Modeling,‚Äù arXiv preprint
enableawiderangeofavatarapplications. arXiv:2403.11589,2024. 1,3
Similar to previous 3D generation methods [20], [21], [17] Y.Zheng,Q.Zhao,G.Yang,W.Yifan,D.Xiang,F.Dubost,D.La-
[28],DreamWaltz-Ggenerates3Davatarsthroughscoredis- gun,T.Beeler,F.Tombari,L.Guibasetal.,‚ÄúPhysAvatar:Learning
the Physics of Dressed 3D Avatars from Visual Observations,‚Äù
tillation[20].Leveragingmorepowerfulfoundationalmod-
arXivpreprintarXiv:2404.04421,2024. 1
els[45],[46]andadvancedscoredistillationtechniques[55],
[18] A.Ramesh,P.Dhariwal,A.Nichol,C.Chu,andM.Chen,‚ÄúHier-
[56] can further enhance the generation quality and effi- archical Text-Conditional Image Generation with CLIP Latents,‚Äù
ciency.Additionally,thegenerated3Davatarsstilllackhier- arXivpreprintarXiv:2204.06125,2022. 1,2,4
[19] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,
archicalsemanticstructures andphysicalproperties,which
‚ÄúHigh-ResolutionImageSynthesiswithLatentDiffusionModels,‚Äù
willbeadirectionworthexploringinfuturework. in Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,2022,pp.10684‚Äì10695. 1,2,7
[20] B.Poole,A.Jain,J.T.Barron,andB.Mildenhall,‚ÄúDreamFusion:
REFERENCES Text-to-3D using 2D Diffusion,‚Äù arXiv preprint arXiv:2209.14988,
2022. 1,3,4,7,15
[1] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra- [21] H. Wang, X. Du, J. Li, R. A. Yeh, and G. Shakhnarovich, ‚ÄúScore
mamoorthi, and R. Ng, ‚ÄúNeRF: Representing Scenes as Neural JacobianChaining:LiftingPretrained2DDiffusionModelsfor3D
RadianceFieldsforViewSynthesis,‚ÄùCommunicationsoftheACM, Generation,‚ÄùarXivpreprintarXiv:2212.00774,2022. 1,3,4,15
vol.65,no.1,pp.99‚Äì106,2021. 1,2,4,5,15 [22] F.Hong,M.Zhang,L.Pan,Z.Cai,L.Yang,andZ.Liu,‚ÄúAvatar-
[2] P. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang, CLIP: Zero-Shot Text-Driven Generation and Animation of 3D
‚ÄúNeuS:LearningNeuralImplicitSurfacesbyVolumeRendering Avatars,‚ÄùACMTransactionsonGraphics(TOG),vol.41,no.4,pp.
for Multi-view Reconstruction,‚Äù Advances in Neural Information 1‚Äì19,2022. 1,3,4
ProcessingSystems,vol.34,pp.27171‚Äì27183,2021. 1,2 [23] R.Jiang,C.Wang,J.Zhang,M.Chai,M.He,D.Chen,andJ.Liao,
[3] T.Shen,J.Gao,K.Yin,M.-Y.Liu,andS.Fidler,‚ÄúDeepmarching ‚ÄúAvatarCraft: Transforming Text into Neural Human Avatars
tetrahedra: a hybrid representation for high-resolution 3d shape with Parameterized Shape and Pose Control,‚Äù arXiv preprint
synthesis,‚Äù in Advances in Neural Information Processing Systems, arXiv:2303.17606,2023. 1,3
2021. 1 [24] T.Liao,H.Yi,Y.Xiu,J.Tang,Y.Huang,J.Thies,andM.J.Black,
[4] B.Kerbl,G.Kopanas,T.Leimku¬®hler,andG.Drettakis,‚Äú3DGaus- ‚ÄúTADA!TexttoAnimatableDigitalAvatars,‚ÄùinInternationalCon-
sian Splatting for Real-Time Radiance Field Rendering,‚Äù ACM ferenceon3DVision(3DV),2024. 1,3,8,9,11
Transactions on Graphics, vol. 42, no. 4, July 2023. 1, 2, 3, 5, 6, 7,
[25] N.Kolotouros,T.Alldieck,A.Zanfir,E.Bazavan,M.Fieraru,and
8,11,14,15
C. Sminchisescu, ‚ÄúDreamHuman: Animatable 3D Avatars from
[5] S.Saito,Z.Huang,R.Natsume,S.Morishima,A.Kanazawa,and
Text,‚Äù Advances in Neural Information Processing Systems, vol. 36,
H. Li, ‚ÄúPifu: Pixel-aligned implicit function for high-resolution
2024. 1,3,8,9
clothed human digitization,‚Äù in Proceedings of the IEEE/CVF In-
[26] Y. Yuan, X. Li, Y. Huang, S. De Mello, K. Nagano, J. Kautz, and
ternationalConferenceonComputerVision,2019,pp.2304‚Äì2314. 1
U.Iqbal,‚ÄúGAvatar:Animatable3DGaussianAvatarswithImplicit
[6] Y.Xiu,J.Yang,D.Tzionas,andM.J.Black,‚ÄúIcon:Implicitclothed
MeshLearning,‚ÄùinProceedingsoftheIEEEConferenceonComputer
humans obtained from normals,‚Äù in Proceedings of the IEEE/CVF
VisionandPatternRecognition,2024. 1,3,5,8,9
ConferenceonComputerVisionandPatternRecognition. IEEE,2022,
[27] X. Liu, X. Zhan, J. Tang, Y. Shan, G. Zeng, D. Lin, X. Liu, and
pp.13286‚Äì13296. 1
Z. Liu, ‚ÄúHumanGaussian: Text-Driven 3D Human Generation
[7] Y.Xiu,J.Yang,X.Cao,D.Tzionas,andM.J.Black,‚ÄúEcon:Explicit
withGaussianSplatting,‚ÄùinProceedingsoftheIEEE/CVFConference
clothedhumansoptimizedvianormalintegration,‚ÄùinProceedings
onComputerVisionandPatternRecognition,2024,pp.6646‚Äì6657. 1,
oftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
3,5,8,9,11
tion,2023,pp.512‚Äì523. 1
[28] Y. Huang, J. Wang, A. Zeng, H. Cao, X. Qi, Y. Shi, Z.-J. Zha,
[8] C.-Y. Weng, P. P. Srinivasan, B. Curless, and I. Kemelmacher-
and L. Zhang, ‚ÄúDreamWaltz: Make a Scene with Complex 3D
Shlizerman,‚ÄúPersonnerf:Personalizedreconstructionfromphoto
collections,‚ÄùinProceedingsoftheIEEE/CVFConferenceonComputer
AnimatableAvatars,‚ÄùinAdvancesinNeuralInformationProcessing
VisionandPatternRecognition,2023,pp.524‚Äì533. 1 Systems,2023. 1,2,3,5,6,7,8,9,15
[9] J. Wang, J. S. Yoon, T. Y. Wang, K. K. Singh, and U. Neumann, [29] L. Zhang and M. Agrawala, ‚ÄúAdding Conditional Control to
‚ÄúComplete 3d human reconstruction from a single incomplete Text-to-Image Diffusion Models,‚Äù in Proceedings of the IEEE/CVF
image,‚Äù in Proceedings of the IEEE/CVF Conference on Computer InternationalConferenceonComputerVision,2023. 1,3,4,5,6,7,8,
VisionandPatternRecognition,2023,pp.8748‚Äì8758. 1 11
[10] C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and [30] X.Ju,A.Zeng,C.Zhao,J.Wang,L.Zhang,andQ.Xu,‚ÄúHumanSD:
I.Kemelmacher-Shlizerman,‚ÄúHumannerf:Free-viewpointrender- A Native Skeleton-Guided Diffusion Model for Human Image
ingofmovingpeoplefrommonocularvideo,‚ÄùinProceedingsofthe Generation,‚ÄùinProceedingsoftheIEEE/CVFInternationalConference
IEEE/CVF Conference on Computer Vision and Pattern Recognition, onComputerVision,2023. 1,3,4
2022,pp.16210‚Äì16220. 1 [31] M.Loper,N.Mahmood,J.Romero,G.Pons-Moll,andM.J.Black,
[11] W.Jiang,K.M.Yi,G.Samei,O.Tuzel,andA.Ranjan,‚ÄúNeuman: ‚ÄúSMPL:askinnedmulti-personlinearmode,‚ÄùACMtransactionson
Neuralhumanradiancefieldfromasinglevideo,‚ÄùinProceedingsof graphics(TOG),vol.34,no.6,pp.1‚Äì16,2015. 1,2,3,4,6
theEuropeanconferenceoncomputervision(ECCV). Springer,2022, [32] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. Osman,
pp.402‚Äì418. 1 D.Tzionas,andM.J.Black,‚ÄúExpressivebodycapture:3dhands,
[12] Z. Yu, W. Cheng, X. Liu, W. Wu, and K.-Y. Lin, ‚ÄúMonoHuman: face,andbodyfromasingleimage,‚ÄùinProceedingsoftheIEEE/CVF
Animatable Human Neural Field from Monocular Video,‚Äù arXiv Conference on Computer Vision and Pattern Recognition, 2019, pp.
preprintarXiv:2304.02001,2023. 1 10975‚Äì10985. 1,2,4,5,6,7,12,1516
[33] T. Mu¬®ller, A. Evans, C. Schied, and A. Keller, ‚ÄúInstant Neural [52] C.-H. Lin, J. Gao, L. Tang, T. Takikawa, X. Zeng, X. Huang,
GraphicsPrimitiveswithaMultiresolutionHashEncoding,‚ÄùACM K. Kreis, S. Fidler, M.-Y. Liu, and T.-Y. Lin, ‚ÄúMagic3D:
TransactionsonGraphics(ToG),vol.41,no.4,pp.1‚Äì15,2022. 2,4,5, High-Resolution Text-to-3D Content Creation,‚Äù arXiv preprint
6,7,8,12 arXiv:2211.10440,2022. 3,4
[34] A.Nichol,P.Dhariwal,A.Ramesh,P.Shyam,P.Mishkin,B.Mc- [53] R.Chen,Y.Chen,N.Jiao,andK.Jia,‚ÄúFantasia3D:Disentangling
Grew,I.Sutskever,andM.Chen,‚ÄúGLIDE:TowardsPhotorealistic Geometry and Appearance for High-quality Text-to-3D Content
ImageGenerationandEditingwithText-GuidedDiffusionMod- Creation,‚ÄùarXivpreprintarXiv:2303.13873,2023. 3
els,‚ÄùarXivpreprintarXiv:2112.10741,2021. 2 [54] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, ‚ÄúDreamGaussian:
[35] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, GenerativeGaussianSplattingforEfficient3DContentCreation,‚Äù
S. K. S. Ghasemipour, B. K. Ayan, S. S. Mahdavi, R. G. Lopes inInternationalConferenceonLearningRepresentations,2024. 3
et al., ‚ÄúPhotorealistic Text-to-Image Diffusion Models with Deep [55] Y.Huang,J.Wang,Y.Shi,B.Tang,X.Qi,andL.Zhang,‚ÄúDream-
Language Understanding,‚Äù arXiv preprint arXiv:2205.11487, 2022. Time: An Improved Optimization Strategy for Diffusion-Guided
2,4 3DGeneration,‚ÄùinInternationalConferenceonLearningRepresenta-
[36] P.DhariwalandA.Nichol,‚ÄúDiffusionModelsBeatGANsonIm- tions,2024. 3,15
ageSynthesis,‚ÄùAdvancesinNeuralInformationProcessingSystems, [56] O.Katzir,O.Patashnik,D.Cohen-Or,andD.Lischinski,‚ÄúNoise-
vol.34,pp.8780‚Äì8794,2021. 3 free Score Distillation,‚Äù in International Conference on Learning
[37] J. Song, C. Meng, and S. Ermon, ‚ÄúDenoising Diffusion Implicit Representations,2024. 3,15
Models,‚Äù in International Conference on Learning Representations, [57] X.Yu,Y.-C.Guo,Y.Li,D.Liang,S.-H.Zhang,andX.QI,‚ÄúText-to-
2021. 3 3dwithclassifierscoredistillation,‚ÄùinInternationalConferenceon
[38] A. Q. Nichol and P. Dhariwal, ‚ÄúImproved Denoising Diffusion LearningRepresentations,2024. 3
ProbabilisticModels,‚ÄùinInternationalConferenceonMachineLearn- [58] Y. Liang, X. Yang, J. Lin, H. Li, X. Xu, and Y. Chen, ‚ÄúLucid-
ing. PMLR,2021,pp.8162‚Äì8171. 3,4 dreamer:Towardshigh-fidelitytext-to-3dgenerationviainterval
[39] C.Schuhmann,R.Beaumont,R.Vencu,C.Gordon,R.Wightman, scorematching,‚ÄùarXivpreprintarXiv:2311.11284,2023. 3
M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al., [59] J.Zhu,P.Zhuang,andS.Koyejo,‚ÄúHiFA:High-fidelityText-to-3D
‚ÄúLAION-5B:Anopenlarge-scaledatasetfortrainingnextgenera- Generation with Advanced Diffusion Guidance,‚Äù in International
tionimage-textmodels,‚ÄùarXivpreprintarXiv:2210.08402,2022. 3 ConferenceonLearningRepresentations,2024. 3
[40] P. Sharma, N. Ding, S. Goodman, and R. Soricut, ‚ÄúConceptual [60] Z.Wang,C.Lu,Y.Wang,F.Bao,C.Li,H.Su,andJ.Zhu,‚ÄúProlific-
Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Dreamer: High-Fidelity and Diverse Text-to-3D Generation with
Automatic Image Captioning,‚Äù in Proceedings of the 56th Annual Variational Score Distillation,‚Äù in Advances in Neural Information
Meeting of the Association for Computational Linguistics (Volume 1: ProcessingSystems,2023. 3
LongPapers),2018,pp.2556‚Äì2565. 3 [61] Y.Cao,Y.-P.Cao,K.Han,Y.Shan,andK.-Y.K.Wong,‚ÄúDreamA-
[41] S.Changpinyo,P.Sharma,N.Ding,andR.Soricut,‚ÄúConceptual vatar:Text-and-ShapeGuided3DHumanAvatarGenerationvia
12M: Pushing Web-Scale Image-Text Pre-Training To Recognize DiffusionModels,‚ÄùarXivpreprintarXiv:2304.00916,2023. 3
Long-Tail Visual Concepts,‚Äù in Proceedings of the IEEE/CVF Con- [62] H.Zhang,B.Chen,H.Yang,L.Qu,X.Wang,L.Chen,C.Long,
ferenceonComputerVisionandPatternRecognition,2021,pp.3558‚Äì F.Zhu,D.Du,andM.Zheng,‚ÄúAvatarVerse:High-quality&Stable
3568. 3 3DAvatarCreationfromTextandPose,‚ÄùinProceedingsoftheAAAI
[42] L.Huang,D.Chen,Y.Liu,Y.Shen,D.Zhao,andJ.Zhou,‚ÄúCom- Conference on Artificial Intelligence, vol. 38, no. 7, 2024, pp. 7124‚Äì
poser:Creativeandcontrollableimagesynthesiswithcomposable 7132. 3
conditions,‚ÄùinInternationalConferenceonMachineLearning,2023. [63] R.A.Gu¬®ler,N.Neverova,andI.Kokkinos,‚ÄúDensePose:DenseHu-
3 manPoseEstimationintheWild,‚ÄùinProceedingsoftheIEEE/CVF
[43] J. Xiao, K. Zhu, H. Zhang, Z. Liu, Y. Shen, Z. Yang, R. Feng, Conference on Computer Vision and Pattern Recognition, 2018, pp.
Y.Liu,X.Fu,andZ.-J.Zha,‚ÄúCCM:Real-TimeControllableVisual 7297‚Äì7306. 3
Content Creation Using Text-to-Image Consistency Models,‚Äù in [64] X. Huang, R. Shao, Q. Zhang, H. Zhang, Y. Feng, Y. Liu, and
InternationalConferenceonMachineLearning,2024. 3 Q. Wang, ‚ÄúHumanNorm: Learning Normal Diffusion Model for
[44] W. Peebles and S. Xie, ‚ÄúScalable Diffusion Models with Trans- High-qualityandRealistic3DHumanGeneration,‚ÄùinProceedings
formers,‚Äù in Proceedings of the IEEE/CVF International Conference ofthe IEEEConferenceon ComputerVision andPatternRecognition,
onComputerVision,2023,pp.4195‚Äì4205. 3 2024. 3
[45] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, [65] T. Alldieck, H. Xu, and C. Sminchisescu, ‚ÄúimGHUM: Implicit
J. Mu¬®ller, J. Penna, and R. Rombach, ‚ÄúSDXL: Improving Latent Generative Models of 3D Human Shape and Articulated Pose,‚Äù
Diffusion Models for High-Resolution Image Synthesis,‚Äù arXiv inProceedingsoftheIEEE/CVFInternationalConferenceonComputer
preprintarXiv:2307.01952,2023. 3,15 Vision,2021,pp.5461‚Äì5470. 3
[46] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Mu¬®ller, H. Saini, [66] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, ‚ÄúDe-
Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al., ‚ÄúScaling Rectified formable 3D Gaussians for High-Fidelity Monocular Dynamic
Flow Transformers for High-Resolution Image Synthesis,‚Äù in In- Scene Reconstruction,‚Äù in Proceedings of the IEEE/CVF Conference
ternationalConferenceonMachineLearning,2024. 3,15 onComputerVisionandPatternRecognition,2024,pp.20331‚Äì20341.
[47] X. Liu, J. Ren, A. Siarohin, I. Skorokhodov, Y. Li, D. Lin, X. Liu, 3,6
Z.Liu,andS.Tulyakov,‚ÄúHyperHuman:Hyper-RealisticHuman [67] L. Hu, H. Zhang, Y. Zhang, B. Zhou, B. Liu, S. Zhang, and
GenerationwithLatentStructuralDiffusion,‚ÄùinInternationalCon- L.Nie,‚ÄúGaussianAvatar:TowardsRealisticHumanAvatarMod-
ferenceonLearningRepresentations,2024. 3 eling from a Single Video via Animatable 3D Gaussians,‚Äù in
[48] M.Deitke,D.Schwenk,J.Salvador,L.Weihs,O.Michel,E.Van- IEEE/CVF Conference on Computer Vision and Pattern Recognition,
derBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi, 2024. 3
‚ÄúObjaverse:AUniverseofAnnotated3DObjects,‚ÄùinProceedingsof [68] G. Moon, T. Shiratori, and S. Saito, ‚ÄúExpressive whole-body 3d
theIEEE/CVFConferenceonComputerVisionandPatternRecognition, gaussianavatar,‚ÄùarXivpreprintarXiv:2407.21686,2024. 3
2023,pp.13142‚Äì13153. 3 [69] J. Ho, A. Jain, and P. Abbeel, ‚ÄúDenoising Diffusion Probabilistic
[49] A. Jain, B. Mildenhall, J. T. Barron, P. Abbeel, and B. Poole, Models,‚ÄùAdvancesinNeuralInformationProcessingSystems,vol.33,
‚ÄúZero-Shot Text-Guided Object Generation With Dream Fields,‚Äù pp.6840‚Äì6851,2020. 4
in Proceedings of the IEEE/CVF Conference on Computer Vision and [70] J. Tang, ‚ÄúStable-dreamfusion: Text-to-3d with stable-diffusion,‚Äù
PatternRecognition,2022,pp.867‚Äì876. 3 2022,https://github.com/ashawkey/stable-dreamfusion. 4
[50] N.MohammadKhalid,T.Xie,E.Belilovsky,andT.Popa,‚ÄúCLIP- [71] G.Metzer,E.Richardson,O.Patashnik,R.Giryes,andD.Cohen-
Mesh: Generating textured meshes from text using pretrained Or,‚ÄúLatent-NeRFforShape-GuidedGenerationof3DShapesand
image-text models,‚Äù in SIGGRAPH Asia 2022 Conference Papers, Textures,‚ÄùarXivpreprintarXiv:2211.07600,2022. 4,6
2022,pp.1‚Äì8. 3 [72] A. Zeng, X. Ju, L. Yang, R. Gao, X. Zhu, B. Dai, and Q. Xu,
[51] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal, ‚ÄúDeciWatch:ASimpleBaselinefor10√óEfficient2Dand3DPose
G. Sastry, A. Askell, P. Mishkin, J. Clark et al., ‚ÄúLearning Trans- Estimation,‚Äù in Proceedings of the European conference on computer
ferable Visual Models From Natural Language Supervision,‚Äù in vision(ECCV). Springer,2022,pp.607‚Äì624. 4
International Conference on Machine Learning. PMLR, 2021, pp. [73] N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J.
8748‚Äì8763. 3 Black, ‚ÄúAMASS: Archive of motion capture as surface shapes,‚Äù17
inProceedingsoftheIEEE/CVFInternationalConferenceonComputer Ailing Zeng (Member, IEEE) is a senior re-
Vision,2019,pp.5442‚Äì5451. 4 searcheratTencentAILab.Previously,sheob-
[74] A. Mohr and M. Gleicher, ‚ÄúBuilding efficient, accurate character tainedherPhDdegreefromtheDepartmentof
skinsfromexamples,‚ÄùACMTransactionsonGraphics(TOG),vol.22, Computer Science and Engineering, the Chi-
no.3,pp.562‚Äì568,2003. 4,6 neseUniversityofHongKong.Herresearchtar-
[75] I.PantazopoulosandS.Tzafestas,‚ÄúOcclusionCullingAlgorithms: gets to build multi-modal human-like intelligent
A Comprehensive Survey,‚Äù Journal of Intelligent and Robotic Sys- agentsonscalablebigdata,especiallyforLarge
tems,vol.35,pp.123‚Äì156,2002. 5 MotionModelstocapture,understand,interact,
[76] A. Gue¬¥don and V. Lepetit, ‚ÄúSuGaR: Surface-Aligned Gaussian and generate the motion of humans, animals,
SplattingforEfficient3DMeshReconstructionandHigh-Quality andtheworld.Shehaspublishedoverthirtytop-
Mesh Rendering,‚Äù in Proceedings of the IEEE/CVF Conference on tierconferencepapersatCVPR,NeurIPS,etc.
Computer Vision and Pattern Recognition, 2024, pp. 5354‚Äì5363. 6,
15
[77] J. Waczyn¬¥ska, P. Borycki, S. Tadeja, J. Tabor, and P. Spurek,
‚ÄúGaMeS: Mesh-Based Adapting and Modification of Gaussian
Splatting,‚ÄùarXivpreprintarXiv:2402.01459,2024. 6
[78] T. Von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and Zheng-Jun Zha (Member, IEEE) received the
G.Pons-Moll,‚ÄúRecoveringAccurate3DHumanPoseinTheWild BEandPhDdegreesfromtheUniversityofSci-
UsingIMUsandaMovingCamera,‚ÄùinProceedingsoftheEuropean enceandTechnologyofChina,Hefei,China,in
conferenceoncomputervision(ECCV),2018,pp.601‚Äì617. 8 2004 and 2009, respectively. He is currently a
[79] R.Li,S.Yang,D.A.Ross,andA.Kanazawa,‚ÄúLearntoDancewith fullprofessorwiththeSchoolofInformationSci-
AIST++:MusicConditioned3DDanceGeneration,‚Äù2021. 8 enceandTechnology,UniversityofScienceand
[80] J.Lin,A.Zeng,S.Lu,Y.Cai,R.Zhang,H.Wang,andL.Zhang, Technology of China, and the executive direc-
‚ÄúMotion-X:ALarge-scale3DExpressiveWhole-bodyHumanMo- torwiththeNationalEngineeringLaboratoryfor
tionDataset,‚ÄùinAdvancesinNeuralInformationProcessingSystems, Brain-Inspired Intelligence Technology and Ap-
2023. 8,14 plication(NEL-BITA).Hehasauthoredorcoau-
[81] H. Yi, H. Liang, Y. Liu, Q. Cao, Y. Wen, T. Bolkart, D. Tao, and thored more than 200 papers in his research
M.J.Black,‚ÄúGeneratingHolistic3DHumanMotionfromSpeech,‚Äù field with a series of publications on top journals and conferences,
in Proceedings of the IEEE/CVF Conference on Computer Vision and whichincludemultimediaanalysisandunderstanding,computervision,
PatternRecognition,2023. 8 patternrecognition,andbrain-inspiredintelligence.Hewasarecipientof
[82] J.T.Barron,B.Mildenhall,D.Verbin,P.P.Srinivasan,andP.Hed- multiplepaperawardsfromprestigiousconferences,includingtheBest
man,‚ÄúMip-NeRF360:UnboundedAnti-AliasedNeuralRadiance Paper/Student Paper Award in Association for Computing Machinery
Fields,‚ÄùinProceedingsoftheIEEE/CVFconferenceoncomputervision (ACM) Multimedia and AAAI Distinguished Paper. He serves/served
andpatternrecognition,2022,pp.5470‚Äì5479. 14 as an associated editor for IEEE Transactions on Multimedia, IEEE
TransactionsonCircuitsandSystemsforVideoTechnology,etc.
LeiZhang(Fellow,IEEE)receivedthePhDde-
greeincomputersciencefromTsinghuaUniver-
sity,Beijing,China,in2001.Heiscurrentlythe
YukunHuangisaPost-doctoralResearchFel-
chief scientist of computer vision and robotics
lowattheHKUMusketeersFoundationInstitute
with International Digital Economy Academy
of Data Science (HKU IDS). Previously, he ob-
(IDEA)andanadjunctprofessorwiththeHong
tained his PhD degree from the University of
Kong University of Science and Technology,
Science and Technology of China (USTC) and
Guangzhou,China.Priortohiscurrentpost,he
didhisundergraduatestudiesattheSouthChina
was a principal researcher and research man-
UniversityofTechnology.Hisresearchinterests
ager with Microsoft. He has authored or coau-
broadlylieinthecomputer visionandmachine
thored more than 150 techinical papers, and
learning. In particular, he is interested in 3D
holds more than 60 U.S. patents in his research field, which include
synthesis,virtualhuman,generativemodel,and
computervisionandmachinelearning,withparticularfocusongeneric
personre-identification.
visualrecognitionatlargescale.Hewasaeditorialboardmemberfor
IEEE Transactions on Multimedia, IEEE Transactions on Circuits and
SystemsforVideoTechnology,andMultimediaSystemJournalandas
theareachairofmanytopconferences.
Jianan Wang received the MSc degree from Xihui Liu (Member, IEEE) is an assistant pro-
the University of Oxford and currently serves fessoratDepartmentofElectricalandElectronic
asthechief researcherinAIcognition atAstri- EngineeringandInstituteofDataScience,The
bot.ShehaspreviouslyworkedwithDeepMind University of Hong Kong. Before joining HKU,
andtheInternationalDigitalEconomyAcademy shewasapostdoctoralresearcheratUniversity
(IDEA).Herresearchinterestsandpublications ofCalifornia,Berkeley.ShereceivedtheBache-
spancomputervisionandmachinelearningthe- lor‚Äôs degree from Tsinghua University and PhD
ory, with a recent focus on generative AI and degree from The Chinese University of Hong
robotics. Kong. Her research interests include computer
vision, deep learning, generative models, and
multimodal AI. She was awarded Adobe Re-
search Fellowship 2020, EECS Rising Stars 2021, and WAIC Rising
StarAward2022.SheservesasareachairsforCVPR2024,ACMMM
2024,andICLR2025.