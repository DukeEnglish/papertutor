ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation
ChenHuangâ™ â™£, YipingJinâ™¡, IlijaIlievskiâ™¢, WenqiangLeiâ™ â™£*, JianchengLvâ™ â™£
â™ CollegeofComputerScience,SichuanUniversity,China
â™¡NLPGroup,PompeuFabraUniversity,Spain
â™¢DepartmentofIndustrialSystemsEngineeringandManagement,NationalUniversityofSingapore,Singapore
â™£EngineeringResearchCenterofMachineLearningandIndustryIntelligence,MinistryofEducation,China
huangc.scu@gmail.com,yiping.jin@upf.edu,wenqianglei@gmail.com
Abstract Unlabeled set Manual Annotation Labeled set
â‘  Input â‘¡ Annotation
Human annotation is a time-consuming task ð‘¥ð‘¥ð‘¡ð‘¡
thatrequiresasignificantamountofeffort. To
Interactive Annotatioð‘¥ð‘¥nð‘¡ð‘¡,ð‘¦ð‘¦ð‘¡ð‘¡
â‘  â‘¡ Suggestion â‘¢ Accept suggestion or
addressthisissue,interactivedataannotation
ð‘¥ð‘¥ð‘¡ð‘¡ Correct it by re-annotation
utilizesanannotationmodeltoprovidesugges- ð‘¥ð‘¥ð‘¡ð‘¡,ð‘“ð‘“ð‘¡ð‘¡(ð‘¥ð‘¥â‘£ð‘¡ð‘¡) Update Model Annotator
tionsforhumanstoapproveorcorrect. How-
ever, annotation models trained with limited Figure1: Comparisonbetweenmanualannotationand
labeleddataarepronetogeneratingincorrect interactiveannotation.
suggestions,leadingtoextrahumancorrection
effort. To tackle this challenge, we propose
ARAIDA, an analogical reasoning-based ap- 2018,2020;Leetal.,2021). AsillustratedinFig.1,
proachthatenhancesautomaticannotationac-
these methods introduce an annotation model to
curacyintheinteractivedataannotationsetting
suggest labels (model annotations) to human an-
and reduces the need for human corrections.
notators. Theannotatorsacceptasuggestedlabel
ARAIDA involves an error-aware integration
ifitiscorrect. Otherwise,theyhavetocorrectthe
strategythatdynamicallycoordinatesananno-
tationmodelandak-nearestneighbors(KNN) label manually. Compared to manual annotation,
model,givingmoreimportancetoKNNâ€™spre- interactive annotation requires less human effort
dictionswhenpredictionsfromtheannotation becausehumanannotatorsonlyhavetoverifythe
modelaredeemedinaccurate. Empiricalstud-
model annotations instead of coming up with an
ies demonstrate that ARAIDA is adaptable to
answerfromscratch,leadingtopotentialspeedup
differentannotationtasksandmodels. Onav-
oftheannotationprocess(Klieetal.,2020).
erage, it reduces human correction labor by
11.02% compared to vanilla interactive data Evidently, the annotation modelâ€™s accuracy is
annotationmethods. crucialbecauseincorrectsuggestionsrequireaddi-
tionalhumanefforttorectify. Existingmethodsup-
1 Introduction
datetheannotationmodelbasedonpreviouslyac-
Dataannotationisachallengingtaskthatinvolves ceptedorcorrecteddata(ground-truthannotation),
atradeoffbetweenannotationqualityandbudget. aimingtoreducehumancorrectionsbyimproving
While some platforms offer a cost-effective solu- prediction accuracy at each iteration (Klie et al.,
tion by relying on ML models to annotate data 2020;Wuetal.,2022). However,inthecontextof
automatically 1, the quality of such annotations limiteddataannotation,theannotationmodellacks
is often compromised (Wang et al., 2022a). It is sufficient labeled training data to reach a reason-
particularly true in the limited data annotation ableaccuracyandispronetoprovidingincorrect
scenario where the annotation budget is limited suggestions(RietzandMaedche,2021). Forexam-
orwhenunlabeleddataarescarce(Ringgeretal., ple,inthespanrelationannotationexampleshown
2007;Chaudharyetal.,2021). in Fig. 2 (blue), the annotation model continues
Human-machineinteractiveannotationmeth- tomakemistakesonsimilarexamples([car,tyre])
ods were introduced to reduce annotation effort evenafterthehumanannotatorcorrectsthelabel
while maintaining annotation quality (Klie et al., â€˜[tree,leaf]=>componentâ€™. Asaresult,thisleads
to more human corrections. Such a problem is
*CorrespondencetoWenqiangLei.
crucialforinteractiveannotationandhasbeeniden-
1For example, https://aws.amazon.com/sagemaker/
groundtruth/. tifiedbyrecentwork(RietzandMaedche,2021),
4202
yaM
02
]LC.sc[
1v21911.5042:viXraRound Round Round with ARAIDA
[tree, leaf]ð‘¡ð‘¡: ? [car, tyre]ð‘¡ð‘¡:2 ? [car, tyre]: ? t 2 KNN Inference
KNN Annotation
component
[tree, leaf]: synonym [car, tyre]: synonym [car, tyre]: synonym
Errorprediction on
similar annotated data
Correction No 0.7*component 0.7
Correction Labors Correction +0.3*synonym 0.3
CorrectWrong
[tree, leaf]: component Update [car, tyre]: component [car,tyre],component Error-aware Integration Strategy
Figure2: Exampleonspanrelationannotation. Anunder-trainedannotationmodelresultsinmoresuggestionerrors
andincreaseshumancorrectioneffort. ARAIDAimprovesthemodelannotationaccuracyviatheKNNmodeland
[tree, leaf], component [tree, leaf]: component
theerror-awareintegrationstrategyfordynamicalcoordinationofannotations.
f1 Synonym f0 f1 Synonym
butithasyettobeaddressed. ourcontribUuptdioatnesareasfollows:
Inspiredbycognitivestudiesonefficientlearn-
â€¢ Calling attention to the limited data annotation
ing (Lake et al., 2017, 2015; Mitchell, 2021),
scenario. We highlight the under-trained prob-
finding that the human brain can learn from a
lemoftheannotationmodel,whichiscrucialin
few examples because our brain is continuously
practicebutoverlookedininteractiveannotation.
buildinganalogiesduringthelearningprocessof
â€¢ Introducing ARAIDA thatinvolvesaKNNmod-
conceptstofacilitatecomprehension,wepropose
uleandanerror-awareintegrationstrategytoal-
AnalogicalReasoning-AugmentedInteractiveData
leviatetheunder-trainedproblembyfacilitating
Annotation (ARAIDA), which is designed to im-
coordinationbetweenthetwomodelannotators
proveinteractiveannotationunderthelimiteddata
(i.e.,thevanillaannotationmodelandKNN).
annotation setting. ARAIDA provides an annota-
â€¢ DemonstratingtheefficacyofARAIDAinenhanc-
tionreferencetotheannotationmodelbyretrieving
ingsuggestionaccuracy,reducinghumancorrec-
previously human-labeled examples in the prox-
tions,andshowcasingitsflexibilitytocombine
imity of the example in consideration using the
with various annotation models through exten-
k-nearestneighbors(KNN)method. Asillustrated
siveexperiments.
in Fig. 2(red), the final suggestion combines the
modelannotationandtheannotationreferencepro-
videdbyKNNviaanerror-awareintegrationstrat- 2 RelatedWork
egy. Thisstrategydynamicallycoordinatesthean-
Ourresearchistiedtointeractivedataannotation,
notationmodelandKNN,givingmoreimportance
humananalogicalreasoning(KNN),andretrieval-
toKNNâ€™spredictionifthepredictedlabelfromthe
based language models. We provide a literature
annotationmodelisestimatedtobeinaccurate.
reviewandhighlightourdifferences.
Weconductsimulatedexperimentsforthelim-
ited data annotation task and estimate the hu- Interactive Data Annotation. Interactive data
man annotation effort based on the number of annotationaimstoreducehumanannotationeffort
human corrections (or the number of suggestion byincorporatinganannotationmodelthatsuggests
errors) following Hwa (2000) and Kristjansson labels to human annotators during an interactive
etal.(2004). Wetest ARAIDA ondifferentword- process (Klie et al., 2018, 2020; Le et al., 2021).
levelandsentence-levelannotationtasks,combin- Theannotationmodelmustbesample-efficientbe-
ingwithdifferentannotationmodels(i.e.,classic cause,whenwestartanewannotationtask,there
and LLM-based models). The result shows that are few labeled examples to learn from. Current
ARAIDA consistently improves different annota- studies focus on employing active learning (Klie
tion modelsâ€™ accuracy across various tasks. On et al., 2018; Laws et al., 2011; Casanova et al.,
average,itreduceshumancorrectionsby11.02%. 2020;Lietal.,2021;Huangetal.,2023)toprior-
Furtheranalysisattributesthisimprovementtothe itizeannotatingexamplesmorelikelytoimprove
few-shot capability of the KNN module and the modelaccuracy. Whileactivelearningcanreduce
error-awareintegrationstrategythateffectivelysyn- the required training data to some extent, it may
ergizescomplementaryannotations. Insummary, notbeeffectiveinlimiteddataannotationscenariosor when complex hypotheses or semantics are to cantlyfortwomainreasons: 1)Differenttasks. We
be learned (Dasgupta, 2005; Rietz and Maedche, arethepioneersinintroducingKNNtotheinterac-
2021). AnotherapproachistoemployLLMsforau- tive data annotation task, whereas these methods
tomaticdataannotation,whichhavedemonstrated areprimarilydesignedformachinetranslation. 2)
strongperformanceunderzero-shotandfew-shot Differenttechniques. Weadjusttheweightbyesti-
settings(Heetal.,2023;Gilardietal.,2023). How- matingtheerrorofmodelpredictionsforeachdata
ever,suchperformancemightnotbeconsistentfor point(e.g.,sentence),whereasthesemethodslearn
difficult tasks, as they may even perform worse theweightforeachtokenwithouterrorestimation.
thanfine-tunedsmalllanguagemodels(Xiaoetal.,
2023). Regardlessofwhetherweuseactivelearn- 3 ARAIDA: TheProposedMethod
ing and whether we use a classic or LLM-based
annotationmodel,ourempiricalevidencedemon- We present ARAIDA, an analogical reasoning-
strates that ARAIDA can effectively decrease the basedmethodforinteractivedataannotationthat
amountofhumancorrectionsrequired. providesanannotationreferencetotheannotation
modelbyretrievingpreviouslyhuman-labeledex-
KNNandAnalogicalReasoning. WhileKNN
amplesintheproximityoftheexampleinconsid-
has been extensively utilized in NLP commu-
eration. We detail the KNN inference module in
nity (Wang et al., 2019; Liu et al., 2023; Wang
Section3.1andtheerror-awareintegrationstrategy
et al., 2022b), its underlying mechanism is of-
inSection3.2. Finally,theoptimizationdetailsare
ten overlooked. To shed light on this, cognitive
providedinSection3.3.
studies (Lake et al., 2017, 2015; Mitchell, 2021)
revealed that the KNN inference process aligns Task Formalization and Overview. Let X de-
withhumananalogicalreasoning,enablingefficient notethedatasetthatneedstobeannotated,withC
learning (Lake et al., 2017, 2015). In particular,
being the number of classes. Given a data batch
analogical reasoning establishes connections be- x attimet,theannotationmodelf predictslabel
t t
tweenrelevantaspectsofthecurrenttaskandpast vectors f t(x t) âˆˆ R|xt|Ã—C, and the KNN module
experiences,formingabstractionsthatenhancehu- g
t
inferslabelvectorsg t(x t) âˆˆ R|xt|Ã—C usingpre-
man reasoning capabilities (Mitchell, 2021). In viously annotated data stored in a datastore A .
t
thiscontext,KNNfacilitatessample-efficientlearn- Then, we estimate the probability Î»
t
âˆˆ R|xt|Ã—1
ingbyleveragingsimilaritiesbetweentheexample of the annotation modelâ€™s predictions f (x ) be-
t t
tobelabeledandpreviouslyannotatedexamples, ing reliable, i.e., argmax(f (x )) = y , where
t t t
resulting in exemplary solutions (Bautista et al., argmax(Â·)returnsindicesoftheclasseswiththe
2016),whichreducethetrainingdatarequirement. highestpredictedprobabilityandy
t
âˆˆ R|xt|Ã—1 are
thegroundtruthlabels. Finally,weuseÎ» toweigh
Retrieval-Based Language Models. There is t
the two predictions f (x ) and g (x ) through a
growing interest in enhancing the output of lan- t t t t
linearweightedcombination:
guage models by incorporating a retrieval mod-
ule(usuallyKNNoralike)thatinterpolateswitha
datastorebuiltfromthetrainingdata(Khandelwal F t(x t) = Î» tÂ·f t(x t)+(1âˆ’Î» t)Â·g t(x t). (1)
et al., 2019; Kassner and SchÃ¼tze, 2020). Com-
paredtovanillalanguagemodels,retrieval-based Notably,closed-sourcelanguagemodelssuchas
modelsgroundthepredictionsinlabeledtraining ChatGPT produce discrete labels rather than pre-
examples,potentiallyyieldingbetterexplainability dicteddistributions. Therefore,wecannotcombine
andsampleefficiency(Asaietal.,2023). Thisap- itspredictionswithKNNâ€™susinglinearcombina-
proachhasshownpromisingresultsintaskssuch tion. In such case, we use binary values (0 or 1)
as machine translation (Khandelwal et al., 2021; for Î» , which acts as a function allocation to de-
t
Liuetal.,2023),namedentityrecognition(Wang terminewhetherf org shouldapplytoeachex-
t t
et al., 2022b), and question answering (Kassner ample. Once the human approves or corrects the
andSchÃ¼tze,2020). Whilesomestudieshaveex- finalsuggestions,thedatastoreA isupdatedwith
t
plored the use of dynamically adjusted combina- thenewlyarriveddatabatchanditscorresponding
tionweightsbetweenthelanguagemodelandthe labels. Inaddition,theannotationmodelf (ifap-
t
retrievalmodule(Wangetal.,2021;Zhengetal., plicable),KNNmoduleg ,andweightingstrategy
t
2021;Jiangetal.,2021),ourmethoddifferssignifi- Î» areupdatedviaback-propagation.
t3.1 KNNInference Error Estimation of Model Annotation. We
base on the intuition that if the model f consis-
WeutilizeaweightedKNNtoperforminference, t
defined as g t(xi t) = (cid:80) (cid:80)aâˆˆ aâˆˆÏi Ïiw wa aya , where Ï
i
âˆˆ A
t
t le an rt tl oy tm heak ce us rrm enis tt da ak te as po on inp tre xv
i
ti ,o tu hs enex ita sm pp rele ds ics ti im oni-
is the k nearest neighbors of each example xi, f (xi)willlikelybeincorrect. Toachievethis,we
t t t
y a corresponds to the human annotation of each parameterizetheintegrationstrategyÎ» t asaneural
neighbor a âˆˆ Ï i. The similarity between xi
t
and networktolearnfromthecustomizedinput.
a is measured byw = 1 , where d(xi,a) =
âˆ¥w (xi âˆ’a)âˆ¥
isa
a
did s( tx ai t n,a c)
e metric
parat
meter-
â€¢ CustomizedInputs. For each data point xi t, we
knn t 2 derivetheinputxi totheintegrationstrategyÎ» ,
ized by w . We use the similarity measure to t t
knn which considers the local error estimation Ei
retrieve Ï . To avoid overconfidence in KNN in- t
i andlocaldensityDi. Specifically,Ei isavector
ference, we apply label smoothing to the labels t t
withelementsei = 1[argmax(f (a )) = y ]
of the retrieved neighbors. Specifically, we set t,j t j aj
indicates if the annotation model f predicted
y = y (1âˆ’Î±)+Î±/C,whereÎ± = 1âˆ’ 1. t
a a C correctlyoneachannotatedexamplea âˆˆ Ï in
j i
DatastoreMaintenanceStrategy. Thedatastore thek nearestneighborsofxi. Thelocaldensity
t
A consistingofhistoricallyannotateddatagrows Di isadistancevector,witheachelementbeing
t t
insizeastheinteractiveannotationcontinues,caus- d(xi t,a j). Thesetwovectorsarecombinedusing
ing the KNNâ€™s retrieval to be less time-efficient. the element-wise multiplication operator âŠ™ to
To address this issue, we impose a constraint on createtheinput: xi = DiâŠ™Eiâˆ’DiâŠ™(1âˆ’Ei).
t t t t t
the maximum datastore size using a pre-defined Notably, xi measures the error regularity of xi
t t
hyperparameter. Weproposeaclass-awaremainte- among its neighborhood, as the more positive
nancestrategy. Precisely,ifA t exceedsitsbudget, values in the vector xi t, the less likely f t would
datathatisfromthemajorityclass2 andmostsim- makeanerroronxi.
t
ilar to its class prototype3 is discarded first. This â€¢ LearningObjectives. Wecollectthegroundtruth
strategyensuresthatthedatastorecontainsasmany labelsy throughhumanfeedback. Tooptimize
t
labeleddatafromdifferentclassesaspossiblewhile the error-aware integration strategy Î» , we use
t
minimizingtheimpactontheclassprototype. Ap- a mean squared error (MSE) loss, denoted as
pendixA.1andA.3presentexperimentsusingdif- â„“t(y ,f (x ),Î» ) = MSE(1[argmax(f (x )) =
d t t t t t t
ferentdatastoresizesandmaintenancestrategies. y ],Î» (x )), where 1[Â·] indicates whether the
t t t
ground truth labels y are the same as the pre-
t
3.2 Error-awareIntegrationStrategy
dictionsf (x ). Thepurposeofthislossfunction
t t
Motivation. A popular method to combine an- istoguideÎ» byencouragingittopredicterrors
t
notationsfromtwomodels(i.e.,annotationmodel madebyf .
t
and KNN) is to use a weighted linear combina-
tion with a constant weight 4 (Liu et al., 2023;
3.3 Optimizationof ARAIDA
Wangetal.,2022b). However,assumingthatone
Tosimplifytheoptimizationprocess,weindepen-
model consistently outperforms the other on all
dently optimize the annotation model f , KNN
unlabeled data is unrealistic. Furthermore, both t
modelg ,anderror-awareintegrationstrategyÎ» .
modelsareupdatedwitheachnewbatchofdatain t t
We treat human feedback y as the ground truth
interactive data annotation, and their relative per- t
following previous studies on interactive annota-
formance will alter, making it infeasible to find
tion (Klie et al., 2018, 2020; Le et al., 2021). It
theoptimalweightthroughaone-offhyperparam-
eter tuning. To address this issue, we propose an
is worth noting that ARAIDA supports any task-
specificannotationmodelandusesitscorrespond-
error-awareintegrationstrategythatautomatically
inglossfunctionâ„“ toupdatetheparameters. Com-
assignsweightstodifferentmodels,relyingmore f
biningtheâ„“ lossandthenegativelog-likelihood
on KNN inference when the annotation modelâ€™s d
loss â„“ to optimize KNN, we formulate the final
predictionisestimatedtobeinaccurate. g
lossfunctionasfollows:
2The majority class refers to the class with the highest
freq 3u Te hn ecy clain ssA pt r.
ototypesaretheaverageofthefeaturevectors
L(f,g,Î»)=(cid:88)Bt
â„“ (y ,f (x ))+â„“ (y ,g (x ))
f i t i g i t i
inA thatbelongtoeachclass. (2)
t i=1
4EquivalenttowhenÎ» t(x t)inEq.1isaconstant. +â„“ d(y i,f t(x i),Î» t),where B represents the total data accumulated Dataset #Val. Classes
t
until round t. There are two challenges to op- WN18RR 3,034 Hypernym; Derivation; Member;
Component; Synset; Synonym;
timizing this objective function. Firstly, the op-
Verbgroup;Instanceofhypernym;
erator used in KNN to retrieve the k nearest
FreeBase 5,116 Contains; Country; Track_role;
neighbors is not differentiable. To address this
Profession;Group_role;Adjoins;
problem,weutilizetheGumbel-softmax-basedre-
Film_release;Nutrient
parameterization trick (Jang et al., 2016) to facil-
IMDB 5,000 Positive;Negative
itatetheoptimizationprocess. Secondly,theloss SST-5 1,101 Strongpositive;Positive;Neutral;
function L presents a bi-level optimization prob- Negative;Strongnegative
lem,wheretheoptimizationofÎ» isnestedwithin
t
Table 1: Statistics of datasets. For each dataset, we
the optimization problems of f and g . As a re-
t t
randomlysample5Kexamplesfromtheoriginaltrain-
sult, we update f , g , and Î» iteratively using a
t t t ingdatasettoformtheunlabeleddata,andthevalida-
coordinate-descentapproach. Formally,ateachop-
tiondatasetistakenfromtheoriginaldataset. Table4
timizationiterationk,wehavenetworkparameters
presentsthemappingfromtheoriginalcategoriestothe
Î¸k, Î¸k, and Î¸k corresponding to fk, gk, and Î»k. categoriesweuse.
f g Î»
Theupdateproceduresareasfollows:
Î¸k+1 = Î¸k âˆ’â–½ L(f,gk,Î»k), ments, namely the WN18RR (Dettmers et al.,
f f f
2018)5 and Freebase (Bollacker et al., 2008)6
Î¸k+1 = Î¸k âˆ’â–½ L(fk,g,Î»k), (3)
g g g dataset. Weexperimentwiththeeightmostfre-
Î¸ Î»k+1 = Î¸ Î»k âˆ’â–½ Î»L(fk+1,gk+1,Î»). quentclassesforeachdataset.
4 Experiments â€¢ Sentence-levelannotation. Weconsiderthesen-
timentclassificationtaskandexperimentontwo
We conduct extensive experiments to assess
benchmark datasets, including SST-5 (Socher
ARAIDAâ€™s effectiveness in the limited data anno-
et al., 2013)7, and IMDB (Maas et al., 2011)8.
tation scenario. Our primary focus is to assess
SST-5 dataset contains categories on a scale of
whether ARAIDA can decrease the human effort
1-5 while IMDB contains two categories (posi-
requiredforcorrectionsbyprovidingmoreprecise
tive/negative).
annotationsatvariousstagesoftheannotationpro-
cess(seeSection4.2). Furthermore,weperforma
EvaluationMetric. Weaimtominimizethetotal
comprehensiveexaminationtoinvestigatethebe-
humancorrections(i.e.,thetotalmodelsuggestion
havior and impact of KNN and the error-aware
errors) annotating a given amount of data using
integrationstrategy(seeSection4.3). Additional
theinteractiveannotationprocess. Therefore,we
analysisofourerror-awareintegrationstrategyis
reporttheMachineCumulativeAccuracy(MCA),
presented in Section 4.4. Lastly, we analyze the
definedasthetotalcorrectsuggestionsdividedby
sensitivityoftheparametersinAppendicesA.
thetotalsuggestionsfordifferentdatasetsizes. To
assesstheperformanceofeachmethodinthelim-
4.1 ExperimentalSetup
iteddataannotationscenario,wepresentthemean
Tasks & Datasets. We experiment with word-
andthecorrespondingstandarddeviationbyvary-
and sentence-level annotation tasks. These tasks
ingthedatasetsize({1K,2K,3K,4K,5K}).
have been highlighted as crucial in various web
applications (Yaoetal.,2021;Marcos-Pablosand AnnotationModels. Toverifythegeneralizabil-
GarcÃ­a-PeÃ±alvo, 2020; Lee et al., 2022). To sim- ity of ARAIDA, we apply it in conjunction with
ulate the scenario of limited data annotation, we differentannotationmodels:
followDouetal.(2019)byimposingdatasetsize
â€¢ Classicannotationmodels. We utilize
restrictions, ranging from 1K to 5K. Table 1
lightweight annotation models following
overviewsthedatasetstatistics.
5https://paperswithcode.com/dataset/wn18rr
â€¢ Word-levelannotation. Wefocusontheknowl- 6https://www.microsoft.com/en-us/download/
edge graph completion task, which annotates details.aspx?id=52312
the semantic classes of input word pairs (e.g.,
7https://nlp.stanford.edu/sentiment/code.html
8https://www.kaggle.com/
â€˜[tree,leaf]=>componentâ€™). Weusetwobench-
datasets/lakshmi25npathi/
mark knowledge graph datasets in our experi- imdb-dataset-of-50k-movie-reviewsWithoutActiveLearning(AL) WithActiveLearning(AL)
Annotation
Word-levelAnnotation Sentence-levelAnnotation Word-levelAnnotation Sentence-levelAnnotation
Model
WN18RR FreeBase IMDB SST-5 WN18RR FreeBase IMDB SST-5
Dist./FT 50.44Â±1.02 32.47Â±12.37 70.18Â±8.43 36.02Â±3.14 47.77Â±0.91 26.92Â±10.85 66.98Â±6.31 35.20Â±3.76
Dist./FT+ARAIDA 52.16Â±1.37 43.02Â±6.43 79.33Â±2.81 37.21Â±3.03 49.54Â±0.84 39.06Â±4.57 75.84Â±1.14 37.02Â±2.50
LLaMa2 31.35Â±1.89 24.41Â±1.77 80.21Â±2.64 37.83Â±1.64 31.35Â±1.89 24.41Â±1.77 80.21Â±2.64 37.83Â±1.64
LLaMa2+ARAIDA 45.15Â±1.96 38.20Â±1.91 88.47Â±2.03 42.03Â±1.88 46.33Â±2.01 38.79Â±1.96 89.68Â±2.25 42.61Â±1.91
LLaMa2sft 58.24Â±2.79 53.11Â±1.38 94.06Â±9.02 46.84Â±6.30 59.28Â±2.57 55.39Â±2.01 95.13Â±10.15 47.45Â±6.17
LLaMa2sft+ARAIDA 60.74Â±2.33 55.23Â±1.46 95.15Â±9.32 49.62Â±5.98 61.02Â±2.18 56.71Â±2.09 95.88Â±12.27 49.51Â±6.03
Table2: Machinecumulativeaccuracy(MCA)scoresusingvariousmethods. Wereporttheaveragedresultsacross
varyingamountsofdata. LLaMa2withALhasidenticalperformanceasLLaMa2becausethevanillaLLaMa2
modelisnotupdatedduringtheinteractiveannotationprocess.Thus,thedataorderdoesnotimpacttheperformance.
WhencombinedwithARAIDA,theperformancesw/andw/oALaredifferentbecausetheKNNandintegration
strategymodelsareupdated.
previous works (Desmond et al., 2021; Chen make more mistakes. Previous studies focused
etal.,2020;Hedderichetal.,2021). Specifically, on applying active learning in interactive annota-
for word-level tasks, we use a distributional tiontoenhancetheannotationmodelsâ€™sampleeffi-
model (Roller et al., 2014; Kober et al., 2021) ciency(Lawsetal.,2011;Klieetal.,2018;Lietal.,
with pretrained GloVe word embeddings 2021). Tostudytheimpactofactivelearninginthe
embedding (Pennington et al., 2014)9. For limited data annotation scenario, we compare an
sentence-level tasks, we use FastText (Joulin uncertainty-basedactivelearningmethodwithran-
etal.,2017)toderivethesentenceembeddings. domdataorderingfordifferentannotationmodels
WedenotethisbaselineasDist./FT. withandwithout ARAIDA. Notethatweomitthe
ChatGPTwithALresultsbecauseweareunableto
â€¢ LLM-basedannotators. We also use large lan-
estimateitspredictionuncertaintyaccurately.
guage models (LLMs) as annotation models,
whose few-shot and in-context learning capa- Implementation Details. All experiments are
bilities might help with the limited data anno- carried out on a machine with Intel(R) Xeon(R)
tation process. We experiment with LLaMa2- Gold5317CPU@3.00GHzandaGeForceRTX
7B(Touvronetal.,2023)andChatGPT(Ouyang 3090GPU.Forsimplicity,weimplementourinte-
etal.,2022)10. Weusezero-shotandfew-shot grationstrategyusingathree-layer,fully-connected
promptsforChatGPT(denotedasChatGPT network with ReLu activation and dropout. For
zero
and ChatGPT few). Detailed prompts can be KNN, we set k = 20 for Ï t. KNN runs in the
foundinTable5. ForLLaMa2,weconsiderboth
embeddingspaceoftext-embedding-ada-002(Nee-
thevanillaLLaMa2thatusesthesamezero-shot lakantanetal.,2022)whencombiningwithChat-
promptsasChatGPT andLLaMa2 ,which GPT.ForDist./FTandLLaMa2models,KNNruns
zero sft
isfine-tunedusinganopen-sourcetoolkit11 dur- in the corresponding modelâ€™s embedding space.
ing the interactive annotation process. Fine- Moreover, we leave the details of LLM prompts
tuningdataexamplescanbefoundinTable6. andmodefine-tuningexamplesinAppendixB.
4.2 MainResult
Impact of Active Learning. Regardless of the
annotationmodel,thesequenceofthedatatoanno- We evaluate the effectiveness of ARAIDA in en-
tatealsoaffectstheamountofhumancorrections. hancing the model annotation quality, hence re-
Intuitively,ifweshowunambiguousexamplesfirst, ducing human correction effort. Table 2 and
few corrections are needed. However, the anno- Figure 3 show the machine cumulative accuracy
tation model and KNN may not learn to handle scores averaged across varying amounts of data
morechallengingexamplesandmaysubsequently ({1K,2K,3K,4K,5K})fordifferentexperimen-
talsettings. Wemakethefollowingobservations:
9Wedidnotusecontextualizedembeddingsbecausethe
word-leveltaskinourexperimenthasnocontext.
ARAIDA reduces human corrections consis-
10The gpt-3.5-turbo checkpoint in OpenAIâ€™s tently. As illustrated in Table 2 and Figure 3,
API (https://platform.openai.com/docs/models/ ARAIDA consistentlyimprovesthemodelsugges-
gpt-3-5).
tion accuracy across different annotation models
11https://github.com/Alpha-VLLM/
LLaMA2-Accessory and annotation tasks. Specifically, it achieves aAction Sensitivity Inconsistent Recommendation Consistent Recommendation
Sensitivity (inconsistent and inaccurate) Diversity (inconsistent but accurate)
Consistent Recommendation Sensitivity (inconsistent and inaccurate)
Inconsistent Recommendation Diversity (inconsistent but accurate)
Action Sensitivity
99.62% 47.52% 99.76% 55.64%
99.62% 47.52%
52.48% 44.36%
72.55% 87.03%
52.48% 27.45%
27.45% 12.97%
(a) Redial (b) OpendialKG 72.55%
(a) Redial
99.76% 55.64%
44.36%
Action Sensitivity 12.97%
Inconsistent Recommendation Diversity (inconsistent but accurate)
87.03%
moreconservativeinmakingcorrections,yielding
Consistent Recommendation Sensitivity (inconsistent and inaccurate) (b) OpendialKG 95 CChhaattGGPPTTzzeerroo
ACRhaAtGIDPAT+zeCroh a+t GAPRTAzIDerAo a smaller but consistent improvement. It demon-
85
CChhaattGGPPTTffeeww
strates ARAIDAâ€™srobustnesstocombinewithan-
75 ACRhaAtGIDPAT+feCwh +a tAGRPTAfIeDwA
notationsmodelswithdifferentperformances.
65
55 Annotation Word-levelAnnotation Sentence-levelAnnotation
Model WN18RR FreeBase IMDB SST-5
45 Dist./FT
ARAIDA 52.16Â±1.37 43.02Â±6.43 79.33Â±2.81 37.21Â±3.03
35
-w/oKNN 50.44Â±1.02 32.47Â±12.37 70.18Â±8.43 36.02Â±3.14
WN18RR FreeBase IMDB SST-5
-w/of(Â·) 50.28Â±3.01 41.52Â±5.87 78.48Â±0.68 35.02Â±1.37
Word-level Annotation Sentence-level Annotation -w/const. 51.85Â±4.77 41.78Â±6.78 78.58Â±3.94 37.07Â±2.68
LLaMa2sft
Figure3: MCAscoresusingChatGPT-basedmethods.
ARAIDA 60.74Â±2.33 55.23Â±1.46 95.15Â±9.32 49.62Â±5.98
-w/oKNN 58.24Â±2.79 53.11Â±1.38 94.06Â±9.02 46.84Â±6.30
WeomittheChatGPTwithALresultsbecauseweare -w/of(Â·) 45.23Â±2.58 39.82Â±3.41 89.13Â±0.55 42.93Â±1.71
unabletoestimateitspredictionuncertainty. ARAIDA -w/const. 58.24Â±2.79 53.11Â±1.38 94.06Â±9.02 46.84Â±6.30
canfurtherimproveChatGPTâ€™sperformance
ChatGPTzero
ARAIDA 50.42Â±1.37 43.11Â±1.83 93.52Â±1.24 48.41Â±1.06
-w/oKNN 47.62Â±2.89 38.85Â±2.23 92.44Â±0.95 41.30Â±1.72
-w/of(Â·) 48.73Â±2.64 41.30Â±5.04 90.61Â±0.26 45.84Â±1.12
-w/const. 48.73Â±2.64 41.30Â±5.04 92.44Â±0.95 45.84Â±1.12
13.06% performance gain in MCA on Dist./FT,
ChatGPTfew
3.85% on LLaMa2 sft, 16.80% on Dist./FT with ARAIDA 52.53Â±1.67 52.26Â±3.44 94.92Â±1.02 55.12Â±1.36
AL, 2.61% on LLaMa2 with AL, 30.48% on
-w/oKNN 51.30Â±1.72 51.60Â±3.15 94.78Â±0.99 53.85Â±2.03
sft -w/of(Â·) 48.73Â±2.64 41.30Â±5.04 90.61Â±0.26 45.84Â±1.12
LLaMa2, 8.81% on ChatGPT zero, and 1.55% on -w/const. 51.30Â±1.72 51.60Â±3.15 94.78Â±0.99 53.85Â±2.03
ChatGPT . Onaverage, itachievesan11.02%
few
Table3:AblationstudyontheKNNandtheerror-aware
performance gain in model annotation accuracy,
integrationstrategymodules.WereporttheMCAscores
translating into a reduction in human correction
usingvariousmethods,averagingresultswithdifferent
effortinpractice.
amountsofdata. Error-awareintegrationstrategyeffec-
Active learning does not always help. When tivelycoordinatesthetwoannotators.
comparing the annotation models with and with-
outactivelearning,weobservethatactivelearning
4.3 AblationStudy
doesnotalwaysimprovemodelperformanceand
canevenharmtheperformanceinsomeinstances Thissectionaimstoperformacomprehensiveex-
(e.g.,Dist./FT).Wehypothesizethatthemorechal- amination to investigate the behavior and impact
lenging cases selected by active learning might oftheKNNandoutintegrationstrategy. Wecon-
require more training data for the models to cor- ductanablationstudytoanalyzetheeffectiveness
rect their predictions (Dasgupta, 2005; Rietz and ofeachcomponent. Inparticular,weconsiderthe
Maedche,2021),whichareunavailableinlimited followingbaselines:
dataannotationscenarios. Insteadofrelyingonan â€¢ ARAIDAw/oKNN:Usingtheannotationmodel
annotationmodelalone,ARAIDAactsasaposthoc alonetosuggestlabels. EquivalenttoÎ» (Â·) = 1.
t
â€œplug-inâ€thatfixestheannotationmodelâ€™smistakes
using retrieved annotation references and yields â€¢ ARAIDAw/of(Â·): UsingKNNalonetosuggest
arobustimprovementundervarioussettingswith labels. EquivalenttoÎ» t(Â·) = 0.
andwithoutactivelearning.
â€¢ ARAIDA w/ const.: Using a constant Î»âˆ— value
t
LLMsarestrongannotationmodels. ARAIDA
forallexamples,incontrasttoARAIDA,which
canimprovethemfurther. LLMsperformbetter
varies Î» for different examples. In our experi-
t
than classic distributional models, especially for
ments,wereporttheresultwiththebestÎ»âˆ—tuned
t
sentence-leveltasks. Furthermore,LLaMa2with
onthevalidationset.
fine-tuning consistently outperforms the vanilla
LLaMa2 model, and few-shot ChatGPT consis- The ablation test results are presented in Table 3.
tently beats its zero-shot counterpart. Interest- Duetolimitedspace,weomittheresultforvanilla
ingly, comparing these two pairs of annotation LLaMa2,whichismuchweakerthanotherLLM-
models, we observe that ARAIDA brings a more basedbaselines. Thedetailedobservationsarepro-
substantialimprovementtoweakermodels. When videdbelow.
the annotation models are already strong (in the KNNisastrongstand-aloneannotator. Table
caseofLLaMa2
sft
andChatGPT few), ARAIDAis 3 reveals that although KNN (ARAIDA w/o f(Â·))
26.74 24.05
3.15
35.25
58.83
11.34
6.15
62.25
44.29 25.39 87.49 29.49
3.14
14.84
58.35 21.5570% 70% 100% 50%
60%
60% 40%
50% 80%
50% 40% 30%
60%
40% 30% 20%
20% 40%
30% ð‘€ð‘€ð‘€ð‘€ð´ð´ð‘“ð‘“ð‘œð‘œð‘œð‘œðœ†ðœ†>0.5 10% 10%
20%
ð‘€ð‘€ð‘€ð‘€ð´ð´ð‘“ð‘“ð‘œð‘œð‘œð‘œðœ†ðœ†â‰¤0.5
0% 20% 0%
1 51K9 1 32K1ð‘€ð‘€7 ð‘€ð‘€ 2ð´ð´ 13ð¾ð¾ð¾ð¾ 2Kð¾ð¾ 5 ð‘œð‘œ 2 ð‘œð‘œ 94ðœ†ðœ†3K3â‰¤ 3 0 75.5K41 1 51K9 1 32K17 2 13K25 2 94K33 3 57K 1 51K9 1 32K17 2 13K2 5 2 49K3 3 357K 1 51K9 1 32K17 2 31K2 5 2 49K3 3 357K
WN18RR FreeBase IMDB SST-5
Figure4: AnalyzingourintegrationstrategywiththeDist./FTmodel. ThesolidlinesshowtheMACscoresofthe
annotationmodelf(Â·),separatedbyexampleswithÎ»>0.5(higherweightsassignedtotheannotationmodelf(Â·))
andÎ»â‰¤0.5(higherweightsassignedtoKNN).ThedottedlineshowsKNNâ€™sperformanceonthelatterset.
doesnotmatchtheperformanceof ARAIDA,itob- and MCA
f
on Î» â‰¤ 0.5), showing that our er-
tainscomparableaccuracyasusingtheannotation ror estimation model effectively identifies cases
model alone (ARAIDA w/o KNN). This result is wheref(Â·)islikelytoerrorandassignsitalower
surprising, given KNNâ€™s simplicity compared to weight. Secondly, KNN reaches a reasonable ac-
theannotationmodels(includingLLMs). Suchthe curacy much faster than the annotation model at
results also highlight that significance and effec- the initial stage of the interactive annotation pro-
tivenessofanalogicalreasoning,allowinghumans cess. Even as the number of annotated examples
toreasonmoreeffectively(Mitchell,2021). increases, its accuracy is still higher than the an-
Error-aware integration strategy effectively notation model f(Â·) when Î» â‰¤ 0.5. This result
coordinates the two annotators. Table 3 shows revealsthatKNNcompensatesfortheperformance
that ARAIDAâ€™s error-aware integration strategy deficienciesoff(Â·)ondatawhereitismorelikely
achieves consistent performance gain compared tomakeamistake. Therefore,combiningKNNus-
to using a constant weight Î»âˆ—. This in turn also ingtheerror-awareintegrationstrategyinARAIDA
t
confirmstheoriginalintentionbehindourconstruc- leadstoanoverallimprovementinannotationqual-
tion of the error-aware integration strategy: it is ity,hencereducinghumancorrectioneffort.
infeasible to find the optimal weight through a
5 Conclusion
one-off hyperparameter tuning. Here, it is worth
notingthatChatGPToutputsdiscretelabelsrather Ininteractivedataannotation,anannotationmodel
than probabilistic vectors. In this case, the con- suggestslabelstohumanannotatorstoverify. How-
stant strategy reduces into an indicator function: ever,theannotationmodelispronetoerrorswhen
if Î»âˆ— t > 0.5, F t(x) = f t(x),âˆ€x t; otherwise, trainedonlimitedlabeleddata. Totacklethischal-
F t(x) = g t(x),âˆ€x t. Inthiscase,ARAIDAw/const. lenge,weproposedARAIDA,anapproachinspired
prefers ChatGPT or KNN based on their perfor- byanalogicalreasoning,tocompensatefortheper-
mance. However,itcannotimprovetheannotation formancedeficienciesoftheannotationmodeland
qualitybeyondtheindividualcomponents(i.e.,the correct its mistakes using an error-aware integra-
KNNandtheannotationmodel)duetothediscrete tionstrategy. Extensiveexperimentsdemonstrated
outputofChatGPT. that ARAIDA isflexibletocombinewithdifferent
annotationmodelsacrossvarioustasksandyields
4.4 QualitativeAnalysis
consistentimprovementinlabelsuggestionaccu-
To shed light on how the error-aware integration racy,whichleadstoareductionofhumancorrec-
strategy works, we measure the cumulative accu- tioneffort. Inthisstudy,ourmethodexploresanew
racyoftheannotatormodelandKNNthroughout solutiontobringmoreflexibilitybyallowingthe
theannotationprocessforeachdatasetandpresent human to design any preferred annotation model
theresultwiththeDist./FTannotationmodelinFig- according to different annotation tasks. We are
ure 4. We also separate the cases where Î» > 0.5 devotedtooptimizinghuman-machineutilitiesby
(higherweightsassignedtotheannotationmodel emphasizingthelearningoftask-specifiedconcepts
f(Â·)) and Î» â‰¤ 0.5 (higher weights assigned to efficiently from a few human demonstrations. In
KNN).Ourobservationsareasfollows. future work, we plan to extend ARAIDA to other
Firstly,weobservethatthereisasubstantialgap annotationtasksanddevelopitasageneraltoolkit
between the two solid lines (MCA on Î» > 0.5 thatcanbenefittheNLPcommunity.
f6 Acknowledgements inFigure5showsthat ARAIDA stilloutperforms
thebaseline. Themodifieddatastoremaintenance
This work was supported in part by the Na-
strategy(ARAIDA-dis)furtherimprovestheperfor-
tional Natural Science Foundation of China (No.
mance by a slight margin. Further research and
62272330);inpartbytheFundamentalResearch
morerigorousexperimentsarerequiredtoaddress
FundsfortheCentralUniversities(No. YJ202219).
thehumanannotationnoiseproblemininteractive
annotation.
7 Limitations
Human Stu6d0%ies. This work aims to3r9e%duce hu- 40%
mancorrecti5o0n%effortininteractivedata 3a 4n %notation. 35%
Wefollowp4re0v%iouswork(Hwa,2000;Kristjansson
30% 29% 30%
etal.,2004)tousethenumberofmodelsuggestion
20% (AARLI)A A +R DIiAst.w/ AL Dist./FT
errorstoapproximateth(DAeisLth.) wBu/a AmsLelainne corre2c4ti%oneffort
25% ARAIDA
10% (APRLI)A A +R DIAist.w/ PL
needed. However,theac(DPtuiLst)a. wBl/a ePseLflfinoertneededdepends ARAIDA-dis
0% 19% 20%
on the particular example and the type of errors
1K 2K 3K 4K 5K 1K 2K 3K 4K 5K 1 61K1 1 126K2 13K2 6 3 41K3 6 5K
(e.g.,whetheritisobvious). Ideally,wewouldin-
85%
volvehumanannotatorsandmeasureth50 e% savingof Figure5: MACscoresofvariousmethodswithsynthe-
80% 28%
annotationt7i5m%e. However,duetothela4r0g%enumber sizedlabelnoiseontheSST-5dataset. Dist./FTisused
astheannotati2o6n%model. ARAIDA-disreferstoARAIDA
ofexperime7n0t%alsettings,conductinghu3m0%anstudies
withamodifie2d4%datastoremaintenancestrategy.
witheacha6n5n%otationmodelandablationbaseline
20%
wasinfeasib60le%. (AALR) IAAR I+A FastTextw/ AL 22% (AL) Baseline
55% ( (F AA PLaL R)s) AItBT ARaes I e A+xlit n Few a/ s tA TL extw/ PL 10% Latency. T 2h 0e %KNNcom(pAoL)n AeRnItArequiresretriev-
FastText w/ PL
Error-Pron50e%Human A(PL n) B nasel oine tation. T0%his paper ingsimilarexamplesasthe(AiLn)p AuRtIAd-adtisa,whichmay
18%
treatshumanannota1 tK ion2 sK a3 sK gro4 uK nd5K truthfollow1iKng2K 3lKimi4tKou5rKmethodâ€™s 1ti 61mK1e
1
e21Kf 6fi
2
3c 1Ki 2e 6n 43cK1y
3
w56Khenthedatas-
previousstudiesininteractivedataannotation(Klie toresizeislarge. Toaddressthisproblem,besides
etal.,2018;Leetal.,2021). However,uncertainty the proposed datastore management strategy, we
andinconsistencyofhumanannotationsdooccur. can also employ an efficient similarity search li-
Wereferreaderstotheliteratureonhandlingerror- brarysuchasFAISS12 tospeeduptheretrieval.
prone human annotation, such as crowd-sourced
dataannotation(Larsonetal.,2020).
References
Although human annotation errors are not the
focus of this work, we explore ARAIDAâ€™s perfor-
Akari Asai, Sewon Min, Zexuan Zhong, and Danqi
mance under synthesized label noise conditions. Chen.2023. Retrieval-basedlanguagemodelsand
We consider the crowd-sourced data annotation applications. InProceedingsofthe61stAnnualMeet-
ingoftheAssociationforComputationalLinguistics
scenarioandassumethateachhumanannotatorh
i (Volume6:TutorialAbstracts),pages41â€“46,Toronto,
makes mistakes with the latent probability pi âˆ¼
e Canada.AssociationforComputationalLinguistics.
(0,0.3). We set the total number of annotators
O = 10andsampletheircorrespondingerrorprob- Miguel A Bautista, Artsiom Sanakoyeu, Ekaterina
abilities P = {p1,p2,...,pO}. Then, we sample Tikhoncheva,andBjornOmmer.2016. Cliquecnn:
e e e e
Deepunsupervisedexemplarlearning. InAdvances
u from a uniform distribution U(0,1) for each
i in Neural Information Processing Systems, vol-
annotation. If u â‰¤ pi, h assigns a randomly
i e i ume29.CurranAssociates,Inc.
sampled incorrect label; otherwise, it assigns the
correctone. Weusemajorityvotingofthe10an- Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
notators to obtain the final annotations following Sturge,andJamieTaylor.2008. Freebase: acollabo-
rativelycreatedgraphdatabaseforstructuringhuman
Shiranietal.(2019).
knowledge. InProceedingsofthe2008ACMSIG-
Weslightlymodified ARAIDAâ€™sdatastoremain- MOD international conference on Management of
tenance strategy. When the datastore exceeds its data,pages1247â€“1250.
budge,wediscardthedatafromthemajorityclass
Arantxa Casanova, Pedro O Pinheiro, Negar Ros-
andmostdis-similartoitsclassprototypeinstead
tamzadeh,andChristopherJPal.2020. Reinforced
ofremovingthemostsimilarone(asintheoriginal
activelearningforimagesegmentation. arxiv.
ARAIDAstrategy). Thisstrategymayhelpremove
incorrectly labeled data. The experimental result 12https://github.com/facebookresearch/faissAditi Chaudhary, Antonios Anastasopoulos, Zaid EricJang,ShixiangGu,andBenPoole.2016. Categori-
Sheikh,andGrahamNeubig.2021. Reducingcon- calreparameterizationwithgumbel-softmax. arxiv.
fusioninactivelearningforpart-of-speechtagging.
TACL,9:1â€“16. Qingnan Jiang, Mingxuan Wang, Jun Cao, Shanbo
Cheng,ShujianHuang,andLeiLi.2021. Learning
XuChen,ChangyingDu,XiuqiangHe,andJunWang. kernel-smoothedmachinetranslationwithretrieved
2020. Jit2r: A joint framework for item tagging examples. In Proceedings of the 2021 Conference
andtag-basedrecommendation. InProceedingsof onEmpiricalMethodsinNaturalLanguageProcess-
the 43rd international ACM SIGIR conference on ing,pages7280â€“7290,OnlineandPuntaCana,Do-
researchanddevelopmentininformationretrieval, minican Republic. Association for Computational
pages1681â€“1684. Linguistics.
Sanjoy Dasgupta. 2005. Coarse sample complexity
ArmandJoulin,EdouardGrave,PiotrBojanowski,and
bounds for active learning. In Proceedings of the
TomasMikolov.2017. Bagoftricksforefficienttext
18thInternationalConferenceonNeuralInformation
classification. InEACL,pages427â€“431.Association
ProcessingSystems,pages235â€“242.
forComputationalLinguistics.
Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer.
NoraKassnerandHinrichSchÃ¼tze.2020. BERT-kNN:
2005. Theforgetron: Akernel-basedperceptronona
AddingakNNsearchcomponenttopretrainedlan-
fixedbudget. NeurIPS,18.
guagemodelsforbetterQA. InFindingsoftheAsso-
ciationforComputationalLinguistics:EMNLP2020,
MichaelDesmond,EvelynDuesterwald,KristinaBrimi-
pages3424â€“3430,Online.AssociationforComputa-
join,MichelleBrachman,andQianPan.2021. Semi-
automated data labeling. In Proceedings of the tionalLinguistics.
NeurIPS2020CompetitionandDemonstrationTrack,
volume133ofProceedingsofMachineLearningRe- UrvashiKhandelwal,AngelaFan,DanJurafsky,Luke
search,pages156â€“169.PMLR. Zettlemoyer,andMikeLewis.2021. Nearestneigh-
bormachinetranslation. InternationalConference
TimDettmers, PasqualeMinervini, PontusStenetorp, onLearningRepresentations.
andSebastianRiedel.2018. Convolutional2dknowl-
edgegraphembeddings. InProceedingsoftheAAAI UrvashiKhandelwal,OmerLevy,DanJurafsky,Luke
conferenceonartificialintelligence,volume32. Zettlemoyer,andMikeLewis.2019. Generalization
throughmemorization: Nearestneighborlanguage
Zi-Yi Dou, Keyi Yu, and Antonios Anastasopoulos. models. InICLR.
2019. Investigating meta-learning algorithms for
low-resourcenaturallanguageunderstandingtasks. Jan-Christoph Klie, Michael Bugert, Beto Boullosa,
InEMNLP,pages1192â€“1197. Richard Eckart de Castilho, and Iryna Gurevych.
2018. TheINCEpTIONplatform: Machine-assisted
Fabrizio Gilardi, Meysam Alizadeh, and MaÃ«l Kubli. and knowledge-oriented interactive annotation. In
2023. Chatgptoutperformscrowd-workersfortext- COLING:SystemDemonstrations,pages5â€“9,Santa
annotationtasks. arXivpreprintarXiv:2303.15056. Fe,NewMexico.ACL.
XingweiHe,ZhenghaoLin,YeyunGong,HangZhang,
Jan-Christoph Klie, Richard Eckart de Castilho, and
Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan,
IrynaGurevych.2020. FromZerotoHero: Human-
WeizhuChen,etal.2023. Annollm: Makinglarge
In-The-Loop Entity Linking in Low Resource Do-
languagemodelstobebettercrowdsourcedannota-
mains. InACL,pages6982â€“6993,Online.ACL.
tors. arXivpreprintarXiv:2303.16854.
Thomas Kober, Julie Weeds, Lorenzo Bertolini, and
Michael A Hedderich, Lukas Lange, and Dietrich
DavidJ.Weir.2021. Dataaugmentationforhyper-
Klakow. 2021. Anea: distant supervision for low-
nymydetection. InEACL,pages1034â€“1048.
resourcenamedentityrecognition. thePracticalMa-
chineLearningForDevelopingCountriesWorkshop
TraustiKristjansson,AronCulotta,PaulViola,andAn-
atICLR.
drewMcCallum.2004. Interactiveinformationex-
ChenHuang,PeixinQin,WenqiangLei,andJiancheng tractionwithconstrainedconditionalrandomfields.
Lv. 2023. Reduce human labor on evaluating con- InAAAI,volume4,pages412â€“418.
versationalinformationretrievalsystem: Ahuman-
machinecollaborationapproach. InProceedingsof BrendenMLake,RuslanSalakhutdinov,andJoshuaB
the2023ConferenceonEmpiricalMethodsinNatu- Tenenbaum. 2015. Human-level concept learning
ralLanguageProcessing,pages10876â€“10891,Sin- through probabilistic program induction. Science,
gapore.AssociationforComputationalLinguistics. 350(6266):1332â€“1338.
Rebecca Hwa. 2000. Sample selection for statistical Brenden M Lake, Tomer D Ullman, Joshua B Tenen-
grammarinduction. In2000JointSIGDATConfer- baum,andSamuelJGershman.2017. Buildingma-
ence on Empirical Methods in Natural Language chinesthatlearnandthinklikepeople. Behavioral
ProcessingandVeryLargeCorpora,pages45â€“52. andbrainsciences,40.Stefan Larson, Adrian Cheung, Anish Mahendran, LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
Kevin Leach, and Jonathan K Kummerfeld. 2020. CarrollWainwright,PamelaMishkin,ChongZhang,
Inconsistenciesincrowdsourcedslot-fillingannota- SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
tions: A typology and identification methods. In 2022. Training languagemodelsto followinstruc-
Proceedingsofthe28thInternationalConferenceon tions with human feedback. Advances in Neural
ComputationalLinguistics,pages5035â€“5046. InformationProcessingSystems,35:27730â€“27744.
FlorianLaws,ChristianScheible,andHinrichSchÃ¼tze. JeffreyPennington,RichardSocher,andChristopherD.
2011. Active learning with Amazon Mechanical Manning. 2014. Glove: Global vectors for word
Turk. In EMNLP, pages 1546â€“1556, Edinburgh, representation. InEMNLP,pages1532â€“1543.
Scotland,UK.ACL.
TimRietzandAlexanderMaedche.2021. Cody: Anai-
Trung-Nghia Le, Tam V Nguyen, Quoc-Cuong Tran, basedsystemtosemi-automatecodingforqualitative
Lam Nguyen, Trung-Hieu Hoang, Minh-Quan Le, research. InCHI,pages1â€“14.
andMinh-TrietTran.2021. Interactivevideoobject
maskannotation. InAAAI,volume35,pages16067â€“
Eric Ringger, Peter McClanahan, Robbie Haertel,
16070.
GeorgeBusby,MarcCarmen,JamesCarroll,Kevin
Seppi,andDeryleLonsdale.2007. Activelearning
YoonjooLee,JohnJoonYoungChung,TaeSooKim,
forpart-of-speechtagging: Acceleratingcorpusan-
Jean Y Song, and Juho Kim. 2022. Promptiverse:
notation. InProceedingsoftheLinguisticAnnotation
Scalablegenerationofscaffoldingpromptsthrough
Workshop,pages101â€“108.
human-ai hybrid knowledge graph annotation. In
CHI Conference on Human Factors in Computing
StephenRoller,KatrinErk,andGemmaBoleda.2014.
Systems,pages1â€“18.
Inclusiveyetselective: Superviseddistributionalhy-
pernymydetection. InCOLING:TechnicalPapers,
Yanzeng Li, Bowen Yu, Li Quangang, and Tingwen
pages1025â€“1036.
Liu. 2021. Fitannotator: A flexible and intelligent
textannotationsystem. InProceedingsofthe2021
Amirreza Shirani, Franck Dernoncourt, Paul Asente,
Conference of the North American Chapter of the
NedimLipka,SeokhwanKim,JoseEchevarria,and
AssociationforComputationalLinguistics: Human
ThamarSolorio.2019. Learningemphasisselection
LanguageTechnologies: Demonstrations,pages35â€“
forwrittentextinvisualmediafromcrowd-sourced
41.
label distributions. In Proceedings of the 57th An-
nualMeetingoftheAssociationforComputational
ShudongLiu,XueboLiu,DerekFWong,ZhaocongLi,
Linguistics,pages1167â€“1172.
WenxiangJiao,LidiaSChao,andMinZhang.2023.
knn-tl: k-nearest-neighbortransferlearningforlow-
Richard Socher, Alex Perelygin, Jean Wu, Jason
resourceneuralmachinetranslation. InProceedings
Chuang,ChristopherDManning,AndrewYNg,and
of the 61st Annual Meeting of the Association for
ChristopherPotts.2013. Recursivedeepmodelsfor
ComputationalLinguistics(Volume1: LongPapers),
semanticcompositionalityoverasentimenttreebank.
pages1878â€“1891.
In Proceedings of the 2013 conference on empiri-
calmethodsinnaturallanguageprocessing,pages
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
1631â€“1642.
DanHuang, AndrewY.Ng, andChristopherPotts.
2011. Learningwordvectorsforsentimentanalysis.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
In Proceedings of the 49th Annual Meeting of the
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
AssociationforComputationalLinguistics: Human
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Language Technologies, pages 142â€“150, Portland,
Bhosale, et al. 2023. Llama 2: Open founda-
Oregon, USA. Association for Computational Lin-
tion and fine-tuned chat models. arXiv preprint
guistics.
arXiv:2307.09288.
SamuelMarcos-PablosandFranciscoJGarcÃ­a-PeÃ±alvo.
2020. Information retrieval methodology for aid- DingWang,ShantanuPrabhat,andNithyaSambasivan.
ing scientific database search. Soft Computing, 2022a. Whoseaidream? insearchoftheaspiration
24(8):5551â€“5560. indataannotation. InCHI,pages1â€“16.
Melanie Mitchell. 2021. Abstraction and analogy- Dongqi Wang, Haoran Wei, Zhirui Zhang, Shujian
makinginartificialintelligence. AnnalsoftheNew Huang,JunXie,WeihuaLuo,andJiajunChen.2021.
YorkAcademyofSciences,1505(1):79â€“101. Non-parametric online learning from human feed-
backforneuralmachinetranslation. arXiv.
Arvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-
ford,JesseMichaelHan,JerryTworek,QimingYuan, ShuheWang,XiaoyaLi,YuxianMeng,TianweiZhang,
NikolasTezak,JongWookKim,ChrisHallacy,etal. RongbinOuyang,JiweiLi,andGuoyinWang.2022b.
2022. Textandcodeembeddingsbycontrastivepre- k nn-ner: Named entity recognition with nearest
training. arXivpreprintarXiv:2201.10005. neighborsearch. arXivpreprintarXiv:2203.17103.YanWang,Wei-LunChao,KilianQWeinberger,and A.3 DatastoreMaintenanceStrategy
Laurens van der Maaten. 2019. Simpleshot: Re-
We propose a class-aware datastore maintenance
visitingnearest-neighborclassificationforfew-shot
learning. arXiv. strategyforARAIDA,whichremoveslabeledexam-
ples from the majority class most similar to their
XingjiaoWu,LuweiXiao,YixuanSun,JunhangZhang,
class prototype. Compared to the conventional
Tianlong Ma, and Liang He. 2022. A survey of
First-In-First-Out (FIFO) strategy (Dekel et al.,
human-in-the-loop for machine learning. Future
GenerationComputerSystems,135:364â€“381. 2005), our method ensures that 1) the datastore
containsasmuchdatafromdifferentclassesaspos-
RuixuanXiao, YiwenDong, JunboZhao, RunzeWu, sible, and 2) the class prototype is least affected
Minmin Lin, Gang Chen, and Haobo Wang. 2023.
aftertheremoval. Wecomparewithtwovariantsof
FreeAL:Towardshuman-freeactivelearninginthe
eraoflargelanguagemodels. InProceedingsofthe ARAIDA,includingARAIDAw/FIFOandARAIDA
2023ConferenceonEmpiricalMethodsinNatural w/classFIFO.Theformerdiscardstheoldestex-
Language Processing, pages 14520â€“14535, Singa- ampleregardlessoftheclass;thelatterdiscardsthe
pore.AssociationforComputationalLinguistics.
oldestexamplebelongingtothemajorityclassin
KaichunYao,ChuanQin,HengshuZhu,ChaoMa,Jing- thedatastore.
shuai Zhang, Yi Du, and Hui Xiong. 2021. An in- As shown in Fig.8, ARAIDA w/ FIFO can suf-
teractiveneuralnetworkapproachtokeyphraseex- fer from a sudden decrease in performance, as it
tractionintalentrecruitment. InProceedingsofthe
mayremoveimportantexamplesarbitrarily. After
30thACMInternationalConferenceonInformation
&KnowledgeManagement,pages2383â€“2393.
integratingtheclassinformation,ARAIDAw/class
FIFOremovestheoldestanalogyfromthemajor-
Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian ityclass,achievingacomparativeperformanceto
Huang,BoxingChen,WeihuaLuo,andJiajunChen.
ARAIDA. However, ARAIDA stillperformsbetter
2021. Adaptivenearestneighbormachinetranslation.
when the number of classes increases (e.g., Free-
pages368â€“374.
Base).
A HyperparameterAnalysis B PromptsandFine-TuningExamplesfor
LLM-BasedAnnotationModels
Inthissection,weprovideadetailedhyperparam-
eter analysis of ARAIDA, including the datastore Table 5 presents prompts used for zero-shot and
sizeA ,thenumberofneighborsk forÏ ,andthe few-shot annotation. Table 6 shows fine-tuning
t t
datastore maintenance strategy. We show results examplesforLLaMa2 sft.
onlyfortheDist./FTannotationmodelforbrevity.
A.1 DatastoreSize
We tune the size of the datastore A in
t
{100,500,1000,2000} and evaluate the machine
cumulative accuracy of ARAIDA with or without
activelearning. WeillustratetheresultsinFig.6. A
largerdatastoresizegenerallybringshigheranno-
tation performance since it allows us to maintain
moredatafrompasthuman-machineinteractions.
However,italsorequireslargermemoryusageand
causes longer latency. We found that a datastore
size of 1000 is a reasonable tradeoff, which we
utilizeinourmainexperiments.
A.2 Topk forÏ
t
We tune the number of neighbors A in
t
{5,10,20,50} and evaluate the machine cumula-
tive accuracy of ARAIDA with or without active
learning. As shown in Fig.7, k = 20 seems to
performwellforalltasks.SST IM ID MDB B with AL SST with AL SST IMD IMB DB w ith AL SST with AL
41% 90%80% 41% 41% 90%80% 41%
75% 75%
36% 80% 36% 36% 80% 36%
70% 70%
31% 70%65% 31% 31% 70%65% 31%
60% 60%
26% 60% 26% 26% 60% 26%
55% 55%
21% 50%50% 21% 21% 50%50% 21%
1 41K71 0 123K16 1 932K22 5 248K313 453K7 1 141K7151 K0 21 93K 1 16 231 K3912K 7 2 322 5 K1 2428 K 53 1 2 4 395K4K33 7 353K7 1 41K71 0123K16 1 932K22 5 248K313 543K7 1 41K7 1 0 123K16 1 932K22 5 248K31 3 453K7 1 141K71 51 K 0 21 9 3K 1 126 3 K1 391 2K7 2 32 2 K51 224 8 5K 3 124 39K543K3 7353K7 1 41K7 1 0 123K16 1 932K22 5 248K31 3 543K7
FreeBase FWreeNBase-AL WN FreeBase FWreeNBase-AL WN
50% 60% 50% 60%
50% 60% 50% 60%
40% 50% 40% 50%
40% 50% 40% 50%
40% 40%
40%30% 40%30%
30% 30%
30% 30%
30% 30%
20% 20%20% A AR RA AI ID DA A- -1 50 00
0
20% A AR RA AI ID DA A- -1 50 00
0
20% 20%20% A AR RI IA A- -1 50 00
0
20% A AR RI IA A- -1 50 00
0
10% 10%10% ARAIDA-1000 10% ARAIDA-1000 10% 10%10% ARIA-1000 10% ARIA-1000
ARAIDA-2000 ARAIDA-2000 ARIA-2000 ARIA-2000
0% 0%0% 0% 0% 0%0% 0%
1 41K71 0 123K16 1 932K22 5 248K313 453K7 1141K7 411 K0 72131K 01 612 31 K3916K 212 932 25 K 224 28K 53 21 48 35K341K33 7543K7 141K7 1 0 123K16 1 932K22 5 2 48K31 3 453K7 1 41K7 1 0 123K16 1 932K22 5 248K31 3 453K7 1141K741 1 K 07 21 1 3K 01 12 63 K1 1396 K2 1 293 2 2K5 2 24 28 5K 3 241 8 3K5341K 33 7543K7 141K7 1 0 123K16 1 932K22 5 2 48K31 3 453K7
SST (a)W IMN DB1 8RR (b)WIMND1B8 wRitRh A+LAL (c S) SITMF wrDite ShB S e ATL B waitshe AL (d)Fre ISMe STDB Bwa iths Ae L+AL
41% 90% 80% 41%41%80% 90%41%
75% 75%
36% 80% 36%36% 80%36%
70% 70%
31% 70% 65% 31%31%65% 70%31%
60% 60%
26% 60% 26%26% 60%26%
55% 55%
21% 50% 50% 21%21%50% 50%21%
1 41K7 1 0 123K16 1 932K22 5 248K31 3 453K7 1 14K7 1 0 213K16 1 392K2 2 5 248K31 354K37 115K 9 123K1 7 32K12 5 249K3 353K7 1 411K7411 1 K 07 1512 1K3 K0 1 9126 3 1K 119326 32KK 12 1932 7 2 5K 2 23242 8K1 5K3 221 4 8 53 K354 2413K 9K37 453 3K 3753K7 1 141K741 1 K 07 21 1 3K 0 1126 3 K1 1396 2K 12 93 2 2K5 2 242 8 5K 3 2148 3K3541K 33 7543K7
FreeBase (e)IWMNDB (f)IMFreDeBaBse-A+LAL (g)WFFSrNereeSeBBaTases-e-5AL (h)SSWTNW-N5+AL
50% 60%50% 60%
50% 60% 50% 60%
40% 50% Figure6: Machi4n0%eCumulativeAccuracyo50f% 4A 0%4R0%AIDAwithdifferentda 5t 0a %5s0t%oresizes.
30% 40% 30% 40% 30%30% 40%40%
30% 30% 30%30%
20% 20% A AR RI IA A- -1 50 00 0 20% 20%20%20% A AR RI IA A- -1 50 00 0 20%20% A AR RA AI IR RA AI I- -A A1 50 0- -1 50 00 00 0
10% 10% ARIA-1000 10% 10%10%10% ARIA-1000 10%10% ARAIRAI-A10-100000
WANR1IA8R-2R000 AFRrIeAe-B20a0s0e ARAIRAI-A20-200000
0% 1 41K7 1 0 123K16 1 932K22 5 248K31 3 453K7 Ra0 w% Cl 1as14sK7 1 0 213K1 6 1 39K22 2 5 248K31 354K3M 7appedC0%lass 1 41K7 1 0 123R K16a 1w 932KC 2 2l 5a 2s 48s K31 3 543K7 0%0%0 1% 411K741 11 K 07 4112 13K K07 1 162 1 3 1 0K 1931262 3KK 121 9326 25 K1 22 93428 2K 5K32 12 24 8 35 K435 24138K 7K3 3451 3K3 7543K7 0%0% 1141K741 1 K 07 21 1 3K 0 112 63 K1 1396 K2 1 29M 3 22 K5 2a 224 p 85K p 32 418 e K33d 541K 33C 4753l Ka 7ss
_hypernym hypernym /location/location/contains contains
_derivationally_related_form derivation /olympics/olympic_sport/athletes./olympics/olympic_athlete_affiliation/country country
WN SST
_member_meronym member /music/performance_role/track_perfoWrmNan cAesL./music/track_contribution/roleSST AtrLack_role
_has_part component /people/person/professio7 7n0 0% % 404%0% profession
_synset_domain_topic_of synset /music/performance_rol6e0/r%egular_performances./music/group_membership/role group_role
60%
_instance_hypernym instanceofhypernym /location/location/adjoin5_0s%./location/adjoining_relationship/ad3j5o3%i5n%s adjoins
50%
_also_see synonym /film/film/release_date_s./film/film_regional_release_date/film_release_region film_release
_verb_group verbgroup /food/food/nutrients./foo44d00/% n%utrition_fact/nutrient 303%0% nutrient
3300%% A AR RI IR R- -5 5 252%5%
Table4: Classmappingdetailsf2 o200% r%WN18RRandA AFR RrI IR eR- e-1 1B0 0ase
1100%%
AARRIIRR--2200 202%0%
AARRIIRR--5500
00%% 151%5%
111515KK 9 9 12 123K 3K11 7 7 3223K11K2 2 5 5 24 249K 9K33 3 35533KK77 1115K15 9K 9 12 3K 1213 K 71 3 27 K 1232 1K 5 2 42 5 K 92439 K 335 33 K573K7
WN S WS NT SWWSTNN AALL SSISMSTTD F A ABB LL AL imdfbb
70% 4700%% 40%7700%% 4460080%%0%% 855%0%
56 00 %% 3 56 5 00 % %% 35% 56 56 00 00 %% %% 33 45 55 77 00 %% 05 %% %% 78 50 4 %% 0%
40% 3400%% 30%4400%% 3300%% 703%0%
3605%%
23 00 %% A AR RI IR R- -5 10 2 235 00% %% AA RR AA II DD AA -- 15 0 25% 23 23 00 00 %% %% A AA AR RA AR RI II ID DR RA A- -- -5 15 10 0 22 255 60%% 0%% 66 052 %%0%
1 00 %% A AR RI IR R- -2 50 0 12 1 50 00 %% %% A AR RA AI ID DA A- -2 50 0 12 50 %% 11 00 00 %% %% A AA AR RA AR RI II ID DR RA A- -- -2 52 50 00 0 12 12 1 50 50 550 %% %% 005% %%% 55 051 %% 00 %%
115K 9 123K1 7 32K12 5 249K3 353K7 111155KK 9 9 112233KK11 77 3322KK1122 55 24429KK933 3 3553K3K77 1151K1 91 515 K K12 9 3K9 1 1 21 72 3 K332K 1K11 7 7 2 23 235 1K 1K422 K29 5 53 2 4 3245 9K93KK 37 3 3 355 3K3K 77 111115151K5K15K 9 K9 9 9 1 2 1 2 31K23 1K2 13K31 K 71 71 23 7 723 1K 321K23 2K1 1K2 52 5 2 254 524 9K 2492K4 39K9 3K 3 335 3 5 3 3K 335K5 7373KK 77 1115K15 9K 9 12 3K1 2 13 K 71 3 27 K 123 2 1K 5 2 42 5 K 924 39 K 335 33 K5 73K7
(a)WN18RR (b)WN18RR+AL (c)FreeBase (d)FreeBase+AL
FB imFdBb iIImMMdDDbBB W AA NLL WSffbSNb T AL SST AL
0.6 850%.6 85%8800%%70% 55 4007 0%%0 %% 40%
0000 .... 2345 66778 05050 0000 %%%%% .... 2345 66778 05050 %%%%% 6677 6677 0505 0505 %%%% %%%% 23456 00000 %%%%%
A AR RI IR R- -5
10
234 234 233 000 000 23456 505 %%% %%% 00000 %%% %%%%%
A AR RI IR R- -5
10
233 505 %%%
0.1 550 %.1 55%5555%% 10% ARIR-20 11 200 10%% 0%% ARIR-20 20%
0 115K 9 123K1 7 32K12 5 249K3 353K7 50%0 1111 55KK 9 9 1122 33KK1 1 7 7 332 2KK1 12 2 5 5 24 429K K93 3 3 35 53K 3K7 7 50% 5500%% 101% 5 1K 1 91 5151 K K12 9 13K 59 K1 1 21 72 93 K332K 1 K 111 27 37 2 K 23 1235 1K 71K 42 2 3K 229 5 K153 2 4 3 224 5 9 K593A K K 37 2 34R 39 35KI 5 3KR 33 K 7 37- 55 3K0 7 1005%%0%% 1111151155KKK159 K9 9 1 9 1 2 1232 3K 13KK2 113 1K 7 71 72 3 2733 21K 1KK 23 12 21K 2 5 525 2 4 2544 2A 9K 92K9K4R 393 K 33 I 3 5 3R 35 5 3K 33- 3K5K 75 737K0 7 15% 1 15K9 123K1 7 231K2 5 249K3 3 53K7
(e)IMDB (f)IMDB F+ BAL (gI)MiS mDS dBbT A-5L (h)SSTf-b5+AL
0.6 8850%% 50%
Figure7: Mach 0i .n 5eCumulativeAccuracy 870o 5%%fARAIDAwithdifferentk 40f %orÏ t.
0.4
7750%%
30%
70%
0.3 65%
65% 20%
0.2 60%
60%
0.1 55 55 %% 10%
0 5500%% 0%
115K 9 123K1 7 32K12 5 249K3 353K7 1115K15K 9 9 12 13K231K 1 7 73 2 K23 11K 2 25 542 2K949K 3 335 33K537K7 1 15K9 1 23K1 7 231K2 5 249K3 3 53K7SST SST F-A BL IMDB SST-AL IMDB
45% 45 6% 0% 0.8 45% 0.8
40% 405%0% 0.75 40% 0.75
35% 354%0% 0.7 35% 0.7
30% 0.65 0.65
30% 30 2% 0% 0.6 30% 0.6
25% 25 1% 0% 0.55 25% 0.55
20% 20%0% 0.5 20% 0.5
1 15K 9 123K1 7 32K12 5 249K3 353K7 1115K15K 9 9 12 31K231K 71 3 72 K1231K2 25 542 K 2949K3 335 33K537K7 1 15K9 123K1 7 32K12 5 249K3 353K7 115K 9 123K1 7 32K12 5 42K93 353K7 1 15K9 123K1 7 32K12 5 249K3 353K7
IMDB FWSBSNT SST WN FB FB FB WN
85% 5074%05%% 45% 60% 60% 60% 50% 60%
80% 406 4%0 0% % 40% 50% 50% 50% 40% 50%
75% 50% 40% 40% 40% 40%
70% 3043%05%% 35% 30%
30% 30% 30% 30%
65% 2033%00%% 30% 20%
60% 20% ARAIDA 20% 2 A0 R% AIDA 20% 20% ARIA
55% 10 12% 05 %% ARAIDA w/ FIFO 25% 10% 1A0R%AIDA w/ FIFO 10% 10% 10% ARIA w/ FIFO
ARAIDA w/class FIFO ARAIDA w/class FIFO ARIA w/class FIFO
50% 02%00%% 20% 0% 0% 0% 0% 0%
1 15K 9 123K1 7 32K12 5 249K3 353K7 11115K1515K 9K 9 9 12 31K 2 1231K 3 K 71 1 372 7 K123 321K2 K1 25 2 542 5 2K94 249K3 9K 335 3 3 3K5 3375K 37K7 1 15K 9 123K11 7 3215K1K2 9 5 214293KK31 37 15 332K1K715K2 59 2 4 192K3K3 1 35 7 3K 2371K2 5 249K3 353K71 15K9 123K1 7 231K2 5 249K3 353K7 115K 9 123K1 7 32K12 5 42K93 353K7 115K 9 123K1 7 32K12 5 249K3 353K7
(a) SW SIMTN -DA1 LB8RR (IbM)DWBN1 IM8 DR BR+ WAL N (c)Fr SWe SeN TBase (d)FreSeSBTa-AseL+ FA BL IMDB
4855%% 85% 80% 70% 7 40 5% % 45% 60% 0.8
4 78 0 50 % %% 78 50 %% 75% 56 00 %% 56 4 00 0 %% % 40% 50% 0.75
3750%% 70% 70% 40% 4305%% 35% 40% 0.7
3605%% 65% 65% 30% 3 30 0% % 30% 30% 0.65
60% 60% 60% 20% ARIA 20% ARIA 20% 0.6
2 55 5% % 55% 55% 10% A AR RI IA A w w/ / cF laIF ssO12 F05 I%% FO A AR RI IA A w w/ / cF laIF ssO FIFO 25% 10% 0.55
2500%% 50% 50% 0% 200%% 20% 0% 0.5
11151K5K 9 9 12 13K231K 1 7 37 2 K321K12 52 542 K92493K 335 33K573K7 1 15K 9 123K1 17 3215K1K2 9 5 2 4 129K 3K31 317 5 332K17K15K2 59 24 129K3K3 1 3 57 3K 2371K2 5 249K3 353K7111155KK9 9 1122 33KK 11 77 3223K11K 22 55 2424 9K9K 33 3 355 33KK 77 115K 9 123K11 7 1352KK19 2 5 1 2423KK91 37 35233K1K72 5 249K3 353K7 1 15K9 123K1 7 32K12 5 249K3 353K7
(e)I FM BDB (f)IMD WB N+AL (g)IMSSDTB-5 (h)SSTF-B5+AWLN
WN
50% 60% 85% 50% 70% 60%
40%
Figure8: MachineCumu 5l 0a %tiveAccuracyofARAIDA80w%ithdifferentdatastorema 4i 0n %tenan6c0e%strategies.
50%
75% 50%
40% 40%
30% 70% 30% 40%
30% 65% 30% 30%
20% 20%
20% ARIA 60% 20% ARIA 20% ARIA
10% 10% A AR RI IA A w w/ / cF laIF ssO FIFO 55% 10% 10% A AR RI IA A w w/ / cF laIF ssO FIFO10% A AR RI IA A w w/ / cF laIF ssO FIFO
0% 115K 9 123K1 7 32K12 5 42K93 353K7 0% 115K 9 123K1 7 32K12 5 249K3 353K7 50% 1 15K 9 123K1 7 32K12 5 249K3 353K7 0% 115K0 9 % 123K1 1 7 1 35 2K K19 2 5 1 2 423K K91 37 3523 3K1K 72 5 249K3 353K70% 115K 9 123K1 7 32K12 5 249K3 353K7Dataset Prompts
==================
input:Ifyoulikeoriginalgutwrenchinglaughter...it.GreatCamp!!!.
output:positive
input:IsawthismoviewhenIwasabout12whenit...Therearenorules.
IMDB
output:negative
==================
Youneedtoidentifythesentimentsofthefollowingsentences,outputpositiveornegative.
{INPUTS}
=====================
input:Thegorgeouslyelaboratecontinuation...Tolkienâ€™sMiddle-earth.
output:StrongPositive
input:Singer/composerBryanAdamscontributesaslewof...,spiritofthepiece.
output:Positive
input:Youâ€™dthinkbynow...withheartsofgold.
output:Neutral
SST-5 input:Thisisnâ€™tanewidea.
output:Negative
input:Asourlittlemovieat...Whatwasitallfor?
output:StrongNegative
====================
Identifythesentimentofeachparagraph,
youhavefiveoptions:â€™StrongPositiveâ€™,â€™Positiveâ€™,â€™Neutralâ€™,â€™Negativeâ€™orâ€™StrongNegativeâ€™
{INPUTS}
=============
input:abilityunfitness
output:antonym
input:dissentdebating
output:entailment
...
input:abandonmentapostasy
output:hypernym
input:abandonmentdiscard
WN18RR output:hyponym
input:AfghanistanAfghan
output:member
input:abandonmentabandonment
output:synonym
=============
Identifythesemanticrelationoftheeachwordpair,
youhaveeightoptions:â€™componentâ€™,â€™synsetâ€™,...,â€™hypernymâ€™,â€™derivationâ€™,â€™memberâ€™,â€™synonymâ€™
YouMUSTonlyoutputthesemanticrelationwordforeachinput!
{INPUTS}
==========
input:LibyaEgypt
output:adjoins
input:HonoluluPunahou
output:contains
input:BobsleighNetherlands
output:country
input:BlackbriarLithuania
output:film_release
input:AutoharpGuitar
output:group_role
FreeBase
input:IceCreamWater
output:nutrient
input:ShriyaActor
output:profession
input:CelloPennywhistle
output:track_role
=========
Identifythesemanticrelationoftheeachwordpair,
youhaveeightoptions:â€™containsâ€™,â€™countryâ€™,â€™track_roleâ€™,â€™professionâ€™,â€™group_roleâ€™,â€™adjoinsâ€™,â€™film_releaseâ€™,â€™nutrientâ€™.
onlyoutputthesemanticrelation.
{INPUTS}
Table5: Promptsfordifferentdatasetstoobtaintheannotation. Weremovethefew-shotdemonstrationsinthe
promptsinthezero-shotscenarios.Dataset FinetuningdataexampleforLLaMa2sft
{"instruction":"Identifythesentimentofthefollowingparagraph,outputâ€™positiveâ€™orâ€™negativeâ€™.",
"input":"ThecastplayedShakespeare.Shakespearelost.IappreciatethatthisistryingtobringShakespearetothemasses,
butwhyruinsomethingsogood.Isitbecauseâ€™TheScottishPlayâ€™ismyfavoriteShakespeare?Idonotknow.
WhatIdoknowisthatacertainRevBowdler(hencebowdlerization)triedtodosomethingsimilarinthe
IMDB
Victorianera.Inotherwords,youcannotimproveperfection.IhavenomoretowritebutasIhaveto
writeatleasttenlinesoftext(andEnglishcompositionwasnevermyforteIwilljusthavetokeep
goingandsaythatthismovie,asthesayinggoes,justdoesnotcutit.",
"output":"negative"},
{"instruction":"Identifythesentimentofthefollowingparagraph,
youhavefiveoptions:â€™StrongPositiveâ€™,â€™Positiveâ€™,â€™Neutralâ€™,â€™Negativeâ€™orâ€™StrongNegativeâ€™.",
"input":"ThegorgeouslyelaboratecontinuationofTheLordoftheRingstrilogyissohugethata
SST-5
columnofwordscannotadequatelydescribeco-writer/directorPeterJacksonâ€™sexpandedvisionofJ.R.R.
Tolkienâ€™sMiddle-earth.",
"output":"StrongPositive"},
{"instruction":"Identifythesemanticrelationofthefollowingwordpair,
youhaveeightoptions:â€™antonymâ€™,...,â€™synonymâ€™.",
WN18RR
"input":"a.m.A.M.",
"output":"synonym"},
{"instruction":"Identifythesemanticrelationofthefollowingwordpair,
youhaveeightoptions:â€™containsâ€™,...,â€™nutrientâ€™.",
FreeBase
"input":"AutoharpGuitar",
"output":"group_role"},
Table6: FinetuningdataexamplesforLLaMa2
sft