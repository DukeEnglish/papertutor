Configurable Mirror Descent: Towards a Unification of Decision Making
PengdengLi1 ShuxinLi1* ChangYang2* XinrunWang1 ShuyueHu3 XiaoHuang2 HauChan4 BoAn15
Abstract 1.Introduction
Decision-makingproblems,categorizedassingle- Decision-makingproblemsarepervasiveintherealworld
agent, e.g., Atari, cooperative multi-agent, e.g., (Sutton&Barto,2018;Shoham&Leyton-Brown,2008),
Hanabi, competitivemulti-agent, e.g., Hold‚Äôem whichcanbegenerallycategorizedintosingle-agent,e.g.,
poker, and mixed cooperative and competitive, Atari (Mnih et al., 2015), cooperative multi-agent, e.g.,
e.g., football, are ubiquitous in the real world. Hanabigame(Bardetal.,2020),competitivemulti-agent,
Variousmethodsareproposedtoaddressthespe- e.g., Hold‚Äôem poker (Brown & Sandholm, 2018; 2019),
cificdecision-makingproblems. Despitethesuc- andmixedcooperativeandcompetitive(MCC),e.g.,foot-
cessesinspecificcategories,thesemethodstyp- ball(Kurachetal.,2020;Liuetal.,2022a). Tosolvethese
icallyevolveindependentlyandcannotgeneral- problems,variousmethodsareproposedwherenotableex-
ize to other categories. Therefore, a fundamen- amplesincludePPO(Schulmanetal.,2017)forsingle-agent
talquestionfordecision-makingis: Canwede- category,QMIX(Rashidetal.,2018)forcooperativemulti-
velopasinglealgorithmtotackleALLcategories agentcategoryandPSRO(Lanctotetal.,2017)forcompeti-
ofdecision-makingproblems? Thereareseveral tivecategory. Despitethesuccessesinspecificcategories,
mainchallengestoaddressthisquestion: i)dif- thesemethodsaredevelopedalmostindependentlyandcan-
ferentdecision-makingcategoriesinvolvediffer- notgeneralizetoothercategories. Therefore,afundamental
entnumbersofagentsanddifferentrelationships questionfordecisionmakingtoansweris:
betweenagents,ii)differentcategorieshavedif-
CanwedevelopasinglealgorithmtotackleALL
ferentsolutionconceptsandevaluationmeasures,
categoriesofdecision-makingproblems?
andiii)therelacksacomprehensivebenchmark
coveringallthecategories. Thisworkpresentsa Agent Solution
Cooperative
preliminaryattempttoaddressthequestionwith Number Multi-agent Concepts
threemaincontributions. i)Weproposethegener-
Single Decision MCC
alizedmirrordescent(GMD),ageneralizationof
Agent Making Multi-agent
MDvariants,whichconsidersmultiplehistorical
policiesandworkswithabroaderclassofBreg- Competitive
Agent Multi-agent Evaluation
mandivergences. ii)Weproposetheconfigurable Relations Measures
mirror descent (CMD) where a meta-controller
is introduced to dynamically adjust the hyper- Figure1.Overviewofthecategoriesofdecisionmakingandthe
parametersinGMDconditionalontheevaluation fourdesideratafortherequiredmethodtosatisfy.
measures. iii) We construct the GAMEBENCH
with 15 academic-friendly games across differ- Thereareseveralcriticalchallengestoaddressthisfunda-
entdecision-makingcategories. Extensiveexper- mentalquestion. First,thedifferentcategoriesofdecision-
iments demonstrate that CMD achieves empiri- makingproblemsincludedifferentnumbersofagentsand
callycompetitiveorbetteroutcomescomparedto differentrelationshipsbetweenagents. Thereisoneagent
baselineswhileprovidingthecapabilityofexplor- forthesingle-agentcategory,whilemultipleagentsforthe
ingdiversedimensionsofdecisionmaking. other three categories, therefore, the reinforcement learn-
ingmethods,e.g.,PPO,mainlydevelopedforsingle-agent
*Equalcontribution 1NanyangTechnologicalUniversity2The decision-making problems, cannot be directly applied to
Hong Kong Polytechnic University 3Shanghai Artifcial Intelli- multi-agentcategories. Furthermore,evenformulti-agent
genceLaboratory4UniversityofNebraska-Lincoln5SkyworkAI. categories,QMIX(Rashidetal.,2018)isdevelopedtohan-
Correspondenceto:XinrunWang<xinrun.wang@ntu.edu.sg>.
dlethecooperativemulti-agentcategoryandcannotbeap-
Proceedings of the 41st International Conference on Machine pliedtothecompetitivecategory.Second,differentdecision-
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by makingcategorieshavedifferentsolutionconcepts,where
theauthor(s). theoptimal(joint)policyisconsideredinthesingle-agent
1
4202
yaM
02
]IA.sc[
1v64711.5042:viXraConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
andcooperativemulti-agentcategories,whileforthecom- 2.AReal-WorldMotivatingScenario
petitiveandMCCmulti-agentcategories,Nashequilibrium
Weprovideanillustrativeexampletohighlighttheimpor-
(NE) (Nash, 1951) is the canonical solution concept and
tance and real-world implications of a unified algorithm
other solution concepts, e.g., correlated equilibrium (Au-
framework. Considerthataroboticcompanyisdeveloping
mann, 1987) are also considered. Furthermore, even for
and selling generalist domestic robots to users. The user
onesolutionconcept,e.g.,NE,therearedifferentevaluation
mayasktherobottolearntocompletedifferentnoveltasks,
measures,e.g.,NashConvorNashConvwithsocialwelfare
includingsingle-agent,cooperative,competitive,andMCC
andfairness1. Tosummarizethechallenges,weproposethe
categories,byspecifyingtheobjective. Therefore,ifwecan
fourdesideratathatthemethodsshouldsatisfy:
deployaunifiedalgorithmintotherobot,therobotcanlearn
tocompletedifferentnoveltaskswithasinglealgorithm.
‚Ä¢ D1: Applicabletosingle-andmulti-agentcategories
‚Ä¢ D2: Applicabletocoop.,comp.,&MCCcategories Developinganddeployingsuchaunifiedalgorithmwould
benefit both the development and users. For the develop-
‚Ä¢ D3: Applicabletodifferentsolutionconcepts
mentside,asonlyasinglepolicylearningruleisrequired,
‚Ä¢ D4: Applicabletodifferentevaluationmeasures
thedeploymentanduserinterfacedesigncouldbelargely
simplified,whichwouldbemorecost-efficientthandeploy-
An overall illustration of the categories of the decision
ingdifferentspecializedalgorithmssuchasMAPPOand
makingandthedesiderataisdisplayedinFigure1. Third,
PSROastheymaycomplicatethedevelopmentpipelineand
existingbenchmarksaretypicallyspecializedforspecific
userinterfacedesign. Fortheuserside,theuseronlyneeds
decision-makingcategories,whileacomprehensivebench-
toconfigureonesetofparametersfordifferentnoveltasks,
markthatsatisfiesthefollowingtwodesiderataislacking.
e.g.,onlyneedstospecifytheoptimizationobjectiveofthe
meta-controllerinourproposedCMDalgorithm.
‚Ä¢ D5: (Comprehensive)Itcoversallcategories
‚Ä¢ D6: (Academic-friendly)Itislessresource-intensive
3.RelatedWork
Inthiswork,wemakeapreliminaryattempttoaddressthese Therelatedliteratureistoovasttocoverinitsentirety. We
challengesandprovidethreemaincontributions. i)Wepro- presentanoverviewbelowtoemphasizeourcontributions
posethegeneralizedmirrordescent(GMD),ageneralization whilemorerelatedworkscanbefoundinAppendixB.
of existing MD algorithms (Nemirovskij & Yudin, 1983;
DecisionMaking. Substantialprogresshasbeenachieved
Beck&Teboulle,2003),whichincorporatesmultiplehistor-
indevelopingalgorithmstoaddressdifferentcategoriesof
icalpoliciesintothepolicyupdatingandisabletoexplore
decision-makingproblems,e.g.,DQN(Mnihetal.,2015)
abroaderclassofBregmandivergencebyaddressingthe
andPPO(Schulmanetal.,2017)forsingle-agentcategory,
Karush‚ÄìKuhn‚ÄìTucker(KKT)conditionsateachiteration.
QMIX(Rashidetal.,2018)andMAPPO(Yuetal.,2022)for
AsGMDisadoptedbyeachagentindependently,itcanbe
cooperativemulti-agentcategory,self-play(Tesauroetal.,
appliedtodifferentdecision-makingcategoriesinvolving
1995)andPSRO(Lanctotetal.,2017)forcompetitiveand
differentnumbersofagentsanddifferentrelationshipsbe-
MCCcategories,tonamejustafew. Despitethesuccesses
tweenagents(D1andD2). ii)Weproposetheconfigurable
inspecificcategories,thesemethodsoftencannotdirectly
mirrordescent(CMD)byintroducingameta-controllerto
generalizetodifferentcategories. Inthiswork,wemakea
dynamically adjust the hyper-parameters in GMD condi-
preliminaryattempttodevelopasinglealgorithmcapableof
tionalontheevaluationmeasures,allowingustostudydif-
tacklingallcategoriesofdecision-makingproblemswhich
ferentsolutionconceptsaswellasevaluationmeasures(D3
typicallyinvolvedifferentnumbersofagents,differentre-
andD4),withminimalmodifications. iii)Weconstructthe
lationshipsbetweenagents,differentsolutionconceptsas
GAMEBENCHconsistingof15gameswhichcoverallthe
wellasdifferentevaluationmeasures.
decision-makingcategories(D5)andaredeliberatelycon-
structedwiththeprinciplethatrunningalgorithmsonthese Mirror Descent. Mirror descent (MD) (Nemirovskij &
gamesdoesnotrequiremuchcomputationalresource(D6), Yudin,1983;Beck&Teboulle,2003;Vuraletal.,2022)has
andhence,formingacomprehensiveandacademic-friendly showneffectivenessinlearningoptimalpoliciesinsingle-
testbedforresearcherstoefficientlydevelopandtestnovel agentRL(Tomaretal.,2022)andprovedthelast-iteratecon-
algorithms. Extensive experiments on the GAMEBENCH vergenceinlearningapproximateequilibriuminzero-sum
showthatCMDachievesempiricallycompetitiveorbetter games(Bailey&Piliouras,2018;Kangarshahietal.,2018;
outcomescomparedtobaselineswhileofferingtheability Wibisonoetal.,2022;Kozunoetal.,2021;Leeetal.,2021;
toinvestigatediversedimensionsofdecisionmaking. Jainetal.,2022;Aoetal.,2023;Liuetal.,2023;Cenetal.,
2023;Sokotaetal.,2023)andsomeclassesofgeneral-sum
1Thisisrelatedtotheequilibriumselectionproblem(Harsanyi
games,e.g.,polymatrixandpotentialgames(Anagnostides
etal.,1988)anddifferentmeasuresleadtodifferentequilibria.
2ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
etal.,2022b). Despitetheprogress,existingworkstypically wehaveœÄ Œ† . Thejointpolicyofallagentsisdenoted
i i
‚àà
focusonsomespecificBregmandivergencesuchastheKL asœÄ =œÄ œÄ andœÄ Œ†whereŒ†denotesthejoint
1 N
‚äô¬∑¬∑¬∑‚äô ‚àà
divergence. WerelaxthispremisebyaddressingtheKKT policyspaceofallagents. Aspecialcaseofjointpolicyis
conditionsateachiteration,enablingustoexploreabroader theproductpolicydenotedasœÄ =œÄ œÄ . Also,let
1 N
√ó¬∑¬∑¬∑√ó
classofBregmandivergence. Moreover,byintroducinga œÄ =œÄ œÄ œÄ œÄ denotethejointpolicy
i 1 i 1 i+1 N
‚àí ‚äô¬∑¬∑¬∑ ‚àí ‚äô ¬∑¬∑¬∑‚äô
meta-controllertodynamicallyadjustthehyper-parameters, of all agents except i. Given the initial state s = s, the
0
ourCMDcanbeappliedtodifferentsolutionconceptsand valuefunctionofagentiisV i(s,œÄ):=E[ ‚àû t=0Œ≥tr it |s,œÄ]
evaluationmeasureswithminimalmodifications. wherertistheagenti‚Äôsrewardattimet 0. Furthermore,
wehavei V (ŒΩ,œÄ):=E [V (s,œÄ)]. ‚â•(cid:80)
Hyper-Parameter Tuning. Existing works typically de- i s ‚àºŒΩ i
terminethehyper-parametervaluesoftheMDalgorithms Solution Concepts. The policy of an agent is said to be
dependingondomainknowledge(Sokotaetal.,2023;Anag- optimalifitisoptimalineverydecisionpointbelongingto
nostidesetal.,2022b;Hsiehetal.,2021),whichmaynot theagent. Insingle-agentandcooperativecategories,this
beeasytogeneralizetodifferentgames. Ontheotherhand, optimalpolicymaximizestheexpectedreturnfortheagent
gradient-based hyper-parameter tuning methods such as ortheteam. Inmulti-agentcompetitiveandmixedcoopera-
STAC(Zahavyetal.,2020)arelessapplicableastheevalu- tiveandcompetitivecategories,weconsidertwocommon
ationmeasures,e.g.,NashConv,couldbenon-differentiable equilibriumconcepts: Nashequilibrium(NE)(Nash,1951)
withrespecttothehyper-parameters. Toaddresstheissue, andcoarsecorrelatedequilibrium(CCE)(Moulin&Vial,
weproposeasimpleyeteffectivezero-orderoptimization 1978). LetœÄ œÄ denotetheproductpolicyandœÄ œÄ
i i i i
√ó ‚àí ‚äô ‚àí
methodwheretheperformancedifferencebetweentwocan- denotethejointpolicy. Then,œÄ iscalledanNEifforeach
‚àó
didatesisusedtoonlydeterminetheupdatedirectionofthe agentiitsatisfies: œÄ Œ† ,V (ŒΩ,œÄ ) V (ŒΩ,œÄ œÄ ).
hyper-parametersratherthantheupdatemagnitude,which Similarly,œÄ
iscal‚àÄ ledi‚Ä≤ a‚àà CCi Eifi forea‚àó ch‚â• agei ntiiti‚Ä≤ s√ó atisfi‚àí‚àó ei
s:
‚àó
ismoreeffectivethanexistingmethods(Wangetal.,2022) œÄ Œ† ,V (ŒΩ,œÄ ) V (ŒΩ,œÄ œÄ ).
whenthevalueoftheperformanceisextremelysmall.
‚àÄ
i‚Ä≤
‚àà
i i ‚àó
‚â•
i i‚Ä≤
‚äô
‚àí‚àói
EvaluationMeasures. Let (œÄ)denotemeasuresusedto
L
evaluatea(joint)policyœÄ. Insingle-agentandcooperative
4.Preliminaries categories,themeasureisthedistanceofthe(joint)policy
totheoptimal(joint)policyœÄ ,whichisdefinedas (œÄ)=
In this section, we first introduce the model of decision ‚àó L
OptGap(œÄ)=V(ŒΩ,œÄ ) V(ŒΩ,œÄ). Inothercategories,we
makingandthesolutionconceptsandevaluationmeasures ‚àó ‚àí
considermultipleevaluationmeasures. Thefirstoneisthe
consideredinourwork. Then,wepresenttheclassicmirror
distanceofthejointpolicytotheequilibrium(NEorCCE).
descentalgorithm(Beck&Teboulle,2003).
For NE, we refer to this distance as NashConv, and for
CCE,werefertoitasCCEGap,asisconventioninprevious
4.1.DecisionMaking
works (Lanctot et al., 2017; Marris et al., 2021). More
POSG. A decision-making problem, either single-agent, specifically,wehaveNashConv(œÄ)= i [V i(ŒΩ,œÄ iBR √ó
c po eto ip tie vr ea cti av te e, goc ro ym ,cp ae ntit biv ee d, eo scr rm ibi ex ded asc ao po ap re tr iaa lt li yve oba sn ed rvc ao bm le- œÄ œÄ‚àí ‚àíi i)
)
‚àí‚àí VV i(i ŒΩ(ŒΩ ,œÄ,œÄ )]) ,] wa hn ed reC œÄC iBE RG isap th( eœÄ b) es=
tre
(cid:80)(cid:80) spi o‚àà‚àà nNN se[V (i B( RŒΩ, )œÄ piB oR lic‚äô
y
stochasticgame(POSG)(Oliehoek&Amato,2016)denoted ofagentiagainstallotheragents. Thesecondevaluation
as , , , ,‚Ñ¶,P,R,Œ≥,ŒΩ . = 1, ,N istheset measureweconsideristhesocialwelfare(SW)(Davis&
ofa‚ü® =N genS ts.A
S
iO st wh he efi rn eitese at no df‚ü© thN e as rt eat te h{ s e. fiA¬∑ n¬∑ i¬∑ =
tes√ó
ei t}
‚àà oN faA
ci tia on nd
s
Whinston,1962),denotedas L(œÄ)= i
‚ààN
V i(ŒΩ,œÄ).
O √ói ‚ààNOi Ai Oi (cid:80)
andobservationsofagenti,respectively. Leta denote 4.2.MirrorDescent
‚ààA
thejointactionofagentswherea isagenti‚Äôsaction.
i ‚ààAi From a single agent‚Äôs perspective, the condition for the
‚Ñ¶ = ‚Ñ¶ where‚Ñ¶ : istheobservation
√ói ‚ààN i i S √óA ‚Üí Oi optimalorequilibriumpolicycanbeexpressedbythefol-
functionspecifyingagenti‚Äôsobservationo whenall
i ‚ààOi lowingoptimizationproblemateachdecisionpointofthe
agentstakea atstates .P : ‚àÜ( )isthe
‚ààA ‚ààS S√óA‚Üí S agent(Tomaretal.,2022;Sokotaetal.,2023): œÑt t,
transitionfunctionwhichspecifiestheprobabilityoftransit- ‚àÄ i ‚ààTi
i dn eg noto tes
s‚Ä≤ th‚àà eS
simw ph le exn .a Rge =nts rta ike
i
a
‚àà whA
era et rs ita :tes ‚ààS. ‚àÜ R(
i¬∑
s) œÄ i‚àó(œÑ it)=ar œÄg i‚ààm Œ†a ixE
a ‚àºœÄi(œÑ
it)Q(œÑ it,a,œÄ
i
‚äôœÄ ‚àí‚àói), (1)
{ }‚ààN S√óA‚Üí
t fh ace tr oe rw
.
ŒΩardf ‚àÜun (cti )o dn eo nf oa teg sen tht ei da in sd triŒ≥ bu‚àà tio[0 n,1 ov) ei rs it nh ie tid ai ls sc to au ten st
.
w ach te iore n-Q va( lœÑ uit e,a fu, nœÄ c) ti= onE o[ fth‚àû h e= at+ ct1 ioŒ≥ nhr aih |œÑ it,at i at= tha e, dœÄ e] ci is sit oh ne
Attimest‚àà ept S 0,eachagenthasanaction-observationhis- pointœÑt. Withoutlosso(cid:80) fgenerality,w‚àà ewA ii llonlyfocuson
‚â• i
tory(i.e.,adecisionpoint)œÑ it ‚ààTitwhere Tit =( Oi √óAi)t thepolicylearningofasingleagentiinasingledecision
a on wd nc ro en tus rt nru .c Lts etit Œ†s ip do eli nc oy teœÄ ti h: eT pi ot li‚Üí cy‚àÜ sp( aA cei) ot fo agm ea nx ti im ,ti hz ae ti it ss , p asoi tn ht eœÑ yit ar‚àà eT cli et aa rn fd roh men tc he efo cr oth n, tet xh te ,i an nd dex wi ita hn ad sœÑ li it ga hr te ai bg un so ere od f
3ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Table1.Comparisonofdifferentmethods.‚àóNotethatMMDcanberegardedasthemethodthatcanconsidermultiplepreviouspolicies
bysettingthemagnetpolicytotheinitialpolicy(typicallyauniformpolicy).
Multipleprevious Workingonany Workingonany Configurablefor
Method
policies Bregmandivergence solutionconcept anymeasure
MD(Nemirovskij&Yudin,1983) (cid:37) (cid:37) (cid:37) (cid:37)
MMD (Sokotaetal.,2023) (cid:33) (cid:37) (cid:37) (cid:37)
‚àó
GMD(Thiswork) (cid:33) (cid:33) (cid:37) (cid:37)
CMD(Thiswork) (cid:33) (cid:33) (cid:33) (cid:33)
notation,wedenote theactionset ofagenti,œÄ Œ† Meta-Controller (MC)
i
A A ‚àà
theagent‚Äôspolicy,andQ(a)(orQ(a,œÄ))theaction-value
GMD
œÄ1 Œ±1
of the action a . Then, to learn the optimal or equi- ¬∑¬∑¬∑
‚àà A .
libriumpolicy,weaimtosolvetheoptimizationproblem: ¬∑¬∑¬∑ . . œÄ th‚àó is= pra or bg lem max isœÄ ‚àà thŒ† eE ma i‚àº rrœÄ oQ r( da e) s. cA enf te (a Msi Dbl )e ,m we ht ih co hd tato kes so tl hv ee upŒ±
date
eœÄ v1 a¬∑l¬∑u¬∑aœÄ tD
e
œÄD GMD sam¬∑¬∑ p¬∑
le
Œ±D
form(Beck&Teboulle,2003;Tomaretal.,2022):
Œ± Œ±
M 1 0
‚àí ¬∑¬∑¬∑¬∑¬∑¬∑
œÄ k+1 =argmax œÄ ‚ààŒ† ‚ü®Q(œÄ k),œÄ ‚ü©‚àíŒ± Bœï(œÄ,œÄ k), (2) œÄ k ‚àíM+1
¬∑¬∑¬∑¬∑¬∑¬∑
œÄ k
where 1 k K is the iteration, Q(œÄ ) is the action-
œÄ k+1=argmax
œÄ
‚ààŒ†‚ü®Q(œÄ k),œÄ
‚ü©‚àí
M œÑ=‚àí01Œ± œÑBœï(œÄ,œÄ
k
‚àíœÑ)
k
valuevect‚â§ orind‚â§ ucedbyœÄ k (forsimplicity,weletQ(œÄ k)= Generalized Mirror DesP cent (GMD)
(Q(a,œÄ )) ),Œ±istheregularizationintensity, isthe
k a œï
Bregmandi‚àà vA ergencewithrespecttothemirrormB apœï,de- Figure2. OverviewofCMD.
finedas (x;y)=œï(x) œï(y) œï(y),x y with
œï
B ‚àí ‚àí‚ü®‚àá ‚àí ‚ü© ‚ü®¬∑‚ü©
beingthestandardinnerproductandx,y ‚àÜ( ).
‚àà A 5.1.GeneralizedMirrorDescent
ThoughexistingMDalgorithms,e.g.,Eq. (2),canbealso
5.ConfigurableMirrorDescent
executedbyeachagentindependently,theycouldnotgen-
In this section, we propose a novel algorithm which sat- eralizewelltosatisfythedesiderataD1andD2. Themain
isfies the four desiderata (D1‚ÄìD4) presented in the Intro- reasonsaretwo-fold. First,classicMDalgorithms(Beck&
duction. First,weproposethegeneralizedmirrordescent Teboulle,2003;Tomaretal.,2022)typicallyonlyconsider
(GMD),ageneralizationofexistingMDalgorithms,which thecurrentpolicywhenderivingthenewpolicyateachitera-
whenindependentlyexecutedbyeachagent,caneffectively tion.However,ithasbeenshownthatincorporatingmultiple
tackledifferentdecision-makingcategoriesinvolvingdiffer- previouspolicies(e.g.,theinitialandcurrentpolicies)could
entnumbersofagentsanddifferentrelationshipsbetween bepowerfulinsolvingtwo-playerzero-sumgames(Sokota
agents(D1andD2). Second,weproposetheconfigurable etal.,2023;Liuetal.,2023). Second,eventhoughmultiple
mirrordescent(CMD)whereameta-controllerisintroduced previouspoliciesareconsidered,existingMDalgorithms
todynamicallyadjustthehyper-parametersofGMDcondi- typicallyfocusonsomespecificBregmandivergencesby
tionalontheevaluationmeasures,whichcanbeconfigured restrictingœïtocertainconvexfunctionswhichmaynotbe
toaccountfordifferentsolutionconceptsaswellasevalu- the optimal choices across different decision-making cat-
ationmeasures(D3andD4),withminimalmodifications. egories. Toaddressthesechallenges, weproposeamore
CMDsharessimilaritieswiththecentralizedtrainingand generalMDmethodsatisfyingthedesiderataD1andD2.
decentralizedexecution(CTDE)(Loweetal.,2017)since
AGeneralMDMethod. WeproposeamoregeneralMD
themeta-controllerconsidersallagentstooptimizethetar-
approach which takes multiple historical policies into ac-
getedevaluationmeasures(‚Äúcentralized‚Äùtrainingfromthe
countwhenderivingthenewpolicy,asgivenbelow:
controller‚Äôsperspective)whileGMDisexecutedbyeach
agentindependently(‚Äúdecentralized‚Äùexecutionfromeach M 1
œÄ =argmax Q(œÄ ),œÄ ‚àí Œ± (œÄ,œÄ ),
k+1 k œÑ œï k œÑ
agent‚Äôs perspective). The overview of CMD is shown in œÄ Œ† ‚ü® ‚ü©‚àí œÑ=0 B ‚àí
Figure2andTable1presentsacomparisontomoreclearly ‚àà (cid:88)
s.t. œÄ (a)=1andœÄ (a) 0, a , (3)
k k
positionourmethodsinthecontextofrelatedliterature. a ‚â• ‚àÄ ‚ààA
‚ààA
(cid:88)
4
)œÄ(
LConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
where M 1 is the number of historical policies, Œ± exploringabroaderclassofBregmandivergenceas,viathe
œÑ
‚â• ‚àà
(0,1]istheregularizationintensityofœÄ ,0 œÑ M 1, numericalmethodtocomputethevalueofŒª,itiscapableof
k œÑ
‚àí ‚â§ ‚â§ ‚àí
andletŒ±=(Œ± ) . Notethatsolvingtheproblem takingmorepossibleconvexfunctionsintoaccount. Table2
œÑ 0 œÑ M 1
‚â§ ‚â§ ‚àí
(3)toderivethepolicyupdatingrulecouldbechallenging. presentsthefunctionsconsideredinourwork. See(Boyd&
Inpractice,theproblemcouldhaveaclosed-formsolution Vandenberghe,2004)formoreexamples. x2 (i.e.,n = 2)
onlyincertainsettingssuchastheconvexfunctionœïisthe andxlnxrespectivelycorrespondtotheEuclideannorm
negativeentropyandtheconstraintsareremoved(i.e.,the andentropy. MoredetailscanbefoundinAppendixD.1.
unconstraineddomains(Sokotaetal.,2023)). Toaddress
thisissue,weproposeanovelmethodtosolvetheproblem Table2.Listofconvexfunctionsandrelatedfunctions,x (0,1].
‚àà
(3), which does not rely on the availability of the closed-
œà(x) œà (x) œà 1(x) [œà 1](x)
formsolutionandhence,canconsidermorepossibleoptions ‚Ä≤ ‚Ä≤‚àí ‚Ä≤‚àí ‚Ä≤
ofœï. Tothisend,first,wehavethefollowingresult: xn,n>1 nxn ‚àí1 ( nx)n‚àí1 1 n1 1( nx)2 n‚àí ‚àín 1
Proposition5.1. Assumethati)œÄ(a) ‚â•œµ, ‚àÄa ‚ààA,whereœµ xlnx lnx+1 ex ‚àí1 ‚àí ex ‚àí1
isasmallpositivevalueandii)theœï(œÄ)definedonŒ†canbe
d coe nc vo em xp fo unse cd tioto n2 dœï efi(œÄ ne) d= on[0a ,‚àà1A].œà T( hœÄ en(a ,) s) olw vih ne gre thœà epis ros bo lm eme 0<‚àí nxn <,
1
‚àínxn ‚àí1 ( ‚àínx)n‚àí1 1
1
‚àí1 n( ‚àínx)2 n‚àí ‚àín 1
(cid:80)
(3)canbeconvertedtosolvethefollowingequation: ekx,k >0 kekx ln(x/k)/k 1
x
œÄ(a)= œà ‚Ä≤‚àí1(A(a) Œª)/B)=1, (4)
a a ‚àí GMDSummary. Thepseudo-codeofGMDisprovidedin
‚ààA ‚ààA
(cid:88) (cid:88) Algorithm1. ComparedtoexistingMDalgorithms,GMD
whereA=Q(œÄ k)+ M œÑ=‚àí 01Œ± œÑœï ‚Ä≤(œÄ k œÑ),B = M œÑ=‚àí 01Œ± œÑ, couldsatisfywellthedesiderataD1andD2asitnotonly
Œªisthedualvariable,œà ‚Ä≤‚àí1istheinver‚àísefunctionofœà ‚Ä≤(the
takesmultiplepreviouspoliciesintoaccountbutisalsoca-
(cid:80) (cid:80)
derivativeofœà),andœÄ(a)=œà ‚Ä≤‚àí1(A(a) Œª)/B).
pableofleveragingmorepossibleBregmandivergencesthat
‚àí
maybebetterthanexistingchoicessuchasKLdivergence
ThisresultisobtainedviatheKarush‚ÄìKuhn‚ÄìTucker(KKT)
acrossdifferentdecision-makingcategories.
conditionsoftheLagrangefunctionobtainedbyapplying
theLagrangemultiplierŒª(i.e.,thedualvariable)totheprob-
Algorithm1GeneralizedMirrorDescent(GMD)
lem(3). ThefullderivationcanbefoundinAppendixD.1.
1: Givenœà,initialpolicyœÄ 1,M,Œ±,œµ
Numerical Method for Computing Œª. Now we need to 2: fork =1, ,K do
¬∑¬∑¬∑
solveEq. (4)toobtainthevalueofŒª. Unfortunately,this 3: ComputeAandBwithœÄ k andŒ±
typically cannot be solved analytically, rendering it less 4: ComputeŒªviaNewtonmethod(Algorithm3)
possibletoderivethepolicyupdatingrulewithouttheavail- 5: ComputeœÄ(a)=œà ‚Ä≤‚àí1(A(a) Œª)/B), a
‚àí ‚àÄ ‚ààA
abilityoftheclosed-formsolution. Toaddressthisissue,we 6: ComputeœÄ k+1(a)viaprojectionoperation, a
‚àÄ ‚ààA
usetheNewtonmethod(Ypma,1995)tocomputethevalue 7: endfor
ofŒª: repeatedlyexecutingŒª=Œª g(Œª)/g (Œª)forC >0
‚Ä≤
‚àí
iterations,whereg(Œª)= œà 1(A(a) Œª)/B) 1,
g (Œª) = 1[œà 1](Aa ‚àà(Aa) ‚Ä≤‚àí Œª)/B),‚àí and[œà 1]‚àí is 5.2.Meta-ControllerforDifferentMeasures
th‚Ä≤ ederivativa e‚ààAof‚àí œàB 1.‚Ä≤‚àí T(cid:2) h(cid:80)‚Ä≤ epseud‚àí o-codecanbefo‚Ä≤ u‚àí n(cid:3) d‚Ä≤
in
‚Ä≤‚àí WhileGMDcansatisfythedesiderataD1andD2,itcannot
(cid:80)
Algorithm3inAppendixD.1.
satisfy well the last two desiderata D3 and D4. The pri-
ProjectionOperation. AftercomputingthevalueofŒª,we maryreasonisthatwheneachagentindependentlyexecutes
cangetthepolicyœÄ(a)bysubstitutingitintotheexpression GMD,itisnotimmediatelyfeasibletoinvestigatedifferent
ofœÄ(a)aspresentedinEq. (4). Furthermore,weemploya solution concepts and evaluation measures as no explicit
projectionoperationtoensurethatœÄ(a) œµ. Specifically, objectiveregardingthedifferentmeasuresarisesinsucha
wehave: ‚àÄa ‚ààA,œÄ k+1(a)= (cid:80) a‚Ä≤m ‚ààAax m{ aœµ, xœÄ {‚â• ( œµa ,œÄ) } (a‚Ä≤) }. ‚Äú led ae dce tn otr da il fi fz ee rd e‚Äù ntex soec luu tt ii oo nn cp oro nc ce es ps ts(d ai nff der he en nt cm efe oa rs tu hr ,e ws eco wu il ld
l
DifferentBregmanDivergences. Inadditiontotakingmul- onlyfocusonthedifferentmeasures). Toaddressthisprob-
tiple historical policies into consideration, GMD3 further lem, we propose the configurable mirror descent (CMD)
generalizesexistingMDalgorithmswiththecapabilityof byintroducingameta-controller(MC)toadjustthehyper-
parametersinGMDconditionalontheevaluationmeasures,
2Thesumofconvexfunctionsisstillaconvexfunction.Further-
whichisa‚Äúcentralized‚Äùprocessfromthemeta-controller‚Äôs
more,thenegativeentropyandsquaredEuclideannormaretwo
perspective as it considers all the agents (joint policy) to
specialvariantsthathavebeenextensivelyadoptedinliterature.
3ThetermGMDisalsousedin(Radhakrishnanetal.,2020), optimizethetargetedevaluationmeasure(andhence, the
whichdiffersfromourmethod. targetedsolutionconcept),i.e.,D3andD4.
5055
056
057
058
059
060
061
ConfigurableMirrorDescent:Tow0a6r2dsaUnificationofDecisionMaking
063
Zero-OrderMeta-Controller. AsshowninEq. (3),given064 6. GAMEBENCH
065
thenumberofhistoricalpoliciesM 1,theonlycontrol-066
lablevariableisthehyper-parameters‚â• Œ±=(Œ± ) .067 Inthissection,wepresenttheGAMEBENCH,anovelbench-
Attheiterationk,letœÄ
=GMD(Œ±)4denotetheœÑ jo0 ‚â§inœÑ t‚â§pM ol‚àíic1 y0 06 68
9
markwhichconsistsof15gamescoveringallcategoriesof
derivedfromthepreviousjointpolicyœÄ byusingGMD070 decisionmakingandincludesdifferentevaluationmeasures
k 071 anddifferentalgorithms,whichisshowninFigure3.
withthegivenŒ±,andtheperformanceofthisjointpolicyis072
denotedas (œÄ). Notably,optimizingŒ±,unfortunately,is073
L 074 MirrorDescent CFR non-trivialastheevaluationmeasure isnon-differentiable075
L 076 CMD GMD MMD MD CFR CFR+
withrespecttoŒ±. Toaddressthisissue,weconstructanef-
077 This This (Sokota (Nemirovskij& (Zinkevich (Tammelin,
ficientzero-orderMCbyleveragingazero-ordermethodto078 work work etal.,2023) Yudin,1983) etal.,2007) 2014)
079
optimizeŒ±. AsshowninFigure2,MCupdatesŒ±through 080 Optimality Equilibria
threesteps: i)itsamplesDcandidates Œ±j D byperturb-081 OptGap SocialWalfare NashConv CCEGap
in œÄg jt =he Gc Murr De (n Œ±t jŒ±
)
a Dnd bth ye en md pe lr oi yv ie ns gD G{ Mne Dw} ,j i= j io )1 i in tt evp ao ll uic ai te es s0 0 08 8 82 3
4
V V(ŒΩ (, ŒΩœÄ ,‚àó œÄ) )‚àí V(ŒΩ,œÄ) PœÄi ‚àí‚àà iN)[ ‚àíVi V(ŒΩ i(, ŒΩœÄ ,iB œÄR )√ó ] PœÄi ‚àí‚àà iN)[ ‚àíVi V(ŒΩ i(, ŒΩœÄ ,iB œÄR )‚äô ]
{ thesenewjointpo} lij c= ie1 s {L(œÄj) }D j=1,andiii)itupdatesŒ±0 08 85 6 Sin Kg ule h- na -g Aent TC ino yo Hp ae nra abti iv -Ae Ze Kro u- hs num G Ben are gr aa il n- is nu gm MCM CKC uC
hn-A
basedontheperformanceofthesenewjointpolicies. 087 Kuhn-B TinyHanabi-B Leduc TradeComm MCCKuhn-B
088 Goofspiel-S TinyHanabi-C Goofspiel Battleship Goofspiel
Direction-GuidedUpdate.
LetŒ±1andŒ±2denotethetwo089
090
candidates sampled by perturbing the current Œ± and the091 Figure3. OverviewofGAMEBENCH.
corresponding joint policies œÄ1 and œÄ2 are obtained via092
093
GMD. Existing zero-order methods such as the random094 Motivation. Althoughvariousbenchmarkshavebeensug-
search(RS)(Liuetal.,2020)typicallyupdatetheŒ±directly095 gestedinliterature,theyaretypicallyspecializedforspe-
096
basedontheperformancedifferencebetweenthetwocan-097 cific decision-making categories, e.g., Atari (Bellemare
didates Œ¥ = (œÄ1) (œÄ2), which could be ineffective098 etal.,2013)forsingle-agentcategory,Hanabi(Bardetal.,
asthevalueoL f co‚àí uldL betoosmall(asshowninourex-0 19 09 0 2020)forcooperativecategory,Hold‚Äôempoker(Brown&
periments) to dL erive an effective update. To address this101 Sandholm,2018;2019)forcompetitivecategory,andfoot-
102
problem,weproposetoupdateŒ±basedonthesignofthe103 ball(Kurachetal.,2020)formixedcooperativeandcompet-
performance difference. Precisely, Œ¥ only determines the104 itivecategory. Ontheotherhand,asMDalgorithmsrequire
105
updatedirection,nottheupdatemagnitude,whichismore106 toexecutethepolicyupdatingateachdecisionpointateach
effective when the value of is too small. We call this107 iteration,runningthemontheexistingbenchmarkscouldbe
simpleyeteffectivetechniqueL thedirection-guidedupdate.1 10 08 9 resource-intensiveasthenumberofdecisionpointsinthe
Inourexperiments,weconstructanMC‚Äìdirection-guided environmentscouldbeextremelylarge(e.2g.,itisimpractical
randomsearch(DRS)‚Äìbyapplyingthismethodtotheexist- toenumeratetheobservationsinAtariastheyareimages).
ingRS(Wangetal.,2022). MoredetailsondifferentMCs
Desiderata. Motivatedbytheabovefacts,weconstructa
canbefoundinAppendixD.4.
newbenchmark‚ÄìGAMEBENCH.Itsatisfiesthetwodesider-
ataD5andD6presentedintheIntroduction. Thatis,itcov-
Algorithm2ConfigurableMirrorDescent(CMD) ersallcategoriesofdecision-makingproblems(comprehen-
1: Given ,œà,initial(joint)policyœÄ 1,M,D,œµ sive),andrunningMDalgorithms(orotheralgorithmssuch
L
2: fork =1, ,K do asCFR(Zinkevichetal.,2007))onallthegamesdoesnot
¬∑¬∑¬∑
3: SampleDcandidates Œ±j D requiremuchcomputationalresource(academic-friendly).
{ }j=1
4: Derivenewjointpolicies œÄj =GMD(Œ±j) D ThecomponentsofGAMEBENCHaregivenbelow.
{ }j=1
5: Evaluatenewjointpolicies {L(œÄj) }D j=1 Games. WecuratetheGAMEBENCHontopoftheOpen-
6: UpdateŒ±basedon (œÄj) D Spiel(Lanctotetal.,2019). Thereare15gameswhichare
{L }j=1
7: ComputeœÄ k+1viaGMDwiththeupdatedŒ± dividedinto5categories: single-agent,cooperativemulti-
8: endfor agent,competitivemulti-agentzero-sum,competitivemulti-
agentgeneral-sum,andmixedcooperativeandcompetitive
(MCC)categories. InourGAMEBENCH,theoriginalcom-
CMDSummary. ByincorporatingtheMCintoGMD,we
petitivecategoryisfurtherdividedintotwosubcategories‚Äì
establishtheCMD.Intuitively,CMDcanbeconfiguredto
zero-sumandgeneral-sum‚Äìastheycaninvolvedifferent
applytodifferentevaluationmeasuresandhence,cansatisfy
solutionconceptsandevaluationmeasures(givenbelow).
thedesiderataD3andD4whileonlyminimalmodifications
Weconstructall15gamesundertwoprimaryprinciples: i)
arerequired: specifyingtheMC‚Äôsoptimizationobjective .
L thesegamesinvolveasmanyaspectsofdecisionmakingas
Thepseudo-codeofCMDisshowninAlgorithm2.
possible,e.g.,thenumberofagents(singleormultiple)and
4Œ±isappliedtoalltheagentsinmulti-agentcategories. therelationshipbetweenagents(cooperative,competitive,
6
smhtiroglA
serusaeM
semaGConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
       
   
       
               
                   
                   
               
       
   
       
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                                                     
                   
                       
      
          
      
          
      
          
   
           
      
          
 & 0 '  * 0 '  0 0 '  . /  0 0 '  ( 8  & ) 5  & ) 5 
Figure4.Summaryofresults.Thefirst6figurescorrespondtosingle-agentandcooperativecategorieswherethey-axisisOptGap.The
restfigurescorrespondtoothercategorieswherethey-axisisNashConv.Forallthefigures,thex-axisisthenumberofiterations.
ormixed),andii)thesegamesarerelativelysimple,have 7.Experiments
alowbarriertoentry,andyetcomplexenough,andhence,
Inthissection,weevaluateourmethodonGAMEBENCH.
runningalgorithmsonthesegamesislessresource-intensive.
Wefirstdescribetheexperimentalsetup. Then,wepresent
Thedetailsoftheconstructionsandthestatisticsofthe15
theresultsbyansweringseveralresearchquestions(RQs)
gamescanbefoundinAppendixE.2.
Setup. Wecomparethefollowingmethods: i)CMD:our
Measures. AsGAMEBENCHincludesdifferentcategories
methodwherethehyper-parametersaredeterminedbythe
ofdecision-makingproblems,itisindispensabletoconsider
meta-controller introduced in Section 5.2, ii) GMD: the
multipleevaluationmeasures. Roughlyspeaking,thereare
hyper-parametersarefixedtoŒ± =1/M,0 œÑ M 1
twotypesofmeasures: i)thenotionofoptimality,including œÑ ‚â§ ‚â§ ‚àí
(a uniform distribution), iii) MMD-KL: the state-of-the-
OptGapandsocialwelfare,andii)thenotionofequilibrium,
art method called magnetic mirror descent (Sokota et al.,
includingNashConvandCCEGap. Notethatcomputingthe
2023)wherethepolicyupdatingruleisinducedwithKL
equilibrium-typemeasuresfortheMCCcategoryrequiresa
divergence, iv) MMD-EU: similar to MMD-KL but the
newmethodtocomputetheteam‚Äôsbestresponse(BR)(not
policyupdatingruleisinducedwithsquaredEuclideannorm
asingleagent‚Äôs). ThedetailscanbefoundinAppendixE.3.
(seeAppendixD.3forthederivation),v)CFR:thepolicy
Algorithms. WeincorporatedifferentMDalgorithmsinto isupdatedbasedonregret(Zinkevichetal.,2007),andvi)
theGAMEBENCH,includingthestate-of-the-artbaselines. CFR+: anadvancedversionofCFR(Tammelin,2014). In
ThecomparisonbetweentheseMDalgorithmscanbefound CMDandGMD,wealsoincludeamagnetpolicy,whichhas
inTable1.Inaddition,wealsoincludeCFR-typealgorithms beenargueddesirable(Sokotaetal.,2023;Liuetal.,2023).
asthebaselines,includingtheCFR(Zinkevichetal.,2007) Nevertheless,wenotethatthisdoesnotcauseinconsistency
andCFR+(Tammelin,2014). Althoughthesealgorithms with our method as we can equivalently obtain them by
needtoupdatethepolicyateachdecisionpoint,sincethe settingM andŒ± (seeAppendixD.2). Moreover,without
œÑ
numbersofdecisionpointsofthegamesinGAMEBENCH explicitlyspecifying,theresultsareobtainedunderœà(x)=
arenottoolarge,runningthesealgorithmsonthesegames xlnx,x (0,1].InRQ4,westudymorepossibleBregman
‚àà
isrelativelyeasy(academic-friendly). divergencesbysettingdifferentœàinTable2.
7ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
7.1.ResultsandAnalysis      . X K Q      / H G X F
RQ1. (DesiderataD1andD2)CanCMDeffectivelytackle
            
        a shll oc wat te hg eo lr eie as rno if nd ge pc eis ri fo on r- mm aa nk ci eng ofpr do ifb fl ee rm ens? tmIn etF hi og du sre a4 c, row se
s
          '
 '
 5 6 5  * 6
 / ' 6
             '
 '
 5 6 5  * 6
 / ' 6
1 ti5 veg la ym se os l. veFr ao lm lct ah tee gr oe rs iu el sts o, fw de ec ca isn ios nee -mth aa kt i, nC gM pD robc la en me sf :fe ic n-           *  *
 &  & )
 ) /  /
 5
 5 '  '   6              *  *
 &  & )
 ) /  /
 5
 5 '  '   6
single-agentandcooperativecategories,CMDcanfindthe
      
                      
   
                       
 , W H U D W L R Q  , W H U D W L R Q
approximateoptimal(joint)policy(theOptGapconverges
toanextremelysmallvalue),andinothercategories,CMD Figure5. ResultsfordifferenttypesofMC.
canfindtheapproximateNashequilibrium(theNashConv
convergestoanextremelysmallvalue). weshowthelearningcurvesofdifferentinstancesofCMD
instantiatedwithdifferentconvexfunctions. Fromthere-
RQ2. (ComparisonwithBaselines)HowdoesCMDper-
sults,wecanseethattheKLdivergence(œà(x) = xlnx),
formcomparedwithbaselines? AsshowninFigure4,by
thoughhasbeenwidelyadopted,couldbenottheoptimal
comparison,wecanobtainthefollowingtakeaways.
choice across all the decision-making categories. To our
‚Ä¢ Incorporatingmultiplehistoricalpoliciesanddynamically knowledge,CMDisthefirstalgorithmthatisendowedwith
adjustingthehyper-parameterscouldacceleratepolicy thecapabilityof(empirically)exploringabroaderclassof
learningintermsofthenumberofiterations. Thiscanbe Bregmandivergence, aprominentfeaturecomparedwith
verifiedbycomparingCMDwithMMD-KLwhereCMD existingMDmethods. SeeAppendixF.6formoreresults.
canconvergetoasimilarOptGaporNashConvvaluewith
MMD-KLusingfeweriterations. Thisadvantageholds  * R R I V S L H O  0 & & * R R I V S L H O
       
evenwithouttuningthehyper-parameters: inmostofthe     x xl 2nx exx0.1  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5      x xl 2nx exx0.1  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5 
games,GMD(thehyper-parametersarefixed)canalso        
       
converge to a similar OptGap or NashConv value with        
MMD-KLusingfeweriterations.ForGMDwithdifferent        
         
heuristichyper-parameteradjustmentstrategiessuchas
         
lineardecaycanbefoundinAppendixF.4.       
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q
‚Ä¢ Introducingthemeta-controllerisimportantasitnotonly
acceleratespolicylearningbutalsocouldachievecompet- Figure6. Resultsfordifferentconvexfunctions.
itiveorbetteroutcomes. Specifically,intermsofNash-
Conv,CMDoutperformsthebaselinesinMCCKuhn-A RQ5. (DesiderataD3andD4)CanCMDgeneralizetocon-
andMCCKuhn-Bandperformsonparwiththebaselines siderdifferentsolutionconceptsandevaluationmeasures?
inallothergames. However,inthemostdifficultLeduc InFigure7,weapplyCMDtotwodifferentmeasures:CCE-
poker,GMDcannoteffectivelydecreasetheNashConv, Gap(topline)andsocialwelfare(bottomline). Theresults
showcasingtheindispensabilityoftheMC. verify the effectiveness of incorporating multiple histori-
calpoliciesanddynamicallyadjustingthehyper-parameter
‚Ä¢ RegardingtheCFR-typealgorithms,theresultssimilarto
valuesconditionalontheevaluationmeasureswhenconsid-
previousworks(Sokotaetal.,2023)arealsoobserved: i)
eringotherevaluationmeasuresbeyondNashConv. More
ThevanillaCFRistypicallyinferiortoCFR+andCMD
resultsandanalysiscanbefoundinAppendixF.8. Notably,
in all the games, and ii) in most of the games, CFR+
ourCMDcanbeeasilyappliedtodifferentmeasureswith
outperforms CMD over short iteration horizons but is
minimalmodifications: changingtheMC‚Äôsobjective .
quicklycaughtbyCMDforlongerhorizons. L
RQ6. (DesiderataD5andD6)Isrunningthedifferentalgo-
RQ3. (DifferentMCs)Isthedirection-guidedupdateinthe
rithmscomputationallydifficult? Wefoundthat,although
meta-controllerimportant? InFigure5,wecompareDRS
extraoperationsmayberequired,runningtheMDandCFR
proposedinSection5.2withDGLDS,RS,GLDS,andGLD
algorithmsonGAMEBENCHdoesnotcausemuchburden
(seeAppendixD.4fordetailsontheseMCs).Aswecansee,
onthecomputationalresource. Theanalysisofthecompu-
ourproposedMCsignificantlyoutperformsotherseitherin
tationalcomplexitycanbefoundinAppendixF.9.
convergencerateorfinalperformance. InAppendixF.5,we
visualizetheevolutionofthehyper-parametervaluesduring
thepolicylearning,verifyingtheintuitionthatourDRScan 8.Limitations,FutureWorks,andConclusions
moreefficientlyadjustthehyper-parameters.
In this section, we discuss the limitations of the current
RQ4. (DifferentBregmanDivergences)HowdoesCMD versionofourapproachandpresentthefuturedirections,
performunderdifferentBregmandivergences? InFigure6, followedbyconclusionsofthiswork.
8
 Y Q R & K V D 1
 Y Q R & K V D 1
 Y Q R & K V D 1
 Y Q R & K V D 1ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 . X K Q  * R R I V S L H O Thirdly,thoughourmethodsarecapableofconsideringa
       
     &  * 0  0 '
 '
 0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5
 5 
     &  * 0  0 '
 '
 0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5
 5 
broaderclassofBregmandivergence,theystillrequirethe
        mathematicalformulationoftheconvexfunctionœàascom-
                     p mu et ti hn og dt rh ee qv ua irl eu se ao sf et th oe fd reu la al tev da fr uia nb cl te iob ny sdu es ri in vg edth fe roN mew œàt (o an
s
       
        showninTable2). Inotherwords,thenumberofBregman
                                                            divergencesconsideredinourmethodisstilllimited. An
 , W H U D W L R Q  , W H U D W L R Q interesting future direction is to develop a novel method
 7 U D G H & R P P  % D W W O H V K L S tomoreeffectivelyexploretheentirespaceoftheconvex
           
       &  * 0  0 '  '  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5         &  * 0  0 '  '  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5  function, such as using a neural network to represent the
            convexfunctionœà,whichleadstotheneuralBregmandi-
     
      vergence(Luetal.,2023;Siahkamarietal.,2020;Cilingir
     
      et al., 2020; Amos et al., 2017). Moreover, using neural
     
            Bregmandivergencecouldalsobeapossiblesolutionfor
                                                                automaticallychoosingtheBregmandivergencesfordiffer-
 , W H U D W L R Q  , W H U D W L R Q
entdecision-makingcategories. Nevertheless,itcouldbe
Figure7.ResultsforthemeasuresCCEGapandsocialwelfare. non-trivialtointegratetheneuralBregmandivergenceas
trainingtheneuralnetworktowellapproximatetheoptimal
convexfunctionforthegivencategorymaynotbeeasyand
8.1.LimitationsandFutureWorks thusmayrequirenewtreatment.
Thisworkaimstodevelopasinglealgorithmtoeffectively Finally,initscurrentversion,GAMEBENCHconsistsof15
tackleallcategoriesofdecision-makingproblems: single- academic-friendlygamescoveringallcategoriesofdecision-
agent,cooperativemulti-agent,competitivemulti-agent,and makingproblems,differentevaluationmeasures,andseveral
mixedcooperativeandcompetitivemulti-agentcategories. baselinealgorithms. Webelievefurtherextensionscouldbe
As a preliminary attempt, there are still some limitations valuable.i)Wecouldincludemoregameswithvaryingcom-
thatareworthinvestigatinginfutureworks. plexity(e.g.,differentnumbersofdecisionpoints)(Lanctot
etal.,2023). ii)Wecouldincludemoreevaluationmeasures
Firstly,asthemeta-controllerdeterminesthevaluesofthe
suchasfairness(Rabin,1993). iii)Wecouldincludemore
hyper-parameter by sampling multiple candidates, extra
algorithms. In particular, we may include deep learning-
computationalcostisneededtoevaluatetheperformanceof
based algorithms (Schulman et al., 2017; Yu et al., 2022;
thesecandidates. InAppendixF.9,wepresenttherunning
Lanctot et al., 2017) and investigate whether there exists
timeofaniterationofdifferentmethods. Whilerequiring
asingledeeplearning-basedalgorithmthatcaneffectively
extra computational cost, we view this as one of the fu-
solveallcategoriesofdecision-makingproblems. iv)Re-
turedirections: developingmorecomputationallyefficient
cently,muchattentionhasbeendrawntostudyingtheability
hyper-parametervalueupdatingmethodswithoutsacrific-
oflargelanguagemodels(LLMs)tosolvevariousdecision-
ing performance. For example, in contrast to the current
makingproblems(Hongetal.,2023;Bakhtinetal.,2022;
samplingmethodwherethehistoricalsamplesareentirely
Maoetal.,2023). Therefore,aninterestingextensionisto
ignoredaftereachupdate,thesehistoricalsamplescouldbe
includeLLMsasthebaselines, andifnecessary, develop
usedtoguidetheselectionofthenewhyper-parameterval-
newLLMstomoreeffectivelysolvedifferentcategoriesof
ues,e.g.,viaBayesianoptimization(Lindaueretal.,2022)
decision-makingproblems.
orofflinelearningapproaches(Chenetal.,2022). Asare-
sult,wemaynotneedtosamplemultiplecandidates,which
couldfurtherreducetheextracomputationalcost. 8.2.Conclusions
Secondly,weevaluatedCMDprimarilyfromtheempirical Inthiswork,wemakethepreliminaryattempttodevelop
a single algorithm to tackle ALL categories of decision-
perspective,andtheresultsdemonstrateitspromiseinsolv-
ingallcategoriesofdecision-makingproblems. Theoretical making problems and provide three contributions: i) the
analysisofthebehavior(e.g.,theconvergencerate)ofCMD GMD,ageneralizationofexitingMDalgorithms,whichcan
couldbeaninterestingproblemandmayrequirenoveltools beappliedtodifferentdecision-makingcategoriesinvolv-
that may be different from existing works since the pol- ingdifferentnumbersofagentsanddifferentrelationships
icyupdatingruleofCMDisestablishedwithanumerical betweenagents(D1andD2),ii)theCMDwhichcanbecon-
method,ratherthandependsontheclosed-formsolutionto figuredtoapplytodifferentsolutionconceptsandevaluation
theregularizedoptimizationproblemineachdecisionpoint measures(D3andD4),andiii)thecomprehensive(D5)and
under some specific Bregman divergence (Sokota et al., academic-friendly(D6)benchmark‚ÄìGAMEBENCH. Exten-
2023;Liuetal.,2023;Leeetal.,2021). siveexperimentsdemonstratetheeffectivenessofCMD.
9
 S D * ( & &
 H U D I O H :  O D L F R 6
 S D * ( & &
 H U D I O H :  O D L F R 6ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Acknowledgements Bailey,J.P.andPiliouras,G. Fastandfuriouslearningin
zero-sum games: Vanishing regret with non-vanishing
ThisworkissupportedbytheNationalResearchFounda-
stepsizes. InNeurIPS,pp.12997‚Äì13007,2019.
tion,SingaporeunderitsIndustryAlignmentFund‚ÄìPre-
positioning (IAF-PP) Funding Initiative. Any opinions, Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty,
findings and conclusions, or recommendations expressed C., Fried, D., Goff, A., Gray, J., Hu, H., Jacob, A. P.,
in this material are those of the author(s) and do not re- Komeili, M., Konath, K., Kwon, M., Lerer, A., Lewis,
flecttheviewsofNationalResearchFoundation,Singapore. M.,Miller,A.H.,Mitts,S.,Renduchintala,A.,Roller,S.,
Hau Chan is supported by the National Institute of Gen- Rowe,D.,Shi,W.,Spisak,J.,Wei,A.,Wu,D.,Zhang,
eralMedicalSciencesoftheNationalInstitutesofHealth H., and Zijlstra, M. Human-level play in the game of
[P20GM130461],theRuralDrugAddictionResearchCen- Diplomacybycombininglanguagemodelswithstrategic
terattheUniversityofNebraska-Lincoln,andtheNational reasoning. Science,378(6624):1067‚Äì1074,2022.
ScienceFoundationundergrantIIS:RI#2302999. Thecon-
Bard, N., Foerster, J. N., Chandar, S., Burch, N., Lanc-
tentissolelytheresponsibilityoftheauthorsanddoesnot
tot,M.,Song,H.F.,Parisotto,E.,Dumoulin,V.,Moitra,
necessarilyrepresenttheofficialviewsofthefundingagen-
S.,Hughes,E.,Dunning,I.,Mourad,S.,Larochelle,H.,
cies. This work is also supported by Shanghai Artificial
Bellemare, M. G., and Bowling, M. The Hanabi chal-
IntelligenceLaboratory.
lenge: AnewfrontierforAIresearch. ArtificialIntelli-
gence,280:103216,2020.
ImpactStatement
Beck, A.andTeboulle, M. Mirrordescentandnonlinear
Thispaperpresentsworkwhosegoalistoadvancethefield
projectedsubgradientmethodsforconvexoptimization.
ofdecisionmaking. Therearemanypotentialsocietalcon-
OperationsResearchLetters,31(3):167‚Äì175,2003.
sequences of our work, none of which we feel must be
specificallyhighlightedhere. Bellemare,M.G.,Naddaf,Y.,Veness,J.,andBowling,M.
The arcade learning environment: An evaluation plat-
formforgeneralagents. JournalofArtificialIntelligence
References
Research,47:253‚Äì279,2013.
Albrecht, J., Fetterman, A., Fogelman, B., Kitanidis, E.,
Wr√≥blewski, B., Seo, N., Rosenthal, M., Knutins, M., Berner,C.,Brockman,G.,Chan,B.,Cheung,V.,DeÀõbiak,P.,
Polizzi, Z., Simon, J., and Qiu, K. Avalon: A bench- Dennison,C.,Farhi,D.,Fischer,Q.,Hashme,S.,Hesse,
markforRLgeneralizationusingprocedurallygenerated C., J√≥zefowicz, R., Gray, S., Olsson, C., Pachocki, J.,
worlds. InNeurIPSDatasetsandBenchmarksTrack,pp. Petrov,M.,Pinto,H.P.d.O.,Raiman,J.,Salimans,T.,
12813‚Äì12825,2022. Schlatter,J.,Schneider,J.,Sidor,S.,Sutskever,I.,Tang,
J.,Wolski,F.,andZhang,S. Dota2withlargescaledeep
Amos, B., Xu, L., and Kolter, J. Z. Input convex neural reinforcementlearning.arXivpreprintarXiv:1912.06680,
networks. InICML,pp.146‚Äì155,2017. 2019.
Boyd, S. P. and Vandenberghe, L. Convex Optimization.
Anagnostides,I.,Farina,G.,Panageas,I.,andSandholm,T.
CambridgeUniversityPress,2004.
OptimisticmirrordescenteitherconvergestoNashorto
strongcoarsecorrelatedequilibriainbimatrixgames. In Brown,N.andSandholm,T. SuperhumanAIforheads-up
NeurIPS,pp.16439‚Äì16454,2022a. no-limitpoker: Libratusbeatstopprofessionals. Science,
359(6374):418‚Äì424,2018.
Anagnostides,I.,Panageas,I.,Farina,G.,andSandholm,T.
Onlast-iterateconvergencebeyondzero-sumgames. In Brown,N.andSandholm,T.SuperhumanAIformultiplayer
ICML,pp.536‚Äì581,2022b. poker. Science,365(6456):885‚Äì890,2019.
Campbell,M.,HoaneJr,A.J.,andHsu,F.-h. DeepBlue.
Ao,R.,Cen,S.,andChi,Y. Asynchronousgradientplayin
ArtificialIntelligence,134(1-2):57‚Äì83,2002.
zero-summulti-agentgames. InICLR,2023.
Carminati,L.,Zhang,B.H.,Farina,G.,Gatti,N.,andSand-
Aumann, R. J. Correlated equilibrium as an expression
holm,T. Hidden-rolegames: Equilibriumconceptsand
of Bayesian rationality. Econometrica: Journal of the computation. arXivpreprintarXiv:2308.16017,2023.
EconometricSociety,55(1):1‚Äì18,1987.
Cen,S.,Chi,Y.,Du,S.S.,andXiao,L. Fasterlast-iterate
Bailey,J.P.andPiliouras,G. Multiplicativeweightsupdate convergenceofpolicyoptimizationinzero-sumMarkov
inzero-sumgames. InEC,pp.321‚Äì338,2018. games. InICLR,2023.
10ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Chen,Y.,Song,X.,Lee,C.,Wang,Z.,Zhang,R.,Dohan, Jain,R.,Piliouras,G.,andSim,R. Matrixmultiplicative
D.,Kawakami,K.,Kochanski,G.,Doucet,A.,Ranzato, weights updates in quantum zero-sum games: Conser-
M., et al. Towards learning universal hyperparameter vationlaws&recurrence. InNeurIPS,pp.4123‚Äì4135,
optimizers with transformers. In NeurIPS, pp. 32053‚Äì 2022.
32068,2022.
Kangarshahi,E.A.,Hsieh,Y.-P.,Sahin,M.F.,andCevher,
Cilingir,H.K.,Manzelli,R.,andKulis,B. Deepdivergence V. Let‚Äôsbehonest: Anoptimalno-regretframeworkfor
learning. InICML,pp.2027‚Äì2037,2020. zero-sumgames. InICML,pp.2488‚Äì2496,2018.
Davis,O.A.andWhinston,A. Externalities,welfare,and Kozuno,T.,M√©nard,P.,Munos,R.,andValko,M. Model-
thetheoryofgames. JournalofPoliticalEconomy,70(3): freelearningfortwo-playerzero-sumpartiallyobservable
241‚Äì262,1962. Markovgameswithperfectrecall.InNeurIPS,pp.11987‚Äì
11998,2021.
deWitt,C.S.,Gupta,T.,Makoviichuk,D.,Makoviychuk,
V., Torr, P. H., Sun, M., and Whiteson, S. Is indepen- Kuhn,H.W. Asimplifiedtwo-personpoker. Contributions
dent learning all you need in the StarCraft multi-agent totheTheoryofGames,1:97‚Äì103,1950.
challenge? arXivpreprintarXiv:2011.09533,2020.
Kurach,K.,Raichuk,A.,Stan¬¥czyk,P.,ZajaÀõc,M.,Bachem,
Fang,Y.,Tang,Z.,Ren,K.,Liu,W.,Zhao,L.,Bian,J.,Li, O.,Espeholt,L.,Riquelme,C.,Vincent,D.,Michalski,
D., Zhang, W., Yu, Y., and Liu, T.-Y. Learning multi- M.,Bousquet,O.,andGelly,S. Googleresearchfootball:
agentintention-awarecommunicationforoptimalmulti- Anovelreinforcementlearningenvironment. InAAAI,
order execution in finance. In KDD, pp. 4003‚Äì4012, pp.4501‚Äì4510,2020.
2023.
Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A.,
Farina,G.,Bianchi,T.,andSandholm,T.Coarsecorrelation Tuyls,K.,P√©rolat,J.,Silver,D.,andGraepel,T. Auni-
inextensive-formgames. InAAAI,pp.1934‚Äì1941,2020. fiedgame-theoreticapproachtomultiagentreinforcement
learning. InNeurIPS,pp.4190‚Äì4203,2017.
Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and
Lanctot, M., Lockhart, E., Lespiau, J.-B., Zambaldi, V.,
Whiteson,S. Counterfactualmulti-agentpolicygradients.
InAAAI,pp.2974‚Äì2982,2018. Upadhyay, S., P√©rolat, J., Srinivasan, S., Timbers, F.,
Tuyls, K., Omidshafiei, S., Hennes, D., Morrill, D.,
Foerster,J.,Song,F.,Hughes,E.,Burch,N.,Dunning,I., Muller, P., Ewalds, T., Faulkner, R., Kram√°r, J., De
Whiteson,S.,Botvinick,M.,andBowling,M. Bayesian Vylder,B.,Saeta,B.,Bradbury,J.,Ding,D.,Borgeaud,
actiondecoderfordeepmulti-agentreinforcementlearn- S., Lai, M., Schrittwieser, J., Anthony, T., Hughes, E.,
ing. InICML,pp.1942‚Äì1951,2019. Danihelka,I.,andRyan-Davis,J. OpenSpiel: Aframe-
workforreinforcementlearningingames. arXivpreprint
Golowich,N.,Pattathil,S.,andDaskalakis,C. Tightlast- arXiv:1908.09453,2019.
iterateconvergenceratesforno-regretlearninginmulti-
playergames. InNeurIPS,pp.20766‚Äì20778,2020. Lanctot,M.,Schultz,J.,Burch,N.,Smith,M.O.,Hennes,
D.,Anthony,T.,andPerolat,J. Population-basedevalua-
Harsanyi, J. C., Selten, R., et al. A General Theory of tioninrepeatedrock-paper-scissorsasabenchmarkfor
EquilibriumSelectioninGames. TheMITPress,1988. multiagentreinforcementlearning. TMLR,2023. ISSN
2835-8856.
Hong,S.,Zheng,X.,Chen,J.,Cheng,Y.,Wang,J.,Zhang,
C.,Wang,Z.,Yau,S.K.S.,Lin,Z.,Zhou,L.,Ran,C., Lee,C.-W.,Kroer,C.,andLuo,H. Last-iterateconvergence
Xiao,L.,Wu,C.,andSchmidhuber,J. MetaGPT:Meta inextensive-formgames. InNeurIPS,pp.14293‚Äì14305,
programming for multi-agent collaborative framework. 2021.
arXivpreprintarXiv:2308.00352,2023.
Lewis,M.,Yarats,D.,Dauphin,Y.,Parikh,D.,andBatra,
Hsieh, Y.-G., Antonakopoulos, K., and Mertikopoulos, P. D. Dealornodeal? End-to-endlearningofnegotiation
Adaptivelearningincontinuousgames: Optimalregret dialogues. InEMNLP,pp.2443‚Äì2453,2017.
boundsandconvergencetoNashequilibrium. InCOLT,
pp.2388‚Äì2422,2021. Lindauer, M., Eggensperger, K., Feurer, M., Biedenkapp,
A.,Deng,D.,Benjamins,C.,Ruhkopf,T.,Sass,R.,and
Ilyas,A.,Engstrom,L.,Athalye,A.,andLin,J. Black-box Hutter, F. SMAC3: A versatile Bayesian optimization
adversarialattackswithlimitedqueriesandinformation. packageforhyperparameteroptimization. TheJournal
InICML,pp.2137‚Äì2146,2018. ofMachineLearningResearch,23(1):2475‚Äì2483,2022.
11ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Liu,M.,Ozdaglar,A.E.,Yu,T.,andZhang,K. Thepower Nemirovskij, A. S. and Yudin, D. B. Problem Com-
of regularization in solving extensive-form games. In plexity and Method Efficiency in Optimization. Wiley-
ICLR,2023. Interscience,1983.
Liu, S., Chen, P.-Y., Kailkhura, B., Zhang, G., Hero III, Oliehoek,F.A.andAmato,C. AConciseIntroductionto
A. O., and Varshney, P. K. A primer on zeroth-order DecentralizedPOMDPs. Springer,2016.
optimizationinsignalprocessingandmachinelearning:
Perolat,J.,DeVylder,B.,Hennes,D.,Tarassov,E.,Strub,
Principals,recentadvances,andapplications. IEEESig-
F., de Boer, V., Muller, P., Connor, J. T., Burch, N.,
nalProcessingMagazine,37(5):43‚Äì54,2020.
Anthony, T., McAleer, S., Elie, R., Cen, S. H., Wang,
Liu, S., Lever, G., Wang, Z., Merel, J., Eslami, S. A., Z., Gruslys, A., Malysheva, A., Khan, M., Ozair, S.,
Hennes,D.,Czarnecki,W.M.,Tassa,Y.,Omidshafiei,S., Timbers,F.,Pohlen,T.,Eccles,T.,Rowland,M.,Lanctot,
Abdolmaleki,A.,etal. Frommotorcontroltoteamplay M., Lespiau, J.-B., Piot, B., Omidshafiei, S., Lockhart,
insimulatedhumanoidfootball. ScienceRobotics,7(69): E.,Sifre,L.,Beauguerlange,N.,Munos,R.,Silver,D.,
eabo0235,2022a. Singh,S.,Hassabis,D.,andTuyls,K.Masteringthegame
of Stratego with model-free multiagent reinforcement
Liu,W.,Jiang,H.,Li,B.,andLi,H. Equivalenceanalysis
learning. Science,378(6623):990‚Äì996,2022.
betweencounterfactualregretminimizationandonline
mirrordescent. InICML,pp.13717‚Äì13745,2022b. Rabin, M. Incorporating fairness into game theory and
economics.TheAmericanEconomicReview,83(5):1281‚Äì
Lowe,R.,Wu,Y.,Tamar,A.,Harb,J.,Abbeel,P.,andMor-
1302,1993.
datch,I. Multi-agentactor-criticformixedcooperative-
competitiveenvironments. InNeurIPS,pp.6382‚Äì6393, Radhakrishnan,A.,Belkin,M.,andUhler,C.Linearconver-
2017. genceofgeneralizedmirrordescentwithtime-dependent
mirrors. arXivpreprintarXiv:2009.08574,2020.
Lu,F.,Raff,E.,andFerraro,F.NeuralBregmandivergences
fordistancelearning. InICLR,2023. Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G.,
Foerster,J.,andWhiteson,S. QMIX:Monotonicvalue
Mao, S., Cai, Y., Xia, Y., Wu, W., Wang, X., Wang, F.,
functionfactorisationfordeepmulti-agentreinforcement
Ge, T., and Wei, F. Alympics: Language agents meet
learning. InICML,pp.4295‚Äì4304,2018.
gametheory‚Äìexploringstrategicdecision-makingwith
AIagents. arXivpreprintarXiv:2311.03220,2023. Rashid, T., Farquhar, G., Peng, B., and Whiteson, S.
WeightedQMIX:Expandingmonotonicvaluefunction
Marris, L., Muller, P., Lanctot, M., Tuyls, K., and Grae-
factorisationfordeepmulti-agentreinforcementlearning.
pel,T. Multi-agenttrainingbeyondzero-sumwithcorre-
InNeurIPS,pp.10199‚Äì10210,2020.
latedequilibriummeta-solvers. InICML,pp.7480‚Äì7491,
2021. Rizk,Y.,Awad,M.,andTunstel,E.W. Cooperativehetero-
geneousmulti-robotsystems: Asurvey. ACMComputing
Mertikopoulos,P.,Lecouat,B.,Zenati,H.,Foo,C.-S.,Chan-
Surveys,52(2):1‚Äì31,2019.
drasekhar,V.,andPiliouras,G. Optimisticmirrordescent
insaddle-pointproblems:Goingtheextra(gradient)mile. Ross,S.M. Goofspiel‚Äìthegameofpurestrategy. Journal
InICLR,2019. ofAppliedProbability,8(3):621‚Äì625,1971.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve- Samvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G.,
ness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H.,
Fidjeland,A.K.,Ostrovski,G.,Petersen,S.,Beattie,C., Foerster,J.,andWhiteson,S. TheStarCraftmulti-agent
Sadik,A.,Antonoglou,I.,King,H.,Kumaran,D.,Wier- challenge. arXivpreprintarXiv:1902.04043,2019.
stra,D.,Legg,S.,andHassabis,D. Human-levelcontrol
Schmid,M.,Moravcik,M.,Burch,N.,Kadlec,R.,David-
throughdeepreinforcementlearning. Nature,518(7540):
son, J., Waugh, K., Bard, N., Timbers, F., Lanctot, M.,
529‚Äì533,2015.
ZachariasHolland,G.,Davoodi,E.,Christianson,A.,and
Moulin, H. and Vial, J. P. Strategically zero-sum games: Bowling,M. Studentofgames: Aunifiedlearningalgo-
Theclassofgameswhosecompletelymixedequilibria rithmforbothperfectandimperfectinformationgames.
cannotbeimprovedupon. InternationalJournalofGame ScienceAdvances,9(46):eadg3256,2023.
Theory,7:201‚Äì221,1978.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Nash,J. Non-cooperativegames. AnnalsofMathematics, Klimov, O. Proximal policy optimization algorithms.
54(2):286‚Äì295,1951. arXivpreprintarXiv:1707.06347,2017.
12ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Serrino,J.,Kleiman-Weiner,M.,Parkes,D.C.,andTenen- Tammelin,O. Solvinglargeimperfectinformationgames
baum,J. Findingfriendandfoeinmulti-agentgames. In usingCFR+. arXivpreprintarXiv:1407.5042,2014.
NeurIPS,pp.1251‚Äì1261,2019.
Tesauro, G. et al. Temporal difference learning and TD-
Shoham, Y. and Leyton-Brown, K. Multiagent Systems: Gammon. Communications of the ACM, 38(3):58‚Äì68,
Algorithmic,Game-theoretic,andLogicalFoundations. 1995.
CambridgeUniversityPress,2008.
Tomar, M., Shani, L., Efroni, Y., and Ghavamzadeh, M.
Siahkamari,A.,XIA,X.,Saligrama,V.,Casta√±√≥n,D.,and Mirrordescentpolicyoptimization. InICLR,2022.
Kulis,B. LearningtoapproximateaBregmandivergence.
Tsai, Y.-Y., Chen, P.-Y., and Ho, T.-Y. Transfer learning
InNeurIPS,pp.3603‚Äì3612,2020.
without knowing: Reprogramming black-box machine
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, learningmodelswithscarcedataandlimitedresources.
I.,Huang,A.,Guez,A.,Hubert,T.,Baker,L.,Lai,M., InICML,pp.9614‚Äì9624,2020.
Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L.,
v.Neumann,J. Zurtheoriedergesellschaftsspiele. Mathe-
van den Driessche, G., Graepel, T., and Hassabis, D.
matischeAnnalen,100(1):295‚Äì320,1928.
Mastering the game of Go without human knowledge.
Nature,550(7676):354‚Äì359,2017.
Vural,N.M.,Yu,L.,Balasubramanian,K.,Volgushev,S.,
andErdogdu,M.A. Mirrordescentstrikesagain: Opti-
Singh, B., Kumar, R., and Singh, V. P. Reinforcement
malstochasticconvexoptimizationunderinfinitenoise
learninginroboticapplications: Acomprehensivesurvey.
variance. InCOLT,pp.65‚Äì102,2022.
ArtificialIntelligenceReview,55(2):945‚Äì990,2022.
Wang,J.,Ren,Z.,Liu,T.,Yu,Y.,andZhang,C. QPLEX:
Sokota,S.,Lockhart,E.,Timbers,F.,Davoodi,E.,D‚ÄôOrazio,
Duplexduelingmulti-agentQ-learning. InICLR,2021a.
R.,Burch,N.,Schmid,M.,Bowling,M.,andLanctot,M.
Solvingcommon-payoffgameswithapproximatepolicy Wang,J.,Xu,W.,Gu,Y.,Song,W.,andGreen,T.C. Multi-
iteration. InAAAI,pp.9695‚Äì9703,2021. agentreinforcementlearningforactivevoltagecontrolon
powerdistributionnetworks. InNeurIPS,pp.3271‚Äì3284,
Sokota,S.,D‚ÄôOrazio,R.,Kolter,J.Z.,Loizou,N.,Lanctot,
2021b.
M., Mitliagkas, I., Brown, N., and Kroer, C. A uni-
fiedapproachtoreinforcementlearning,quantalresponse Wang,T.andKaneko,T. Applicationofdeepreinforcement
equilibria, and two-player zero-sum games. In ICLR, learninginwerewolfgameagents. InTAAI,pp.28‚Äì33,
2023. 2018.
Son,K.,Kim,D.,Kang,W.J.,Hostallero,D.E.,andYi,Y. Wang,X.,Guo,W.,Su,J.,Yang,X.,andYan,J. ZARTS:
QTRAN:Learningtofactorizewithtransformationfor Onzero-orderoptimizationforneuralarchitecturesearch.
cooperativemulti-agentreinforcementlearning. InICML, InNeurIPS,pp.12868‚Äì12880,2022.
pp.5887‚Äì5896,2019.
Wibisono,A.,Tao,M.,andPiliouras,G. Alternatingmirror
Song,X.,Gao,W.,Yang,Y.,Choromanski,K.,Pacchiano, descentforconstrainedmin-maxgames. InNeurIPS,pp.
A.,andTang,Y. ES-MAML:SimpleHessian-freemeta 35201‚Äì35212,2022.
learning. InICLR,2020.
Xu,B.,Wang,Y.,Wang,Z.,Jia,H.,andLu,Z. Hierarchi-
Su,H.,Zhong,Y.D.,Dey,B.,andChakraborty,A. Emv- callyandcooperativelylearningtrafficsignalcontrol. In
light: Adecentralizedreinforcementlearningframework AAAI,pp.669‚Äì677,2021.
forefficientpassageofemergencyvehicles. InAAAI,pp.
4593‚Äì4601,2022. Xu,Z.,Liang,Y.,Yu,C.,Wang,Y.,andWu,Y. Fictitious
cross-play: LearningglobalNashequilibriuminmixed
Sun,M.,Devlin,S.,Beck,J.,Hofmann,K.,andWhiteson, cooperative-competitive games. In AAMAS, pp. 1053‚Äì
S. TrustregionboundsfordecentralizedPPOundernon- 1061,2023.
stationarity. InAAMAS,pp.5‚Äì13,2023a.
Ypma,T.J. HistoricaldevelopmentoftheNewton-Raphson
Sun,S.,Wang,R.,andAn,B. Reinforcementlearningfor method. SIAMReview,37(4):531‚Äì551,1995.
quantitative trading. ACM Transactions on Intelligent
SystemsandTechnology,14(3):1‚Äì29,2023b. Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen,
A.,andWu,Y. ThesurprisingeffectivenessofPPOin
Sutton,R.S.andBarto,A.G. ReinforcementLearning: An cooperativemulti-agentgames. InNeurIPSDatasetsand
Introduction. MITpress,2018. BenchmarksTrack,pp.24611‚Äì24624,2022.
13ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Zahavy, T., Xu, Z., Veeriah, V., Hessel, M., Oh, J., van
Hasselt,H.,Silver,D.,andSingh,S. Aself-tuningactor-
criticalgorithm. InNeurIPS,pp.20913‚Äì20924,2020.
Zhou,Z.,Mertikopoulos,P.,Athey,S.,Bambos,N.,Glynn,
P.W.,andYe,Y. Learningingameswithlossyfeedback.
InNeurIPS,pp.5134‚Äì5144,2018.
Zinkevich,M.,Johanson,M.,Bowling,M.,andPiccione,C.
Regretminimizationingameswithincompleteinforma-
tion. InNeurIPS,pp.1729‚Äì1736,2007.
14ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
A.CodeRepository
Codeforexperimentsisavailableathttps://github.com/IpadLi/CMD.
B.MoreRelatedWorks
Single-AgentCategory. Inthesingle-agentcategory, reinforcementlearning(RL)(Sutton&Barto,2018)hasproved
successful in many real-world applications. The power of RL is further amplified with the integration of deep neural
networks,leadingtovariousdeepRLalgorithmsthathavebeensuccessfullyappliedtovariousapplicationdomainssuch
asvideogames(Mnihetal.,2015), robotnavigation(Singhetal.,2022), andfinancialtechnology(Sunetal.,2023b).
Amongthesealgorithms,PPO(Schulmanetal.,2017)isoneofthemostcommonlyusedmethodstosolvesingle-agentRL
problems. RecentworkshaveshownthatindependentPPO(deWittetal.,2020;Sunetal.,2023a)caneffectivelysolve
single-agentandcooperativemulti-agentRLproblems. Inaddition,avariantofPPOisalsoshowntobeeffectiveinsolving
two-playerzero-sumgameswhenbothplayersadoptthisalgorithm(Sokotaetal.,2023). Nevertheless,itremainselusive
whetherthesesingle-agentalgorithmscanbeappliedtosolveothercategoriesofdecision-makingproblemswhichmay
involvedifferentpropertiesincludingdifferentnumbersofagents,differentrelationshipsbetweenagents,differentsolution
concepts,anddifferentevaluationmeasures. Inthiswork,weaimtodevelopasinglealgorithmthat,whenexecutedbyeach
agent,providesaneffectiveapproachtoaddressdifferentcategoriesofdecision-makingproblems.
CooperativeMulti-AgentCategory. Cooperativemulti-agentRL(MARL)hasbeendemonstratedsuccessfulinsolving
manyreal-worldcooperativetaskssuchastrafficsignalcontrol(Xuetal.,2021;Suetal.,2022),powermanagement(Wang
etal.,2021b),finance(Fangetal.,2023),andmulti-robotcooperation(Rizketal.,2019). Inthepastdecade,avarietyof
MARLalgorithms,e.g.,QMIX(Rashidetal.,2018)anditsvariants(Sonetal.,2019;Rashidetal.,2020;Wangetal.,
2021a),MADDPG(Loweetal.,2017),COMA(Foersteretal.,2018),andMAPPO(Yuetal.,2022),tonamejustafew,
havebeenproposedandachievedsignificantperformanceinvariousmulti-agentbenchmarks,e.g.,SMAC(Samvelyan
etal.,2019)andDotaII(Berneretal.,2019). Thesealgorithmstypicallyfollowtheprincipleofcentralizedtrainingand
decentralizedexecution(CTDE)whereglobalinformationisonlyavailableduringtraining. Despitetheirsuccess,they
cannotbedirectlyappliedtocompetitiveandmixedcooperativeandcompetitivecategories.Inthiswork,ourproposedCMD
canbeappliedtodifferentdecision-makingcategoriesandsharesimilaritieswiththeCTDEparadigm: themeta-controller
takesalltheagents(i.e.,thejointpolicy)intoaccounttooptimizethehyper-parametersconditionalonthetargetedevaluation
measure(a‚Äúcentralized‚Äùprocess)whileeachagentintheenvironmentindependentlyexecutetheGMDwiththegiven
hyper-parameterstoupdatethepolicy(a‚Äúdecentralized‚Äùprocess).
CompetitiveMulti-AgentCategory. Therehaslongbeenahistoryofresearcherspursuingartificialintelligence(AI)
agentsthatcanachievehuman-levelorsuper-human-levelperformanceinsolvingvariouscompetitivemulti-agentgames
suchaschess(Campbelletal.,2002),Go(Silveretal.,2017),poker(Brown&Sandholm,2019),andStratego(Perolatetal.,
2022). Duetothecompetitivenature,thedevelopmentoflearningalgorithmsforsolvingthesegamesistypicallylargely
differentfromsingle-agentandcooperativeMARL.Amongothers,counterfactualregretminimization(CFR)(Zinkevich
etal.,2007)andpolicy-spaceresponseoracles(PSRO)(Lanctotetal.,2017)aretworepresentativealgorithmsthathave
beenwidelyusedtosolvecomplexgames(Schmidetal.,2023). Anothercategoryofalgorithmthathasdrawnincreasing
attentionrecentlyisthemirrordescent(MD)(Nemirovskij&Yudin,1983;Beck&Teboulle,2003). IncontrasttoCFR
and PSRO which are ‚Äúaverage-iterate‚Äù algorithms, MD has proved the ‚Äúlast-iterate‚Äù convergence property in solving
two-player zero-sum games (Bailey & Piliouras, 2018; Kangarshahi et al., 2018; Wibisono et al., 2022; Kozuno et al.,
2021;Leeetal.,2021;Jainetal.,2022;Aoetal.,2023;Liuetal.,2023;Cenetal.,2023;Sokotaetal.,2023)andsome
classesofgeneral-sumgames(Anagnostidesetal.,2022b). Moreover,MDhasalsobeendemonstratedeffectiveinsolving
single-agentRLproblems(Tomaretal.,2022). Despitetheirsuccess,existingMDalgorithmstypicallyfocusonsome
specific Bregman divergences which may not be the optimal choices across different decision-making categories. Our
proposedCMDgeneralizesexistingMDalgorithmstoconsiderabroaderclassofBregmandivergence,whichcouldachieve
betterlearningperformanceinaddressingdifferentcategoriesofdecision-makingproblems.
MixedCooperativeandCompetitiveCategory. Insomereal-worldscenarios,therelationshipbetweenagentscouldbe
neitherpurelycooperativenorpurelycompetitive. Forexample,inafootballgame,theagentsbelongingtothesameteam
needtocooperatewhilealsocompetingwiththeotherteam(Kurachetal.,2020). Inhidden-rolegames(Carminatietal.,
2023),eachagenttriestoidentifytheir(unknown)teammatesandcompetewithother(unknown)adversaries(Wang&
Kaneko,2018;Serrinoetal.,2019;Albrechtetal.,2022). However,incontrasttotheotherthreecategories(single-agent,
purelycooperative,andpurelycompetitive),mixedcooperativeandcompetitive(MCC)gamesarelargelyunstudied(Xu
15ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
etal.,2023). Furthermore,asMDalgorithmstypicallyrequireupdatingthepolicyateachdecisionpoint,runningthemon
thecurrentbenchmarkgamessuchasfootball(Kurachetal.,2020)couldbecomputationallyprohibited. Inthepresent
work,weconstruct3MCCgamesthatareacademic-friendly‚Äìtheirnumbersofdecisionpointsarenottoolargeandhence,
runningMDalgorithms(andotheralgorithmssuchasCFR-type(Zinkevichetal.,2007;Tammelin,2014))onthesegames
doesnotrequiremuchcomputationalresource(e.g.,runningtimeandmemoryusage).
Hyper-ParameterTuning. Existingworkstypicallydeterminethehyper-parametervaluesoftheMDalgorithmsdepending
onthedomainknowledge(Sokotaetal.,2023;Anagnostidesetal.,2022b;Hsiehetal.,2021;Zhouetal.,2018;Mertikopoulos
et al., 2019; Bailey & Piliouras, 2019; Golowich et al., 2020), which, though convenient for theoretical analysis, may
not be easy to generalize to different games. On the other hand, as the evaluation measures, e.g., NashConv, could be
non-differentiablewithrespecttothehyper-parameters,thegradient-basedmethodssuchasSTAC(Zahavyetal.,2020)
couldalsobelessapplicable. Inthissense,amorefeasiblemethodisthezero-orderhyper-parameteroptimizationwhichcan
updatetheparametersofinterestwithoutaccesstothetruegradient,whichhasbeenextensivelyadoptedintheadversarial
robustnessofdeepneuralnetworks(Ilyasetal.,2018), meta-learning(Songetal.,2020), transferlearning(Tsaietal.,
2020),andneuralarchitecturesearch(NSA)(Wangetal.,2022). Nevertheless,wefoundthatdirectlyapplyingexisting
zero-ordermethodscouldbeineffectiveaswhenthevalueoftheevaluationmeasureistoosmall,theymaynotbeable
toderiveaneffectiveupdateforthehyper-parameter. Toaddressthisissue,weproposeasimpleyeteffectivetechnique‚Äì
direction-guidedupdate‚Äìwheretheperformancedifferencebetweentwocandidatesisusedtoonlydeterminetheupdate
directionofthehyper-parametersratherthantheupdatemagnitude,whichismoreeffectivethanexistingmethods(Wang
etal.,2022)whenthevalueoftheperformanceisextremelysmall.
16ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
C.NotationTable
Table3. NotationTable.
= 1, ,N ,thesetofN agents.
N N { ¬∑¬∑¬∑ }
thefinitesetofstates.
S
= where isthefinitesetofactionsofagenti.
i i i
A A √ó‚ààNA A
= where isthefinitesetofobservationsofagenti.
i i i
O O √ó‚ààNO O
‚Ñ¶ ‚Ñ¶= ‚Ñ¶ where‚Ñ¶ : istheobservationfunctionofagenti.
i i i i
√ó‚ààN S√óA‚ÜíO
P P : ‚àÜ( ),thestatetransitionfunction.
R R=S√ó r A‚Üí wheS rer : Ristherewardfunctionofagenti.
i i i
{ }‚ààN S√óA‚Üí
Œ≥ Œ≥ [0,1),thediscountfactor.
‚àà
ŒΩ ŒΩ ‚àÜ( ),theinitialstatedistribution.
‚àà S
œÑt thedecisionpoint(action-observationhistory)ofagentiattimet,œÑt t.
i i ‚ààTi
t t =( )t,thespaceofdecisionpointsofagentiattimestept.
Ti Ti Oi √óAi
Œ† Œ†= Œ† whereŒ† isthepolicyspaceofagenti.
i i i
√ó‚ààN
œÄ œÄ =œÄ œÄ ,thejointpolicy,œÄ =œÄ œÄ ,theproductpolicy.
1 N 1 N
V (s,œÄ),V (ŒΩ,œÄ) thevalue‚äô fu¬∑ n¬∑¬∑ ct‚äô ionsofagenti,V (ŒΩ,œÄ):=E √ó¬∑ [V¬∑¬∑ (√ó s,œÄ)].
i i i s ŒΩ i
‚àº
(œÄ) theevaluationmeasureofthejointpolicyœÄ.
L
œÄ thesingleagent‚Äôspolicyattheiterationkofanalgorithm.
k
œÄ thejointpolicyattheiterationkofanalgorithm.
k
Q(œÄ ) Q(œÄ )=(Q(a,œÄ )) ,theaction-valuevectorofasingleagentinducedbyœÄ .
k k k a k
‚ààA
(x;y) (x;y)=œï(x) œï(y) œï(y),x y ,theBregmandivergencewithrespecttoœï.
œï œï
B B ‚àí ‚àí‚ü®‚àá ‚àí ‚ü©
K thenumberofiterationsofanalgorithm.
M M 1,thenumberofhistoricalpolicies.
‚â•
Œ± Œ±=(Œ± ) ,Œ± istheregularizationintensityofœÄ .
œÑ 0 œÑ M 1 œÑ k œÑ
‚â§ ‚â§ ‚àí ‚àí
œµ œµ>0,thesmallestprobabilityofanaction.
œï(œÄ) œï(œÄ)= œà(œÄ(a)),œàissomeconvexfunctiondefinedon[0,1].
a
Œª,Œ≤ Œ≤ =(Œ≤ ) ‚ààA,thedualvariables.
a a
A,B A=Q(œÄ(cid:80) k‚àà )A + M œÑ=‚àí 01Œ± œÑœï ‚Ä≤(œÄ k œÑ),B = M œÑ=‚àí 01Œ± œÑ,whereœï ‚Ä≤isthederivativeofœï.
œà 1 theinversefunctionofœà (thed‚àí erivativeofœà).
‚Ä≤‚àí ‚Ä≤
(cid:80) (cid:80)
C C >0,thenumberofiterationsfortheNewtonmethod.
g(Œª),g (Œª) g(Œª)= œà 1(A(a) Œª)/B) 1,g (Œª)= 1[œà 1](A(a) Œª)/B).
‚Ä≤ a ‚Ä≤‚àí ‚àí ‚àí ‚Ä≤ a ‚àíB ‚Ä≤‚àí ‚Ä≤ ‚àí
[œà 1] thederivative‚ààoAfœà 1. ‚ààA
‚Ä≤‚àí ‚Ä≤ ‚Ä≤‚àí
(cid:2)(cid:80) (cid:3) (cid:80)
D thenumberofsampledcandidateŒ±‚Äôs.
Œ±j D DcandidatesbyperturbingthecurrentŒ±.
{ }j=1
œÄj D œÄj =GMD(Œ±j) D ,DnewjointpoliciesderivedviaGMD.
{ }j=1 { }j=1
¬µ thesmoothingparameterinDRSandRS.
[r ,r ] theintervaloftheradiusesofthespheresinDGLDS,GLDS,andGLD.
L H
uj D Dcandidateupdatessampledfromasphericallysymmetricdistributionuj q.
{ }j=1 ‚àº
Œ±j ,Œ±j Œ±j =CLIP1(Œ±+¬µuj),Œ±j =CLIP1(Œ± ¬µuj),thecandidatesbyperturbingthecurrentŒ±.
œÄ+ j,œÄj‚àí œÄj+ =GMDŒπ (Œ±j ),œÄj =G‚àí MD(Œ±j ),Œπ then‚àí ewjointpoliciesobtainedviaGMD.
+ + +
Œ¥j ‚àí Œ¥j = (œÄj) (œÄj‚àí ),theperform‚àí ancedifferencebetweenœÄj andœÄj .
u thefinL alu+ pda‚àí te.L ‚àí + ‚àí
‚àó
Sgn Sgn(z)=1ifz >0,Sgn(z)= 1ifz <0,otherwise,Sgn(z)=0.
‚àí
CLIP1 CLIP1(z)=Œπifz <Œπ,CLIP1(z)=1ifz >1,otherwise,CLIP1(z)=z,where0<Œπ<1.
Œπ Œπ Œπ Œπ
Œ∫ Œ∫ 1,updatetheŒ±everyŒ∫iterations.
‚â•
œÅ themagnetpolicyinMMD.
Œæ Œæ >0,theregularizationintensityofthemagnetpolicy.
Œ∑ Œ∑ >0,thestepsizeinMMD.
Œ∑Àú Œ∑Àú>0,thestepsizeofthemagnetpolicyinMMD.
17ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
D.ConfigurableMirrorDescent
Inthissection,wepresentallthedetailsofourmethods. InSectionD.1,wepresentthedetailsofGMD.InSectionD.2,we
establishsomeconnectionsbetweenGMDandexistingMDalgorithms. InSectionD.3,werestricttheconvexfunction
œïtothesquaredEuclideannormandderivetheclosed-formsolutionundertheMMDpolicyupdatingrule. Finally,in
SectionD.4,wepresentthedetailsofdifferentmeta-controllers.
D.1.GeneralizedMirrorDescent
Inthissection,wepresenttheproofofProposition5.1,thepseudo-codeofNewton‚Äôsmethodforcomputingthevalueofthe
dualvariable,andtheconvexfunctionsconsideredinourwork.
ProofofProposition5.1. Considertheoptimizationproblem(3). BythedefinitionofBregmandivergence,wehave:
M 1
œÄ =argmax Q(œÄ ),œÄ ‚àí Œ± (œÄ,œÄ ), (5)
k+1 œÄ Œ† k œÑ œï k œÑ
‚àà ‚ü® ‚ü©‚àí œÑ=0 B ‚àí
(cid:88)M 1
œÄ
k+1
=argmax
œÄ Œ†
Q(œÄ k),œÄ ‚àí Œ± œÑ[œï(œÄ) œï(œÄ
k
œÑ) œï ‚Ä≤(œÄ
k
œÑ),œÄ œÄ
k œÑ
], (6)
‚áí ‚àà ‚ü® ‚ü©‚àí œÑ=0 ‚àí ‚àí ‚àí‚ü® ‚àí ‚àí ‚àí ‚ü©
(cid:88)M 1 M 1
œÄ
k+1
=argmax
œÄ Œ†
Q(œÄ k)+ ‚àí Œ± œÑœï ‚Ä≤(œÄ
k
œÑ),œÄ œï(œÄ) ‚àí Œ±
œÑ
+const., (7)
‚áí ‚àà ‚ü® œÑ=0 ‚àí ‚ü©‚àí œÑ=0
(cid:88) (cid:88)
where‚Äúconst.‚Äù summarizesalltermsthatareirrelevanttoœÄ. LetA=Q(œÄ k)+ M œÑ=‚àí 01Œ± œÑœï ‚Ä≤(œÄ k œÑ)andB = œÑM =‚àí 01Œ± œÑ
‚àí
whicharefixedatthecurrentiterationk. Then,wecanconvertEq. (3)tothefollowingoptimizationproblem:
(cid:80) (cid:80)
œÄ =argmax A,œÄ Bœï(œÄ)+const.,
k+1 œÄ ‚ààŒ†‚ü® ‚ü©‚àí
(8)
s.t. œÄ (a)=1andœÄ (a) 0, a .
k k
a ‚â• ‚àÄ ‚ààA
‚ààA
(cid:88)
Tosolvetheconstrainedoptimizationproblem(8),wecanapplytheLagrangemultiplier,whichgivesus:
L(œÄ,Œª,Œ≤)= A,œÄ Bœï(œÄ)+const. Œª œÄ(a) 1 + Œ≤ œÄ(a), (9)
a
‚ü® ‚ü©‚àí ‚àí a ‚àí a
(cid:16)(cid:88) ‚ààA (cid:17) (cid:88) ‚ààA
whereŒªandŒ≤ =(Œ≤ ) arethedualvariables.Forsuchproblems,wecangettheKarush‚ÄìKuhn‚ÄìTucker(KKT)conditions
a a
‚ààA
foreachcomponent(action)a asfollows:
‚ààA
A(a)+Bœï ‚Ä≤(œÄ)(a) Œª+Œ≤
a
=0, (10a)
‚àí
œÄ(a)=1, (10b)
a
‚ààA
(cid:88) Œ≤ œÄ(a)=0, (10c)
a
œÄ(a) 0,Œ≤ 0. (10d)
a
‚â• ‚â•
ThentheproblemistofindapolicyœÄsuchthatitsatisfiesalltheaboveconditions,whichcouldbedifficultowingtotwo
reasons: i)itsimultaneouslyinvolvesthetwodualvariablesŒªandŒ≤ ,andii)inEq. (10a),computingthevalueœï(œÄ)(a)
a ‚Ä≤
involvesallthecomponents(actions)asœïisdefinedonthepolicyœÄ,nottheindividualcomponent(action)a .
‚ààA
Toaddressthechallenges,weapplythetwoconditions: œÄ(a) œµandœï(œÄ)= œà(œÄ(a)). Then,wehaveœï(œÄ)(a)=
‚â• a ‚Ä≤
œà (œÄ(a)). Asaresult,theproblem(10a‚Äì10d)issimplifiedtothefollowingproblem‚ààA:
‚Ä≤
(cid:80)
A(a)+Bœà ‚Ä≤(œÄ(a)) Œª=0, (11a)
‚àí
œÄ(a)=1, (11b)
a
‚ààA
(cid:88) œÄ(a) œµ. (11c)
‚â•
FromEq. (11a),wecangetthat: a ,
‚àÄ ‚ààA
A(a) Œª
œÄ(a)=œà ‚Ä≤‚àí1 ‚àí , (12)
B
(cid:18) (cid:19)
18ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
whereœà 1istheinversefunctionofœà . SubstitutingtheaboveexpressionforœÄ(a)intotheconstraint(10b),wehave:
‚Ä≤‚àí ‚Ä≤
A(a) Œª
œÄ(a)= œà ‚Ä≤‚àí1 ‚àí =1, (13)
a a B
‚ààA ‚ààA (cid:18) (cid:19)
(cid:88) (cid:88)
whichcompletestheproof.
NumericalMethodforComputingŒª. Notably,Eq. (13)typicallycannotbesolvedanalytically. Toaddressthisproblem,
weproposetouseanumericalmethodtocomputethevalueofŒª,offeringthepossibilityofexploringabroaderclassof
Bregmandivergence. Specifically,foranyconvexfunctionœà,weemploytheNewtonmethod(Ypma,1995)tocomputethe
valueofŒª,whichisshowninAlgorithm3,whereC isthenumberofiterations.
Algorithm3NewtonmethodforcomputingthevalueofŒª
1: Givenœà,A,andB. RandomlyinitializethevalueofŒª
2: forC iterationsdo
3: Œª=Œª g(Œª)
‚àí g‚Ä≤(Œª)
4: endfor
DifferentBregmanDivergences. InTable2,welisttheconvexfunctionsconsideredinourwork. Tobemoreintuitive,we
plottheseconvexfunctionsinFigure8.
                
            
        
            
                
                
    
       
        
            
                
                                                                                                   
x x x x
(a) xlnx (b) x2 (c) x0.1 (d) ex
‚àí
Figure8. Plotsfordifferentconvexfunctionsœà.
D.2.ConnectionBetweenGMDandExistingMDAlgorithms
Inthissection,wepresentsomediscussionontheconnectionbetweenGMDandexistingMDalgorithms. InTable4,we
presenttheconditionsforconvertingGMDtodifferentMDalgorithmsandtheirformulations.
Table4. ConnectionbetweenGMDandexistingMDalgorithms.
Method Conditions Formulation
MD M =1,Œ± (0,1] œÄ =argmax Q(œÄ ),œÄ Œ± (œÄ,œÄ )
0 k+1 œÄ Œ† k 0 œï k
‚àà ‚àà ‚ü® ‚ü©‚àí B
M =k,Œ± ,Œ± (0,1],
MMD Œ±
œÑ
=0,0k <‚àí1 œÑ <0 k‚àà 1 œÄ k+1 =argmax œÄ ‚ààŒ† ‚ü®Q(œÄ k),œÄ ‚ü©‚àíŒ± k ‚àí1 Bœï(œÄ,œÄ 1) ‚àíŒ± 0 Bœï(œÄ,œÄ k)
‚àí
GMD MD.ItistrivialtogettheMDalgorithm(Nemirovskij&Yudin,1983;Beck&Teboulle,2003)bysettingM =1
‚Üí
andŒ± >0,thatis,MDonlyconsidersthecurrentpolicyœÄ whenderivingthenewpolicyœÄ .
0 k k+1
GMD MMD.ToobtainMMD(Sokotaetal.,2023),wecansetM =kandthenonlyletŒ± k 1 andŒ± 0 tobepositive,
‚Üí ‚àí
whileallothertermsare0. Thatis,MMDconsiderstwopreviouspolicies‚ÄìtheinitialpolicyœÄ andthecurrentpolicyœÄ ‚Äì
1 k
whenderivingthenewpolicy. IntheMMD‚Äôsterminology,theinitialpolicyœÄ servesasthemagnetpolicy.
1
Inpractice,wecangetmorevariantsbysettingtheM andŒ±,whichshowsthatGMDisageneralmethod. Forexample,we
canconsiderbothM <kpreviouspoliciesandtheinitialpolicyœÄ (i.e.,addingamagnetpolicy)whenderivingthenew
1
19
)x( )x( )x( )x(ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
policyœÄ ,whichistakenasthedefaultchoiceforinstantiatingtheGMDinourexperiments,thatis,
k+1
M 1
‚àí
œÄ =argmax Q(œÄ ),œÄ Œ± (œÄ,œÄ ) Œ± (œÄ,œÄ ). (14)
k+1 k k 1 œï 1 œÑ œï k œÑ
œÄ Œ†‚ü® ‚ü©‚àí ‚àí B ‚àí B ‚àí
‚àà œÑ=0
(cid:88)
InAppendixF.7,weperformanablationstudytoshowtheeffectivenessofaddingsuchamagnetpolicy. Nevertheless,itis
worthnotingthatthisparticularchoiceshouldnotbeconfusedwiththeoriginalMMDevenwhenM = 1asthepolicy
updatingruleisderivedviaanumericalmethod,insteadofrelyingontheclosed-formsolution(Sokotaetal.,2023).
D.3.DerivationofMMD-EU
Inthissection,wepresentthedetailsofthebaseline,MMD-EU,usedinourexperiments. Thisbaselinefollowsthespiritof
MMD-KL(Sokotaetal.,2023). Considerthefollowingproblem:
1
œÄ =argmax Q(œÄ ),œÄ Œæ (œÄ,œÅ) (œÄ,œÄ ), (15)
k+1 k œï œï k
œÄ Œ†‚ü® ‚ü©‚àí B ‚àí Œ∑B
‚àà
whereŒæ >0istheregularizationintensity,Œ∑ >0isthestepsize,andœÅisthemagnetpolicy. Letœï(œÄ)= 1 œÄ(a) 2,
a 2‚à• ‚à•
i.e.,thesquaredEuclideannorm. Then,weneedtooptimizethefollowingobjective: ‚ààA
(cid:80)
Œæ 1
Q(œÄ ),œÄ œÄ œÅ 2 œÄ œÄ 2, (16)
‚ü® k ‚ü©‚àí 2|| ‚àí ||2‚àí 2Œ∑|| ‚àí k ||2
withtheconstraint œÄ(a)=1andœÄ(a)>0. WecanusetheLagrangemultipliertogetthefollowingobjective:
a
‚ààA
(cid:80)
Œæ 1
Q(œÄ ),œÄ œÄ œÅ 2 œÄ œÄ 2+Œª 1 œÄ(a) . (17)
‚ü® k ‚ü©‚àí 2|| ‚àí ||2‚àí 2Œ∑|| ‚àí k ||2 (cid:32) ‚àí (cid:33)
a
(cid:88)‚ààA
TakingthederivativeofbothœÄandŒª,wehave:
1
Q(a,œÄ ) Œæ(œÄ(a) œÅ(a)) (œÄ(a) œÄ (a)) Œª=0, a , (18)
k k
‚àí ‚àí ‚àí Œ∑ ‚àí ‚àí ‚àÄ ‚ààA
œÄ(a)=1. (19)
a
(cid:88)‚ààA
ThereforefromEq.(18),wehave:
ŒæœÅ(a)+ 1œÄ (a)+Q(a,œÄ ) Œª
œÄ(a)= Œ∑ k k ‚àí . (20)
(Œæ+ 1)
Œ∑
SubstitutingtheaboveequationtoEq.(19),wehave:
ŒæœÅ(a)+ 1œÄ (a)+Q(a,œÄ ) Œª
Œ∑ k k ‚àí =1, (21)
(Œæ+ 1)
a Œ∑
(cid:88)‚ààA
1 1
ŒæœÅ(a)+ œÄ (a)+Q(a,œÄ ) =(Œæ+ )+ Œª. (22)
k k
‚áí Œ∑ Œ∑
a (cid:20) (cid:21) a
(cid:88)‚ààA (cid:88)‚ààA
Notethat ŒæœÅ(a)+ 1œÄ (a) =Œæ+ 1,wehave:
a Œ∑ k Œ∑
‚ààA
(cid:104) (cid:105)
(cid:80)
Q(a,œÄ )
Œª= a ‚ààA k . (23)
(cid:80) |A|
Thenwecancomputethenewpolicyasfollows:
ŒæœÅ(a)+ 1œÄ (a)+Q(a,œÄ ) 1 Q(a,œÄ )
œÄ(a)= Œ∑ k k ‚àí |A| a‚Ä≤ ‚ààA ‚Ä≤ k . (24)
(Œæ+ 1)
Œ∑ (cid:80)
20ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
Theoretically,wenotethatbychoosingthesuitablevaluesforŒæ andŒ∑,wecanalwaysensurethatœÄiswell-defined,i.e.,
œÄ(a) 0, a . Inexperiments,wecanuseaprojectionoperationtoensurethiscondition(Œ∂ =1e 10isusedtoavoid
‚â• ‚àÄ ‚ààA ‚àí
divisionbyzero):
max 0,œÄ(a) +Œ∂
œÄ (a)= { } . (25)
k+1
max 0,œÄ(a) +Œ∂
a‚Ä≤ { ‚Ä≤ }
‚ààA
Inaddition,similartoMMD-KL,themagnetpolicyisupdatedas(i.e.,movingmagnet):
(cid:80)
œÅ (a)=(1 Œ∑Àú)œÅ (a)+Œ∑ÀúœÄ (a), (26)
k+1 k k+1
‚àí
whereŒ∑Àú>0isthelearningrateforthemagnetpolicyœÅ. Inpractice,theinitialmagnetpolicyœÅ canbesettotheinitial
1
policyœÄ whichistypicallyauniformpolicy.
1
D.4.Meta-ControllerforDifferentMeasures
GMDgeneralizesexitingMDalgorithmsintwoaspects: i)ittakesmultiplepreviouspoliciesintoaccountandcanrecover
someoftheexistingMDalgorithmsbysettingtheM andŒ±,andii)itcanconsiderabroaderclassofBregmandivergence
bysettingœïtomorepossibleconvexfunctions(Table2). Asaconsequence,wearguethatwhenGMDisexecutedbyeach
agentindependently,itcouldsatisfythefirsttwodesiderataD1andD2presentedintheIntroduction. However,asmentioned
inSection5.2,sincethereisnoexplicitobjectiveregardingdifferentevaluationmeasures(anddifferentsolutionconcepts)
arisesinthis‚Äúdecentralized‚Äùexecutionprocess, GMDitselfcannotsatisfywellthelasttwodesiderataD3andD4. To
addressthechallenges,oursolutionisthezero-ordermeta-controller(MC)whichdynamicallyadjuststhehyper-parameters
conditionalontheevaluationmeasures(Section5.2). Inthissection,wepresentthedetailsofdifferentMCs.
Direction-GuidedRandomSearch(DRS).OurDRSmethodisobtainedbyapplyingthedirection-guidedupdate(Sec-
tion5.2)totheexistingRSmethodpresentedin(Wangetal.,2022). Specifically, attheiterationk, wefirstsampleD
candidateupdates uj D fromasphericallysymmetricdistributionuj q. Then,weupdateŒ±asfollows:
{ }j=1 ‚àº
Œ±j =CLIP1(Œ±+¬µuj), Œ±j =CLIP1(Œ± ¬µuj), 1 j D,
+ Œπ ‚àí Œπ ‚àí ‚â§ ‚â§
œÄj =GMD(Œ±j ), œÄj =GMD(Œ±j ), 1 j D,
+ + ‚àí ‚àí ‚â§ ‚â§
Œ¥j = (œÄj) (œÄj ), 1 j D,
L + ‚àíL ‚àí ‚â§ ‚â§ (DRS)
D
u
‚àó
= Sgn(Œ¥j)uj,
‚àí j=1
Œ± ‚ÜêCL(cid:88)IP1 Œπ(Œ±+u ‚àó).
Sgn(z)isdefinedas: Sgn(z)=1ifz >0,Sgn(z)= 1ifz <0,otherwise,Sgn(z)=0. ¬µisthesmoothingparameter
determiningtheradiusofthesphere. CLIP1 istheel‚àí ement-wiseclippingoperationdefinedas: CLIP1(z) = Œπifz < Œπ,
Œπ Œπ
CLIP1(z) = 1ifz > 1,otherwise,CLIP1(z) = z,where0 < Œπ < 1. NotethattheclippingoperationwhichboundsŒ±
Œπ Œπ
aboveŒπ>0isnecessaryasthetermBisusedasthedenominatorinEq. (12). Inaddition,theoperationSgn()playsan
¬∑
importantroleanddifferentiatesourDRSfromthevanillaRS(Wangetal.,2022). Intuitively,inthegameswherethevalue
oftheevaluationmeasure isextremelysmallandconvergesquickly,themagnitudeofŒ¥j wouldbetoosmalltoderivean
effectiveupdate. IncontrasL t,byusingtheoperationSgn(),thedifferencebetweentheperformanceofŒ±j andŒ±j willonly
determinetheupdatedirection,nottheupdatemagnitud¬∑ e,whichcouldbemoreeffective. + ‚àí
RandomSearch(RS).ThevanillaRSwhichisadaptedfrom(Wangetal.,2022). TheonlydifferencefromDRSisit
updatesŒ±directlybasedontheperformancedifferenceŒ¥j. Precisely,wehave:
Œ±j =CLIP1(Œ±+¬µuj), Œ±j =CLIP1(Œ± ¬µuj), 1 j D,
+ Œπ ‚àí Œπ ‚àí ‚â§ ‚â§
œÄj =GMD(Œ±j ), œÄj =GMD(Œ±j ), 1 j D,
+ + ‚àí ‚àí ‚â§ ‚â§
Œ¥j = (œÄj) (œÄj ), 1 j D,
L + ‚àíL ‚àí ‚â§ ‚â§ (RS)
D
u = Œ¥juj,
‚àó
‚àí j=1
Œ± ‚ÜêCL(cid:88)IP1 Œπ(Œ±+u ‚àó).
GradientLessDescent(GLD).Thismethodisadaptedfrom(Wangetal.,2022). Attheiterationk,wefirstsampleD
candidateupdates uj D . DifferentfromRSwhichsamplesthecandidatesfromafixedradius(thesmoothingparameter
{ }j=1
21ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
¬µinDRSandRS),weindependentlysamplethecandidatesonsphereswithvariousradiusesuniformlysampledfromthe
interval[r ,r ]. Then,weupdateŒ±asfollows:
L H
Œ±j =CLIP1(Œ±+uj), œÄj =GMD(Œ±j ), 1 j D,
+ Œπ + + ‚â§ ‚â§
j =argmin (œÄj) D ,
‚àó j {L + }j=1 (GLD)
u
=uj‚àó
,
‚àó
Œ± ‚ÜêCLIP1 Œπ(Œ±+u ‚àó).
Intuitively,bycomparingtheperformanceofDcandidates,Œ±isupdatedbythecandidatewiththesmallestvalueof .
L
GradientLessDescentwithSummation(GLDS).DifferentfromGLDwhichusesonlyoneoftheDsamplestoupdatethe
Œ±,wecanfollowtheideaofRS/DRStotakeallthecandidatesintoaccountbysummation. Specifically,let (œÄ )denote
k
L
theperformanceofthecurrentpolicyœÄ ,thenwehave:
k
Œ±j =CLIP1(Œ±+uj), œÄj =GMD(Œ±j ), 1 j D,
+ Œπ + + ‚â§ ‚â§
Œ¥j = (œÄj) (œÄ ), 1 j D,
L + ‚àíL k ‚â§ ‚â§
D (GLDS)
u = Œ¥juj,
‚àó
‚àí j=1
Œ± ‚ÜêCL(cid:88)IP1 Œπ(Œ±+u ‚àó).
Direction-GuidedGLDS(DGLDS).Applyingthedirection-guidedupdatetotheGLDS,wecangetthismethod. Precisely,
let (œÄ )denotetheperformanceofthecurrentpolicyœÄ ,thenwehave:
k k
L
Œ±j =CLIP1(Œ±+uj), œÄj =GMD(Œ±j ), 1 j D,
+ Œπ + + ‚â§ ‚â§
Œ¥j = (œÄj) (œÄ ), 1 j D,
L + ‚àíL k ‚â§ ‚â§
D (DGLDS)
u
‚àó
= Sgn(Œ¥j)uj,
‚àí j=1
Œ± ‚ÜêCL(cid:88)IP1 Œπ(Œ±+u ‚àó).
Asthemeta-controllerneedstoevaluatetheperformanceofthecandidates,extracomputationalcostisrequired. Inour
experiments, to trade-off between the learning performance and running time, we update Œ± every Œ∫ 1 iteration. In
‚â•
addition,duringthefirstM 1iterations,i.e.,k < M,asthereareonlyk < M historicalpolicies,wesetŒ± = 1 for
‚àí œÑ k
0 œÑ k 1. Inotherwords,MCwillstarttoupdateŒ±onlyafterM iterations. Algorithm2inthemaintextisthe
‚â§ ‚â§ ‚àí
simplifiedversionwhichshowstheprimaryprincipleofCMD.InAlgorithm4,wepresentthefulldetailsofCMD.
Algorithm4ConfigurableMirrorDescent(CMD)
1: Given ,œà,initial(joint)policyœÄ 1,M,D,œµ,Œπ,
L
2: fork =1, ,K do
¬∑¬∑¬∑
3: ifk M then
‚â§
4: Œ± œÑ = k1, ‚àÄ0 ‚â§œÑ ‚â§k ‚àí1
5: else
6: ifk%Œ∫=0then
7: SampleDcandidates Œ±j D
{ }j=1
8: Derivenewjointpolicies œÄj =GMD(Œ±j) D
{ }j=1
9: Evaluatenewjointpolicies (œÄj) D
{L }j=1
10: UpdateŒ±basedon (œÄj) D
{L }j=1
11: endif
12: endif
13: ComputeœÄ k+1viaGMDwiththeupdatedŒ±
14: endfor
22ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
E. GAMEBENCH
In this section, we present the details of GAMEBENCH (see Figure 3 for an overview). In Section E.1, we discuss the
motivation and desiderata by briefly reviewing the games that have been employed to test existing MD algorithms. In
SectionE.2,wepresentthedetailsoftheconstructionofall15games. Finally,inSectionE.3,wepresenttheevaluation
measuresconsideredinthiswork.
E.1.MotivationandDesiderata
AsmentionedinSection6,existingbenchmarksfordecisionmakingaretypicallyspecializedforsomespecificcategories.
Furthermore,runningMDalgorithmsonthesebenchmarkscouldbecomputationallyprohibitiveasthenumberofdecision
pointsintheenvironmentscouldbeextremelylarge. Ontheotherhand,thoughMDalgorithmshavebeendemonstrated
powerfulinsingle-agentRL(Tomaretal.,2022)andtwo-playerzero-sumgames(Wibisonoetal.,2022;Kozunoetal.,
2021;Leeetal.,2021;Liuetal.,2022b;Jainetal.,2022;Aoetal.,2023;Liuetal.,2023;Cenetal.,2023;Sokotaetal.,
2023)inrecentworks,theirexperimentsaretypicallyconductedonahandfulofgames. Itremainselusivehowwillthese
MDalgorithmsperformwhenappliedtoothercategoriesofdecision-makingproblems. InTable5,webrieflyreviewthe
gamesthathavebeenusedinsomerecentworks.
Table5.ThegamesthathavebeenusedinrecentworksonMDalgorithms.Notethatthislistdoesnotincludethegamesthatareused
tobenchmarkdeeplearning-basedalgorithmsinthesereferences. 1Thisgameismadetobeageneral-sumgameviaatie-breaking
mechanisminthisreference.2Thisgameismadetobeazero-sumgameinthisreference.
Reference Game Category
KuhnPoker Two-PlayerZero-Sum
LeducPoker Two-PlayerZero-Sum
(Sokotaetal.,2023)
2x2AbruptDarkHex Two-PlayerZero-Sum
4-SidedLiar‚ÄôsDice Two-PlayerZero-Sum
KuhnPoker Two-PlayerZero-Sum
(Liuetal.,2023)
LeducPoker Two-PlayerZero-Sum
KuhnPoker Two-PlayerZero-Sum
(Anagnostidesetal.,2022b)
LeducPoker Two-PlayerZero-Sum
Sheriff Two-PlayerGeneral-Sum
Battleship Two-PlayerGeneral-Sum
(Anagnostidesetal.,2022a)
Goofspiel1 Two-PlayerGeneral-Sum
Liar‚ÄôsDice Two-PlayerZero-Sum
KuhnPoker Two-PlayerZero-Sum
(Leeetal.,2021) LeducPoker Two-PlayerZero-Sum
Pursuit-Evasion Two-PlayerZero-Sum
LeducPoker Two-PlayerZero-Sum
Goofspiel Two-PlayerZero-Sum
(Liuetal.,2022b)
Liar‚ÄôsDice Two-PlayerZero-Sum
Battleship2 Two-PlayerZero-Sum
Inviewoftheabovefacts,weaimtoconstructanovelbenchmarkwhichshouldsatisfytwodesiderata(D5andD6presented
intheIntroduction): i)itshouldcoverallcategoriesofdecisionmaking(comprehensive),andii)thegamesarerelatively
simpleandrunningMDalgorithmsonthesegamesdoesnotrequiremuchcomputationalresource(academic-friendly).
E.2.Games
Inthissection,wepresentthedetailsoftheconstructionofall15gamesinourGAMEBENCH. Allthegamesaredividedinto
5categories: single-agent,cooperativemulti-agent,competitivemulti-agentzero-sum(zero-sum),competitivemulti-agent
general-sum(general-sum),andmixedcooperativeandcompetitive(MCC)categories. InTable6,wegiveanoverview
23ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
ofallthegames. WecuratetheGAMEBENCHontopofOpenSpiel(Lanctotetal.,2019). Forcooperative,zero-sum,and
general-sumcategories,weconstructthegamebypassingtheconfigurationstothegamesimplementedinOpenSpiel. The
configurationsforthesegamesaredeliberatelyselectedsuchthattheinstancesofthesegamesareacademic-friendly(i.e.,
theirnumbersofdecisionpointsarenottoolarge). Forsingle-agentandMCCcategories,weobtainthegamesbymodifying
theoriginalgamesinOpenSpiel. Inthefollowing,wepresentthedetailsofeachcategory.
Table6.ThegamesandtheirstatisticsinGAMEBENCH.N isthenumberofplayersand‚Äú#DP‚Äùstandsforthenumberofdecisionpoints.
Category NameofGamew/Config. Shorthand N #DP EvaluationMeasure
single_agent_kuhn_a Kuhn-A 1 6 OptGap
Single-Agent single_agent_kuhn_b Kuhn-B 1 6 OptGap
single_agent_goofspiel Goofspiel-S 1 8 OptGap
tiny_hanabi_game_a TinyHanabi-A 2 8 OptGap
Cooperative tiny_hanabi_game_b TinyHanabi-B 2 6 OptGap
tiny_hanabi_game_c TinyHanabi-C 2 6 OptGap
kuhn_poker(players=3) Kuhn 3 48 NashConv,CCEGap
Zero-Sum leduc_poker(players=2) Leduc 2 936 NashConv
goofspiel(players=3) Goofspiel 3 30 NashConv,CCEGap
bargaining(max_turns=2) Bargaining 2 178 NashConv,SW
General-Sum trade_comm(num_items=2) TradeComm 2 22 NashConv,SW
battleship Battleship 2 210 NashConv,SW
mix_kuhn_3p_game_a MCCKuhn-A 3 48 NashConv
MCC mix_kuhn_3p_game_b MCCKuhn-B 3 48 NashConv
mix_goofspiel_3p MCCGoofspiel 3 30 NashConv
Single-Agent. Weconstructthreesingle-agentgames: Kuhn-A,Kuhn-B,andGoofspiel-S,fromtheoriginaltwo-player
KuhnpokerandGoofspielinOpenSpiel. Consideratwo-playerKuhnpokergame. Toobtainasingle-agentcounterpart,we
fixoneplayer‚Äôspolicyastheuniformpolicy(calledthebackgroundplayer)whileonlyupdatingtheotherplayer‚Äôspolicy
(calledthefocalplayer)ateachiteration. InKuhn-A,player1isselectedasthefocalplayerwhileinKuhn-B,player2is
chosenasthefocalplayer,asthetwoplayersareasymmetric(Kuhn,1950). Similarly,wecangetGoofspiel-S.Asthetwo
playersaresymmetricinGoofspiel(Ross,1971),wechooseplayer1asthefocalplayerwithoutlossofgenerality.
Cooperative. For cooperative games, we consider the following three two-player tiny Hanabi games (Foerster et al.,
2019;Sokotaetal.,2021): TinyHanabi-A,TinyHanabi-B,andTinyHanabi-C.Thepayoffmatricesalongwiththeoptimal
valuesofthesegamesaregiveninFigure9. ThesegamesareeasytoobtaininOpenSpielbysettingthethreeparameters:
num_chance,num_actions,andpayoff. Fornum_chance,theyare2,2,and2,respectively. Fornum_actions,
theyare3,2,and2,respectively.
CompetitiveZero-SumandGeneral-Sum. Weconsiderthefollowingthreezero-sumgames: three-playerKuhn,two-
playerLeduc,andthree-playerGoofspiel,andthefollowingthreegeneral-sumgames: two-playerBattleship(Farinaetal.,
2020),two-playerTradeComm(Sokotaetal.,2021),andtwo-playerBargaining(Lewisetal.,2017),whichareimplemented
inOpenSpiel. TheconfigurationsofthesegamesaregiveninthesecondcolumninTable6. Notethatincontrasttomostof
theexistingworkswhichonlyfocusontwo-playergames,wesetthenumberofplayerstomorethantwoplayersinsomeof
thegames: KuhnandGoofspielarethree-playergames.
Mixed Cooperative and Competitive (MCC). We construct the following three-player MCC games: MCCKuhn-A,
MCCKuhn-B,andMCCGoofspiel,fromtheoriginalthree-playerKuhnpokerandthree-playerGoofspielinOpenSpiel.
Considerathree-playerKuhnpokergame. ToobtainanMCCcounterpart,wepartitionthethreeplayersintotwoteams:
Team1includestwoplayerswhileTeam2onlyconsistsofoneplayer(i.e.,twovs. one). Whencomputingtherewardsof
theplayers,inTeam1,eachplayerwillgettheaveragerewardoftheteam. Precisely,letrteam =r1+r2denotetheteam
rewardwhichisthesumoftheoriginalrewardsofthetwoteammembers. Then,thetruerewardsofthetwoplayersare
rÀú1 =rÀú2 =rteam/2. InMCCKuhn-A,Team1includesplayers1and2(i.e.,{1,2}vs. 3),whileinMCCKuhn-B,Team1
includesplayers1and3(i.e.,{1,3}vs. 2). Similarly,wecangetMCCGoofspielinthesamemanner.
24ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
TinyHanabi-A TinyHanabi-B TinyHanabi-C
Opt: 10 Opt: 1 Opt: 2.5
10 0 0 0 0 10
)ts
r 4 8 4 4 8 4
if
s
tc 10 0 0 0 0 10 1 0 0 1 3 0 3 1
a
(
1
r 0 0 10 10 0 0
1 0 0 0 1 3 3 0
e
y
a
lP 4 8 4 4 8 4 0 1 1 0 3 2 0 1
0 0 0 10 0 0 0 0 1 0 0 2 0 0
Player 2 (acts second)
Figure9. PayoffmatricesandoptimalvaluesofthethreetinyHanabigames.
AsshowninTable6,thenumberofdecisionpoints(#DP)variesacrossdifferentcategories,whichshowsthatGAMEBENCH
includesdiverseenoughenvironmentsas,tosomeextent,thenumberofdecisionpointsreflectsthedifficultyofthegame.
E.3.EvaluationMeasures
AsshowninFigure3,weconsidermultipleevaluationmeasuresinGAMEBENCH. Therearetwotypesofmeasures: i)
thenotionofoptimality,includingOptGapandsocialwelfare,andii)thenotionofequilibrium,includingNashConvand
CCEGap. InthelastcolumnofTable6,wepresentthemeasuresemployedineachgame. Insingle-agentandcooperative
categories,weuseOptGapasthemeasurewhichcapturesthedistanceofthecurrent(joint)policytotheoptimal(joint)
policy. Intheotherthreecategories,theprimarymeasureisNashConvwhichcapturesthedistanceofthecurrentjoint
policytotheNashequilibrium. Inaddition,wealsoconsiderothersolutionconceptsandevaluationmeasuresinsomeofthe
games. Forzero-sumKuhnandGoofspiel,astherearethreeplayers,wealsoconsiderthemeasureCCEGapwhichcaptures
thedistanceofthecurrentjointpolicytothecoarsecorrelatedequilibrium(CCE).Forgeneral-sumgames,wealsoconsider
thesocialwelfare(SW)ofalltheagents.
ExceptfortheMCCcategory,allthemeasurescanbeeasilycomputedbyusingthebuilt-inimplementationfunctionsin
OpenSpiel. However,tocomputetheNashConvintheMCCgames,weneedtocomputethebestresponsepolicyofthe
team,i.e.,ajointpolicyoftheteammembers,ratherthanthepolicyofasingleagent. Thisisincompatiblewiththebuilt-in
implementationinOpenSpiel,whichonlycomputesthebestresponsepolicyofasingleagent. Inotherwords,ifwedirectly
adoptthebuilt-inimplementation,theNashConvwillcorrespondtotheoriginalthree-playergame,notthemodifiedgame.
Unfortunately,computingtheexactjointpolicyoftheteammembersisnoteasyinpractice. Nevertheless,itisworthnoting
thatfromourexperiments,wefoundthatMMD-KLcaneffectivelysolvecooperativedecision-makingproblems. Asaresult,
wecanapplyMMD-KLtocomputetheapproximatebestresponseoftheteamasitisapurelycooperativeenvironmentfrom
theteam‚Äôsperspective(theotherteam‚Äôspolicyisfixedwhencomputingthebestresponseoftheteam). Forateamthatonly
hasasingleplayer,weusethebuilt-inimplementationinOpenSpieltocomputetheexactbestresponsepolicyoftheplayer.
Insummary,duringthepolicylearningprocess,whentheevaluationofthecurrentjointpolicyisneeded,weuseMMD-KL
asasubroutinetocomputeateam‚Äôsapproximatebestresponsewhileusingbuilt-inimplementationtocomputeasingle
player‚Äôsexactbestresponse. IntheMMD-KLsubroutine,thestartingpointofthebestresponseissettothecurrentjoint
policyoftheteammembers. Inexperiments,tobalancetheaccuracyoftheapproximatebestresponseandrunningtime,the
numberofupdatesintheMMD-KLsubroutineissetto100(thereturnedjointpolicycanbealsocalledabetterresponse).
Forexample,inMCCKuhn-A,supposethecurrentjointpolicyisœÄ =œÄ œÄ whereœÄ =œÄ œÄ istheteam‚Äôsjoint
team 3 team 1 2
√ó ‚äô
policy. Thebuilt-inimplementationinOpenSpielcanonlycomputethebestresponsepolicyforeverysingleagentand
hence,theresultingNashConv(œÄ)= 3 [V (ŒΩ,œÄBR œÄ ) V (ŒΩ,œÄ)]correspondstotheoriginalthree-playergame. In
i=1 i i √ó ‚àíi ‚àí i
contrast,inourmethod,weuseMMD-KLtocomputetheteam‚Äôsbestresponseratherthanthesingleagent‚Äôs. Therefore,the
(cid:80)
NashConvofœÄis:
NashConv(œÄ)=V (ŒΩ,œÄBR œÄ ) V (ŒΩ,œÄ)
team team√ó 3 ‚àí team (27)
+V (ŒΩ,œÄ œÄBR) V (ŒΩ,œÄ),
3 team √ó 3 ‚àí 3
whereœÄBR istheteam‚ÄôsBRpolicycomputedviaMMD-KLgiventhatplayer3isfixedtoœÄ (thatis,player3isapartof
team 3
theenvironmentfromtheteam‚Äôsperspective). Asplayers1and2arefullycooperative,theysharethesamevalueV .
team
25ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.MoreExperimentalResults
Inthissection,weprovidemoreexperimentaldetails,results,andanalysis. Webrieflysummarizeeachsectionbelow.
‚Ä¢ SectionF.1. Moredetailsontheexperimentalsetup,includingthehyper-parametersettingsfordifferentmethods.
‚Ä¢ SectionF.2. SearchingofM (thenumberofpreviouspolicies)and¬µ(thesmoothingparameterinDRS)(D1andD2).
‚Ä¢ SectionF.3. Investigationofperformancew.r.t. thenumberofjointactions(D1andD2).
‚Ä¢ SectionF.4. InvestigationofGMDwithdifferentheuristicstrategiesforadjustingŒ±(D1andD2).
‚Ä¢ SectionF.5. Investigationofdifferentmeta-controllers(D1andD2).
‚Ä¢ SectionF.6. InvestigationofdifferentBregmandivergences(D1andD2).
‚Ä¢ SectionF.7. Investigationoftheeffectivenessofaddingthemagnetpolicy(D1andD2).
‚Ä¢ SectionF.8. Investigationofdifferentevaluationmeasuresanddifferentsolutionconcepts(D3andD4).
‚Ä¢ SectionF.9. AnalysisofthecomputationalcomplexityforrunningdifferentalgorithmsonGAMEBENCH(D5andD6).
F.1.ExperimentalSetup
Hyper-parameters. Table7providesthedefaultvaluesofhyper-parametersusedindifferentmethods. IntheRS-type
meta-controllers (RSand DRS),the spherically symmetric distribution q is astandard multivariatenormal distribution
N(0,I).ForCMD/GMD,therearetwocriticalhyper-parameters:thenumberofpreviouspoliciesM 1andthesmoothing
‚â•
parameter¬µinDRS.InSectionF.2,weperformanablationstudytodeterminetheirdefaultvalues(giveninTable7),which
willbefixedinotherexperiments. Thespecificsetupsforeachexperimentwillbegivenineachofthefollowingsections.
Baselines. WeconsidertheMMD-type(MMD-KLandMMD-EU)andCFR-type(CFRandCFR+)algorithmsasthe
baselines. ItisworthnotingthatCFR-typealgorithmscanbealsoappliedtosingle-agentandcooperativecategories.
ComputationalResources. Experimentsareperformedonamachinewitha24-corei9andNVIDIAA4000. ForCMD,the
resultsareobtainedwith3randomseeds. Forothermethods,asthereisnorandomness,nomultiplerunsareneeded.
Table7.Defaultvaluesofthehyper-parametersindifferentmethods.Allthehyper-parametersinGMD‚ÄìC,Œπ,andM ‚Äìarealsousedin
CMD.ForCMD,itshyper-parametersalsoincludei)D(thenumberofsamples)andŒ∫(updateinterval),whicharesharedfordifferent
MCs,ii)¬µintheDRSandRS,andiii)r Landr H intheGLD,GLDS,andDGLDS.
CMD MMD-KL/-EU
GMD Shared (D)RS (D)GLD(S)
Game K œµ C Œπ M D Œ∫ ¬µ r r Œæ Œ∑ Œ∑Àú
L H
Kuhn-A 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
Kuhn-B 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
Goofspiel-S 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
TinyHanabi-A 100000 1e-10 50 1e-6 3 5 10 0.05 0.01 0.05 1 0.1 0.05
TinyHanabi-B 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
TinyHanabi-C 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
Kuhn 100000 1e-10 50 1e-6 5 5 10 0.01 0.01 0.05 1 0.1 0.05
Leduc 100000 1e-10 50 1e-6 3 5 10 0.05 0.01 0.05 1 0.1 0.05
Goofspiel 100000 1e-10 50 1e-6 3 5 10 0.01 0.01 0.05 1 0.1 0.05
Bargaining 100000 1e-10 50 1e-6 5 5 10 0.05 0.01 0.05 1 0.1 0.05
TradeComm 100000 1e-10 50 1e-6 1 5 10 0.01 0.01 0.05 1 0.1 0.05
Battleship 100000 1e-10 50 1e-6 1 5 10 0.05 0.01 0.05 1 0.1 0.05
MCCKuhn-A 100000 1e-10 50 1e-6 1 5 10 0.01 0.01 0.05 1 0.1 0.05
MCCKuhn-B 100000 1e-10 50 1e-6 1 5 10 0.01 0.01 0.05 1 0.1 0.05
MCCGoofspiel 100000 1e-10 50 1e-6 1 5 10 0.01 0.01 0.05 1 0.1 0.05
26ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.2.NumberofHistoricalPoliciesandSmoothingParameter
Inthissection,weexploretheinfluenceofthenumberofpreviouspoliciesM andthesmoothingparameter¬µinDRSon
thelearningperformance. WeconsiderM 1,3,5 and¬µ 0.01,0.05 andthus,thereare6combinationsof(M,¬µ).
‚àà{ } ‚àà{ }
NotethatitwouldbeimpracticaltoenumerateallthecombinationsasM canbeanyintegergreaterthan0and¬µcanbeany
realnumbergreaterthan0.
TheexperimentalresultsareshowninFigure10. Fromtheresults,wecanseethatdifferentdecision-makingproblemsmay
requiredifferentM and¬µ. Notably,M =1,i.e.,onlyconsideringthecurrentpolicywhenderivingthenewpolicywhich
iscommoninexistingMDalgorithms,isnotalwaystheoptimalchoiceacrossdifferentdecision-makingproblems. For
example,inthemostdifficultLeducpokergame,whenM =1,CMDcannotdecreasetheNashConv,meaningthatonly
consideringthecurrentpolicyisineffectiveinsolvingthisgame. Bycomparison,wedeterminethedefaultvaluesofM and
¬µfordifferentgames,whicharegiveninTable7andwillbefixedinotherexperiments.
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
                   
                   
                   
                   
                   
                   
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                   
                   
                   
                    
      
          
      
          
      
          
   
           
      
          
                                                           
Figure10.Experimentalresultsforthecombinationsof(M,¬µ).Thefirst6figurescorrespondtosingle-agentandcooperativecategories
wherethey-axisisOptGap.Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv.Forallthefigures,thex-axisis
thenumberofiterations.
27ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.3.Performancew.r.t. theNumberofJointActions
InFigure11,weplottheperformanceofdifferentmethodswithrespecttothenumberofjointactionsinvolvedineach
iteration. AsbothMD-typeandCFR-typealgorithmswilltraversethewholegametree,thenumberofjointactionsfor
agivenjointpolicyisthesame. Therefore,inthefigure,weonlyneedtochangethescaleofthex-axisfortheCMDby
multiplyingtheconstantD (thenumberofjointpoliciesevaluatedateachiteration), whilekeepingthescalesofother
methodsunchanged. WenotethatasDissmallinourexperiments(D = 5,i.e.,sample5candidatejointpolicies),the
conclusionsintermsofthenumberofiterationspresentedinthemaintextstillholdintermsofthenumberofjointactions.
AsdiscussedinSection8,oneofthefuturedirectionsofourworkwouldbethedevelopmentofamoreefficientmethodfor
updatingtheŒ±,e.g.,amethodthatonlyneedstosampleonecandidate(inthiscase,thenumberofjointactionswillbe
thesameforbothCMDandotherbaselines). Nevertheless,comparedtobaselineMDandCFR-typealgorithms,CMD
providesafeasiblewaytostudydifferentsolutionconceptsandevaluationmeasures, though, inthecurrentversion, it
requiresevaluatingmultiplecandidatesateachiteration.
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
       
   
       
               
                   
                   
               
       
   
       
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                                                     
                   
                       
      
          
      
          
      
          
   
           
      
          
 & 0 '  * 0 '  0 0 '  . /  0 0 '  ( 8  & ) 5  & ) 5 
Figure11.Performance of different methods w.r.t. the number of joint actions. The first 6 figures correspond to single-agent and
cooperativecategorieswherethey-axisisOptGap.Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv.Forall
thefigures,thex-axisisthenumberofiterations.
28ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.4.DifferentHeuristicStrategiesforAdjustingŒ±inGMD
Inthemaintext,thebaselinemethodGMDemploysafixedstrategy‚Äìauniformdistribution‚ÄìtodeterminethevalueofŒ±.
Inthissection,weconsidertwomoreheuristicstrategies: i)‚ÄúGMD(LD)‚ÄùdenotesthattheŒ±islinearlydecayedwiththe
iteration,andii)‚ÄúGMD(ISR)‚ÄùdenotesthattheŒ±isdecayedwiththeiterationintheformofinversesquarerootfunction
Œ± = 1 ,wherek isthek-thiteration. TheresultsareshowninFigure12. Fromtheresults,wecanseethatdifferent
œÑ ‚àök
heuristicstrategiescanperformdifferentlyindifferentdecision-makingscenarios;onecanbeatothersinsomescenarios
whileitcanalsobebeatenbyothersinotherscenarios.
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
                   
       
   
       
                                       
       
   
       
                   
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                                                     
                   
                       
      
          
      
          
      
          
   
           
      
          
 & 0 '  * 0 '  * 0 '  / '   * 0 '  , 6 5   0 0 '  . /  0 0 '  ( 8  & ) 5  & ) 5 
Figure12.ExperimentalresultsforGMDwithdifferentheuristicstrategiesforadjustingŒ±.Thefirst6figurescorrespondtosingle-agent
andcooperativecategorieswherethey-axisisOptGap.Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv.For
allthefigures,thex-axisisthenumberofiterations.
29ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.5.DifferentMeta-Controllers
Inthissection,weinvestigatetheeffectivenessofdifferentMCs,andtheresultsareshowninFigure13. Fromtheresults,we
canseethatDRScanconsistentlyoutperformalltheotherbaselineMCsacrossalmostallofthedecision-makingproblems.
Particularly,inLeducandMCCKuhn-B,DRSachievesasignificantlybetterconvergentperformancethanotherbaseline
MCs. AlthoughinMCCKuhn-A,GLDfinallyconvergestoalowerNashConvthanDRS,itcanperformmuchworsein
othergames,e.g.,inBattleship,GLDcannotdecreasetheNashConv,inLeducandMCCKuhn-B,itonlyconvergestoa
highvalueofNashConv. Inotherwords,GLDcannotconsistentlyworkwellacrossallthedecision-makingcategories. In
addition,inmostofthegames,theRS-typeMCstypicallyperformbetterthantheGLD-typeMCs. Wehypothesizethatthe
RS-typeMCsaremoreefficientinexploringtheparameterspaceastheyusemoresamples(Œ±j andŒ±j foreachuj)to
+
obtainthefinalupdateforthehyper-parameters. ‚àí
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
                   
       
   
       
                                       
       
   
       
                   
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                                                     
                   
                       
      
          
      
          
      
          
   
           
      
          
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '  & ) 5  & ) 5 
Figure13.ExperimentalresultsfordifferentMCs.Thefirst6figurescorrespondtosingle-agentandcooperativecategorieswherethe
y-axisisOptGap. Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv. Forallthefigures,thex-axisisthe
numberofiterations.
EvolutionofHyper-Parameters. ThecriticalobservationthatsupportsourproposedDRSisthatthevalueoftheevaluation
measure isextremelysmallandconvergesrelativelyquickly,whichmakestheoriginalzero-ordermethodsstrugglein
L
adjustingthehyper-parametersastheytypicallyadjustthehyper-parametersdirectlybasedontheperformance. Tofurther
verify this intuition, we visualize the evolution of the hyper-parameter Œ± over the learning process, which is shown in
Figure14‚ÄìFigure18(respectivelycorrespondstosingle-agent,cooperative,zero-sum,general-sum,andMCCcategories).
Weuseindex0torepresentthemagnetpolicyandtherecentM historicalpoliciesareindexedby 1, ,M . Fromthe
{ ¬∑¬∑¬∑ }
results,wecanseethatinallthegamesexceptLeduc,thevalueofŒ±determinedbyRSalmostdoesnotchangeoverthe
learningprocess(thesamephenomenonisobservedforGLDSasitfollowsthesameideaofRS).InLeduc,thisvaluetends
todecreaseto0overthelearningprocess. Inotherwords,theregularizationisvanishing,whichexplainswhyRSandGLDS
cannotdecreasetheNashConvinthisgameasaddingregularizationhasbeenprovenimportanttosolvetwo-playerzero-sum
games(Sokotaetal.,2023;Liuetal.,2023). Inallthegames,DRSandDGLDSsharesomesimilaritiesindeterminingthe
valueofŒ±anddifferfromGLD.Nevertheless,theconvergenceresultsinFigure13showthatDRSisthebestchoiceamong
themasitcanconsistentlyworkwellacrossallcategoriesofdecision-makingproblems.
30ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 . X K Q  $
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 . X K Q  %
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 * R R I V S L H O  6
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
Figure14.Theevolutionofthehyper-parametervaluesofdifferentMCsintheSingle-Agentcategory.They-axisisthevalueofŒ±.The
x-axisisthenumberofiterations.
 7 L Q \ + D Q D E L  $
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                   
         
                   
                   
                   
                                                           
 7 L Q \ + D Q D E L  %
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 7 L Q \ + D Q D E L  &
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
Figure15.Theevolutionofthehyper-parametervaluesofdifferentMCsintheCooperativecategory.They-axisisthevalueofŒ±.The
x-axisisthenumberofiterations.
31ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 . X K Q
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                   
         
                             
         
                   
                   
                                                           
 / H G X F
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                   
         
                   
                   
                   
                                                           
 * R R I V S L H O
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                   
         
                   
                   
                   
                                                           
Figure16.Theevolutionofthehyper-parametervaluesofdifferentMCsintheZero-Sumcategory.They-axisisthevalueofŒ±.The
x-axisisthenumberofiterations.
 % D U J D L Q L Q J
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                   
         
                             
         
                   
                   
                                                           
 7 U D G H & R P P
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 % D W W O H V K L S
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
Figure17.Theevolutionofthehyper-parametervaluesofdifferentMCsintheGeneral-Sumcategory.They-axisisthevalueofŒ±.The
x-axisisthenumberofiterations.
32ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 0 & & . X K Q  $
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 0 & & . X K Q  %
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
 0 & & * R R I V S L H O
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
                   
                   
                   
                   
                                                           
Figure18.Theevolutionofthehyper-parametervaluesofdifferentMCsintheMCCcategory.They-axisisthevalueofŒ±.Thex-axisis
thenumberofiterations.
33ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.6.DifferentBregmanDivergences
OneoftheprominentfeaturesofourCMD(GMD)isthatitiscapableofexploringmorepossibleBregmandivergences.
Inthissection,weinvestigatehowCMDperformsunderthedifferentBregmandivergencesinducedbydifferentconvex
functionsinTable2(theplotsfortheseconvexfunctionsareshowninFigure8).
TheexperimentalresultsareshowninFigure19. Fromtheresults,wecanseethattheentropyfunctionxlnxisstillagood
choiceacrossallthegames. Nevertheless,insomegames,thereexistotherconvexfunctionsthatarebetterchoices. For
example,inKuhn-A,TinyHanabi-B,TinyHanabi-C,Goofspiel,TradeComm,andMCCGoofspiel,x2isbetterthanxlnx.
Furthermore, inMCCGoofspiel, ex isalsobetterthanxlnx, whichverifiesthattheKLdivergence(xlnx)orsquared
Euclideannorm(x2)couldbenotalwaysthebestchoiceacrossdifferentgames. Ontheotherhand,evenundertheentropy
functionxlnx,ourCMDcouldalsooutperformMMD-KLinsomegamessuchasMCCKuhn-AandMCCKuhn-B.
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                               
                     
                     
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
                   
       
   
       
                                       
        
   
        
                     
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
                   
                                               
                     
                      
      
          
      
          
      
          
   
           
      
          
xlnx x2 x0.1 ex  0 0 '  . /  0 0 '  ( 8  & ) 5  & ) 5 
Figure19.ExperimentalresultsfordifferentBregmandivergences. Thefirst6figurescorrespondtothesingle-agentandcooperative
categorieswherethey-axisisOptGap.Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv.Forallthefigures,
thex-axisisthenumberofiterations.
34ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.7.EffectivenessofMagnet
AsmentionedinSectionD.2,inourexperiments,wheninstantiatingCMD,webydefaultaddamagnetpolicy(theinitial
policy)intothepolicyupdatingasithasbeendemonstratedthataddingamagnetpolicyispowerfulinsolvingtwo-player
zero-sumgames(Sokotaetal.,2023;Liuetal.,2023). Toverifythis,weconductanablationstudywhere‚ÄúCMDw/oMag‚Äù
denotesthemethodthatonlyconsidersthemostrecentM historicalpolicieswithoutaddingtheinitialpolicy.
TheexperimentalresultsareshowninFigure20. Fromtheresults,wecanseethati)Forsingle-agentandcooperative
categories,addingthemagnetpolicycouldresultinaslightlyslowerconvergencerate;wehypothesizethatthismaybedue
tothefactthatthesingle-agentandcooperativegamesarerelativelysimplerthantheothergames(asshowninTable6,
thenumbersofdecisionpointsofsingle-agentandcooperativegamesaresmallerthantheothergames). ii)Fortheother
threecategories,addingthemagnetpolicyisnecessaryforCMDtoworkconsistentlywellacrossallthegames;thoughin
MCCGoofspiel,CMDfinallyconvergestoalowerNashConvwithoutthemagnet,itcouldperformworseinsomeother
games,e.g.,inBattleship,itfinallydivergeswithoutthemagnet,andinLeduc,MCCKuhn-A,andMCCKuhn-B,itconverges
toahighNashConvwithoutthemagnet. Nevertheless,aspointedoutinSectionD.2,thisdefaultinstanceofCMD(GMD)
shouldnotbeconfusedwiththeoriginalMMDevenwhenM =1asthepolicyupdatingruleisderivedviaanumerical
method,insteadofrelyingontheclosed-formsolution(Sokotaetal.,2023).
 . X K Q  $  . X K Q  %  * R R I V S L H O  6  7 L Q \ + D Q D E L  $  7 L Q \ + D Q D E L  %
                   
                   
                   
                                                  
                   
                    
      
          
      
          
      
          
      
          
      
          
 7 L Q \ + D Q D E L  &  . X K Q  / H G X F  * R R I V S L H O  % D U J D L Q L Q J
                   
       
   
       
               
                   
                   
               
       
   
       
      
          
      
          
   
           
      
          
   
           
 7 U D G H & R P P  % D W W O H V K L S  0 & & . X K Q  $  0 & & . X K Q  %  0 & & * R R I V S L H O
                   
                   
   
   
       
   
                                  
   
   
       
   
                      
      
          
      
          
      
          
   
           
      
          
 & 0 '  & 0 '  Z  R  0 D J  0 0 '  . /  0 0 '  ( 8  & ) 5  & ) 5 
Figure20.Experimentalresultsfortheeffectivenessofaddingthemagnetpolicy. Thefirst6figurescorrespondtosingle-agentand
cooperativecategorieswherethey-axisisOptGap.Therestfigurescorrespondtoothercategorieswherethey-axisisNashConv.Forall
thefigures,thex-axisisthenumberofiterations.
35ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.8.DifferentMeasures
Inthissection,weapplyourCMDtodifferentsolutionconceptsandevaluationmeasures(i.e.,thedesiderataD3andD4
presentedintheIntroduction). NotethatwhenrunningCMDfordifferentevaluationmeasures,onlyminimalmodifications
arerequired: changingtheMC‚Äôsoptimizationobjective . WefirstinvestigatetheCCEGap(Moulin&Vial,1978;Marris
L
etal.,2021)inSectionF.8.1andthenthesocialwelfare(Davis&Whinston,1962)inSectionF.8.2.
F.8.1.CCEGAP
Notethatintwo-playerzero-sumgames,NEandCCEcanbeshowntobepayoffequivalent(v.Neumann,1928). Therefore,
weconductexperimentsonthethree-playerKuhnandGoofspiel. WefollowthesameexperimentalpipelineofOptGapand
NashConv: i)investigatingthecombinationofM and¬µ,ii)investigatingdifferentMCs,iii)investigatingdifferentBregman
divergences,andiv)investigatingtheeffectivenessofmagnet.
NumberofHistoricalPoliciesandSmoothingParameter. Wefirstinvestigatetheinfluenceofthenumberofprevious
policiesM andthesmoothingparameter¬µinDRSonthelearningperformance. SimilartoOptGap/NashConv,weconsider
M 1,3,5 and¬µ 0.01,0.05 ,andtheresultsareshowninFigure21. Wecangetthesameconclusion: different
‚àà { } ‚àà { }
gamesmayrequiredifferentM and¬µ. Bycomparison,wedeterminetheirdefaultvalueswhichwillbefixedintheother
experiments: (M,¬µ) = (3,0.01)forbothKuhnandGoofspiel. Alltheotherhyper-parametersettingsarethesameas
OptGap/NashConvgiveninTable7.
 . X K Q  * R R I V S L H O
       
                           
                   
       
                   
                             
                             
                             
         
                                                             
 , W H U D W L R Q  , W H U D W L R Q
Figure21. Experimentalresultsforthecombinationsof(M,¬µ)underthemeasureCCEGap.
DifferentMeta-Controllers. Then,weinvestigatetheeffectivenessofdifferentMCs. InFigure22,wepresentthelearning
curvesoftheperformanceofdifferentMCs,andinFigure23,wepresenttheevolutionofthevalueofŒ±determinedby
differentMCsoverthelearningprocess. WecangetthesameconclusionasOptGap/NashConv: DRSisthebestchoice
amongthe5MCs. FromtheevolutionofŒ±weobservethatDRSandDGLDSfollowtwodifferentpatternstodeterminethe
valueofŒ±,whichisnotthecaseforOptGap/NashConv(seeFigure14‚ÄìFigure18). Incontrast,wefoundthatsinceGLD
followsasimilarpatterntoDRS,itperformsonparwithorbetterthanDGLDS.
 . X K Q  * R R I V S L H O
       
     '  ' 5  * 6
 / ' 6
 5  * 6
 / ' 6
 *  & ) /  5 '  & ) 5       '  ' 5  * 6
 / ' 6
 5  * 6
 / ' 6
 *  & ) /  5 '  & ) 5 
       
       
       
       
       
      
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q
Figure22. Experimentalresultsfordifferentmeta-controllersunderthemeasureCCEGap.
36
 S D * ( & &
 S D * ( & &
 S D * ( & &
 S D * ( & &ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
 . X K Q
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                             
                   
                   
                                                                               
 * R R I V S L H O
 ' 5 6  ' * / ' 6  5 6  * / ' 6  * / '
                   
         
                             
         
                             
                   
                   
                                                                               
Figure23.Evolutionofthehyper-parametersofdifferentMCs.They-axisisthevalueofŒ±.Thex-axisisthenumberofiterations.
DifferentBregmanDivergences. Next,weinvestigatehowCMDperformsunderdifferentBregmandivergencesinduced
bydifferentconvexfunctionsinTable2,andtheresultsaregiveninFigure24. Wecangetthesameconclusionsasfor
OptGap/NashConv: i)theentropyfunctionœà(x)=xlnxisstillagoodchoiceindifferentgames,ii)therecouldexistother
convexfunctionsthatarebetterthantheentropyfunction,e.g.,x2andexinGoofspiel,iii)evenundertheentropyfunction
œà(x)=xlnx,ourCMDcanconvergefasterthantheSOTAMMD-KLintermsofthenumberofiterations.
 . X K Q  * R R I V S L H O
       
    x xl 2nx exx0.1  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5
 5 
    x xl 2nx exx0.1  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5
 5 
       
   
   
   
   
   
       
       
      
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q
Figure24. ExperimentalresultsfordifferentBregmandivergencesunderthemeasureCCEGap.
EffectivenessofMagnet. Finally,weinvestigatetheeffectivenessofaddingthemagnetpolicytothepolicyupdating,and
theresultsarepresentedinFigure25. WecanobserveasimilarphenomenontoNashConv: inKuhn,CMDconverges
remarkablyfaster(intermsofthenumberofiterations)thanalltheothermethods,andinGoofspiel,itconvergesremarkably
faster(intermsofthenumberofiterations)thanalltheothermethodsexceptthe‚ÄúCMDw/oMag‚Äù. Nevertheless,wecan
stillconcludethataddingthemagnetpolicyisnecessaryforourCMD.
 . X K Q  * R R I V S L H O
       
          &  & 0  0 '  '  Z  R  0 D J  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5               &  & 0  0 '  '  Z  R  0 D J  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5 
       
       
            
        
      
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q
Figure25. ExperimentalresultsfortheeffectivenessofthemagnetpolicyunderthemeasureCCEGap.
37
 S D * ( & &
 S D * ( & &
 S D * ( & &
 S D * ( & &ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.8.2.SOCIALWELFARE
Inthissection,weapplyourmethodstotheevaluationmeasure‚Äìsocialwelfare(Davis&Whinston,1962). Weconduct
experimentsonthegeneral-sumgames,andtheresultsareshowninFigure26. Inthisexperiment,weusethedefaultvalues
inTable7forthehyper-parameters.
Fromthetoplineofthefigure,wecanseethatCMD/GMDcanempiricallyachievecompetitiveorbettersocialwelfare
comparedtootherbaselines,demonstratingtheeffectivenessofourmethodwhenconsideringdifferentmeasures.
Inthemedianlineofthefigure,weplottheNashConvwhentheMC‚Äôsobjective issocialwelfare. Wecanseethatin
L
BargainingandTradeComm,thelearningstillcanconvergetotheapproximateNEeventhoughtheMC‚Äôsobjectiveissocial
welfare,nottheNashConv. InBattleship,whileCMDcangetahigher(average)socialwelfare,thefinaljointpolicyis
notanNEasitsNashConvcannotconverge. Inotherwords,anefficient(intermsofsocialwelfare)jointpolicycould
notnecessarilybeanNE.Thissuggestsoneofthefuturedirections: howtoefficientlylearntheNEwithmaximumsocial
welfare,whichinvolvestheequilibriumselectionproblem(Harsanyietal.,1988).
AnotherintuitiveconsequenceofsettingtheMC‚Äôsoptimizationobjectivetosocialwelfareisthatthelearningcouldnot
convergetotheNEorconvergeslowerthanthecasewheretheMC‚ÄôsobjectiveisdirectlytheNashConv. Asshowninthe
bottomlineofthefigure,weplottheNashConvofthetwocases. InBargainingandTradeComm,CMDwithNashConvas
theMC‚ÄôsobjectivecanconvergetotheNEfasterthanthatwithSWastheMC‚Äôsobjective. InBattleship,CMDwithSWas
theMC‚Äôsobjective,thoughcouldachieveahighersocialwelfare,cannotconvergetheNashequilibrium.
 % D U J D L Q L Q J  7 U D G H & R P P  % D W W O H V K L S
              
 & 0 '  0 0 '  . /  & ) 5  & 0 '  0 0 '  . /  & ) 5
           * 0 '  0 0 '  ( 8  & ) 5        * 0 '  0 0 '  ( 8  & ) 5 
         
    
         
    
         
    
         
      &  * 0  0 '  '  0  0 0  0 '  '    .  ( 8 /  &  & )  ) 5  5           
             
                                                                             
 , W H U D W L R Q  , W H U D W L R Q  , W H U D W L R Q
     % D U J D L Q L Q J      7 U D G H & R P P      % D W W O H V K L S
     & 0 '  0 0 '  . /  & ) 5      & 0 '  0 0 '  . /  & ) 5      & 0 '  0 0 '  . /  & ) 5
     * 0 '  0 0 '  ( 8  & ) 5       * 0 '  0 0 '  ( 8  & ) 5       * 0 '  0 0 '  ( 8  & ) 5 
           
                                
           
           
   
                       
      
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q  , W H U D W L R Q
     % D U J D L Q L Q J      7 U D G H & R P P      % D W W O H V K L S
     & 0 '   6 :   & 0 '   1 D V K & R Q Y       & 0 '   6 :   & 0 '   1 D V K & R Q Y       & 0 '   6 :   & 0 '   1 D V K & R Q Y 
   
   
   
       
   
   
   
   
   
   
           
   
                       
      
                      
      
                      
 , W H U D W L R Q  , W H U D W L R Q  , W H U D W L R Q
Figure26. Experimentalresultsfortheevaluationmeasure‚Äìsocialwelfare.
38
 H U D I O H :  O D L F R 6
 Y Q R & K V D 1
 Y Q R & K V D 1
 H U D I O H :  O D L F R 6
 Y Q R & K V D 1
 Y Q R & K V D 1
 H U D I O H :  O D L F R 6
 Y Q R & K V D 1
 Y Q R & K V D 1ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.9.ComputationalComplexity
Inthissection,wegivesomeremarksonthecomputationalcomplexityofdifferentmethods. InSectionF.9.1,wefocus
ontherunningtimeofdifferentmethods,andinSectionF.9.2wepresentthememoryconsumptionofdifferentmethods.
Note that i) these numbers are obtained under the default values of hyper-parameters (Table 7), and ii) these numbers
arenotabsoluteanddependonthepropertyofthegame(seeTable6)andthecomputationalresourcesusedtorunthe
experiments(seeSectionF.1);theyonlyprovideintuitiononthecomputationalcomplexityofdifferentmethodstoshow
thatourGAMEBENCHcansatisfythedesiderataD5andD6mentionedintheIntroduction.
F.9.1.RUNNINGTIME
TherunningtimeofdifferentmethodsindifferentgamesisshowninTable8. Fromtheresults,wecanseethatinmostof
thegames,runningallthealgorithmsdoesnotcauseaverylongrunningtime(thedesiderataD5andD6presentedinthe
Introduction),evenforourmethodswhereasetofextraoperationsisrequired: GMDrequirescomputingthevalueofthe
dualvariableviaanumericalmethodandCMDfurtherrequirestoevaluatemultiplecandidatesofthehyper-parameters.
Notably,weemphasizethat: i)Ourmethods(CMD/GMD)providethecapabilityofexploringmoredimensionsofdecision
making, though they require extra computational cost (the major limitation of the current version of our methods); ii)
ComparingCMDandGMD,wecanseethatthemajorcostcomesfromevaluatingmultiplesamples. Therefore,aspointed
out in Section 8, we view this as a future direction: developing more computationally efficient hyper-parameter value
updatingmethodswithoutsacrificingperformance. Inthisregard,othertechniquessuchasBayesianoptimization(Lindauer
etal.,2022)orofflinehyper-parameteroptimizationapproaches(Chenetal.,2022)mayberequired.
Table8. Therunningtimeofoneiterationofdifferentmethodsindifferentgames(second).
MMD MMD CMD
Game CFR CFR+ GMD
-KL -EU
DRS RS DGLDS GLDS GLD
Kuhn-A 0.0004 0.0004 0.0003 0.0003 0.0034 0.0372 0.0372 0.0204 0.0204 0.0203
Kuhn-B 0.0004 0.0004 0.0003 0.0003 0.0033 0.0370 0.0365 0.0202 0.0201 0.0202
Goofspiel-S 0.0007 0.0006 0.0004 0.0004 0.0046 0.0491 0.0489 0.0269 0.0275 0.0270
TinyHanabi-A 0.0006 0.0006 0.0004 0.0004 0.0045 0.0474 0.0472 0.0262 0.0268 0.0260
TinyHanabi-B 0.0004 0.0004 0.0003 0.0003 0.0033 0.0366 0.0370 0.0198 0.0192 0.0197
TinyHanabi-C 0.0004 0.0004 0.0003 0.0003 0.0032 0.0364 0.0359 0.0195 0.0195 0.0199
Kuhn 0.0084 0.0082 0.0022 0.0021 0.0267 0.4102 0.4058 0.2273 0.2282 0.2288
Leduc 0.0942 0.0961 0.0422 0.0412 0.5146 6.8897 6.9749 3.8188 3.8116 3.8744
Goofspiel 0.0072 0.0073 0.0014 0.0014 0.0167 0.2879 0.2899 0.1636 0.1610 0.1625
Bargaining 0.0279 0.0273 0.0130 0.0116 0.1093 1.5311 1.5308 0.8428 0.8579 0.8516
TradeComm 0.0028 0.0029 0.0011 0.0010 0.0121 0.1704 0.1699 0.0957 0.0942 0.0939
Battleship 0.0245 0.0248 0.0097 0.0094 0.1125 1.5570 1.5771 0.8833 0.8774 0.8835
MCCKuhn-A 0.0083 0.0084 0.0021 0.0021 0.0264 6.9054 6.8377 4.0453 4.1461 4.1034
MCCKuhn-B 0.0080 0.0083 0.0021 0.0021 0.0260 6.8100 6.7847 4.1047 4.0944 4.1104
MCCGoofspiel 0.0070 0.0073 0.0014 0.0014 0.0167 5.3541 5.3852 3.1817 3.1945 3.1836
39ConfigurableMirrorDescent:TowardsaUnificationofDecisionMaking
F.9.2.MEMORYUSAGE
ThememoryusageofdifferentmethodsindifferentgamesisprovidedinTable9. Fromtheresults,wecanseethatrunning
allthealgorithmsdoesnotcausemuchmemoryconsumption,whichshowsthatourGAMEBENCHisacademic-friendly.
Table9. Thememoryusageofdifferentmethodsindifferentgames(MB).
MMD MMD CMD
Game CFR CFR+ GMD
-KL -EU
DRS RS DGLDS GLDS GLD
Kuhn-A 0.8750 0.9805 0.3711 0.3750 0.9492 1.0352 1.0859 0.3750 0.3750 0.3750
Kuhn-B 0.8672 0.8672 0.4297 0.3750 0.7461 1.1289 1.0352 0.3750 0.3750 0.3164
Goofspiel-S 1.1875 1.2969 1.2617 1.2539 1.2578 1.2578 1.2578 1.6992 1.6953 1.6992
TinyHanabi-A 1.0352 1.0156 0.4805 0.4258 0.4883 1.0312 1.1836 0.4453 0.4297 0.4883
TinyHanabi-B 0.9922 1.0898 0.4922 0.4844 0.4336 0.4922 0.5430 0.4922 0.4297 0.4922
TinyHanabi-C 0.9805 0.9922 0.4336 0.4297 0.4336 0.4922 0.4336 0.4336 0.4336 0.4336
Kuhn 1.9961 2.0078 3.0352 3.1367 3.5586 3.7734 3.7227 3.5156 3.5352 3.5273
Leduc 26.262 26.344 51.664 52.465 58.273 59.063 58.555 52.688 51.949 51.359
Goofspiel 2.4844 2.4297 3.5039 3.4961 4.3555 4.3359 4.3047 4.0391 4.0430 4.0898
Bargaining 10.633 10.578 24.816 24.852 35.129 35.422 35.020 31.941 32.344 31.781
TradeComm 1.4961 1.5430 2.0156 2.0703 2.0664 2.0078 2.0117 2.2461 2.2461 2.3633
Battleship 6.9102 6.9023 13.539 13.422 13.543 13.543 13.484 12.098 12.148 12.332
MCCKuhn-A 2.5742 2.5195 2.0312 2.0273 2.5586 2.6719 2.7266 2.3047 2.2930 2.2930
MCCKuhn-B 2.4688 2.5703 2.0273 1.9648 1.9727 2.6562 2.7617 2.2383 2.2305 2.1797
MCCGoofspiel 2.7500 2.7500 3.0078 3.0820 3.0781 3.0195 3.1289 2.8203 2.8047 2.8672
40