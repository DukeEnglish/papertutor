A Backbone for Long-Horizon Robot Task Understanding
Xiaoshuai Chen, Wei Chen, Dongmyoung Lee, Yukun Ge,
Nicolas Rojas, and Petar Kormushev
Abstractâ€”End-to-end robot learning, particularly for long-
Step 1: Extracting backbone from Step 2: Registering context
horizontasks,oftenresultsinunpredictableoutcomesandpoor long-horizon robot task information into backbone
generalization.Toaddressthesechallenges,weproposeanovel Expert
Therblig-based Backbone Framework (TBBF) to enhance robot demonstration Mapping Use Grasp
task understanding and transferability. This framework uses
therbligs(basicactionelements)asthebackbonetodecompose
high-levelrobottasksintoelementalrobotconfigurations,which Matching Initial
arethenintegratedwithcurrentfoundationmodelstoimprove Task related
object reasoning
taskunderstanding.Theapproachconsistsoftwostages:offline
training and online testing. During the offline training stage,
Step 4: New Trajectory Generating Step 3: Adapting New scenario
we developed the Meta-RGate SynerFusion (MGSF) network
for accurate therblig segmentation across various tasks. In Com unp dle et re s tr ao nb do it n t gask Use transferred
the online testing stage, after a one-shot demonstration of Explainable
a new task is collected, our MGSF network extracts high-
Grasp
level knowledge, which is then encoded into the image using New
Action Registration (ActionREG). Additionally, Large Language
transferred
Model (LLM)-Alignment Policy for Visual Correction (LAP-VC)
is employed to ensure precise action execution, facilitating Fig.1:Illustrationoftheproposedrobottaskunderstandingsystem.
trajectory transfer in novel robot scenarios. Experimental By extracting the key backbone of complex tasks and integrating
results validate these methods, achieving 94.37% recall in contextual information from one-shot demonstrations, this system
therblig segmentation and success rates of 94.4% and 80% efficiently reasons about task-related objects and actions, and gen-
in real-world online robot testing for simple and complex erates new trajectories in novel scenarios.
scenarios, respectively. Supplementary material is available at:
https://sites.google.com/view/therbligsbasedbackbone/home
To address these challenges, we propose a structured
I. INTRODUCTION and modular backbone framework. In our context, a back-
Understandingrobottasksencompassesseveralkeystages: bone refers to a structured framework that decomposes
sensing the environment, recognizing task-related objects, high-level tasks into fundamental units, each representing
making decisions, and planning trajectories. Recently, data- a specific action or sequence of actions, facilitating better
driven methods, especially deep learning algorithms, have task understanding and transferability [7]. Complex tasks
greatly advanced the field of robotics. While deep learning involve rich contact interactions (e.g., multiple points of
excels in object recognition and reinforcement learning aids force application) and operations in dynamic or cluttered
in trajectory planning, these models often struggle to gen- environments, requiring the integration of diverse skills and
eralize beyond trained scenarios, especially in long-horizon sensormodalities(e.g.,vision,force)[8].Long-horizontasks
tasks. Thus, improving generalization is crucial for adapting require extended sequences of actions, typically involving
to diverse, dynamic, real-world situations effectively. morethan10individualsteps[9],oroveraprolongedperiod
A major challenge is data efficiency, as current systems (e.g., several minutes) to achieve a specific goal.
require large, resource-intensive datasets [1]â€“[3]. Models Many models depend heavily on pre-training in specific
trained for long-horizon tasks excel in simple tasks such scenarios[10]â€“[15],limitingtheirabilitytogeneralizetonew
as pick&place but struggle with more complex tasks like environments. Effective scenario generalization is essential
liquid pouring, which involve multiple steps like grasping, for adapting to the complexity and variability of real-world
pouring, and releasing. The narrow focus on simple tasks situations [15]. Furthermore, task generalization is crucial,
limits these modelsâ€™ applicability in diverse and complex as many systems require extensive examples to learn new
scenarios. Additionally, end-to-end systems are difficult to tasks, which is impractical in dynamic environments with
explain, analyze, and improve. These systems [4]â€“[6] can frequent task variations. Handling a diverse range of tasks
learn directly from sensory inputs and discover complex is essential for practical robotics. However, existing models
feature representations, eliminating the need for manual often excel only in narrow tasks and struggle with broader
feature engineering. However, they are hard to interpret and activities, reducing their real-world utility. Table I compares
struggle to transfer skills to new tasks. various robot learning systems in key capacities, including
data efficiency, task complexity, explanation, diversity, gen-
X.Chen,W.Chen,D.Lee,Y.Ge,andP.KormushevarewiththeDepartment
eralization, success rate, pre-training scenario requirements,
of Dyson School of Engineering, Imperial College London, London, UK
(cx119@ic.ac.uk),N.RojasiswithTheAIInstitute,Cambridge,MA,USA. scenario complexity, and multi-modal fusion capability.
4202
guA
2
]OR.sc[
1v43310.8042:viXraTABLE I: Comparison of Capacities of Different Robot Systems
Capacity/System Ours Q-attention[12] PerAct[10] Diffuser[13] MimicPlay[14] GLiDE[15] CoarsetoFine[11]
DataEfficiency HighlyEfficient Moderately ModeratelyEfficient Inefficient HighlyEfficient HighlyEfficient ModeratelyEfficient
TaskComplexity Long-Horizon Short-Horizon Short-Horizon Short-Horizon Long-horizon Short-Horizon Short-Horizon
TaskExplanation HighClarity LowClarity MediumClarity LowClarity MediumClarity HighClarity MediumClarity
TaskDiversity WideDomain NarrowDomain NarrowDomain NarrowDomain WideDomain NarrowDomain WideDomain
TaskGeneralisation One-shot Few-shot 53shots 10kshots 20&40shots Few-shot One-shot
SuccessRate 94.4% *62% *37% *54.4% *55% *99.6% *62%
Pre-trainScenarios Non-essential Required Required Required Required Required Required
ScenarioComplexity Complex Moderate Moderate Moderate Moderate Simple Simple
Multi-modalFusion Action,Vision,LLM Vision Vision,LLM Action,Vision Action,Vision Action,Vision,LLM Action,Vision
*Theresultsarefromtheoriginalpresentedpapers.
To overcome these limitations, we propose a structured
and modular backbone framework. This approach enhances R R R R â€¦ D D TE TE TE TE â€¦ D G G G G â€¦ TL TL
the adaptability and efficiency of robot learning systems,
TL
TL TL TL â€¦ U U D D U U U U U U U U â€¦ TL TL TL
improving generalization across various tasks and scenarios
TL
while providing a clearer structure for task execution. Our TL TL TL TL â€¦ RL RL RL â€¦ D TE TE TE TE â€¦ R R R R â€¦
framework (Fig. 1) decomposes robot tasks into two sub-
levels: robot trajectory (internal information) and environ-
mental context (external information). The robot trajectory
covers the entire motion during the task, while the environ-
mental context includes all objects involved.
By decomposing the robot trajectory, we identify elemen- Therblig (G) Therblig (U) â€¦ Therblig (RL) Therblig (R)
Robot task start Robot task end
tal motions, termed â€™Therbligsâ€™ (details in Section 2), which
Fig. 2: Detailed Decomposition of a Robotic Task into therbligs.
serve as backbone elements for understanding the task tra-
The sequence starts with Rest (R), where the robot remains idle.
jectoryandcontext.Decomposingtheenvironmentalcontext
Transport Empty (TE) shows robot moving without any load.
into lower-level elements reveals objects and their attributes, Delay (D) represents a pause, allowing for system adjustments or
such as mask, position, orientation, and task relevance. As synchronizationwithotherprocesses.TheGrasp(G)phaseinvolves
shown in Table I, our approach offers high data efficiency, robotâ€™s precise movement to hold the object. In Transport Load
(TL), robot carries the object to its intended destination. The Use
achievingsuperiorperformancewithsmallerdatasets,which
(U)phaseillustratestherobotperformingtheprimaryfunctionwith
is crucial for practical applications. The main contributions
theobject.Release(R)Loaddepictstherobotreleasingtheobject.
of this work are as follows:
II. RELATEDRESEARCH
â€¢ Therblig-Based Backbone Framework (TBBF): A
framework that decomposes complex tasks into fun- Variousintelligentrobotsystemsachievehighaccuracyin
damental units, enhancing generalization and data ef- specifictasks,suchascablerouting[16],clothmanipulation
ficiency in long-horizon tasks. [17], and fruit grasping [18]. However, they often struggle
â€¢ Meta-RGate SynerFusion Network (MGSF): A net- to generalize across different tasks. Recently, efforts have
work for accurate therblig segmentation during offline been made to develop systems that can handle a variety
training,improvingunderstandingofsequentialactions. of tasks [11], [13], [14]. However, these systems typically
â€¢ Action Registration (ActionREG): Integrates therbligs require large datasets or simple scenarios for generalization
withobjectconfigurationsforpreciseactionregistration and lack a clear backbone for task understanding. To over-
and robust task execution. come these limitations, we propose using therbligs as the
â€¢ LLM-Alignment Policy for Visual Correction (LAP- backbone for better task comprehension. Therbligs, which
VC): Improves tolerance to errors and reduces depen- consist of 18 basic motions, provide a systematic way to
dency on highly accurate demonstrations. describe and analyze detailed actions in various tasks [19].
Initiallydevelopedforstudyinghumanmovements,weapply
We conducted offline and online tests to evaluate the robust- these concepts to visually and conceptually break down the
ness and generalization of our framework. During offline robotâ€™strajectoryduringlearningandtaskexecution.Inrobot
training, the MGSF network segmented therbligs across learning, therbligs offer a clear and structured method to
six tasks: tool-pick&place, crossbeam-cutting, bricks- visualize processes, as shown in Fig. 2.
gluing,tissue-sweeping,surface-wiping,andcup-pouring. An early contribution in this area is the work by Ah-
In online testing, we collected one-shot demonstrations for madzadeh et al. [20], who introduced a novel approach for
five new tasks: board-rolling, foamblock-flipping, plate- converting action sequences into symbolic representations.
scrubbing, spoon-tilting, and paper-stamping. For action modalities, Chen et al. [21] introduced a novelOnline Testing System
Offline Training System
New Robot Tasks New Robot Configurations
Human expert
(Rolling board surface, flipping the foam blocks, circular scrubbing (Different locations, orientations,
demonstration
the plate surfaceâ€¦ ) add new unrelated objects)
Trajectory Therblig Action modality (New task) One-shot demonstration Vision modality (New configuration)
Capturing Labelling
Shuffling Matching
Therblig Backbone
Robot task MGSF (therblig segmentation) Initial configuration (registration)
pick&place,
glue bricks, R R R â€¦ G G G â€¦ U U U U U U U U â€¦ ğ‘» ğ‘³ ğ‘» ğ‘³ â€¦ ğ‘¹ ğ’† ğ‘¹ ğ’† â€¦
wipe surface,
sweep tissue
cutcro s â€¦s-beam U U ğ‘¹ğ’†
(use) (use) (release)
Omniverse ğ‘»ğ‘³
Sensing System (OSS) R (rest) G (grasp) (T-load)
multi-thread
synchronization
Foundation Model Integration
Robot Trajectory States
ğ = (ğœ½,ğœ¬, ğ‚,ğ‰)
ActionREG LAP-VC Context Matching
action sequence (t)
ğœ”ğŸ ğœ”ğŸğœ”ğŸğœ”ğŸ‘ğœ”ğŸ’ â€¦ ğœ”ğ’•â€™ğŸ ğœ”ğ’• (SAM-based) (LLM-based) (YoloV8-PCA)
What & How to grasp, use, release: Uncertain error Where to grasp, use, release:
Offline Model Related object compensation New scenarios adapting
& trajectory reasoning (position, orientation)
MGSF Therblig Inverse Kinematics
Segmentation Network
Fig.3:OverviewoftheproposedTBBF.Thispipelineintegratesofflinetrainingandonlinetestingstages.Duringofflinetraining,human
experts provide demonstrations and label robot trajectories into therbligs, which are then used to train the MGSF network. In the online
testing stage, the trained MGSF network segments new tasks into Therblig-level actions. ActionREG registers these actions into new
configurations,andLAP-VCisutilizedforerrorcompensation.Finally,YOLOv8andPCAareusedtomatchnewconfigurations,ensuring
robust performance across varied and complex scenarios.
approachthatleveragessequentialmotionprimitivesthrough to easily generate goals. These verbs are conceptually close
humandemonstration.UtilizingahierarchicalBiLSTMclas- to therbligs, providing a strong basis for task decompo-
sifier,theyextractedanintuitivehigh-levelknowledgecalled sition. However, while this approach closely approximates
therbligs, which reconstructs motion information recorded the therbligs concept, it still does not clearly establish this
from human-guided demonstrations, allowing for more gen- structure nor utilize it as the backbone for constructing
eral representations for different task decompositions. How- the primary framework. As a result, the system lacks a
ever, their work remains at a conceptual stage for simple well-defined structure that could enhance its robustness and
therblig segmentation, and it lacks integration with vision reliabilityindiversescenarios.Withoutaclearbackbone,the
modalities, resulting in limited generalization and scene integrationofaction,vision,andlanguagemodalitiesremains
understanding. For vision modalities, Dessalene et al. [22] fragmented, leading to potential gaps in task representation
introduced a rule-driven, compositional, and hierarchical ac- and execution efficiency.
tionmodelingmethodbasedontherbligstoanalyzecomplex In our research, we propose to use therbligs as the
motions. This model features a novel hierarchical archi- backbone of a robot intelligent system to enhance task un-
tecture comprising a Therblig model and an action model, derstanding.Thistherblig-basedbackbonesystemrepresents
utilizing vision as a medium for robot action segmentation. a significant contribution towards a more structured, inter-
However, it lacks integration with action modalities based pretable,andadaptableframeworkforrobottasklearningand
on human demonstrations, leading to a significant domain execution. By integrating this approach with the foundation
gap and inefficient capture of accurate task representations model, we can easily extract the detailed configurations of
through images alone. the objects. Thus, we can create a more robust and flexible
The concurrent study of multimodal approaches (action, model for robotic systems.
vision, language) was introduced by Wang et al. [15]. This
researchutilizedanovelLLM-basedsystemtoexploremode III. MODELFRAMEWORK
familieswithinrobotmanipulationtrajectories.TheLLMcan
A. TBBF: Explainable robot task understanding framework
generate a list of actions involved in tasks such as scooping
marbles from one bowl into another. For instance, the LLM The TBBF is designed to enhance the understanding and
mightproduceasequenceofverbssuchasâ€œreach,â€â€œscoop,â€ generalization of robotic tasks by breaking them down into
â€œtransport,â€ and â€œpour.â€ This capability allows the system fundamental units called therbligs. This framework provides! " ! "
Timestep 0 $# $"
â€¦
Timestep T
'(, (( '' )' ,, (( )' TraR jo e'b c&,o t ( o&t ry B eni- cL oS dTM er Tra en ns cf oo dr em rer Recâ€¦ u rsive Fusion 43 + â†’Cl 5a (s "sif "ie ,r 6 ,-.) ! $!" Gradients
! $ % & ! ! !!, , ,, ! % &$ " " "", , ,, ! % &$# #
'
##
(, ('
(%S
,
(t
%
''a
)'
,,t
'
((
$e
)'
',
(s
%,$
:
(
%"
'&, (&
HiddenStates !!# HiddenStates !!"
%"
-& â†’
!
/ "(0&,1',-&$%)
* â†’B ,C -E .lo (s +0s *,+) *
up )d $a %t *ing
! $ % & ! ! !!, , ,, ! % &$ " " "", , ,, ! % &$# # ## '$, ($ joint f e 2a 'â†’tu 3re (4 r '(e ,p 4')re ,2s 'e $n %)tation $#Ga â†’te ' (c )o ",n +tr !o ,,l !) G (OT
n
T eh -e hr ob tl i eg
n
L ca ob de inl:
g
+
)
â€¦
â€¦ ! "
$!
Meta Model Meta parameters
!! â†’ #(%,!") â†’ (# )(#$% )â€¦)(% %+,% â€¦ +,#$% +,# $! â†’ ( ,!!
$$ â† ( ,##
' )â† () ."*
Fig. 4: Detailed architecture of the MGSF network. The MGSF network integrates BiLSTM and Transformer sub-networks to capture
sequentialdependenciesinrobotictasks.Itutilizesameta-recursivegatedfusionmechanismtodynamicallycombinetheoutputsofthese
sub-networks. This fusion process adapts to different tasks, ensuring the effective integration of long-term dependencies.
a structured and modular approach, facilitating better gener- Algorithm 1 MGSF network for segmentation
alization across different tasks and scenarios while offering Input: X âˆˆRnÃ—d, mâˆˆN
a clearer and more interpretable structure for task execution. Output: YË† âˆˆRnÃ—O
In the offline training stage, we utilize the MGSF net- Initialize: Î¸ L,Î¸ T,Î¸ g,Î¸ m,Î¸ c
Initialize meta parameter vector M âˆˆRm
work to accurately segment tasks into therbligs, providing
Define the number of fusion steps T âˆˆN
a detailed breakdown of the task into its constituent mo-
for i=1 to n do
tions. During the online testing stage, we collect a one- hL â†BiLSTM(x ;Î¸ ) {BiLSTM hidden states}
i i L
shot demonstration of a new task, from which the MGSF hT â†Transformer(x ;Î¸ ) {Transformer hidden states}
i i T
networkextractshigh-levelknowledgeandtransformsitinto c i â†[hL i âŠ•hT i ]
a structured format. This knowledge is encoded into visual F =F(0) {Initial fusion output}
for t=1 to T do
datausingActionREG,integratingtherbligswiththeobjectsâ€™
Î¸ â†f(M;Î¸ ) {Gate parameters from meta-network}
g m
configurations in the robotâ€™s visual field to ensure precise Compute gate values GâˆˆRdim(F) using Î¸
g
action registration. By using therbligs as the backbone, our G=Ïƒ(Î¸ Â·c ) {Gate values using sigmoid function}
g i
framework significantly improves data efficiency and task Update fusion output F â†GâŠ™c +(1âˆ’G)âŠ™F {where
i
generalization, enabling the robot to handle a wide range of âŠ™ denotes element-wise multiplication}
F =GâŠ™c +(1âˆ’G)âŠ™F {Updated fusion output}
scenarios with robustness and precision. The integration of i
end for
LAP-VCfurtherensuresthatanyvisualdiscrepanciesarecor-
Compute predicted output yË† â†Classifier(F;Î¸ )
i c
rectedinrealtime,providinganadditionallayerofaccuracy yË† =Classifier(F;Î¸ ) {Predicted output}
i c
in task execution. As depicted in Fig. 3, this comprehensive Append predicted output to YË† â†YË† âˆª{yË†}
i
and structured methodology enhances the robotâ€™s ability to YË† =YË† âˆª{yË† i} {Update output set}
adapt to new tasks by leveraging prior knowledge encoded end for
intherbligs,thusimprovingtheinterpretability,stability,and
transferability of robotic learning systems. effectively, limiting its ability to leverage diverse features
and achieve robust performance.
B. MGSF: Effieicent therbligs segmentation network To address these limitations, our MGSF model introduces
a dynamic hybrid architecture that combines the strengths
Our MGSF network, illustrated in Fig. 4 and detailed in of both BiLSTM and Transformer sub-networks with a
Algorithm 1, advances the current state of the art in un- novel adaptive gated fusion mechanism. This architecture
derstanding robot task actions. By combining meta-learning features a meta-recursive gated fusion unit that dynami-
with adaptive gated fusion within a unified framework, this cally adapts to integrate model outputs, thereby enhancing
model significantly enhances robotsâ€™ ability to comprehend performance across diverse tasks. Unlike the static gating
and execute sequential actions across various environments. in MetaGross, our adaptive gated fusion mechanism allows
InspiredbyMetaGross[23],ourMGSFnetworkincorporates for more flexible and responsive integration of sequential
meta-gating and recursive parameterization in a recurrent data, ensuring that long-term dependencies are effectively
model.However,MetaGrosslacksadedicatedfusionprocess captured and processed. By leveraging the strengths of
and struggles to integrate different aspects of the input data both BiLSTM and Transformer sub-networks, the MGSFExpert Inverse SAM Therbig-based Task-related object
demonstration kinematics registration YoloV8 model matching
Initial scenarios Key therbligs Key points Task object mask New scenario
Initial scenario Alignment Shuffle New configuration
Robot states
image policy configurations transfer
Board-rolling task setup Task related therbligs â€˜Graspâ€™ therblig point (!!) â€˜Graspâ€™ object mask ("!) New â€˜Graspâ€™ point (!!#, $!#) #"$ (!"$, $!#)
Grasp
Release
START
Use â€˜Useâ€™
trajectory
END
(#"&)
Long-horizon trajectories â€˜Useâ€™ therblig point (!") â€˜Useâ€™ object mask ("") New â€˜Useâ€™ trajectory (#"#) (!"$,$!$) #"%
Plate-scrubbing task setup Task related therbligs â€˜Graspâ€™ therblig point (!!) â€˜Graspâ€™ object mask ("!) New â€˜Graspâ€™ point (!!#, $!#) #"$ (!"$, $!#)
Use Grasp
Release START
â€˜Useâ€™
trajectory
END (#"&)
Long-horizon trajectories â€˜Useâ€™ therblig point (!") â€˜Useâ€™ object mask ("") New â€˜Useâ€™ trajectory (#"#) #"% (!"$,$!$)
Fig. 5: Details of the action registration and context matching process. This figure illustrates the stages of action registration from initial
scenariostonewconfigurations.Keytherbligssuchasâ€™Graspâ€™andâ€™Useâ€™areidentifiedfromexpertdemonstrationsusingtheSAMmodel
and matched to object masks and trajectories. The therblig-based YOLOv8 model is employed for task-related object matching, while
inverse kinematics ensures accurate positioning.
network excels in handling complex sequences with greater Algorithm 2 ActionREG and Context Matching
precision.Themeta-learningcomponentdynamicallyadjusts Require: Online demonstration D, Therbligs segmentation model
to the changing context of tasks, ensuring that the model M MGSF, Hand-eye calibration matrix H, SAM model M SAM,
remains accurate and applicable across different situations. YOLOv8modelM YOLO,BackgroundpriorB,Newenvironment
image I , Reference image I
This fusion process, informed by meta-learning, allows the new ref
Ensure: Object positions and orientations P ,O
new new
MGSF model to provide a thorough analysis of each action,
S =M (D)
therbligs MGSF
therebyimprovingtherobotâ€™sabilitytoperformawiderange K={Rest,TEmpty,Delay,Grasp,Use,TLoad,Release}
(cid:83)
of tasks with enhanced recall and adaptability. M= kâˆˆKM SAM(HÂ·G(S therbligs,k),B)
for each m âˆˆM do
k
Box =M (I ,ComputeArea(m ))
C. ActionREG: SAM-driven action registration network k YOLO new k
F =SIFT(Box ), F =SIFT(I )
new k ref ref
A cornerstone of our TBBF is the ActionREG, designed M =FLANN(F ,F )
match new ref
for reasoning about and regressing object configurations P new,k =ComputePosition(M match)
O =PCA(M )
(Fig. 5). This innovative component integrates therbligsâ€™ new,k match
end for
prior knowledge into the SAM model as prompts, enabling
return P ,O
new new
sophisticated reasoning about the objects involved in robotic
tasks. The integration of this knowledge serves as a guide
formedbyM =FLANN(F ,F )usingFLANN,and
match new ref
forthemodel,improvingitsabilitytounderstandandpredict
orientationextractionisachievedviaO =PCA(M )
new,k match
the configurations of objects within a task-specific context.
using PCA.
Through ActionREG, we can easily extract the task-
ActionREG excels in performing one-shot regression of
related object mask and the operational workspace mask.
object configurations, meaning it can accurately predict an
Utilizing a hierarchical matching system based on SAM
objectâ€™sconfigurationinnewandunseenenvironmentsusing
(SegmentAnythingModel),YOLOv8,SIFT(Scale-Invariant
just a single example or image. This capability is vital for
Feature Transform), FLANN (Fast Library for Approxi-
robotic systems operating in dynamic, real-world settings,
mate Nearest Neighbors), and PCA (Principal Component
ensuring robust and adaptable performance.
Analysis), we can identify task-related objects and deter-
D. LAP-VC: LLM-Alignment Policy for Visual Correction
mine their configurations in new scenarios. Specifically,
(cid:83)
M = M (HÂ·G(S ,k),B) represents object Expert demonstration may have some errors: robot end-
kâˆˆK SAM therbligs
mask segmentation using the SAM model, while Box = effector grasping point may not always be perpendicular
k
M (I ,ComputeArea(m )) denotes bounding box de- to the object itself. In addition, hand-eye calibration may
YOLO new k
tection through YOLOv8. The feature extraction is handled also introduce some errors. These issues in real-world ex-
byF =SIFT(Box )usingSIFT,featurematchingisper- perimentsmaycauseincorrectpositionestimationandaffect
new kFlipping Flipping
Robot eef
Side view Top-down view
projection
Target grasp Region of
point ("$) interest
LLM Alignment %$='(% (%)
Policy (LAP) Actual eef
center point (")
Fig. 7: Therblig segmentation recall for different robot tasks. It
shows the recall rates for various therbligs across multiple robotic
tasks, highlighting the performance of our segmentation network.
Stamping Tilting Rolling Scrubbing
MGSF network outperforms other state-of-the-art methods.
Fig. 6: Application of LAP-VC in robotic tasks. This process
As shown in Table II, our network achieved the highest
involves inputting predicted points along with the scenario image
into the LLM. The pre-built prompt guides the LLM to output recallof94.37%,surpassingtraditionalmethodslikeCascade
corrected points, compensating for errors due to factors such as SVM and deep learning models such as CNN-RNN and
imperfect grasping angles or hand-eye calibration inaccuracies. BiLSTM-Type3.Inaddition,weconductedanablationstudy
for our MGSF network to evaluate the impact of different
our action registration results, especially in the grasp stage. components. The baseline recall of our backbone model
To address these challenges, we introduced a novel method (without fusion) was approximately 79. 77% with a trans-
(Fig. 6) using LLM to correct these errors. By inputting our formernetwork.Whenweintroducedthefusionmechanism,
predictedpointsalongwiththescenarioimageintotheLLM the recall improved to 84.29%. The addition of gate fusion
and utilizing a pre-built prompt, the LLM outputs corrected mechanism boosted the recall to 90.92%. The final MGSF
points. This method significantly reduces the impact of poor model, which incorporates gated fusion and meta-learning,
demonstrationsandcumulativesystemerrors,minimizingthe achieved an recall of 94.37% (Table II).
need for highly accurate expert demonstrations.
Moreover,wealsoanalyzedtherecallofdifferenttherbligs
in terms of various tasks (Fig. 7). Generally, surface wiping
IV. EXPERIMENTSANDSETUP
achieved the highest segmentation results (97.17%) with
To demonstrate the modelâ€™s generality, we developed the six diverse robot tasks, while tissue sweeping achieved the
Omniverse Robot System (ORS) for convenient data collec- lowest results (93.88%). This discrepancy may be attributed
tion. Offline data collection involved two individuals: one to the complexity of tissue sweeping actions required to
performing expert demonstrations and the other labeling the effectively put tissue into the dustpan. TLoad and Re-
robot taskâ€™s status. The system collected data on robot joint lease achieved the lowest recall results, around 85.56% and
angles,endeffectorpose,jointspeed,force,andtorqueread- 83.41% respectively. However, it is important to note that
ings from the â€™Robotiqâ€™ sensor, task images, and real-time eventhoughthereleaserecallisrelativelylower,thisdoesnot
labels. We gathered data from six tasks (tool-pick&place, significantly affect the overall results. This is because, after
crossbeam-cutting, bricks-gluing, tissue-sweeping, surface- intricate manipulation operations, the force-torque sensors
wiping, cup-pouring) with expert demonstrations. For online may be affected by drift.
testing, a human expert performed new tasks in unseen sce-
Furthermore, as shown in Fig. 8, the LAP-VC system
narios,withORSrecordingrobotstatesandscenarioimages
consistently achieves high alignment performance scores
before and after the task. We tested five challenging tasks
across various tasks, outperforming traditional methods like
(board-rolling, foamblock-flipping, plate-scrubbing, spoon-
KNN,SIFT,ORB,AKAZE,FAST,andBRISK.Inparticular,
tilting, and paper-stamping) to evaluate performance.
for the Roller task, the LAP-VC system achieved a score
We utilized a UR5 robotic arm with an origami gripper,
of 0.92, which is second only to the manual alignment
inspired by Liu et al. [24], for manipulation tasks. This
score of 0.98. Similarly, in the Spoon task, the LAP-VC
setup improves adaptability and precision. Visual and force
scored 0.88, close to the perfect manual score of 1.00. In
feedbackwerecapturedusingaRealSenseD435icameraand
the Stamp, Sponge, and Scraper tasks, the LAP-VC system
aRoboTiqforcesensor.ThesetupranonUbuntu22.04,with
scored0.84,0.94,and0.90respectively,showingaconsistent
ROS2 Humble for component communication and MoveIt2
high performance. Compared to other automated methods,
formotionplanning.DevelopmentwasconductedinVSCode
LAP-VC shows superior robustness and reliability.
and model training utilized an NVIDIA RTX 4090 GPU.
For the results of task execution, Table III showed that
V. RESULTSANDANALYSIS
our system can achieve promising performance in terms
For offline training, we used robot states as input to of simple scenario (SimScenario) and complex scenario
segment therbligs. In this time-series segmentation task, our (ComScenario) with different new tasks.TABLE II: Therblig Segmentation General Performance
Benchmark Model BCE-Loss â†“ Precision â†‘ Recall â†‘ F1-Score â†‘ Kappa â†‘ TP-Range â†‘
TCNs [25] 0.623Â±0.002 85.04Â±0.79 88.48Â±0.79 86.51Â±0.80 85.54Â±0.99 [85.45,92.97]
ABLG-CNN [26] 0.472Â±0.014 25.32Â±0.55 24.08Â±0.65 21.18Â±0.98 8.23Â±0.48 [18.71,32.05]
MS-CRN [27] 0.058Â±0.008 92.57Â±1.20 92.55Â±1.19 92.51Â±1.21 90.76Â±1.48 [75.58,90.11]
BiLSTM-T3 [21] 0.137Â±0.002 78.84Â±0.56 81.56Â±0.53 79.74Â±0.51 76.82Â±0.64 [73.64,86.56]
GATv2 [28] 0.145Â±0.008 81.07Â±0.79 82.22Â±0.63 80.47Â±0.65 77.58Â±0.78 [91.88,93.80]
Reformer [29] 0.145Â±0.005 77.87Â±0.69 80.67Â±0.74 78.79Â±0.72 75.75Â±0.91 [68.50,88.17]
TFT [30] 0.307Â±0.004 69.37Â±0.46 71.81Â±0.44 69.28Â±0.41 64.45Â±0.52 [62.36,82.02]
TSMixer [31] 0.125Â±0.005 81.14Â±0.56 83.51Â±0.44 81.68Â±0.46 79.25Â±0.56 [80.06,90.30]
Ablation Model (descending) BCE-Loss â†“ Precision â†‘ Recall â†‘ F1-Score â†‘ Kappa â†‘ TP-Range â†‘
MGFN (Ours) 0.043Â±0.007 94.36Â±0.60 94.37Â±0.59 94.36Â±0.60 93.03Â±0.73 [93.88,97.17]
GrNT (no meta) [32] 0.068Â±0.008 90.96Â±0.83 90.92Â±0.80 90.84Â±0.83 88.72Â±0.99 [88.74,94.73]
Adaptive-DF (no gate) [33] 0.118Â±0.002 81.86Â±0.29 84.29Â±0.24 82.44Â±0.25 80.24Â±0.30 [81.11,89.82]
Backbone (no fusion) [34] 0.145Â±0.004 77.00Â±0.92 79.77Â±0.80 77.94Â±0.82 74.57Â±0.99 [74.44,87.67]
TABLE III: Robot Task Success Rate Comparison (Long-horizon)
Model / Task Board-rolling FoamBlock-flipping Plate-scrubbing Spoon-tilting Paper-stamping Total
SM(ST)-SimScenario 13/50 (26%) 2/50 (4%) 11/50 (22%) 0/50 (0%) 15/50 (30%) 13.7%
BC(ST)-SimScenario 6/50 (12%) 0/50 (0%) 0/50 (0%) 2/50 (4%) 0/50 (0%) 2.7%
BC(MT)-SimScenario 0/50 (0%) 0/50 (0%) 0/50 (0%) 0/50 (0%) 0/50 (0%) 0%
Ours-SimScenario 48/50 (96%) 47/50 (94%) 46/50 (92%) 48/50 (96%) 47/50 (94%) 94.4%
SM(ST)-ComScenario 0/50 (0%) 0/50 (0%) 0/50 (0%) 0/50 (0%) 0/50 (0%) 0%
BC(ST)-ComScenario 0/50 (0%) 0/50 (0%) 0/50 (0%) 0/50 (0%) 0/50 (0%) 0%
BC(MT)-ComScenario 0/50 (0%) 0/50 (0%) 0/50 (0%) 0/50 (0%) 0/50 (0%) 0%
Ours-ComScenario 43/50 (86%) 42/50 (84%) 40/50 (80%) 34/50 (68%) 41/50 (82%) 80%
statemachinesachievesomesuccesswithwell-builtpolicies
andexcellentvisionfeatures.However,differenttasksrequire
differentpolicies,significantlyincreasingsystemcomplexity,
as noted by Luo et al. [16]. We collected 250 expert demon-
strations for BC training and found that MT-BC training is
challengingwithlimiteddata.ST-BCshowedconvergingloss
but resulted in unstable trajectories and task failures with
minor errors. Applying BC to ComScenario led to divergent
training and unpredictable trajectories.
ACT and diffusion policies were not used as baselines
for a few reasons. Our system performs one-shot tasks and
operates in zero-shot scenarios without requiring extensive
pre-training for scene objects, thus needing less data. In
Fig.8:Performancecomparisonacrossvariousmethodsandtasks. contrast, ACT approaches typically require large datasets.
Additionally, our system handles long-horizon tasks involv-
For our SimScenario, we only consider two task-related
ingcomprehensiveactionsequences,whilediffusionpolicies
and unseen objects appearing in our vision area. For Com-
focus on discrete sub-tasks. Therefore, a direct comparison
Scenario, we add 3-6 unrelated and unseen objects to-
may not provide meaningful insights. This distinction high-
gether with our task-related objects to mimic the real-world
lights our systemâ€™s data efficiency and generalization.
cluttered environments. For SimScenario mode, our system
achieved around 94.4% average success rate for five tasks, VI. CONCLUSIONANDFUTUREWORK
while our success rate decreased to 80% if we switched In this paper, we presented a novel TBBF to enhance
to ComScenario mode. We find that spoon-tilting has a the understanding and execution of robotic tasks. By de-
lower success rate because it requires a more dynamic and composing complex tasks into fundamental therbligs, our
complex trajectory while robot solvers easily get trapped innovative approach provides a structured and interpretable
by singularity. We compared our system against Single-Task representation of robotic actions. The MGSF network was
State Machine (ST-SM), Single-Task Behavior Clone (ST- employed for accurate therblig segmentation during the of-
BC), and Multi-Task Behavior Clone (MT-BC) baselines. fline training stage, while ActionREG efficiently facilitated
State machines, built for single tasks, use pre-built policies precise action registration and integration of therbligs with
and vision features to generate trajectories. In SimScenario, object configurations during the online testing stage.Our experimental results demonstrate the effectiveness [13] M. Janner, Y. Du, J. B. Tenenbaum, and S. Levine, â€œPlan-
of our framework, showcasing high recall in therblig seg- ning with diffusion for flexible behavior synthesis,â€ arXiv preprint
arXiv:2205.09991,2022.
mentation and robust performance in real-world robot task
[14] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu, Y. Zhu,
execution. Specifically, we achieved results with 94.37% andA.Anandkumar,â€œMimicplay:Long-horizonimitationlearningby
recall in therblig segmentation and impressive successful watchinghumanplay,â€arXivpreprintarXiv:2302.12422,2023.
[15] Y.Wang,T.-H.Wang,J.Mao,M.Hagenow,andJ.Shah,â€œGrounding
execution rates of 94.4% for new and long-horizon tasks in
language plans in demonstrations through counterfactual perturba-
simple scenarios, and 80% in complex scenarios. However, tions,â€arXivpreprintarXiv:2403.17124,2024.
somelimitationsshouldbeaddressedinfuturework.Firstly, [16] J. Luo, C. Xu, X. Geng, G. Feng, K. Fang, L. Tan, S. Schaal, and
S. Levine, â€œMulti-stage cable routing through hierarchical imitation
our model relies heavily on standard expert demonstrations,
learning,â€IEEETransactionsonRobotics,2024.
which must follow specific protocols. It struggles with ex- [17] W. Chen and N. Rojas, â€œTrakdis: A transformer-based knowledge
tremely noisy or unprofessional demonstrations. Secondly, distillationapproachforvisualreinforcementlearningwithapplication
toclothmanipulation,â€IEEERoboticsandAutomationLetters,2024.
our model, like many existing approaches, is affected by
[18] D.Lee,W.Chen,andN.Rojas,â€œSyntheticdataenablesfasteranno-
object shadows and light reflections. Additionally, we fo- tation and robust segmentation for multi-object grasping in clutter,â€
cused on the generation of 2D object configurations and did arXivpreprintarXiv:2401.13405,2024.
[19] A.BaumgartandD.Neuhauser,â€œFrankandlilliangilbreth:scientific
not account for 3D object configurations, which could be
managementintheoperatingroom,â€pp.413â€“415,2009.
improved with depth information. [20] S. R. Ahmadzadeh, A. Paikan, F. Mastrogiovanni, L. Natale, P. Ko-
FutureresearchwillfocusonrefiningtheTBBFtoenhance rmushev, and D. G. Caldwell, â€œLearning symbolic representations
of actions from human demonstrations,â€ in 2015 IEEE International
itsrobustnessandadaptabilityinmorecomplexanddynamic
Conference on Robotics and Automation (ICRA). IEEE, 2015, pp.
environments. We will also incorporate more user data to 3801â€“3808.
train a more robust action segmentation network. Further- [21] C.-S.Chen,S.-K.Chen,C.-C.Lai,andC.-T.Lin,â€œSequentialmotion
primitives recognition of robotic arm task via human demonstration
more, we plan to extend the applicability of our methods to
using hierarchical bilstm classifier,â€ IEEE Robotics and Automation
abroaderrangeofroboticsplatforms,facilitatingthetransfer Letters,vol.6,no.2,pp.502â€“509,2020.
of robot knowledge from the UR5 to other platforms such [22] E. Dessalene, M. Maynord, C. FermuÂ¨ller, and Y. Aloimonos,
â€œTherbligsinaction:Videounderstandingthroughmotionprimitives,â€
as Panda or Kinova robots.
inProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,2023,pp.10618â€“10626.
REFERENCES [23] Y. Tay, Y. Shen, A. Chan, and Y. S. Ong, â€œMetagross: Meta gated
recursive controller units for sequence modeling,â€ 2020. [Online].
[1] H. Wu, Y. Jing, C. Cheang, G. Chen, J. Xu, X. Li, M. Liu, H. Li, Available:https://openreview.net/forum?id=Sygn20VtwH
andT.Kong,â€œUnleashinglarge-scalevideogenerativepre-trainingfor [24] J. Liu, Z. Chen, G. Wen, J. He, H. Wang, L. Xue, K. Long, and
visualrobotmanipulation,â€arXivpreprintarXiv:2312.13139,2023.
Y. M. Xie, â€œOrigami chomper-based flexible gripper with superior
[2] Y. Jing, X. Zhu, X. Liu, Q. Sima, T. Yang, Y. Feng, and T. Kong, grippingperformances,â€AdvancedIntelligentSystems,vol.5,no.10,
â€œExploringvisualpre-trainingforrobotmanipulation:Datasets,mod- p.2300238,2023.
els and methods,â€ in 2023 IEEE/RSJ International Conference on [25] S.Bai,J.Z.Kolter,andV.Koltun,â€œAnempiricalevaluationofgeneric
Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 11390â€“ convolutional and recurrent networks for sequence modeling,â€ arXiv
11395. preprintarXiv:1803.01271,2018.
[3] R.Jiang,B.He,Z.Wang,X.Cheng,H.Sang,andY.Zhou,â€œRobot [26] J. Deng, L. Cheng, and Z. Wang, â€œAttention-based bilstm fused cnn
skill learning and the data dilemma it faces: a systematic review,â€ with gating mechanism model for chinese long text classification,â€
Robotic Intelligence and Automation, vol. 44, no. 2, pp. 270â€“286, ComputerSpeech&Language,vol.68,p.101182,2021.
2024. [27] C.Lea,R.Vidal,A.Reiter,andG.D.Hager,â€œTemporalconvolutional
[4] A.Sharma,A.M.Ahmed,R.Ahmad,andC.Finn,â€œSelf-improving networks: A unified approach to action segmentation,â€ in Computer
robots: End-to-end autonomous visuomotor reinforcement learning,â€ Visionâ€“ECCV2016Workshops:Amsterdam,TheNetherlands,October
arXivpreprintarXiv:2303.01488,2023. 8-10and15-16,2016,Proceedings,PartIII14. Springer,2016,pp.
[5] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, 47â€“54.
T.Armstrong,I.Krasin,D.Duong,V.Sindhwanietal.,â€œTransporter
[28] S.Brody,U.Alon,andE.Yahav,â€œHowattentivearegraphattention
networks:Rearrangingthevisualworldforroboticmanipulation,â€in networks?â€arXivpreprintarXiv:2105.14491,2021.
ConferenceonRobotLearning. PMLR,2021,pp.726â€“747. [29] N. Kitaev, Å. Kaiser, and A. Levskaya, â€œReformer: The efficient
[6] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, transformer,â€arXivpreprintarXiv:2001.04451,2020.
N. Heess, T. RothoÂ¨rl, T. Lampe, and M. Riedmiller, â€œLeveraging [30] B. Lim, S. OÂ¨. ArÄ±k, N. Loeff, and T. Pfister, â€œTemporal fusion
demonstrationsfordeepreinforcementlearningonroboticsproblems transformers for interpretable multi-horizon time series forecasting,â€
withsparserewards,â€arXivpreprintarXiv:1707.08817,2017. International Journal of Forecasting, vol. 37, no. 4, pp. 1748â€“1764,
[7] S.Levine,P.Pastor,A.Krizhevsky,J.Ibarz,andD.Quillen,â€œLearning 2021.
hand-eye coordination for robotic grasping with deep learning and [31] V.Ekambaram,A.Jati,N.Nguyen,P.Sinthong,andJ.Kalagnanam,
large-scale data collection,â€ The International journal of robotics â€œTsmixer: Lightweight mlp-mixer model for multivariate time series
research,vol.37,no.4-5,pp.421â€“436,2018. forecasting,â€inProceedingsofthe29thACMSIGKDDConferenceon
[8] A. Kumar, Z. Fu, D. Pathak, and J. Malik, â€œRma: Rapid motor KnowledgeDiscoveryandDataMining,2023,pp.459â€“469.
adaptationforleggedrobots,â€arXivpreprintarXiv:2107.04034,2021.
[32] Y. Zhang, P. Gu, Y. Zhang, C. Wang, and D. Z. Chen, â€œGrnt:
[9] S. Pirk, K. Hausman, A. Toshev, and M. Khansari, â€œModeling long- Gate-regularized network training for improving multi-scale fusion
horizon tasks as sequential interaction landscapes,â€ arXiv preprint in medical image segmentation,â€ in 2023 IEEE 20th International
arXiv:2006.04843,2020. SymposiumonBiomedicalImaging(ISBI). IEEE,2023,pp.1â€“5.
[10] M. Shridhar, L. Manuelli, and D. Fox, â€œPerceiver-actor: A multi- [33] B.Shi,Y.Liu,S.Lu,andZ.-W.Gao,â€œAnewadaptivefeaturefusion
task transformer for robotic manipulation,â€ in Conference on Robot andselectionnetworkforintelligenttransportationsystems,â€Control
Learning. PMLR,2023,pp.785â€“799. EngineeringPractice,vol.146,p.105885,2024.
[11] E.Johns,â€œCoarse-to-fineimitationlearning:Robotmanipulationfrom [34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
a single demonstration,â€ in 2021 IEEE international conference on Gomez, Å. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€
roboticsandautomation(ICRA). IEEE,2021,pp.4613â€“4619. Advancesinneuralinformationprocessingsystems,vol.30,2017.
[12] S.JamesandA.J.Davison,â€œQ-attention:Enablingefficientlearning
forvision-basedroboticmanipulation,â€IEEERoboticsandAutomation
Letters,vol.7,no.2,pp.1612â€“1619,2022.