Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for
Video Motion Editing
YiZuo LinglingLi LichengJiao
XidianUniversity XidianUniversity XidianUniversity
Xiâ€™an,China Xiâ€™an,China Xiâ€™an,China
yiiizuo@163.com llli@xidian.edu.cn lchjiao@mail.xidian.edu.cn
FangLiu XuLiu WenpingMa
XidianUniversity XidianUniversity XidianUniversity
Xiâ€™an,China Xiâ€™an,China Xiâ€™an,China
f63liu@163.com xuliu361@163.com wpma@mail.xidian.edu.cn
ShuyuanYang YuweiGuo
XidianUniversity XidianUniversity
Xiâ€™an,China Xiâ€™an,China
syyang@xidian.edu.cn ywguo@xidian.edu.cn
Source video Source Prompt: â€œA girl with a striped top and a black short skirt is dancing.â€
Reference video Reference Prompt: â€œA girl with a pink top and a white dress is waving.â€
Edited video Target Prompt: â€œA girl with a striped top and a black short skirt is waving.â€
Figure1:Edit-Your-Motioneditsthemotionofthesourcevideotoalignthereferencevideowithoutchangingtheobjectâ€™s
contentorbackground.
Source videos
Reference videos
Edited videos
4202
yaM
7
]VC.sc[
1v69440.5042:viXraABSTRACT usingavideomotioneditingmodel,eliminatingthenecessityfor
Existingdiffusion-basedvideoeditingmethodshaveachievedim- complexsoftware.
pressiveresultsinmotionediting.Mostoftheexistingmethods Inpriorstudies,researchersprimarilyutilizedgenerativemeth-
focusonthemotionalignmentbetweentheeditedvideoandthe ods to create videos featuring specific actions, with few efforts
referencevideo.However,thesemethodsdonotconstraintheback- focusing on editing motions within a specific video. For exam-
groundandobjectcontentofthevideotoremainunchanged,which ple,severalpriorstudies[26,64,65]havefocusedonpose-guided
makesitpossibleforuserstogenerateunexpectedvideos.Inthis videogeneration,whichinvolvescreatingvideosthatalignwith
paper,weproposeaone-shotvideomotioneditingmethodcalled specifiedhumanposes.Otherstudies[9,17,25,35,57,66]togen-
Edit-Your-Motionthatrequiresonlyasingletext-videopairfor eratevideoswiththesamemotionbylearningthemotionfeatures
training.Specifically,wedesigntheDetailedPrompt-GuidedLearn- inthesourcevideo.Thesestudiesoperatewithinthetext-driven
ingStrategy(DPL)todecouplespatio-temporalfeaturesinspace- space-timediffusionmodelframework,engineeredtolearnthelink
timediffusionmodels.DPLseparateslearningobjectcontentand betweentextualpromptinputsandcorrespondingvideooutputs.
motion into two training stages. In the first training stage, we However,thespatialandtemporalfeaturesofthevideoarenot
focusonlearningthespatialfeatures(thefeaturesofobjectcon- separatedduringthetraining,whichmakesthementangled.The
tent)andbreakingdownthetemporalrelationshipsinthevideo spatialfeaturesareusuallyrepresentedastheobjectâ€™scontent,and
framesbyshufflingthem.WefurtherproposeRecurrent-Causal thetemporalfeaturesareusuallyrepresentedasthebackground
Attention (RC-Attn) to learn the consistent content features of andmotion.Thisentangledstateleadstooverlappingobjectcon-
theobjectfromunorderedvideoframes.Inthesecondtraining tent,backgroundandmotioninthespace-timediffusionmodel.As
stage,werestorethetemporalrelationshipinvideoframestolearn aresult,itischallengingtogeneratehighlyalignedvideoswith
thetemporalfeature(thefeaturesofthebackgroundandobjectâ€™s thefine-grainedforegroundandbackgroundofthesourcevideo,
motion).WealsoadopttheNoiseConstraintLosstosmoothout evenwhendetailedtextdescriptionsareused.Intuitively,thekey
inter-framedifferences.Finally,intheinferencestage,weinject tovideomotioneditingliesindecoupling[8,54,60]thetemporal
thecontentfeaturesofthesourceobjectintotheeditingbranch andspatialfeaturesofthespace-timediffusionmodel.
throughatwo-branchstructure(editingbranchandreconstruc- MotionEditor[45]firstexploredthisproblembyutilizingatwo-
tionbranch).WithEdit-Your-Motion,userscaneditthemotionof branchstructureintheinferencestagetodecoupletheobjectâ€™s
objectsinthesourcevideotogeneratemoreexcitinganddiverse contentandbackgroundinthefeaturelayerbytheobjectâ€™ssegmen-
videos.Comprehensivequalitativeexperiments,quantitativeex- tationmask.However,sincetheMotionEditorâ€™smodellearnsthe
perimentsanduserpreferencestudiesdemonstratethatEdit-Your- relationshipbetweenthepromptandtheentirevideoduringthe
Motionperformsbetterthanothermethods.Codesareavailableat: trainingstage,thefeaturesofobjectsandthebackgroundoverlap
https://github.com/yiiizuo/Edit-Your-Motion. inthefeaturelayer.Thisoverlapmakesitchallengingtodistinguish
betweenthebackgroundandtheobjectsusingonlythesegmenta-
CCSCONCEPTS tionmask[23,39,50].
Inthispaper,weexploremethodstoseparatethelearningof
â€¢Appliedcomputingâ†’Mediaarts.
temporalandspatialfeaturesinspace-timediffusionmodels.To
this end, we propose a one-shot video motion editing method
KEYWORDS
namedEdit-Your-Motionthatrequiresonlyasingletext-videopair
VideoMotionEditing,Space-TimeDiffusionDecouplingLearning,
fortraining.Specifically,weproposetheDetailedPrompt-Guided
VideoUnderstand.
LearningStrategy(DPL),atwo-stagelearningstrategydesignedto
separatespatio-temporalfeatureswithinspace-timediffusionmod-
1 INTRODUCTION
els.Furthermore,weproposeRecurrent-CausalAttention(RC-Attn)
Diffusion-based[22,41,44,49,53]videomotioneditingaimsto asanenhancementoverSparse-CausalAttention.TheRecurrent-
control the motion (e.g., standing, dancing, running) of objects CausalAttentionallowsearlyframesinavideotoreceiveinfor-
in the source video based on text prompts or other conditions mationfromsubsequentframes,ensuringconsistentcontentof
(e.g.,depthmap,visibleedges,humanposes,etc),whilepreserving objectsthroughoutthevideowithoutaddingcomputationalbur-
theintegrityofthesourcebackgroundandobjectâ€™scontent.This den.Additionally,weconstructtheNoiseConstraintLoss[31]to
techniqueisespeciallyvaluableinmultimedia[6,10,21,33,52,56, minimizeinter-framedifferencesoftheeditedvideoduringthe
58,63],includingadvertising,artisticcreation,andfilmproduction. secondtrainingstage.
Itallowsuserstoeffortlesslymodifythemotionofobjectsinvideos DuringDPL,weusethespace-timediffusionmodel(inflatedU-
Net[37])asthebackboneandintegrateControlNet[61]tocontrol
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
thegenerationofmotion.Inthefirsttrainingstage,weactivate
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation Recurrent-CausalAttentionandfreezetheotherparameters.Then,
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe werandomlydisrupttheorderofframesinthesourcevideoand
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
maskthebackgroundtoguideRecurrent-CausalAttentiontofocus
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. onlearningthecontentfeaturesofobjects.Inthesecondtraining
ACMMM,2024,Melbourne,Australia stage,weactivateTemporalAttention[48]andfreezeotherparam-
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
eterstolearnmotionandbackgroundfeaturesfromorderedvideo
ACMISBN978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnnEdit-Your-Motion:Space-TimeDiffusionDecouplingLearningforVideoMotionEditing ACMMM,2024,Melbourne,Australia
frames.Concurrently,NoiseConstraintLossisusedtominimize However,thesemethodslacktemporalmodeling,anditisdif-
thedifferencebetweenframes. ficulttomaintainconsistencybetweenframeswhengenerating
Intheinferencestage,wefirstperformaDDIM[42]inversionfor video.
thesourcevideotointroducelatentnoiseandfacilitatethesmooth-
nessoftheeditedvideo.Then,theposeinformationofthereference 2.2 Pose-guidedandMotion-Customization
videoisintroducedviaControlNet.Next,toensurethatthecontent VideoGeneration
oftheobjectsintheeditedvideoremainsconsistentwiththatof
Pose-guidedimageandvideogenerationisamethodtocontrol
thesourcevideo,weutilizeatwo-branchstructure(editbranchand
imageandvideogenerationbyaddingadditionalhumanposes.Con-
reconstructionbranch)similarto[45].However,unlikeMotionEd-
trolNet[61]referencesadditionalconditionsviaauxiliarybranches
itor,DPLdistinctlydecoupledspatialandtemporalfeaturesinto
toproduceimagesconsistentwiththeconditionmap.Follow-Your-
Recurrent-CausalAttentionandTemporalAttention,respectively.
Pose[26]controlsvideogenerationgivenhumanskeletons.Ituses
Therefore,weonlyinjectthekeyandvalueofRecurrent-Causal
atwo-stagetrainingtolearntoposeandcontroltemporalcon-
Attentionfromthereconstructionbranchintotheeditingbranch,
sistency.ControlVideo[64]isadaptedfromControlNetanduses
eliminatingtheneedforthesegmentationmask.
cross-frameinteractiontoconstrainappearancecoherencebetween
Inconclusion,ourcontributionsareasfollows:
frames.Control-A-Video[65]enhancesfaithfulnessandtemporal
â€¢ Wefurtherexploredhowtodecouplespatio-temporalfea- consistencybyfine-tuningtheattentionmodulesinboththediffu-
turesinvideomotioneditingexplicitlyandproposedaone- sionmodelsandControlNet.
shotvideomotioneditingmethodnamedEdit-Your-Motion. Unlike the pose-guided video generation model, the motion-
â€¢ WedesignedtheDetailedPrompt-GuidedLearningStrat- customizationvideogenerationmodelgeneratesvideoswiththe
egy(DPL),atwo-stagetrainingmethod.Itcandecouplethe samemotionbylearningthemotionfeaturesinthesourcevideo.
space-timediffusionmodelâ€™soverlappingspatialandtempo- Customize-A-Video[35]designedanAppearanceAbsorbermodule
ralfeatures,therebyavoidinginterferencefrombackground todecomposethespatialinformationofmotion,thusdirectingthe
featuresduringtheeditingobjectâ€™smotion. TemporalLoRA[16]tolearnthemotioninformation.MotionCrafter
â€¢ WedesignedRecurrent-CausalAttentiontoassistDPLin [66]customizesthecontentandmotionofthevideobyinjectingmo-
learningthemorecomprehensivecontentofobjectsinthe tioninformationintoU-Netâ€™stemporalattentionmodulethrough
firsttrainingstage.Inaddition,WeconstructedtheNoise aparallelspatial-temporalarchitecture.VMC[17]fine-tunesonly
ConstraintLosstosmoothoutinter-framedifferencesinthe thetemporalattentionlayerinthevideodiffusionmodeltoachieve
secondtrainingstage. successfulmotioncustomization.
â€¢ Weconductexperimentsonin-the-wildvideos,wherethe Unlikethesemethods,videomotioneditingrequirescontrolling
resultsshowthesuperiorityofourmethodcomparedwith themotionofthesourcevideoobjectwhilemaintainingitscontent
thestate-of-the-art. andbackground.
2.3 VideoEditing
2 RELATEDWORK
Thecurrentvideoeditingmodelscanbedividedintotwocategories:
Inthissection,weprovideabriefoverviewofthefieldsrelatedto
video content editing models [1, 5, 20, 24, 32, 51, 67] and video
videomotioneditingandpointouttheconnectionsanddifferences
motioneditingmodels[45].Thevideocontenteditingmodelis
betweenthemandvideomotionediting.
designedtomodifythebackgroundandobjectâ€™scontent(e.g.,the
sceneinthebackground,theclothescolour,thevehicleâ€™sshape,
2.1 ImageEditing
etc.)inthesourcevideo.
Recently,alargeamountofworkhasbeendoneonimageediting Invideocontentediting,Tune-A-Video[51]introducestheOne-
usingdiffusionmodels[7,30,36].SDEdit[28]isthefirstmethod ShotVideoTuningtaskforthefirsttime,whichtrainsthespace-
forimagesynthesisandeditingbasedondiffusionmodels.Prompt- timediffusionmodelbyasingletext-videopair.FateZero[32]uses
to-Prompt[13]editsimagesbyreferencingcross-attentioninthe cross-attention maps to edit the content of videos without any
diffusionprocess.Plug-and-play[46]providesfine-grainedcontrol training.Mix-of-show[12]fine-tunethemodelthroughlow-rank
overthegenerativestructurebymanipulatingspatialfeaturesdur- adaptions[16](LoRA)topreventthecrashofknowledgelearned
inggeneration.UniTune[47]completestext-conditionedimage bythepre-trainedmodel.Someotherapproaches[2,5,20]use
editingtasksbyfine-tuning.Fornon-rigidlytransformedimage NLA[18]mappingtomapthevideotoa2Datlastodecouplethe
editing, Imagic [19] preserves the overall structure and compo- objectcontentfromthebackgroundtoeditthecontentoftheobject
sitionoftheimagebylinearlyinterpolatingbetweentexts,thus effectively.
accomplishingnon-rigideditingwhile.Masactrl[4]convertsself- In video motion editing, MotionEditor [45] uses the objectâ€™s
attentiontomutualself-attentionfornon-rigidimageediting.On segmentationmasktodecouplethecontentandbackgroundinthe
theotherhand,InstructPix2Pix[3]hasdevisedamethodofedit- featurelayer.Contentfeaturesaretheninjectedintotheediting
ingimagesbywritteninstructionsratherthantextualdescriptions branchtomaintaincontentconsistency.Sincetheobjectandthe
ofimagecontent.Unliketext-drivenimageediting,DreamBooth backgroundoverlapinthefeaturelayer,itisdifficulttoaccurately
[38]generatesnewimageswiththemeattributesbyusingseveral separatetheobjectâ€™scontentfromthebackgroundfeatureswith
differentimagesofagiventheme. thesegmentationmask.ACMMM,2024,Melbourne,Australia YiZuo,LinglingLi,LichengJiao,FangLiu,XuLiu,WenpingMa,ShuyuanYang,andYuweiGuo
Ourapproachdecouplestheobjectfromthebackgroundduring LatentDiffusionModels.LatentDiffusionmodels(LDM)[29,
thetrainingstageanddirectsRC-AttnandTemporalAttentionto 36,59]isanewlyintroducedvariantofDDPMthatoperatesin
learnspatialandtemporalfeatures,respectively.Thisensuresthat the latent space of the autoencoder. Specifically, the encoder E
thesourcevideocontentisaccuratelyinjected. compressestheimagetolatentfeaturesğ’›=E(ğ’™).Thenperforms
adiffusionprocessoverğ‘§,andfinallyreconstructslatentfeatures
3 METHOD back into pixel space using the decoder D. The corresponding
objectivecanberepresentedas:
In video motion editing, the focus is on decoupling the spatio-
tem Tp oo tr ha il sf ee na dtu ,r we eso pf roth pe osd eiff Eu ds iti -o Yn oum ro -Mde ol t.
ion,aone-shotvideomo-
ğ¿ ğ¿ğ·ğ‘€ =E E(ğ‘¥),ğœ–âˆ¼N(0,1),ğ‘¡(cid:104) âˆ¥ğœ–âˆ’ğœ– ğœƒ(ğ‘§ ğ‘¡,ğ‘¡)âˆ¥2 2(cid:105) . (4)
tioneditingmethodtrainedonlyonapairofsourceandreference Text-to-VideoDiffusionModels.Text-to-VideoDiffusionModels
videos.Specifically,wedesigntheDetailedPrompt-GuidedLearn- [43]traina3DUNetğœ–3ğ· withtextpromptsğ‘ asaconditionto
ğœƒ
ingstrategy(DPL),atwo-stagelearningstrategycapableofdecou- generatevideosusingtheT2Vmodel.Giventheğ¹ framesğ’™1...ğ¹ of
plingspatio-temporalfeaturesinthespace-timediffusionmodel.In
avideo,the3DUNetistrainedby
thefirsttrainingstage,weshufflethevideoframestodisruptthe
t lee am rp no ir na tl er ne tl la yti so pn as th iaip lfo ef at th ue rev sid (oe bo j. eT ch te cn o, nm tea ns tk )ft rh oe mba tc hk eg uro nu on rdd ea rn edd ğ¿ ğ‘‡2ğ‘‰ =E E(ğ‘¥1...ğ¹),ğœ–âˆ¼N(0,1),ğ‘¡,ğ‘ (cid:20)(cid:13) (cid:13) (cid:13)ğœ–âˆ’ğœ– ğœƒ3ğ· (ğ‘§ ğ‘¡1...ğ¹,ğ‘¡,ğ‘)(cid:13) (cid:13) (cid:13)2 2(cid:21) , (5)
f inra sm tee as d. oW fe Spfu ar rsth e-e Cr ap ur so ap lo Ase ttR ene tc iu or nre tont c- oC na su ts ra ul cA tct ote nn st ii so ten n( tR fC ea-A tut rt en s) whereğ‘§ ğ‘¡1...ğ¹ isthelatentfeaturesofğ’™1...ğ¹ ,ğ‘§ ğ‘¡1...ğ¹ =E(ğ’™1...ğ¹).
ofobjectsoverthewholesequence.Inthesecondtrainingstage,we
3.2 Recurrent-CausalAttention
recoverthetemporalrelationshipsinthevideoframestolearnthe
temporalfeatures(thebackgroundandobjectmotion).Tosmooth LikeTune-A-Video[51],weusetheinflatedU-Netnetwork(space-
outtheinter-framedifferences,wealsoconstructNoiseConstraint timediffusionmodel)asthebackboneofEdit-Your-Motion,con-
Loss.Finally,intheinferencestage,weusethedeconstructionwith sistingofstacked3Dconvolutionalresidualblocksandtransform
a two-branch structure [66] (reconstruction branch and editing blocks.EachtransformerblockconsistsofSparse-CausalAtten-
branch).Sincethespatialandtemporalfeatureshavebeendecou- tion,CrossAttention,TemporalAttention,andaFeed-Forward
pledinthetrainingstage,weobtainthebackgroundandmotion Network(FFN).Tosavecomputationaloverhead,Tune-A-Video
featuresintheeditingbranchandinjectthecontentfeaturesofthe usesthecurrentframelatentğ‘§ ğ‘£ğ‘– âˆˆ (cid:8)ğ‘§ ğ‘£ 0,...,ğ‘§ ğ‘£ğ‘–ğ‘šğ‘ğ‘¥(cid:9) asthequery
objectsinthereconstructionbranchintotheeditingbranch.Fig.2 forSparse-CausalAttention.Meanwhile,thepreviousframelatent
illustratesthepipelineofEdit-Your-Motion. ğ‘§ ğ‘£ğ‘–âˆ’1 iscombinedwiththefirstframelatentğ‘§ ğ‘£
1
toobtainthekey
TointroduceourproposedEdit-Your-Motion,wefirstintroduce andvalue.Thespecificformulaisasfollows:
thebasicsofthetext-videodiffusionmodelinSec.3.1.Then,Sec.3.2
Ain ft tr eo rd tu hc ae ts ,io nu Sr ep cr .o 3p .3o ,s oe ud rR pe rc ou pr ore sn edt-C Da eu tas ia ll edAt Pt re on mtio pn t-G(R uC id-A edtt Len eati ro n) -. ğ‘„ =ğ‘Šğ‘„ğ‘§ ğ‘£ğ‘–,ğ¾ =ğ‘Šğ¾ (cid:2)ğ‘§ ğ‘£ 1,ğ‘§ ğ‘£ğ‘–âˆ’1(cid:3),ğ‘‰ =ğ‘Šğ‘‰ (cid:2)ğ‘§ ğ‘£ 1,ğ‘§ ğ‘£ğ‘–âˆ’1(cid:3), (6)
ingstrategyandNoiseConstraintLossaredescribed.Finally,we where [Â·] denotesconcatenationoperation.whereğ‘Šğ‘„ ,ğ‘Šğ¾ and
willintroducetheinferencestageinSec.3.4. ğ‘Šğ‘‰ areprojectionmatrices.However,becausethereislessinfor-
mationintheearlyframesofavideo,Sparse-CausalAttentiondoes
3.1 Preliminaries notconsidertheconnectionwiththesubsequentframes.Asaresult,
DenoisingDiffusionProbabilisticModels.Thedenoisingdif- itmayleadtoinconsistenciesbetweenthecontentatthebeginning
fusionprobabilisticmodels[11,14,27,55](DDPMs)consistsofa andtheendofthevideo.
forwarddiffusionprocessandareversedenoisingprocess.During Tosolvethisproblem,weproposeasimpleRecurrent-CausalAt-
theforwarddiffusionprocess,itgraduallyaddsnoiseğœ–toaclean tentionwithnoincreaseincomputationalcomplexity.InRecurrent-
imageğ’™0 âˆ¼ğ‘(ğ’™0) withtimestepğ‘¡,obtaininganoisysampleğ‘¥ ğ‘¡. CausalAttention,keyandvalueareobtainedbycombiningthe
Theprocessofaddingnoisecanberepresentedas:
previousframelatentğ‘§ ğ‘£ğ‘–âˆ’1withthecurrentframelatentğ‘§ ğ‘£ğ‘–,notğ‘§
ğ‘£ 1
ğ‘(ğ’™ğ‘¡|ğ’™ğ‘¡âˆ’1)=N(ğ’™ğ‘¡|âˆšï¸ 1âˆ’ğ›½ ğ‘¡ğ’™ğ‘¡âˆ’1,ğ›½ ğ‘¡I), (1) w ari eth obğ‘§ tğ‘£ ağ‘–âˆ’ in1. eN do frt oab mly t, ht ehe lak stey fraa mnd ev laa tl eu ne to ğ‘§f ğ‘£ğ‘–t ğ‘šh ğ‘e ğ‘¥fi wrs it thfr ta hm ee fil ra st te fn rt amğ‘§ ğ‘£ e1
whereğ›½ ğ‘¡ âˆˆ (0,1)isavarianceschedule.Theentireforwardprocess latentğ‘§ ğ‘£ 1.Thisallowstheobjectâ€™scontenttopropagatethroughout
thevideosequencewithoutaddinganycomputationalcomplexity.
ofthediffusionmodelcanberepresentedasaMarkovchainfrom
TheformulaforRecurrent-CausalAttentionisasfollows:
timeğ‘¡ totimeğ‘‡,
ğ‘‡ ğ‘„ =ğ‘Šğ‘„ğ‘§ ğ‘£ğ‘–, (7)
(cid:214)
ğ‘(ğ’™1:ğ‘‡)=ğ‘(ğ’™0) ğ‘(ğ’™ğ‘¡|ğ’™ğ‘¡âˆ’1). (2)
Then,inreverseprocessing,noğ‘¡ i= s1
eisremovedthroughadenois- ğ¾
=(cid:40) ğ‘Šğ‘Š ğ¾ğ¾ (cid:2)(cid:2) ğ‘§ğ‘§ ğ‘£ğ‘–âˆ’
,1
ğ‘§,ğ‘§ ğ‘£ (cid:3)ğ‘–(cid:3) ğ‘’if ğ‘™ğ‘ ğ‘– ğ‘’<ğ‘– ğ‘šğ‘ğ‘¥
, (8)
ingautoencodersğœ– ğœƒ(ğ‘¥ ğ‘¡,ğ‘¡) togenerateacleanimage.Thecorre-
ğ‘£
0
ğ‘£ğ‘–
spondingobjectivecanbesimplifiedto:
ğ‘‰
=(cid:40) ğ‘Šğ‘‰ (cid:2)ğ‘§ ğ‘£ğ‘–âˆ’1,ğ‘§ ğ‘£ğ‘–(cid:3) ifğ‘– <ğ‘–
ğ‘šğ‘ğ‘¥
. (9)
ğ¿ ğ·ğ‘€ =E ğ‘¥,ğœ–âˆ¼N(0,1),ğ‘¡ (cid:2) âˆ¥ğœ–âˆ’ğœ– ğœƒ(ğ‘¥ ğ‘¡,ğ‘¡)âˆ¥2 2(cid:3). (3) ğ‘Šğ‘‰ (cid:2)ğ‘§ ğ‘£ 0,ğ‘§ ğ‘£ğ‘–(cid:3) ğ‘’ğ‘™ğ‘ ğ‘’Edit-Your-Motion:Space-TimeDiffusionDecouplingLearningforVideoMotionEditing ACMMM,2024,Melbourne,Australia
The First Training Stage: Learning Spatial Features from Inference Stage: A Two-Branch
Shuffled Images Structure that Injects Spatial Features
Source Reference
video video
Unordered P
a
Frames
nn nn
ControlNet
nn CCttttAA
-- ss ss
oottttAA
--
pptttt mmAA
-- P a P t
RR rr CC ee TT
P
a â€œA boy wearing black clothes and gray pants.â€
C
o
The Second Trainin Og rS dt ea rg ee d: VL ie da er on i Fn rg
a
T me em sporal Feature from h c
n
a Br h c
n
eNlortn
n o
itc k
ga Br t
u r ts
n
o
V n iti
Ed
c
e
R P
t
P
Ordered s
Frames
nn nn â€œA boy
ControlNet
nn CCttttAA
-- ss ss
oottttAA
--
pptttt mmAA
--
w ce la or ti hn eg
s
b al na dc k
RR rr CC ee TT gray pants is
dancing.â€
S
sr
P sâ€œA boy wearing black clothes and gray Pants is playing basketball.â€ Edited video
Figure 2: The overall pipline of Edit-Your-Motion. Edit-Your-Motion decouples spatial features (object appearance) from
temporalfeatures(backgroundandmotioninformation)ofthesourcevideousingtheDetailedPrompt-GuidedLearning
Strategy(DPL).Inthefirsttrainingstage,Recurrent-Causalattention(RC-Attn)isguidedtolearnspatialfeatures.Inthesecond
trainingstage,TemporalAttention(Temp-Attn)isguidedtolearntemporalfeatures.Duringinference,thespatialfeaturesof
thesourcevideoareinjectedintotheeditingbranchthroughthekeyandvalueofRecurrent-CausalAttention,thuskeeping
thesourcecontentandbackgroundunchanged.
Overall, Recurrent-Causal Attention enables early frames to In order to be able to decouple overlapping spatio-temporal
acquire more comprehensive content information compared to features,wedesigntheDetailedPrompt-GuidedLearningStrategy
Sparse-CausalAttention,byestablishingalinktothelastframein (DPL).
thefirstframe. DPLisdividedintotwotrainingstages:(1)TheFirstTraining
Stage:LearningSpatialFeaturesfromShuffledImages,and(2)The
SecondTrainingStage:LearningTemporalFeaturesfromOrdered
videoframes.Next,wewilldescribethetwostagesindetail.
3.3 TheDetailedPrompt-GuidedLearning
TheFirstTrainingStage:LearningSpatialFeaturesfromShuf-
Strategy fledImages.Inthisstage,thespace-timediffusionmodelfocuses
Thepurposeofdiffusion-basedvideomotioneditingistocontrol onlearningthespatialfeaturesofthesourceobject.First,wedisrupt
themotionofobjectsinthesourcevideobasedonareferencevideo theorderofvideoframestodestroytheirtemporalinformation
withapromptandtoensurethatthecontentandbackgroundofthe
andgenerateunorderedvideoframesU ={ğ‘¢ ğ‘–|ğ‘– âˆˆ [1,ğ‘›]},whereğ‘›
objectsremainunchanged.Thekeyliesindecouplingthediffusion isthelengthofthevideo.
modelâ€™soverlappingtemporalandspatialfeatures.MotionEditor Ifwetrainthemodeldirectlyusingunorderedframes,thefea-
usestheobjectâ€™ssegmentationmasktodecoupletheobjectcontent turesoftheobjectandthebackgroundwilloverlap.Suchoverlap-
andthebackgroundinthefeaturelayer.However,thedecoupled pingspatio-temporalfeaturesarechallengingtodecouplelaterand
featuresalsooverlapsincethespatio-temporalfeatureshavebeen willleadtointerferencefrombackgroundfeatureswhencontrol-
obfuscatedinthemodel. lingobjectmotion.Therefore,weuseanexistingsegmentation
S
sr
C
C
rf
sr
S
C
rf
rfACMMM,2024,Melbourne,Australia YiZuo,LinglingLi,LichengJiao,FangLiu,XuLiu,WenpingMa,ShuyuanYang,andYuweiGuo
networktoextractthesegmentationmaskğ‘€ fortheunordered
videoframes.Therefore,weuseanexistingsegmentationnetwork ğ¶ ğ‘Ÿğ‘“ =ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ğ‘ğ‘’ğ‘¡(ğ‘† ğ‘Ÿğ‘“,ğ‘ƒ ğ‘¡), (16)
toextractthesegmentationmaskMforthevideoframesandmask whereğ¶ ğ‘Ÿğ‘“ istheposefeatureofthereferencevideotobeusedto
outthebackgroundas:
guidethegenerationofmotionintheeditingbranch.Next,wewill
injectthespatialfeaturesfromthereconstructionbranchintothe
UM =UÂ·M, (10) editingbranch.Duetodisruptingthetimerelationshipandmask
thebackgroundinthefirsttrainingstageofDPL.Therefore,we
Z ğ‘¡M =E(UM), (11) directlyinjectthekeysandvaluesoftheRC-Attninthereconstruc-
whereZ ğ‘¡MisthelatentfeaturesofUM,andE(Â·)isencoder.Then, tionbranchintotheeditingbranchwithoutneedingsegmentation
masks.Thespecificformulacanbewrittenas:
weutilizeanexistingskeletonextractionnetworktoobtainthe
h alu om ngan ws ik thele thto en pğ‘† rğ‘  oğ‘Ÿ mi pn tt ğ‘ƒh ğ‘e .sourcevideoandfeeditintoControlNet ğ¾ğ‘Ÿ =ğ‘Šğ¾ğ‘§ğ‘  ğ‘£ğ‘–,ğ‘‰ğ‘Ÿ =ğ‘Šğ‘‰ğ‘§ğ‘  ğ‘£ğ‘–, (17)
ğ¶ ğ‘ ğ‘Ÿ =ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ğ‘ğ‘’ğ‘¡(ğ‘† ğ‘ ğ‘Ÿ,ğ‘ƒ ğ‘), (12)
ğ¾ğ‘’ = (cid:104) ğ‘Šğ¾ğ‘§ğ‘’ ğ‘£ğ‘–âˆ’1,ğ‘Šğ¾ğ‘§ğ‘’ ğ‘£ğ‘–,ğ¾ğ‘Ÿ(cid:105) ,ğ‘‰ğ‘’ = (cid:104) ğ‘Šğ‘‰ğ‘§ğ‘’ ğ‘£ğ‘–âˆ’1,ğ‘Šğ‘‰ğ‘§ğ‘’ ğ‘£ğ‘–,ğ‘‰ğ‘Ÿ(cid:105) , (18)
ğ‘‰ğ‘Ÿ whereğ‘’representstheeditingbranch.ğ‘Ÿ representstherecon-
whereğ¶ ğ‘ ğ‘Ÿ istheposefeatureofsourcevideo.Next,wewillfreeze
structionbranch.Intheend,weobtainedtheeditedvideo.
otherparametersandonlyactivateRecurrent-CausalAttention.
Finally,wewillğ‘ƒ ğ‘andğ¶ ğ‘ ğ‘Ÿ intothespace-timediffusionmodelfor
4 EXPERIMENTAL
training.Thereconstructionlosscanbewrittenasfollows:
4.1 ImplementationDetails
ğ¿ ğ‘Ÿğ‘’ğ‘ =E ğ‘§ğ‘š
ğ‘¡
,ğœ–âˆ¼N(0,1),ğ‘¡,ğ‘ƒğ‘,ğ¶ğ‘ ğ‘Ÿ (cid:20)(cid:13) (cid:13) (cid:13)ğœ–âˆ’ğœ– ğœƒ3ğ· (ğ‘§ğ‘š ğ‘¡ ,ğ‘¡,ğ‘ƒ ğ‘,ğ¶ ğ‘ ğ‘Ÿ)(cid:13) (cid:13) (cid:13)2 2(cid:21) . (13) O Mu or dep lro [3p 6o ]s (e Sd taE bd ei lt- DYo iffu ur s-M iono )t .io Tn heis db aa ts ae id no tn hit sh ae rtL ia clt een ct omDi eff su fs ri oo mn
TaichiHD[40]andYouTubevideodatasets,inwhicheachvideohas
TheSecondTrainingStage:LearningTemporalFeaturesfrom
aminimumof70frames.Duringtraining,wefinetune300steps
OrderedVideoFrames.Unlikethefirsttrainingstage,werestored
foreachofthetwotrainingstagesatalearningrateof3Ã—10âˆ’5.
thetemporalrelationshipofvideoframes.Then,guidethespace-
Forinference,weusedtheDDIMsampler[42]withnoclassifier
timediffusionmodeltolearnthetemporalfeaturesofmotionand
backgroundfromorderedvideoframesV ={ğ‘£ ğ‘–|ğ‘– âˆˆ [1,ğ‘›]}. guidance[15]inourexperiments.Foreachvideo,thefine-tuning
Specifically,Weconstructanewpromptğ‘ƒ ğ‘ ,whichaddsadescrip- takesabout15minuteswithasingleNVIDIAA100GPU.
tionofthemotiontoğ‘ƒ ğ‘.Then,TemporalAttentionisactivated
4.2 ComparisonsMethod
to learn motion features while other parameters are frozen. To
smooththevideo,weaddedNoiseConstraintLoss[31].Thenoise TodemonstratethesuperiorityofourEdit-Your-Motion,wehave
constraintlosscanbewrittenasfollows: selectedmethodsfrommotioncustomization,pose-guidedvideo
generation, video content editing, and video motion editing as
ğ¿ ğ‘›ğ‘œğ‘–ğ‘ ğ‘’ =
ğ‘›âˆ’1 1ğ‘› âˆ‘ï¸ ğ‘–=âˆ’ 11(cid:13)
(cid:13) (cid:13)ğœ–
ğ’›ğ‘“ğ‘–
ğ‘¡ âˆ’ğœ–
ğ’›ğ‘“ğ‘– ğ‘¡+1(cid:13)
(cid:13)
(cid:13)2
2, (14)
wc mo
o
om
r
dkp ea lor tfi ooso
n
3n De-m
s th
oe ot hh
t
avo nid
d
ds
e
l. eo(1
te
h)
d
eT itu
vin
in dge e.- oIA
t
t-
i
aV
n sfl
kid
a
.te
(e
2o
s
)[
a
M5 p1 or] te: i-T oth
r
nae Einfi der ids tt oTp
r2
1r Ie [ds 4e
i
5ffn ]ut :s
s
Tit
o
hh
n
ee
whereğ‘“ ğ‘– denotetheğ‘–-thframeofthevideo.ğœ–
ğ’›ğ‘“ğ‘–
ğ‘¡
isthenoisepredic- firstexaminestheworkofvideomotioneditingwhilemaintaining
tionattimestepğ‘¡.Thetotallossforthesecondtrainingstageis theobjectcontentandbackgroundunchanged.(3)Follow-Your-
constructedasfollows: Pose[26]:Generatingpose-controllablevideosusingtwo-stage
training.(4)MotionDirector[66]:Generatemotion-alignedvideos
ğ¿ ğ‘‡ğ‘œğ‘¡ğ‘ğ‘™ =(1âˆ’ğœ†)ğ¿ ğ‘›ğ‘œğ‘–ğ‘ ğ‘’ +ğœ†ğ¿ ğ‘Ÿğ‘’ğ‘, (15) bydecouplingappearanceandmotioninreferencevideosforvideo-
whereğ¿ ğ‘Ÿğ‘’ğ‘ isconstructedfromorderedvideoframesV without motion-customization.
segmentationmaskğ‘€.ğœ†issetto0.9.
4.3 Evaluation
3.4 InferencePipelines Ourmethodcaneditthemotionofobjectsinthesourcevideobyus-
Intheinferencestage,wefirstextractthehumanskeletonğ‘† ğ‘Ÿğ‘“ from ingthereferencevideoandpromptingwithoutchangingtheobject
contentandthebackground.Fig.4showssomeofourexamples.
thereferencevideotoguidemotiongeneration.Then,toensure
Ascanbeseen,ourproposedEdit-Your-Motionaccuratelycontrols
thattheobjectâ€™scontentandbackgroundareunchanged,weusea
themotionandpreservestheobjectâ€™scontentandbackgroundwell.
two-brancharchitecture(reconstructionbranchandeditingbranch)
Themorecasesareintheappendix.
similarto[45]toinjecttheobjectâ€™scontentandbackgroundfeatures
QualitativeResults.Fig.3showstheresultsofthevisualcom-
intotheeditingbranch.
Specifically,wefirstinputthelatentnoiseğ‘§ğ‘ 
fromthesource
parisonofEdit-Your-Motionwithothercomparisonmethodson25
videoDDIMinversionandğ‘ƒ ğ‘ intothereconstructionbranch.Si- in-the-wildcases.AlthoughFollow-Your-PoseandMotionDirector
multaneouslyinputğ‘§ğ‘  andğ‘ƒ ğ‘¡ intotheeditingbranch.Then,we canalignwellwiththemotionofthereferencevideo,itisdifficultto
willinputthehumanskeletonğ‘†
ğ‘Ÿğ‘“
fromthereferencevideoandğ‘ƒ
ğ‘¡ 1Sincethearticleâ€™scodeisnotprovided,theexperimentalresultsinthispaperare
intoControlNettoobtainfeatureğ¶ ğ‘Ÿğ‘“ as: obtainedbyreplication.Edit-Your-Motion:Space-TimeDiffusionDecouplingLearningforVideoMotionEditing ACMMM,2024,Melbourne,Australia
0 2 22 4 8 12 16
6
Source video
Reference video
Follow-Your-Pose
MotionDirector
Tune-A-Video
MotionEditor
Ours
A girl in a plaid top and black skirt is dancing A boy with a black top and gray pants is playing basketball
practicing wugong. dancing.
0 2 22
Figure3:Qualitativecomparisonwithstate-of-the-artme6thods.Comparedtootherbaselines,Edit-Your-Motionsuccessfully
achievesmotionalignmentwiththereferencevideoandmaintainsthecontentconsistencyofthebackgroundandobjects.
Source video
maintainconsistencybetweentheobjectcontentandbackground Table1:QuantitativeevaluationusingCLIPandLPIPS.TA,
inboththesourceandreferencevideos.Itdemonstratesthatgen- TC, L-N, L-S represent Text Alignment, Temporal Consis-
eratingspecificbackgroundandcontentusingonlytextpromptsis tency,LPIPS-NandLPIPS-S,respectively.
Reference video
difficult.Tune-A-VideoandMotionEditorshownoticeablecontent
changes.Inaddition,MotionEditorshowsmotionoverlap(arms) Method TAâ†‘ TCâ†‘ L-Nâ†“ L-Sâ†“
causedbyusingofthesegmentationmasktodecoupleoverlapping
Follow-Your-Pose[26] 0.236 0.913 0.213 0.614
features.Incontrasttotheabove,ourproposedEdit-Your-Motion
Follow Your Pose MotionDirector[66] 0.239 0.872 0.141 0.430
alignsthemotionoftheeditedvideoandthereferencevideowell
Tune-A-Video[51] 0.278 0.934 0.137 0.359
andpreservesthecontentandbackgroundoftheobjectsinthe
MotionEditor[45] 0.286 0.948 0.102 0.300
sourcevideointact.Thisalsodemonstratestheeffectivenessofour
Ours 0.289 0.950 0.109 0.276
methodinvideomotionediting.
MotionDirector
Quantitativeresults.Weevaluatethemethodswithautomatic
evaluationsandhumanevaluationson25in-the-wildcases.
AutomaticEvaluations.Toquantitativelyassessthedifferences
betweenourproposed TuE nd e-i At -- VY io du eor-Motionandothercomparative Similaritybetweeneditedframesandsourceframes.Table1shows
methods,weusethefollowingmetricstomeasuretheresults:(1) thequantitativeresultsofEdit-Your-Motionwithothercomparative
TextAlignment(TA).WeuseCLIP[34]tocomputetheaverage methods.TheresultsshowthatEdit-Your-Motionoutperformsthe
cosinesimilaritybetweenthepromptandtheeditedframes.(2) othermethodsonallmetrics.
TemporalConsistency(MToCti)o.nWEdeitourseCLIPtoobtainimagefeatures UserStudy.Weinvited70participantstoparticipateintheuser
andcomputetheaveragecosinesimilaritybetweenneighbouring study.Eachparticipantcouldseethesourcevideo,thereference
videoframes.(3)LPIPS-N(L-N):WecalculateLearnedPerceptual video,andtheresultsofourandothercomparisonmethods.For
ImagePatchSimilarity[62]betweeneditedneighbouringframes. eachcase,wecombinedtheresultsofEdit-Your-Motionwiththe
(4)LPIPS-S(L-S):WecalcOuularsteLearnedPerceptualImagePatch resultsofeachofthefourcomparisonmethods.Then,wesetthreeACMMM,2024,Melbourne,Australia YiZuo,LinglingLi,LichengJiao,FangLiu,XuLiu,WenpingMa,ShuyuanYang,andYuweiGuo
A woman in a blue top and white skirt is waving her hand dancing. A girl with a black top and black skirt is dancing practicing Tai Chi.
A boy wearing black clothes and gray pants is playing basketball dancing. A man with a dark green top and black pants is standing practicing Tai Chi.
Figure4:SomeexamplesofmotioneditingresultsforEdit-Your-Motion. 22
6 12 18
0
Table2:UserStudy.Higherindicatestheusersprefermoreto thatRC-Attncanbetterestablishcontentconsistencyovertheen-
ourMotionEditor.TA,CA,andMArepresentTextAlignment, tiresequencethanwithSparseAttention.Incolumn4,w/oNoise
ContentAlignment,andMotionAlignment,respectively. ConstraintLoss(NCL)affectsthesmoothnessbetweenframes,caus-
ingthebackgroundtobeinconsistentbetweenframes.Incolumn5,
Method TA CA MA wetrainRC-AttnandTemporalAttentioninatrainingstage.How-
ever,thelackofspatio-temporaldecouplingresultsinbackground
Follow-Your-Pose[26] 87.142% 96.663% 90.953%
andobjectcontentinterfering,generatingundesirableeditedvideos.
MotionDirector[66] 94.522% 96.190% 86.188%
Atthesametime,italsodemonstratestheeffectivenessofDPLin
Tune-A-Video[51] 78.810% 82.145% 84.047%
decouplingtimeandspace.
MotionEditor[45] 76.428% 82.380% 80.950%
questionstoevaluateTextAlignment,ContentAlignmentandMo-
tionAlignment.Thethreequestionsare"Whichismorealigned
tothetextprompt?","Whichismorecontentalignedtothesource
video?"and"Whichismoremotionalignedtothereferencevideo?".
Table2showsthatourmethodoutperformstheothercompared
methodsinallthreeaspects.
4.4 AblationStudy
Toverifytheeffectivenessoftheproposedmodule,weshowthe
resultsoftheablationexperimentsinFig.5.Incolumn3,wereplace
RC-AttnwithSparseAttention,whichmakesthefirstframeincon-
sistentwiththeobjectcontentinthesubsequentframes.Thisshows4488
4400 4444 5588
Edit-Your-Motion:Space-TimeDiffusionDecouplingLearningforVideoMotionEditing ACMMM,2024,Melbourne,Australia
Source video
Source video Reference video w/o RCA w/o NCL w/o DPT Edit-Your-Motion
[5] WenhaoChai,XunGuo,GaoangWang,andYanLu.2023. Stablevideo:Text-
drivenconsistency-awarediffusionvideoediting.InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision.23040â€“23050.
Reference video 40 [6] YangChen,YingweiPan,YehaoLi,TingYao,andTaoMei.2023. Control3d:
Towardscontrollabletext-to-3dgeneration.InProceedingsofthe31stACMInter-
nationalConferenceonMultimedia.1148â€“1156.
[7] Florinel-AlinCroitoru,VladHondru,RaduTudorIonescu,andMubarakShah.
2023.Diffusionmodelsinvision:Asurvey.IEEETransactionsonPatternAnalysis
w/o RCA 44 andMachineIntelligence(2023).
[8] HanFang,KejiangChen,YupengQiu,JiayangLiu,KeXu,ChengfangFang,
WeimingZhang,andEe-ChienChang.2023.DeNoL:AFew-Shot-Sample-Based
DecouplingNoiseLayerforCross-channelWatermarkingRobustness.InPro-
ceedingsofthe31stACMInternationalConferenceonMultimedia.7345â€“7353.
w/o loss2 48 [9] RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHBermano,Gal
Chechik,andDanielCohen-Or.2022.Animageisworthoneword:Personalizing
text-to-imagegenerationusingtextualinversion.arXivpreprintarXiv:2208.01618
(2022).
[10] YifanGao,JinpengLin,MinZhou,ChuanbinLiu,HongtaoXie,TiezhengGe,
w/o DPT 58 andYuningJiang.2023.TextPainter:MultimodalTextImageGenerationwith
Visual-harmonyandText-comprehensionforPosterDesign.InProceedingsof
the31stACMInternationalConferenceonMultimedia.7236â€“7246.
A girl with a black top and black shorts is waving her hand dancing. [11] DeepanwayGhosal,NavonilMajumder,AmbujMehrish,andSoujanyaPoria.
2023.Text-to-AudioGenerationusingInstructionGuidedLatentDiffusionModel.
ACE InProceedingsofthe31stACMInternationalConferenceonMultimedia.3590â€“3598.
Figure5:Someexamplesofvideomotioneditingresultsfor [12] YuchaoGu,XintaoWang,JayZhangjieWu,YujunShi,YunpengChen,Zihan
Edit-Your-Motion. Fan,WuyouXiao,RuiZhao,ShuningChang,WeijiaWu,etal.2024.Mix-of-show:
Decentralizedlow-rankadaptationformulti-conceptcustomizationofdiffusion
models.AdvancesinNeuralInformationProcessingSystems36(2024).
[13] AmirHertz,RonMokady,JayTenenbaum,KfirAberman,YaelPritch,andDaniel
5 CONCLUSION Cohen-Or.2022.Prompt-to-promptimageeditingwithcrossattentioncontrol.
arXivpreprintarXiv:2208.01626(2022).
Inthispaper,weexploremethodstoseparatethelearningoftempo-
[14] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilistic
ralandspatialfeaturesinspace-timediffusionmodels.Tothisend, models.Advancesinneuralinformationprocessingsystems33(2020),6840â€“6851.
weproposeaone-shotvideomotioneditingmethodcalledEdit- [15] JonathanHoandTimSalimans.2022.Classifier-freediffusionguidance.arXiv
preprintarXiv:2207.12598(2022).
Your-Motionthatrequiresonlyasingletext-videopairfortraining. [16] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,Shean
Specifically,wedesigntheDetailedPrompt-GuidedLearningStrat- Wang,LuWang,andWeizhuChen.2021.Lora:Low-rankadaptationoflarge
languagemodels.arXivpreprintarXiv:2106.09685(2021).
egy(DPL)todecouplethediffusionmodelâ€™sspace-timefeaturesin
[17] HyeonhoJeong,GeonYeongPark,andJongChulYe.2023.VMC:VideoMotion
twotrainingstages.Furthermore,weproposeRecurrent-CausalAt- CustomizationusingTemporalAttentionAdaptionforText-to-VideoDiffusion
tention(RC-Attn)asanenhancementoverSparse-CausalAttention. Models.arXivpreprintarXiv:2312.00845(2023).
[18] YoniKasten,DolevOfri,OliverWang,andTaliDekel.2021. Layeredneural
Inthefirsttrainingstage,RC-Attnfocusesonlearningthespatial
atlasesforconsistentvideoediting.ACMTransactionsonGraphics(TOG)40,6
featurebyshufflingthetemporalrelations.Inthesecondtraining (2021),1â€“12.
stage,weguidetheTemporalAttentiontolearntemporalfeatures. [19] BahjatKawar,ShiranZada,OranLang,OmerTov,HuiwenChang,TaliDekel,
InbarMosseri,andMichalIrani.2023.Imagic:Text-basedrealimageeditingwith
Inaddition,NoiseConstraintLossisconstructedtosmooththe diffusionmodels.InProceedingsoftheIEEE/CVFConferenceonComputerVision
video. In the inference stage, we utilize a two-branch structure andPatternRecognition.6007â€“6017.
[20] Yao-ChihLee,Ji-ZeGenevieveJang,Yi-TingChen,ElizabethQiu,andJia-Bin
toinjectspatialfeaturesintotheeditingbranchtogenerateedit
Huang.2023.Shape-awaretext-drivenlayeredvideoediting.InProceedingsofthe
videos.Extensiveexperimentsdemonstratetheeffectivenessofour IEEE/CVFConferenceonComputerVisionandPatternRecognition.14317â€“14326.
proposedEdit-Your-Action. [21] JinpengLin,MinZhou,YeMa,YifanGao,ChenxiFei,YangjianChen,ZhangYu,
andTiezhengGe.2023. AutoPoster:AHighlyAutomaticandContent-aware
LimitationsandFutureWork.AlthoughourproposedEdit-Your-
DesignSystemforAdvertisingPosterGeneration.InProceedingsofthe31stACM
Motionachievescompellingresultsinvideomotionediting,two- InternationalConferenceonMultimedia.1250â€“1260.
stagetrainingconsumesmorecomputationalresources.Therefore, [22] JinLiu,XiWang,XiaomengFu,YeshengChai,CaiYu,JiaoDai,andJizhongHan.
2023.MFR-Net:Multi-facetedResponsiveListeningHeadGenerationviaDenois-
howtoperformvideomotioneditingwithlimitedcomputational ingDiffusionModel.InProceedingsofthe31stACMInternationalConferenceon
resourcesstilldeservesfurtherexplorationinfutureresearch.We Multimedia.6734â€“6743.
[23] JiamingLiu,YueWu,MaoguoGong,QiguangMiao,WenpingMa,andCaiXu.
alsoexpectvideomotioneditingtoreceivemoreattentionfrom
2023. ExploringDualRepresentationsinLarge-ScalePointClouds:ASimple
researchers. WeaklySupervisedSemanticSegmentationFramework.InProceedingsofthe31st
ACMInternationalConferenceonMultimedia.2371â€“2380.
REFERENCES [24] ShaotengLiu,YuechenZhang,WenboLi,ZheLin,andJiayaJia.2023.Video-p2p:
Videoeditingwithcross-attentioncontrol.arXivpreprintarXiv:2303.04761(2023).
[1] JianhongBai,TianyuHe,YuchiWang,JunliangGuo,HaojiHu,ZuozhuLiu,and [25] JianMa,JunhaoLiang,ChenChen,andHaonanLu.2023.Subject-diffusion:Open
JiangBian.2024.UniEdit:AUnifiedTuning-FreeFrameworkforVideoMotion domainpersonalizedtext-to-imagegenerationwithouttest-timefine-tuning.
andAppearanceEditing.arXivpreprintarXiv:2402.13185(2024). arXivpreprintarXiv:2307.11410(2023).
[2] OmerBar-Tal,DolevOfri-Amar,RafailFridman,YoniKasten,andTaliDekel.2022. [26] YueMa,YingqingHe,XiaodongCun,XintaoWang,SiranChen,XiuLi,and
Text2live:Text-drivenlayeredimageandvideoediting.InEuropeanconference QifengChen.2024.Followyourpose:Pose-guidedtext-to-videogenerationusing
oncomputervision.Springer,707â€“723. pose-freevideos.InProceedingsoftheAAAIConferenceonArtificialIntelligence,
[3] TimBrooks,AleksanderHolynski,andAlexeiAEfros.2023. Instructpix2pix: Vol.38.4117â€“4125.
Learningtofollowimageeditinginstructions.InProceedingsoftheIEEE/CVF [27] JiafengMao,XuetingWang,andKiyoharuAizawa.2023.Guidedimagesynthe-
ConferenceonComputerVisionandPatternRecognition.18392â€“18402. sisviainitialimageeditingindiffusionmodel.InProceedingsofthe31stACM
[4] MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiang InternationalConferenceonMultimedia.5321â€“5329.
Zheng.2023.Masactrl:Tuning-freemutualself-attentioncontrolforconsistent [28] ChenlinMeng,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefano
imagesynthesisandediting.InProceedingsoftheIEEE/CVFInternationalConfer- Ermon.2021. Sdedit:Imagesynthesisandeditingwithstochasticdifferential
enceonComputerVision.22560â€“22570. equations.arXivpreprintarXiv:2108.01073(2021).ACMMM,2024,Melbourne,Australia YiZuo,LinglingLi,LichengJiao,FangLiu,XuLiu,WenpingMa,ShuyuanYang,andYuweiGuo
[29] DavideMorelli,AlbertoBaldrati,GiuseppeCartella,MarcellaCornia,Marco InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.7623â€“
Bertini,andRitaCucchiara.2023.LaDI-VTON:latentdiffusiontextual-inversion 7633.
enhancedvirtualtry-on.InProceedingsofthe31stACMInternationalConference [52] RuiXu,LeHui,YuehuiHan,JianjunQian,andJinXie.2023.SceneGraphMasked
onMultimedia.8580â€“8589. VariationalAutoencodersfor3DSceneGeneration.InProceedingsofthe31st
[30] AlexanderQuinnNicholandPrafullaDhariwal.2021.Improveddenoisingdiffu- ACMInternationalConferenceonMultimedia.5725â€“5733.
sionprobabilisticmodels.InInternationalconferenceonmachinelearning.PMLR, [53] HaiboYang,YangChen,YingweiPan,TingYao,ZhinengChen,andTaoMei.
8162â€“8171. 2023. 3dstyle-diffusion:Pursuingfine-grainedtext-driven3dstylizationwith
[31] LiangPeng,HaoranCheng,ZhengYang,RuisiZhao,LinxuanXia,ChaotianSong, 2ddiffusionmodels.InProceedingsofthe31stACMInternationalConferenceon
QinglinLu,WeiLiu,andBoxiWu.2023. SmoothVideoSynthesiswithNoise Multimedia.6860â€“6868.
ConstraintsonDiffusionModelsforOne-shotVideoTuning. arXivpreprint [54] KunYang,DingkangYang,JingyuZhang,HanqiWang,PengSun,andLiangSong.
arXiv:2311.17536(2023). 2023.What2comm:Towardscommunication-efficientcollaborativeperception
[32] ChenyangQi,XiaodongCun,YongZhang,ChenyangLei,XintaoWang,Ying viafeaturedecoupling.InProceedingsofthe31stACMInternationalConference
Shan,andQifengChen.2023.Fatezero:Fusingattentionsforzero-shottext-based onMultimedia.7686â€“7695.
videoediting.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer [55] ZhaoYang,BingSu,andJi-RongWen.2023.SynthesizingLong-TermHuman
Vision.15932â€“15942. MotionswithDiffusionModelsviaCoherentSampling.InProceedingsofthe31st
[33] LeigangQu,ShengqiongWu,HaoFei,LiqiangNie,andTat-SengChua.2023. ACMInternationalConferenceonMultimedia.3954â€“3964.
Layoutllm-t2i:Elicitinglayoutguidancefromllmfortext-to-imagegeneration. [56] JietengYao,JunjieChen,LiNiu,andBinSheng.2023. Scene-awarehuman
InProceedingsofthe31stACMInternationalConferenceonMultimedia.643â€“654. posegenerationusingtransformer.InProceedingsofthe31stACMInternational
[34] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh, ConferenceonMultimedia.2847â€“2855.
SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark, [57] DanahYatim,RafailFridman,OmerBarTal,YoniKasten,andTaliDekel.2023.
etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision. Space-TimeDiffusionFeaturesforZero-ShotText-DrivenMotionTransfer.arXiv
InInternationalconferenceonmachinelearning.PMLR,8748â€“8763. preprintarXiv:2311.17009(2023).
[35] YixuanRen,YangZhou,JimeiYang,JingShi,DifanLiu,FengLiu,MingiKwon,and [58] ChaohuiYu,QiangZhou,JingliangLi,ZheZhang,ZhibinWang,andFanWang.
AbhinavShrivastava.2024.Customize-A-Video:One-ShotMotionCustomization 2023.Points-to-3d:Bridgingthegapbetweensparsepointsandshape-controllable
ofText-to-VideoDiffusionModels.arXivpreprintarXiv:2402.14780(2024). text-to-3dgeneration.InProceedingsofthe31stACMInternationalConferenceon
[36] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rn Multimedia.6841â€“6850.
Ommer.2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.In [59] JunYu,JiZhao,GuochenXie,FengxinChen,YeYu,LiangPeng,MingleiLi,
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition. andZonghongDai.2023. Leveragingthelatentdiffusionmodelsforoffline
10684â€“10695. facialmultipleappropriatereactionsgeneration.InProceedingsofthe31stACM
[37] OlafRonneberger,PhilippFischer,andThomasBrox.2015. U-net:Convolu- InternationalConferenceonMultimedia.9561â€“9565.
tionalnetworksforbiomedicalimagesegmentation.InMedicalimagecomputing [60] HuiminZeng,WeinongWang,XinTao,ZhiweiXiong,Yu-WingTai,andWenjie
andcomputer-assistedinterventionâ€“MICCAI2015:18thinternationalconference, Pei.2023.FeatureDecoupling-RecyclingNetworkforFastInteractiveSegmen-
Munich,Germany,October5-9,2015,proceedings,partIII18.Springer,234â€“241. tation.InProceedingsofthe31stACMInternationalConferenceonMultimedia.
[38] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,and 6665â€“6675.
KfirAberman.2023.Dreambooth:Finetuningtext-to-imagediffusionmodelsfor [61] LvminZhang,AnyiRao,andManeeshAgrawala.2023.Addingconditionalcon-
subject-drivengeneration.InProceedingsoftheIEEE/CVFConferenceonComputer troltotext-to-imagediffusionmodels.InProceedingsoftheIEEE/CVFInternational
VisionandPatternRecognition.22500â€“22510. ConferenceonComputerVision.3836â€“3847.
[39] LeoShan,WenzhangZhou,andGraceZhao.2023.IncrementalFewShotSemantic [62] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.
SegmentationviaClass-agnosticMaskProposalandLanguage-drivenClassifier. 2018. Theunreasonableeffectivenessofdeepfeaturesasaperceptualmetric.
InProceedingsofthe31stACMInternationalConferenceonMultimedia.8561â€“8570. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
[40] AliaksandrSiarohin,StÃ©phaneLathuiliÃ¨re,SergeyTulyakov,ElisaRicci,andNicu 586â€“595.
Sebe.2019.Firstordermotionmodelforimageanimation.Advancesinneural [63] Shao-KuiZhang,Jia-HongLiu,YikeLi,TianyiXiong,Ke-XinRen,HongboFu,
informationprocessingsystems32(2019). andSong-HaiZhang.2023. AutomaticGenerationofCommercialScenes.In
[41] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Proceedingsofthe31stACMInternationalConferenceonMultimedia.1137â€“1147.
2015. Deepunsupervisedlearningusingnonequilibriumthermodynamics.In [64] YaboZhang,YuxiangWei,DongshengJiang,XiaopengZhang,WangmengZuo,
Internationalconferenceonmachinelearning.PMLR,2256â€“2265. andQiTian.2023.Controlvideo:Training-freecontrollabletext-to-videogenera-
[42] JiamingSong,ChenlinMeng,andStefanoErmon.2020. Denoisingdiffusion tion.arXivpreprintarXiv:2305.13077(2023).
implicitmodels.arXivpreprintarXiv:2010.02502(2020). [65] MinZhao,RongzhenWang,FanBao,ChongxuanLi,andJunZhu.2023.Con-
[43] XueSong,JingjingChen,andYu-GangJiang.2023.RelationTripletConstruc- trolvideo:Addingconditionalcontrolforoneshottext-to-videoediting.arXiv
tionforCross-modalText-to-VideoRetrieval.InProceedingsofthe31stACM preprintarXiv:2305.17098(2023).
InternationalConferenceonMultimedia.4759â€“4767. [66] RuiZhao,YuchaoGu,JayZhangjieWu,DavidJunhaoZhang,JiaweiLiu,Weijia
[44] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,Stefano Wu,JussiKeppo,andMikeZhengShou.2023. Motiondirector:Motioncus-
Ermon,andBenPoole.2020.Score-basedgenerativemodelingthroughstochastic tomizationoftext-to-videodiffusionmodels. arXivpreprintarXiv:2310.08465
differentialequations.arXivpreprintarXiv:2011.13456(2020). (2023).
[45] ShuyuanTu,QiDai,Zhi-QiCheng,HanHu,XintongHan,ZuxuanWu,and [67] ZhichaoZuo,ZhaoZhang,YanLuo,YangZhao,HaijunZhang,YiYang,and
Yu-GangJiang.2023.MotionEditor:EditingVideoMotionviaContent-Aware MengWang.2023.Cut-and-Paste:Subject-DrivenVideoEditingwithAttention
Diffusion.arXivpreprintarXiv:2311.18830(2023). Control.arXivpreprintarXiv:2311.11697(2023).
[46] NarekTumanyan,MichalGeyer,ShaiBagon,andTaliDekel.2023.Plug-and-play
diffusionfeaturesfortext-drivenimage-to-imagetranslation.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.1921â€“1930.
[47] DaniValevski,MatanKalman,YossiMatias,andYanivLeviathan.2022.Unitune:
Text-drivenimageeditingbyfinetuninganimagegenerationmodelonasingle
image.arXivpreprintarXiv:2210.094772,3(2022),5.
[48] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
AidanNGomez,ÅukaszKaiser,andIlliaPolosukhin.2017. Attentionisall
youneed.Advancesinneuralinformationprocessingsystems30(2017).
[49] YaohuiWang,XinyuanChen,XinMa,ShangchenZhou,ZiqiHuang,YiWang,
CeyuanYang,YinanHe,JiashuoYu,PeiqingYang,etal.2023. Lavie:High-
qualityvideogenerationwithcascadedlatentdiffusionmodels.arXivpreprint
arXiv:2309.15103(2023).
[50] ZixinWang,YadanLuo,ZhiChen,SenWang,andZiHuang.2023.Cal-SFDA:
Source-FreeDomain-adaptiveSemanticSegmentationwithDifferentiableEx-
pectedCalibrationError.InProceedingsofthe31stACMInternationalConference
onMultimedia.1167â€“1178.
[51] JayZhangjieWu,YixiaoGe,XintaoWang,StanWeixianLei,YuchaoGu,Yufei
Shi,WynneHsu,YingShan,XiaohuQie,andMikeZhengShou.2023.Tune-a-
video:One-shottuningofimagediffusionmodelsfortext-to-videogeneration.