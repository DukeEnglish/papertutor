Policy Space Response Oracles: A Survey
AriyanBighashdel*,12, YongzhaoWang*,‚Ä†,3, StephenMcAleer4, RahulSavani35, FransA.
Oliehoek1
*Equalcontributions,‚Ä†Correspondingauthor
1DelftUniversityofTechnology,NL;2EindhovenUniversityofTechnology,NL;3TheAlanTuring
Institute,UK;4CarnegieMellonUniversity,USA;5UniversityofLiverpool,UK
{a.bighashdel,f.a.oliehoek}@tudelft.nl,yongzhao.wang@turing.ac.uk,smcaleer@cs.cmu.edu,
rahul.savani@liverpool.ac.uk
Abstract games, a sample equilibrium already provides valuable in-
sights into effective strategic play, for example, by reveal-
Ingametheory, agamereferstoamodelofinter- ingthegame‚Äôs(unique)value,whichcanbeachievedwitha
action among rational decision-makers or players, strongstrategyirrespectiveofthestrategyoftheotherplayer.
makingchoiceswiththegoalofachievingtheirin- However, even in zero-sum settings, many games that arise
dividual objectives. Understanding their behavior frompracticalapplicationsaretoolargetosolveforasample
in games is often referred to as game reasoning. equilibrium even with polynomial-time methods for Linear
Thissurveyprovidesacomprehensiveoverviewof Programming‚Äìthesegamescanbeextremelylargethatthey
a fast-developing game-reasoning framework for are infeasibleto be explicitlyrepresented as a payoffmatrix
largegames,knownasPolicySpaceResponseOra- inpractice. Itisthesehugegamesthatareourprimaryfocus
cles(PSRO).WefirstmotivatePSRO,providehis- inthissurvey.
torical context, and position PSRO within game-
As an alternative to traditional equilibrium computation
reasoningapproaches. Wethenfocusonthestrat-
methods, to reason about such huge games, a wide range
egy exploration problem for PSRO, the challenge
of learning methods have been applied (with one of the
of assembling an effective strategy portfolio for
mostprominentapproachesbeingmultiagentReinforcement
modelingtheunderlyinggamewithminimumcom- Learning (RL) [Albrecht et al., 2024]). While learning ap-
putationalcost. Wealsosurveycurrentresearchdi-
proaches have significantly contributed to the development
rectionsforenhancingtheefficiencyofPSRO,and
of intelligent agents, they face many inherent challenges in
explore the applications of PSRO across various
games. Forexample,independentlearningacrossagentscan
domains. We conclude by discussing open ques-
rendertheenvironmentnon-stationary,whichisachallenge
tionsandfutureresearch.
forconvergenceaseachindividuallearnerfacesapotentially
moving target. Another challenge is non-transitivity of a
game, where there is not a clear notion of ‚Äúbetter‚Äù strategy
1 Introduction
foranagent,andthuseffectivelearningrequiresthelearning
Inrecentdecades,theexplorationofmultiagentsystemshas schemetomaintainapopulationofstrategiesforeachagent.
been a central focus in Artificial Intelligence (AI) research. Againstthisbackdrop,thePolicySpaceResponseOracles
A multiagent system, often referred to as a game, com- (PSRO) framework [Lanctot et al., 2017] emerged as a nat-
prises multiple decision-making agents that interact within ural blend that combines traditional game-theoretic equilib-
a shared environment. To understand the strategic behav- riumcomputationwithlearning.InPSRO,akeyconceptisan
ioramongtheseagents‚Äìwheretheoptimalbehaviorofone empiricalgamemodel,whichactsasanapproximationofthe
agent depends on the behavior of others ‚Äì game theory pro- underlyingfullgame. Gamemodelsareinducedfromsimu-
vides a mathematical framework that defines behavioral sta- lationsrunovercombinationsofaparticularsetofstrategies.
bility through solution concepts like the Nash equilibrium This set of strategies is typically much smaller than the full
(NE).Toidentifysuchsolutions,variousequilibriumcompu- game,andthusgamemodelsarefeasibletoanalyzewithtra-
tation approaches have been developed [von Stengel, 2002; ditional equilibrium computation methods. PSRO alternates
Savani and Turocy, 2023], either to enumerate all equilibria between the analysis of the current game model, defining a
(see the work by Avis et al. [2010] for bimatrix games), or newlearningtarget,andgamemodelrefinementbyincluding
to find a single sample equilibrium (e.g., with the Lemke- thenewstrategiesgeneratedvialearning.
Howson algorithm for a bimatrix game, or Linear Program- Asageneralsolverforlarge-scalegames,PSROhasbeen
mingforazero-summatrixgame[vonStengel,2002]). successfully applied to a wide range of game types and di-
As the size of the game (i.e., the number of players and verse application domains, from mechanism design [Zhang
strategies) grows, the computational feasibility of enumera- et al., 2023] to robust RL [Liang et al., 2023]. Numer-
tion diminishes, and one tends to focus on finding a sample ous PSRO variants have been developed, each tailored to
equilibrium. Unlike for general-sum games, for zero-sum leveragethespecificcharacteristicsoftheunderlyinggames.
4202
raM
4
]TG.sc[
1v72220.3042:viXraAs a notable example of its success, algorithms inspired by to Multiagent Reinforcement Learning‚Äù, bringing many of
PSRO have reached state-of-the-art performance in large- the mentioned ideas from the planning, multiagent RL, and
scalegamessuchasBarrageStratego[McAleeretal.,2020], co-evolution communities together in a unified framework,
andinStarCraft[Vinyalsetal.,2019],wheretheyhavecon- which also covers classical game-theoretic learning dynam-
vincinglyoutperformedhumanexpertsandpriorAIsystems. icssuchasFictitiousPlay[Brown,1951]. Technically,PSRO
While PSRO and its variants have been covered to some generalizes DO by introducing the concept of meta-strategy
extentinexistingmultiagentlearningsurveys(e.g.,[Yangand solver (MSS), which extracts a profile from the current re-
Wang,2020;Longetal.,2023]),adedicatedsurveyonPSRO strictedgameasthenextbest-responsetarget(seeFigure1).
likethisonehasbeenlacking.Inthissurvey,wereflectonthe This enables the best-response target to extend beyond NE,
historicaldevelopmentofPSRO,whicharosefromdifferent transforming PSRO into a versatile framework capable of
research communities, and we place PSRO within the space generalizingvariousclassicalandmoderngame-theoretical-
of game reasoning approaches. We then present the latest gorithms. Forsimplicity,weoftenrefertoasolutionconcept
developmentsonPSRO,highlightingbothcurrentandfuture astheMSSthatcomputesitwhenmeaningisclearfromthe
researchdirections. context. Forexample,wemaysay‚ÄúNash‚ÄùtomeantheMSS
that computes NE. Moreover, the original version of PSRO
specifieddeepRLasthebest-responseoracle1,enablingbest-
2 ThePSROFramework
responsecomputationinenvironmentswithalargenumberof
ThePSROframeworkincorporatesasynthesisofideasorig- statesandactions.Finally,thepayoffsofprofilesinPSROare
inatingfromdistinctresearchcommunities. Withintheplan- estimatedthroughsimulation,andthereforearestrictedgame
ning community, McMahan et al. [2003] laid the ground- with estimated payoffs is usually called an empirical game.
work by formulating robust planning in Markov Decision Thisideaofcombininggame-theoreticanalysiswithsimula-
Processes as a zero-sum game characterized by a vast strat- tioninPSROhasbeenknownasEmpiricalGame-Theoretic
egyspace.DrawinginspirationfromBender‚Äôsdecomposition Analysis (EGTA) [Wellman, 2006], which provides a broad
inoptimization[Benders,1962],theyintroducedtheDouble toolboxoftechniquesforgamereasoningwithgamemodels
Oracle(DO)algorithm.DOisaniterativealgorithmforsolv- basedonsimulationdata. InanearlyEGTAwork,Schvartz-
ing games with a finite number of strategies. DO maintains man and Wellman [2009] deployed tabular RL as a best-
a restricted version of the full game and iteratively expands response oracle (at a time when deep RL did not exist yet)
the restricted game by adding best responses to the current and NE of the empirical game as a best-response target for
equilibrium.WhenDOterminates,noplayercandeviateuni- strategy generation. PSRO extends tabular RL to deep RL
laterallytogainextrapayoffandthereforetheequilibriumin and can be viewed as an instance of EGTA with strategies
the current restricted game is an NE of the full game. In fi- generatedbyiterativelybestrespondingtoMSSsolutionsus-
nitegames,DOisguaranteedtoconvergetoNE,thoughthe ingdeepRL.
restrictedgamecouldincludeallstrategiesofthefullgamein
2.1 TheFramework
theworstcase.
Concurrently,similarmethodswerebeingdevelopedfrom Anormal-form(akastrategic-form)representationofthefull
a different perspective in the co-evolution research commu- game G = (N,(S ),(u )) is a tuple, where N is a finite set
i i
nity.Co-evolutionarymethodsevolvemultiplepopulationsof of players, each with a non-empty set of strategies S and
i
(different) species in parallel. In this setting, there is no ex- a utility function u : Œ† S ‚Üí R. A restricted game
i j‚ààN j
plicitfitnessfunction,butthefitnessofapopulationmember G isaprojectionofthefullgameG,withplayerschoos-
S‚ÜìX
dependsonhowwellitinteractswithmembersofotherpop- ingfromrestrictedstrategysetsX ‚äÜS . Anempiricalgame
i i
ulations. Traditional challenges for co-evolution include the GÀÜ = (N,(X ),(uÀÜ ))isarestrictedgamewithsimulated
S‚ÜìX i i
presenceofintransitivecyclesintraits,andtherelatedprob- payoffs, where uÀÜ represents the utility function estimated
i
lem of forgetting a trait that seems not useful at one point overtherestrictedstrategyspaceX.
in the evolutionary process but later becomes useful again. Figure 1 shows the special case of DO applied to a bi-
The community explored memory mechanisms and game matrix game on the left, and the general PSRO framework
formulations where the co-evolutionary process was setup on the right. In PSRO, each player is initialized with a set
to discover NE as mixtures of traits [Angeline et al., 1993; of strategies X and the utilities for profiles in the profile
i
Popovicietal.,2012]. Toovercomethementionedobstacles spaceX aresimulated,resultinginaninitialempiricalgame
of intransitive cycles and forgetting traits, various archival GÀÜ . At each iteration of PSRO, an MSS extracts a pro-
S‚ÜìX
structures were devised to facilitate monotonic convergence
file œÉ ‚àà ‚àÜX from the current empirical game GÀÜ as the
towardsarangeofgame-theoreticsolutionconcepts. Forex- S‚ÜìX
next best response target, where ‚àÜ represents the probabil-
ample,the‚ÄúNashmemory‚Äùforsymmetricgames[Ficiciand
ity simplex over a set. Then each player i ‚àà N indepen-
Pollack, 2003] identifies equilibrium strategies within a dis-
dently computes (learns) a best response s‚Ä≤ ‚àà S against its
covered restricted game. This was extended to asymmetric i i
gamesviatheParallelNashMemory[Oliehoeketal.,2006],
1PSROactuallyworkswithanyformof(approximate)response
broadeningthestandardDOframeworktoaccommodateal-
oracles, including search, planning, and evolutionary algorithms
ternativeoracletypesbeyondthebestresponse. etc. For example, Li and Wellman [2021] applied natural evolu-
The PSRO framework [Lanctot et al., 2017] was intro- tionstrategies[Wierstraetal.,2014]asthebestresponseoraclefor
ducedinapapertitled‚ÄúAUnifiedGame-TheoreticApproach PSRO.Double Oracle Framework Policy Space Response Oracles Framework
Payoffs Linear Empirical game
prog. Meta-
2 -1 2 -1 Game Strategy
{{
S2
ùë†‚Ä≤2
0 3 0
.qe
hsaN3 stR re as tt eri gc yte sd
et
simulation S eliforpolv tegraTer
S1 s‚Ä≤1 resB pe os nt se re osB rp ae o cs n lt
e
se fuU nt cil ti it oy n (Ap rep sr pox o. n) sb eest re osB rp ae o cs n lt
e
se R Oe bs jp eco tn ivs ee
Full game Full strategy set
Figure1: IllustrationoftheDOandPSROframeworksrespectively. ThePSROframeworkgeneralizestheDOframeworkbyintroducing
MSSs,enablingbest-responsetargetsotherthanNE.Besides,PSROaccommodatesvariousROsand(approximate)best-responseoracles.
responseobjective(RO),whichisafunctionofstrategypro- two natural ways to aggregate regret over players: the max
files, denotedasRO (œÉ). InstandardPSRO,theROcanbe over regrets and the sum of regrets. Specifically, the sum
i
written as RO (œÉ) = u (s‚Ä≤,œÉ ) and maximizing it over s‚Ä≤ of regrets of a strategy profile œÉ over players, denoted as
givesplayeriai bestrespi onsi ea‚àí gai instotherplayers‚Äôstrategiesi œÅ(œÉ) = (cid:80) œÅ (œÉ),isknownasNashConv(œÉ)[Lanctotet
i‚ààN i
œÉ ‚àíi. Duringthisprocedure,theotherplayers‚ÄôstrategiesœÉ ‚àíi al.,2017]. NashConv(œÉ)measureshowfarthestrategypro-
are fixed, which renders the environment stationary for the fileisfromNE.Inthecontextoftwo-playerzero-sumgames,
learningplayertocomputetheirresponse. Thenthebestre- NashConv(œÉ)isoftenreferredtoasexploitability,indicating
sponses‚Ä≤ i willbeaddedtoitsstrategysetX i intheempirical theextenttowhichthestrategyprofilecanbeexploitedbyan
game. This procedure repeats until a stopping criterion has adversary. The second way to aggregate is taking the max
been satisfied (e.g., a fixed number of iterations or the esti- ofregretsoverplayers. Althoughthemaxofregretsismore
matedregretoftheempirical-gameNEisbelowathreshold, standardthanNashConvingametheory,asitdirectlycorre-
which can be computed by employing one more iteration of spondstothedefinitionofœµ-NE(i.e., aprofilewithinwhich
(approximate)bestresponses). noplayercangainmorethanœµbyunilateraldeviation),itsus-
ageislesscommonthanNashConvinexistingPSROworks.
2.2 StrategyExplorationinPSRO
In practice, the computation of regrets requires an exact
Inessence,game-theoreticanalysisinPSROisperformedby best-response oracle, which is achievable in small games
reasoning about empirical game models, induced from sim- through methods such as strategy enumeration or dynamic
ulations run over combinations of strategies. To construct programming. However, in large games, computing an ex-
a feasible and effective game model for game analysis, the act best response becomes impractical. In such cases, ap-
selection of the strategies is pivotal. In particular, a game proximate best responses are employed, providing a lower
modelisexpectedtocontainamuchsmallernumberofstrate- boundonregrets.Theaccuracyofregretestimationimproves
giesthanthefullgameforrepresentationtractabilityyetstill withahigher-qualityoracle. Withlimitedcomputationalre-
maintainthekeystrategicinformationofthefullgame[Bal- sources, when we cannot find a ‚Äúbetter‚Äù response for any
duzzietal., 2018]. Thischallengeofgamemodelconstruc- player, Oliehoek et al. [2019] named the resulting profile a
tionisdescribedasthestrategyexplorationproblem[Jordan resource-bounded NE, with the interpretation: ‚Äúwith these
etal.,2010],whichisthemainresearchfocusfordeveloping resources,wedidnotrefutethatthisisanNE‚Äù.
PSRO methods. The goal of strategy exploration is to as-
sembleaneffectivestrategyportfolioforagamemodelwith
minimumcomputationalcost(i.e.,withthefeweststrategies
2.3 OrganizationoftheSurvey
required). In PSRO, strategy exploration can be controlled
bysettingMSSsandROs,whichhaveacoupledimpact;we
refertothejointchoiceasanMSS-ROcombination. AsdiscussedinSection2.2,akeydesignquestionforPSRO
The performance of strategy exploration given a specific revolvesaroundhowtoconducteffectivestrategyexploration
MSS-RO combination is normally monitored through the given a specific goal. Although the performance of strat-
concept of regret. The regret œÅ (œÉ) for a player i in a strat- egy exploration depends on the interplay between the cho-
i
egy profile œÉ is the difference between the player‚Äôs payoff senMSSsandROs,existingliteraturepredominantlyfocuses
underœÉandthepayofftheycouldhaveachievedbyemploy- on setting either MSSs or ROs independently. Therefore,
ing their best-response strategy. Formally, it is defined as we organize our discussion on research directions and cor-
œÅ (œÉ) = max u (s‚Ä≤,œÉ )‚àíu (œÉ ,œÉ ). Thismeasure responding PSRO variants by first discussing setting MSSs
rei flects the ms a‚Ä≤ i x‚àà iS mi al i expi ect‚àí edi gaini ofi pla‚àí yei r i from unilat- and ROs independently (Sections 3 and 4 respectively). We
erally deviating from their current mixed strategy in œÉ to an then, in Section 5, discuss works that have investigated the
alternative strategy in S . In an NE, each player‚Äôs strategy joint choice of MSS-RO combination, before moving on to
i
is a best response to the strategies of the others, which im- discussinSection6howbesttoevaluatetheeffectivenessof
plies that no player can gain by unilaterally changing their such choices for strategy exploration. We then discuss re-
strategy. Consequently, at an NE, the regret is zero for all searchonimprovingtheefficiencyofPSRO(Section7),ex-
players. Moreover, the stability of a profile œÉ depends on plore applications of PSRO (Section 8), and conclude with
the aggregation of regrets over players. There are basically openquestionsandfutureresearchdirections(Section9).3 StrategyExplorationviaMSS the best response to an NE strategy mixed with exploration
elements. Wrightetal.[2019]developedahistory-awareap-
In prior works, setting MSSs was a primary way to control
proachwithabest-response(BR)targetmixedwithprevious
strategyexploration. Inthissection,wediscusstheseworks,
targets. Online double oracle [Dinh et al., 2022] integrated
theirmotivations,andtheirefficacyforstrategyexploration.
PSROwithonlinelearningtoselectedtheBRtarget.
3.1 PSROwithNormal-FormEmpiricalGames WangandWellman[2023b]addressedtheoverfittingprob-
lem by adopting an explicit view of regularization and in-
In the standardPSRO framework, an empiricalgame isrep-
troduced Regularized Replicator Dynamics (RRD), an MSS
resentedinnormalform. Whilethesimulationstypicallyun-
variant that truncates the NE search process in intermediate
foldthroughsequentialobservationsanddecisionsovertime,
game models based on a regret criterion. Specifically, RRD
the empirical game abstracts away this temporal structure.
computestheBRtargetbyrunningRDintheempiricalgame,
ThissectiondealswithresearchquestionsandPSROvariants
stoppingoncetheregretofthecurrentprofilewithrespectto
basedonthenormal-formrepresentation.
the empirical game meets a specified regret threshold. The
UsingNashanditsVariantsasMSSs. regretcriterionenablesRRDtosupportdirectcontrolofthe
degree of regularization and can be adjusted to fit a specific
In PSRO, the most common MSS target is NE, which can
game.
be computed by various game-theoretic methods based on
the normal-form empirical game. PSRO with NE is essen-
MSSsBeyondNash
tiallyDOwithdeepRLforcomputing(approximate)bestre-
Rectified Nash Balduzzi et al. [2019] reformulated the
sponses. Therefore,PSROwithNEinheritstheconvergence
strategy exploration problem as that of enlarging what they
propertyofDO,givenmildassumptionsaboutthequalityof
called the gamescape, which describes the strategy space
thebestresponses:Infinitegames,aslongasbeneficialdevi-
covered by the empirical game. For zero-sum games, they
ationscanalwaysbefoundwithnon-zeroprobability,PSRO
proposed rectified Nash as an MSS designed to expand the
withNEasMSStargetwillconvergetoanNEinthelimit.
gamescape and enhance a diversity measure called effective
The Overfitting Problem. Despite its convergence guaran- diversity. In rectified Nash, best responses are only applied
tee in the limit, achieving exact convergence in large games to opponent‚Äôs equilibrium strategies that the learning player
isoftenunattainableduetoconstraintssuchaslimitedcom- defeatsortieswith.
putationalresources. Consequently,manypriorworksstudy-
Minimum-RegretConstrainedProfile. Onenotableobser-
ing strategy exploration in large games revolves around the
vation for PSRO with NE is that the full-game regret, used
developmentofnewalgorithmsthatexhibitstrongempirical
as a measure for evaluating the performance of PSRO, of
performance(e.g.,rapidconvergenceintermsofregretwithin
the empirical-game NE does not decrease monotonically
asmallnumberofPSROiterations). Theoverfittingproblem
over PSRO iterations. In the worst case, the full-game re-
hastwomeaningsforstrategyexploration.First,itmeansthat
grets will increase until the last iteration, when a full-game
strategy exploration may overfit to the NE of the empirical
NE is found (one example can be found in the work by
game,whichmaynotbeaneffectivebest-responsetargetdue
McAleer et al. [2022b]). To address this issue, it was pro-
tothelimitedinformationintheempiricalgame, potentially
posed to use minimum-regret constrained profiles (MRCP)
yielding ineffective strategy exploration. Second, it means
[Jordanetal.,2010;Wangetal.,2022]asanMSS.AnMRCP
overfitting to a specific equilibrium in general games (e.g.,
is the profile with minimum regret with respect to the full
games with more than two players or general-sum games)
game 2. With MRCP as the MSS, the resulting PSRO vari-
withoutsufficientlyexploringthewholestrategyspace. Note
antisknownasanytimePSRObecauseregretmonotonically
that the overfitting can be a problem for any solution con-
decreasesastheempiricalgamegrows. Despitethedifficulty
cept;wediscussitinthecontextofNEspecificallydueNE‚Äôs
ofcomputingMRCPingeneralgames,anytimePSROlever-
prominenceastheMSStargetintheliterature.
ages the properties of two-player zero-sum games and com-
RegularizationtoPreventOverfitting. Toaddresstheover- putes MRCP by regret minimization against a best response
fittingproblem,Lanctotetal.[2017]proposedanMSS,called (RM-BR)[Johansonetal.,2012].Inafurtherwork[McAleer
Projected Replicator Dynamics (PRD), an adaptation of tra- etal.,2022a],anytimePSROwasextendedbyincludingnot
ditionalreplicatordynamics[TaylorandJonker,1978]. PRD onebuttwostrategiesintheempiricalgameateachiteration,
ensuresaprobabilitylowerboundforselectingeachstrategy the first a best best response to MRCP and the other a best
intheempiricalgame,allowingthenewbestresponsetotrain responsetotheotherplayer‚Äôslateststrategy(i.e.,thestrategy
againstnotonlystrategiesintheequilibriumsupportbutalso addedatthelastPSROiteration). Thismodificationwasob-
those outside the support. PRD can be viewed as a form of served to improve the performance of anytime PSRO. How-
regularizationtopreventoverfittingthebestresponsetothe ever, Wang et al. [2022] pointed out that MRCP regret will
exactNEoftheempiricalgame. Duetothediversetraining monotonically decrease for any MSS, which suggests that
targets,PRDalsoimprovesthestabilityofthenewstrategy. the changes to the MSS in anytime PSRO were not justified
Building on the concept of regularization, subsequent re- purelybythedesiretomonotonicallydecreaseMRCPregret.
search has focused on designing MSSs that prevent overfit- We discuss the evaluation of strategy exploration further in
ting by effectively regularizing the target profile. For in-
stance, Wang et al. [2019] proposed an MSS that combines 2MRCPwascalledtheleast-exploitablerestricteddistributionin
NEwithauniformdistributionasthetargetprofile,enabling thetwo-playerzero-sumcontextbyMcAleeretal.[2022b].Section6. InaworkbyKonickietal.[2022],thebenefitsofleverag-
ingtheextensive-formrepresentationfortheempiricalgame
Correlated Equilibrium. Apart from the issues we dis-
were further explored. They showed that with an extensive-
cussed above, Marris et al. [2021] further argued that NE
formrepresentationinPSRO,thetruegamecanbeapproxi-
may not be an appropriate MSS for PSRO or even a solu-
matedmoreaccuratelythanusinganormal-formmodelcon-
tion concept in general-sum games due to its computational
structedfromthesameamountofsimulationdata.Thisaccu-
intractability. Therefore, they proposed utilizing correlated
racyimprovementstemsfromthefactthatthesimulationdata
equilibrium (CE) and coarse correlated equilibrium (CCE)
formodelingachancenodeinextensiveformcanbereused
as MSSs, introducing a variant called Joint PSRO (JPSRO).
while the modeling needs to be re-simulated every time for
Sincemultiple(C)CEexistinanempiricalgame,theyselect
evaluatingaprofileinnormalform.
theunique(C)CEthatmaximizestheGiniimpurity. Theoret-
icalanalysisdemonstratedthatJPSROconvergestoa(C)CE. Mean-Field Games. Muller et al. [2022] and Wang and
Relatedly, Team-PSRO finds TMECor equilibrium in two- Wellman [2023a] adapted PSRO to mean-field games
teamzero-sumgames[McAleeretal.,2023b].
(MFGs). SincetheutilityfunctionforMFGsisnotgenerally
MSSs Motivated by Specific Games. In addition to the linearinthemeanfield,theempiricalMFGmodelcannotbe
above well-established solution concepts, the literature on represented explicitly. Instead, Wang and Wellman [2023a]
PSRO has also investigated other solution concepts, which employed a game model learning approach [Sokota et al.,
originatefromspecificgamesbutcanbegenerallyappliedto 2019], which is essentially a form of regression that learns
variousgamesettings. OneexampleistheRisk-AverseEqui- autilityfunctionoverarestrictedsetofstrategiesandmean
librium (RAE) introduced by Slumbers et al. [2023], aimed fieldsderivedbythesestrategies. Theyprovedtheexistence
at managing risk in multiagent systems. Specifically, RAE ofNEintheempiricalgamewitharestrictedstrategysetand
minimizespotentialvarianceinrewardsbyaccountingforthe theconvergenceofPSROtoNEinMFGs.
strategies of other players. Besides, Li et al. [2023b] pre-
sentedtheNashBargainingSolution(NBS),aconceptorig-
4 StrategyExplorationviaRO
inating from the bargaining game settings, as an MSS. NBS
can be computed by maximizing the product of player utili- BesidessettingMSSs,establishingROstoguidestrategyex-
tiesintheempiricalgame. ploration in PSRO has also been explored in the literature.
AutomatedMSSDesign. Distinctfrompriorworksthatde- One predominant way to design novel ROs is to include di-
signMSSsbasedonvarioussolutionconceptsandheuristics, versitymeasuresinthestandardRO,aimingatincreasingthe
Feng et al. [2021] proposed Neural Auto-Curricula (NAC) diversityofstrategiesintheempiricalgame.
basedonmeta-learning,whichautomatesthedesignofMSSs For example, Perez-Nieves et al. [2021] proposed diverse
in an end-to-end manner. Specifically, MSSs in NAC are PSRO, which incorporates expected cardinality, a diversity
parametrized by a neural network, which is trained by min- measure defined through a determinantal point process, into
imizing the regret of the meta-strategy in the resulting em- the standard RO. Liu et al. [2021] define behavioral diver-
pirical games. The resulting empirical games are generated sity(BD)andresponsediversity(RD),measuringdiversityon
throughPSROwiththecurrentMSS(i.e.,thecurrentneural differentscales. BDisdefinedbasedonthedistancebetween
network) in games sampled from a game distribution. With action-statecoveragesgivenbydifferentstrategies,whileRD
thistrainingscheme,Fengetal.[2021]showedthatNACcan measuresthedistancebetweenthepayoffvectorinducedby
learnaneffectiveMSSforafamilyofgames. the new strategy and the current empirical game. Subse-
quently, Liu et al. [2022b] proposed unified diversity mea-
3.2 PSROwithAlternativeGameForms sures to capture a variety of diversity metrics, which were
Extensive-FormGames. Insteadofemployingthenormal- later combined with the standard RO. Yao et al. [2023] ob-
form representation, some PSRO variants use an extensive- served that the current diversity measures for enlarging the
formrepresentationfortheempiricalgame,offeringaricher gamescape fail to connect to the quality of NE approxima-
waytoencompasstemporalpatternsinactionsandinforma- tion. TheyconnectedtheROtothequalityofNEapproxima-
tion for underlying sequential games. One such example is tion by introducing population exploitability (PE), which is
givenbyextensive-formDO(XDO)[McAleeretal.,2021],a essentiallytheregretofMRCP[Wangetal.,2022],toreflect
DOvariantwithextensive-formgamemodels.Theextensive- thecoverageofapolicyhull(i.e.,theconvexcombinationsof
form empirical game tree in XDO is constructed using the strategiesintheempiricalgame). Theyshowedthatalarger
restrictedsetofplayers‚Äôstrategies. SimilartoDO,NEisde- policy hull indicates lower PE. Therefore, they proposed an
ployed as an MSS target, computed through Counterfactual RO variant to enlarge the policy hull, aiming at promoting
RegretMinimization[Zinkevichetal.,2007],andthebestre- strategy exploration. We list the specifications of different
sponsecomputationwillresultinnewactionsatinformation methodsforenhancingdiversityinTable1.
states in the empirical game tree. Note that when a new ac- Aside from diversity, Li et al. [2023b] deployed Monte
tionisaddedtoaninformationstate,multiplestrategieswill Carlo tree search (MCTS) as the best-response oracle us-
be added. So one iteration in XDO implicitly needs more ingdifferentvalues(e.g., socialwelfare)toupdatevaluesof
simulationforprofileevaluationthanoneDOiteration. Peri- nodesalongthesamplepathintheback-propagationstepof
odicDO[Tangetal., 2023]extendsXDObyimprovingthe MCTS. The employment of different back-propagation val-
stoppingthresholdoftherestrictedgamesolver. uescanalsobeviewedasmodificationsoftheRO.DiversityMeasure Concept DiversityType EnlargementTarget CompatibleMSS
Behavioral Response Gamescape PolicyHull Nash Œ±-Rank
EffectiveDiversity[Balduzzietal.,2019] RectifiedNashstrategy ‚úì ‚úì ‚úì
ExpectedCardinality[Perez-Nievesetal.,2021] Determinantalpointprocesses ‚úì ‚úì ‚úì ‚úì
ConvexHullEnlargement[Liuetal.,2021] Euclideanprojection ‚úì ‚úì ‚úì
OccupancyMeasureMismatching[Liuetal.,2021] f-divergence,occupancymeasure ‚úì ‚úì ‚úì
UnifiedDiversityMeasure[Liuetal.,2022b] Strategyfeature,diversitykernel ‚úì ‚úì ‚úì ‚úì
PolicySpaceDiversity[Yaoetal.,2023] Bregmandivergence,sequence-form ‚úì ‚úì ‚úì
Table1:Specificationsofpromoting-diversitymethods.
5 StrategyExplorationviaJointMSS-RO strategies,andthustheempiricalgamemodelatanypointre-
flectsadistinctstrategyspace. Thecomparisonsofdifferent
Generally speaking, MSSs and ROs are not functionally in-
MSS-RO combinations are across different strategy spaces,
terchangeableandcanbringacouplingimpacttostrategyex-
which may not be faithfully represented by a simple sum-
ploration,especiallywhenagoalisspecified. Inthissection,
mary such as an interim solution. Therefore, they proposed
wediscusspriorworksthatjointlyvaryMSSsandROs.
to use MRCP, the profile closest to the full-game NE (in re-
Œ±-Rank. Muller et al. [2020] proposed the adoption of Œ±- gret)intheempiricalgame,astheevaluationmetricforeval-
Rank[Omidshafieietal.,2019]asthepreferredsolutioncon- uating the performance of multiple MSS-RO combinations.
cept due to its computational scalability and uniqueness in This means that the MSS employed for strategy generation
many-player general-sum games. To make PSRO with Œ±- isindependentoftheMSS(e.g.,MRCP)usedforevaluation,
RankasMSSconverge,theyintroducedthepreference-based which should be fixed when comparing different empirical
best-responseoracle,whichessentiallyreturnsasetofstrate- games,regardlessoftheMSSwithwhichtheyaregenerated.
giesthatmaximizestheprobabilitymassunderŒ±-Rankfrom TherequirementoffixingtheevaluationMSSisnamedasthe
the set of better responses to the current strategy in a pro- consistencycriteriabytheauthors.
file. WiththisMSS-ROcombination,theyprovedthatPSRO
with Œ±-Rank converge to a concept, called sink strongly- 7 ImprovementsinTrainingEfficiency
connected components, which describes the distribution of
strategiesinlong-terminteractions. TherearetwocomponentsinPSROthatarecomputationally
demanding: bestresponsecomputation(usuallyachievedby
InvestigatingtheJointImpactofMSSsandROs. Anem- deep RL) and empirical game simulation. To improve the
piricalstudybyWangandWellman[2024]explicitlyinvesti- efficiency of PSRO, various methods have been developed,
gatedthecouplingimpactofMSSsandROsinstrategyexplo- addressingissuesrelatedtothesetwoaspects.
ration. ThisresearchexperimentedwithamultitudeofMSS
Parallelization. Leveraging parallelization, Lanc-
andROcombinationswitheachROpossessinguniquechar-
tot et al. [2017] proposed the Deep Cognitive Hierarchy
acteristics. Theirexperimentalresultsunderscorethepivotal
(DCH)model,whichcreatesatraininghierarchywhereeach
role of ROs in steering strategy exploration towards desired
playertrainsabestresponsestrategy(withdeepRL)against
objectives, such as higher social welfare. Moreover, they
the NE of the empirical game with strategies at the same
showedthatwithacarefulselectionofMSS,theperformance
level or below it. This warm-starts best response training
ofstrategyexplorationcanbefurtherimproved.
and speeds up PSRO compared to training best responses
from scratch. Motivated by DCH, McAleer et al. [2020]
6 EvaluatingStrategyExploration
proposedPipelinePSRO(P2SRO).SimilartoDCH,P2SRO
initializes a bunch of strategies and assigns each strategy
In addition to designing novel strategy exploration algo-
a level. Then P2SRO warm-starts training each strategy
rithms,asignificantefforthasalsobeenputintoinvestigating
in parallel against the NE of the empirical game involving
methodological considerations in evaluating strategy explo-
strategies with lower levels, which accelerates the overall
ration,andproposingandjustifyingnewevaluationmethods.
training of PSRO. A major contribution of P2SRO is that it
In PSRO, it may seem natural to employ the same MSS for
optimizesthetraininghierarchy,addressingissueswithDCH
bothstrategygenerationandevaluation,asmuchoftheprior
such as heavy parallelization requirements and the need for
work in PSRO exploration has done. For example, if NE is
anestimateofthenumberofPSROiterations.
used as the MSS for strategy generation, then the regret of
NEofintermediateempiricalgameswillbeusedastheper- SampleEfficiency. Adistinctivecharacteristicofempirical
formance measure at each iteration of PSRO. Similarly, the gamemodelsisthattheyarederivedorestimatedfromsimu-
regretofauniformdistributionoverstrategieswillbetheper- lationdata.ToimprovethesampleefficiencyofPSRO,Smith
formancemeasurewhentheuniformisemployedasMSS,in and Wellman [2023] proposed to learn a full-game model
whichcasePSROreducestofictitiousplay. about the game dynamics from the simulator concurrently
WangandWellman[2022]arguedthisevaluationapproach with running PSRO, aiming at reducing the simulation cost
could yield a misleading conclusion on the performance of byqueryingthefull-gamemodel. Besides,Zhouetal.[2022]
MSS-RO combinations. They highlighted that each MSS- developedanefficientPSRO(EPSRO)implementationforre-
RO combination essentially generates a distinct sequence of ducing the simulation cost of PSRO in two-player zero-sumgames.Thekeyinsightisthatthesimulationfortheempirical video tracking [Fathony et al., 2018] underscore the PSRO
gameisonlyusedforcomputingbestresponsetargetprofiles. framework‚Äôsimpactonenhancingcomputervisionandstruc-
Soaslongasbestresponsetargetprofilescanbecomputedin tured prediction methodologies. Furthermore, PSRO-type
otherways(e.g.,auniformMSSdoesnotneedtheevaluation methods have been adapted to enhance the training of Gen-
ofagamemodel),thereisnoneedtomaintainthecomplete erativeAdversarialNetworks(GAN)[Oliehoeketal., 2019;
empiricalgamemodel,avoidingunnecessarysimulations. Aungetal.,2022],withnotableimplementationssuchasDO-
GAN, which facilitates GAN training in high-dimensional
Transfer learning. Transfer learning is a machine learn-
spacesthroughstrategypruningandcontinuallearningmech-
ing technique where a model trained on one task is repur-
anisms.
posed for a different task. In PSRO, best-responding to dif-
ferentstrategiescanbeviewedassuchtasksandthustransfer
learning can be applied to warm-start training new best re- 9 OpenResearchQuestions
sponses. One example leveraging this idea is NeuPL [Liu
The research directions outlined in the previous sections re-
et al., 2022a], which represents all strategies in the strategy
mainopen-ended. Next,wedescribeseveralfurtherresearch
set via a shared neural network. NeuPL utilizes explicit pa-
directionsthatwebelievedeservethecommunity‚Äôsattention.
rameter sharing for skill transfer, which was shown to ef-
fectively accelerate the adaptation to the opponent‚Äôs meta- Scalability in the Number of Players. In the standard
strategy. Smith et al. [2023] also utilized transfer learn- PSRO framework, an empirical game is represented in nor-
ing to reduce simulation costs in computing best responses. mal form. As the number of players increases, the normal-
Their Mixed-Oracle method constructs a new best response form representation expands exponentially, leading to a sig-
bylearningandmaintainingbestresponsestothepurestrate- nificantincreaseinthecostofevaluatingtheempiricalgame.
gies of the opponent (represented as Q-value functions) and Whilethisproblemcanbemitigatedincertainspecialgame
thenmixing(Q-values)accordingtothemeta-strategy. types such as symmetric games, the practicality of applying
PSRO becomes impractical when dealing with a very large
number of players. A potential remedy involves employing
8 Applications
gamemodellearning[Sokotaetal.,2019],essentiallyautil-
Game-theoretic analysis in PSRO relies on game models, ityfunctionregression. Theacquiredutilityfunctionaimsto
which abstract away the underlying game structures. This extrapolateacrossthe(empirical)strategyspace,eliminating
abstractionenablesPSROtosolveavarietyofgamesorad- theneedtoevaluateeachindividualprofile.
dressissuesthatcanbeformulatedasagame,andyieldsnu- Anotherpossibleapproachistousepolymatrixgamerep-
merous applications of PSRO in disparate domains. Specif- resentations. A polymatrix game is a many-player game
ically, PSRO has been applied to specialized games, includ- where each player can be thought of as a node in an inter-
ing security games [Wang et al., 2019; Wright et al., 2019; actiongraphandoneveryedgeinthisgraphthereisabima-
Fang,2019;Tongetal.,2020;Xuetal.,2021;CuiandYang, trixgamethatthenodeplayswithitsneighbours,gainingas
2023], bargaining games [Li et al., 2023b; Wang and Well- utilitythesumofpayoffsacrossthesebimatrixgames. Poly-
man, 2024], Colonel Blotto games [An and Zhou, 2023], matrixgameshavebeenusedwithinequilibriumcomputation
Pursuit-Evasion games [Li et al., 2023a], auctions [Li and methods for many-player normal-form games, due to their
Wellman,2021],andmechanismdesign[Zhangetal.,2023]. linear formulation. In the work of both Govindan and Wil-
Moreover, PSRO applications over time have been ex- son[2004]andGempatal.[2022]apolymatrixapproxima-
tended to real-world domains including anti-jamming in tionoftheinputgameismaintainedandupdated(implicitly
satellite communication [Zou et al., 2022], decision-making in the latter case). There appears to be potential to combine
inbeyond-visual-rangeaircombat[Maetal.,2019],solving orextractelementsfromthesemethodstobuildpolymatrix-
the power imbalance in power system resilience [Niu et al., basedempiricalgamemodelsforPSRO-typemethods.
2021], and tackling the traveling salesman problem in com-
A Completely Parallel PSRO Implementation. Both best-
binatorialoptimization[Wangetal.,2021]. Itsutilityisalso
response computation and empirical game simulation can
evident in social network analysis for competitive influence
benefit from parallelization. Firstly, best-response computa-
maximizationstrategies[Ansarietal.,2019]andindevelop-
tions among players are parallelizable as they are indepen-
ingdefensestrategiesforelectionsafety[Yinetal.,2018].
dent. Secondly, the evaluation of strategy profiles in the
Besidesthereal-worldapplications,PSRO(includingDO)
empirical game can be parallelized. Additionally, since the
has also been applied for designing novel algorithms in do-
evaluation of each strategy profile involves running simula-
mainsthatcanbemodeledasagame.Forexample,thePSRO
tions multiple times and averaging the results, these simula-
frameworkhasfacilitateddevelopmentsinrobustpolicydis-
tion runs can also be parallelized. Consequently, the PSRO
coverywithinrobustreinforcementlearningdomains[Liang
framework lends itself well to high parallelization. Despite
etal.,2023],valuealignmentinlargelanguagemodels[Maet
thispotential,thereiscurrentlynoPSROimplementationthat
al.,2023],publichealthservices[Killianetal.,2023],andthe
fullyharnessesthesethreeaspects.
discoveryofinformationinimages[Giboulotetal.,2023].
In the field of computer vision, several contributions to- Equilibrium Refinements. As discussed in Section 3,
wards data augmentation for object detection [Behpour et PSROcanbeadaptedtocalculatingvarioussolutionconcepts
al., 2019a], active learning [Behpour et al., 2019b], semi- such as NE and CE. These solutions are often not unique,
supervised multi-label classification [Behpour, 2018], and andthefieldofequilibriumrefinementingametheorystud-ies restrictions of these concepts to make the resulting solu- strategyspacebyaddingbestresponses. Itwouldbeinterest-
tions more realistic and better predictions of how the games ingtocombinethebestresponseslearnedinthefullgamevia
will/shouldbeplayed[vanDamme,2012].Anaturalresearch PSRO with those in the subgame that are used for subgame
directionistodesignPSROvariantstocomputeequilibrium solving.
refinements. For example, XDO, which uses an extensive-
AutomatedHyperparameterTuning. Hyperparametersex-
form representation, is a natural starting point to develop
ist universally in PSRO variants, including the lower bound
equilibriumrefinementmethodswithinextensive-formgames of the probability of playing a strategy in PRD [Lanctot et
(e.g. findingsubgame-perfectequilibria). HowcanPSROef- al., 2017], the regret threshold in RRD [Wang and Well-
fectivelycomputetheserefinementsbyappropriatelychoos- man,2023b],theprobabilityofemployingtheuniformMSS
ingtheMSSsandROs? [Wang et al., 2019], and the weights for diversity regulariz-
MultipleEquilibria. Anotherimportantquestionrelatesto ers[Perez-Nievesetal.,2021]. Thesehyperparametersoften
the existence and computation of multiple equilibria, partic- requiredeliberatetuningtofitaspecificgamebyrunningthe
ularly within general-sum games. While current PSRO re- same experiments multiple times for different hyperparame-
searchprimarilyfocusesonidentifyingasampleequilibrium, ter choices, which can be costly. Although some of the hy-
the capability of PSRO to compute multiple equilibria re- perparameters can be set heuristically (e.g., by employing a
mainsunder-explored. Inprinciple, onecouldenumerateall simple annealing scheme), the question of how to tune the
equilibriaintherestrictedgame,anduseallinseparatecalls hyperparametersautomaticallyandefficientlyremainsopen.
tothebest-responseoracles. Inthisway,wewouldbegrow- Specifically,canwedesignanautomaticadjustmentscheme
ingtherestrictedgameinawaythatencompassesallstrategic forthesehyperparameterssuchthatPSROcanbedirectlyap-
information collected so far. This approach requires main- pliedtoanygamewithoutmanualgame-specificfineturning?
taining a larger restricted game, which inherently demands PSROwithLargeLanguageModels. Recentachievements
increasedcomputationalresourcesandnecessitatesmoreeffi- inlargelanguagemodels(LLMs)havedrawnsignificantat-
cientcomputationalframeworkstomanagetheenlargedstrat- tention from various research communities. In the study of
egyspace;itisaninterestingresearchquestiontodecidehow multiagentsystems,researchershaveexploreddiversemeth-
tomosteffectivelysplittheuseoflimitedcomputationalre- ods for integrating LLMs with game-theoretic principles,
sourcesbetweenrespondingtomultipleequilibriainasingle each serving distinct objectives. One research direction in-
PSROiteration,versusbeingabletocompletemoreofthose volvesleveraginggame-theoreticapproachestoaugmentthe
iterations. Not all equilibria in the restricted game may be capabilitiesofLLMs,suchasenablingstrategicresponsesin
equally valuable for guiding strategy exploration. Can we the presence of other agents [Gemp et al., 2024], or foster-
predictthisinadvance? ing alignment with human values [Ma et al., 2023]. Con-
versely, LLMs can also be integrated into existing game-
Combining PSRO with Subgame Solving or CFR. Other
theoreticframeworks,expandingtherangeofapplicabledo-
game-theoreticRLapproaches,suchasthosebasedoncoun-
mains.SincePSROprovidesaflexiblegame-theoreticframe-
terfactual regret minimization (CFR) [Brown et al., 2019;
work, its potential to be combined with LLM in both ways
McAleer et al., 2023a] or policy gradients [Fu et al., 2022;
shouldbenoticedandinvestigatedinfuturework.
Perolat et al., 2022], could be potentially combined with
PSRO. In contrast to PSRO, these approaches solve the en-
Acknowledgement
tiregameupfront,andasaresultmaybeslowertoconverge
oncertaingamesandfasteronothers.CFR-basedandpolicy- We thank Michael P. Wellman for his invaluable advice on
gradient-basedmethodsmightperformbetteringameswhere improvingtheaccuracyandreadabilityofthesurvey.
mixing carefully at many decision nodes is crucial, because
PSRO usually mixes only at the root of the game and as a CallforFeedback
resultmightrequiremanypoliciestoeffectivelymixatinfor-
Weencourageandwelcomesuggestionsandcommentsfrom
mationsets[McAleeretal.,2021]. Notably,XDOdoesmix
ourreaders. Despiteourbestefforts,thereisalwaysapossi-
at every decision node, and as a result is a great candidate
bility of oversight, including missed citations or overlooked
for hybrid approaches with CFR-based and policy-gradient-
information. Therefore, we invite our audience to engage
based techniques. In particular, it would be useful to auto-
withthecontentcriticallyandtoprovidefeedbackiftheyno-
maticallydecideonwhichdecisionnodestomix.Sincesome
ticeanydiscrepanciesoromissions. Yourinsightsandcontri-
gameshavesectionsthatdonotrequiremixingsuchascom-
butionsareinvaluableinensuringtheaccuracyandintegrity
plex control tasks, those would be more efficient to use the
ofourwork.
deep RL policy from PSRO or XDO, while nodes that re-
quirecarefulmixingshoulddefertoaCFR-basedorpolicy-
References
gradient-basedtechnique.
[Albrechtetal.,2024] S. V. Albrecht, F. Christianos, and
Additionally, PSROcanbecombinedwithsubgamesolv-
L. Scha¬®fer. Multi-Agent Reinforcement Learning: Foun-
ing methods. Subgame solving methods, also called search
dationsandModernApproaches. MITPress,2024.
methods, use extra compute at test time to analyze and im-
prove a policy at a given information set without increasing [AnandZhou,2023] Z.AnandL.Zhou. DoubleOracleAl-
exploitability.Somemethods,suchasdepth-limitedsubgame gorithmforGame-TheoreticRobotAllocationonGraphs.
solving [Brown et al., 2018], already iteratively expand the arXiv:2312.11791,2023.[Angelineetal.,1993] P.J.Angeline,J.B.Pollack,andOth- [Fengetal.,2021] X. Feng, O. Slumbers, Z. Wan, B. Liu,
ers. Competitive Environments Evolve Better Solutions S.McAleer, Y.Wen, J.Wang, andY.Yang. Neuralauto-
forComplexTasks. InICGA,1993. curricula in two-player zero-sum games. In NeurIPS,
2021.
[Ansarietal.,2019] A. Ansari, M. Dadgar, A. Hamzeh,
J. Schlo¬®tterer, and M. Granitzer. Competitive influence [FiciciandPollack,2003] S. G. Ficici and J. B. Pollack. A
maximization: integrating budget allocation and seed se- game-theoretic memory mechanism for coevolution. In
lection. arXiv:1912.12283,2019. GECCO,2003.
[Aungetal.,2022] A. P. P. Aung, X. Wang, R. Yu, B. An, [Fuetal.,2022] H. Fu, W. Liu, S. Wu, Y. Wang, T. Yang,
S. Jayavelu, and X. Li. DO-GAN: A Double Oracle K. Li, J. Xing, B. Li, B. Ma, Q. FU, and Y. Wei.
Framework for Generative Adversarial Networks. In Actor-criticpolicyoptimizationinalarge-scaleimperfect-
CVRP,2022. informationgame. InICLR,2022.
[Avisetal.,2010] D.Avis,G.D.Rosenberg,R.Savani,and [Gempetal.,2022] I. Gemp, R. Savani, M. Lanctot,
B. Von Stengel. Enumeration of nash equilibria for two- Y. Bachrach, T. W. Anthony, R. Everett, A. Tacchetti,
playergames. Economictheory,42,2010. T. Eccles, and J. Krama¬¥r. Sample-based approximation
of nash in large many-player games via gradient descent.
[Balduzzietal.,2018] D.Balduzzi,K.Tuyls,J.Pe¬¥rolat,and
InAAMAS,2022.
T.Graepel. Re-evaluatingevaluation. InNIPS,2018.
[Gempetal.,2024] I. Gemp, Y. Bachrach, M. Lanctot,
[Balduzzietal.,2019] D. Balduzzi, M. Garnelo,
R.Patel,V.Dasagi,L.Marris,G.Piliouras,andK.Tuyls.
Y. Bachrach, W. Czarnecki, J. Perolat, M. Jaderberg,
States as strings as strategies: Steering language models
and T. Graepel. Open-ended learning in symmetric
withgame-theoreticsolvers. arXiv:2402.01704,2024.
zero-sumgames. InICML,2019.
[Giboulotetal.,2023] Q.Giboulot,T.Pevny`,andA.D.Ker.
[Behpouretal.,2019a] S.Behpour,K.M.Kitani,andB.D.
The non-zero-sum game of steganography in heteroge-
Ziebart. Ada: Adversarial data augmentation for object
neous environments. IEEE Transactions on Information
detection. InWACV.IEEE,2019.
ForensicsandSecurity,2023.
[Behpouretal.,2019b] S. Behpour, A. Liu, and B. Ziebart.
[GovindanandWilson,2004] S. Govindan and R. Wilson.
Active learning for probabilistic structured prediction of
Computingnashequilibriabyiteratedpolymatrixapprox-
cutsandmatchings. InICML,2019.
imation. JournalofEconomicDynamicsandCtrl.,2004.
[Behpour,2018] S. Behpour. Arc: Adversarial robust cuts
[Johansonetal.,2012] M. Johanson, N. Bard, N. Burch,
for semi-supervised and multi-label classification. In
and M. Bowling. Finding optimal abstract strategies in
CVPR,2018.
extensive-formgames. InAAAI,2012.
[Benders,1962] J. F. Benders. Partitioning procedures for
[Jordanetal.,2010] P. R. Jordan, L. J. Schvartzman, and
solving mixed-variables programming problems. Nu-
M. P. Wellman. Strategy exploration in empirical games.
merischeMathematik,4,1962.
InAAMAS,2010.
[Brownetal.,2018] N.Brown,T.Sandholm,andB.Amos.
[Killianetal.,2023] J. A. Killian, A. Biswas, L. Xu,
Depth-limited solving for imperfect-information games.
S. Verma, V. Nair, A. Taneja, A. Hegde, N. Madhiwalla,
Advances in neural information processing systems, 31,
P. R. Diaz, S. Johnson-Yu, and Others. Robust planning
2018.
overrestlessgroups:engagementinterventionsforalarge-
[Brownetal.,2019] N. Brown, A. Lerer, S. Gross, and scale maternal telehealth program. In AAAI, volume 37,
T.Sandholm. Deepcounterfactualregretminimization. In 2023.
ICML,2019.
[Konickietal.,2022] C.Konicki,M.Chakraborty,andM.P.
[Brown,1951] G.W.Brown. Iterativesolutionofgamesby Wellman.Exploitingextensive-formstructureinempirical
fictitiousplay. Act.Anal.ProdAllocation,13,1951. game-theoreticanalysis. InWINE,2022.
[CuiandYang,2023] J. Cui and X. Yang. Macta: A multi- [Lanctotetal.,2017] M. Lanctot, V. Zambaldi, A. Gruslys,
agentreinforcementlearningapproachforcachetimingat- A.Lazaridou,K.Tuyls,J.Pe¬¥rolat,D.Silver,andT.Grae-
tacksanddetection. ICLR,2023. pel. A Unified Game-Theoretic Approach to Multiagent
[Dinhetal.,2022] L. C. Dinh, S. M. McAleer, Z. Tian, ReinforcementLearning. NIPS,2017.
N. Perez-Nieves, O. Slumbers, D. H. Mguni, J. Wang, [LiandWellman,2021] Z.LiandM.P.Wellman. Evolution
H.B.Ammar,andY.Yang. Onlinedoubleoracle. TMLR,
strategiesforapproximatesolutionofbayesiangames. In
2022. Proceedings of the AAAI Conference on Artificial Intelli-
[Fang,2019] F. Fang. Integrate learning with game theory gence,volume35,pages5531‚Äì5540,2021.
forsocietalchallenges. InIJCAI,2019. [Lietal.,2023a] S. Li, X. Wang, Y. Zhang, W. Xue,
[Fathonyetal.,2018] R.Fathony,S.Behpour,X.Zhang,and J. CÀáerny`, and B. An. Solving large-scale pursuit-evasion
B. Ziebart. Efficient and consistent adversarial bipartite games using pre-trained strategies. In AAAI, volume 37,
matching. InICML,2018. 2023.[Lietal.,2023b] Z.Li,M.Lanctot,K.R.McKee,L.Marris, [McAleeretal.,2023b] S.M.McAleer,G.Farina,G.Zhou,
I. Gemp, D. Hennes, P. Muller, K. Larson, Y. Bachrach, M. Wang, Y. Yang, and T. Sandholm. Team-PSRO for
and M. P. Wellman. Combining Tree-Search, Genera- learningapproximateTMECorinlargeteamgamesviaco-
tive Models, and Nash Bargaining Concepts in Game- operativereinforcementlearning. InNeurIPS,2023.
Theoretic Reinforcement Learning. arXiv:2302.00797,
[McMahanetal.,2003] H.B.McMahan,G.J.Gordon,and
2023.
A.Blum. Planninginthepresenceofcostfunctionscon-
[Liangetal.,2023] Y. Liang, Y. Sun, R. Zheng, X. Liu, trolledbyanadversary. InICML,2003.
T.Sandholm, F.Huang, andS.McAleer. Game-theoretic [Mulleretal.,2020] P. Muller, S. Omidshafiei, M. Row-
robustreinforcementlearninghandlestemporally-coupled land, K. Tuyls, J. Pe¬¥rolat, S. Liu, D. Hennes, L. Marris,
perturbations. arXiv:2307.12062,2023. M.Lanctot,E.Hughes,andOthers. AGeneralizedTrain-
[Liuetal.,2021] X. Liu, H. Jia, Y. Wen, Y. Hu, Y. Chen, ingApproachforMultiagentLearning. InICLR,2020.
C.Fan,Z.Hu,andY.Yang. Towardsunifyingbehavioral [Mulleretal.,2022] P.Muller,M.Rowland,R.Elie,G.Pil-
and response diversity for open-ended learning in zero- iouras, J. Perolat, M. Lauriere, R. Marinier, O. Pietquin,
sumgames. InNeurIPS,2021. andK.Tuyls. LearningEquilibriainMean-FieldGames:
[Liuetal.,2022a] S. Liu, L. Marris, D. Hennes, J. Merel, IntroducingMean-FieldPSRO. InAAMAS,2022.
N. Heess, and T. Graepel. NeuPL: Neural population [Niuetal.,2021] L. Niu, D. Sahabandu, A. Clark, and
learning. arXiv:2202.07415,2022. R. Poovendran. A game-theoretic framework for con-
trolled islanding in the presence of adversaries. In
[Liuetal.,2022b] Z.Liu,C.Yu,Y.Yang,Z.Wu,Y.Li,and
GameSec.Springer,2021.
Others. AUnifiedDiversityMeasureforMultiagentRein-
forcementLearning. InNeurIPS,2022. [Oliehoeketal.,2006] F. A. Oliehoek, E. D. De Jong, and
N. Vlassis. The parallel Nash memory for asymmetric
[Longetal.,2023] W.Long,T.Hou,X.Wei,S.Yan,P.Zhai,
games. InGECCO,2006.
andL.Zhang. ASurveyonPopulation-BasedDeepRein-
forcementLearning. Mathematics,11,2023. [Oliehoeketal.,2019] F.A.Oliehoek,R.Savani,J.Gallego,
E.vanderPol,andR.Gro√ü. BeyondlocalNashequilibria
[Maetal.,2019] Y. Ma, G. Wang, X. Hu, H. Luo, and
foradversarialnetworks. InBNAIC,2019.
X.Lei. CooperativeoccupancydecisionmakingofMulti-
UAVinBeyond-Visual-Rangeaircombat: Agametheory [Omidshafieietal.,2019] S.Omidshafiei,C.Papadimitriou,
approach. IeeeAccess,8,2019. G.Piliouras,K.Tuyls,M.Rowland,J.-B.Lespiau,W.M.
Czarnecki,M.Lanctot,J.Perolat,andR.Munos. Œ±-rank:
[Maetal.,2023] C. Ma, Z. Yang, M. Gao, H. Ci, J. Gao, Multi-agentevaluationbyevolution. Scientificreports,9,
X. Pan, and Y. Yang. Red teaming game: A game- 2019.
theoretic framework for red teaming language models.
[Perez-Nievesetal.,2021] N. Perez-Nieves, Y. Yang,
arXiv:2310.00322,2023.
O. Slumbers, D. H. Mguni, Y. Wen, and J. Wang. Mod-
[Marrisetal.,2021] L. Marris, P. Muller, M. Lanctot, elling behavioural diversity for learning in open-ended
K. Tuyls, and T. Graepel. Multi-agent training beyond games. InICML,2021.
zero-sum with correlated equilibrium meta-solvers. In
[Perolatetal.,2022] J. Perolat, B. De Vylder, D. Hennes,
ICML,2021.
E. Tarassov, F. Strub, V. de Boer, P. Muller, J. T. Con-
[McAleeretal.,2020] S.McAleer,J.B.Lanier,R.Fox,and nor, N. Burch, T. Anthony, et al. Mastering the game of
P.Baldi. PipelinePSRO:Ascalableapproachforfinding stratego with model-free multiagent reinforcement learn-
approximateNashequilibriainlargegames. InNeurIPS, ing. Science,378,2022.
2020.
[Popovicietal.,2012] E.Popovici,A.Bucci,R.P.Wiegand,
[McAleeretal.,2021] S.McAleer,J.B.Lanier,K.A.Wang, andE.D.DeJong. CoevolutionaryPrinciples,2012.
P.Baldi,andR.Fox. XDO:Adoubleoraclealgorithmfor [SavaniandTurocy,2023] R.SavaniandT.L.Turocy.Gam-
extensive-formgames. InNeurIPS,2021.
bit: ThePackagefordoingComputationinGameTheory,
[McAleeretal.,2022a] S. McAleer, J. B. Lanier, K. Wang, version16.1.0.,2023.
P. Baldi, R. Fox, and T. Sandholm. Self-play psro: To- [SchvartzmanandWellman,2009] L. J. Schvartzman and
ward optimal populations in two-player zero-sum games. M.P.Wellman. Exploringlargestrategyspacesinempiri-
arXiv:2207.06541,2022. calgamemodeling. InAAMAS-AMECWorkshop,2009.
[McAleeretal.,2022b] S. McAleer, K. Wang, M. Lanctot, [Slumbersetal.,2023] O. Slumbers, D. H. Mguni, S. B.
J.Lanier,P.Baldi,andR.Fox. Anytimeoptimalpsrofor Blumberg,S.M.Mcaleer,Y.Yang,andJ.Wang. Agame-
two-playerzero-sumgames. arXiv:2201.07700,2022. theoreticframeworkformanagingriskinmulti-agentsys-
tems. InICML,2023.
[McAleeretal.,2023a] S.M.McAleer,G.Farina,M.Lanc-
tot, and T. Sandholm. ESCHER: Eschewing importance [SmithandWellman,2023] M. O. Smith and M. P. Well-
samplingingamesbycomputingahistoryvaluefunction man. Co-Learning Empirical Games and World Models.
toestimateregret. InICLR,2023. arXiv:2305.14223,2023.[Smithetal.,2023] M. O. Smith, T. Anthony, and M. P. [Wierstraetal.,2014] D. Wierstra, T. Schaul, T. Glasmach-
Wellman. Strategicknowledgetransfer. JMLR,24,2023. ers, Y. Sun, J. Peters, and J. Schmidhuber. Natural evo-
lution strategies. The Journal of Machine Learning Re-
[Sokotaetal.,2019] S. Sokota, C. Ho, and B. Wiedenbeck.
search,15(1):949‚Äì980,2014.
Learningdeviationpayoffsinsimulation-basedgames. In
AAAI,2019. [Wrightetal.,2019] M. Wright, Y. Wang, and M. P. Well-
man. Iterated deep reinforcement learning in games:
[Tangetal.,2023] X.Tang, L.C.Dinh, S.M.Mcaleer, and
History-awaretrainingforimprovedstability.InACMEC,
Y. Yang. Regret-minimizing double oracle for extensive-
2019.
formgames. InICML,2023.
[Xuetal.,2021] L. Xu, A. Perrault, F. Fang, H. Chen, and
[TaylorandJonker,1978] P. D. Taylor and L. B. Jonker.
M.Tambe. Robustreinforcementlearningunderminimax
Evolutionarystablestrategiesandgamedynamics. Math-
regretforgreensecurity. InUAI,2021.
ematicalbiosciences,40,1978.
[YangandWang,2020] Y.YangandJ.Wang. Anoverview
[Tongetal.,2020] L. Tong, A. Laszka, C. Yan, N. Zhang,
ofmulti-agentreinforcementlearningfromgametheoret-
and Y. Vorobeychik. Finding needles in a moving
icalperspective. arXiv:2011.00583,2020.
haystack: Prioritizing alerts with adversarial reinforce-
mentlearning. InAAAI,volume34,2020. [Yaoetal.,2023] J. Yao, W. Liu, H. Fu, Y. Yang,
S. McAleer, Q. Fu, and W. Yang. Policy space diversity
[vanDamme,2012] E.vanDamme.RefinementsoftheNash
fornon-transitivegames. arXiv:2306.16884,2023.
EquilibriumConcept. SpringerBerlinHeidelberg,2012.
[Yinetal.,2018] Y.Yin,Y.Vorobeychik,B.An,andN.Ha-
[Vinyalsetal.,2019] O.Vinyals,I.Babuschkin,W.M.Czar-
zon. Optimaldefenseagainstelectioncontrolbydeleting
necki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi,
votergroups. ArtificialIntelligence,259,2018.
R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan,
M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. [Zhangetal.,2023] B. Zhang, G. Farina, I. Anagnostides,
Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, F. Cacciamani, S. McAleer, A. Haupt, A. Celli, N. Gatti,
T. Pohlen, V. Dalibard, D. Budden, Y. Sulsky, J. Mol- V. Conitzer, and T. Sandholm. Computing optimal equi-
loy, T. L. Paine, C¬∏. Gu¬®lc¬∏ehre, Z. Wang, T. Pfaff, libriaandmechanismsvialearninginzero-sumextensive-
Y. Wu, R. Ring, D. Yogatama, D. Wu¬®nsch, K. McKin- formgames. InNeurIPS,2023.
ney,O.Smith,T.Schaul,T.P.Lillicrap,K.Kavukcuoglu, [Zhouetal.,2022] M. Zhou, J. Chen, Y. Wen, W. Zhang,
D. Hassabis, C. Apps, and D. Silver. Grandmaster level
Y. Yang, Y. Yu, and J. Wang. Efficient Policy Space Re-
in StarCraft II using multi-agent reinforcement learning.
sponseOracles. arXiv:2202.00633,2022.
Nature,575,2019.
[Zinkevichetal.,2007] M. Zinkevich, M. Johanson,
[vonStengel,2002] B. von Stengel. Computing equilibria
M. Bowling, and C. Piccione. Regret minimization in
fortwo-persongames. InHandbookofgametheorywith
gameswithincompleteinformation. NIPS,2007.
economicapplications,volume3.Elsevier,2002.
[Zouetal.,2022] M. Zou, J. Chen, J. Luo, Z. Hu, and
[WangandWellman,2023a] Y. Wang and M. P. Wellman.
S. Chen. Equilibrium Approximating and Online Learn-
Empirical Game-Theoretic Analysis for Mean Field
ing for Anti-Jamming Game of Satellite Communication
Games. InAAMAS,2023. PowerAllocation. Electronics,11(21):3526,2022.
[WangandWellman,2023b] Y. Wang and M. P. Wellman.
Regularization for Strategy Exploration in Empirical
Game-TheoreticAnalysis. arXiv:2302.04928,2023.
[WangandWellman,2024] Y. Wang and M. P. Wellman.
Generalizedresponseobjectivesforstrategyexplorationin
empiricalgame-theoreticanalysis. InAAMAS,2024.
[Wangetal.,2019] Y. Wang, Z. R. Shi, L. Yu, Y. Wu,
R.Singh,L.Joppa,andF.Fang.Deepreinforcementlearn-
ingforgreensecuritygameswithreal-timeinformation.In
AAAI,2019.
[Wangetal.,2021] C.Wang,Y.Yang,O.Slumbers,C.Han,
T. Guo, H. Zhang, and J. Wang. A game-theoretic ap-
proachforimprovinggeneralizationabilityoftspsolvers.
arXivpreprintarXiv:2110.15105,2021.
[Wangetal.,2022] Y. Wang, Q. Ma, and M. P. Well-
man. Evaluating strategy exploration in empirical game-
theoreticanalysis. InAAMAS,2022.
[Wellman,2006] M. P. Wellman. Methods for empirical
game-theoreticanalysis. InAAAI,2006.