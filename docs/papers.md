# cs.CL 

| Item |Content|
| --- |---|
|idx| 2407.06192v1 |
|title| Multi-Object Hallucination in Vision-Language Models |
|authors| Xuweiyi ChenZiqiao MaXuejun ZhangSihan XuShengyi QianJianing YangDavid F. FouheyJoyce Chai
|links| http://arxiv.org/abs/2407.06192v1 |
|updated| 2024-07-08 17:59:57 UTC |
|summary| Large vision language models LVLMs often suffer from object hallucinationproducing objects not present in the given images. While current benchmarks forobject hallucination primarily concentrate on the presence of a single objectclass rather than individual entities this work systematically investigatesmulti-object hallucination examining how models misperceive e.g. inventnonexistent objects or become distracted when tasked with focusing on multipleobjects simultaneously. We introduce Recognition-based Object ProbingEvaluation ROPE an automated evaluation protocol that considers thedistribution of object classes within a single image during testing and usesvisual referring prompts to eliminate ambiguity. With comprehensive empiricalstudies and analysis of potential factors leading to multi-objecthallucination we found that 1 LVLMs suffer more hallucinations when focusingon multiple objects compared to a single object. 2 The tested object classdistribution affects hallucination behaviors indicating that LVLMs may followshortcuts and spurious correlations.3 Hallucinatory behaviors are influencedby data-specific factors salience and frequency and model intrinsicbehaviors. We hope to enable LVLMs to recognize and reason about multipleobjects that often occur in realistic visual scenes provide insights andquantify our progress towards mitigating the issues. |


| Item |Content|
| --- |---|
|idx| 2407.06177v1 |
|title| Vision-Language Models under Cultural and Inclusive Considerations |
|authors| Antonia KaramolegkouPhillip RustYong CaoRuixiang CuiAnders SøgaardDaniel Hershcovich
|links| http://arxiv.org/abs/2407.06177v1 |
|updated| 2024-07-08 17:50:00 UTC |
|summary| Large vision-language models VLMs can assist visually impaired people bydescribing images from their daily lives. Current evaluation datasets may notreflect diverse cultural user backgrounds or the situational context of thisuse case. To address this problem we create a survey to determine captionpreferences and propose a culture-centric evaluation benchmark by filteringVizWiz an existing dataset with images taken by people who are blind. We thenevaluate several VLMs investigating their reliability as visual assistants ina culturally diverse setting. While our results for state-of-the-art models arepromising we identify challenges such as hallucination and misalignment ofautomatic evaluation metrics with human judgment. We make our survey datacode and model outputs publicly available. |


| Item |Content|
| --- |---|
|idx| 2407.06172v1 |
|title| On Speeding Up Language Model Evaluation |
|authors| Jin Peng ZhouChristian K. BelardiRuihan WuTravis ZhangCarla P. GomesWen SunKilian Q. Weinberger
|links| http://arxiv.org/abs/2407.06172v1 |
|updated| 2024-07-08 17:48:42 UTC |
|summary| Large language models LLMs currently dominate the field of natural languageprocessing NLP representing the state-of-the-art across a diverse array oftasks. Developing a model of this nature from training to inference requiresmaking numerous decisions which define a combinatorial search problem. Forexample selecting the optimal pre-trained LLM prompt or hyperparameters toattain the best performance for a task often requires evaluating multiplecandidates on an entire test set. This exhaustive evaluation can betime-consuming and costly as both inference and metric computation with LLMsare resource-intensive. In this paper we address the challenge of identifyingthe best method within a limited budget for evaluating methods on testexamples. By leveraging the well-studied multi-armed bandit framework whichsequentially selects the next method-example pair to evaluate our approachcombining multi-armed bandit algorithms with low-rank factorizationsignificantly reduces the required resources. Experiments show that ouralgorithms can identify the top-performing method using only 5-15 of thetypically needed resources resulting in an 85-95 reduction in cost. |


| Item |Content|
| --- |---|
|idx| 2407.06153v1 |
|title| What's Wrong with Your Code Generated by Large Language Models? An Extensive Study |
|authors| Shihan DouHaoxiang JiaShenxi WuHuiyuan ZhengWeikang ZhouMuling WuMingxu ChaiJessica FanCaishuang HuangYunbo TaoYan LiuEnyu ZhouMing ZhangYuhao ZhouYueming WuRui ZhengMing WenRongxiang WengJingang WangXunliang CaiTao GuiXipeng QiuQi ZhangXuanjing Huang
|links| http://arxiv.org/abs/2407.06153v1 |
|updated| 2024-07-08 17:27:17 UTC |
|summary| The increasing development of large language models LLMs in code generationhas drawn significant attention among researchers. To enhance LLM-based codegeneration ability current efforts are predominantly directed towardscollecting high-quality datasets and leveraging diverse training technologies.However there is a notable lack of comprehensive studies examining thelimitations and boundaries of these existing methods. To bridge this gap weconducted an extensive empirical study evaluating the performance of threeleading closed-source LLMs and four popular open-source LLMs on three commonlyused benchmarks. Our investigation which evaluated the length cyclomaticcomplexity and API number of the generated code revealed that these LLMs facechallenges in generating successful code for more complex problems and tend toproduce code that is shorter yet more complicated as compared to canonicalsolutions. Additionally we developed a taxonomy of bugs for incorrect codesthat includes three categories and 12 sub-categories and analyze the rootcause for common bug types. Furthermore to better understand the performanceof LLMs in real-world projects we manually created a real-world benchmarkcomprising 140 code generation tasks. Our analysis highlights distinctdifferences in bug distributions between actual scenarios and existingbenchmarks. Finally we propose a novel training-free iterative method thatintroduces self-critique enabling LLMs to critique and correct their generatedcode based on bug types and compiler feedback. Experimental results demonstratethat our approach can significantly mitigate bugs and increase the passing rateby 29.2 after two iterations indicating substantial potential for LLMs tohandle more complex problems. |


| Item |Content|
| --- |---|
|idx| 2407.06146v1 |
|title| Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks |
|authors| Lukas NetzJan ReimarBernhard Rumpe
|links| http://arxiv.org/abs/2407.06146v1 |
|updated| 2024-07-08 17:19:59 UTC |
|summary| We present and evaluate a method called grammar masking which is used toguide large language models LLMs toward producing syntactically correctmodels for a given context-free grammar. Prompt engineering methods such asfew-shot learning or priming can be used to improve the chances of an LLMproducing correct syntax but the more complex the grammar the moretime-consuming and less promising these methods become. Previous work isfocused primarily on the usage of either language model training or promptengineering. In this work a method is presented that restricts the output to agiven grammar using constrained decoding to ensure the output adheres to avalid syntax. We use several DSLs built with MontiCore and task multiple LLMsto produce models with and without constrained decoding. A corresponding parseris used to confirm the syntactic correctness of each model. We show thatgrammar masking can dramatically improve the modeling capabilities of severalLLMs reducing the need for well-refined prompting while increasing the chanceof producing correct models. |


# cs.AI 

| Item |Content|
| --- |---|
|idx| 2407.06192v1 |
|title| Multi-Object Hallucination in Vision-Language Models |
|authors| Xuweiyi ChenZiqiao MaXuejun ZhangSihan XuShengyi QianJianing YangDavid F. FouheyJoyce Chai
|links| http://arxiv.org/abs/2407.06192v1 |
|updated| 2024-07-08 17:59:57 UTC |
|summary| Large vision language models LVLMs often suffer from object hallucinationproducing objects not present in the given images. While current benchmarks forobject hallucination primarily concentrate on the presence of a single objectclass rather than individual entities this work systematically investigatesmulti-object hallucination examining how models misperceive e.g. inventnonexistent objects or become distracted when tasked with focusing on multipleobjects simultaneously. We introduce Recognition-based Object ProbingEvaluation ROPE an automated evaluation protocol that considers thedistribution of object classes within a single image during testing and usesvisual referring prompts to eliminate ambiguity. With comprehensive empiricalstudies and analysis of potential factors leading to multi-objecthallucination we found that 1 LVLMs suffer more hallucinations when focusingon multiple objects compared to a single object. 2 The tested object classdistribution affects hallucination behaviors indicating that LVLMs may followshortcuts and spurious correlations.3 Hallucinatory behaviors are influencedby data-specific factors salience and frequency and model intrinsicbehaviors. We hope to enable LVLMs to recognize and reason about multipleobjects that often occur in realistic visual scenes provide insights andquantify our progress towards mitigating the issues. |


| Item |Content|
| --- |---|
|idx| 2407.06189v1 |
|title| Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision |
|authors| Orr ZoharXiaohan WangYonatan BittonIdan SzpektorSerena Yeung-Levy
|links| http://arxiv.org/abs/2407.06189v1 |
|updated| 2024-07-08 17:59:42 UTC |
|summary| The performance of Large Vision Language Models LVLMs is dependent on thesize and quality of their training datasets. Existing video instruction tuningdatasets lack diversity as they are derived by prompting large language modelswith video captions to generate question-answer pairs and are therefore mostlydescriptive. Meanwhile many labeled video datasets with diverse labels andsupervision exist - however we find that their integration into LVLMs isnon-trivial. Herein we present Video Self-Training with augmented ReasoningVideo-STaR the first video self-training approach. Video-STaR allows theutilization of any labeled video dataset for video instruction tuning. InVideo-STaR an LVLM cycles between instruction generation and finetuning whichwe show I improves general video understanding and II adapts LVLMs to noveldownstream tasks with existing supervision. During generation an LVLM isprompted to propose an answer. The answers are then filtered only to those thatcontain the original video labels and the LVLM is then re-trained on thegenerated dataset. By only training on generated answers that contain thecorrect video labels Video-STaR utilizes these existing video labels as weaksupervision for video instruction tuning. Our results demonstrate thatVideo-STaR-enhanced LVLMs exhibit improved performance in I general video QAwhere TempCompass performance improved by 10 and II on downstream taskswhere Video-STaR improved Kinetics700-QA accuracy by 20 and action qualityassessment on FineDiving by 15. |


| Item |Content|
| --- |---|
|idx| 2407.06177v1 |
|title| Vision-Language Models under Cultural and Inclusive Considerations |
|authors| Antonia KaramolegkouPhillip RustYong CaoRuixiang CuiAnders SøgaardDaniel Hershcovich
|links| http://arxiv.org/abs/2407.06177v1 |
|updated| 2024-07-08 17:50:00 UTC |
|summary| Large vision-language models VLMs can assist visually impaired people bydescribing images from their daily lives. Current evaluation datasets may notreflect diverse cultural user backgrounds or the situational context of thisuse case. To address this problem we create a survey to determine captionpreferences and propose a culture-centric evaluation benchmark by filteringVizWiz an existing dataset with images taken by people who are blind. We thenevaluate several VLMs investigating their reliability as visual assistants ina culturally diverse setting. While our results for state-of-the-art models arepromising we identify challenges such as hallucination and misalignment ofautomatic evaluation metrics with human judgment. We make our survey datacode and model outputs publicly available. |


| Item |Content|
| --- |---|
|idx| 2407.06172v1 |
|title| On Speeding Up Language Model Evaluation |
|authors| Jin Peng ZhouChristian K. BelardiRuihan WuTravis ZhangCarla P. GomesWen SunKilian Q. Weinberger
|links| http://arxiv.org/abs/2407.06172v1 |
|updated| 2024-07-08 17:48:42 UTC |
|summary| Large language models LLMs currently dominate the field of natural languageprocessing NLP representing the state-of-the-art across a diverse array oftasks. Developing a model of this nature from training to inference requiresmaking numerous decisions which define a combinatorial search problem. Forexample selecting the optimal pre-trained LLM prompt or hyperparameters toattain the best performance for a task often requires evaluating multiplecandidates on an entire test set. This exhaustive evaluation can betime-consuming and costly as both inference and metric computation with LLMsare resource-intensive. In this paper we address the challenge of identifyingthe best method within a limited budget for evaluating methods on testexamples. By leveraging the well-studied multi-armed bandit framework whichsequentially selects the next method-example pair to evaluate our approachcombining multi-armed bandit algorithms with low-rank factorizationsignificantly reduces the required resources. Experiments show that ouralgorithms can identify the top-performing method using only 5-15 of thetypically needed resources resulting in an 85-95 reduction in cost. |


| Item |Content|
| --- |---|
|idx| 2407.06152v1 |
|title| Uni-ELF: A Multi-Level Representation Learning Framework for Electrolyte Formulation Design |
|authors| Boshen ZengSian ChenXinxin LiuChanghong ChenBin DengXiaoxu WangZhifeng GaoYuzhi ZhangWeinan ELinfeng Zhang
|links| http://arxiv.org/abs/2407.06152v1 |
|updated| 2024-07-08 17:26:49 UTC |
|summary| Advancements in lithium battery technology heavily rely on the design andengineering of electrolytes. However current schemes for molecular design andrecipe optimization of electrolytes lack an effectivecomputational-experimental closed loop and often fall short in accuratelypredicting diverse electrolyte formulation properties. In this work weintroduce Uni-ELF a novel multi-level representation learning framework toadvance electrolyte design. Our approach involves two-stage pretraining:reconstructing three-dimensional molecular structures at the molecular levelusing the Uni-Mol model and predicting statistical structural propertiese.g. radial distribution functions from molecular dynamics simulations atthe mixture level. Through this comprehensive pretraining Uni-ELF is able tocapture intricate molecular and mixture-level information which significantlyenhances its predictive capability. As a result Uni-ELF substantiallyoutperforms state-of-the-art methods in predicting both molecular propertiese.g. melting point boiling point synthesizability and formulationproperties e.g. conductivity Coulombic efficiency. Moreover Uni-ELF can beseamlessly integrated into an automatic experimental design workflow. Webelieve this innovative framework will pave the way for automated AI-basedelectrolyte design and engineering. |


# cs.LG 

| Item |Content|
| --- |---|
|idx| 2407.06190v1 |
|title| 4D Contrastive Superflows are Dense 3D Representation Learners |
|authors| Xiang XuLingdong KongHui ShuaiWenwei ZhangLiang PanKai ChenZiwei LiuQingshan Liu
|links| http://arxiv.org/abs/2407.06190v1 |
|updated| 2024-07-08 17:59:54 UTC |
|summary| In the realm of autonomous driving accurate 3D perception is the foundation.However developing such models relies on extensive human annotations -- aprocess that is both costly and labor-intensive. To address this challenge froma data representation learning perspective we introduce SuperFlow a novelframework designed to harness consecutive LiDAR-camera pairs for establishingspatiotemporal pretraining objectives. SuperFlow stands out by integrating twokey designs: 1 a dense-to-sparse consistency regularization which promotesinsensitivity to point cloud density variations during feature learning and 2a flow-based contrastive learning module carefully crafted to extractmeaningful temporal cues from readily available sensor calibrations. To furtherboost learning efficiency we incorporate a plug-and-play view consistencymodule that enhances the alignment of the knowledge distilled from cameraviews. Extensive comparative and ablation studies across 11 heterogeneous LiDARdatasets validate our effectiveness and superiority. Additionally we observeseveral interesting emerging properties by scaling up the 2D and 3D backbonesduring pretraining shedding light on the future research of 3D foundationmodels for LiDAR-based perception. |


| Item |Content|
| --- |---|
|idx| 2407.06183v1 |
|title| Stepping on the Edge: Curvature Aware Learning Rate Tuners |
|authors| Vincent RouletAtish AgarwalaJean-Bastien GrillGrzegorz SwirszczMathieu BlondelFabian Pedregosa
|links| http://arxiv.org/abs/2407.06183v1 |
|updated| 2024-07-08 17:56:00 UTC |
|summary| Curvature information -- particularly the largest eigenvalue of the lossHessian known as the sharpness -- often forms the basis for learning ratetuners. However recent work has shown that the curvature information undergoescomplex dynamics during training going from a phase of increasing sharpness toeventual stabilization. We analyze the closed-loop feedback effect betweenlearning rate tuning and curvature. We find that classical learning rate tunersmay yield greater one-step loss reduction yet they ultimately underperform inthe long term when compared to constant learning rates in the full batchregime. These models break the stabilization of the sharpness which we explainusing a simplified model of the joint dynamics of the learning rate and thecurvature. To further investigate these effects we introduce a new learningrate tuning method Curvature Dynamics Aware Tuning CDAT which prioritizeslong term curvature stabilization over instantaneous progress on the objective.In the full batch regime CDAT shows behavior akin to prefixed warm-upschedules on deep learning objectives outperforming tuned constant learningrates. In the mini batch regime we observe that stochasticity introducesconfounding effects that explain the previous success of some learning ratetuners at appropriate batch sizes. Our findings highlight the critical role ofunderstanding the joint dynamics of the learning rate and curvature beyondgreedy minimization to diagnose failures and design effective adaptivelearning rate tuners. |


| Item |Content|
| --- |---|
|idx| 2407.06178v1 |
|title| Transfer Learning with Self-Supervised Vision Transformers for Snake Identification |
|authors| Anthony MiyaguchiMurilo GustineliAustin FischerRyan Lundqvist
|links| http://arxiv.org/abs/2407.06178v1 |
|updated| 2024-07-08 17:52:23 UTC |
|summary| We present our approach for the SnakeCLEF 2024 competition to predict snakespecies from images. We explore and use Metas DINOv2 vision transformer modelfor feature extraction to tackle species high variability and visualsimilarity in a dataset of 182261 images. We perform exploratory analysis onembeddings to understand their structure and train a linear classifier on theembeddings to predict species. Despite achieving a score of 39.69 our resultsshow promise for DINOv2 embeddings in snake identification. All code for thisproject is available at https://github.com/dsgt-kaggle-clef/snakeclef-2024. |


| Item |Content|
| --- |---|
|idx| 2407.06169v1 |
|title| Potential Based Diffusion Motion Planning |
|authors| Yunhao LuoChen SunJoshua B. TenenbaumYilun Du
|links| http://arxiv.org/abs/2407.06169v1 |
|updated| 2024-07-08 17:48:39 UTC |
|summary| Effective motion planning in high dimensional spaces is a long-standing openproblem in robotics. One class of traditional motion planning algorithmscorresponds to potential-based motion planning. An advantage of potential basedmotion planning is composability -- different motion constraints can be easilycombined by adding corresponding potentials. However constructing motion pathsfrom potentials requires solving a global optimization across configurationspace potential landscape which is often prone to local minima. We propose anew approach towards learning potential based motion planning where we train aneural network to capture and learn an easily optimizable potentials overmotion planning trajectories. We illustrate the effectiveness of such approachsignificantly outperforming both classical and recent learned motion planningapproaches and avoiding issues with local minima. We further illustrate itsinherent composability enabling us to generalize to a multitude of differentmotion constraints. |


| Item |Content|
| --- |---|
|idx| 2407.06167v1 |
|title| DεpS: Delayed ε-Shrinking for Faster Once-For-All Training |
|authors| Aditya AnnavajjalaAlind KhareAnimesh AgrawalIgor FedorovHugo LatapieMyungjin LeeAlexey Tumanov
|links| http://arxiv.org/abs/2407.06167v1 |
|updated| 2024-07-08 17:45:40 UTC |
|summary| CNNs are increasingly deployed across different hardware dynamicenvironments and low-power embedded devices. This has led to the design andtraining of CNN architectures with the goal of maximizing accuracy subject tosuch variable deployment constraints. As the number of deployment scenariosgrows there is a need to find scalable solutions to design and trainspecialized CNNs. Once-for-all training has emerged as a scalable approach thatjointly co-trains many models subnets at once with a constant training costand finds specialized CNNs later. The scalability is achieved by training thefull model and simultaneously reducing it to smaller subnets that share modelweights weight-shared shrinking. However existing once-for-all trainingapproaches incur huge training costs reaching 1200 GPU hours. We argue this isbecause they either start the process of shrinking the full model too early ortoo late. Hence we propose Delayed epsilon-Shrinking DepsilonpS thatstarts the process of shrinking the full model when it is partially trained50 which leads to training cost improvement and better in-place knowledgedistillation to smaller models. The proposed approach also consists of novelheuristics that dynamically adjust subnet learning rates incrementally Eleading to improved weight-shared knowledge distillation from larger to smallersubnets as well. As a result DEpS outperforms state-of-the-art once-for-alltraining techniques across different datasets including CIFAR10/100ImageNet-100 and ImageNet-1k on accuracy and cost. It achieves 1.83 higherImageNet-1k top1 accuracy or the same accuracy with 1.3x reduction in FLOPs and2.5x drop in training cost GPUhrs |


# cs.CV 

| Item |Content|
| --- |---|
|idx| 2407.06192v1 |
|title| Multi-Object Hallucination in Vision-Language Models |
|authors| Xuweiyi ChenZiqiao MaXuejun ZhangSihan XuShengyi QianJianing YangDavid F. FouheyJoyce Chai
|links| http://arxiv.org/abs/2407.06192v1 |
|updated| 2024-07-08 17:59:57 UTC |
|summary| Large vision language models LVLMs often suffer from object hallucinationproducing objects not present in the given images. While current benchmarks forobject hallucination primarily concentrate on the presence of a single objectclass rather than individual entities this work systematically investigatesmulti-object hallucination examining how models misperceive e.g. inventnonexistent objects or become distracted when tasked with focusing on multipleobjects simultaneously. We introduce Recognition-based Object ProbingEvaluation ROPE an automated evaluation protocol that considers thedistribution of object classes within a single image during testing and usesvisual referring prompts to eliminate ambiguity. With comprehensive empiricalstudies and analysis of potential factors leading to multi-objecthallucination we found that 1 LVLMs suffer more hallucinations when focusingon multiple objects compared to a single object. 2 The tested object classdistribution affects hallucination behaviors indicating that LVLMs may followshortcuts and spurious correlations.3 Hallucinatory behaviors are influencedby data-specific factors salience and frequency and model intrinsicbehaviors. We hope to enable LVLMs to recognize and reason about multipleobjects that often occur in realistic visual scenes provide insights andquantify our progress towards mitigating the issues. |


| Item |Content|
| --- |---|
|idx| 2407.06191v1 |
|title| Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side Images |
|authors| Zhangyang QiYunhan YangMengchen ZhangLong XingXiaoyang WuTong WuDahua LinXihui LiuJiaqi WangHengshuang Zhao
|links| http://arxiv.org/abs/2407.06191v1 |
|updated| 2024-07-08 17:59:55 UTC |
|summary| Recent advances in 3D AIGC have shown promise in directly creating 3D objectsfrom text and images offering significant cost savings in animation andproduct design. However detailed edit and customization of 3D assets remains along-standing challenge. Specifically 3D Generation methods lack the abilityto follow finely detailed instructions as precisely as their 2D image creationcounterparts. Imagine you can get a toy through 3D AIGC but with undesiredaccessories and dressing. To tackle this challenge we propose a novel pipelinecalled Tailor3D which swiftly creates customized 3D assets from editabledual-side images. We aim to emulate a tailors ability to locally changeobjects or perform overall style transfer. Unlike creating 3D assets frommultiple views using dual-side images eliminates conflicts on overlappingareas that occur when editing individual views. Specifically it begins byediting the front view then generates the back view of the object throughmulti-view diffusion. Afterward it proceeds to edit the back views. Finally aDual-sided LRM is proposed to seamlessly stitch together the front and back 3Dfeatures akin to a tailor sewing together the front and back of a garment. TheDual-sided LRM rectifies imperfect consistencies between the front and backviews enhancing editing capabilities and reducing memory burdens whileseamlessly integrating them into a unified 3D representation with the LoRATriplane Transformer. Experimental results demonstrate Tailor3Ds effectivenessacross various 3D generation and editing tasks including 3D generative filland style transfer. It provides a user-friendly efficient solution for editing3D assets with each editing step taking only seconds to complete. |


| Item |Content|
| --- |---|
|idx| 2407.06190v1 |
|title| 4D Contrastive Superflows are Dense 3D Representation Learners |
|authors| Xiang XuLingdong KongHui ShuaiWenwei ZhangLiang PanKai ChenZiwei LiuQingshan Liu
|links| http://arxiv.org/abs/2407.06190v1 |
|updated| 2024-07-08 17:59:54 UTC |
|summary| In the realm of autonomous driving accurate 3D perception is the foundation.However developing such models relies on extensive human annotations -- aprocess that is both costly and labor-intensive. To address this challenge froma data representation learning perspective we introduce SuperFlow a novelframework designed to harness consecutive LiDAR-camera pairs for establishingspatiotemporal pretraining objectives. SuperFlow stands out by integrating twokey designs: 1 a dense-to-sparse consistency regularization which promotesinsensitivity to point cloud density variations during feature learning and 2a flow-based contrastive learning module carefully crafted to extractmeaningful temporal cues from readily available sensor calibrations. To furtherboost learning efficiency we incorporate a plug-and-play view consistencymodule that enhances the alignment of the knowledge distilled from cameraviews. Extensive comparative and ablation studies across 11 heterogeneous LiDARdatasets validate our effectiveness and superiority. Additionally we observeseveral interesting emerging properties by scaling up the 2D and 3D backbonesduring pretraining shedding light on the future research of 3D foundationmodels for LiDAR-based perception. |


| Item |Content|
| --- |---|
|idx| 2407.06189v1 |
|title| Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision |
|authors| Orr ZoharXiaohan WangYonatan BittonIdan SzpektorSerena Yeung-Levy
|links| http://arxiv.org/abs/2407.06189v1 |
|updated| 2024-07-08 17:59:42 UTC |
|summary| The performance of Large Vision Language Models LVLMs is dependent on thesize and quality of their training datasets. Existing video instruction tuningdatasets lack diversity as they are derived by prompting large language modelswith video captions to generate question-answer pairs and are therefore mostlydescriptive. Meanwhile many labeled video datasets with diverse labels andsupervision exist - however we find that their integration into LVLMs isnon-trivial. Herein we present Video Self-Training with augmented ReasoningVideo-STaR the first video self-training approach. Video-STaR allows theutilization of any labeled video dataset for video instruction tuning. InVideo-STaR an LVLM cycles between instruction generation and finetuning whichwe show I improves general video understanding and II adapts LVLMs to noveldownstream tasks with existing supervision. During generation an LVLM isprompted to propose an answer. The answers are then filtered only to those thatcontain the original video labels and the LVLM is then re-trained on thegenerated dataset. By only training on generated answers that contain thecorrect video labels Video-STaR utilizes these existing video labels as weaksupervision for video instruction tuning. Our results demonstrate thatVideo-STaR-enhanced LVLMs exhibit improved performance in I general video QAwhere TempCompass performance improved by 10 and II on downstream taskswhere Video-STaR improved Kinetics700-QA accuracy by 20 and action qualityassessment on FineDiving by 15. |


| Item |Content|
| --- |---|
|idx| 2407.06188v1 |
|title| CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation |
|authors| Xinying GuoMingyuan ZhangHaozhe XieChenyang GuZiwei Liu
|links| http://arxiv.org/abs/2407.06188v1 |
|updated| 2024-07-08 17:59:36 UTC |
|summary| Crowd Motion Generation is essential in entertainment industries such asanimation and games as well as in strategic fields like urban simulation andplanning. This new task requires an intricate integration of control andgeneration to realistically synthesize crowd dynamics under specific spatialand semantic constraints whose challenges are yet to be fully explored. On theone hand existing human motion generation models typically focus on individualbehaviors neglecting the complexities of collective behaviors. On the otherhand recent methods for multi-person motion generation depend heavily onpre-defined scenarios and are limited to a fixed small number of inter-personinteractions thus hampering their practicality. To overcome these challengeswe introduce CrowdMoGen a zero-shot text-driven framework that harnesses thepower of Large Language Model LLM to incorporate the collective intelligenceinto the motion generation framework as guidance thereby enablinggeneralizable planning and generation of crowd motions without paired trainingdata. Our framework consists of two key components: 1 Crowd Scene Planner thatlearns to coordinate motions and dynamics according to specific scene contextsor introduced perturbations and 2 Collective Motion Generator thatefficiently synthesizes the required collective motions based on the holisticplans. Extensive quantitative and qualitative experiments have validated theeffectiveness of our framework which not only fills a critical gap byproviding scalable and generalizable solutions for Crowd Motion Generation taskbut also achieves high levels of realism and flexibility. |


# stat.ML 

| Item |Content|
| --- |---|
|idx| 2407.06120v1 |
|title| Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning |
|authors| Yijun DongHoang PhanXiang PanQi Lei
|links| http://arxiv.org/abs/2407.06120v1 |
|updated| 2024-07-08 16:57:26 UTC |
|summary| We revisit data selection in a modern context of finetuning from afundamental perspective. Extending the classical wisdom of varianceminimization in low dimensions to high-dimensional finetuning ourgeneralization analysis unveils the importance of additionally reducing biasinduced by low-rank approximation. Inspired by the variance-bias tradeoff inhigh dimensions from the theory we introduce Sketchy Moment Matching SkMM ascalable data selection scheme with two stages. i First the bias iscontrolled using gradient sketching that explores the finetuning parameterspace for an informative low-dimensional subspace mathcalS ii then thevariance is reduced over mathcalS via moment matching between the originaland selected datasets. Theoretically we show that gradient sketching is fastand provably accurate: selecting n samples by reducing variance overmathcalS preserves the fast-rate generalization OdimmathcalS/nindependent of the parameter dimension. Empirically we concretize thevariance-bias balance via synthetic experiments and demonstrate theeffectiveness of SkMM for finetuning in real vision tasks. |


| Item |Content|
| --- |---|
|idx| 2407.06015v1 |
|title| Simulation-based Benchmarking for Causal Structure Learning in Gene Perturbation Experiments |
|authors| Luka KovačevićIzzy NewshamSach MukherjeeJohn Whittaker
|links| http://arxiv.org/abs/2407.06015v1 |
|updated| 2024-07-08 15:06:03 UTC |
|summary| Causal structure learning CSL refers to the task of learning causalrelationships from data. Advances in CSL now allow learning of causal graphs indiverse application domains which has the potential to facilitate data-drivencausal decision-making. Real-world CSL performance depends on a number oftextitcontext-specific factors including context-specific datadistributions and non-linear dependencies that are important in practicaluse-cases. However our understanding of how to assess and select CSL methodsin specific contexts remains limited. To address this gap we presenttextitCausalRegNet a multiplicative effect structural causal model thatallows for generating observational and interventional data incorporatingcontext-specific properties with a focus on the setting of gene perturbationexperiments. Using real-world gene perturbation data we show that CausalRegNetgenerates accurate distributions and scales far better than current simulationframeworks. We illustrate the use of CausalRegNet in assessing CSL methods inthe context of interventional experiments in biology. |


| Item |Content|
| --- |---|
|idx| 2407.05895v1 |
|title| Link Representation Learning for Probabilistic Travel Time Estimation |
|authors| Chen XuQiang WangLijun Sun
|links| http://arxiv.org/abs/2407.05895v1 |
|updated| 2024-07-08 13:01:53 UTC |
|summary| Travel time estimation is a crucial application in navigation apps and webmapping services. Current deterministic and probabilistic methods primarilyfocus on modeling individual trips assuming independence among trips. Howeverin real-world scenarios we often observe strong inter-trip correlations due tofactors such as weather conditions traffic management and road works. In thispaper we propose to model trip-level link travel time using a Gaussianhierarchical model which can characterize both inter-trip and intra-tripcorrelations. The joint distribution of travel time of multiple trips becomes amultivariate Gaussian parameterized by learnable link representations. Toeffectively use the sparse GPS trajectories we also propose a dataaugmentation method based on trip sub-sampling which allows for fine-grainedgradient backpropagation in learning link representations. During inference weestimate the probability distribution of the travel time of a queried tripconditional on the completed trips that are spatiotemporally adjacent. We referto the overall framework as ProbTTE. We evaluate ProbTTE on two real-world GPStrajectory datasets and the results demonstrate its superior performancecompared to state-of-the-art deterministic and probabilistic baselines.Additionally we find that the learned link representations align well with thephysical geometry of the network making them suitable as input for otherapplications. |


| Item |Content|
| --- |---|
|idx| 2407.05790v1 |
|title| Kinetic Interacting Particle Langevin Monte Carlo |
|authors| Paul Felix Valsecchi OlivaO. Deniz Akyildiz
|links| http://arxiv.org/abs/2407.05790v1 |
|updated| 2024-07-08 09:52:46 UTC |
|summary| This paper introduces and analyses interacting underdamped Langevinalgorithms termed Kinetic Interacting Particle Langevin Monte Carlo KIPLMCmethods for statistical inference in latent variable models. We propose adiffusion process that evolves jointly in the space of parameters and latentvariables and exploit the fact that the stationary distribution of thisdiffusion concentrates around the maximum marginal likelihood estimate of theparameters. We then provide two explicit discretisations of this diffusion aspractical algorithms to estimate parameters of statistical models. For eachalgorithm we obtain nonasymptotic rates of convergence for the case where thejoint log-likelihood is strongly concave with respect to latent variables andparameters. In particular we provide convergence analysis for the diffusiontogether with the discretisation error providing convergence rate estimatesfor the algorithms in Wasserstein-2 distance. To demonstrate the utility of theintroduced methodology we provide numerical experiments that demonstrate theeffectiveness of the proposed diffusion for statistical inference and thestability of the numerical integrators utilised for discretisation. Our settingcovers a broad number of applications including unsupervised learningstatistical inference and inverse problems. |


| Item |Content|
| --- |---|
|idx| 2407.05760v1 |
|title| Dirichlet process mixture model based on topologically augmented signal representation for clustering infant vocalizations |
|authors| Guillem BonafosClara BourotPierre PudloJean-Marc FreyermuthLaurence ReboulSamuel TronçonArnaud Rey
|links| http://arxiv.org/abs/2407.05760v1 |
|updated| 2024-07-08 09:12:52 UTC |
|summary| Based on audio recordings made once a month during the first 12 months of achilds life we propose a new method for clustering this set of vocalizations.We use a topologically augmented representation of the vocalizations employingtwo persistence diagrams for each vocalization: one computed on the surface ofits spectrogram and one on the Takens embeddings of the vocalization. Asynthetic persistent variable is derived for each diagram and added to theMFCCs Mel-frequency cepstral coefficients. Using this representation we fita non-parametric Bayesian mixture model with a Dirichlet process prior to modelthe number of components. This procedure leads to a novel data-drivencategorization of vocal productions. Our findings reveal the presence of 8clusters of vocalizations allowing us to compare their temporal distributionand acoustic profiles in the first 12 months of life. |


# cs.HC 

| Item |Content|
| --- |---|
|idx| 2407.06129v1 |
|title| Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization |
|authors| Hannah K. BakoArshnoor ButhaniXinyi LiuKwesi A. CobbinaZhicheng Liu
|links| http://arxiv.org/abs/2407.06129v1 |
|updated| 2024-07-08 17:04:31 UTC |
|summary| Automatically generating data visualizations in response to human utteranceson datasets necessitates a deep semantic understanding of the data utteranceincluding implicit and explicit references to data attributes visualizationtasks and necessary data preparation steps. Natural Language Interfaces NLIsfor data visualization have explored ways to infer such information yetchallenges persist due to inherent uncertainty in human speech. Recent advancesin Large Language Models LLMs provide an avenue to address these challengesbut their ability to extract the relevant semantic information remainsunexplored. In this study we evaluate four publicly available LLMs GPT-4Gemini-Pro Llama3 and Mixtral investigating their ability to comprehendutterances even in the presence of uncertainty and identify the relevant datacontext and visual tasks. Our findings reveal that LLMs are sensitive touncertainties in utterances. Despite this sensitivity they are able to extractthe relevant data context. However LLMs struggle with inferring visualizationtasks. Based on these results we highlight future research directions on usingLLMs for visualization generation. |


| Item |Content|
| --- |---|
|idx| 2407.06125v1 |
|title| Depression Detection and Analysis using Large Language Models on Textual and Audio-Visual Modalities |
|authors| Avinash AnandChayan TankSarthak PolVinayak KatochShaina MehtaRajiv Ratn Shah
|links| http://arxiv.org/abs/2407.06125v1 |
|updated| 2024-07-08 17:00:51 UTC |
|summary| Depression has proven to be a significant public health issue profoundlyaffecting the psychological well-being of individuals. If it remainsundiagnosed depression can lead to severe health issues which can manifestphysically and even lead to suicide. Generally Diagnosing depression or anyother mental disorder involves conducting semi-structured interviews alongsidesupplementary questionnaires including variants of the Patient HealthQuestionnaire PHQ by Clinicians and mental health professionals. Thisapproach places significant reliance on the experience and judgment of trainedphysicians making the diagnosis susceptible to personal biases. Given that theunderlying mechanisms causing depression are still being actively researchedphysicians often face challenges in diagnosing and treating the conditionparticularly in its early stages of clinical presentation. Recentlysignificant strides have been made in Artificial neural computing to solveproblems involving text image and speech in various domains. Our analysis hasaimed to leverage these state-of-the-art SOTA models in our experiments toachieve optimal outcomes leveraging multiple modalities. The experiments wereperformed on the Extended Distress Analysis Interview Corpus Wizard of Ozdataset E-DAIC corpus presented in the Audio/Visual Emotion Challenge AVEC2019 Challenge. The proposed solutions demonstrate better results achieved byProprietary and Open-source Large Language Models LLMs which achieved a RootMean Square Error RMSE score of 3.98 on Textual Modality beating the AVEC2019 challenge baseline results and current SOTA regression analysisarchitectures. Additionally the proposed solution achieved an accuracy of71.43 in the classification task. The paper also includes a novel audio-visualmulti-modal network that predicts PHQ-8 scores with an RMSE of 6.51. |


| Item |Content|
| --- |---|
|idx| 2407.06123v1 |
|title| Investigating User Perceptions of Collaborative Agenda Setting in Virtual Health Counseling Session |
|authors| Mina FallahFarnaz NouraeiHye Sun YunTimothy Bickmore
|links| http://arxiv.org/abs/2407.06123v1 |
|updated| 2024-07-08 16:59:50 UTC |
|summary| Virtual health counselors offer the potential to provide users withinformation and counseling in complex areas such as disease management andhealth education. However ensuring user engagement is challengingparticularly when the volume of information and length of counseling sessionsincrease. Agenda setting a clinical counseling technique where a patient andclinician collaboratively decide on session topics is an effective approach totailoring discussions for individual patient needs and sustaining engagement.We explore the effectiveness of agenda setting in a virtual counselor systemdesigned to counsel women for breast cancer genetic testing. In a betweensubjects study we assessed three versions of the system with varying levels ofuser control in the systems agenda setting approach. We found thatparticipants knowledge improved across all conditions. Although our resultsshowed that any type of agenda setting was perceived as useful regardless ofuser control interviews revealed a preference for more collaboration and userinvolvement in the agenda setting process. Our study highlights the importanceof using patient-centered approaches such as tailored discussions when usingvirtual counselors in healthcare. |


| Item |Content|
| --- |---|
|idx| 2407.05977v1 |
|title| Exploring Human-LLM Conversations: Mental Models and the Originator of Toxicity |
|authors| Johannes SchneiderArianna Casanova FloresAnne-Catherine Kranz
|links| http://arxiv.org/abs/2407.05977v1 |
|updated| 2024-07-08 14:20:05 UTC |
|summary| This study explores real-world human interactions with large language modelsLLMs in diverse unconstrained settings in contrast to most prior researchfocusing on ethically trimmed models like ChatGPT for specific tasks. We aim tounderstand the originator of toxicity. Our findings show that although LLMs arerightfully accused of providing toxic content it is mostly demanded or atleast provoked by humans who actively seek such content. Our manual analysis ofhundreds of conversations judged as toxic by APIs commercial vendors alsoraises questions with respect to current practices of what user requests arerefused to answer. Furthermore we conjecture based on multiple empiricalindicators that humans exhibit a change of their mental model switching fromthe mindset of interacting with a machine more towards interacting with ahuman. |


| Item |Content|
| --- |---|
|idx| 2407.05874v1 |
|title| Investigating Trading Mechanisms as a Driver for User Experience in Racing Games |
|authors| Georg Arbesser-RastburgThomas OlipJohanna Pirker
|links| http://arxiv.org/abs/2407.05874v1 |
|updated| 2024-07-08 12:33:51 UTC |
|summary| The exchange of digital goods has become a significant aspect of the globaleconomy with digital products offering inexpensive reproduction anddistribution. In-game objects a type of digital currency have emerged astradable commodities within gaming ecosystems. Despite extensive research onvarious aspects of digital goods little attention has been given to the impactof in-game trading mechanisms on user experience. This paper presents a studyaimed at evaluating the influence of trading systems on user experience in aracing game context. We developed a simple racing game featuring an in-gamemarket for buying and selling car variants and conducted an A/B study comparinguser experiences between groups utilizing the trading system and thoseunlocking cars through race completion. Our findings suggest that while thetrading system did not significantly alter the overall user experience furtherexploration of diverse trading approaches may offer insights into their impacton user engagement. |


# cs.MA 

| Item |Content|
| --- |---|
|idx| 2407.05763v1 |
|title| Homogeneous Distributed Observers for Quasilinear Systems |
|authors| Min LiAndrey PolyakovSiyuan WangGang Zheng
|links| http://arxiv.org/abs/2407.05763v1 |
|updated| 2024-07-08 09:16:00 UTC |
|summary| The problem of finite/fixed-time cooperative state estimation is consideredfor a class of quasilinear systems with nonlinearities satisfying a Holdercondition. A strongly connected nonlinear distributed observer is designedunder the assumption of global observability. By proper parameter tuning withlinear matrix inequalities the observer error equation possessesfinite/fixed-time stability in the perturbation-free case and input-to-statestability with respect to bounded perturbations. Numerical simulations areperformed to validate this design. |


| Item |Content|
| --- |---|
|idx| 2407.05132v1 |
|title| Fair Money -- Public Good Value Pricing With Karma Economies |
|authors| Kevin RiehlAnastasios KouvelasMichail Makridis
|links| http://arxiv.org/abs/2407.05132v1 |
|updated| 2024-07-06 16:56:29 UTC |
|summary| City road infrastructure is a public good and over-consumption byself-interested rational individuals leads to traffic jams. Congestion pricingis effective in reducing demand to sustainable levels but also controversialas it introduces equity issues and systematically discriminates lower-incomegroups. Karma is a non-monetary fair and efficient resource allocationmechanism that employs an artificial currency different from money thatincentivizes cooperation amongst selfish individuals and achieves a balancebetween giving and taking. Where money does not do its job Karma achievessocially more desirable resource allocations by being aligned with consumersneeds rather than their financial power. This work highlights the valueproposition of Karma gives guidance on important Karma mechanism designelements and equips the reader with a useful software framework to model Karmaeconomies and predict consumers behaviour. A case study demonstrates thepotential of this feasible alternative to money without the burden ofadditional fees. |


| Item |Content|
| --- |---|
|idx| 2407.04608v1 |
|title| A Multi-Player Potential Game Approach for Sensor Network Localization with Noisy Measurements |
|authors| Gehui XuGuanpu ChenBaris FidanYiguang HongHongsheng QiThomas ParisiniKarl H. Johansson
|links| http://arxiv.org/abs/2407.04608v1 |
|updated| 2024-07-05 16:00:19 UTC |
|summary| Sensor network localization SNL is a challenging problem due to itsinherent non-convexity and the effects of noise in inter-node rangingmeasurements and anchor node position. We formulate a non-convex SNL problem asa multi-player non-convex potential game and investigate the existence anduniqueness of a Nash equilibrium NE in both the ideal setting withoutmeasurement noise and the practical setting with measurement noise. We firstshow that the NE exists and is unique in the noiseless case and corresponds tothe precise network localization. Then we study the SNL for the case witherrors affecting the anchor node position and the inter-node distancemeasurements. Specifically we establish that in case these errors aresufficiently small the NE exists and is unique. It is shown that the NE is anapproximate solution to the SNL problem and that the position errors can bequantified accordingly. Based on these findings we apply the results to casestudies involving only inter-node distance measurement errors and only anchorposition information inaccuracies. |


| Item |Content|
| --- |---|
|idx| 2407.04503v1 |
|title| When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions |
|authors| Jérémy PerezCorentin LégerGrgur KovačCédric ColasGaia MolinaroMaxime DerexPierre-Yves OudeyerClément Moulin-Frier
|links| http://arxiv.org/abs/2407.04503v1 |
|updated| 2024-07-05 13:44:09 UTC |
|summary| As large language models LLMs start interacting with each other andgenerating an increasing amount of text online it becomes crucial to betterunderstand how information is transformed as it passes from one LLM to thenext. While significant research has examined individual LLM behaviorsexisting studies have largely overlooked the collective behaviors andinformation distortions arising from iterated LLM interactions. Small biasesnegligible at the single output level risk being amplified in iteratedinteractions potentially leading the content to evolve towards attractorstates. In a series of telephone game experiments we apply a transmissionchain design borrowed from the human cultural evolution literature: LLM agentsiteratively receive produce and transmit texts from the previous to the nextagent in the chain. By tracking the evolution of text toxicity positivitydifficulty and length across transmission chains we uncover the existence ofbiases and attractors and study their dependence on the initial text theinstructions language model and model size. For instance we find that moreopen-ended instructions lead to stronger attraction effects compared to moreconstrained tasks. We also find that different text properties displaydifferent sensitivity to attraction effects with toxicity leading to strongerattractors than length. These findings highlight the importance of accountingfor multi-step transmission dynamics and represent a first step towards a morecomprehensive understanding of LLM cultural dynamics. |


| Item |Content|
| --- |---|
|idx| 2407.03956v1 |
|title| Solving Zebra Puzzles Using Constraint-Guided Multi-Agent Systems |
|authors| Shmuel BermanBaishakhi RayKathleen McKeown
|links| http://arxiv.org/abs/2407.03956v1 |
|updated| 2024-07-04 14:22:25 UTC |
|summary| Prior research has enhanced the ability of Large Language Models LLMs tosolve logic puzzles using techniques such as chain-of-thought prompting orintroducing a symbolic representation. These frameworks are still usuallyinsufficient to solve complicated logical problems such as Zebra puzzles dueto the inherent complexity of translating natural language clues into logicalstatements. We introduce a multi-agent system ZPS that integrates LLMs withan off the shelf theorem prover. This system tackles the complex puzzle-solvingtask by breaking down the problem into smaller manageable parts generatingSMT Satisfiability Modulo Theories code to solve them with a theorem proverand using feedback between the agents to repeatedly improve their answers. Wealso introduce an automated grid puzzle grader to assess the correctness of ourpuzzle solutions and show that the automated grader is reliable by evaluatingit in a user-study. Our approach shows improvement in all three LLMs we testedwith GPT-4 showing 166 improvement in the number of fully correct solutions. |


