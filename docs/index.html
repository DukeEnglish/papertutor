
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation</h3>
                <p>Authors: Xiangyu XuLijuan LiuShuicheng Yan</p>
                <p><a href="http://arxiv.org/abs/2404.15276v1">Link to paper</a></p>
                <p>Existing Transformers for monocular 3D human shape and pose estimationtypically have a quadratic computation and memory complexity with respect tothe feature length which hinders the exploitation of fine-grained informationin high-resolution features that is beneficial for accurate reconstruction. Inthis work we propose an SMPL-based Transformer framework SMPLer to addressthis issue. SMPLer incorporates two key ingredients: a decoupled attentionoperation and an SMPL-based target representation which allow effectiveutilization of high-resolution features in the Transformer. In addition basedon these two designs we also introduce several novel modules including amulti-scale attention and a joint-aware attention to further boost thereconstruction performance. Extensive experiments demonstrate the effectivenessof SMPLer against existing 3D human shape and pose estimation methods bothquantitatively and qualitatively. Notably the proposed algorithm achieves anMPJPE of 45.2 mm on the Human3.6M dataset improving upon Mesh Graphormer bymore than 10 with fewer than one-third of the parameters. Code and pretrainedmodels are available at https://github.com/xuxy09/SMPLer.</p>
                <p>Last Updated: 2024-04-23 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2404.15276v1">Interpret</button>
                <div id="interpretation-2404.15276v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios</h3>
                <p>Authors: Jingyang LinYingda XiaJianpeng ZhangKe YanLe LuJiebo LuoLing Zhang</p>
                <p><a href="http://arxiv.org/abs/2404.15272v1">Link to paper</a></p>
                <p>Medical Vision-Language Pretraining Med-VLP establishes a connectionbetween visual content from medical images and the relevant textualdescriptions. Existing Med-VLP methods primarily focus on 2D images depicting asingle body part notably chest X-rays. In this paper we extend the scope ofMed-VLP to encompass 3D images specifically targeting full-body scenarios byusing a multimodal dataset of CT images and reports. Compared with the 2Dcounterpart 3D VLP is required to effectively capture essential semantics fromsignificantly sparser representation in 3D imaging. In this paper we introduceCT-GLIP Grounded Language-Image Pretraining with CT scans a novel methodthat constructs organ-level image-text pairs to enhance multimodal contrastivelearning aligning grounded visual features with precise diagnostic text.Additionally we developed an abnormality dictionary to augment contrastivelearning with diverse negative samples. Our method trained on a multimodal CTdataset comprising 44011 organ-level vision-text pairs from 17702 patientsacross 104 organs demonstrates it can identify organs and abnormalities in azero-shot manner using natural languages. The performance of CT-GLIP isvalidated on a separate test set of 1130 patients focusing on the 16 mostfrequent abnormalities across 7 organs. The experimental results show ourmodels superior performance over the standard CLIP framework across zero-shotand fine-tuning scenarios using both CNN and ViT architectures.</p>
                <p>Last Updated: 2024-04-23 17:59:01 UTC</p>
                <button class="interpret-button" data-id="2404.15272v1">Interpret</button>
                <div id="interpretation-2404.15272v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models</h3>
                <p>Authors: Wanrong ZhuJennifer HealeyRuiyi ZhangWilliam Yang WangTong Sun</p>
                <p><a href="http://arxiv.org/abs/2404.15271v1">Link to paper</a></p>
                <p>Recent advancements in instruction-following models have made userinteractions with models more user-friendly and efficient broadening theirapplicability. In graphic design non-professional users often struggle tocreate visually appealing layouts due to limited skills and resources. In thiswork we introduce a novel multimodal instruction-following framework forlayout planning allowing users to easily arrange visual elements into tailoredlayouts by specifying canvas size and design purpose such as for book coversposters brochures or menus. We developed three layout reasoning tasks totrain the model in understanding and executing layout instructions. Experimentson two benchmarks show that our method not only simplifies the design processfor non-professionals but also surpasses the performance of few-shot GPT-4Vmodels with mIoU higher by 12 on Crello. This progress highlights thepotential of multimodal instruction-following models to automate and simplifythe design process providing an approachable solution for a wide range ofdesign tasks on visually-rich documents.</p>
                <p>Last Updated: 2024-04-23 17:58:33 UTC</p>
                <button class="interpret-button" data-id="2404.15271v1">Interpret</button>
                <div id="interpretation-2404.15271v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Aligning LLM Agents by Learning Latent Preference from User Edits</h3>
                <p>Authors: Ge GaoAlexey TaymanovEduardo SalinasPaul MineiroDipendra Misra</p>
                <p><a href="http://arxiv.org/abs/2404.15269v1">Link to paper</a></p>
                <p>We study interactive learning of language agents based on user edits made tothe agents output. In a typical setting such as writing assistants the userinteracts with a language agent to generate a response given a context and mayoptionally edit the agent response to personalize it based on their latentpreference in addition to improving the correctness. The edit feedback isnaturally generated making it a suitable candidate for improving the agentsalignment with the users preference and for reducing the cost of user editsover time. We propose a learning framework PRELUDE that infers a descriptionof the users latent preference based on historic edit data and using it todefine a prompt policy that drives future response generation. This avoidsfine-tuning the agent which is costly challenging to scale with the number ofusers and may even degrade its performance on other tasks. Furthermorelearning descriptive preference improves interpretability allowing the user toview and modify the learned preference. However user preference can be complexand vary based on context making it challenging to learn. To address this wepropose a simple yet effective algorithm named CIPHER that leverages a largelanguage model LLM to infer the user preference for a given context based onuser edits. In the future CIPHER retrieves inferred preferences from thek-closest contexts in the history and forms an aggregate preference forresponse generation. We introduce two interactive environments -- summarizationand email writing for evaluation using a GPT-4 simulated user. We compare withalgorithms that directly retrieve user edits but do not learn descriptivepreference and algorithms that learn context-agnostic preference. On bothtasks CIPHER achieves the lowest edit distance cost and learns preferencesthat show significant similarity to the ground truth preferences</p>
                <p>Last Updated: 2024-04-23 17:57:47 UTC</p>
                <button class="interpret-button" data-id="2404.15269v1">Interpret</button>
                <div id="interpretation-2404.15269v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TOP-Nav: Legged Navigation Integrating Terrain, Obstacle and Proprioception Estimation</h3>
                <p>Authors: Junli RenYikai LiuYingru DaiGuijin Wang</p>
                <p><a href="http://arxiv.org/abs/2404.15256v1">Link to paper</a></p>
                <p>Legged navigation is typically examined within open-world off-road andchallenging environments. In these scenarios estimating external disturbancesrequires a complex synthesis of multi-modal information. This underlines amajor limitation in existing works that primarily focus on avoiding obstacles.In this work we propose TOP-Nav a novel legged navigation framework thatintegrates a comprehensive path planner with Terrain awareness Obstacleavoidance and close-loop Proprioception. TOP-Nav underscores the synergiesbetween vision and proprioception in both path and motion planning. Within thepath planner we present and integrate a terrain estimator that enables therobot to select waypoints on terrains with higher traversability whileeffectively avoiding obstacles. In the motion planning level we not onlyimplement a locomotion controller to track the navigation commands but alsoconstruct a proprioception advisor to provide motion evaluations for the pathplanner. Based on the close-loop motion feedback we make online correctionsfor the vision-based terrain and obstacle estimations. Consequently TOP-Navachieves open-world navigation that the robot can handle terrains ordisturbances beyond the distribution of prior knowledge and overcomesconstraints imposed by visual conditions. Building upon extensive experimentsconducted in both simulation and real-world environments TOP-Nav demonstratessuperior performance in open-world navigation compared to existing methods.</p>
                <p>Last Updated: 2024-04-23 17:42:45 UTC</p>
                <button class="interpret-button" data-id="2404.15256v1">Interpret</button>
                <div id="interpretation-2404.15256v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Score matching for sub-Riemannian bridge sampling</h3>
                <p>Authors: Erlend GrongKaren HabermannStefan Sommer</p>
                <p><a href="http://arxiv.org/abs/2404.15258v1">Link to paper</a></p>
                <p>Simulation of conditioned diffusion processes is an essential tool ininference for stochastic processes data imputation generative modelling andgeometric statistics. Whilst simulating diffusion bridge processes is alreadydifficult on Euclidean spaces when considering diffusion processes onRiemannian manifolds the geometry brings in further complications. In evenhigher generality advancing from Riemannian to sub-Riemannian geometriesintroduces hypoellipticity and the possibility of finding appropriate explicitapproximations for the score of the diffusion process is removed. We handlethese challenges and construct a method for bridge simulation on sub-Riemannianmanifolds by demonstrating how recent progress in machine learning can bemodified to allow for training of score approximators on sub-Riemannianmanifolds. Since gradients dependent on the horizontal distribution wegeneralise the usual notion of denoising loss to work with non-holonomic framesusing a stochastic Taylor expansion and we demonstrate the resulting schemeboth explicitly on the Heisenberg group and more generally using adaptedcoordinates. We perform numerical experiments exemplifying samples from thebridge process on the Heisenberg group and the concentration of this processfor small time.</p>
                <p>Last Updated: 2024-04-23 17:45:53 UTC</p>
                <button class="interpret-button" data-id="2404.15258v1">Interpret</button>
                <div id="interpretation-2404.15258v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GIST: Gibbs self-tuning for locally adaptive Hamiltonian Monte Carlo</h3>
                <p>Authors: Nawaf Bou-RabeeBob CarpenterMilo Marsden</p>
                <p><a href="http://arxiv.org/abs/2404.15253v1">Link to paper</a></p>
                <p>We present a novel and flexible framework for localized tuning of HamiltonianMonte Carlo samplers by sampling the algorithms tuning parametersconditionally based on the position and momentum at each step. For adaptivelysampling path lengths we show that randomized Hamiltonian Monte Carlo theNo-U-Turn Sampler and the Apogee-to-Apogee Path Sampler all fit within thisunified framework as special cases. The framework is illustrated with a simplealternative to the No-U-Turn Sampler for locally adapting path lengths.</p>
                <p>Last Updated: 2024-04-23 17:39:20 UTC</p>
                <button class="interpret-button" data-id="2404.15253v1">Interpret</button>
                <div id="interpretation-2404.15253v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PHLP: Sole Persistent Homology for Link Prediction -- Interpretable Feature Extraction</h3>
                <p>Authors: Junwon YouEunwoo HeoJae-Hun Jung</p>
                <p><a href="http://arxiv.org/abs/2404.15225v1">Link to paper</a></p>
                <p>Link prediction LP inferring the connectivity between nodes is asignificant research area in graph data where a link represents essentialinformation on relationships between nodes. Although graph neural networkGNN-based models have achieved high performance in LP understanding why theyperform well is challenging because most comprise complex neural networks. Weemploy persistent homology PH a topological data analysis method that helpsanalyze the topological information of graphs to explain the reasons for thehigh performance. We propose a novel method that employs PH for LP PHLPfocusing on how the presence or absence of target links influences the overalltopology. The PHLP utilizes the angle hop subgraph and new node labeling calleddegree double radius node labeling Degree DRNL distinguishing theinformation of graphs better than DRNL. Using only a classifier PHLP performssimilarly to state-of-the-art SOTA models on most benchmark datasets.Incorporating the outputs calculated using PHLP into the existing GNN-basedSOTA models improves performance across all benchmark datasets. To the best ofour knowledge PHLP is the first method of applying PH to LP without GNNs. Theproposed approach employing PH while not relying on neural networks enablesthe identification of crucial factors for improving performance.</p>
                <p>Last Updated: 2024-04-23 16:54:56 UTC</p>
                <button class="interpret-button" data-id="2404.15225v1">Interpret</button>
                <div id="interpretation-2404.15225v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Conformal Predictive Systems Under Covariate Shift</h3>
                <p>Authors: Jef JonkersGlenn Van WallendaelLuc DuchateauSofie Van Hoecke</p>
                <p><a href="http://arxiv.org/abs/2404.15018v1">Link to paper</a></p>
                <p>Conformal Predictive Systems CPS offer a versatile framework forconstructing predictive distributions allowing for calibrated inference andinformative decision-making. However their applicability has been limited toscenarios adhering to the Independent and Identically Distributed IID modelassumption. This paper extends CPS to accommodate scenarios characterized bycovariate shifts. We therefore propose Weighted CPS WCPS akin to WeightedConformal Prediction WCP leveraging likelihood ratios between training andtesting covariate distributions. This extension enables the construction ofnonparametric predictive distributions capable of handling covariate shifts. Wepresent theoretical underpinnings and conjectures regarding the validity andefficacy of WCPS and demonstrate its utility through empirical evaluations onboth synthetic and real-world datasets. Our simulation experiments indicatethat WCPS are probabilistically calibrated under covariate shift.</p>
                <p>Last Updated: 2024-04-23 13:23:27 UTC</p>
                <button class="interpret-button" data-id="2404.15018v1">Interpret</button>
                <div id="interpretation-2404.15018v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Estimating the Distribution of Parameters in Differential Equations with Repeated Cross-Sectional Data</h3>
                <p>Authors: Hyeontae JoSung Woong ChoHyung Ju Hwang</p>
                <p><a href="http://arxiv.org/abs/2404.14873v1">Link to paper</a></p>
                <p>Differential equations are pivotal in modeling and understanding the dynamicsof various systems offering insights into their future states throughparameter estimation fitted to time series data. In fields such as economypolitics and biology the observation data points in the time series are oftenindependently obtained i.e. Repeated Cross-Sectional RCS data. With RCSdata we found that traditional methods for parameter estimation indifferential equations such as using mean values of time trajectories orGaussian Process-based trajectory generation have limitations in estimatingthe shape of parameter distributions often leading to a significant loss ofdata information. To address this issue we introduce a novel methodEstimation of Parameter Distribution EPD providing accurate distribution ofparameters without loss of data information. EPD operates in three main steps:generating synthetic time trajectories by randomly selecting observed values ateach time point estimating parameters of a differential equation that minimizethe discrepancy between these trajectories and the true solution of theequation and selecting the parameters depending on the scale of discrepancy.We then evaluated the performance of EPD across several models includingexponential growth logistic population models and target cell-limited modelswith delayed virus production demonstrating its superiority in capturing theshape of parameter distributions. Furthermore we applied EPD to real-worlddatasets capturing various shapes of parameter distributions rather than anormal distribution. These results effectively address the heterogeneity withinsystems marking a substantial progression in accurately modeling systems usingRCS data.</p>
                <p>Last Updated: 2024-04-23 10:01:43 UTC</p>
                <button class="interpret-button" data-id="2404.14873v1">Interpret</button>
                <div id="interpretation-2404.14873v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios</h3>
                <p>Authors: Jingyang LinYingda XiaJianpeng ZhangKe YanLe LuJiebo LuoLing Zhang</p>
                <p><a href="http://arxiv.org/abs/2404.15272v1">Link to paper</a></p>
                <p>Medical Vision-Language Pretraining Med-VLP establishes a connectionbetween visual content from medical images and the relevant textualdescriptions. Existing Med-VLP methods primarily focus on 2D images depicting asingle body part notably chest X-rays. In this paper we extend the scope ofMed-VLP to encompass 3D images specifically targeting full-body scenarios byusing a multimodal dataset of CT images and reports. Compared with the 2Dcounterpart 3D VLP is required to effectively capture essential semantics fromsignificantly sparser representation in 3D imaging. In this paper we introduceCT-GLIP Grounded Language-Image Pretraining with CT scans a novel methodthat constructs organ-level image-text pairs to enhance multimodal contrastivelearning aligning grounded visual features with precise diagnostic text.Additionally we developed an abnormality dictionary to augment contrastivelearning with diverse negative samples. Our method trained on a multimodal CTdataset comprising 44011 organ-level vision-text pairs from 17702 patientsacross 104 organs demonstrates it can identify organs and abnormalities in azero-shot manner using natural languages. The performance of CT-GLIP isvalidated on a separate test set of 1130 patients focusing on the 16 mostfrequent abnormalities across 7 organs. The experimental results show ourmodels superior performance over the standard CLIP framework across zero-shotand fine-tuning scenarios using both CNN and ViT architectures.</p>
                <p>Last Updated: 2024-04-23 17:59:01 UTC</p>
                <button class="interpret-button" data-id="2404.15272v1">Interpret</button>
                <div id="interpretation-2404.15272v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models</h3>
                <p>Authors: Wanrong ZhuJennifer HealeyRuiyi ZhangWilliam Yang WangTong Sun</p>
                <p><a href="http://arxiv.org/abs/2404.15271v1">Link to paper</a></p>
                <p>Recent advancements in instruction-following models have made userinteractions with models more user-friendly and efficient broadening theirapplicability. In graphic design non-professional users often struggle tocreate visually appealing layouts due to limited skills and resources. In thiswork we introduce a novel multimodal instruction-following framework forlayout planning allowing users to easily arrange visual elements into tailoredlayouts by specifying canvas size and design purpose such as for book coversposters brochures or menus. We developed three layout reasoning tasks totrain the model in understanding and executing layout instructions. Experimentson two benchmarks show that our method not only simplifies the design processfor non-professionals but also surpasses the performance of few-shot GPT-4Vmodels with mIoU higher by 12 on Crello. This progress highlights thepotential of multimodal instruction-following models to automate and simplifythe design process providing an approachable solution for a wide range ofdesign tasks on visually-rich documents.</p>
                <p>Last Updated: 2024-04-23 17:58:33 UTC</p>
                <button class="interpret-button" data-id="2404.15271v1">Interpret</button>
                <div id="interpretation-2404.15271v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Aligning LLM Agents by Learning Latent Preference from User Edits</h3>
                <p>Authors: Ge GaoAlexey TaymanovEduardo SalinasPaul MineiroDipendra Misra</p>
                <p><a href="http://arxiv.org/abs/2404.15269v1">Link to paper</a></p>
                <p>We study interactive learning of language agents based on user edits made tothe agents output. In a typical setting such as writing assistants the userinteracts with a language agent to generate a response given a context and mayoptionally edit the agent response to personalize it based on their latentpreference in addition to improving the correctness. The edit feedback isnaturally generated making it a suitable candidate for improving the agentsalignment with the users preference and for reducing the cost of user editsover time. We propose a learning framework PRELUDE that infers a descriptionof the users latent preference based on historic edit data and using it todefine a prompt policy that drives future response generation. This avoidsfine-tuning the agent which is costly challenging to scale with the number ofusers and may even degrade its performance on other tasks. Furthermorelearning descriptive preference improves interpretability allowing the user toview and modify the learned preference. However user preference can be complexand vary based on context making it challenging to learn. To address this wepropose a simple yet effective algorithm named CIPHER that leverages a largelanguage model LLM to infer the user preference for a given context based onuser edits. In the future CIPHER retrieves inferred preferences from thek-closest contexts in the history and forms an aggregate preference forresponse generation. We introduce two interactive environments -- summarizationand email writing for evaluation using a GPT-4 simulated user. We compare withalgorithms that directly retrieve user edits but do not learn descriptivepreference and algorithms that learn context-agnostic preference. On bothtasks CIPHER achieves the lowest edit distance cost and learns preferencesthat show significant similarity to the ground truth preferences</p>
                <p>Last Updated: 2024-04-23 17:57:47 UTC</p>
                <button class="interpret-button" data-id="2404.15269v1">Interpret</button>
                <div id="interpretation-2404.15269v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts</h3>
                <p>Authors: Yifeng DingJiawei LiuYuxiang WeiTerry Yue ZhuoLingming Zhang</p>
                <p><a href="http://arxiv.org/abs/2404.15247v1">Link to paper</a></p>
                <p>We introduce XFT a simple yet powerful training scheme by simply mergingupcycled Mixture-of-Experts MoE to unleash the performance limit ofinstruction-tuned code Large Language Models LLMs. While vanilla sparseupcycling fails to improve instruction tuning XFT introduces a shared expertmechanism with a novel routing weight normalization strategy into sparseupcycling which significantly boosts instruction tuning. After fine-tuning theupcycled MoE model XFT introduces a learnable model merging mechanism tocompile the upcycled MoE model back to a dense model achieving upcycledMoE-level performance with only dense-model compute. By applying XFT to a 1.3Bmodel we create a new state-of-the-art tiny code LLM 3B with 67.1 and 64.6pass1 on HumanEval and HumanEval respectively. With the same data and modelarchitecture XFT improves supervised fine-tuning SFT by 13 on HumanEvalalong with consistent improvements from 2 to 13 on MBPP MultiPL-E andDS-1000 demonstrating its generalizability. XFT is fully orthogonal toexisting techniques such as Evol-Instruct and OSS-Instruct opening a newdimension for improving code instruction tuning. Codes are available athttps://github.com/ise-uiuc/xft .</p>
                <p>Last Updated: 2024-04-23 17:32:24 UTC</p>
                <button class="interpret-button" data-id="2404.15247v1">Interpret</button>
                <div id="interpretation-2404.15247v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies</h3>
                <p>Authors: Weiyan ShiRyan LiYutong ZhangCaleb ZiemsChunhua yuRaya HoreshRog√©rio Abreu de PaulaDiyi Yang</p>
                <p><a href="http://arxiv.org/abs/2404.15238v1">Link to paper</a></p>
                <p>To enhance language models cultural awareness we design a generalizablepipeline to construct cultural knowledge bases from different onlinecommunities on a massive scale. With the pipeline we construct CultureBank aknowledge base built upon users self-narratives with 12K cultural descriptorssourced from TikTok and 11K from Reddit. Unlike previous cultural knowledgeresources CultureBank contains diverse views on cultural descriptors to allowflexible interpretation of cultural knowledge and contextualized culturalscenarios to help grounded evaluation. With CultureBank we evaluate differentLLMs cultural awareness and identify areas for improvement. We also fine-tunea language model on CultureBank: experiments show that it achieves betterperformances on two downstream cultural tasks in a zero-shot setting. Finallywe offer recommendations based on our findings for future culturally awarelanguage technologies. The project page is https://culturebank.github.io . Thecode and model is at https://github.com/SALT-NLP/CultureBank . The releasedCultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank .</p>
                <p>Last Updated: 2024-04-23 17:16:08 UTC</p>
                <button class="interpret-button" data-id="2404.15238v1">Interpret</button>
                <div id="interpretation-2404.15238v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation</h3>
                <p>Authors: Xiangyu XuLijuan LiuShuicheng Yan</p>
                <p><a href="http://arxiv.org/abs/2404.15276v1">Link to paper</a></p>
                <p>Existing Transformers for monocular 3D human shape and pose estimationtypically have a quadratic computation and memory complexity with respect tothe feature length which hinders the exploitation of fine-grained informationin high-resolution features that is beneficial for accurate reconstruction. Inthis work we propose an SMPL-based Transformer framework SMPLer to addressthis issue. SMPLer incorporates two key ingredients: a decoupled attentionoperation and an SMPL-based target representation which allow effectiveutilization of high-resolution features in the Transformer. In addition basedon these two designs we also introduce several novel modules including amulti-scale attention and a joint-aware attention to further boost thereconstruction performance. Extensive experiments demonstrate the effectivenessof SMPLer against existing 3D human shape and pose estimation methods bothquantitatively and qualitatively. Notably the proposed algorithm achieves anMPJPE of 45.2 mm on the Human3.6M dataset improving upon Mesh Graphormer bymore than 10 with fewer than one-third of the parameters. Code and pretrainedmodels are available at https://github.com/xuxy09/SMPLer.</p>
                <p>Last Updated: 2024-04-23 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2404.15276v1">Interpret</button>
                <div id="interpretation-2404.15276v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ID-Animator: Zero-Shot Identity-Preserving Human Video Generation</h3>
                <p>Authors: Xuanhua HeQuande LiuShengju QianXin WangTao HuKe CaoKeyu YanMan ZhouJie Zhang</p>
                <p><a href="http://arxiv.org/abs/2404.15275v1">Link to paper</a></p>
                <p>Generating high fidelity human video with specified identities has attractedsignificant attention in the content generation community. However existingtechniques struggle to strike a balance between training efficiency andidentity preservation either requiring tedious case-by-case finetuning orusually missing the identity details in video generation process. In thisstudy we present ID-Animator a zero-shot human-video generation approach thatcan perform personalized video generation given single reference facial imagewithout further training. ID-Animator inherits existing diffusion-based videogeneration backbones with a face adapter to encode the ID-relevant embeddingsfrom learnable facial latent queries. To facilitate the extraction of identityinformation in video generation we introduce an ID-oriented datasetconstruction pipeline which incorporates decoupled human attribute and actioncaptioning technique from a constructed facial image pool. Based on thispipeline a random face reference training method is further devised toprecisely capture the ID-relevant embeddings from reference images thusimproving the fidelity and generalization capacity of our model for ID-specificvideo generation. Extensive experiments demonstrate the superiority ofID-Animator to generate personalized human videos over previous models.Moreover our method is highly compatible with popular pre-trained T2V modelslike animatediff and various community backbone models showing highextendability in real-world applications for video generation where identitypreservation is highly desired. Our codes and checkpoints will be released athttps://github.com/ID-Animator/ID-Animator.</p>
                <p>Last Updated: 2024-04-23 17:59:43 UTC</p>
                <button class="interpret-button" data-id="2404.15275v1">Interpret</button>
                <div id="interpretation-2404.15275v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Metric-guided Image Reconstruction Bounds via Conformal Prediction</h3>
                <p>Authors: Matt Y CheungTucker J NethertonLaurence E CourtAshok VeeraraghavanGuha Balakrishnan</p>
                <p><a href="http://arxiv.org/abs/2404.15274v1">Link to paper</a></p>
                <p>Recent advancements in machine learning have led to novel imaging systems andalgorithms that address ill-posed problems. Assessing their trustworthiness andunderstanding how to deploy them safely at test time remains an important andopen problem. We propose a method that leverages conformal prediction toretrieve upper/lower bounds and statistical inliers/outliers of reconstructionsbased on the prediction intervals of downstream metrics. We apply our method tosparse-view CT for downstream radiotherapy planning and show 1 thatmetric-guided bounds have valid coverage for downstream metrics whileconventional pixel-wise bounds do not and 2 anatomical differences ofupper/lower bounds between metric-guided and pixel-wise methods. Our work pavesthe way for more meaningful reconstruction bounds. Code available athttps://github.com/matthewyccheung/conformal-metric</p>
                <p>Last Updated: 2024-04-23 17:59:12 UTC</p>
                <button class="interpret-button" data-id="2404.15274v1">Interpret</button>
                <div id="interpretation-2404.15274v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios</h3>
                <p>Authors: Jingyang LinYingda XiaJianpeng ZhangKe YanLe LuJiebo LuoLing Zhang</p>
                <p><a href="http://arxiv.org/abs/2404.15272v1">Link to paper</a></p>
                <p>Medical Vision-Language Pretraining Med-VLP establishes a connectionbetween visual content from medical images and the relevant textualdescriptions. Existing Med-VLP methods primarily focus on 2D images depicting asingle body part notably chest X-rays. In this paper we extend the scope ofMed-VLP to encompass 3D images specifically targeting full-body scenarios byusing a multimodal dataset of CT images and reports. Compared with the 2Dcounterpart 3D VLP is required to effectively capture essential semantics fromsignificantly sparser representation in 3D imaging. In this paper we introduceCT-GLIP Grounded Language-Image Pretraining with CT scans a novel methodthat constructs organ-level image-text pairs to enhance multimodal contrastivelearning aligning grounded visual features with precise diagnostic text.Additionally we developed an abnormality dictionary to augment contrastivelearning with diverse negative samples. Our method trained on a multimodal CTdataset comprising 44011 organ-level vision-text pairs from 17702 patientsacross 104 organs demonstrates it can identify organs and abnormalities in azero-shot manner using natural languages. The performance of CT-GLIP isvalidated on a separate test set of 1130 patients focusing on the 16 mostfrequent abnormalities across 7 organs. The experimental results show ourmodels superior performance over the standard CLIP framework across zero-shotand fine-tuning scenarios using both CNN and ViT architectures.</p>
                <p>Last Updated: 2024-04-23 17:59:01 UTC</p>
                <button class="interpret-button" data-id="2404.15272v1">Interpret</button>
                <div id="interpretation-2404.15272v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models</h3>
                <p>Authors: Wanrong ZhuJennifer HealeyRuiyi ZhangWilliam Yang WangTong Sun</p>
                <p><a href="http://arxiv.org/abs/2404.15271v1">Link to paper</a></p>
                <p>Recent advancements in instruction-following models have made userinteractions with models more user-friendly and efficient broadening theirapplicability. In graphic design non-professional users often struggle tocreate visually appealing layouts due to limited skills and resources. In thiswork we introduce a novel multimodal instruction-following framework forlayout planning allowing users to easily arrange visual elements into tailoredlayouts by specifying canvas size and design purpose such as for book coversposters brochures or menus. We developed three layout reasoning tasks totrain the model in understanding and executing layout instructions. Experimentson two benchmarks show that our method not only simplifies the design processfor non-professionals but also surpasses the performance of few-shot GPT-4Vmodels with mIoU higher by 12 on Crello. This progress highlights thepotential of multimodal instruction-following models to automate and simplifythe design process providing an approachable solution for a wide range ofdesign tasks on visually-rich documents.</p>
                <p>Last Updated: 2024-04-23 17:58:33 UTC</p>
                <button class="interpret-button" data-id="2404.15271v1">Interpret</button>
                <div id="interpretation-2404.15271v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation</h3>
                <p>Authors: Xiangyu XuLijuan LiuShuicheng Yan</p>
                <p><a href="http://arxiv.org/abs/2404.15276v1">Link to paper</a></p>
                <p>Existing Transformers for monocular 3D human shape and pose estimationtypically have a quadratic computation and memory complexity with respect tothe feature length which hinders the exploitation of fine-grained informationin high-resolution features that is beneficial for accurate reconstruction. Inthis work we propose an SMPL-based Transformer framework SMPLer to addressthis issue. SMPLer incorporates two key ingredients: a decoupled attentionoperation and an SMPL-based target representation which allow effectiveutilization of high-resolution features in the Transformer. In addition basedon these two designs we also introduce several novel modules including amulti-scale attention and a joint-aware attention to further boost thereconstruction performance. Extensive experiments demonstrate the effectivenessof SMPLer against existing 3D human shape and pose estimation methods bothquantitatively and qualitatively. Notably the proposed algorithm achieves anMPJPE of 45.2 mm on the Human3.6M dataset improving upon Mesh Graphormer bymore than 10 with fewer than one-third of the parameters. Code and pretrainedmodels are available at https://github.com/xuxy09/SMPLer.</p>
                <p>Last Updated: 2024-04-23 17:59:59 UTC</p>
                <button class="interpret-button" data-id="2404.15276v1">Interpret</button>
                <div id="interpretation-2404.15276v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Metric-guided Image Reconstruction Bounds via Conformal Prediction</h3>
                <p>Authors: Matt Y CheungTucker J NethertonLaurence E CourtAshok VeeraraghavanGuha Balakrishnan</p>
                <p><a href="http://arxiv.org/abs/2404.15274v1">Link to paper</a></p>
                <p>Recent advancements in machine learning have led to novel imaging systems andalgorithms that address ill-posed problems. Assessing their trustworthiness andunderstanding how to deploy them safely at test time remains an important andopen problem. We propose a method that leverages conformal prediction toretrieve upper/lower bounds and statistical inliers/outliers of reconstructionsbased on the prediction intervals of downstream metrics. We apply our method tosparse-view CT for downstream radiotherapy planning and show 1 thatmetric-guided bounds have valid coverage for downstream metrics whileconventional pixel-wise bounds do not and 2 anatomical differences ofupper/lower bounds between metric-guided and pixel-wise methods. Our work pavesthe way for more meaningful reconstruction bounds. Code available athttps://github.com/matthewyccheung/conformal-metric</p>
                <p>Last Updated: 2024-04-23 17:59:12 UTC</p>
                <button class="interpret-button" data-id="2404.15274v1">Interpret</button>
                <div id="interpretation-2404.15274v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Estimation Network Design framework for efficient distributed optimization</h3>
                <p>Authors: Mattia BianchiSergio Grammatico</p>
                <p><a href="http://arxiv.org/abs/2404.15273v1">Link to paper</a></p>
                <p>Distributed decision problems features a group of agents that can onlycommunicate over a peer-to-peer network without a central memory. Inapplications such as network control and data ranking each agent is onlyaffected by a small portion of the decision vector: this sparsity is typicallyignored in distributed algorithms while it could be leveraged to improveefficiency and scalability. To address this issue our recent paper introducesEstimation Network Design END a graph theoretical language for the analysisand design of distributed iterations. END algorithms can be tuned to exploitthe sparsity of specific problem instances reducing communication overhead andminimizing redundancy yet without requiring case-by-case convergence analysis.In this paper we showcase the flexility of END in the context of distributedoptimization. In particular we study the sparsity-aware version of manyestablished methods including ADMM AugDGM and Push-Sum DGD. Simulations on anestimation problem in sensor networks demonstrate that END algorithms can boostconvergence speed and greatly reduce the communication and memory cost.</p>
                <p>Last Updated: 2024-04-23 17:59:09 UTC</p>
                <button class="interpret-button" data-id="2404.15273v1">Interpret</button>
                <div id="interpretation-2404.15273v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Aligning LLM Agents by Learning Latent Preference from User Edits</h3>
                <p>Authors: Ge GaoAlexey TaymanovEduardo SalinasPaul MineiroDipendra Misra</p>
                <p><a href="http://arxiv.org/abs/2404.15269v1">Link to paper</a></p>
                <p>We study interactive learning of language agents based on user edits made tothe agents output. In a typical setting such as writing assistants the userinteracts with a language agent to generate a response given a context and mayoptionally edit the agent response to personalize it based on their latentpreference in addition to improving the correctness. The edit feedback isnaturally generated making it a suitable candidate for improving the agentsalignment with the users preference and for reducing the cost of user editsover time. We propose a learning framework PRELUDE that infers a descriptionof the users latent preference based on historic edit data and using it todefine a prompt policy that drives future response generation. This avoidsfine-tuning the agent which is costly challenging to scale with the number ofusers and may even degrade its performance on other tasks. Furthermorelearning descriptive preference improves interpretability allowing the user toview and modify the learned preference. However user preference can be complexand vary based on context making it challenging to learn. To address this wepropose a simple yet effective algorithm named CIPHER that leverages a largelanguage model LLM to infer the user preference for a given context based onuser edits. In the future CIPHER retrieves inferred preferences from thek-closest contexts in the history and forms an aggregate preference forresponse generation. We introduce two interactive environments -- summarizationand email writing for evaluation using a GPT-4 simulated user. We compare withalgorithms that directly retrieve user edits but do not learn descriptivepreference and algorithms that learn context-agnostic preference. On bothtasks CIPHER achieves the lowest edit distance cost and learns preferencesthat show significant similarity to the ground truth preferences</p>
                <p>Last Updated: 2024-04-23 17:57:47 UTC</p>
                <button class="interpret-button" data-id="2404.15269v1">Interpret</button>
                <div id="interpretation-2404.15269v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>All You Need is Resistance: On the Equivalence of Effective Resistance and Certain Optimal Transport Problems on Graphs</h3>
                <p>Authors: Sawyer RobertsonZhengchao WanAlexander Cloninger</p>
                <p><a href="http://arxiv.org/abs/2404.15261v1">Link to paper</a></p>
                <p>The fields of effective resistance and optimal transport on graphs are filledwith rich connections to combinatorics geometry machine learning and beyond.In this article we put forth a bold claim: that the two fields should beunderstood as one and the same up to a choice of p. We make this claimprecise by introducing the parameterized family of p-Beckmann distances forprobability measures on graphs and relate them sharply to certain Wassersteindistances. Then we break open a suite of results including explicitconnections to optimal stopping times and random walks on graphs graph Sobolevspaces and a Benamou-Brenier type formula for 2-Beckmann distance. Wefurther explore empirical implications in the world of unsupervised learningfor graph data and propose further study of the usage of these metrics whereWasserstein distance may produce computational bottlenecks.</p>
                <p>Last Updated: 2024-04-23 17:50:52 UTC</p>
                <button class="interpret-button" data-id="2404.15261v1">Interpret</button>
                <div id="interpretation-2404.15261v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Estimation Network Design framework for efficient distributed optimization</h3>
                <p>Authors: Mattia BianchiSergio Grammatico</p>
                <p><a href="http://arxiv.org/abs/2404.15273v1">Link to paper</a></p>
                <p>Distributed decision problems features a group of agents that can onlycommunicate over a peer-to-peer network without a central memory. Inapplications such as network control and data ranking each agent is onlyaffected by a small portion of the decision vector: this sparsity is typicallyignored in distributed algorithms while it could be leveraged to improveefficiency and scalability. To address this issue our recent paper introducesEstimation Network Design END a graph theoretical language for the analysisand design of distributed iterations. END algorithms can be tuned to exploitthe sparsity of specific problem instances reducing communication overhead andminimizing redundancy yet without requiring case-by-case convergence analysis.In this paper we showcase the flexility of END in the context of distributedoptimization. In particular we study the sparsity-aware version of manyestablished methods including ADMM AugDGM and Push-Sum DGD. Simulations on anestimation problem in sensor networks demonstrate that END algorithms can boostconvergence speed and greatly reduce the communication and memory cost.</p>
                <p>Last Updated: 2024-04-23 17:59:09 UTC</p>
                <button class="interpret-button" data-id="2404.15273v1">Interpret</button>
                <div id="interpretation-2404.15273v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>From Space-Time to Space-Order: Directly Planning a Temporal Planning Graph by Redefining CBS</h3>
                <p>Authors: Yu WuRishi VeerapaneniJiaoyang LiMaxim Likhachev</p>
                <p><a href="http://arxiv.org/abs/2404.15137v1">Link to paper</a></p>
                <p>The majority of multi-agent path finding MAPF methods computecollision-free space-time paths which require agents to be at a specificlocation at a specific discretized timestep. However executing thesespace-time paths directly on robotic systems is infeasible due to real-timeexecution differences e.g. delays which can lead to collisions. To combatthis current methods translate the space-time paths into a temporal plan graphTPG that only requires that agents observe the order in which they navigatethrough locations where their paths cross. However planning space-time pathsand then post-processing them into a TPG does not reduce the requiredagent-to-agent coordination which is fixed once the space-time paths arecomputed. To that end we propose a novel algorithm Space-Order CBS that candirectly plan a TPG and explicitly minimize coordination. Our main theoreticalinsight is our novel perspective on viewing a TPG as a set of space-visitationorder paths where agents visit locations in relative orders e.g. 1st vs 2ndas opposed to specific timesteps. We redefine unique conflicts and constraintsfor adapting CBS for space-order planning. We experimentally validate howSpace-Order CBS can return TPGs which significantly reduce coordination thussubsequently reducing the amount of agent-agent communication and leading tomore robustness to delays during execution.</p>
                <p>Last Updated: 2024-04-23 15:42:31 UTC</p>
                <button class="interpret-button" data-id="2404.15137v1">Interpret</button>
                <div id="interpretation-2404.15137v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Opinion Update in a Subjective Logic Model for Social Networks</h3>
                <p>Authors: M√°rio S. AlvimSophia KnightJos√© C. Oliveira</p>
                <p><a href="http://arxiv.org/abs/2404.14789v1">Link to paper</a></p>
                <p>Subjective Logic SL is a logic incorporating uncertainty and opinions foragents in dynamic systems. In this work we investigate the use of subjectivelogic to model opinions and belief change in social networks. In particular wework toward the development of a subjective logic belief/opinion updatefunction appropriate for modeling belief change as communication occurs insocial networks. We found through experiments that an update function withbelief fusion from SL does not have ideal properties to represent a rationalupdate. Even without these properties we found that an update function withcumulative belief fusion can describe behaviors not explored by the socialnetwork model defined by Alvim Knight and Valencia 2019.</p>
                <p>Last Updated: 2024-04-23 07:02:00 UTC</p>
                <button class="interpret-button" data-id="2404.14789v1">Interpret</button>
                <div id="interpretation-2404.14789v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>STROOBnet Optimization via GPU-Accelerated Proximal Recurrence Strategies</h3>
                <p>Authors: Ted Edward HolmbergMahdi AbdelguerfiElias Ioup</p>
                <p><a href="http://dx.doi.org/10.1109/BigData59044.2023.10386774">Link to paper</a></p>
                <p>Spatiotemporal networks observational capabilities are crucial for accuratedata gathering and informed decisions across multiple sectors. This studyfocuses on the Spatiotemporal Ranged Observer-Observable Bipartite NetworkSTROOBnet linking observational nodes e.g. surveillance cameras to eventswithin defined geographical regions enabling efficient monitoring. Using datafrom Real-Time Crime Camera RTCC systems and Calls for Service CFS in NewOrleans where RTCC combats rising crime amidst reduced police presence weaddress the networks initial observational imbalances. Aiming for uniformobservational efficacy we propose the Proximal Recurrence approach. Itoutperformed traditional clustering methods like k-means and DBSCAN by offeringholistic event frequency and spatial consideration enhancing observationalcoverage.</p>
                <p>Last Updated: 2024-04-22 17:46:29 UTC</p>
                <button class="interpret-button" data-id="2404.14388v1">Interpret</button>
                <div id="interpretation-2404.14388v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Stochastic Geo-spatiotemporal Bipartite Network to Optimize GCOOS Sensor Placement Strategies</h3>
                <p>Authors: Ted Edward HolmbergElias IoupMahdi Abdelguerfi</p>
                <p><a href="http://dx.doi.org/10.1109/BigData55660.2022.10020928">Link to paper</a></p>
                <p>This paper proposes two new measures applicable in a spatial bipartitenetwork model: coverage and coverage robustness. The bipartite network mustconsist of observer nodes observable nodes and edges that connect observernodes to observable nodes. The coverage and coverage robustness scores evaluatethe effectiveness of the observer node placements. This measure is beneficialfor stochastic data as it may be coupled with Monte Carlo simulations toidentify optimal placements for new observer nodes. In this paper we constructa Geo-SpatioTemporal Bipartite Network GSTBN within the stochastic anddynamical environment of the Gulf of Mexico. This GSTBN consists of GCOOSsensor nodes and HYCOM Region of Interest RoI event nodes. The goal is toidentify optimal placements to expand GCOOS to improve the forecasting outcomesby the HYCOM ocean prediction model.</p>
                <p>Last Updated: 2024-04-22 17:12:06 UTC</p>
                <button class="interpret-button" data-id="2404.14357v1">Interpret</button>
                <div id="interpretation-2404.14357v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Augmented Voices: An Augmented Reality Experience Highlighting the Social Injustices of Gender-Based Violence in the Muslim South-Asian Diaspora</h3>
                <p>Authors: Hamida Khatri</p>
                <p><a href="http://arxiv.org/abs/2404.15239v1">Link to paper</a></p>
                <p>This paper delves into the distressing prevalence of gender-based violenceGBV and its deep-seated psychological ramifications particularly amongMuslim South Asian women living in diasporic communities. Despite the gravityof GBV these women often face formidable barriers in voicing their experiencesand accessing support. Augmented Voices emerges as a technological beaconharnessing the potential of augmented reality AR to bridge the digital andphysical realms through mobile devices enhancing the visibility of theseoften-silenced voices. With its technological motivation firmly anchored in theconvergence of AR and real-world interactions Augmented Voices offers adigital platform where storytelling acts as a catalyst bringing to the forethe experiences shared by these women. By superimposing their narratives ontophysical locations via Geographic Information System GIS Mapping theapplication augments their voices in the diaspora providing a conduit forexpression and solidarity. This project currently at its developmental stageaspires to elevate the stories of GBV victims to a level where their strugglesare not just heard but felt forging a powerful connection between the user andthe narrative. It is designed to transcend the limitations of conventionalstorytelling creating an augmented reality where voices that are often mutedby societal constraints can resonate powerfully. The project underscores theurgent imperative to confront GBV catalyzing societal transformation andfostering robust support networks for those in the margins. It is a pioneeringexample of how technology can become a formidable ally in the fight for socialjustice and the empowerment of the oppressed. Additionally this paper delvesinto the AR workflow illustrating its relevance and contribution to the broadertheme of site-specific AR for social justice.</p>
                <p>Last Updated: 2024-04-23 17:17:52 UTC</p>
                <button class="interpret-button" data-id="2404.15239v1">Interpret</button>
                <div id="interpretation-2404.15239v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Evaluating Physician-AI Interaction for Cancer Management: Paving the Path towards Precision Oncology</h3>
                <p>Authors: Zeshan HussainBarbara D. LamFernando A. Acosta-PerezIrbaz Bin RiazMaia JacobsAndrew J. YeeDavid Sontag</p>
                <p><a href="http://arxiv.org/abs/2404.15187v1">Link to paper</a></p>
                <p>We evaluated how clinicians approach clinical decision-making when givenfindings from both randomized controlled trials RCTs and machine learningML models. To do so we designed a clinical decision support system CDSSthat displays survival curves and adverse event information from a syntheticRCT and ML model for 12 patients with multiple myeloma. We conducted aninterventional study in a simulated setting to evaluate how clinicianssynthesized the available data to make treatment decisions. Participants wereinvited to participate in a follow-up interview to discuss their choices in anopen-ended format. When ML model results were concordant with RCT resultsphysicians had increased confidence in treatment choice compared to when theywere given RCT results alone. When ML model results were discordant with RCTresults the majority of physicians followed the ML model recommendation intheir treatment selection. Perceived reliability of the ML model wasconsistently higher after physicians were provided with data on how it wastrained and validated. Follow-up interviews revealed four major themes: 1variability in what variables participants used for decision-making 2perceived advantages to an ML model over RCT data 3 uncertainty arounddecision-making when the ML model quality was poor and 4 perception thatthis type of study is an important thought exercise for clinicians. OverallML-based CDSSs have the potential to change treatment decisions in cancermanagement. However meticulous development and validation of these systems aswell as clinician training are required before deployment.</p>
                <p>Last Updated: 2024-04-23 16:28:03 UTC</p>
                <button class="interpret-button" data-id="2404.15187v1">Interpret</button>
                <div id="interpretation-2404.15187v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Voice Passing : a Non-Binary Voice Gender Prediction System for evaluating Transgender voice transition</h3>
                <p>Authors: David DoukhanSimon DevauchelleLucile Girard-MonneronM√≠a Ch√°vez RuzV. ChaddoukIsabelle WagnerAlbert Rilliard</p>
                <p><a href="http://dx.doi.org/10.21437/Interspeech.2023-1835">Link to paper</a></p>
                <p>This paper presents a software allowing to describe voices using a continuousVoice Femininity Percentage VFP. This system is intended for transgenderspeakers during their voice transition and for voice therapists supporting themin this process. A corpus of 41 French cis- and transgender speakers wasrecorded. A perceptual evaluation allowed 57 participants to estimate the VFPfor each voice. Binary gender classification models were trained on externalgender-balanced data and used on overlapping windows to obtain average genderprediction estimates which were calibrated to predict VFP and obtained higheraccuracy than F_0 or vocal track length-based models. Training data speakingstyle and DNN architecture were shown to impact VFP estimation. Accuracy of themodels was affected by speakers age. This highlights the importance of styleage and the conception of gender as binary or not to build adequatestatistical representations of cultural concepts.</p>
                <p>Last Updated: 2024-04-23 16:15:39 UTC</p>
                <button class="interpret-button" data-id="2404.15176v1">Interpret</button>
                <div id="interpretation-2404.15176v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Lost in Magnitudes: Exploring the Design Space for Visualizing Data with Large Value Ranges</h3>
                <p>Authors: Katerina BatziakoudiFlorent CabricSt√©phanie ReyJean-Daniel Fekete</p>
                <p><a href="http://arxiv.org/abs/2404.15150v1">Link to paper</a></p>
                <p>We explore the design space for the static visualization of datasets withquantitative attributes that vary over multiple orders of magnitude-we callthese attributes Orders of Magnitude Values OMVs-and provide designguidelines and recommendations on effective visual encodings for OMVs. Currentcharts rely on linear or logarithmic scales to visualize values leading tolimitations in performing simple tasks for OMVs. In particular linear scalesprevent the reading of smaller magnitudes and their comparisons whilelogarithmic scales are challenging for the general public to understand. Ourdesign space leverages the approach of dividing OMVs into two different parts:mantissa and exponent in a way similar to scientific notation. This separationallows for a visual encoding of both parts. For our exploration we use fourdatasets each with two attributes: an OMV divided into mantissa and exponentand a second attribute that is nominal ordinal time or quantitative. Westart from the original design space described by the Grammar of Graphics andsystematically generate all possible visualizations for these datasetsemploying different marks and visual channels. We refine this design space byenforcing integrity constraints from visualization and graphical perceptionliterature. Through a qualitative assessment of all viable combinations wediscuss the most effective visualizations for OMVs focusing on channel andtask effectiveness. The articles main contributions are 1 the presentation ofthe design space of OMVs 2 the generation of a large number of OMVvisualizations among which some are novel and effective 3 the refineddefinition of a scale that we call EM for OMVs and 4 guidelines andrecommendations for designing effective OMV visualizations. These efforts aimto enrich visualization systems to better support data with OMVs and guidefuture research.</p>
                <p>Last Updated: 2024-04-23 15:52:53 UTC</p>
                <button class="interpret-button" data-id="2404.15150v1">Interpret</button>
                <div id="interpretation-2404.15150v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Virtual Takeovers in the Metaverse: Interrogating Power in Our Past and Future(s) with Multi-Layered Narratives</h3>
                <p>Authors: Heather Snyder QuinnJessa Dickinson</p>
                <p><a href="http://arxiv.org/abs/2404.15108v1">Link to paper</a></p>
                <p>Mariah is an augmented reality AR mobile application that exposes powerstructures e.g. capitalism patriarchy white supremacy through storytellingand celebrates acts of resistance against them. People can use Mariah tolegally trespass the metaverse as a form of protest. Mariah provideshistorical context to the users physical surroundings by superimposing imagesand playing stories about people who have experienced and resisted injustice.We share two implementations of Mariah that raise questions about free speechand property rights in the metaverse: 1 a protest against museums acceptingdirty money from the opioid epidemic and 2 a commemoration of sites wherepeople have resisted power structures. Mariah is a case study for howexperimenting with a technology in non-sanctioned ways i.e. hacking canexpose ways that it might interact with and potentially amplify existingpower structures.</p>
                <p>Last Updated: 2024-04-23 15:02:31 UTC</p>
                <button class="interpret-button" data-id="2404.15108v1">Interpret</button>
                <div id="interpretation-2404.15108v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-04-24</p>
        </div>
    
        </div>
    </body>
    </html>
    