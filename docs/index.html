
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>GroundUp: Rapid Sketch-Based 3D City Massing</h3>
                <p>Authors: Gizem Esra UnluMohamed SayedYulia GryaditskayaGabriel Brostow</p>
                <p><a href="http://arxiv.org/abs/2407.12739v1">Link to paper</a></p>
                <p>We propose GroundUp the first sketch-based ideation tool for 3D city massingof urban areas. We focus on early-stage urban design where sketching is acommon tool and the design starts from balancing building volumes masses andopen spaces. With Human-Centered AI in mind we aim to help architects quicklyrevise their ideas by easily switching between 2D sketches and 3D modelsallowing for smoother iteration and sharing of ideas. Inspired by feedback fromarchitects and existing workflows our system takes as a first input a usersketch of multiple buildings in a top-down view. The user then draws aperspective sketch of the envisioned site. Our method is designed to exploitthe complementarity of information in the two sketches and allows users toquickly preview and adjust the inferred 3D shapes. Our model has two maincomponents. First we propose a novel sketch-to-depth prediction network forperspective sketches that exploits top-down sketch shapes. Second we use depthcues derived from the perspective sketch as a condition to our diffusion modelwhich ultimately completes the geometry in a top-down view. Thus our final 3Dgeometry is represented as a heightfield allowing users to construct the cityfrom the ground up.</p>
                <p>Last Updated: 2024-07-17 16:59:29 UTC</p>
                <button class="interpret-button" data-id="2407.12739v1">Interpret</button>
                <div id="interpretation-2407.12739v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Future of Learning: Large Language Models through the Lens of Students</h3>
                <p>Authors: He ZhangJingyi XieChuhao WuJie CaiChanMin KimJohn M. Carroll</p>
                <p><a href="http://arxiv.org/abs/2407.12723v1">Link to paper</a></p>
                <p>As Large-Scale Language Models LLMs continue to evolve they demonstratesignificant enhancements in performance and an expansion of functionalitiesimpacting various domains including education. In this study we conductedinterviews with 14 students to explore their everyday interactions withChatGPT. Our preliminary findings reveal that students grapple with the dilemmaof utilizing ChatGPTs efficiency for learning and information seeking whilesimultaneously experiencing a crisis of trust and ethical concerns regardingthe outcomes and broader impacts of ChatGPT. The students perceive ChatGPT asbeing more human-like compared to traditional AI. This dilemma characterizedby mixed emotions inconsistent behaviors and an overall positive attitudetowards ChatGPT underscores its potential for beneficial applications ineducation and learning. However we argue that despite its human-likequalities the advanced capabilities of such intelligence might lead to adverseconsequences. Therefore its imperative to approach its application cautiouslyand strive to mitigate potential harms in future developments.</p>
                <p>Last Updated: 2024-07-17 16:40:37 UTC</p>
                <button class="interpret-button" data-id="2407.12723v1">Interpret</button>
                <div id="interpretation-2407.12723v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AudienceView: AI-Assisted Interpretation of Audience Feedback in Journalism</h3>
                <p>Authors: William BrannonDoug BeefermanHang JiangAndrew HeywardDeb Roy</p>
                <p><a href="http://arxiv.org/abs/2407.12613v1">Link to paper</a></p>
                <p>Understanding and making use of audience feedback is important but difficultfor journalists who now face an impractically large volume of audiencecomments online. We introduce AudienceView an online tool to help journalistscategorize and interpret this feedback by leveraging large language modelsLLMs. AudienceView identifies themes and topics connects them back tospecific comments provides ways to visualize the sentiment and distribution ofthe comments and helps users develop ideas for subsequent reporting projects.We consider how such tools can be useful in a journalists workflow andemphasize the importance of contextual awareness and human judgment.</p>
                <p>Last Updated: 2024-07-17 14:41:35 UTC</p>
                <button class="interpret-button" data-id="2407.12613v1">Interpret</button>
                <div id="interpretation-2407.12613v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Abstraction Alignment: Comparing Model and Human Conceptual Relationships</h3>
                <p>Authors: Angie BoggustHyemin BangHendrik StrobeltArvind Satyanarayan</p>
                <p><a href="http://arxiv.org/abs/2407.12543v1">Link to paper</a></p>
                <p>Abstraction -- the process of generalizing specific examples into broadreusable patterns -- is central to how people efficiently process and storeinformation and apply their knowledge to new data. Promisingly research hasshown that ML models learn representations that span levels of abstractionfrom specific concepts like bolo tie and car tire to more general conceptslike CEO and model. However existing techniques analyze theserepresentations in isolation treating learned concepts as independentartifacts rather than an interconnected web of abstraction. As a resultalthough we can identify the concepts a model uses to produce its output it isdifficult to assess if it has learned a human-aligned abstraction of theconcepts that will generalize to new data. To address this gap we introduceabstraction alignment a methodology to measure the agreement between a modelslearned abstraction and the expected human abstraction. We quantify abstractionalignment by comparing model outputs against a human abstraction graph such aslinguistic relationships or medical disease hierarchies. In evaluation tasksinterpreting image models benchmarking language models and analyzing medicaldatasets abstraction alignment provides a deeper understanding of modelbehavior and dataset content differentiating errors based on their agreementwith human knowledge expanding the verbosity of current model quality metricsand revealing ways to improve existing human abstractions.</p>
                <p>Last Updated: 2024-07-17 13:27:26 UTC</p>
                <button class="interpret-button" data-id="2407.12543v1">Interpret</button>
                <div id="interpretation-2407.12543v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Application of Prompt Learning Models in Identifying the Collaborative Problem Solving Skills in an Online Task</h3>
                <p>Authors: Mengxiao ZhuXin WangXiantao WangZihang ChenWei Huang</p>
                <p><a href="http://arxiv.org/abs/2407.12487v1">Link to paper</a></p>
                <p>Collaborative problem solving CPS competence is considered one of theessential 21st-century skills. To facilitate the assessment and learning of CPScompetence researchers have proposed a series of frameworks to conceptualizeCPS and explored ways to make sense of the complex processes involved incollaborative problem solving. However encoding explicit behaviors intosubskills within the frameworks of CPS skills is still a challenging task.Traditional studies have relied on manual coding to decipher behavioral datafor CPS but such coding methods can be very time-consuming and cannot supportreal-time analyses. Scholars have begun to explore approaches for constructingautomatic coding models. Nevertheless the existing models built using machinelearning or deep learning techniques depend on a large amount of training dataand have relatively low accuracy. To address these problems this paperproposes a prompt-based learning pre-trained model. The model can achieve highperformance even with limited training data. In this study three experimentswere conducted and the results showed that our model not only produced thehighest accuracy macro F1 score and kappa values on large training sets butalso performed the best on small training sets of the CPS behavioral data. Theapplication of the proposed prompt-based learning pre-trained model contributesto the CPS skills coding task and can also be used for other CSCW coding tasksto replace manual coding.</p>
                <p>Last Updated: 2024-07-17 11:12:02 UTC</p>
                <button class="interpret-button" data-id="2407.12487v1">Interpret</button>
                <div id="interpretation-2407.12487v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models</h3>
                <p>Authors: Kaichen ZhangBo LiPeiyuan ZhangFanyi PuJoshua Adrian CahyonoKairui HuShuai LiuYuanhan ZhangJingkang YangChunyuan LiZiwei Liu</p>
                <p><a href="http://arxiv.org/abs/2407.12772v1">Link to paper</a></p>
                <p>The advances of large foundation models necessitate wide-coverage low-costand zero-contamination benchmarks. Despite continuous exploration of languagemodel evaluations comprehensive studies on the evaluation of Large Multi-modalModels LMMs remain limited. In this work we introduce LMMS-EVAL a unifiedand standardized multimodal benchmark framework with over 50 tasks and morethan 10 models to promote transparent and reproducible evaluations. AlthoughLMMS-EVAL offers comprehensive coverage we find it still falls short inachieving low cost and zero contamination. To approach this evaluationtrilemma we further introduce LMMS-EVAL LITE a pruned evaluation toolkit thatemphasizes both coverage and efficiency. Additionally we present MultimodalLIVEBENCH that utilizes continuously updating news and online forums to assessmodels generalization abilities in the wild featuring a low-cost andzero-contamination evaluation approach. In summary our work highlights theimportance of considering the evaluation trilemma and provides practicalsolutions to navigate the trade-offs in evaluating large multi-modal modelspaving the way for more effective and reliable benchmarking of LMMs. Weopensource our codebase and maintain leaderboard of LIVEBENCH athttps://github.com/EvolvingLMMs-Lab/lmms-eval andhttps://huggingface.co/spaces/lmms-lab/LiveBench.</p>
                <p>Last Updated: 2024-07-17 17:51:53 UTC</p>
                <button class="interpret-button" data-id="2407.12772v1">Interpret</button>
                <div id="interpretation-2407.12772v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Role of Network and Identity in the Diffusion of Hashtags</h3>
                <p>Authors: Aparna AnanthasubramaniamYufei ZhuDavid JurgensDaniel Romero</p>
                <p><a href="http://arxiv.org/abs/2407.12771v1">Link to paper</a></p>
                <p>Although the spread of behaviors is influenced by many social factorsexisting literature tends to study the effects of single factors -- most oftenproperties of the social network -- on the final cascade. In order to movetowards a more integrated view of cascades this paper offers the firstcomprehensive investigation into the role of two social factors in thediffusion of 1337 popular hashtags representing the production of novelculture on Twitter: 1 the topology of the Twitter social network and 2performance of each users probable demographic identity. Here we show thatcascades are best modeled using a combination of network and identity ratherthan either factor alone. This combined model best reproduces a composite indexof ten cascade properties across all 1337 hashtags. However there isimportant heterogeneity in what social factors are required to reproducedifferent properties of hashtag cascades. For instance while a combinednetworkidentity model best predicts the popularity of cascades a network-onlymodel has better performance in predicting cascade growth and an identity-onlymodel in adopter composition. We are able to predict what type of hashtag isbest modeled by each combination of features and use this to further improveperformance. Additionally consistent with prior literature on the combinednetworkidentity model most outperforms the single-factor counterfactuals amonghashtags used for expressing racial or regional identity stance-takingtalking about sports or variants of existing cultural trends with very slow-or fast-growing communicative need. In sum our results imply the utility ofmulti-factor models in predicting cascades in order to account for the variedways in which network identity and other social factors play a role in thediffusion of hashtags on Twitter.</p>
                <p>Last Updated: 2024-07-17 17:51:49 UTC</p>
                <button class="interpret-button" data-id="2407.12771v1">Interpret</button>
                <div id="interpretation-2407.12771v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>HDLCopilot: Hardware Design Library Querying with Natural Language</h3>
                <p>Authors: Manar AbdelattySherief Reda</p>
                <p><a href="http://arxiv.org/abs/2407.12749v1">Link to paper</a></p>
                <p>Hardware design engineers routinely work with multiple Process Design KitsPDKs from various fabrication labs each containing several standard celllibraries optimized for specific metric such as speed power or density.These libraries include multiple views such as liberty files for timinginformation LEF files for abstract layout details and technology LEF forprocess design rules. Navigating this complex landscape to retrieve specificinformation about gates or design rules is often time-consuming anderror-prone. To address this we present HDLCopilot an LLM-powered PDK querysystem that allows engineers to streamline interactions with PDKs in naturallanguage format making information retrieval accurate and more efficient.HDLCopilot achieves an accuracy of 94.23 on an evaluation set comprised ofdiverse and complex natural language queries. HDLCopilot positions itself as apowerful assistant in the hardware design process enhancing productivity andreducing potential human errors.</p>
                <p>Last Updated: 2024-07-17 17:11:13 UTC</p>
                <button class="interpret-button" data-id="2407.12749v1">Interpret</button>
                <div id="interpretation-2407.12749v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A LLM Benchmark based on the Minecraft Builder Dialog Agent Task</h3>
                <p>Authors: Chris MadgeMassimo Poesio</p>
                <p><a href="http://arxiv.org/abs/2407.12734v1">Link to paper</a></p>
                <p>In this work we proposing adapting the Minecraft builder task into an LLMbenchmark suitable for evaluating LLM ability in spatially orientated tasksand informing builder agent design. Previous works have proposed corpora withvarying complex structures and human written instructions. We instead attemptto provide a comprehensive synthetic benchmark for testing builder agents overa series of distinct tasks that comprise of common building operations. Webelieve this approach allows us to probe specific strengths and weaknesses ofdifferent agents and test the ability of LLMs in the challenging area ofspatial reasoning and vector based math.</p>
                <p>Last Updated: 2024-07-17 16:52:23 UTC</p>
                <button class="interpret-button" data-id="2407.12734v1">Interpret</button>
                <div id="interpretation-2407.12734v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language Models?</h3>
                <p>Authors: Ben YaoYazhou ZhangQiuchi LiJing Qin</p>
                <p><a href="http://arxiv.org/abs/2407.12725v1">Link to paper</a></p>
                <p>Elaborating a series of intermediate reasoning steps significantly improvesthe ability of large language models LLMs to solve complex problems as suchsteps would evoke LLMs to think sequentially. However human sarcasmunderstanding is often considered an intuitive and holistic cognitive processin which various linguistic contextual and emotional cues are integrated toform a comprehensive understanding of the speakers true intention which isargued not be limited to a step-by-step reasoning process. To verify thisargument we introduce a new prompting framework called SarcasmCue whichcontains four prompting strategies viz. chain of contradiction CoC graphof cues GoC bagging of cues BoC and tensor of cues ToC which elicitsLLMs to detect human sarcasm by considering sequential and non-sequentialprompting methods. Through a comprehensive empirical comparison on fourbenchmarking datasets we show that the proposed four prompting methodsoutperforms standard IO prompting CoT and ToT with a considerable margin andnon-sequential prompting generally outperforms sequential prompting.</p>
                <p>Last Updated: 2024-07-17 16:42:03 UTC</p>
                <button class="interpret-button" data-id="2407.12725v1">Interpret</button>
                <div id="interpretation-2407.12725v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Information Compression in Dynamic Games</h3>
                <p>Authors: Dengwang TangVijay SubramanianDemosthenis Teneketzis</p>
                <p><a href="http://arxiv.org/abs/2407.12318v1">Link to paper</a></p>
                <p>One of the reasons why stochastic dynamic games with an underlying dynamicsystem are challenging is since strategic players have access to enormousamount of information which leads to the use of extremely complex strategies atequilibrium. One approach to resolve this challenge is to simplify playersstrategies by identifying appropriate compression of information maps so thatthe players can make decisions solely based on the compressed version ofinformation called the information state. For finite dynamic games withasymmetric information inspired by the notion of information state forsingle-agent control problems we propose two notions of information statesnamely mutually sufficient information MSI and unilaterally sufficientinformation USI. Both these information states are obtained with informationcompression maps independent of the strategy profile. We show that Bayes-NashEquilibria BNE and Sequential Equilibria SE exist when all players useMSI-based strategies. We prove that when all players employ USI-basedstrategies the resulting sets of BNE and SE payoff profiles are the same as thesets of BNE and SE payoff profiles resulting when all players use fullinformation-based strategies. We prove that when all players use USI-basedstrategies the resulting set of weak Perfect Bayesian Equilibrium wPBE payoffprofiles can be a proper subset of all wPBE payoff profiles. We identify MSIand USI in specific models of dynamic games in the literature. We end bypresenting an open problem: Do there exist strategy-dependent informationcompression maps that guarantee the existence of at least one equilibrium ormaintain all equilibria that exist under perfect recall We show by acounterexample that a well-known strategy-dependent information compressionmap used in the literature does not possess any of the properties of MSI orUSI.</p>
                <p>Last Updated: 2024-07-17 05:08:47 UTC</p>
                <button class="interpret-button" data-id="2407.12318v1">Interpret</button>
                <div id="interpretation-2407.12318v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Graph-based Adversarial Imitation Learning Framework for Reliable & Realtime Fleet Scheduling in Urban Air Mobility</h3>
                <p>Authors: Prithvi PoddarSteve PaulSouma Chowdhury</p>
                <p><a href="http://arxiv.org/abs/2407.12113v1">Link to paper</a></p>
                <p>The advent of Urban Air Mobility UAM presents the scope for atransformative shift in the domain of urban transportation. However itswidespread adoption and economic viability depends in part on the ability tooptimally schedule the fleet of aircraft across vertiports in a UAM networkunder uncertainties attributed to airspace congestion changing weatherconditions and varying demands. This paper presents a comprehensiveoptimization formulation of the fleet scheduling problem while alsoidentifying the need for alternate solution approaches since directly solvingthe resulting integer nonlinear programming problem is computationallyprohibitive for daily fleet scheduling. Previous work has shown theeffectiveness of using graph reinforcement learning RL approaches to trainreal-time executable policy models for fleet scheduling. However such policiescan often be brittle on out-of-distribution scenarios or edge cases. Moreovertraining performance also deteriorates as the complexity e.g. number ofconstraints of the problem increases. To address these issues this paperpresents an imitation learning approach where the RL-based policy exploitsexpert demonstrations yielded by solving the exact optimization using a GeneticAlgorithm. The policy model comprises Graph Neural Network GNN based encodersthat embed the space of vertiports and aircraft Transformer networks to encodedemand passenger fare and transport cost profiles and a Multi-head attentionMHA based decoder. Expert demonstrations are used through the GenerativeAdversarial Imitation Learning GAIL algorithm. Interfaced with a UAMsimulation environment involving 8 vertiports and 40 aircrafts in terms of thedaily profits earned reward the new imitative approach achieves better meanperformance and remarkable improvement in the case of unseen worst-casescenarios compared to pure RL results.</p>
                <p>Last Updated: 2024-07-16 18:51:24 UTC</p>
                <button class="interpret-button" data-id="2407.12113v1">Interpret</button>
                <div id="interpretation-2407.12113v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Map of Elections</h3>
                <p>Authors: Stanis≈Çaw Szufa</p>
                <p><a href="http://arxiv.org/abs/2407.11889v1">Link to paper</a></p>
                <p>Our main contribution is the introduction of the map of elections framework.A map of elections consists of three main elements: 1 a dataset of electionsi.e. collections of ordinal votes over given sets of candidates 2 a wayof measuring similarities between these elections and 3 a representation ofthe elections in the 2D Euclidean space as points so that the more similar twoelections are the closer are their points. In our maps we mostly focus ondatasets of synthetic elections but we also show an example of a map overreal-life ones. To measure similarities we would have preferred to use e.g.the isomorphic swap distance but this is infeasible due to its highcomputational complexity. Hence we propose polynomial-time computablepositionwise distance and use it instead. Regarding the representations in 2DEuclidean space we mostly use the Kamada-Kawai algorithm but we also show twoalternatives.  We develop the necessary theoretical results to form our maps and argueexperimentally that they are accurate and credible. Further we show howcoloring the elections in a map according to various criteria helps inanalyzing results of a number of experiments. In particular we show coloringsaccording to the scores of winning candidates or committees running times ofILP-based winner determination algorithms and approximation ratios achieved byparticular algorithms.</p>
                <p>Last Updated: 2024-07-16 16:18:29 UTC</p>
                <button class="interpret-button" data-id="2407.11889v1">Interpret</button>
                <div id="interpretation-2407.11889v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning to Imitate Spatial Organization in Multi-robot Systems</h3>
                <p>Authors: Ayomide O. AgunloyeSarvapali D. RamchurnMohammad D. Soorati</p>
                <p><a href="http://arxiv.org/abs/2407.11592v1">Link to paper</a></p>
                <p>Understanding collective behavior and how it evolves is important to ensurethat robot swarms can be trusted in a shared environment. One way to understandthe behavior of the swarm is through collective behavior reconstruction usingprior demonstrations. Existing approaches often require access to the swarmcontroller which may not be available. We reconstruct collective behaviors indistinct swarm scenarios involving shared environments without using swarmcontroller information. We achieve this by transforming prior demonstrationsinto features that sufficiently describe multi-agent interactions beforebehavior reconstruction with multi-agent generative adversarial imitationlearning MA-GAIL. We show that our approach outperforms existing algorithmsin all investigated swarm scenarios and can be used to observe and reconstructa swarms behavior for further analysis and testing which might be impracticalor undesirable on the original robot swarm.</p>
                <p>Last Updated: 2024-07-16 10:50:39 UTC</p>
                <button class="interpret-button" data-id="2407.11592v1">Interpret</button>
                <div id="interpretation-2407.11592v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Navigating the swarm: Deep neural networks command emergent behaviours</h3>
                <p>Authors: Dongjo KimJeongsu LeeHo-Young Kim</p>
                <p><a href="http://arxiv.org/abs/2407.11330v1">Link to paper</a></p>
                <p>Interacting individuals in complex systems often give rise to coherent motionexhibiting coordinated global structures. Such phenomena are ubiquitouslyobserved in nature from cell migration bacterial swarms animal and insectgroups and even human societies. Primary mechanisms responsible for theemergence of collective behavior have been extensively identified includinglocal alignments based on average or relative velocity non-local pairwiserepulsive-attractive interactions such as distance-based potentials interplaybetween local and non-local interactions and cognitive-based inhomogeneousinteractions. However discovering how to adapt these mechanisms to modulateemergent behaviours remains elusive. Here we demonstrate that it is possibleto generate coordinated structures in collective behavior at desired momentswith intended global patterns by fine-tuning an inter-agent interaction rule.Our strategy employs deep neural networks obeying the laws of dynamics tofind interaction rules that command desired collective structures. Thedecomposition of interaction rules into distancing and aligning forcesexpressed by polynomial series facilitates the training of neural networks topropose desired interaction models. Presented examples include altering themean radius and size of clusters in vortical swarms timing of transitions fromrandom to ordered states and continuously shifting between typical modes ofcollective motions. This strategy can even be leveraged to superimposecollective modes resulting in hitherto unexplored but highly practical hybridcollective patterns such as protective security formations. Our findingsreveal innovative strategies for creating and controlling collective motionpaving the way for new applications in robotic swarm operations active matterorganisation and for the uncovering of obscure interaction rules in biologicalsystems.</p>
                <p>Last Updated: 2024-07-16 02:46:11 UTC</p>
                <button class="interpret-button" data-id="2407.11330v1">Interpret</button>
                <div id="interpretation-2407.11330v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases</h3>
                <p>Authors: Zhaorun ChenZhen XiangChaowei XiaoDawn SongBo Li</p>
                <p><a href="http://arxiv.org/abs/2407.12784v1">Link to paper</a></p>
                <p>LLM agents have demonstrated remarkable performance across variousapplications primarily due to their advanced capabilities in reasoningutilizing external knowledge and tools calling APIs and executing actions tointeract with environments. Current agents typically utilize a memory module ora retrieval-augmented generation RAG mechanism retrieving past knowledge andinstances with similar embeddings from knowledge bases to inform task planningand execution. However the reliance on unverified knowledge bases raisessignificant concerns about their safety and trustworthiness. To uncover suchvulnerabilities we propose a novel red teaming approach AgentPoison the firstbackdoor attack targeting generic and RAG-based LLM agents by poisoning theirlong-term memory or RAG knowledge base. In particular we form the triggergeneration process as a constrained optimization to optimize backdoor triggersby mapping the triggered instances to a unique embedding space so as to ensurethat whenever a user instruction contains the optimized backdoor trigger themalicious demonstrations are retrieved from the poisoned memory or knowledgebase with high probability. In the meantime benign instructions without thetrigger will still maintain normal performance. Unlike conventional backdoorattacks AgentPoison requires no additional model training or fine-tuning andthe optimized backdoor trigger exhibits superior transferability in-contextcoherence and stealthiness. Extensive experiments demonstrate AgentPoisonseffectiveness in attacking three types of real-world LLM agents: RAG-basedautonomous driving agent knowledge-intensive QA agent and healthcareEHRAgent. On each agent AgentPoison achieves an average attack success ratehigher than 80 with minimal impact on benign performance less than 1 with apoison rate less than 0.1.</p>
                <p>Last Updated: 2024-07-17 17:59:47 UTC</p>
                <button class="interpret-button" data-id="2407.12784v1">Interpret</button>
                <div id="interpretation-2407.12784v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Contrastive Adversarial Training for Unsupervised Domain Adaptation</h3>
                <p>Authors: Jiahong ChenZhilin ZhangLucy LiBehzad ShahrasbiArjun Mishra</p>
                <p><a href="http://arxiv.org/abs/2407.12782v1">Link to paper</a></p>
                <p>Domain adversarial training has shown its effective capability for findingdomain invariant feature representations and been successfully adopted forvarious domain adaptation tasks. However recent advances of large modelse.g. vision transformers and emerging of complex adaptation scenarios e.g.DomainNet make adversarial training being easily biased towards source domainand hardly adapted to target domain. The reason is twofold: relying on largeamount of labelled data from source domain for large model training and lackingof labelled data from target domain for fine-tuning. Existing approaches widelyfocused on either enhancing discriminator or improving the training stabilityfor the backbone networks. Due to unbalanced competition between the featureextractor and the discriminator during the adversarial training existingsolutions fail to function well on complex datasets. To address this issue weproposed a novel contrastive adversarial training CAT approach that leveragesthe labeled source domain samples to reinforce and regulate the featuregeneration for target domain. Typically the regulation forces the targetfeature distribution being similar to the source feature distribution. CATaddressed three major challenges in adversarial learning: 1 ensure the featuredistributions from two domains as indistinguishable as possible for thediscriminator resulting in a more robust domain-invariant feature generation2 encourage target samples moving closer to the source in the feature spacereducing the requirement for generalizing classifier trained on the labeledsource domain to unlabeled target domain 3 avoid directly aligning unpairedsource and target samples within mini-batch. CAT can be easily plugged intoexisting models and exhibits significant performance improvements.</p>
                <p>Last Updated: 2024-07-17 17:59:21 UTC</p>
                <button class="interpret-button" data-id="2407.12782v1">Interpret</button>
                <div id="interpretation-2407.12782v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Jigsaw Game: Federated Clustering</h3>
                <p>Authors: Jinxuan XuHong-You ChenWei-Lun ChaoYuqian Zhang</p>
                <p><a href="http://arxiv.org/abs/2407.12764v1">Link to paper</a></p>
                <p>Federated learning has recently garnered significant attention especiallywithin the domain of supervised learning. However despite the abundance ofunlabeled data on end-users unsupervised learning problems such as clusteringin the federated setting remain underexplored. In this paper we investigatethe federated clustering problem with a focus on federated k-means. We outlinethe challenge posed by its non-convex objective and data heterogeneity in thefederated framework. To tackle these challenges we adopt a new perspective bystudying the structures of local solutions in k-means and propose a one-shotalgorithm called FeCA Federated Centroid Aggregation. FeCA adaptively refineslocal solutions on clients then aggregates these refined solutions to recoverthe global solution of the entire dataset in a single round. We empiricallydemonstrate the robustness of FeCA under various federated scenarios on bothsynthetic and real-world data. Additionally we extend FeCA to representationlearning and present DeepFeCA which combines DeepCluster and FeCA forunsupervised feature learning in the federated setting.</p>
                <p>Last Updated: 2024-07-17 17:42:25 UTC</p>
                <button class="interpret-button" data-id="2407.12764v1">Interpret</button>
                <div id="interpretation-2407.12764v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A survey and taxonomy of methods interpreting random forest models</h3>
                <p>Authors: Maissae HaddouchiAbdelaziz Berrado</p>
                <p><a href="http://arxiv.org/abs/2407.12759v1">Link to paper</a></p>
                <p>The interpretability of random forest RF models is a research topic ofgrowing interest in the machine learning ML community. In the state of theart RF is considered a powerful learning ensemble given its predictiveperformance flexibility and ease of use. Furthermore the inner process ofthe RF model is understandable because it uses an intuitive and intelligibleapproach for building the RF decision tree ensemble. However the RF resultingmodel is regarded as a black box because of its numerous deep decision trees.Gaining visibility over the entire process that induces the final decisions byexploring each decision tree is complicated if not impossible. This complexitylimits the acceptance and implementation of RF models in several fields ofapplication. Several papers have tackled the interpretation of RF models. Thispaper aims to provide an extensive review of methods used in the literature tointerpret RF resulting models. We have analyzed these methods and classifiedthem based on different axes. Although this review is not exhaustive itprovides a taxonomy of various techniques that should guide users in choosingthe most appropriate tools for interpreting RF models depending on theinterpretability aspects sought. It should also be valuable for researchers whoaim to focus their work on the interpretability of RF or ML black boxes ingeneral.</p>
                <p>Last Updated: 2024-07-17 17:33:32 UTC</p>
                <button class="interpret-button" data-id="2407.12759v1">Interpret</button>
                <div id="interpretation-2407.12759v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LookupViT: Compressing visual information to a limited number of tokens</h3>
                <p>Authors: Rajat KonerGagan JainPrateek JainVolker TrespSujoy Paul</p>
                <p><a href="http://arxiv.org/abs/2407.12753v1">Link to paper</a></p>
                <p>Vision Transformers ViT have emerged as the de-facto choice for numerousindustry grade vision solutions. But their inference cost can be prohibitivefor many settings as they compute self-attention in each layer which suffersfrom quadratic computational complexity in the number of tokens. On the otherhand spatial information in images and spatio-temporal information in videosis usually sparse and redundant. In this work we introduce LookupViT thataims to exploit this information sparsity to reduce ViT inference cost.LookupViT provides a novel general purpose vision transformer block thatoperates by compressing information from higher resolution tokens to a fixednumber of tokens. These few compressed tokens undergo meticulous processingwhile the higher-resolution tokens are passed through computationally cheaperlayers. Information sharing between these two token sets is enabled through abidirectional cross-attention mechanism. The approach offers multipleadvantages - a easy to implement on standard ML accelerators GPUs/TPUs viastandard high-level operators b applicable to standard ViT and its variantsthus generalizes to various tasks c can handle different tokenization andattention approaches. LookupViT also offers flexibility for the compressedtokens enabling performance-computation trade-offs in a single trained model.We show LookupViTs effectiveness on multiple domains - a forimage-classification ImageNet-1K and ImageNet-21K b video classificationKinetics400 and Something-Something V2 c image captioning COCO-Captionswith a frozen encoder. LookupViT provides 2times reduction in FLOPs whileupholding or improving accuracy across these domains. In addition LookupViTalso demonstrates out-of-the-box robustness and generalization on imageclassification ImageNet-CRAO improving by up to 4 over ViT.</p>
                <p>Last Updated: 2024-07-17 17:22:43 UTC</p>
                <button class="interpret-button" data-id="2407.12753v1">Interpret</button>
                <div id="interpretation-2407.12753v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>SMooDi: Stylized Motion Diffusion Model</h3>
                <p>Authors: Lei ZhongYiming XieVarun JampaniDeqing SunHuaizu Jiang</p>
                <p><a href="http://arxiv.org/abs/2407.12783v1">Link to paper</a></p>
                <p>We introduce a novel Stylized Motion Diffusion model dubbed SMooDi togenerate stylized motion driven by content texts and style motion sequences.Unlike existing methods that either generate motion of various content ortransfer style from one sequence to another SMooDi can rapidly generate motionacross a broad range of content and diverse styles. To this end we tailor apre-trained text-to-motion model for stylization. Specifically we proposestyle guidance to ensure that the generated motion closely matches thereference style alongside a lightweight style adaptor that directs the motiontowards the desired style while ensuring realism. Experiments across variousapplications demonstrate that our proposed framework outperforms existingmethods in stylized motion generation.</p>
                <p>Last Updated: 2024-07-17 17:59:42 UTC</p>
                <button class="interpret-button" data-id="2407.12783v1">Interpret</button>
                <div id="interpretation-2407.12783v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Contrastive Adversarial Training for Unsupervised Domain Adaptation</h3>
                <p>Authors: Jiahong ChenZhilin ZhangLucy LiBehzad ShahrasbiArjun Mishra</p>
                <p><a href="http://arxiv.org/abs/2407.12782v1">Link to paper</a></p>
                <p>Domain adversarial training has shown its effective capability for findingdomain invariant feature representations and been successfully adopted forvarious domain adaptation tasks. However recent advances of large modelse.g. vision transformers and emerging of complex adaptation scenarios e.g.DomainNet make adversarial training being easily biased towards source domainand hardly adapted to target domain. The reason is twofold: relying on largeamount of labelled data from source domain for large model training and lackingof labelled data from target domain for fine-tuning. Existing approaches widelyfocused on either enhancing discriminator or improving the training stabilityfor the backbone networks. Due to unbalanced competition between the featureextractor and the discriminator during the adversarial training existingsolutions fail to function well on complex datasets. To address this issue weproposed a novel contrastive adversarial training CAT approach that leveragesthe labeled source domain samples to reinforce and regulate the featuregeneration for target domain. Typically the regulation forces the targetfeature distribution being similar to the source feature distribution. CATaddressed three major challenges in adversarial learning: 1 ensure the featuredistributions from two domains as indistinguishable as possible for thediscriminator resulting in a more robust domain-invariant feature generation2 encourage target samples moving closer to the source in the feature spacereducing the requirement for generalizing classifier trained on the labeledsource domain to unlabeled target domain 3 avoid directly aligning unpairedsource and target samples within mini-batch. CAT can be easily plugged intoexisting models and exhibits significant performance improvements.</p>
                <p>Last Updated: 2024-07-17 17:59:21 UTC</p>
                <button class="interpret-button" data-id="2407.12782v1">Interpret</button>
                <div id="interpretation-2407.12782v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control</h3>
                <p>Authors: Sherwin BahmaniIvan SkorokhodovAliaksandr SiarohinWilli MenapaceGuocheng QianMichael VasilkovskyHsin-Ying LeeChaoyang WangJiaxu ZouAndrea TagliasacchiDavid B. LindellSergey Tulyakov</p>
                <p><a href="http://arxiv.org/abs/2407.12781v1">Link to paper</a></p>
                <p>Modern text-to-video synthesis models demonstrate coherent photorealisticgeneration of complex videos from a text description. However most existingmodels lack fine-grained control over camera movement which is critical fordownstream applications related to content creation visual effects and 3Dvision. Recently new methods demonstrate the ability to generate videos withcontrollable camera poses these techniques leverage pre-trained U-Net-baseddiffusion models that explicitly disentangle spatial and temporal generation.Still no existing approach enables camera control for new transformer-basedvideo diffusion models that process spatial and temporal information jointly.Here we propose to tame video transformers for 3D camera control using aControlNet-like conditioning mechanism that incorporates spatiotemporal cameraembeddings based on Plucker coordinates. The approach demonstratesstate-of-the-art performance for controllable video generation afterfine-tuning on the RealEstate10K dataset. To the best of our knowledge ourwork is the first to enable camera control for transformer-based videodiffusion models.</p>
                <p>Last Updated: 2024-07-17 17:59:05 UTC</p>
                <button class="interpret-button" data-id="2407.12781v1">Interpret</button>
                <div id="interpretation-2407.12781v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Generalizable Human Gaussians for Sparse View Synthesis</h3>
                <p>Authors: Youngjoong KwonBaole FangYixing LuHaoye DongCheng ZhangFrancisco Vicente CarrascoAlbert Mosella-MontoroJianjin XuShingo TakagiDaeil KimAayush PrakashFernando De la Torre</p>
                <p><a href="http://arxiv.org/abs/2407.12777v1">Link to paper</a></p>
                <p>Recent progress in neural rendering has brought forth pioneering methodssuch as NeRF and Gaussian Splatting which revolutionize view rendering acrossvarious domains like AR/VR gaming and content creation. While these methodsexcel at interpolating em within the training data the challenge ofgeneralizing to new scenes and objects from very sparse views persists.Specifically modeling 3D humans from sparse views presents formidable hurdlesdue to the inherent complexity of human geometry resulting in inaccuratereconstructions of geometry and textures. To tackle this challenge this paperleverages recent advancements in Gaussian Splatting and introduces a new methodto learn generalizable human Gaussians that allows photorealistic and accurateview-rendering of a new human subject from a limited set of sparse views in afeed-forward manner. A pivotal innovation of our approach involvesreformulating the learning of 3D Gaussian parameters into a regression processdefined on the 2D UV space of a human template which allows leveraging thestrong geometry prior and the advantages of 2D convolutions. In addition amulti-scaffold is proposed to effectively represent the offset details. Ourmethod outperforms recent methods on both within-dataset generalization as wellas cross-dataset generalization settings.</p>
                <p>Last Updated: 2024-07-17 17:56:30 UTC</p>
                <button class="interpret-button" data-id="2407.12777v1">Interpret</button>
                <div id="interpretation-2407.12777v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>OMG-Net: A Deep Learning Framework Deploying Segment Anything to Detect Pan-Cancer Mitotic Figures from Haematoxylin and Eosin-Stained Slides</h3>
                <p>Authors: Zhuoyan ShenMikael SimardDouglas BrandVanghelita AndreiAli Al-KhaderFatine OumlilKatherine TreversThomas ButtersSimon HaefligerEleanna KaraFernanda AmaryRoberto TiraboscoPaul CoolGary RoyleMaria A. HawkinsAdrienne M. FlanaganCharles-Antoine Collins Fekete</p>
                <p><a href="http://arxiv.org/abs/2407.12773v1">Link to paper</a></p>
                <p>Mitotic activity is an important feature for grading several cancer types.Counting mitotic figures MFs is a time-consuming laborious task prone tointer-observer variation. Inaccurate recognition of MFs can lead to incorrectgrading and hence potential suboptimal treatment. In this study we propose anartificial intelligence AI-aided approach to detect MFs in digitisedhaematoxylin and eosin-stained whole slide images WSIs. Advances in this areaare hampered by the limited number and types of cancer datasets of MFs. Here weestablish the largest pan-cancer dataset of mitotic figures by combining anin-house dataset of soft tissue tumours STMF with five open-source mitoticdatasets comprising multiple human cancers and canine specimens ICPR TUPACCCMCT CMC and MIDOG. This new dataset identifies 74620 MFs and 105538mitotic-like figures. We then employed a two-stage framework the OptimisedMitoses Generator Network OMG-Net to classify MFs. The framework firstdeploys the Segment Anything Model SAM to automate the contouring of MFs andsurrounding objects. An adapted ResNet18 is subsequently trained to classifyMFs. OMG-Net reaches an F1-score of 0.84 on pan-cancer MF detection breastcarcinoma neuroendocrine tumour and melanoma largely outperforming theprevious state-of-the-art MIDOG benchmark model on its hold-out testing sete.g. 16 F1-score on breast cancer detection p0.001 thereby providingsuperior accuracy in detecting MFs on various types of tumours obtained withdifferent scanners.</p>
                <p>Last Updated: 2024-07-17 17:53:37 UTC</p>
                <button class="interpret-button" data-id="2407.12773v1">Interpret</button>
                <div id="interpretation-2407.12773v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>OMG-Net: A Deep Learning Framework Deploying Segment Anything to Detect Pan-Cancer Mitotic Figures from Haematoxylin and Eosin-Stained Slides</h3>
                <p>Authors: Zhuoyan ShenMikael SimardDouglas BrandVanghelita AndreiAli Al-KhaderFatine OumlilKatherine TreversThomas ButtersSimon HaefligerEleanna KaraFernanda AmaryRoberto TiraboscoPaul CoolGary RoyleMaria A. HawkinsAdrienne M. FlanaganCharles-Antoine Collins Fekete</p>
                <p><a href="http://arxiv.org/abs/2407.12773v1">Link to paper</a></p>
                <p>Mitotic activity is an important feature for grading several cancer types.Counting mitotic figures MFs is a time-consuming laborious task prone tointer-observer variation. Inaccurate recognition of MFs can lead to incorrectgrading and hence potential suboptimal treatment. In this study we propose anartificial intelligence AI-aided approach to detect MFs in digitisedhaematoxylin and eosin-stained whole slide images WSIs. Advances in this areaare hampered by the limited number and types of cancer datasets of MFs. Here weestablish the largest pan-cancer dataset of mitotic figures by combining anin-house dataset of soft tissue tumours STMF with five open-source mitoticdatasets comprising multiple human cancers and canine specimens ICPR TUPACCCMCT CMC and MIDOG. This new dataset identifies 74620 MFs and 105538mitotic-like figures. We then employed a two-stage framework the OptimisedMitoses Generator Network OMG-Net to classify MFs. The framework firstdeploys the Segment Anything Model SAM to automate the contouring of MFs andsurrounding objects. An adapted ResNet18 is subsequently trained to classifyMFs. OMG-Net reaches an F1-score of 0.84 on pan-cancer MF detection breastcarcinoma neuroendocrine tumour and melanoma largely outperforming theprevious state-of-the-art MIDOG benchmark model on its hold-out testing sete.g. 16 F1-score on breast cancer detection p0.001 thereby providingsuperior accuracy in detecting MFs on various types of tumours obtained withdifferent scanners.</p>
                <p>Last Updated: 2024-07-17 17:53:37 UTC</p>
                <button class="interpret-button" data-id="2407.12773v1">Interpret</button>
                <div id="interpretation-2407.12773v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LookupViT: Compressing visual information to a limited number of tokens</h3>
                <p>Authors: Rajat KonerGagan JainPrateek JainVolker TrespSujoy Paul</p>
                <p><a href="http://arxiv.org/abs/2407.12753v1">Link to paper</a></p>
                <p>Vision Transformers ViT have emerged as the de-facto choice for numerousindustry grade vision solutions. But their inference cost can be prohibitivefor many settings as they compute self-attention in each layer which suffersfrom quadratic computational complexity in the number of tokens. On the otherhand spatial information in images and spatio-temporal information in videosis usually sparse and redundant. In this work we introduce LookupViT thataims to exploit this information sparsity to reduce ViT inference cost.LookupViT provides a novel general purpose vision transformer block thatoperates by compressing information from higher resolution tokens to a fixednumber of tokens. These few compressed tokens undergo meticulous processingwhile the higher-resolution tokens are passed through computationally cheaperlayers. Information sharing between these two token sets is enabled through abidirectional cross-attention mechanism. The approach offers multipleadvantages - a easy to implement on standard ML accelerators GPUs/TPUs viastandard high-level operators b applicable to standard ViT and its variantsthus generalizes to various tasks c can handle different tokenization andattention approaches. LookupViT also offers flexibility for the compressedtokens enabling performance-computation trade-offs in a single trained model.We show LookupViTs effectiveness on multiple domains - a forimage-classification ImageNet-1K and ImageNet-21K b video classificationKinetics400 and Something-Something V2 c image captioning COCO-Captionswith a frozen encoder. LookupViT provides 2times reduction in FLOPs whileupholding or improving accuracy across these domains. In addition LookupViTalso demonstrates out-of-the-box robustness and generalization on imageclassification ImageNet-CRAO improving by up to 4 over ViT.</p>
                <p>Last Updated: 2024-07-17 17:22:43 UTC</p>
                <button class="interpret-button" data-id="2407.12753v1">Interpret</button>
                <div id="interpretation-2407.12753v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference</h3>
                <p>Authors: Mohammad Erfan SadeghiArash FayyaziSuhas SomashekarMassoud Pedram</p>
                <p><a href="http://arxiv.org/abs/2407.12736v1">Link to paper</a></p>
                <p>Vision Transformers ViTs represent a groundbreaking shift in machinelearning approaches to computer vision. Unlike traditional approaches ViTsemploy the self-attention mechanism which has been widely used in naturallanguage processing to analyze image patches. Despite their advantages inmodeling visual tasks deploying ViTs on hardware platforms notablyField-Programmable Gate Arrays FPGAs introduces considerable challenges.These challenges stem primarily from the non-linear calculations and highcomputational and memory demands of ViTs. This paper introduces CHOSEN asoftware-hardware co-design framework to address these challenges and offer anautomated framework for ViT deployment on the FPGAs in order to maximizeperformance. Our framework is built upon three fundamental contributions:multi-kernel design to maximize the bandwidth mainly targeting benefits ofmulti DDR memory banks approximate non-linear functions that exhibit minimalaccuracy degradation and efficient use of available logic blocks on the FPGAand efficient compiler to maximize the performance and memory-efficiency of thecomputing kernels by presenting a novel algorithm for design space explorationto find optimal hardware configuration that achieves optimal throughput andlatency. Compared to the state-of-the-art ViT accelerators CHOSEN achieves a1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.</p>
                <p>Last Updated: 2024-07-17 16:56:06 UTC</p>
                <button class="interpret-button" data-id="2407.12736v1">Interpret</button>
                <div id="interpretation-2407.12736v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RoDE: Linear Rectified Mixture of Diverse Experts for Food Large Multi-Modal Models</h3>
                <p>Authors: Pengkun JiaoXinlan WuBin ZhuJingjing ChenChong-Wah NgoYugang Jiang</p>
                <p><a href="http://arxiv.org/abs/2407.12730v1">Link to paper</a></p>
                <p>Large Multi-modal Models LMMs have significantly advanced a variety ofvision-language tasks. The scalability and availability of high-qualitytraining data play a pivotal role in the success of LMMs. In the realm of foodwhile comprehensive food datasets such as Recipe1M offer an abundance ofingredient and recipe information they often fall short of providing ampledata for nutritional analysis. The Recipe1M dataset despite offering a subsetfor nutritional evaluation is limited in the scale and accuracy of nutritioninformation. To bridge this gap we introduce Uni-Food a unified food datasetthat comprises over 100000 images with various food labels includingcategories ingredients recipes and ingredient-level nutritional information.Uni-Food is designed to provide a more holistic approach to food data analysisthereby enhancing the performance and capabilities of LMMs in this domain. Tomitigate the conflicts arising from multi-task supervision during fine-tuningof LMMs we introduce a novel Linear Rectification Mixture of Diverse ExpertsRoDE approach. RoDE utilizes a diverse array of experts to address tasks ofvarying complexity thereby facilitating the coordination of trainableparameters i.e. it allocates more parameters for more complex tasks andconversely fewer parameters for simpler tasks. RoDE implements linearrectification union to refine the routers functionality thereby enhancing theefficiency of sparse task allocation. These design choices endow RoDE withfeatures that ensure GPU memory efficiency and ease of optimization. Ourexperimental results validate the effectiveness of our proposed approach inaddressing the inherent challenges of food-related multitasking.</p>
                <p>Last Updated: 2024-07-17 16:49:34 UTC</p>
                <button class="interpret-button" data-id="2407.12730v1">Interpret</button>
                <div id="interpretation-2407.12730v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>An Evaluation of Continual Learning for Advanced Node Semiconductor Defect Inspection</h3>
                <p>Authors: Amit PrasadBappaditya DeyVictor BlancoSandip Halder</p>
                <p><a href="http://arxiv.org/abs/2407.12724v1">Link to paper</a></p>
                <p>Deep learning-based semiconductor defect inspection has gained traction inrecent years offering a powerful and versatile approach that provides highaccuracy adaptability and efficiency in detecting and classifying nano-scaledefects. However semiconductor manufacturing processes are continuallyevolving leading to the emergence of new types of defects over time. Thispresents a significant challenge for conventional supervised defect detectorsas they may suffer from catastrophic forgetting when trained on new defectdatasets potentially compromising performance on previously learned tasks. Analternative approach involves the constant storage of previously traineddatasets alongside pre-trained model versions which can be utilized forre-training from scratch or fine-tuning whenever encountering a new defectdataset. However adhering to such a storage template is impractical in termsof size particularly when considering High-Volume Manufacturing HVM.Additionally semiconductor defect datasets especially those encompassingstochastic defects are often limited and expensive to obtain thus lackingsufficient representation of the entire universal set of defectivity. This workintroduces a task-agnostic meta-learning approach aimed at addressing thischallenge which enables the incremental addition of new defect classes andscales to create a more robust and generalized model for semiconductor defectinspection. We have benchmarked our approach using real resist-wafer SEMScanning Electron Microscopy datasets for two process steps ADI and AEIdemonstrating its superior performance compared to conventional supervisedtraining methods.</p>
                <p>Last Updated: 2024-07-17 16:41:22 UTC</p>
                <button class="interpret-button" data-id="2407.12724v1">Interpret</button>
                <div id="interpretation-2407.12724v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Scalable Monte Carlo for Bayesian Learning</h3>
                <p>Authors: Paul FearnheadChristopher NemethChris J. OatesChris Sherlock</p>
                <p><a href="http://arxiv.org/abs/2407.12751v1">Link to paper</a></p>
                <p>This book aims to provide a graduate-level introduction to advanced topics inMarkov chain Monte Carlo MCMC algorithms as applied broadly in the Bayesiancomputational context. Most if not all of these topics stochastic gradientMCMC non-reversible MCMC continuous time MCMC and new techniques forconvergence assessment have emerged as recently as the last decade and havedriven substantial recent practical and theoretical advances in the field. Aparticular focus is on methods that are scalable with respect to either theamount of data or the data dimension motivated by the emerging high-priorityapplication areas in machine learning and AI.</p>
                <p>Last Updated: 2024-07-17 17:19:56 UTC</p>
                <button class="interpret-button" data-id="2407.12751v1">Interpret</button>
                <div id="interpretation-2407.12751v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Methodology Establishing Linear Convergence of Adaptive Gradient Methods under PL Inequality</h3>
                <p>Authors: Kushal ChakrabartiMayank Baranwal</p>
                <p><a href="http://arxiv.org/abs/2407.12629v1">Link to paper</a></p>
                <p>Adaptive gradient-descent optimizers are the standard choice for trainingneural network models. Despite their faster convergence than gradient-descentand remarkable performance in practice the adaptive optimizers are not as wellunderstood as vanilla gradient-descent. A reason is that the dynamic update ofthe learning rate that helps in faster convergence of these methods also makestheir analysis intricate. Particularly the simple gradient-descent methodconverges at a linear rate for a class of optimization problems whereas thepractically faster adaptive gradient methods lack such a theoretical guarantee.The Polyak-Lojasiewicz PL inequality is the weakest known class for whichlinear convergence of gradient-descent and its momentum variants has beenproved. Therefore in this paper we prove that AdaGrad and Adam twowell-known adaptive gradient methods converge linearly when the cost functionis smooth and satisfies the PL inequality. Our theoretical framework follows asimple and unified approach applicable to both batch and stochastic gradientswhich can potentially be utilized in analyzing linear convergence of othervariants of Adam.</p>
                <p>Last Updated: 2024-07-17 14:56:21 UTC</p>
                <button class="interpret-button" data-id="2407.12629v1">Interpret</button>
                <div id="interpretation-2407.12629v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Test-Time Adaptation with State-Space Models</h3>
                <p>Authors: Mona SchirmerDan ZhangEric Nalisnick</p>
                <p><a href="http://arxiv.org/abs/2407.12492v1">Link to paper</a></p>
                <p>Distribution shifts between training and test data are all but inevitableover the lifecycle of a deployed model and lead to performance decay. Adaptingthe model can hopefully mitigate this drop in performance. Yet adaptation ischallenging since it must be unsupervised: we usually do not have access to anylabeled data at test time. In this paper we propose a probabilisticstate-space model that can adapt a deployed model subjected to distributiondrift. Our model learns the dynamics induced by distribution shifts on the lastset of hidden features. Without requiring labels we infer time-evolving classprototypes that serve as a dynamic classification head. Moreover our approachis lightweight modifying only the models last linear layer. In experiments onreal-world distribution shifts and synthetic corruptions we demonstrate thatour approach performs competitively with methods that require back-propagationand access to the model backbone. Our model especially excels in the case ofsmall test batches - the most difficult setting.</p>
                <p>Last Updated: 2024-07-17 11:18:49 UTC</p>
                <button class="interpret-button" data-id="2407.12492v1">Interpret</button>
                <div id="interpretation-2407.12492v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Why Do You Grok? A Theoretical Analysis of Grokking Modular Addition</h3>
                <p>Authors: Mohamad Amin MohamadiZhiyuan LiLei WuDanica J. Sutherland</p>
                <p><a href="http://arxiv.org/abs/2407.12332v1">Link to paper</a></p>
                <p>We present a theoretical explanation of the grokking phenomenon where amodel generalizes long after overfittingfor the originally-studied problem ofmodular addition. First we show that early in gradient descent when thekernel regime approximately holds no permutation-equivariant model canachieve small population error on modular addition unless it sees at least aconstant fraction of all possible data points. Eventually however modelsescape the kernel regime. We show that two-layer quadratic networks thatachieve zero training loss with bounded ell_infty norm generalize wellwith substantially fewer training points and further show such networks existand can be found by gradient descent with small ell_infty regularization.We further provide empirical evidence that these networks as well as simpleTransformers leave the kernel regime only after initially overfitting. Takentogether our results strongly support the case for grokking as a consequenceof the transition from kernel-like behavior to limiting behavior of gradientdescent on deep networks.</p>
                <p>Last Updated: 2024-07-17 06:15:30 UTC</p>
                <button class="interpret-button" data-id="2407.12332v1">Interpret</button>
                <div id="interpretation-2407.12332v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Information-Theoretic Foundations for Machine Learning</h3>
                <p>Authors: Hong Jun JeonBenjamin Van Roy</p>
                <p><a href="http://arxiv.org/abs/2407.12288v1">Link to paper</a></p>
                <p>The staggering progress of machine learning in the past decade has been asight to behold. In retrospect it is both remarkable and unsettling that thesemilestones were achievable with little to no rigorous theory to guideexperimentation. Despite this fact practitioners have been able to guide theirfuture experimentation via observations from previous large-scale empiricalinvestigations. However alluding to Platos Allegory of the cave it is likelythat the observations which form the fields notion of reality are but shadowsrepresenting fragments of that reality. In this work we propose a theoreticalframework which attempts to answer what exists outside of the cave. To thetheorist we provide a framework which is mathematically rigorous and leavesopen many interesting ideas for future exploration. To the practitioner weprovide a framework whose results are very intuitive general and which willhelp form principles to guide future investigations. Concretely we provide atheoretical framework rooted in Bayesian statistics and Shannons informationtheory which is general enough to unify the analysis of many phenomena inmachine learning. Our framework characterizes the performance of an optimalBayesian learner which considers the fundamental limits of information.Throughout this work we derive very general theoretical results and apply themto derive insights specific to settings ranging from data which isindependently and identically distributed under an unknown distribution todata which is sequential to data which exhibits hierarchical structureamenable to meta-learning. We conclude with a section dedicated tocharacterizing the performance of misspecified algorithms. These results areexciting and particularly relevant as we strive to overcome increasinglydifficult machine learning challenges in this endlessly complex world.</p>
                <p>Last Updated: 2024-07-17 03:18:40 UTC</p>
                <button class="interpret-button" data-id="2407.12288v1">Interpret</button>
                <div id="interpretation-2407.12288v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-07-18</p>
        </div>
    
        </div>
    </body>
    </html>
    