
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Mapping Social Choice Theory to RLHF</h3>
                <p>Authors: Jessica DaiEve Fleisig</p>
                <p><a href="http://arxiv.org/abs/2404.13038v1">Link to paper</a></p>
                <p>Recent work on the limitations of using reinforcement learning from humanfeedback RLHF to incorporate human preferences into model behavior oftenraises social choice theory as a reference point. Social choice theorysanalysis of settings such as voting mechanisms provides technicalinfrastructure that can inform how to aggregate human preferences amiddisagreement. We analyze the problem settings of social choice and RLHFidentify key differences between them and discuss how these differences mayaffect the RLHF interpretation of well-known technical results in socialchoice.</p>
                <p>Last Updated: 2024-04-19 17:49:56 UTC</p>
                <button class="interpret-button" data-id="2404.13038v1">Interpret</button>
                <div id="interpretation-2404.13038v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>When Life gives you LLMs, make LLM-ADE: Large Language Models with Adaptive Data Engineering</h3>
                <p>Authors: Stephen ChoiWilliam Gazeley</p>
                <p><a href="http://arxiv.org/abs/2404.13028v1">Link to paper</a></p>
                <p>This paper presents the LLM-ADE framework a novel methodology for continuedpre-training of large language models LLMs that addresses the challenges ofcatastrophic forgetting and double descent. LLM-ADE employs dynamicarchitectural adjustments including selective block freezing and expansiontailored to specific datasets. This strategy enhances model adaptability to newdata while preserving previously acquired knowledge. We demonstrate LLM-ADEseffectiveness on the TinyLlama model across various general knowledgebenchmarks showing significant performance improvements without the drawbacksof traditional continuous training methods. This approach promises a moreversatile and robust way to keep LLMs current and efficient in real-worldapplications.</p>
                <p>Last Updated: 2024-04-19 17:43:26 UTC</p>
                <button class="interpret-button" data-id="2404.13028v1">Interpret</button>
                <div id="interpretation-2404.13028v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation</h3>
                <p>Authors: Tianyuan ZhangHong-Xing YuRundi WuBrandon Y. FengChangxi ZhengNoah SnavelyJiajun WuWilliam T. Freeman</p>
                <p><a href="http://arxiv.org/abs/2404.13026v1">Link to paper</a></p>
                <p>Realistic object interactions are crucial for creating immersive virtualexperiences yet synthesizing realistic 3D object dynamics in response to novelinteractions remains a significant challenge. Unlike unconditional ortext-conditioned dynamics generation action-conditioned dynamics requiresperceiving the physical material properties of objects and grounding the 3Dmotion prediction on these properties such as object stiffness. Howeverestimating physical material properties is an open problem due to the lack ofmaterial ground-truth data as measuring these properties for real objects ishighly difficult. We present PhysDreamer a physics-based approach that endowsstatic 3D objects with interactive dynamics by leveraging the object dynamicspriors learned by video generation models. By distilling these priorsPhysDreamer enables the synthesis of realistic object responses to novelinteractions such as external forces or agent manipulations. We demonstrateour approach on diverse examples of elastic objects and evaluate the realism ofthe synthesized interactions through a user study. PhysDreamer takes a steptowards more engaging and realistic virtual experiences by enabling static 3Dobjects to dynamically respond to interactive stimuli in a physically plausiblemanner. See our project page at https://physdreamer.github.io/.</p>
                <p>Last Updated: 2024-04-19 17:41:05 UTC</p>
                <button class="interpret-button" data-id="2404.13026v1">Interpret</button>
                <div id="interpretation-2404.13026v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models</h3>
                <p>Authors: Chuofan MaYi JiangJiannan WuZehuan YuanXiaojuan Qi</p>
                <p><a href="http://arxiv.org/abs/2404.13013v1">Link to paper</a></p>
                <p>We introduce Groma a Multimodal Large Language Model MLLM with groundedand fine-grained visual perception ability. Beyond holistic imageunderstanding Groma is adept at region-level tasks such as region captioningand visual grounding. Such capabilities are built upon a localized visualtokenization mechanism where an image input is decomposed into regions ofinterest and subsequently encoded into region tokens. By integrating regiontokens into user instructions and model responses we seamlessly enable Gromato understand user-specified region inputs and ground its textual output toimages. Besides to enhance the grounded chat ability of Groma we curate avisually grounded instruction dataset by leveraging the powerful GPT-4V andvisual prompting techniques. Compared with MLLMs that rely on the languagemodel or external module for localization Groma consistently demonstratessuperior performances in standard referring and grounding benchmarkshighlighting the advantages of embedding localization into image tokenization.Project page: https://groma-mllm.github.io/.</p>
                <p>Last Updated: 2024-04-19 17:22:51 UTC</p>
                <button class="interpret-button" data-id="2404.13013v1">Interpret</button>
                <div id="interpretation-2404.13013v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>FinLangNet: A Novel Deep Learning Framework for Credit Risk Prediction Using Linguistic Analogy in Financial Data</h3>
                <p>Authors: Yu LeiZixuan WangChu LiuTongyao WangDongyang Lee</p>
                <p><a href="http://arxiv.org/abs/2404.13004v1">Link to paper</a></p>
                <p>Recent industrial applications in risk prediction still heavily rely onextensively manually-tuned statistical learning methods. Real-world financialdata characterized by its high-dimensionality sparsity high noise levelsand significant imbalance poses unique challenges for the effectiveapplication of deep neural network models. In this work we introduce a noveldeep learning risk prediction framework FinLangNet which conceptualizescredit loan trajectories in a structure that mirrors linguistic constructs.This framework is tailored for credit risk prediction using real-worldfinancial data drawing on structural similarities to language by adaptingnatural language processing techniques. It focuses on analyzing the evolutionand predictability of credit histories through detailed financial eventsequences. Our research demonstrates that FinLangNet surpasses traditionalstatistical methods in predicting credit risk and that its integration withthese methods enhances credit card fraud prediction models achieving asignificant improvement of over 1.5 points in the Kolmogorov-Smirnov metric.</p>
                <p>Last Updated: 2024-04-19 17:01:46 UTC</p>
                <button class="interpret-button" data-id="2404.13004v1">Interpret</button>
                <div id="interpretation-2404.13004v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Optimizing Calibration by Gaining Aware of Prediction Correctness</h3>
                <p>Authors: Yuchi LiuLei WangYuli ZouJames ZouLiang Zheng</p>
                <p><a href="http://arxiv.org/abs/2404.13016v1">Link to paper</a></p>
                <p>Model calibration aims to align confidence with prediction correctness. TheCross-Entropy CE loss is widely used for calibrator training which enforcesthe model to increase confidence on the ground truth class. However we findthe CE loss has intrinsic limitations. For example for a narrowmisclassification a calibrator trained by the CE loss often produces highconfidence on the wrongly predicted class e.g. a test sample is wronglyclassified and its softmax score on the ground truth class is around 0.4which is undesirable. In this paper we propose a new post-hoc calibrationobjective derived from the aim of calibration. Intuitively the proposedobjective function asks that the calibrator decrease model confidence onwrongly predicted samples and increase confidence on correctly predictedsamples. Because a sample itself has insufficient ability to indicatecorrectness we use its transformed versions e.g. rotated greyscaled andcolor-jittered during calibrator training. Trained on an in-distributionvalidation set and tested with isolated individual test samples our methodachieves competitive calibration performance on both in-distribution andout-of-distribution test sets compared with the state of the art. Further ouranalysis points out the difference between our method and commonly usedobjectives such as CE loss and mean square error loss where the latterssometimes deviates from the calibration aim.</p>
                <p>Last Updated: 2024-04-19 17:25:43 UTC</p>
                <button class="interpret-button" data-id="2404.13016v1">Interpret</button>
                <div id="interpretation-2404.13016v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Neural Flow Diffusion Models: Learnable Forward Process for Improved Diffusion Modelling</h3>
                <p>Authors: Grigory BartoshDmitry VetrovChristian A. Naesseth</p>
                <p><a href="http://arxiv.org/abs/2404.12940v1">Link to paper</a></p>
                <p>Conventional diffusion models typically relies on a fixed forward processwhich implicitly defines complex marginal distributions over latent variables.This can often complicate the reverse process task in learning generativetrajectories and results in costly inference for diffusion models. To addressthese limitations we introduce Neural Flow Diffusion Models NFDM a novelframework that enhances diffusion models by supporting a broader range offorward processes beyond the fixed linear Gaussian. We also propose a novelparameterization technique for learning the forward process. Our frameworkprovides an end-to-end simulation-free optimization objective effectivelyminimizing a variational upper bound on the negative log-likelihood.Experimental results demonstrate NFDMs strong performance evidenced bystate-of-the-art likelihood estimation. Furthermore we investigate NFDMscapacity for learning generative dynamics with specific characteristics suchas deterministic straight lines trajectories. This exploration underscoresNFDMs versatility and its potential for a wide range of applications.</p>
                <p>Last Updated: 2024-04-19 15:10:54 UTC</p>
                <button class="interpret-button" data-id="2404.12940v1">Interpret</button>
                <div id="interpretation-2404.12940v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Probabilistic-Numeric SMC Sampling for Bayesian Nonlinear System Identification in Continuous Time</h3>
                <p>Authors: Joe D. LongbottomMax D. ChampneysTimothy J. Rogers</p>
                <p><a href="http://arxiv.org/abs/2404.12923v1">Link to paper</a></p>
                <p>In engineering accurately modeling nonlinear dynamic systems from datacontaminated by noise is both essential and complex. Established SequentialMonte Carlo SMC methods used for the Bayesian identification of thesesystems facilitate the quantification of uncertainty in the parameteridentification process. A significant challenge in this context is thenumerical integration of continuous-time ordinary differential equationsODEs crucial for aligning theoretical models with discretely sampled data.This integration introduces additional numerical uncertainty a factor that isoften over looked. To address this issue the field of probabilistic numericscombines numerical methods such as numerical integration with probabilisticmodeling to offer a more comprehensive analysis of total uncertainty. Byretaining the accuracy of classical deterministic methods these probabilisticapproaches offer a deeper understanding of the uncertainty inherent in theinference process. This paper demonstrates the application of a probabilisticnumerical method for solving ODEs in the joint parameter-state identificationof nonlinear dynamic systems. The presented approach efficiently identifieslatent states and system parameters from noisy measurements. Simultaneouslyincorporating probabilistic solutions to the ODE in the identificationchallenge. The methodologys primary advantage lies in its capability toproduce posterior distributions over system parameters thereby representingthe inherent uncertainties in both the data and the identification process.</p>
                <p>Last Updated: 2024-04-19 14:52:14 UTC</p>
                <button class="interpret-button" data-id="2404.12923v1">Interpret</button>
                <div id="interpretation-2404.12923v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Guide to Feature Importance Methods for Scientific Inference</h3>
                <p>Authors: Fiona Katharina EwaldLudwig BothmannMarvin N. WrightBernd BischlGiuseppe CasalicchioGunnar KÃ¶nig</p>
                <p><a href="http://arxiv.org/abs/2404.12862v1">Link to paper</a></p>
                <p>While machine learning ML models are increasingly used due to their highpredictive power their use in understanding the data-generating process DGPis limited. Understanding the DGP requires insights into feature-targetassociations which many ML models cannot directly provide due to their opaqueinternal mechanisms. Feature importance FI methods provide useful insightsinto the DGP under certain conditions. Since the results of different FImethods have different interpretations selecting the correct FI method for aconcrete use case is crucial and still requires expert knowledge. This paperserves as a comprehensive guide to help understand the differentinterpretations of FI methods. Through an extensive review of FI methods andproviding new proofs regarding their interpretation we facilitate a thoroughunderstanding of these methods and formulate concrete recommendations forscientific inference. We conclude by discussing options for FI uncertaintyestimation and point to directions for future research aiming at fullstatistical inference from black-box ML models.</p>
                <p>Last Updated: 2024-04-19 13:01:59 UTC</p>
                <button class="interpret-button" data-id="2404.12862v1">Interpret</button>
                <div id="interpretation-2404.12862v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation</h3>
                <p>Authors: Jianliang HeHan ZhongZhuoran Yang</p>
                <p><a href="http://arxiv.org/abs/2404.12648v1">Link to paper</a></p>
                <p>We study infinite-horizon average-reward Markov decision processes AMDPs inthe context of general function approximation. Specifically we propose a novelalgorithmic framework named Local-fitted Optimization with OPtimism LOOPwhich incorporates both model-based and value-based incarnations. Inparticular LOOP features a novel construction of confidence sets and alow-switching policy updating scheme which are tailored to the average-rewardand function approximation setting. Moreover for AMDPs we propose a novelcomplexity measure -- average-reward generalized eluder coefficient AGEC --which captures the challenge of exploration in AMDPs with general functionapproximation. Such a complexity measure encompasses almost all previouslyknown tractable AMDP models such as linear AMDPs and linear mixture AMDPs andalso includes newly identified cases such as kernel AMDPs and AMDPs withBellman eluder dimensions. Using AGEC we prove that LOOP achieves a sublineartildemathcalOmathrmpolyd mathrmspV sqrtTbeta regret where d and beta correspond to AGEC and log-covering number of thehypothesis class respectively mathrmspV is the span of the optimalstate bias function T denotes the number of steps and tildemathcalOcdot  omits logarithmic factors. When specialized to concrete AMDP modelsour regret bounds are comparable to those established by the existingalgorithms designed specifically for these special cases. To the best of ourknowledge this paper presents the first comprehensive theoretical frameworkcapable of handling nearly all AMDPs.</p>
                <p>Last Updated: 2024-04-19 06:24:22 UTC</p>
                <button class="interpret-button" data-id="2404.12648v1">Interpret</button>
                <div id="interpretation-2404.12648v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Data Alignment for Zero-Shot Concept Generation in Dermatology AI</h3>
                <p>Authors: Soham GadgilMahtab Bigverdi</p>
                <p><a href="http://arxiv.org/abs/2404.13043v1">Link to paper</a></p>
                <p>AI in dermatology is evolving at a rapid pace but the major limitation totraining trustworthy classifiers is the scarcity of data with ground-truthconcept level labels which are meta-labels semantically meaningful to humans.Foundation models like CLIP providing zero-shot capabilities can help alleviatethis challenge by leveraging vast amounts of image-caption pairs available onthe internet. CLIP can be fine-tuned using domain specific image-caption pairsto improve classification performance. However CLIPs pre-training data is notwell-aligned with the medical jargon that clinicians use to perform diagnoses.The development of large language models LLMs in recent years has led to thepossibility of leveraging the expressive nature of these models to generaterich text. Our goal is to use these models to generate caption text that alignswell with both the clinical lexicon and with the natural human language used inCLIPs pre-training data. Starting with captions used for images in PubMedarticles we extend them by passing the raw captions through an LLM fine-tunedon the fields several textbooks. We find that using captions generated by anexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot conceptclassification performance.</p>
                <p>Last Updated: 2024-04-19 17:57:29 UTC</p>
                <button class="interpret-button" data-id="2404.13043v1">Interpret</button>
                <div id="interpretation-2404.13043v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LaPA: Latent Prompt Assist Model For Medical Visual Question Answering</h3>
                <p>Authors: Tiancheng GuKaicheng YangDongnan LiuWeidong Cai</p>
                <p><a href="http://arxiv.org/abs/2404.13039v1">Link to paper</a></p>
                <p>Medical visual question answering Med-VQA aims to automate the predictionof correct answers for medical images and questions thereby assistingphysicians in reducing repetitive tasks and alleviating their workload.Existing approaches primarily focus on pre-training models using additional andcomprehensive datasets followed by fine-tuning to enhance performance indownstream tasks. However there is also significant value in exploringexisting models to extract clinically relevant information. In this paper wepropose the Latent Prompt Assist model LaPA for medical visual questionanswering. Firstly we design a latent prompt generation module to generate thelatent prompt with the constraint of the target answer. Subsequently wepropose a multi-modal fusion block with latent prompt fusion module thatutilizes the latent prompt to extract clinical-relevant information fromuni-modal and multi-modal features. Additionally we introduce a priorknowledge fusion module to integrate the relationship between diseases andorgans with the clinical-relevant information. Finally we combine the finalintegrated information with image-language cross-modal information to predictthe final answers. Experimental results on three publicly available Med-VQAdatasets demonstrate that LaPA outperforms the state-of-the-art model ARLachieving improvements of 1.83 0.63 and 1.80 on VQA-RAD SLAKE andVQA-2019 respectively. The code is publicly available athttps://github.com/GaryGuTC/LaPA_model.</p>
                <p>Last Updated: 2024-04-19 17:51:52 UTC</p>
                <button class="interpret-button" data-id="2404.13039v1">Interpret</button>
                <div id="interpretation-2404.13039v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs</h3>
                <p>Authors: Biyang GuoHe WangWenyilin XiaoHong ChenZhuxin LeeSongqiao HanHailiang Huang</p>
                <p><a href="http://arxiv.org/abs/2404.13033v1">Link to paper</a></p>
                <p>In the burgeoning field of Large Language Models LLMs like ChatGPT andLLaMA Prompt Engineering PE is renowned for boosting zero-shot or in-contextlearning ICL through prompt modifications. Yet the realm of the sampledesign for downstream fine-tuning crucial for task-specific LLM adaptation islargely unexplored. This paper introduces Sample Design Engineering SDE amethodical approach to enhancing LLMs post-tuning performance by refininginput output and reasoning designs. We conduct a series of in-domain ID andout-of-domain OOD experiments to assess the impact of various design optionson LLMs downstream performance revealing several intriguing patterns thathold consistently across different LLMs. Based on these insights we propose anintegrated SDE strategy combining the most effective options and validate itsconsistent superiority over heuristic sample designs in complex downstreamtasks like multi-aspect sentiment analysis event extraction and nested entityrecognition. Additionally analyses of LLMs inherent prompt/output perplexityzero-shot and ICL abilities illustrate that good PE strategies may not alwaystranslate to good SDE strategies. Code available athttps://github.com/beyondguo/LLM-Tuning.</p>
                <p>Last Updated: 2024-04-19 17:47:02 UTC</p>
                <button class="interpret-button" data-id="2404.13033v1">Interpret</button>
                <div id="interpretation-2404.13033v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Stronger Random Baselines for In-Context Learning</h3>
                <p>Authors: Gregory YauneyDavid Mimno</p>
                <p><a href="http://arxiv.org/abs/2404.13020v1">Link to paper</a></p>
                <p>Evaluating the in-context learning classification performance of languagemodels poses challenges due to small dataset sizes extensive prompt-selectionusing the validation set and intentionally difficult tasks that lead tonear-random performance. The standard random baseline -- the expected accuracyof guessing labels uniformly at random -- is stable when the evaluation set isused only once or when the dataset is large. We account for the common practiceof validation set reuse and existing small datasets with a stronger randombaseline: the expected maximum accuracy across multiple random classifiers.When choosing the best prompt demonstrations across six quantized languagemodels applied to 16 BIG-bench Lite tasks more than 20 of the few-shotresults that exceed the standard baseline do not exceed this stronger randombaseline. When held-out test sets are available this stronger baseline is alsoa better predictor of held-out performance than the standard baseline avoidingunnecessary test set evaluations. This maximum random baseline provides aneasily calculated drop-in replacement for the standard baseline.</p>
                <p>Last Updated: 2024-04-19 17:30:10 UTC</p>
                <button class="interpret-button" data-id="2404.13020v1">Interpret</button>
                <div id="interpretation-2404.13020v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models</h3>
                <p>Authors: Chuofan MaYi JiangJiannan WuZehuan YuanXiaojuan Qi</p>
                <p><a href="http://arxiv.org/abs/2404.13013v1">Link to paper</a></p>
                <p>We introduce Groma a Multimodal Large Language Model MLLM with groundedand fine-grained visual perception ability. Beyond holistic imageunderstanding Groma is adept at region-level tasks such as region captioningand visual grounding. Such capabilities are built upon a localized visualtokenization mechanism where an image input is decomposed into regions ofinterest and subsequently encoded into region tokens. By integrating regiontokens into user instructions and model responses we seamlessly enable Gromato understand user-specified region inputs and ground its textual output toimages. Besides to enhance the grounded chat ability of Groma we curate avisually grounded instruction dataset by leveraging the powerful GPT-4V andvisual prompting techniques. Compared with MLLMs that rely on the languagemodel or external module for localization Groma consistently demonstratessuperior performances in standard referring and grounding benchmarkshighlighting the advantages of embedding localization into image tokenization.Project page: https://groma-mllm.github.io/.</p>
                <p>Last Updated: 2024-04-19 17:22:51 UTC</p>
                <button class="interpret-button" data-id="2404.13013v1">Interpret</button>
                <div id="interpretation-2404.13013v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>MoVA: Adapting Mixture of Vision Experts to Multimodal Context</h3>
                <p>Authors: Zhuofan ZongBingqi MaDazhong ShenGuanglu SongHao ShaoDongzhi JiangHongsheng LiYu Liu</p>
                <p><a href="http://arxiv.org/abs/2404.13046v1">Link to paper</a></p>
                <p>As the key component in multimodal large language models MLLMs the abilityof the visual encoder greatly affects MLLMs understanding on diverse imagecontent. Although some large-scale pretrained vision encoders such as visionencoders in CLIP and DINOv2 have brought promising performance we found thatthere is still no single vision encoder that can dominate various image contentunderstanding e.g. the CLIP vision encoder leads to outstanding results ongeneral image understanding but poor performance on document or chart content.To alleviate the bias of CLIP vision encoder we first delve into the inherentbehavior of different pre-trained vision encoders and then propose the MoVA apowerful and novel MLLM adaptively routing and fusing task-specific visionexperts with a coarse-to-fine mechanism. In the coarse-grained stage we designa context-aware expert routing strategy to dynamically select the most suitablevision experts according to the user instruction input image and expertise ofvision experts. This benefits from the powerful model function understandingability of the large language model LLM equipped with expert-routing low-rankadaptation LoRA. In the fine-grained stage we elaborately conduct themixture-of-vision-expert adapter MoV-Adapter to extract and fusetask-specific knowledge from various experts. This coarse-to-fine paradigmeffectively leverages representations from experts based on multimodal contextand model expertise further enhancing the generalization ability. We conductextensive experiments to evaluate the effectiveness of the proposed approach.Without any bells and whistles MoVA can achieve significant performance gainsover current state-of-the-art methods in a wide range of challenging multimodalbenchmarks. Codes and models will be available athttps://github.com/TempleX98/MoVA.</p>
                <p>Last Updated: 2024-04-19 17:59:48 UTC</p>
                <button class="interpret-button" data-id="2404.13046v1">Interpret</button>
                <div id="interpretation-2404.13046v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Unified Scene Representation and Reconstruction for 3D Large Language Models</h3>
                <p>Authors: Tao ChuPan ZhangXiaoyi DongYuhang ZangQiong LiuJiaqi Wang</p>
                <p><a href="http://arxiv.org/abs/2404.13044v1">Link to paper</a></p>
                <p>Enabling Large Language Models LLMs to interact with 3D environments ischallenging. Existing approaches extract point clouds either from ground truthGT geometry or 3D scenes reconstructed by auxiliary models. Text-imagealigned 2D features from CLIP are then lifted to point clouds which serve asinputs for LLMs. However this solution lacks the establishment of 3Dpoint-to-point connections leading to a deficiency of spatial structureinformation. Concurrently the absence of integration and unification betweenthe geometric and semantic representations of the scene culminates in adiminished level of 3D scene understanding. In this paper we demonstrate theimportance of having a unified scene representation and reconstructionframework which is essential for LLMs in 3D scenes. Specifically we introduceUni3DR2 extracts 3D geometric and semantic aware representation features viathe frozen pre-trained 2D foundation models e.g. CLIP and SAM and amulti-scale aggregate 3D decoder. Our learned 3D representations not onlycontribute to the reconstruction process but also provide valuable knowledgefor LLMs. Experimental results validate that our Uni3DR2 yields convincinggains over the baseline on the 3D reconstruction dataset ScanNet increasingF-Score by 1.8. When applied to LLMs our Uni3DR2-LLM exhibits superiorperformance over the baseline on the 3D vision-language understanding datasetScanQA increasing BLEU-1 by 4.0 and 4.2 on the val set and test setrespectively. Furthermore it outperforms the state-of-the-art method thatuses additional GT point clouds on both ScanQA and 3DMV-VQA.</p>
                <p>Last Updated: 2024-04-19 17:58:04 UTC</p>
                <button class="interpret-button" data-id="2404.13044v1">Interpret</button>
                <div id="interpretation-2404.13044v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Data Alignment for Zero-Shot Concept Generation in Dermatology AI</h3>
                <p>Authors: Soham GadgilMahtab Bigverdi</p>
                <p><a href="http://arxiv.org/abs/2404.13043v1">Link to paper</a></p>
                <p>AI in dermatology is evolving at a rapid pace but the major limitation totraining trustworthy classifiers is the scarcity of data with ground-truthconcept level labels which are meta-labels semantically meaningful to humans.Foundation models like CLIP providing zero-shot capabilities can help alleviatethis challenge by leveraging vast amounts of image-caption pairs available onthe internet. CLIP can be fine-tuned using domain specific image-caption pairsto improve classification performance. However CLIPs pre-training data is notwell-aligned with the medical jargon that clinicians use to perform diagnoses.The development of large language models LLMs in recent years has led to thepossibility of leveraging the expressive nature of these models to generaterich text. Our goal is to use these models to generate caption text that alignswell with both the clinical lexicon and with the natural human language used inCLIPs pre-training data. Starting with captions used for images in PubMedarticles we extend them by passing the raw captions through an LLM fine-tunedon the fields several textbooks. We find that using captions generated by anexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot conceptclassification performance.</p>
                <p>Last Updated: 2024-04-19 17:57:29 UTC</p>
                <button class="interpret-button" data-id="2404.13043v1">Interpret</button>
                <div id="interpretation-2404.13043v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Analysis of Classifier-Free Guidance Weight Schedulers</h3>
                <p>Authors: Xi WangNicolas DufourNefeli AndreouMarie-Paule CaniVictoria Fernandez AbrevayaDavid PicardVicky Kalogeiton</p>
                <p><a href="http://arxiv.org/abs/2404.13040v1">Link to paper</a></p>
                <p>Classifier-Free Guidance CFG enhances the quality and condition adherenceof text-to-image diffusion models. It operates by combining the conditional andunconditional predictions using a fixed weight. However recent works vary theweights throughout the diffusion process reporting superior results butwithout providing any rationale or analysis. By conducting comprehensiveexperiments this paper provides insights into CFG weight schedulers. Ourfindings suggest that simple monotonically increasing weight schedulersconsistently lead to improved performances requiring merely a single line ofcode. In addition more complex parametrized schedulers can be optimized forfurther improvement but do not generalize across different models and tasks.</p>
                <p>Last Updated: 2024-04-19 17:53:43 UTC</p>
                <button class="interpret-button" data-id="2404.13040v1">Interpret</button>
                <div id="interpretation-2404.13040v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LaPA: Latent Prompt Assist Model For Medical Visual Question Answering</h3>
                <p>Authors: Tiancheng GuKaicheng YangDongnan LiuWeidong Cai</p>
                <p><a href="http://arxiv.org/abs/2404.13039v1">Link to paper</a></p>
                <p>Medical visual question answering Med-VQA aims to automate the predictionof correct answers for medical images and questions thereby assistingphysicians in reducing repetitive tasks and alleviating their workload.Existing approaches primarily focus on pre-training models using additional andcomprehensive datasets followed by fine-tuning to enhance performance indownstream tasks. However there is also significant value in exploringexisting models to extract clinically relevant information. In this paper wepropose the Latent Prompt Assist model LaPA for medical visual questionanswering. Firstly we design a latent prompt generation module to generate thelatent prompt with the constraint of the target answer. Subsequently wepropose a multi-modal fusion block with latent prompt fusion module thatutilizes the latent prompt to extract clinical-relevant information fromuni-modal and multi-modal features. Additionally we introduce a priorknowledge fusion module to integrate the relationship between diseases andorgans with the clinical-relevant information. Finally we combine the finalintegrated information with image-language cross-modal information to predictthe final answers. Experimental results on three publicly available Med-VQAdatasets demonstrate that LaPA outperforms the state-of-the-art model ARLachieving improvements of 1.83 0.63 and 1.80 on VQA-RAD SLAKE andVQA-2019 respectively. The code is publicly available athttps://github.com/GaryGuTC/LaPA_model.</p>
                <p>Last Updated: 2024-04-19 17:51:52 UTC</p>
                <button class="interpret-button" data-id="2404.13039v1">Interpret</button>
                <div id="interpretation-2404.13039v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Data Alignment for Zero-Shot Concept Generation in Dermatology AI</h3>
                <p>Authors: Soham GadgilMahtab Bigverdi</p>
                <p><a href="http://arxiv.org/abs/2404.13043v1">Link to paper</a></p>
                <p>AI in dermatology is evolving at a rapid pace but the major limitation totraining trustworthy classifiers is the scarcity of data with ground-truthconcept level labels which are meta-labels semantically meaningful to humans.Foundation models like CLIP providing zero-shot capabilities can help alleviatethis challenge by leveraging vast amounts of image-caption pairs available onthe internet. CLIP can be fine-tuned using domain specific image-caption pairsto improve classification performance. However CLIPs pre-training data is notwell-aligned with the medical jargon that clinicians use to perform diagnoses.The development of large language models LLMs in recent years has led to thepossibility of leveraging the expressive nature of these models to generaterich text. Our goal is to use these models to generate caption text that alignswell with both the clinical lexicon and with the natural human language used inCLIPs pre-training data. Starting with captions used for images in PubMedarticles we extend them by passing the raw captions through an LLM fine-tunedon the fields several textbooks. We find that using captions generated by anexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot conceptclassification performance.</p>
                <p>Last Updated: 2024-04-19 17:57:29 UTC</p>
                <button class="interpret-button" data-id="2404.13043v1">Interpret</button>
                <div id="interpretation-2404.13043v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Analysis of Classifier-Free Guidance Weight Schedulers</h3>
                <p>Authors: Xi WangNicolas DufourNefeli AndreouMarie-Paule CaniVictoria Fernandez AbrevayaDavid PicardVicky Kalogeiton</p>
                <p><a href="http://arxiv.org/abs/2404.13040v1">Link to paper</a></p>
                <p>Classifier-Free Guidance CFG enhances the quality and condition adherenceof text-to-image diffusion models. It operates by combining the conditional andunconditional predictions using a fixed weight. However recent works vary theweights throughout the diffusion process reporting superior results butwithout providing any rationale or analysis. By conducting comprehensiveexperiments this paper provides insights into CFG weight schedulers. Ourfindings suggest that simple monotonically increasing weight schedulersconsistently lead to improved performances requiring merely a single line ofcode. In addition more complex parametrized schedulers can be optimized forfurther improvement but do not generalize across different models and tasks.</p>
                <p>Last Updated: 2024-04-19 17:53:43 UTC</p>
                <button class="interpret-button" data-id="2404.13040v1">Interpret</button>
                <div id="interpretation-2404.13040v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Stronger Random Baselines for In-Context Learning</h3>
                <p>Authors: Gregory YauneyDavid Mimno</p>
                <p><a href="http://arxiv.org/abs/2404.13020v1">Link to paper</a></p>
                <p>Evaluating the in-context learning classification performance of languagemodels poses challenges due to small dataset sizes extensive prompt-selectionusing the validation set and intentionally difficult tasks that lead tonear-random performance. The standard random baseline -- the expected accuracyof guessing labels uniformly at random -- is stable when the evaluation set isused only once or when the dataset is large. We account for the common practiceof validation set reuse and existing small datasets with a stronger randombaseline: the expected maximum accuracy across multiple random classifiers.When choosing the best prompt demonstrations across six quantized languagemodels applied to 16 BIG-bench Lite tasks more than 20 of the few-shotresults that exceed the standard baseline do not exceed this stronger randombaseline. When held-out test sets are available this stronger baseline is alsoa better predictor of held-out performance than the standard baseline avoidingunnecessary test set evaluations. This maximum random baseline provides aneasily calculated drop-in replacement for the standard baseline.</p>
                <p>Last Updated: 2024-04-19 17:30:10 UTC</p>
                <button class="interpret-button" data-id="2404.13020v1">Interpret</button>
                <div id="interpretation-2404.13020v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Optimizing Calibration by Gaining Aware of Prediction Correctness</h3>
                <p>Authors: Yuchi LiuLei WangYuli ZouJames ZouLiang Zheng</p>
                <p><a href="http://arxiv.org/abs/2404.13016v1">Link to paper</a></p>
                <p>Model calibration aims to align confidence with prediction correctness. TheCross-Entropy CE loss is widely used for calibrator training which enforcesthe model to increase confidence on the ground truth class. However we findthe CE loss has intrinsic limitations. For example for a narrowmisclassification a calibrator trained by the CE loss often produces highconfidence on the wrongly predicted class e.g. a test sample is wronglyclassified and its softmax score on the ground truth class is around 0.4which is undesirable. In this paper we propose a new post-hoc calibrationobjective derived from the aim of calibration. Intuitively the proposedobjective function asks that the calibrator decrease model confidence onwrongly predicted samples and increase confidence on correctly predictedsamples. Because a sample itself has insufficient ability to indicatecorrectness we use its transformed versions e.g. rotated greyscaled andcolor-jittered during calibrator training. Trained on an in-distributionvalidation set and tested with isolated individual test samples our methodachieves competitive calibration performance on both in-distribution andout-of-distribution test sets compared with the state of the art. Further ouranalysis points out the difference between our method and commonly usedobjectives such as CE loss and mean square error loss where the latterssometimes deviates from the calibration aim.</p>
                <p>Last Updated: 2024-04-19 17:25:43 UTC</p>
                <button class="interpret-button" data-id="2404.13016v1">Interpret</button>
                <div id="interpretation-2404.13016v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models</h3>
                <p>Authors: Chuofan MaYi JiangJiannan WuZehuan YuanXiaojuan Qi</p>
                <p><a href="http://arxiv.org/abs/2404.13013v1">Link to paper</a></p>
                <p>We introduce Groma a Multimodal Large Language Model MLLM with groundedand fine-grained visual perception ability. Beyond holistic imageunderstanding Groma is adept at region-level tasks such as region captioningand visual grounding. Such capabilities are built upon a localized visualtokenization mechanism where an image input is decomposed into regions ofinterest and subsequently encoded into region tokens. By integrating regiontokens into user instructions and model responses we seamlessly enable Gromato understand user-specified region inputs and ground its textual output toimages. Besides to enhance the grounded chat ability of Groma we curate avisually grounded instruction dataset by leveraging the powerful GPT-4V andvisual prompting techniques. Compared with MLLMs that rely on the languagemodel or external module for localization Groma consistently demonstratessuperior performances in standard referring and grounding benchmarkshighlighting the advantages of embedding localization into image tokenization.Project page: https://groma-mllm.github.io/.</p>
                <p>Last Updated: 2024-04-19 17:22:51 UTC</p>
                <button class="interpret-button" data-id="2404.13013v1">Interpret</button>
                <div id="interpretation-2404.13013v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Private Agent-Based Modeling</h3>
                <p>Authors: Ayush ChopraArnau Quera-BofarullNurullah Giray-KuruMichael WooldridgeRamesh Raskar</p>
                <p><a href="http://arxiv.org/abs/2404.12983v1">Link to paper</a></p>
                <p>The practical utility of agent-based models in decision-making relies ontheir capacity to accurately replicate populations while seamlessly integratingreal-world data streams. Yet the incorporation of such data poses significantchallenges due to privacy concerns. To address this issue we introduce aparadigm for private agent-based modeling wherein the simulation calibrationand analysis of agent-based models can be achieved without centralizing theagents attributes or interactions. The key insight is to leverage techniquesfrom secure multi-party computation to design protocols for decentralizedcomputation in agent-based models. This ensures the confidentiality of thesimulated agents without compromising on simulation accuracy. We showcase ourprotocols on a case study with an epidemiological simulation comprising over150000 agents. We believe this is a critical step towards deployingagent-based models to real-world applications.</p>
                <p>Last Updated: 2024-04-19 16:30:40 UTC</p>
                <button class="interpret-button" data-id="2404.12983v1">Interpret</button>
                <div id="interpretation-2404.12983v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MAexp: A Generic Platform for RL-based Multi-Agent Exploration</h3>
                <p>Authors: Shaohao ZhuJiacheng ZhouAnjun ChenMingming BaiJiming ChenJinming Xu</p>
                <p><a href="http://arxiv.org/abs/2404.12824v1">Link to paper</a></p>
                <p>The sim-to-real gap poses a significant challenge in RL-based multi-agentexploration due to scene quantization and action discretization. Existingplatforms suffer from the inefficiency in sampling and the lack of diversity inMulti-Agent Reinforcement Learning MARL algorithms across differentscenarios restraining their widespread applications. To fill these gaps wepropose MAexp a generic platform for multi-agent exploration that integrates abroad range of state-of-the-art MARL algorithms and representative scenarios.Moreover we employ point clouds to represent our exploration scenariosleading to high-fidelity environment mapping and a sampling speed approximately40 times faster than existing platforms. Furthermore equipped with anattention-based Multi-Agent Target Generator and a Single-Agent Motion PlannerMAexp can work with arbitrary numbers of agents and accommodate various typesof robots. Extensive experiments are conducted to establish the first benchmarkfeaturing several high-performance MARL algorithms across typical scenarios forrobots with continuous actions which highlights the distinct strengths of eachalgorithm in different scenarios.</p>
                <p>Last Updated: 2024-04-19 12:00:10 UTC</p>
                <button class="interpret-button" data-id="2404.12824v1">Interpret</button>
                <div id="interpretation-2404.12824v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Grasper: A Generalist Pursuer for Pursuit-Evasion Problems</h3>
                <p>Authors: Pengdeng LiShuxin LiXinrun WangJakub CernyYouzhi ZhangStephen McAleerHau ChanBo An</p>
                <p><a href="http://arxiv.org/abs/2404.12626v1">Link to paper</a></p>
                <p>Pursuit-evasion games PEGs model interactions between a team of pursuersand an evader in graph-based environments such as urban street networks. Recentadvancements have demonstrated the effectiveness of the pre-training andfine-tuning paradigm in PSRO to improve scalability in solving large-scalePEGs. However these methods primarily focus on specific PEGs with fixedinitial conditions that may vary substantially in real-world scenarios whichsignificantly hinders the applicability of the traditional methods. To addressthis issue we introduce Grasper a GeneRAlist purSuer for Pursuit-EvasionpRoblems capable of efficiently generating pursuer policies tailored tospecific PEGs. Our contributions are threefold: First we present a novelarchitecture that offers high-quality solutions for diverse PEGs comprisingcritical components such as i a graph neural network GNN to encode PEGsinto hidden vectors and ii a hypernetwork to generate pursuer policies basedon these hidden vectors. As a second contribution we develop an efficientthree-stage training method involving i a pre-pretraining stage for learningrobust PEG representations through self-supervised graph learning techniqueslike GraphMAE ii a pre-training stage utilizing heuristic-guided multi-taskpre-training HMP where heuristic-derived reference policies e.g. throughDijkstras algorithm regularize pursuer policies and iii a fine-tuningstage that employs PSRO to generate pursuer policies on designated PEGs.Finally we perform extensive experiments on synthetic and real-world mapsshowcasing Graspers significant superiority over baselines in terms ofsolution quality and generalizability. We demonstrate that Grasper provides aversatile approach for solving pursuit-evasion problems across a broad range ofscenarios enabling practical deployment in real-world situations.</p>
                <p>Last Updated: 2024-04-19 04:54:38 UTC</p>
                <button class="interpret-button" data-id="2404.12626v1">Interpret</button>
                <div id="interpretation-2404.12626v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Stackelberg Game-Theoretic Learning for Collaborative Assembly Task Planning</h3>
                <p>Authors: Yuhan ZhaoLan ShiQuanyan Zhu</p>
                <p><a href="http://arxiv.org/abs/2404.12570v1">Link to paper</a></p>
                <p>As assembly tasks grow in complexity collaboration among multiple robotsbecomes essential for task completion. However centralized task planning hasbecome inadequate for adapting to the increasing intelligence and versatilityof robots along with rising customized orders. There is a need for efficientand automated planning mechanisms capable of coordinating diverse robots forcollaborative assembly. To this end we propose a Stackelberg game-theoreticlearning approach. By leveraging Stackelberg games we characterize robotcollaboration through leader-follower interaction to enhance strategy seekingand ensure task completion. To enhance applicability across tasks we introducea novel multi-agent learning algorithm: Stackelberg double deep Q-learningwhich facilitates automated assembly strategy seeking and multi-robotcoordination. Our approach is validated through simulated assembly tasks.Comparison with three alternative multi-agent learning methods shows that ourapproach achieves the shortest task completion time for tasks. Furthermore ourapproach exhibits robustness against both accidental and deliberateenvironmental perturbations.</p>
                <p>Last Updated: 2024-04-19 01:37:23 UTC</p>
                <button class="interpret-button" data-id="2404.12570v1">Interpret</button>
                <div id="interpretation-2404.12570v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning a Stable, Safe, Distributed Feedback Controller for a Heterogeneous Platoon of Vehicles</h3>
                <p>Authors: Michael H. ShahamTaskin Padir</p>
                <p><a href="http://arxiv.org/abs/2404.12474v1">Link to paper</a></p>
                <p>Platooning of autonomous vehicles has the potential to increase safety andfuel efficiency on highways. The goal of platooning is to have each vehicledrive at some speed set by the leader while maintaining a safe distance fromits neighbors. Many prior works have analyzed various controllers forplatooning most commonly linear feedback and distributed model predictivecontrollers. In this work we introduce an algorithm for learning a stablesafe distributed controller for a heterogeneous platoon. Our algorithm relieson recent developments in learning neural network stability and safetycertificates. We train a controller for autonomous platooning in simulation andevaluate its performance on hardware with a platoon of four F1Tenth vehicles.We then perform further analysis in simulation with a platoon of 100 vehicles.Experimental results demonstrate the practicality of the algorithm and thelearned controller by comparing the performance of the neural networkcontroller to linear feedback and distributed model predictive controllers.</p>
                <p>Last Updated: 2024-04-18 19:11:34 UTC</p>
                <button class="interpret-button" data-id="2404.12474v1">Interpret</button>
                <div id="interpretation-2404.12474v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Eye-tracking in Mixed Reality for Diagnosis of Neurodegenerative Diseases</h3>
                <p>Authors: Mateusz DaniolDaria HemmerlingJakub SikoraPawel JemioloMarek WodzinskiMagdalena Wojcik-Pedziwiatr</p>
                <p><a href="http://arxiv.org/abs/2404.12984v1">Link to paper</a></p>
                <p>Parkinsons disease ranks as the second most prevalent neurodegenerativedisorder globally. This research aims to develop a system leveraging MixedReality capabilities for tracking and assessing eye movements. In this paperwe present a medical scenario and outline the development of an applicationdesigned to capture eye-tracking signals through Mixed Reality technology forthe evaluation of neurodegenerative diseases. Additionally we introduce apipeline for extracting clinically relevant features from eye-gaze analysisdescribing the capabilities of the proposed system from a medical perspective.The study involved a cohort of healthy control individuals and patientssuffering from Parkinsons disease showcasing the feasibility and potential ofthe proposed technology for non-intrusive monitoring of eye movement patternsfor the diagnosis of neurodegenerative diseases.  Clinical relevance - Developing a non-invasive biomarker for Parkinsonsdisease is urgently needed to accurately detect the diseases onset. This wouldallow for the timely introduction of neuroprotective treatment at the earlieststage and enable the continuous monitoring of intervention outcomes. Theability to detect subtle changes in eye movements allows for early diagnosisoffering a critical window for intervention before more pronounced symptomsemerge. Eye tracking provides objective and quantifiable biomarkers ensuringreliable assessments of disease progression and cognitive function. The eyegaze analysis using Mixed Reality glasses is wireless facilitating convenientassessments in both home and hospital settings. The approach offers theadvantage of utilizing hardware that requires no additional specializedattachments enabling examinations through personal eyewear.</p>
                <p>Last Updated: 2024-04-19 16:34:15 UTC</p>
                <button class="interpret-button" data-id="2404.12984v1">Interpret</button>
                <div id="interpretation-2404.12984v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Ring-a-Pose: A Ring for Continuous Hand Pose Tracking</h3>
                <p>Authors: Tianhong Catherine YuGuilin HuRuidong ZhangHyunchul LimSaif MahmudChi-Jung LeeKe LiDevansh AgarwalShuyang NieJinseok OhFranÃ§ois GuimbretiÃ¨reCheng Zhang</p>
                <p><a href="http://arxiv.org/abs/2404.12980v1">Link to paper</a></p>
                <p>We present Ring-a-Pose a single untethered ring that tracks continuous 3Dhand poses. Located in the center of the hand the ring emits an inaudibleacoustic signal that each hand pose reflects differently. Ring-a-Pose imposesminimal obtrusions on the hand unlike multi-ring or glove systems. It is notaffected by the choice of clothing that may cover wrist-worn systems. In aseries of three user studies with a total of 30 participants we evaluateRing-a-Poses performance on pose tracking and micro-finger gesturerecognition. Without collecting any training data from a user Ring-a-Posetracks continuous hand poses with a joint error of 14.1mm. The joint errordecreases to 10.3mm for fine-tuned user-dependent models. Ring-a-Poserecognizes 7-class micro-gestures with a 90.60 and 99.27 accuracy foruser-independent and user-dependent models respectively. Furthermore the ringexhibits promising performance when worn on any finger. Ring-a-Pose enables thefuture of smart rings to track and recognize hand poses using relativelylow-power acoustic sensing.</p>
                <p>Last Updated: 2024-04-19 16:18:16 UTC</p>
                <button class="interpret-button" data-id="2404.12980v1">Interpret</button>
                <div id="interpretation-2404.12980v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>What We Augment When We Augment Visualizations: A Design Elicitation Study of How We Visually Express Data Relationships</h3>
                <p>Authors: Grace GuoJohn StaskoAlex Endert</p>
                <p><a href="http://dx.doi.org/10.1145/3656650.3656666">Link to paper</a></p>
                <p>Visual augmentations are commonly added to charts and graphs in order toconvey richer and more nuanced information about relationships in the data.However many design spaces proposed for categorizing augmentations weredefined in a top-down manner based on expert heuristics or from surveys ofpublished visualizations. Less well understood are user preferences andintuitions when designing augmentations. In this paper we address the gap byconducting a design elicitation study where study participants were asked todraw the different ways they would visually express the meaning of tendifferent prompts. We obtained 364 drawings from the study and identified theemergent categories of augmentations used by participants. The contributions ofthis paper are: i a user-defined design space of visualization augmentationsii a repository of hand drawn augmentations made by study participants andiii a discussion of insights into participant considerations and connectionsbetween our study and existing design guidelines.</p>
                <p>Last Updated: 2024-04-19 15:33:59 UTC</p>
                <button class="interpret-button" data-id="2404.12952v1">Interpret</button>
                <div id="interpretation-2404.12952v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Visualizing Intelligent Tutor Interactions for Responsive Pedagogy</h3>
                <p>Authors: Grace GuoAishwarya Mudgal Sunil KumarAdit GuptaAdam CosciaChris MacLellanAlex Endert</p>
                <p><a href="http://dx.doi.org/10.1145/3656650.3656667">Link to paper</a></p>
                <p>Intelligent tutoring systems leverage AI models of expert learning andstudent knowledge to deliver personalized tutoring to students. While theseintelligent tutors have demonstrated improved student learning outcomes it isstill unclear how teachers might integrate them into curriculum and courseplanning to support responsive pedagogy. In this paper we conducted a designstudy with five teachers who have deployed Apprentice Tutors an intelligenttutoring platform in their classes. We characterized their challenges aroundanalyzing student interaction data from intelligent tutoring systems and builtVisTA Visualizations for Tutor Analytics a visual analytics system thatshows detailed provenance data across multiple coordinated views. We evaluatedVisTA with the same five teachers and found that the visualizations helpedthem better interpret intelligent tutor data gain insights into studentproblem-solving provenance and decide on necessary follow-up actions - such asproviding students with further support or reviewing skills in the classroom.Finally we discuss potential extensions of VisTA into sequence query anddetection as well as the potential for the visualizations to be useful forencouraging self-directed learning in students.</p>
                <p>Last Updated: 2024-04-19 15:21:26 UTC</p>
                <button class="interpret-button" data-id="2404.12944v1">Interpret</button>
                <div id="interpretation-2404.12944v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TimelinePTC: Development of a unified interface for pathways to care collection, visualization, and collaboration in first episode psychosis</h3>
                <p>Authors: Walter S. MathisMaria FerraraJohn CahillSneha KarmaniSÃ¼meyra N. TayfurVinod Srihari</p>
                <p><a href="http://arxiv.org/abs/2404.12883v1">Link to paper</a></p>
                <p>This paper presents TimelinePTC a web-based tool developed to improve thecollection and analysis of Pathways to Care PTC data in first episodepsychosis FEP research. Accurately measuring the duration of untreatedpsychosis DUP is essential for effective FEP treatment requiring detailedunderstanding of the patients journey to care. However traditional PTC datacollection methods mainly manual and paper-based are time-consuming and oftenfail to capture the full complexity of care pathways.  TimelinePTC addresses these limitations by providing a digital platform forcollaborative real-time data entry and visualization thereby enhancing dataaccuracy and collection efficiency. Initially created for the SpecializedTreatment Early in Psychosis STEP program in New Haven Connecticut itsdesign allows for straightforward adaptation to other healthcare contextsfacilitated by its open-source codebase.  The tool significantly simplifies the data collection process making it moreefficient and user-friendly. It automates the conversion of collected data intoa format ready for analysis reducing manual transcription errors and savingtime. By enabling more detailed and consistent data collection TimelinePTC hasthe potential to improve healthcare access research supporting the developmentof targeted interventions to reduce DUP and improve patient outcomes.</p>
                <p>Last Updated: 2024-04-19 13:33:30 UTC</p>
                <button class="interpret-button" data-id="2404.12883v1">Interpret</button>
                <div id="interpretation-2404.12883v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-04-22</p>
        </div>
    
        </div>
    </body>
    </html>
    