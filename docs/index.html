
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</h3>
                <p>Authors: Zhuoshi PanQianhui WuHuiqiang JiangMenglin XiaXufang LuoJue ZhangQingwei LinVictor RühleYuqing YangChin-Yew LinH. Vicky ZhaoLili QiuDongmei Zhang</p>
                <p><a href="http://arxiv.org/abs/2403.12968v1">Link to paper</a></p>
                <p>This paper focuses on task-agnostic prompt compression for bettergeneralizability and efficiency. Considering the redundancy in naturallanguage existing approaches compress prompts by removing tokens or lexicalunits according to their information entropy obtained from a causal languagemodel such as LLaMa-7B. The challenge is that information entropy may be asuboptimal compression metric: i it only leverages unidirectional context andmay fail to capture all essential information needed for prompt compressionii it is not aligned with the prompt compression objective.  To address these issues we propose a data distillation procedure to deriveknowledge from an LLM to compress prompts without losing crucial informationand meantime introduce an extractive text compression dataset. We formulateprompt compression as a token classification problem to guarantee thefaithfulness of the compressed prompt to the original one and use aTransformer encoder as the base architecture to capture all essentialinformation for prompt compression from the full bidirectional context. Ourapproach leads to lower latency by explicitly learning the compressionobjective with smaller models such as XLM-RoBERTa-large and mBERT.  We evaluate our method on both in-domain and out-of-domain datasetsincluding MeetingBank LongBench ZeroScrolls GSM8K and BBH. Despite itssmall size our model shows significant performance gains over strong baselinesand demonstrates robust generalization ability across different LLMs.Additionally our model is 3x-6x faster than existing prompt compressionmethods while accelerating the end-to-end latency by 1.6x-2.9x withcompression ratios of 2x-5x.</p>
                <p>Last Updated: 2024-03-19 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2403.12968v1">Interpret</button>
                <div id="interpretation-2403.12968v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models</h3>
                <p>Authors: Ce ZhangSimon StepputtisKatia SycaraYaqi Xie</p>
                <p><a href="http://arxiv.org/abs/2403.12964v1">Link to paper</a></p>
                <p>Recently large-scale pre-trained Vision-Language Models VLMs havedemonstrated great potential in learning open-world visual representations andexhibit remarkable performance across a wide range of downstream tasks throughefficient fine-tuning. In this work we innovatively introduce the concept ofdual learning into fine-tuning VLMs i.e. we not only learn what an image isbut also what an image isnt. Building on this concept we introduce a novelDualAdapter approach to enable dual-path adaptation of VLMs from both positiveand negative perspectives with only limited annotated samples. In the inferencestage our DualAdapter performs unified predictions by simultaneouslyconducting complementary positive selection and negative exclusion acrosstarget classes thereby enhancing the overall recognition accuracy of VLMs indownstream tasks. Our extensive experimental results across 15 datasetsvalidate that the proposed DualAdapter outperforms existing state-of-the-artmethods on both few-shot learning and domain generalization tasks whileachieving competitive computational efficiency. Code is available athttps://github.com/zhangce01/DualAdapter.</p>
                <p>Last Updated: 2024-03-19 17:59:39 UTC</p>
                <button class="interpret-button" data-id="2403.12964v1">Interpret</button>
                <div id="interpretation-2403.12964v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Dated Data: Tracing Knowledge Cutoffs in Large Language Models</h3>
                <p>Authors: Jeffrey ChengMarc MaroneOrion WellerDawn LawrieDaniel KhashabiBenjamin Van Durme</p>
                <p><a href="http://arxiv.org/abs/2403.12958v1">Link to paper</a></p>
                <p>Released Large Language Models LLMs are often paired with a claimedknowledge cutoff date or the dates at which training data was gathered. Suchinformation is crucial for applications where the LLM must provide up to dateinformation. However this statement only scratches the surface: do allresources in the training data share the same knowledge cutoff date Does themodels demonstrated knowledge for these subsets closely align to their cutoffdates In this work we define the notion of an effective cutoff. This isdistinct from the LLM designer reported cutoff and applies separately tosub-resources and topics. We propose a simple approach to estimate effectivecutoffs on the resource-level temporal alignment of an LLM by probing acrossversions of the data. Using this analysis we find that effective cutoffs oftendiffer from reported cutoffs. To understand the root cause of this observationwe conduct a direct large-scale analysis on open pre-training datasets. Ouranalysis reveals two reasons for these inconsistencies: 1 temporal biases ofCommonCrawl data due to non-trivial amounts of old data in new dumps and 2complications in LLM deduplication schemes involving semantic duplicates andlexical near-duplicates. Overall our results show that knowledge cutoffs arenot as simple as they have seemed and that care must be taken both by LLMdataset curators as well as practitioners who seek to use information fromthese models.</p>
                <p>Last Updated: 2024-03-19 17:57:58 UTC</p>
                <button class="interpret-button" data-id="2403.12958v1">Interpret</button>
                <div id="interpretation-2403.12958v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models</h3>
                <p>Authors: Joana Ribeiro de FariaHuiyuan XieFelix Steffek</p>
                <p><a href="http://arxiv.org/abs/2403.12936v1">Link to paper</a></p>
                <p>Court transcripts and judgments are rich repositories of legal knowledgedetailing the intricacies of cases and the rationale behind judicial decisions.The extraction of key information from these documents provides a conciseoverview of a case crucial for both legal experts and the public. With theadvent of large language models LLMs automatic information extraction hasbecome increasingly feasible and efficient. This paper presents a comprehensivestudy on the application of GPT-4 a large language model for automaticinformation extraction from UK Employment Tribunal UKET cases. Wemeticulously evaluated GPT-4s performance in extracting critical informationwith a manual verification process to ensure the accuracy and relevance of theextracted data. Our research is structured around two primary extraction tasks:the first involves a general extraction of eight key aspects that holdsignificance for both legal specialists and the general public including thefacts of the case the claims made references to legal statutes references toprecedents general case outcomes and corresponding labels detailed order andremedies and reasons for the decision. The second task is more focused aimedat analysing three of those extracted features namely facts claims andoutcomes in order to facilitate the development of a tool capable ofpredicting the outcome of employment law disputes. Through our analysis wedemonstrate that LLMs like GPT-4 can obtain high accuracy in legal informationextraction highlighting the potential of LLMs in revolutionising the way legalinformation is processed and utilised offering significant implications forlegal research and practice.</p>
                <p>Last Updated: 2024-03-19 17:43:08 UTC</p>
                <button class="interpret-button" data-id="2403.12936v1">Interpret</button>
                <div id="interpretation-2403.12936v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Supporting Energy Policy Research with Large Language Models</h3>
                <p>Authors: Grant BusterPavlo PinchukJacob BarronsRyan McKeeverAaron LevineAnthony Lopez</p>
                <p><a href="http://arxiv.org/abs/2403.12924v1">Link to paper</a></p>
                <p>The recent growth in renewable energy development in the United States hasbeen accompanied by a simultaneous surge in renewable energy siting ordinances.These zoning laws play a critical role in dictating the placement of wind andsolar resources that are critical for achieving low-carbon energy futures. Inthis context efficient access to and management of siting ordinance databecomes imperative. The National Renewable Energy Laboratory NREL recentlyintroduced a public wind and solar siting database to fill this need. Thispaper presents a method for harnessing Large Language Models LLMs to automatethe extraction of these siting ordinances from legal documents enabling thisdatabase to maintain accurate up-to-date information in the rapidly changingenergy policy landscape. A novel contribution of this research is theintegration of a decision tree framework with LLMs. Our results show that thisapproach is 85 to 90 accurate with outputs that can be used directly indownstream quantitative modeling. We discuss opportunities to use this work tosupport similar large-scale policy research in the energy sector. By unlockingnew efficiencies in the extraction and analysis of legal documents using LLMsthis study enables a path forward for automated large-scale energy policyresearch.</p>
                <p>Last Updated: 2024-03-19 17:28:51 UTC</p>
                <button class="interpret-button" data-id="2403.12924v1">Interpret</button>
                <div id="interpretation-2403.12924v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Rapid AIdeation: Generating Ideas With the Self and in Collaboration With Large Language Models</h3>
                <p>Authors: Gionnieve LimSimon T. Perrault</p>
                <p><a href="http://arxiv.org/abs/2403.12928v1">Link to paper</a></p>
                <p>Generative artificial intelligence GenAI can rapidly produce large anddiverse volumes of content. This lends to it a quality of creativity which canbe empowering in the early stages of design. In seeking to understand howcreative ways to address practical issues can be conceived between humans andGenAI we conducted a rapid ideation workshop with 21 participants where theyused a large language model LLM to brainstorm potential solutions andevaluate them. We found that the LLM produced a greater variety of ideas thatwere of high quality though not necessarily of higher quality thanhuman-generated ideas. Participants typically prompted in a straightforwardmanner with concise instructions. We also observed two collaborative dynamicswith the LLM fulfilling a consulting role or an assisting role depending on thegoals of the users. Notably we observed an atypical anti-collaboration dynamicwhere participants used an antagonistic approach to prompt the LLM.</p>
                <p>Last Updated: 2024-03-19 17:32:01 UTC</p>
                <button class="interpret-button" data-id="2403.12928v1">Interpret</button>
                <div id="interpretation-2403.12928v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Effects of Automated Misinformation Warning Labels on the Intents to Like, Comment and Share Posts</h3>
                <p>Authors: Gionnieve LimSimon T. Perrault</p>
                <p><a href="http://dx.doi.org/10.1145/3623809.3623856">Link to paper</a></p>
                <p>With fact-checking by professionals being difficult to scale on social mediaalgorithmic techniques have been considered. However it is uncertain how thepublic may react to labels by automated fact-checkers. In this study weinvestigate the use of automated warning labels derived from misinformationdetection literature and investigate their effects on three forms of postengagement. Focusing on political posts we also consider how partisanshipaffects engagement. In a two-phases within-subjects experiment with 200participants we found that the generic warnings suppressed intents to commenton and share posts but not on the intent to like them. Furthermore whendifferent reasons for the labels were provided their effects on postengagement were inconsistent suggesting that the reasons could haveundesirably motivated engagement instead. Partisanship effects were observedacross the labels with higher engagement for politically congruent posts. Wediscuss the implications on the design and use of automated warning labels.</p>
                <p>Last Updated: 2024-03-19 17:16:34 UTC</p>
                <button class="interpret-button" data-id="2403.12916v1">Interpret</button>
                <div id="interpretation-2403.12916v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fact Checking Chatbot: A Misinformation Intervention for Instant Messaging Apps and an Analysis of Trust in the Fact Checkers</h3>
                <p>Authors: Gionnieve LimSimon T. Perrault</p>
                <p><a href="http://dx.doi.org/10.1007/978-94-024-2225-2_11">Link to paper</a></p>
                <p>In Singapore there has been a rise in misinformation on mobile instantmessaging services MIMS. MIMS support both small peer-to-peer networks andlarge groups. Misinformation in the former may spread due to recipients trustin the sender while in the latter misinformation can directly reach a wideaudience. The encryption of MIMS makes it difficult to address misinformationdirectly. As such chatbots have become an alternative solution where users candisclose their chat content directly to fact checking services. To understandhow effective fact checking chatbots are as an intervention and how trust inthree different fact checkers i.e. Government News Outlets and ArtificialIntelligence may affect this trust we conducted a within-subjects experimentwith 527 Singapore residents. We found mixed results for the fact checkers butsupport for the chatbot intervention overall. We also found a strikingcontradiction between participants trust in the fact checkers and theirbehaviour towards them. Specifically those who reported a high level of trustin the government performed worse and tended to follow the fact checking toolless when it was endorsed by the government.</p>
                <p>Last Updated: 2024-03-19 17:11:25 UTC</p>
                <button class="interpret-button" data-id="2403.12913v1">Interpret</button>
                <div id="interpretation-2403.12913v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LAVA: Long-horizon Visual Action based Food Acquisition</h3>
                <p>Authors: Amisha BhaskarRui LiuVishnu D. SharmaGuangyao ShiPratap Tokekar</p>
                <p><a href="http://arxiv.org/abs/2403.12876v1">Link to paper</a></p>
                <p>Robotic Assisted Feeding RAF addresses the fundamental need for individualswith mobility impairments to regain autonomy in feeding themselves. The goal ofRAF is to use a robot arm to acquire and transfer food to individuals from thetable. Existing RAF methods primarily focus on solid foods leaving a gap inmanipulation strategies for semi-solid and deformable foods. This studyintroduces Long-horizon Visual Action LAVA based food acquisition of liquidsemisolid and deformable foods. Long-horizon refers to the goal of clearingthe bowl by sequentially acquiring the food from the bowl. LAVA employs ahierarchical policy for long-horizon food acquisition tasks. The framework useshigh-level policy to determine primitives by leveraging ScoopNet. At themid-level LAVA finds parameters for primitives using vision. To carry outsequential plans in the real world LAVA delegates action execution which isdriven by Low-level policy that uses parameters received from mid-level policyand behavior cloning ensuring precise trajectory execution. We validate ourapproach on complex real-world acquisition trials involving granular liquidsemisolid and deformable food types along with fruit chunks and soupacquisition. Across 46 bowls LAVA acquires much more efficiently thanbaselines with a success rate of 89 /- 4 and generalizes across realisticplate variations such as different positions varieties and amount of food inthe bowl. Code datasets videos and supplementary materials can be found onour website.</p>
                <p>Last Updated: 2024-03-19 16:21:40 UTC</p>
                <button class="interpret-button" data-id="2403.12876v1">Interpret</button>
                <div id="interpretation-2403.12876v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RASP: A Drone-based Reconfigurable Actuation and Sensing Platform Towards Ambient Intelligent Systems</h3>
                <p>Authors: Minghui ZhaoJunxi XiaKaiyuan HouYanchen LiuStephen XiaXiaofan Jiang</p>
                <p><a href="http://arxiv.org/abs/2403.12853v1">Link to paper</a></p>
                <p>Realizing consumer-grade drones that are as useful as robot vacuumsthroughout our homes or personal smartphones in our daily lives requires dronesto sense actuate and respond to general scenarios that may arise. Towardsthis vision we propose RASP a modular and reconfigurable sensing andactuation platform that allows drones to autonomously swap onboard sensors andactuators in only 25 seconds allowing a single drone to quickly adapt to adiverse range of tasks. RASP consists of a mechanical layer to physically swapsensor modules an electrical layer to maintain power and communication linesto the sensor/actuator and a software layer to maintain a common interfacebetween the drone and any sensor module in our platform. Leveraging recentadvances in large language and visual language models we further introduce thearchitecture implementation and real-world deployments of a personalassistant system utilizing RASP. We demonstrate that RASP can enable a diverserange of useful tasks in home office lab and other indoor settings.</p>
                <p>Last Updated: 2024-03-19 15:57:32 UTC</p>
                <button class="interpret-button" data-id="2403.12853v1">Interpret</button>
                <div id="interpretation-2403.12853v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion</h3>
                <p>Authors: Joe SukArpit Agarwal</p>
                <p><a href="http://arxiv.org/abs/2403.12950v1">Link to paper</a></p>
                <p>In dueling bandits the learner receives preference feedback between armsand the regret of an arm is defined in terms of its suboptimality to a winnerarm. The more challenging and practically motivated non-stationary variant ofdueling bandits where preferences change over time has been the focus ofseveral recent works Saha and Gupta 2022 Buening and Saha 2023 Suk andAgarwal 2023. The goal is to design algorithms without foreknowledge of theamount of change.  The bulk of known results here studies the Condorcet winner setting where anarm preferred over any other exists at all times. Yet such a winner may notexist and to contrast the Borda version of this problem which is alwayswell-defined has received little attention. In this work we establish thefirst optimal and adaptive Borda dynamic regret upper bound which highlightsfundamental differences in the learnability of severe non-stationarity betweenCondorcet vs. Borda regret objectives in dueling bandits.  Surprisingly our techniques for non-stationary Borda dueling bandits alsoyield improved rates within the Condorcet winner setting and reveal newpreference models where tighter notions of non-stationarity are adaptivelylearnable. This is accomplished through a novel generalized Borda scoreframework which unites the Borda and Condorcet problems thus allowingreduction of Condorcet regret to a Borda-like task. Such a generalization wasnot previously known and is likely to be of independent interest.</p>
                <p>Last Updated: 2024-03-19 17:50:55 UTC</p>
                <button class="interpret-button" data-id="2403.12950v1">Interpret</button>
                <div id="interpretation-2403.12950v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On Safety in Safe Bayesian Optimization</h3>
                <p>Authors: Christian FiedlerJohanna MennLukas KreiskötherSebastian Trimpe</p>
                <p><a href="http://arxiv.org/abs/2403.12948v1">Link to paper</a></p>
                <p>Optimizing an unknown function under safety constraints is a central task inrobotics biomedical engineering and many other disciplines and increasinglysafe Bayesian Optimization BO is used for this. Due to the safety criticalnature of these applications it is of utmost importance that theoreticalsafety guarantees for these algorithms translate into the real world. In thiswork we investigate three safety-related issues of the popular class ofSafeOpt-type algorithms. First these algorithms critically rely on frequentistuncertainty bounds for Gaussian Process GP regression but concreteimplementations typically utilize heuristics that invalidate all safetyguarantees. We provide a detailed analysis of this problem and introduceReal-beta-SafeOpt a variant of the SafeOpt algorithm that leverages recentGP bounds and thus retains all theoretical guarantees. Second we identifyassuming an upper bound on the reproducing kernel Hilbert space RKHS norm ofthe target function a key technical assumption in SafeOpt-like algorithms asa central obstacle to real-world usage. To overcome this challenge weintroduce the Lipschitz-only Safe Bayesian Optimization LoSBO algorithmwhich guarantees safety without an assumption on the RKHS bound andempirically show that this algorithm is not only safe but also exhibitssuperior performance compared to the state-of-the-art on several functionclasses. Third SafeOpt and derived algorithms rely on a discrete search spacemaking them difficult to apply to higher-dimensional problems. To widen theapplicability of these algorithms we introduce Lipschitz-only GP-UCBLoS-GP-UCB a variant of LoSBO applicable to moderately high-dimensionalproblems while retaining safety.</p>
                <p>Last Updated: 2024-03-19 17:50:32 UTC</p>
                <button class="interpret-button" data-id="2403.12948v1">Interpret</button>
                <div id="interpretation-2403.12948v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Clustered Mallows Model</h3>
                <p>Authors: Luiza S. C. PiancastelliNial Friel</p>
                <p><a href="http://arxiv.org/abs/2403.12880v1">Link to paper</a></p>
                <p>Rankings are a type of preference elicitation that arise in experiments whereassessors arrange items for example in decreasing order of utility. Orderingsof n items labelled 1...n denoted are permutations that reflect strictpreferences. For a number of reasons strict preferences can be unrealisticassumptions for real data. For example when items share common traits it maybe reasonable to attribute them equal ranks. Also there can be differentimportance attributions to decisions that form the ranking. In a situationwith for example a large number of items an assessor may wish to rank at topa certain number items to rank other items at the bottom and to expressindifference to all others. In addition when aggregating opinions a judgingbody might be decisive about some parts of the rank but ambiguous for others.In this paper we extend the well-known Mallows Mallows 1957 model MM toaccommodate item indifference a phenomenon that can be in place for a varietyof reasons such as those above mentioned.The underlying grouping of similaritems motivates the proposed Clustered Mallows Model CMM. The CMM can beinterpreted as a Mallows distribution for tied ranks where ties are learnedfrom the data. The CMM provides the flexibility to combine strict andindifferent relations achieving a simpler and robust representation of rankcollections in the form of ordered clusters. Bayesian inference for the CMM isin the class of doubly-intractable problems since the models normalisationconstant is not available in closed form. We overcome this challenge bysampling from the posterior with a version of the exchange algorithmcitepmurray2006. Real data analysis of food preferences and results ofFormula 1 races are presented illustrating the CMM in practical situations.</p>
                <p>Last Updated: 2024-03-19 16:25:30 UTC</p>
                <button class="interpret-button" data-id="2403.12880v1">Interpret</button>
                <div id="interpretation-2403.12880v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Primal Methods for Variational Inequality Problems with Functional Constraints</h3>
                <p>Authors: Liang ZhangNiao HeMichael Muehlebach</p>
                <p><a href="http://arxiv.org/abs/2403.12859v1">Link to paper</a></p>
                <p>Constrained variational inequality problems are recognized for their broadapplications across various fields including machine learning and operationsresearch. First-order methods have emerged as the standard approach for solvingthese problems due to their simplicity and scalability. However they typicallyrely on projection or linear minimization oracles to navigate the feasible setwhich becomes computationally expensive in practical scenarios featuringmultiple functional constraints. Existing efforts to tackle such functionalconstrained variational inequality problems have centered on primal-dualalgorithms grounded in the Lagrangian function. These algorithms along withtheir theoretical analysis often require the existence and prior knowledge ofthe optimal Lagrange multipliers. In this work we propose a simple primalmethod termed Constrained Gradient Method CGM for addressing functionalconstrained variational inequality problems without necessitating anyinformation on the optimal Lagrange multipliers. We establish a non-asymptoticconvergence analysis of the algorithm for variational inequality problems withmonotone operators under smooth constraints. Remarkably our algorithms matchthe complexity of projection-based methods in terms of operator queries forboth monotone and strongly monotone settings while utilizing significantlycheaper oracles based on quadratic programming. Furthermore we provide severalnumerical examples to evaluate the efficacy of our algorithms.</p>
                <p>Last Updated: 2024-03-19 16:03:03 UTC</p>
                <button class="interpret-button" data-id="2403.12859v1">Interpret</button>
                <div id="interpretation-2403.12859v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Tighter Confidence Bounds for Sequential Kernel Regression</h3>
                <p>Authors: Hamish FlynnDavid Reeb</p>
                <p><a href="http://arxiv.org/abs/2403.12732v1">Link to paper</a></p>
                <p>Confidence bounds are an essential tool for rigorously quantifying theuncertainty of predictions. In this capacity they can inform theexploration-exploitation trade-off and form a core component in many sequentiallearning and decision-making algorithms. Tighter confidence bounds give rise toalgorithms with better empirical performance and better performance guarantees.In this work we use martingale tail bounds and finite-dimensionalreformulations of infinite-dimensional convex programs to establish newconfidence bounds for sequential kernel regression. We prove that our newconfidence bounds are always tighter than existing ones in this setting. Weapply our confidence bounds to the kernel bandit problem where future actionsdepend on the previous history. When our confidence bounds replace existingones the KernelUCB GP-UCB algorithm has better empirical performance amatching worst-case performance guarantee and comparable computational cost.Our new confidence bounds can be used as a generic tool to design improvedalgorithms for other kernelised learning and decision-making problems.</p>
                <p>Last Updated: 2024-03-19 13:47:35 UTC</p>
                <button class="interpret-button" data-id="2403.12732v1">Interpret</button>
                <div id="interpretation-2403.12732v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>TexTile: A Differentiable Metric for Texture Tileability</h3>
                <p>Authors: Carlos Rodriguez-PardoDan CasasElena GarcesJorge Lopez-Moreno</p>
                <p><a href="http://arxiv.org/abs/2403.12961v1">Link to paper</a></p>
                <p>We introduce TexTile a novel differentiable metric to quantify the degreeupon which a texture image can be concatenated with itself without introducingrepeating artifacts i.e. the tileability. Existing methods for tileabletexture synthesis focus on general texture quality but lack explicit analysisof the intrinsic repeatability properties of a texture. In contrast ourTexTile metric effectively evaluates the tileable properties of a textureopening the door to more informed synthesis and analysis of tileable textures.Under the hood TexTile is formulated as a binary classifier carefully builtfrom a large dataset of textures of different styles semantics regularitiesand human annotations.Key to our method is a set of architectural modificationsto baseline pre-train image classifiers to overcome their shortcomings atmeasuring tileability along with a custom data augmentation and trainingregime aimed at increasing robustness and accuracy. We demonstrate that TexTilecan be plugged into different state-of-the-art texture synthesis methodsincluding diffusion-based strategies and generate tileable textures whilekeeping or even improving the overall texture quality. Furthermore we showthat TexTile can objectively evaluate any tileable texture synthesis methodwhereas the current mix of existing metrics produces uncorrelated scores whichheavily hinders progress in the field.</p>
                <p>Last Updated: 2024-03-19 17:59:09 UTC</p>
                <button class="interpret-button" data-id="2403.12961v1">Interpret</button>
                <div id="interpretation-2403.12961v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WHAC: World-grounded Humans and Cameras</h3>
                <p>Authors: Wanqi YinZhongang CaiRuisi WangFanzhou WangChen WeiHaiyi MeiWeiye XiaoZhitao YangQingping SunAtsushi YamashitaZiwei LiuLei Yang</p>
                <p><a href="http://arxiv.org/abs/2403.12959v1">Link to paper</a></p>
                <p>Estimating human and camera trajectories with accurate scale in the worldcoordinate system from a monocular video is a highly desirable yet challengingand ill-posed problem. In this study we aim to recover expressive parametrichuman models i.e. SMPL-X and corresponding camera poses jointly byleveraging the synergy between three critical players: the world the humanand the camera. Our approach is founded on two key observations. Firstlycamera-frame SMPL-X estimation methods readily recover absolute human depth.Secondly human motions inherently provide absolute spatial cues. Byintegrating these insights we introduce a novel framework referred to asWHAC to facilitate world-grounded expressive human pose and shape estimationEHPS alongside camera pose estimation without relying on traditionaloptimization techniques. Additionally we present a new synthetic datasetWHAC-A-Mole which includes accurately annotated humans and cameras andfeatures diverse interactive human motions as well as realistic cameratrajectories. Extensive experiments on both standard and newly establishedbenchmarks highlight the superiority and efficacy of our framework. We willmake the code and dataset publicly available.</p>
                <p>Last Updated: 2024-03-19 17:58:02 UTC</p>
                <button class="interpret-button" data-id="2403.12959v1">Interpret</button>
                <div id="interpretation-2403.12959v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models</h3>
                <p>Authors: Elaine SuiXiaohan WangSerena Yeung-Levy</p>
                <p><a href="http://arxiv.org/abs/2403.12952v1">Link to paper</a></p>
                <p>Advancements in vision-language models VLMs have propelled the field ofcomputer vision particularly in the zero-shot learning setting. Despite theirpromise the effectiveness of these models often diminishes due to domainshifts in test environments. To address this we introduce the Test-TimePrototype Shifting TPS framework a pioneering approach designed to adaptVLMs to test datasets using unlabeled test inputs. Our method is based on thenotion of modulating per-class prototypes in the shared embedding space. Bypre-computing and caching prototypes generated with the pre-trained textencoder TPS not only facilitates optimization-free prototype reuse forsubsequent predictions but also enables seamless integration with currentadvancements in prompt engineering. At test-time TPS dynamically learns shiftvectors for each prototype based solely on the given test sample effectivelybridging the domain gap and enhancing classification accuracy. A notable aspectof our framework is its significantly reduced memory and computational demandswhen compared to conventional text-prompt tuning methods. Extensive evaluationsacross 15 datasets involving natural distribution shifts and cross-datasetgeneralization demonstrate TPSs superior performance achievingstate-of-the-art results while reducing resource requirements.</p>
                <p>Last Updated: 2024-03-19 17:54:34 UTC</p>
                <button class="interpret-button" data-id="2403.12952v1">Interpret</button>
                <div id="interpretation-2403.12952v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers</h3>
                <p>Authors: Vidhi JainMaria AttarianNikhil J JoshiAyzaan WahidDanny DriessQuan VuongPannag R SanketiPierre SermanetStefan WelkerChristine ChanIgor GilitschenskiYonatan BiskDebidatta Dwibedi</p>
                <p><a href="http://arxiv.org/abs/2403.12943v1">Link to paper</a></p>
                <p>While large-scale robotic systems typically rely on textual instructions fortasks this work explores a different approach: can robots infer the taskdirectly from observing humans This shift necessitates the robots ability todecode human intent and translate it into executable actions within itsphysical constraints and environment. We introduce Vid2Robot a novelend-to-end video-based learning framework for robots. Given a videodemonstration of a manipulation task and current visual observations Vid2Robotdirectly produces robot actions. This is achieved through a unifiedrepresentation model trained on a large dataset of human video and robottrajectory. The model leverages cross-attention mechanisms to fuse prompt videofeatures to the robots current state and generate appropriate actions thatmimic the observed task. To further improve policy performance we proposeauxiliary contrastive losses that enhance the alignment between human and robotvideo representations. We evaluate Vid2Robot on real-world robotsdemonstrating a 20 improvement in performance compared to othervideo-conditioned policies when using human demonstration videos. Additionallyour model exhibits emergent capabilities such as successfully transferringobserved motions from one object to another and long-horizon composition thusshowcasing its potential for real-world applications. Project website:vid2robot.github.io</p>
                <p>Last Updated: 2024-03-19 17:47:37 UTC</p>
                <button class="interpret-button" data-id="2403.12943v1">Interpret</button>
                <div id="interpretation-2403.12943v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models</h3>
                <p>Authors: Joana Ribeiro de FariaHuiyuan XieFelix Steffek</p>
                <p><a href="http://arxiv.org/abs/2403.12936v1">Link to paper</a></p>
                <p>Court transcripts and judgments are rich repositories of legal knowledgedetailing the intricacies of cases and the rationale behind judicial decisions.The extraction of key information from these documents provides a conciseoverview of a case crucial for both legal experts and the public. With theadvent of large language models LLMs automatic information extraction hasbecome increasingly feasible and efficient. This paper presents a comprehensivestudy on the application of GPT-4 a large language model for automaticinformation extraction from UK Employment Tribunal UKET cases. Wemeticulously evaluated GPT-4s performance in extracting critical informationwith a manual verification process to ensure the accuracy and relevance of theextracted data. Our research is structured around two primary extraction tasks:the first involves a general extraction of eight key aspects that holdsignificance for both legal specialists and the general public including thefacts of the case the claims made references to legal statutes references toprecedents general case outcomes and corresponding labels detailed order andremedies and reasons for the decision. The second task is more focused aimedat analysing three of those extracted features namely facts claims andoutcomes in order to facilitate the development of a tool capable ofpredicting the outcome of employment law disputes. Through our analysis wedemonstrate that LLMs like GPT-4 can obtain high accuracy in legal informationextraction highlighting the potential of LLMs in revolutionising the way legalinformation is processed and utilised offering significant implications forlegal research and practice.</p>
                <p>Last Updated: 2024-03-19 17:43:08 UTC</p>
                <button class="interpret-button" data-id="2403.12936v1">Interpret</button>
                <div id="interpretation-2403.12936v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression</h3>
                <p>Authors: Zhuoshi PanQianhui WuHuiqiang JiangMenglin XiaXufang LuoJue ZhangQingwei LinVictor RühleYuqing YangChin-Yew LinH. Vicky ZhaoLili QiuDongmei Zhang</p>
                <p><a href="http://arxiv.org/abs/2403.12968v1">Link to paper</a></p>
                <p>This paper focuses on task-agnostic prompt compression for bettergeneralizability and efficiency. Considering the redundancy in naturallanguage existing approaches compress prompts by removing tokens or lexicalunits according to their information entropy obtained from a causal languagemodel such as LLaMa-7B. The challenge is that information entropy may be asuboptimal compression metric: i it only leverages unidirectional context andmay fail to capture all essential information needed for prompt compressionii it is not aligned with the prompt compression objective.  To address these issues we propose a data distillation procedure to deriveknowledge from an LLM to compress prompts without losing crucial informationand meantime introduce an extractive text compression dataset. We formulateprompt compression as a token classification problem to guarantee thefaithfulness of the compressed prompt to the original one and use aTransformer encoder as the base architecture to capture all essentialinformation for prompt compression from the full bidirectional context. Ourapproach leads to lower latency by explicitly learning the compressionobjective with smaller models such as XLM-RoBERTa-large and mBERT.  We evaluate our method on both in-domain and out-of-domain datasetsincluding MeetingBank LongBench ZeroScrolls GSM8K and BBH. Despite itssmall size our model shows significant performance gains over strong baselinesand demonstrates robust generalization ability across different LLMs.Additionally our model is 3x-6x faster than existing prompt compressionmethods while accelerating the end-to-end latency by 1.6x-2.9x withcompression ratios of 2x-5x.</p>
                <p>Last Updated: 2024-03-19 17:59:56 UTC</p>
                <button class="interpret-button" data-id="2403.12968v1">Interpret</button>
                <div id="interpretation-2403.12968v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TexTile: A Differentiable Metric for Texture Tileability</h3>
                <p>Authors: Carlos Rodriguez-PardoDan CasasElena GarcesJorge Lopez-Moreno</p>
                <p><a href="http://arxiv.org/abs/2403.12961v1">Link to paper</a></p>
                <p>We introduce TexTile a novel differentiable metric to quantify the degreeupon which a texture image can be concatenated with itself without introducingrepeating artifacts i.e. the tileability. Existing methods for tileabletexture synthesis focus on general texture quality but lack explicit analysisof the intrinsic repeatability properties of a texture. In contrast ourTexTile metric effectively evaluates the tileable properties of a textureopening the door to more informed synthesis and analysis of tileable textures.Under the hood TexTile is formulated as a binary classifier carefully builtfrom a large dataset of textures of different styles semantics regularitiesand human annotations.Key to our method is a set of architectural modificationsto baseline pre-train image classifiers to overcome their shortcomings atmeasuring tileability along with a custom data augmentation and trainingregime aimed at increasing robustness and accuracy. We demonstrate that TexTilecan be plugged into different state-of-the-art texture synthesis methodsincluding diffusion-based strategies and generate tileable textures whilekeeping or even improving the overall texture quality. Furthermore we showthat TexTile can objectively evaluate any tileable texture synthesis methodwhereas the current mix of existing metrics produces uncorrelated scores whichheavily hinders progress in the field.</p>
                <p>Last Updated: 2024-03-19 17:59:09 UTC</p>
                <button class="interpret-button" data-id="2403.12961v1">Interpret</button>
                <div id="interpretation-2403.12961v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>WHAC: World-grounded Humans and Cameras</h3>
                <p>Authors: Wanqi YinZhongang CaiRuisi WangFanzhou WangChen WeiHaiyi MeiWeiye XiaoZhitao YangQingping SunAtsushi YamashitaZiwei LiuLei Yang</p>
                <p><a href="http://arxiv.org/abs/2403.12959v1">Link to paper</a></p>
                <p>Estimating human and camera trajectories with accurate scale in the worldcoordinate system from a monocular video is a highly desirable yet challengingand ill-posed problem. In this study we aim to recover expressive parametrichuman models i.e. SMPL-X and corresponding camera poses jointly byleveraging the synergy between three critical players: the world the humanand the camera. Our approach is founded on two key observations. Firstlycamera-frame SMPL-X estimation methods readily recover absolute human depth.Secondly human motions inherently provide absolute spatial cues. Byintegrating these insights we introduce a novel framework referred to asWHAC to facilitate world-grounded expressive human pose and shape estimationEHPS alongside camera pose estimation without relying on traditionaloptimization techniques. Additionally we present a new synthetic datasetWHAC-A-Mole which includes accurately annotated humans and cameras andfeatures diverse interactive human motions as well as realistic cameratrajectories. Extensive experiments on both standard and newly establishedbenchmarks highlight the superiority and efficacy of our framework. We willmake the code and dataset publicly available.</p>
                <p>Last Updated: 2024-03-19 17:58:02 UTC</p>
                <button class="interpret-button" data-id="2403.12959v1">Interpret</button>
                <div id="interpretation-2403.12959v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models</h3>
                <p>Authors: Elaine SuiXiaohan WangSerena Yeung-Levy</p>
                <p><a href="http://arxiv.org/abs/2403.12952v1">Link to paper</a></p>
                <p>Advancements in vision-language models VLMs have propelled the field ofcomputer vision particularly in the zero-shot learning setting. Despite theirpromise the effectiveness of these models often diminishes due to domainshifts in test environments. To address this we introduce the Test-TimePrototype Shifting TPS framework a pioneering approach designed to adaptVLMs to test datasets using unlabeled test inputs. Our method is based on thenotion of modulating per-class prototypes in the shared embedding space. Bypre-computing and caching prototypes generated with the pre-trained textencoder TPS not only facilitates optimization-free prototype reuse forsubsequent predictions but also enables seamless integration with currentadvancements in prompt engineering. At test-time TPS dynamically learns shiftvectors for each prototype based solely on the given test sample effectivelybridging the domain gap and enhancing classification accuracy. A notable aspectof our framework is its significantly reduced memory and computational demandswhen compared to conventional text-prompt tuning methods. Extensive evaluationsacross 15 datasets involving natural distribution shifts and cross-datasetgeneralization demonstrate TPSs superior performance achievingstate-of-the-art results while reducing resource requirements.</p>
                <p>Last Updated: 2024-03-19 17:54:34 UTC</p>
                <button class="interpret-button" data-id="2403.12952v1">Interpret</button>
                <div id="interpretation-2403.12952v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion</h3>
                <p>Authors: Joe SukArpit Agarwal</p>
                <p><a href="http://arxiv.org/abs/2403.12950v1">Link to paper</a></p>
                <p>In dueling bandits the learner receives preference feedback between armsand the regret of an arm is defined in terms of its suboptimality to a winnerarm. The more challenging and practically motivated non-stationary variant ofdueling bandits where preferences change over time has been the focus ofseveral recent works Saha and Gupta 2022 Buening and Saha 2023 Suk andAgarwal 2023. The goal is to design algorithms without foreknowledge of theamount of change.  The bulk of known results here studies the Condorcet winner setting where anarm preferred over any other exists at all times. Yet such a winner may notexist and to contrast the Borda version of this problem which is alwayswell-defined has received little attention. In this work we establish thefirst optimal and adaptive Borda dynamic regret upper bound which highlightsfundamental differences in the learnability of severe non-stationarity betweenCondorcet vs. Borda regret objectives in dueling bandits.  Surprisingly our techniques for non-stationary Borda dueling bandits alsoyield improved rates within the Condorcet winner setting and reveal newpreference models where tighter notions of non-stationarity are adaptivelylearnable. This is accomplished through a novel generalized Borda scoreframework which unites the Borda and Condorcet problems thus allowingreduction of Condorcet regret to a Borda-like task. Such a generalization wasnot previously known and is likely to be of independent interest.</p>
                <p>Last Updated: 2024-03-19 17:50:55 UTC</p>
                <button class="interpret-button" data-id="2403.12950v1">Interpret</button>
                <div id="interpretation-2403.12950v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence Alignment</h3>
                <p>Authors: Mengting ChenXi ChenZhonghua ZhaiChen JuXuewen HongJinsong LanShuai Xiao</p>
                <p><a href="http://arxiv.org/abs/2403.12965v1">Link to paper</a></p>
                <p>This paper introduces a novel framework for virtual try-on termedWear-Any-Way. Different from previous methods Wear-Any-Way is a customizablesolution. Besides generating high-fidelity results our method supports usersto precisely manipulate the wearing style. To achieve this goal we firstconstruct a strong pipeline for standard virtual try-on supportingsingle/multiple garment try-on and model-to-model settings in complicatedscenarios. To make it manipulable we propose sparse correspondence alignmentwhich involves point-based control to guide the generation for specificlocations. With this design Wear-Any-Way gets state-of-the-art performance forthe standard setting and provides a novel interaction form for customizing thewearing style. For instance it supports users to drag the sleeve to make itrolled up drag the coat to make it open and utilize clicks to control thestyle of tuck etc. Wear-Any-Way enables more liberated and flexibleexpressions of the attires holding profound implications in the fashionindustry.</p>
                <p>Last Updated: 2024-03-19 17:59:52 UTC</p>
                <button class="interpret-button" data-id="2403.12965v1">Interpret</button>
                <div id="interpretation-2403.12965v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models</h3>
                <p>Authors: Zuyan LiuYuhao DongYongming RaoJie ZhouJiwen Lu</p>
                <p><a href="http://arxiv.org/abs/2403.12966v1">Link to paper</a></p>
                <p>In the realm of vision-language understanding the proficiency of models ininterpreting and reasoning over visual content has become a cornerstone fornumerous applications. However it is challenging for the visual encoder inLarge Vision-Language Models LVLMs to extract useful features tailored toquestions that aid the language models response. Furthermore a commonpractice among existing LVLMs is to utilize lower-resolution images whichrestricts the ability for visual recognition. Our work introduces theChain-of-Spot CoS method which we describe as Interactive Reasoning a novelapproach that enhances feature extraction by focusing on key regions ofinterest ROI within the image corresponding to the posed questions orinstructions. This technique allows LVLMs to access more detailed visualinformation without altering the original image resolution thereby offeringmulti-granularity image features. By integrating Chain-of-Spot withinstruct-following LLaVA-1.5 models the process of image reasoningconsistently improves performance across a wide range of multimodal datasetsand benchmarks without bells and whistles and achieves new state-of-the-artresults. Our empirical findings demonstrate a significant improvement in LVLMsability to understand and reason about visual content paving the way for moresophisticated visual instruction-following applications. Code and models areavailable at https://github.com/dongyh20/Chain-of-Spot</p>
                <p>Last Updated: 2024-03-19 17:59:52 UTC</p>
                <button class="interpret-button" data-id="2403.12966v1">Interpret</button>
                <div id="interpretation-2403.12966v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models</h3>
                <p>Authors: Ce ZhangSimon StepputtisKatia SycaraYaqi Xie</p>
                <p><a href="http://arxiv.org/abs/2403.12964v1">Link to paper</a></p>
                <p>Recently large-scale pre-trained Vision-Language Models VLMs havedemonstrated great potential in learning open-world visual representations andexhibit remarkable performance across a wide range of downstream tasks throughefficient fine-tuning. In this work we innovatively introduce the concept ofdual learning into fine-tuning VLMs i.e. we not only learn what an image isbut also what an image isnt. Building on this concept we introduce a novelDualAdapter approach to enable dual-path adaptation of VLMs from both positiveand negative perspectives with only limited annotated samples. In the inferencestage our DualAdapter performs unified predictions by simultaneouslyconducting complementary positive selection and negative exclusion acrosstarget classes thereby enhancing the overall recognition accuracy of VLMs indownstream tasks. Our extensive experimental results across 15 datasetsvalidate that the proposed DualAdapter outperforms existing state-of-the-artmethods on both few-shot learning and domain generalization tasks whileachieving competitive computational efficiency. Code is available athttps://github.com/zhangce01/DualAdapter.</p>
                <p>Last Updated: 2024-03-19 17:59:39 UTC</p>
                <button class="interpret-button" data-id="2403.12964v1">Interpret</button>
                <div id="interpretation-2403.12964v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis</h3>
                <p>Authors: Linjiang HuangRongyao FangAiping ZhangGuanglu SongSi LiuYu LiuHongsheng Li</p>
                <p><a href="http://arxiv.org/abs/2403.12963v1">Link to paper</a></p>
                <p>In this study we delve into the generation of high-resolution images frompre-trained diffusion models addressing persistent challenges such asrepetitive patterns and structural distortions that emerge when models areapplied beyond their trained resolutions. To address this issue we introducean innovative training-free approach FouriScale from the perspective offrequency domain analysis. We replace the original convolutional layers inpre-trained diffusion models by incorporating a dilation technique along with alow-pass operation intending to achieve structural consistency and scaleconsistency across resolutions respectively. Further enhanced by apadding-then-crop strategy our method can flexibly handle text-to-imagegeneration of various aspect ratios. By using the FouriScale as guidance ourmethod successfully balances the structural integrity and fidelity of generatedimages achieving an astonishing capacity of arbitrary-size high-resolutionand high-quality generation. With its simplicity and compatibility our methodcan provide valuable insights for future explorations into the synthesis ofultra-high-resolution images. The code will be released athttps://github.com/LeonHLJ/FouriScale.</p>
                <p>Last Updated: 2024-03-19 17:59:33 UTC</p>
                <button class="interpret-button" data-id="2403.12963v1">Interpret</button>
                <div id="interpretation-2403.12963v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation</h3>
                <p>Authors: Shuai YangYifan ZhouZiwei LiuChen Change Loy</p>
                <p><a href="http://arxiv.org/abs/2403.12962v1">Link to paper</a></p>
                <p>The remarkable efficacy of text-to-image diffusion models has motivatedextensive exploration of their potential application in video domains.Zero-shot methods seek to extend image diffusion models to videos withoutnecessitating model training. Recent methods mainly focus on incorporatinginter-frame correspondence into attention mechanisms. However the softconstraint imposed on determining where to attend to valid features cansometimes be insufficient resulting in temporal inconsistency. In this paperwe introduce FRESCO intra-frame correspondence alongside inter-framecorrespondence to establish a more robust spatial-temporal constraint. Thisenhancement ensures a more consistent transformation of semantically similarcontent across frames. Beyond mere attention guidance our approach involves anexplicit update of features to achieve high spatial-temporal consistency withthe input video significantly improving the visual coherence of the resultingtranslated videos. Extensive experiments demonstrate the effectiveness of ourproposed framework in producing high-quality coherent videos marking anotable improvement over existing zero-shot methods.</p>
                <p>Last Updated: 2024-03-19 17:59:18 UTC</p>
                <button class="interpret-button" data-id="2403.12962v1">Interpret</button>
                <div id="interpretation-2403.12962v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Detection of Malicious Agents in Social Learning</h3>
                <p>Authors: Valentina ShumovskaiaMert KayaalpAli H. Sayed</p>
                <p><a href="http://arxiv.org/abs/2403.12619v1">Link to paper</a></p>
                <p>Social learning is a non-Bayesian framework for distributed hypothesistesting aimed at learning the true state of the environment. Traditionally theagents are assumed to receive observations conditioned on the same true statealthough it is also possible to examine the case of heterogeneous models acrossthe graph. One important special case is when heterogeneity is caused by thepresence of malicious agents whose goal is to move the agents towards a wronghypothesis. In this work we propose an algorithm that allows to discover thetrue state of every individual agent based on the sequence of their beliefs. Inso doing the methodology is also able to locate malicious behavior.</p>
                <p>Last Updated: 2024-03-19 10:40:03 UTC</p>
                <button class="interpret-button" data-id="2403.12619v1">Interpret</button>
                <div id="interpretation-2403.12619v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Embodied LLM Agents Learn to Cooperate in Organized Teams</h3>
                <p>Authors: Xudong GuoKaixuan HuangJiale LiuWenhui FanNatalia VélezQingyun WuHuazheng WangThomas L. GriffithsMengdi Wang</p>
                <p><a href="http://arxiv.org/abs/2403.12482v1">Link to paper</a></p>
                <p>Large Language Models LLMs have emerged as integral tools for reasoningplanning and decision-making drawing upon their extensive world knowledge andproficiency in language-related tasks. LLMs thus hold tremendous potential fornatural language interaction within multi-agent systems to foster cooperation.However LLM agents tend to over-report and comply with any instruction whichmay result in information redundancy and confusion in multi-agent cooperation.Inspired by human organizations this paper introduces a framework that imposesprompt-based organization structures on LLM agents to mitigate these problems.Through a series of experiments with embodied LLM agents and human-agentcollaboration our results highlight the impact of designated leadership onteam efficiency shedding light on the leadership qualities displayed by LLMagents and their spontaneous cooperative behaviors. Further we harness thepotential of LLMs to propose enhanced organizational prompts via aCriticize-Reflect process resulting in novel organization structures thatreduce communication costs and enhance team efficiency.</p>
                <p>Last Updated: 2024-03-19 06:39:47 UTC</p>
                <button class="interpret-button" data-id="2403.12482v1">Interpret</button>
                <div id="interpretation-2403.12482v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Online Multi-Agent Pickup and Delivery with Task Deadlines</h3>
                <p>Authors: Hiroya MakinoSeigo Ito</p>
                <p><a href="http://arxiv.org/abs/2403.12377v1">Link to paper</a></p>
                <p>Managing delivery deadlines in automated warehouses and factories is crucialfor maintaining customer satisfaction and ensuring seamless production. Thisstudy introduces the problem of online multi-agent pickup and delivery withtask deadlines MAPD-D which is an advanced variant of the online MAPDproblem incorporating delivery deadlines. MAPD-D presents a dynamicdeadline-driven approach that includes task deadlines with tasks being addedat any time online thus challenging conventional MAPD frameworks. To tackleMAPD-D we propose a novel algorithm named deadline-aware token passing D-TP.The D-TP algorithm is designed to calculate pickup deadlines and assign taskswhile balancing execution cost and deadline proximity. Additionally weintroduce the D-TP with task swaps D-TPTS method to further reduce tasktardiness enhancing flexibility and efficiency via task-swapping strategies.Numerical experiments were conducted in simulated warehouse environments toshowcase the effectiveness of the proposed methods. Both D-TP and D-TPTSdemonstrate significant reductions in task tardiness compared to existingmethods thereby contributing to efficient operations in automated warehousesand factories with delivery deadlines.</p>
                <p>Last Updated: 2024-03-19 02:40:51 UTC</p>
                <button class="interpret-button" data-id="2403.12377v1">Interpret</button>
                <div id="interpretation-2403.12377v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MARPF: Multi-Agent and Multi-Rack Path Finding</h3>
                <p>Authors: Hiroya MakinoYoshihiro OhamaSeigo Ito</p>
                <p><a href="http://arxiv.org/abs/2403.12376v1">Link to paper</a></p>
                <p>In environments where many automated guided vehicles AGVs operate planningefficient collision-free paths is essential. Related research has mainlyfocused on environments with static passages resulting in space inefficiency.We define multi-agent and multi-rack path finding MARPF as the problem ofplanning paths for AGVs to convey target racks to their designated locations inenvironments without passages. In such environments an AGV without a rack canpass under racks whereas an AGV with a rack cannot pass under racks to avoidcollisions. MARPF entails conveying the target racks without collisions whilethe other obstacle racks are positioned without a specific arrangement. AGVsare essential for relocating other racks to prevent any interference with thetarget racks. We formulated MARPF as an integer linear programming problem in anetwork flow. To distinguish situations in which an AGV is or is not loading arack the proposed method introduces two virtual layers into the network. Weoptimized the AGVs movements to move obstacle racks and convey the targetracks. The formulation and applicability of the algorithm were validatedthrough numerical experiments. The results indicated that the proposedalgorithm addressed issues in environments with dense racks.</p>
                <p>Last Updated: 2024-03-19 02:39:41 UTC</p>
                <button class="interpret-button" data-id="2403.12376v1">Interpret</button>
                <div id="interpretation-2403.12376v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Information Compression in Dynamic Information Disclosure Games</h3>
                <p>Authors: Dengwang TangVijay G. Subramanian</p>
                <p><a href="http://arxiv.org/abs/2403.12204v1">Link to paper</a></p>
                <p>We consider a two-player dynamic information design problem between aprincipal and a receiver -- a game is played between the two agents on top of aMarkovian system controlled by the receivers actions where the principalobtains and strategically shares some information about the underlying systemwith the receiver in order to influence their actions. In our setting bothplayers have long-term objectives and the principal sequentially commits totheir strategies instead of committing at the beginning. Further the principalcannot directly observe the system state but at every turn they can chooserandomized experiments to observe the system partially. The principal can sharedetails about the experiments to the receiver. For our analysis we impose thetruthful disclosure rule: the principal is required to truthfully announce thedetails and the result of each experiment to the receiver immediately after theexperiment result is revealed. Based on the received information the receivertakes an action when its their turn with the action influencing the state ofthe underlying system. We show that there exist Perfect Bayesian equilibria inthis game where both agents play Canonical Belief Based CBB strategies usinga compressed version of their information rather than full information tochoose experiments for the principal or actions for the receiver. We alsoprovide a backward inductive procedure to solve for an equilibrium in CBBstrategies.</p>
                <p>Last Updated: 2024-03-18 19:40:16 UTC</p>
                <button class="interpret-button" data-id="2403.12204v1">Interpret</button>
                <div id="interpretation-2403.12204v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-03-20</p>
        </div>
    
        </div>
    </body>
    </html>
    