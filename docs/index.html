
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Iterative Graph Alignment</h3>
                <p>Authors: Fangyuan YuHardeep Singh AroraMatt Johnson</p>
                <p><a href="http://arxiv.org/abs/2408.16667v1">Link to paper</a></p>
                <p>By compressing diverse narratives LLMs go beyond memorization achievingintelligence by capturing generalizable causal relationships. However theysuffer from local representation gaps due to insufficient training datadiversity limiting their real-world utility especially in tasks requiringstrict alignment to rules. Traditional alignment methods relying on heavy humanannotations are inefficient and unscalable. Recent self-alignment techniquesalso fall short as they often depend on self-selection based prompting andmemorization-based learning. To address these issues we introduce IterativeGraph Alignment IGA an annotation-free rule-based alignment algorithm. Ateacher model VLM employs Iterative Graph Prompting IGP to create logicalgraphs and reference answers. The student model LLM identifies localknowledge gaps by attempting to align its responses with these referencescollaborating with helper models to generate diverse answers. These alignedresponses are then used for iterative supervised fine-tuning SFT. Ourevaluations across five rule-based scenarios demonstrate IGPs effectivenesswith a 73.12 alignment improvement in Claude Sonnet 3.5 andLlama3-8B-Instruct achieving an 86.20 improvement outperforming ClaudeSonnet 3.5 in rule-based alignment.</p>
                <p>Last Updated: 2024-08-29 16:15:01 UTC</p>
                <button class="interpret-button" data-id="2408.16667v1">Interpret</button>
                <div id="interpretation-2408.16667v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Consensus Planning with Primal, Dual, and Proximal Agents</h3>
                <p>Authors: Alvaro MaggiarLee DickerMichael Mahoney</p>
                <p><a href="http://arxiv.org/abs/2408.16462v1">Link to paper</a></p>
                <p>Consensus planning is a method for coordinating decision making acrosscomplex systems and organizations including complex supply chain optimizationpipelines. It arises when large interdependent distributed agents systemsshare common resources and must act in order to achieve a joint goal. In thispaper we introduce a generic Consensus Planning Protocol CPP to solve suchproblems. Our protocol allows for different agents to interact with thecoordinating algorithm in different ways e.g. as a primal or dual or proximalagent. In prior consensus planning work all agents have been assumed to havethe same interaction pattern e.g. all dual agents or all primal agents or allproximal agents most commonly using the Alternating Direction Method ofMultipliers ADMM as proximal agents. However this is often not a validassumption in practice where agents consist of large complex systems andwhere we might not have the luxury of modifying these large complex systems atwill. Our generic CPP allows for any mix of agents by combining ADMM-likeupdates for the proximal agents dual ascent updates for the dual agents andlinearized ADMM updates for the primal agents. We prove convergence results forthe generic CPP namely a sublinear O1/k convergence rate under mildassumptions and two-step linear convergence under stronger assumptions. Wealso discuss enhancements to the basic method and provide illustrativeempirical results.</p>
                <p>Last Updated: 2024-08-29 11:51:04 UTC</p>
                <button class="interpret-button" data-id="2408.16462v1">Interpret</button>
                <div id="interpretation-2408.16462v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Parametrization and convergence of a primal-dual block-coordinate approach to linearly-constrained nonsmooth optimization</h3>
                <p>Authors: Olivier Bilenne</p>
                <p><a href="http://arxiv.org/abs/2408.16424v1">Link to paper</a></p>
                <p>This note is concerned with the problem of minimizing a separable convexcomposite smooth and nonsmooth function subject to linear constraints. Westudy a randomized block-coordinate interpretation of the Chambolle-Pockprimal-dual algorithm based on inexact proximal gradient steps. A specificityof the considered algorithm is its robustness as it converges even in theabsence of strong duality or when the linear program is inconsistent. Usingmatrix preconditiong we derive tight sublinear convergence rates with andwithout duality assumptions and for both the convex and the strongly convexsettings. Our developments are extensions and particularizations of originalalgorithms proposed by Malitsky 2019 and Luke and Malitsky 2018. Numericalexperiments are provided for an optimal transport problem of service pricing.</p>
                <p>Last Updated: 2024-08-29 10:34:07 UTC</p>
                <button class="interpret-button" data-id="2408.16424v1">Interpret</button>
                <div id="interpretation-2408.16424v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>3D Topological Modeling and Multi-Agent Movement Simulation for Viral Infection Risk Analysis</h3>
                <p>Authors: Wassim JabiYidan XueThomas E. WoolleyKaterina Kaouri</p>
                <p><a href="http://arxiv.org/abs/2408.16417v1">Link to paper</a></p>
                <p>In this paper a method to study how the design of indoor spaces and peoplesmovement within them affect disease spread is proposed by integratingcomputer-aided modeling multi-agent movement simulation and airborne viraltransmission modeling. Topologicpy spatial design and analysis software is usedto model indoor environments connect spaces and construct a navigation graph.Pathways for agents each with unique characteristics such as walking speedinfection status and activities are computed using this graph. Agents followa schedule of events with specific locations and times. The software calculatestime-to-leave based on walking speed and event start times and agents aremoved along the shortest path within the navigation graph accuratelyconsidering obstacles doorways and walls. Precise distance calculationsbetween agents are enabled by this setup. Viral aerosol concentration is thencomputed and visualized using a reaction-diffusion equation and each agentsinfection risk is determined with an extension of the Wells-Riley ansatz.Infection risk simulations are improved by this spatio-temporal and topologicalapproach incorporating realistic human behavior and spatial dynamics. Theresulting software is designed as a rapid decision-support tool forpolicymakers facility managers stakeholders architects and engineers tomitigate disease spread in existing buildings and inform the design of newones. The softwares effectiveness is demonstrated through a comparativeanalysis of cellular and open commercial office plan layouts.</p>
                <p>Last Updated: 2024-08-29 10:22:51 UTC</p>
                <button class="interpret-button" data-id="2408.16417v1">Interpret</button>
                <div id="interpretation-2408.16417v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Improving the Prediction of Individual Engagement in Recommendations Using Cognitive Models</h3>
                <p>Authors: Roderick SeowYunfan ZhaoDuncan WoodMilind TambeCleotilde Gonzalez</p>
                <p><a href="http://arxiv.org/abs/2408.16147v1">Link to paper</a></p>
                <p>For public health programs with limited resources the ability to predict howbehaviors change over time and in response to interventions is crucial fordeciding when and to whom interventions should be allocated. Using data from areal-world maternal health program we demonstrate how a cognitive model basedon Instance-Based Learning IBL Theory can augment existing purelycomputational approaches. Our findings show that compared to generaltime-series forecasters e.g. LSTMs IBL models which reflect humandecision-making processes better predict the dynamics of individuals states.Additionally IBL provides estimates of the volatility in individuals statesand their sensitivity to interventions which can improve the efficiency oftraining of other time series models.</p>
                <p>Last Updated: 2024-08-28 21:28:45 UTC</p>
                <button class="interpret-button" data-id="2408.16147v1">Interpret</button>
                <div id="interpretation-2408.16147v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>3D Whole-body Grasp Synthesis with Directional Controllability</h3>
                <p>Authors: Georgios PaschalidisRomana WilschutDimitrije AntiÄ‡Omid TaheriDimitrios Tzionas</p>
                <p><a href="http://arxiv.org/abs/2408.16770v1">Link to paper</a></p>
                <p>Synthesizing 3D whole-bodies that realistically grasp objects is useful foranimation mixed reality and robotics. This is challenging because the handsand body need to look natural w.r.t. each other the grasped object as well asthe local scene i.e. a receptacle supporting the object. Only recent worktackles this with a divide-and-conquer approach it first generates aguiding right-hand grasp and then searches for bodies that match this.However the guiding-hand synthesis lacks controllability and receptacleawareness so it likely has an implausible direction i.e. a body cant matchthis without penetrating the receptacle and needs corrections through majorpost-processing. Moreover the body search needs exhaustive sampling and isexpensive. These are strong limitations. We tackle these with a novel methodcalled CWGrasp. Our key idea is that performing geometry-based reasoning earlyon instead of too late provides rich control signals for inference. Tothis end CWGrasp first samples a plausible reaching-direction vector usedlater for both the arm and hand from a probabilistic model built viaraycasting from the object and collision checking. Then it generates areaching body with a desired arm direction as well as a guiding graspinghand with a desired palm direction that complies with the arms one.Eventually CWGrasp refines the body to match the guiding hand whileplausibly contacting the scene. Notably generating already-compatible partsgreatly simplifies the whole. Moreover CWGrasp uniquely tackles both right-and left-hand grasps. We evaluate on the GRAB and ReplicaGrasp datasets.CWGrasp outperforms baselines at lower runtime and budget while allcomponents help performance. Code and models will be released.</p>
                <p>Last Updated: 2024-08-29 17:59:54 UTC</p>
                <button class="interpret-button" data-id="2408.16770v1">Interpret</button>
                <div id="interpretation-2408.16770v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</h3>
                <p>Authors: Ziyu GuoRenrui ZhangXiangyang ZhuChengzhuo TongPeng GaoChunyuan LiPheng-Ann Heng</p>
                <p><a href="http://arxiv.org/abs/2408.16768v1">Link to paper</a></p>
                <p>We introduce SAM2Point a preliminary exploration adapting Segment AnythingModel 2 SAM 2 for zero-shot and promptable 3D segmentation. SAM2Pointinterprets any 3D data as a series of multi-directional videos and leveragesSAM 2 for 3D-space segmentation without further training or 2D-3D projection.Our framework supports various prompt types including 3D points boxes andmasks and can generalize across diverse scenarios such as 3D objects indoorscenes outdoor environments and raw sparse LiDAR. Demonstrations on multiple3D datasets e.g. Objaverse S3DIS ScanNet Semantic3D and KITTI highlightthe robust generalization capabilities of SAM2Point. To our best knowledge wepresent the most faithful implementation of SAM in 3D which may serve as astarting point for future research in promptable 3D segmentation. Online Demo:https://huggingface.co/spaces/ZiyuG/SAM2Point . Code:https://github.com/ZiyuGuo99/SAM2Point .</p>
                <p>Last Updated: 2024-08-29 17:59:45 UTC</p>
                <button class="interpret-button" data-id="2408.16768v1">Interpret</button>
                <div id="interpretation-2408.16768v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning</h3>
                <p>Authors: Noor HusseinFahad ShamshadMuzammal NaseerKarthik Nandakumar</p>
                <p><a href="http://arxiv.org/abs/2408.16769v1">Link to paper</a></p>
                <p>Medical vision-language models Med-VLMs trained on large datasets ofmedical image-text pairs and later fine-tuned for specific tasks have emergedas a mainstream paradigm in medical image analysis. However recent studieshave highlighted the susceptibility of these Med-VLMs to adversarial attacksraising concerns about their safety and robustness. Randomized smoothing is awell-known technique for turning any classifier into a model that iscertifiably robust to adversarial perturbations. However this approachrequires retraining the Med-VLM-based classifier so that it classifies wellunder Gaussian noise which is often infeasible in practice. In this paper wepropose a novel framework called PromptSmooth to achieve efficient certifiedrobustness of Med-VLMs by leveraging the concept of prompt learning. Given anypre-trained Med-VLM PromptSmooth adapts it to handle Gaussian noise bylearning textual prompts in a zero-shot or few-shot manner achieving adelicate balance between accuracy and robustness while minimizing thecomputational overhead. Moreover PromptSmooth requires only a single model tohandle multiple noise levels which substantially reduces the computationalcost compared to traditional methods that rely on training a separate model foreach noise level. Comprehensive experiments based on three Med-VLMs and acrosssix downstream datasets of various imaging modalities demonstrate the efficacyof PromptSmooth. Our code and models are available athttps://github.com/nhussein/promptsmooth.</p>
                <p>Last Updated: 2024-08-29 17:59:45 UTC</p>
                <button class="interpret-button" data-id="2408.16769v1">Interpret</button>
                <div id="interpretation-2408.16769v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model</h3>
                <p>Authors: Fangfu LiuWenqiang SunHanyang WangYikai WangHaowen SunJunliang YeJun ZhangYueqi Duan</p>
                <p><a href="http://arxiv.org/abs/2408.16767v1">Link to paper</a></p>
                <p>Advancements in 3D scene reconstruction have transformed 2D images from thereal world into 3D models producing realistic 3D results from hundreds ofinput photos. Despite great success in dense-view reconstruction scenariosrendering a detailed scene from insufficient captured views is still anill-posed optimization problem often resulting in artifacts and distortions inunseen areas. In this paper we propose ReconX a novel 3D scene reconstructionparadigm that reframes the ambiguous reconstruction challenge as a temporalgeneration task. The key insight is to unleash the strong generative prior oflarge pre-trained video diffusion models for sparse-view reconstruction.However 3D view consistency struggles to be accurately preserved in directlygenerated video frames from pre-trained models. To address this given limitedinput views the proposed ReconX first constructs a global point cloud andencodes it into a contextual space as the 3D structure condition. Guided by thecondition the video diffusion model then synthesizes video frames that areboth detail-preserved and exhibit a high degree of 3D consistency ensuring thecoherence of the scene from various perspectives. Finally we recover the 3Dscene from the generated video through a confidence-aware 3D Gaussian Splattingoptimization scheme. Extensive experiments on various real-world datasets showthe superiority of our ReconX over state-of-the-art methods in terms of qualityand generalizability.</p>
                <p>Last Updated: 2024-08-29 17:59:40 UTC</p>
                <button class="interpret-button" data-id="2408.16767v1">Interpret</button>
                <div id="interpretation-2408.16767v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CSGO: Content-Style Composition in Text-to-Image Generation</h3>
                <p>Authors: Peng XingHaofan WangYanpeng SunQixun WangXu BaiHao AiRenyuan HuangZechao Li</p>
                <p><a href="http://arxiv.org/abs/2408.16766v1">Link to paper</a></p>
                <p>The diffusion model has shown exceptional capabilities in controlled imagegeneration which has further fueled interest in image style transfer. Existingworks mainly focus on training free-based methods e.g. image inversion dueto the scarcity of specific data. In this study we present a data constructionpipeline for content-style-stylized image triplets that generates andautomatically cleanses stylized data triplets. Based on this pipeline weconstruct a dataset IMAGStyle the first large-scale style transfer datasetcontaining 210k image triplets available for the community to explore andresearch. Equipped with IMAGStyle we propose CSGO a style transfer modelbased on end-to-end training which explicitly decouples content and stylefeatures employing independent feature injection. The unified CSGO implementsimage-driven style transfer text-driven stylized synthesis and textediting-driven stylized synthesis. Extensive experiments demonstrate theeffectiveness of our approach in enhancing style control capabilities in imagegeneration. Additional visualization and access to the source code can belocated on the project page: urlhttps://csgo-gen.github.io/.</p>
                <p>Last Updated: 2024-08-29 17:59:30 UTC</p>
                <button class="interpret-button" data-id="2408.16766v1">Interpret</button>
                <div id="interpretation-2408.16766v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>A Score-Based Density Formula, with Applications in Diffusion Generative Models</h3>
                <p>Authors: Gen LiYuling Yan</p>
                <p><a href="http://arxiv.org/abs/2408.16765v1">Link to paper</a></p>
                <p>Score-based generative models SGMs have revolutionized the field ofgenerative modeling achieving unprecedented success in generating realisticand diverse content. Despite empirical advances the theoretical basis for whyoptimizing the evidence lower bound ELBO on the log-likelihood is effectivefor training diffusion generative models such as DDPMs remains largelyunexplored. In this paper we address this question by establishing a densityformula for a continuous-time diffusion process which can be viewed as thecontinuous-time limit of the forward process in an SGM. This formula revealsthe connection between the target density and the score function associatedwith each step of the forward process. Building on this we demonstrate thatthe minimizer of the optimization objective for training DDPMs nearly coincideswith that of the true objective providing a theoretical foundation foroptimizing DDPMs using the ELBO. Furthermore we offer new insights into therole of score-matching regularization in training GANs the use of ELBO indiffusion classifiers and the recently proposed diffusion loss.</p>
                <p>Last Updated: 2024-08-29 17:59:07 UTC</p>
                <button class="interpret-button" data-id="2408.16765v1">Interpret</button>
                <div id="interpretation-2408.16765v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>UV-free Texture Generation with Denoising and Geodesic Heat Diffusions</h3>
                <p>Authors: Simone FotiStefanos ZafeiriouTolga Birdal</p>
                <p><a href="http://arxiv.org/abs/2408.16762v1">Link to paper</a></p>
                <p>Seams distortions wasted UV space vertex-duplication and varyingresolution over the surface are the most prominent issues of the standardUV-based texturing of meshes. These issues are particularly acute whenautomatic UV-unwrapping techniques are used. For this reason instead ofgenerating textures in automatically generated UV-planes like moststate-of-the-art methods we propose to represent textures as colouredpoint-clouds whose colours are generated by a denoising diffusion probabilisticmodel constrained to operate on the surface of 3D objects. Our sampling andresolution agnostic generative model heavily relies on heat diffusion over thesurface of the meshes for spatial communication between points. To enableprocessing of arbitrarily sampled point-cloud textures and ensure long-distancetexture consistency we introduce a fast re-sampling of the mesh spectralproperties used during the heat diffusion and introduce a novelheat-diffusion-based self-attention mechanism. Our code and pre-trained modelsare available at github.com/simofoti/UV3-TeD.</p>
                <p>Last Updated: 2024-08-29 17:57:05 UTC</p>
                <button class="interpret-button" data-id="2408.16762v1">Interpret</button>
                <div id="interpretation-2408.16762v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning of Large Language Models</h3>
                <p>Authors: Alec Solway</p>
                <p><a href="http://arxiv.org/abs/2408.16753v1">Link to paper</a></p>
                <p>Reinforcement learning is used to align language models with human preferencesignals after first pre-training the model to predict the next token of textwithin a large corpus using likelihood maximization. Before being deployed in aspecific domain models are often further fine-tuned on task specific data.Since human preferences are often unavailable for the last step it isperformed using likelihood maximization as that is the typical default method.However reinforcement learning has other advantages besides facilitatingalignment to a human derived reward function. For one whereas likelihoodmaximization is a form of imitation learning in which the model is trained onwhat to do under ideal conditions reinforcement learning is not limited todemonstrating actions just for optimally reached states and trains a model whatto do under a range of scenarios as it explores the policy space. In additionit also trains a model what not to do suppressing competitive but pooractions. This work develops a framework for last-mile fine-tuning usingreinforcement learning and tests whether it garners performance gains. Theexperiments center on abstractive summarization but the framework is generaland broadly applicable. Use of the procedure produced significantly betterresults than likelihood maximization when comparing raw predictions. For thespecific data tested the gap could be bridged by employing post-processing ofthe maximum likelihood outputs. Nonetheless the framework offers a new avenuefor model optimization in situations where post-processing may be lessstraightforward or effective and it can be extended to include more complexclasses of undesirable outputs to penalize and train against such ashallucinations.</p>
                <p>Last Updated: 2024-08-29 17:49:18 UTC</p>
                <button class="interpret-button" data-id="2408.16753v1">Interpret</button>
                <div id="interpretation-2408.16753v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models</h3>
                <p>Authors: Yi-Lin TuanWilliam Yang Wang</p>
                <p><a href="http://arxiv.org/abs/2408.16751v1">Link to paper</a></p>
                <p>Beyond maximum likelihood estimation MLE the standard objective of alanguage model LM that optimizes good examples probabilities many studieshave explored ways that also penalize bad examples for enhancing the quality ofoutput distribution including unlikelihood training exponential maximizingaverage treatment effect ExMATE and direct preference optimization DPO. Tosystematically compare these methods and further provide a unified recipe forLM optimization in this paper we present a unique angle of gradient analysisof loss functions that simultaneously reward good examples and penalize badones in LMs. Through both mathematical results and experiments onCausalDialogue and Anthropic HH-RLHF datasets we identify distinct functionalcharacteristics among these methods. We find that ExMATE serves as a superiorsurrogate for MLE and that combining DPO with ExMATE instead of MLE furtherenhances both the statistical 5-7 and generative 18 win rateperformance.</p>
                <p>Last Updated: 2024-08-29 17:46:18 UTC</p>
                <button class="interpret-button" data-id="2408.16751v1">Interpret</button>
                <div id="interpretation-2408.16751v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</h3>
                <p>Authors: Zhifei XieChangqiao Wu</p>
                <p><a href="http://arxiv.org/abs/2408.16725v2">Link to paper</a></p>
                <p>Recent advances in language models have achieved significant progress.GPT-4o as a new milestone has enabled real-time conversations with humansdemonstrating near-human natural fluency. Such human-computer interactionnecessitates models with the capability to perform reasoning directly with theaudio modality and generate output in streaming. However this remains beyondthe reach of current academic models as they typically depend on extra TTSsystems for speech synthesis resulting in undesirable latency. This paperintroduces the Mini-Omni an audio-based end-to-end conversational modelcapable of real-time speech interaction. To achieve this capability we proposea text-instructed speech generation method along with batch-parallelstrategies during inference to further boost the performance. Our method alsohelps to retain the original models language capabilities with minimaldegradation enabling other works to establish real-time interactioncapabilities. We call this training method Any Model Can Talk. We alsointroduce the VoiceAssistant-400K dataset to fine-tune models optimized forspeech output. To our best knowledge Mini-Omni is the first fully end-to-endopen-source model for real-time speech interaction offering valuable potentialfor future research.</p>
                <p>Last Updated: 2024-08-30 02:53:48 UTC</p>
                <button class="interpret-button" data-id="2408.16725v2">Interpret</button>
                <div id="interpretation-2408.16725v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Auricular Vagus Nerve Stimulation for Enhancing Remote Pilot Training and Operations</h3>
                <p>Authors: William J. Tyler</p>
                <p><a href="http://arxiv.org/abs/2408.16755v1">Link to paper</a></p>
                <p>The rapid growth of the drone industry particularly in the use of smallunmanned aerial systems sUAS and unmanned aerial vehicles UAVs requiresthe development of advanced training protocols for remote pilots. Remote pilotsmust develop a combination of technical and cognitive skills to manage thecomplexities of modern drone operations. This paper explores the integration ofneurotechnology specifically auricular vagus nerve stimulation aVNS as amethod to enhance remote pilot training and performance. The scientificliterature shows aVNS can safely improve cognitive functions such as attentionlearning and memory. It has also been shown useful to manage stress responses.For safe and efficient sUAS/UAV operation it is essential for pilots tomaintain high levels of vigilance and decision-making under pressure. Bymodulating sympathetic stress and cortical arousal aVNS can prime cognitivefaculties before training help maintain focus during training and improvestress recovery post-training. Furthermore aVNS has demonstrated the potentialto enhance multitasking and cognitive control. This may help remote pilotsduring complex sUAS operations by potentially reducing the risk of impulsivedecision-making or cognitive errors. This paper advocates for the inclusion ofaVNS in remote pilot training programs by proposing that it can providesignificant benefits in improving cognitive readiness skill and knowledgeacquisition as well as operational safety and efficiency. Future researchshould focus on optimizing aVNS protocols for drone pilots while assessinglong-term benefits to industrial safety and workforce readiness in real-worldscenarios.</p>
                <p>Last Updated: 2024-08-29 17:53:46 UTC</p>
                <button class="interpret-button" data-id="2408.16755v1">Interpret</button>
                <div id="interpretation-2408.16755v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</h3>
                <p>Authors: Zhifei XieChangqiao Wu</p>
                <p><a href="http://arxiv.org/abs/2408.16725v2">Link to paper</a></p>
                <p>Recent advances in language models have achieved significant progress.GPT-4o as a new milestone has enabled real-time conversations with humansdemonstrating near-human natural fluency. Such human-computer interactionnecessitates models with the capability to perform reasoning directly with theaudio modality and generate output in streaming. However this remains beyondthe reach of current academic models as they typically depend on extra TTSsystems for speech synthesis resulting in undesirable latency. This paperintroduces the Mini-Omni an audio-based end-to-end conversational modelcapable of real-time speech interaction. To achieve this capability we proposea text-instructed speech generation method along with batch-parallelstrategies during inference to further boost the performance. Our method alsohelps to retain the original models language capabilities with minimaldegradation enabling other works to establish real-time interactioncapabilities. We call this training method Any Model Can Talk. We alsointroduce the VoiceAssistant-400K dataset to fine-tune models optimized forspeech output. To our best knowledge Mini-Omni is the first fully end-to-endopen-source model for real-time speech interaction offering valuable potentialfor future research.</p>
                <p>Last Updated: 2024-08-30 02:53:48 UTC</p>
                <button class="interpret-button" data-id="2408.16725v2">Interpret</button>
                <div id="interpretation-2408.16725v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VMC: A Grammar for Visualizing Statistical Model Checks</h3>
                <p>Authors: Ziyang GuoAlex KaleMatthew KayJessica Hullman</p>
                <p><a href="http://arxiv.org/abs/2408.16702v1">Link to paper</a></p>
                <p>Visualizations play a critical role in validating and improving statisticalmodels. However the design space of model check visualizations is not wellunderstood making it difficult for authors to explore and specify effectivegraphical model checks. VMC defines a model check visualization using fourcomponents: 1 samples of distributions of checkable quantities generated fromthe model including predictive distributions for new data and distributions ofmodel parameters 2 transformations on observed data to facilitatecomparison 3 visual representations of distributions and 4 layouts tofacilitate comparing model samples and observed data. We contribute animplementation of VMC as an R package. We validate VMC by reproducing a set ofcanonical model check examples and show how using VMC to generate model checksreduces the edit distance between visualizations relative to existingvisualization toolkits. The findings of an interview study with three expertmodelers who used VMC highlight challenges and opportunities for encouragingexploration of correct effective model check visualizations.</p>
                <p>Last Updated: 2024-08-29 16:56:35 UTC</p>
                <button class="interpret-button" data-id="2408.16702v1">Interpret</button>
                <div id="interpretation-2408.16702v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fostering Creative Visualisation Skills Through Data-Art Exhibitions</h3>
                <p>Authors: Jonathan C. Roberts</p>
                <p><a href="http://arxiv.org/abs/2408.16479v1">Link to paper</a></p>
                <p>Data-art exhibitions offer a unique and real-world setting to foster creativevisualisation skills among students. They serve as real-world platform forstudents to display their work bridging the gap between classroom learning andprofessional practice. Students must develop a technical solution grasp thecontext and produce work that is appropriate for public presentation. Thisscenario helps to encourage innovative thinking engagement with the topic andhelps to enhance technical proficiency. We present our implementation of adata-art exhibition within a computing curriculum for third-year degree-levelstudents. Students create art-based visualisations from selected datasets andpresent their work in a public exhibition. We have used this initiative overthe course of two academic years with different cohorts and reflect on itsimpact on student learning and creativity.</p>
                <p>Last Updated: 2024-08-29 12:16:13 UTC</p>
                <button class="interpret-button" data-id="2408.16479v1">Interpret</button>
                <div id="interpretation-2408.16479v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Human and LLM-Based Voice Assistant Interaction: An Analytical Framework for User Verbal and Nonverbal Behaviors</h3>
                <p>Authors: Szeyi ChanShihan FuJiachen LiBingsheng YaoSmit DesaiMirjana PrpaDakuo Wang</p>
                <p><a href="http://arxiv.org/abs/2408.16465v1">Link to paper</a></p>
                <p>Recent progress in large language model LLM technology has significantlyenhanced the interaction experience between humans and voice assistants VAs.This project aims to explore a users continuous interaction with LLM-based VALLM-VA during a complex task. We recruited 12 participants to interact withan LLM-VA during a cooking task selected for its complexity and therequirement for continuous interaction. We observed that users show both verbaland nonverbal behaviors though they know that the LLM-VA can not capture thosenonverbal signals. Despite the prevalence of nonverbal behavior in human-humancommunication there is no established analytical methodology or framework forexploring it in human-VA interactions. After analyzing 3 hours and 39 minutesof video recordings we developed an analytical framework with threedimensions: 1 behavior characteristics including both verbal and nonverbalbehaviors 2 interaction stages--exploration conflict and integration--thatillustrate the progression of user interactions and 3 stage transitionthroughout the task. This analytical framework identifies key verbal andnonverbal behaviors that provide a foundation for future research and practicalapplications in optimizing human and LLM-VA interactions.</p>
                <p>Last Updated: 2024-08-29 11:54:02 UTC</p>
                <button class="interpret-button" data-id="2408.16465v1">Interpret</button>
                <div id="interpretation-2408.16465v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</h3>
                <p>Authors: Ziyu GuoRenrui ZhangXiangyang ZhuChengzhuo TongPeng GaoChunyuan LiPheng-Ann Heng</p>
                <p><a href="http://arxiv.org/abs/2408.16768v1">Link to paper</a></p>
                <p>We introduce SAM2Point a preliminary exploration adapting Segment AnythingModel 2 SAM 2 for zero-shot and promptable 3D segmentation. SAM2Pointinterprets any 3D data as a series of multi-directional videos and leveragesSAM 2 for 3D-space segmentation without further training or 2D-3D projection.Our framework supports various prompt types including 3D points boxes andmasks and can generalize across diverse scenarios such as 3D objects indoorscenes outdoor environments and raw sparse LiDAR. Demonstrations on multiple3D datasets e.g. Objaverse S3DIS ScanNet Semantic3D and KITTI highlightthe robust generalization capabilities of SAM2Point. To our best knowledge wepresent the most faithful implementation of SAM in 3D which may serve as astarting point for future research in promptable 3D segmentation. Online Demo:https://huggingface.co/spaces/ZiyuG/SAM2Point . Code:https://github.com/ZiyuGuo99/SAM2Point .</p>
                <p>Last Updated: 2024-08-29 17:59:45 UTC</p>
                <button class="interpret-button" data-id="2408.16768v1">Interpret</button>
                <div id="interpretation-2408.16768v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>How Far Can Cantonese NLP Go? Benchmarking Cantonese Capabilities of Large Language Models</h3>
                <p>Authors: Jiyue JiangLiheng ChenPengan ChenSheng WangQinghang BaoLingpeng KongYu LiChuan Wu</p>
                <p><a href="http://arxiv.org/abs/2408.16756v1">Link to paper</a></p>
                <p>The rapid evolution of large language models LLMs has transformed thecompetitive landscape in natural language processing NLP particularly forEnglish and other data-rich languages. However underrepresented languages likeCantonese spoken by over 85 million people face significant development gapswhich is particularly concerning given the economic significance of theGuangdong-Hong Kong-Macau Greater Bay Area and in substantialCantonese-speaking populations in places like Singapore and North America.Despite its wide use Cantonese has scant representation in NLP researchespecially compared to other languages from similarly developed regions. Tobridge these gaps we outline current Cantonese NLP methods and introduce newbenchmarks designed to evaluate LLM performance in factual generationmathematical logic complex reasoning and general knowledge in Cantonesewhich aim to advance open-source Cantonese LLM technology. We also proposefuture research directions and recommended models to enhance Cantonese LLMdevelopment.</p>
                <p>Last Updated: 2024-08-29 17:54:14 UTC</p>
                <button class="interpret-button" data-id="2408.16756v1">Interpret</button>
                <div id="interpretation-2408.16756v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning of Large Language Models</h3>
                <p>Authors: Alec Solway</p>
                <p><a href="http://arxiv.org/abs/2408.16753v1">Link to paper</a></p>
                <p>Reinforcement learning is used to align language models with human preferencesignals after first pre-training the model to predict the next token of textwithin a large corpus using likelihood maximization. Before being deployed in aspecific domain models are often further fine-tuned on task specific data.Since human preferences are often unavailable for the last step it isperformed using likelihood maximization as that is the typical default method.However reinforcement learning has other advantages besides facilitatingalignment to a human derived reward function. For one whereas likelihoodmaximization is a form of imitation learning in which the model is trained onwhat to do under ideal conditions reinforcement learning is not limited todemonstrating actions just for optimally reached states and trains a model whatto do under a range of scenarios as it explores the policy space. In additionit also trains a model what not to do suppressing competitive but pooractions. This work develops a framework for last-mile fine-tuning usingreinforcement learning and tests whether it garners performance gains. Theexperiments center on abstractive summarization but the framework is generaland broadly applicable. Use of the procedure produced significantly betterresults than likelihood maximization when comparing raw predictions. For thespecific data tested the gap could be bridged by employing post-processing ofthe maximum likelihood outputs. Nonetheless the framework offers a new avenuefor model optimization in situations where post-processing may be lessstraightforward or effective and it can be extended to include more complexclasses of undesirable outputs to penalize and train against such ashallucinations.</p>
                <p>Last Updated: 2024-08-29 17:49:18 UTC</p>
                <button class="interpret-button" data-id="2408.16753v1">Interpret</button>
                <div id="interpretation-2408.16753v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models</h3>
                <p>Authors: Yi-Lin TuanWilliam Yang Wang</p>
                <p><a href="http://arxiv.org/abs/2408.16751v1">Link to paper</a></p>
                <p>Beyond maximum likelihood estimation MLE the standard objective of alanguage model LM that optimizes good examples probabilities many studieshave explored ways that also penalize bad examples for enhancing the quality ofoutput distribution including unlikelihood training exponential maximizingaverage treatment effect ExMATE and direct preference optimization DPO. Tosystematically compare these methods and further provide a unified recipe forLM optimization in this paper we present a unique angle of gradient analysisof loss functions that simultaneously reward good examples and penalize badones in LMs. Through both mathematical results and experiments onCausalDialogue and Anthropic HH-RLHF datasets we identify distinct functionalcharacteristics among these methods. We find that ExMATE serves as a superiorsurrogate for MLE and that combining DPO with ExMATE instead of MLE furtherenhances both the statistical 5-7 and generative 18 win rateperformance.</p>
                <p>Last Updated: 2024-08-29 17:46:18 UTC</p>
                <button class="interpret-button" data-id="2408.16751v1">Interpret</button>
                <div id="interpretation-2408.16751v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge</h3>
                <p>Authors: Beidi DongJin R. LeeZiwei ZhuBalassubramanian Srinivasan</p>
                <p><a href="http://arxiv.org/abs/2408.16749v1">Link to paper</a></p>
                <p>The United States has experienced a significant increase in violentextremism prompting the need for automated tools to detect and limit thespread of extremist ideology online. This study evaluates the performance ofBidirectional Encoder Representations from Transformers BERT and GenerativePre-Trained Transformers GPT in detecting and classifying online domesticextremist posts. We collected social media posts containing far-right andfar-left ideological keywords and manually labeled them as extremist ornon-extremist. Extremist posts were further classified into one or more of fivecontributing elements of extremism based on a working definitional framework.The BERT models performance was evaluated based on training data size andknowledge transfer between categories. We also compared the performance of GPT3.5 and GPT 4 models using different prompts: naive layperson-definitionrole-playing and professional-definition. Results showed that the bestperforming GPT models outperformed the best performing BERT models with moredetailed prompts generally yielding better results. However overly complexprompts may impair performance. Different versions of GPT have uniquesensitives to what they consider extremist. GPT 3.5 performed better atclassifying far-left extremist posts while GPT 4 performed better atclassifying far-right extremist posts. Large language models represented byGPT models hold significant potential for online extremism classificationtasks surpassing traditional BERT models in a zero-shot setting. Futureresearch should explore human-computer interactions in optimizing GPT modelsfor extremist detection and classification tasks to develop more efficiente.g. quicker less effort and effective e.g. fewer errors or mistakesmethods for identifying extremist content.</p>
                <p>Last Updated: 2024-08-29 17:43:03 UTC</p>
                <button class="interpret-button" data-id="2408.16749v1">Interpret</button>
                <div id="interpretation-2408.16749v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>A Score-Based Density Formula, with Applications in Diffusion Generative Models</h3>
                <p>Authors: Gen LiYuling Yan</p>
                <p><a href="http://arxiv.org/abs/2408.16765v1">Link to paper</a></p>
                <p>Score-based generative models SGMs have revolutionized the field ofgenerative modeling achieving unprecedented success in generating realisticand diverse content. Despite empirical advances the theoretical basis for whyoptimizing the evidence lower bound ELBO on the log-likelihood is effectivefor training diffusion generative models such as DDPMs remains largelyunexplored. In this paper we address this question by establishing a densityformula for a continuous-time diffusion process which can be viewed as thecontinuous-time limit of the forward process in an SGM. This formula revealsthe connection between the target density and the score function associatedwith each step of the forward process. Building on this we demonstrate thatthe minimizer of the optimization objective for training DDPMs nearly coincideswith that of the true objective providing a theoretical foundation foroptimizing DDPMs using the ELBO. Furthermore we offer new insights into therole of score-matching regularization in training GANs the use of ELBO indiffusion classifiers and the recently proposed diffusion loss.</p>
                <p>Last Updated: 2024-08-29 17:59:07 UTC</p>
                <button class="interpret-button" data-id="2408.16765v1">Interpret</button>
                <div id="interpretation-2408.16765v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models</h3>
                <p>Authors: Yi-Lin TuanWilliam Yang Wang</p>
                <p><a href="http://arxiv.org/abs/2408.16751v1">Link to paper</a></p>
                <p>Beyond maximum likelihood estimation MLE the standard objective of alanguage model LM that optimizes good examples probabilities many studieshave explored ways that also penalize bad examples for enhancing the quality ofoutput distribution including unlikelihood training exponential maximizingaverage treatment effect ExMATE and direct preference optimization DPO. Tosystematically compare these methods and further provide a unified recipe forLM optimization in this paper we present a unique angle of gradient analysisof loss functions that simultaneously reward good examples and penalize badones in LMs. Through both mathematical results and experiments onCausalDialogue and Anthropic HH-RLHF datasets we identify distinct functionalcharacteristics among these methods. We find that ExMATE serves as a superiorsurrogate for MLE and that combining DPO with ExMATE instead of MLE furtherenhances both the statistical 5-7 and generative 18 win rateperformance.</p>
                <p>Last Updated: 2024-08-29 17:46:18 UTC</p>
                <button class="interpret-button" data-id="2408.16751v1">Interpret</button>
                <div id="interpretation-2408.16751v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Statistical and Geometrical properties of regularized Kernel Kullback-Leibler divergence</h3>
                <p>Authors: ClÃ©mentine ChazalAnna KorbaFrancis Bach</p>
                <p><a href="http://arxiv.org/abs/2408.16543v1">Link to paper</a></p>
                <p>In this paper we study the statistical and geometrical properties of theKullback-Leibler divergence with kernel covariance operators KKL introducedby Bach 2022. Unlike the classical Kullback-Leibler KL divergence thatinvolves density ratios the KKL compares probability distributions throughcovariance operators embeddings in a reproducible kernel Hilbert spaceRKHS and compute the Kullback-Leibler quantum divergence. This noveldivergence hence shares parallel but different aspects with both the standardKullback-Leibler between probability distributions and kernel embeddingsmetrics such as the maximum mean discrepancy. A limitation faced with theoriginal KKL divergence is its inability to be defined for distributions withdisjoint supports. To solve this problem we propose in this paper aregularised variant that guarantees that the divergence is well defined for alldistributions. We derive bounds that quantify the deviation of the regularisedKKL to the original one as well as finite-sample bounds. In addition weprovide a closed-form expression for the regularised KKL specificallyapplicable when the distributions consist of finite sets of points which makesit implementable. Furthermore we derive a Wasserstein gradient descent schemeof the KKL divergence in the case of discrete distributions and studyempirically its properties to transport a set of points to a targetdistribution.</p>
                <p>Last Updated: 2024-08-29 14:01:30 UTC</p>
                <button class="interpret-button" data-id="2408.16543v1">Interpret</button>
                <div id="interpretation-2408.16543v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Gradient-free variational learning with conditional mixture networks</h3>
                <p>Authors: Conor HeinsHao WuDimitrije MarkovicAlexander TschantzJeff BeckChristopher Buckley</p>
                <p><a href="http://arxiv.org/abs/2408.16429v1">Link to paper</a></p>
                <p>Balancing computational efficiency with robust predictive performance iscrucial in supervised learning especially for critical applications. Standarddeep learning models while accurate and scalable often lack probabilisticfeatures like calibrated predictions and uncertainty quantification. Bayesianmethods address these issues but can be computationally expensive as model anddata complexity increase. Previous work shows that fast variational methods canreduce the compute requirements of Bayesian methods by eliminating the need forgradient computation or sampling but are often limited to simple models. Wedemonstrate that conditional mixture networks CMNs a probabilistic variantof the mixture-of-experts MoE model are suitable for fast gradient-freeinference and can solve complex classification tasks. CMNs employ linearexperts and a softmax gating network. By exploiting conditional conjugacy andPolya-Gamma augmentation we furnish Gaussian likelihoods for the weights ofboth the linear experts and the gating network. This enables efficientvariational updates using coordinate ascent variational inference CAVIavoiding traditional gradient-based optimization. We validate this approach bytraining two-layer CMNs on standard benchmarks from the UCI repository. Ourmethod CAVI-CMN achieves competitive and often superior predictive accuracycompared to maximum likelihood estimation MLE with backpropagation whilemaintaining competitive runtime and full posterior distributions over all modelparameters. Moreover as input size or the number of experts increasescomputation time scales competitively with MLE and other gradient-basedsolutions like black-box variational inference BBVI making CAVI-CMN apromising tool for deep fast and gradient-free Bayesian networks.</p>
                <p>Last Updated: 2024-08-29 10:43:55 UTC</p>
                <button class="interpret-button" data-id="2408.16429v1">Interpret</button>
                <div id="interpretation-2408.16429v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Iterated Energy-based Flow Matching for Sampling from Boltzmann Densities</h3>
                <p>Authors: Dongyeop WooSungsoo Ahn</p>
                <p><a href="http://arxiv.org/abs/2408.16249v1">Link to paper</a></p>
                <p>In this work we consider the problem of training a generator fromevaluations of energy functions or unnormalized densities. This is afundamental problem in probabilistic inference which is crucial for scientificapplications such as learning the 3D coordinate distribution of a molecule. Tosolve this problem we propose iterated energy-based flow matching iEFM thefirst off-policy approach to train continuous normalizing flow CNF modelsfrom unnormalized densities. We introduce the simulation-free energy-based flowmatching objective which trains the model to predict the Monte Carloestimation of the marginal vector field constructed from known energyfunctions. Our framework is general and can be extended to variance-explodingVE and optimal transport OT conditional probability paths. We evaluate iEFMon a two-dimensional Gaussian mixture model GMM and an eight-dimensionalfour-particle double-well potential DW-4 energy function. Our resultsdemonstrate that iEFM outperforms existing methods showcasing its potentialfor efficient and scalable probabilistic modeling in complex high-dimensionalsystems.</p>
                <p>Last Updated: 2024-08-29 04:06:34 UTC</p>
                <button class="interpret-button" data-id="2408.16249v1">Interpret</button>
                <div id="interpretation-2408.16249v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners</h3>
                <p>Authors: Ziyu GuoRenrui ZhangXiangyang ZhuChengzhuo TongPeng GaoChunyuan LiPheng-Ann Heng</p>
                <p><a href="http://arxiv.org/abs/2408.16768v1">Link to paper</a></p>
                <p>We introduce SAM2Point a preliminary exploration adapting Segment AnythingModel 2 SAM 2 for zero-shot and promptable 3D segmentation. SAM2Pointinterprets any 3D data as a series of multi-directional videos and leveragesSAM 2 for 3D-space segmentation without further training or 2D-3D projection.Our framework supports various prompt types including 3D points boxes andmasks and can generalize across diverse scenarios such as 3D objects indoorscenes outdoor environments and raw sparse LiDAR. Demonstrations on multiple3D datasets e.g. Objaverse S3DIS ScanNet Semantic3D and KITTI highlightthe robust generalization capabilities of SAM2Point. To our best knowledge wepresent the most faithful implementation of SAM in 3D which may serve as astarting point for future research in promptable 3D segmentation. Online Demo:https://huggingface.co/spaces/ZiyuG/SAM2Point . Code:https://github.com/ZiyuGuo99/SAM2Point .</p>
                <p>Last Updated: 2024-08-29 17:59:45 UTC</p>
                <button class="interpret-button" data-id="2408.16768v1">Interpret</button>
                <div id="interpretation-2408.16768v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model</h3>
                <p>Authors: Fangfu LiuWenqiang SunHanyang WangYikai WangHaowen SunJunliang YeJun ZhangYueqi Duan</p>
                <p><a href="http://arxiv.org/abs/2408.16767v1">Link to paper</a></p>
                <p>Advancements in 3D scene reconstruction have transformed 2D images from thereal world into 3D models producing realistic 3D results from hundreds ofinput photos. Despite great success in dense-view reconstruction scenariosrendering a detailed scene from insufficient captured views is still anill-posed optimization problem often resulting in artifacts and distortions inunseen areas. In this paper we propose ReconX a novel 3D scene reconstructionparadigm that reframes the ambiguous reconstruction challenge as a temporalgeneration task. The key insight is to unleash the strong generative prior oflarge pre-trained video diffusion models for sparse-view reconstruction.However 3D view consistency struggles to be accurately preserved in directlygenerated video frames from pre-trained models. To address this given limitedinput views the proposed ReconX first constructs a global point cloud andencodes it into a contextual space as the 3D structure condition. Guided by thecondition the video diffusion model then synthesizes video frames that areboth detail-preserved and exhibit a high degree of 3D consistency ensuring thecoherence of the scene from various perspectives. Finally we recover the 3Dscene from the generated video through a confidence-aware 3D Gaussian Splattingoptimization scheme. Extensive experiments on various real-world datasets showthe superiority of our ReconX over state-of-the-art methods in terms of qualityand generalizability.</p>
                <p>Last Updated: 2024-08-29 17:59:40 UTC</p>
                <button class="interpret-button" data-id="2408.16767v1">Interpret</button>
                <div id="interpretation-2408.16767v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Score-Based Density Formula, with Applications in Diffusion Generative Models</h3>
                <p>Authors: Gen LiYuling Yan</p>
                <p><a href="http://arxiv.org/abs/2408.16765v1">Link to paper</a></p>
                <p>Score-based generative models SGMs have revolutionized the field ofgenerative modeling achieving unprecedented success in generating realisticand diverse content. Despite empirical advances the theoretical basis for whyoptimizing the evidence lower bound ELBO on the log-likelihood is effectivefor training diffusion generative models such as DDPMs remains largelyunexplored. In this paper we address this question by establishing a densityformula for a continuous-time diffusion process which can be viewed as thecontinuous-time limit of the forward process in an SGM. This formula revealsthe connection between the target density and the score function associatedwith each step of the forward process. Building on this we demonstrate thatthe minimizer of the optimization objective for training DDPMs nearly coincideswith that of the true objective providing a theoretical foundation foroptimizing DDPMs using the ELBO. Furthermore we offer new insights into therole of score-matching regularization in training GANs the use of ELBO indiffusion classifiers and the recently proposed diffusion loss.</p>
                <p>Last Updated: 2024-08-29 17:59:07 UTC</p>
                <button class="interpret-button" data-id="2408.16765v1">Interpret</button>
                <div id="interpretation-2408.16765v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical Analysis of Methods and Benchmarks</h3>
                <p>Authors: Hongjun WangSagar VazeKai Han</p>
                <p><a href="http://arxiv.org/abs/2408.16757v2">Link to paper</a></p>
                <p>Detecting test-time distribution shift has emerged as a key capability forsafely deployed machine learning models with the question being tackled undervarious guises in recent years. In this paper we aim to provide a consolidatedview of the two largest sub-fields within the community: out-of-distributionOOD detection and open-set recognition OSR. In particular we aim toprovide rigorous empirical analysis of different methods across settings andprovide actionable takeaways for practitioners and researchers. Concretely wemake the following contributions: i We perform rigorous cross-evaluationbetween state-of-the-art methods in the OOD detection and OSR settings andidentify a strong correlation between the performances of methods for themii We propose a new large-scale benchmark setting which we suggest betterdisentangles the problem tackled by OOD detection and OSR re-evaluatingstate-of-the-art OOD detection and OSR methods in this setting iii Wesurprisingly find that the best performing method on standard benchmarksOutlier Exposure struggles when tested at scale while scoring rules whichare sensitive to the deep feature magnitude consistently show promise and ivWe conduct empirical analysis to explain these phenomena and highlightdirections for future research. Code:https://github.com/Visual-AI/Dissect-OOD-OSR</p>
                <p>Last Updated: 2024-08-30 02:26:01 UTC</p>
                <button class="interpret-button" data-id="2408.16757v2">Interpret</button>
                <div id="interpretation-2408.16757v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge</h3>
                <p>Authors: Beidi DongJin R. LeeZiwei ZhuBalassubramanian Srinivasan</p>
                <p><a href="http://arxiv.org/abs/2408.16749v1">Link to paper</a></p>
                <p>The United States has experienced a significant increase in violentextremism prompting the need for automated tools to detect and limit thespread of extremist ideology online. This study evaluates the performance ofBidirectional Encoder Representations from Transformers BERT and GenerativePre-Trained Transformers GPT in detecting and classifying online domesticextremist posts. We collected social media posts containing far-right andfar-left ideological keywords and manually labeled them as extremist ornon-extremist. Extremist posts were further classified into one or more of fivecontributing elements of extremism based on a working definitional framework.The BERT models performance was evaluated based on training data size andknowledge transfer between categories. We also compared the performance of GPT3.5 and GPT 4 models using different prompts: naive layperson-definitionrole-playing and professional-definition. Results showed that the bestperforming GPT models outperformed the best performing BERT models with moredetailed prompts generally yielding better results. However overly complexprompts may impair performance. Different versions of GPT have uniquesensitives to what they consider extremist. GPT 3.5 performed better atclassifying far-left extremist posts while GPT 4 performed better atclassifying far-right extremist posts. Large language models represented byGPT models hold significant potential for online extremism classificationtasks surpassing traditional BERT models in a zero-shot setting. Futureresearch should explore human-computer interactions in optimizing GPT modelsfor extremist detection and classification tasks to develop more efficiente.g. quicker less effort and effective e.g. fewer errors or mistakesmethods for identifying extremist content.</p>
                <p>Last Updated: 2024-08-29 17:43:03 UTC</p>
                <button class="interpret-button" data-id="2408.16749v1">Interpret</button>
                <div id="interpretation-2408.16749v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-09-02</p>
        </div>
    
        </div>
    </body>
    </html>
    