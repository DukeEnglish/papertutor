
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Boundary Detection Algorithm Inspired by Locally Linear Embedding</h3>
                <p>Authors: Pei-Cheng KuoNan Wu</p>
                <p><a href="http://arxiv.org/abs/2406.18456v1">Link to paper</a></p>
                <p>In the study of high-dimensional data it is often assumed that the data setpossesses an underlying lower-dimensional structure. A practical model for thisstructure is an embedded compact manifold with boundary. Since the underlyingmanifold structure is typically unknown identifying boundary points from thedata distributed on the manifold is crucial for various applications. In thiswork we propose a method for detecting boundary points inspired by the widelyused locally linear embedding algorithm. We implement this method using twonearest neighborhood search schemes: the epsilon-radius ball scheme and theK-nearest neighbor scheme. This algorithm incorporates the geometricinformation of the data structure particularly through its close relation withthe local covariance matrix. We discuss the selection the key parameter andanalyze the algorithm through our exploration of the spectral properties of thelocal covariance matrix in both neighborhood search schemes. Furthermore wedemonstrate the algorithms performance with simulated examples.</p>
                <p>Last Updated: 2024-06-26 16:05:57 UTC</p>
                <button class="interpret-button" data-id="2406.18456v1">Interpret</button>
                <div id="interpretation-2406.18456v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers</h3>
                <p>Authors: Yibo JiangGoutham RajendranPradeep RavikumarBryon Aragam</p>
                <p><a href="http://arxiv.org/abs/2406.18400v1">Link to paper</a></p>
                <p>Large Language Models LLMs have the capacity to store and recall facts.Through experimentation with open-source models we observe that this abilityto retrieve facts can be easily manipulated by changing contexts even withoutaltering their factual meanings. These findings highlight that LLMs mightbehave like an associative memory model where certain tokens in the contextsserve as clues to retrieving facts. We mathematically explore this property bystudying how transformers the building blocks of LLMs can complete suchmemory tasks. We study a simple latent concept association problem with aone-layer transformer and we show theoretically and empirically that thetransformer gathers information using self-attention and uses the value matrixfor associative memory.</p>
                <p>Last Updated: 2024-06-26 14:49:54 UTC</p>
                <button class="interpret-button" data-id="2406.18400v1">Interpret</button>
                <div id="interpretation-2406.18400v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Second Maximum of a Gaussian Random Field and Exact (t-)Spacing test</h3>
                <p>Authors: Azaïs Jean-MarcDalmao FedericoDe Castro Yohann</p>
                <p><a href="http://arxiv.org/abs/2406.18397v1">Link to paper</a></p>
                <p>In this article we introduce the novel concept of the second maximum of aGaussian random field on a Riemannian submanifold. This second maximum servesas a powerful tool for characterizing the distribution of the maximum. Byutilizing an ad-hoc Kac Rice formula we derive the explicit form of themaximums distribution conditioned on the second maximum and some regressedcomponent of the Riemannian Hessian. This approach results in an exact testbased on the evaluation of spacing between these maxima which we refer to asthe spacing test.  We investigate the applicability of this test in detecting sparsealternatives within Gaussian symmetric tensors continuous sparsedeconvolution and two-layered neural networks with smooth rectifiers. Ourtheoretical results are supported by numerical experiments which illustratethe calibration and power of the proposed tests. More generally this test canbe applied to any Gaussian random field on a Riemannian manifold and weprovide a general framework for the application of the spacing test incontinuous sparse kernel regression.  Furthermore when the variance-covariance function of the Gaussian randomfield is known up to a scaling factor we derive an exact Studentized versionof our test coined the t-spacing test. This test is perfectly calibratedunder the null hypothesis and has high power for detecting sparse alternatives.</p>
                <p>Last Updated: 2024-06-26 14:44:24 UTC</p>
                <button class="interpret-button" data-id="2406.18397v1">Interpret</button>
                <div id="interpretation-2406.18397v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning pure quantum states (almost) without regret</h3>
                <p>Authors: Josep LumbrerasMikhail TerekhovMarco Tomamichel</p>
                <p><a href="http://arxiv.org/abs/2406.18370v1">Link to paper</a></p>
                <p>We initiate the study of quantum state tomography with minimal regret. Alearner has sequential oracle access to an unknown pure quantum state and ineach round selects a pure probe state. Regret is incurred if the unknown stateis measured orthogonal to this probe and the learners goal is to minimise theexpected cumulative regret over T rounds. The challenge is to find a balancebetween the most informative measurements and measurements incurring minimalregret. We show that the cumulative regret scales asThetaoperatornamepolylog T using a new tomography algorithm based on amedian of means least squares estimator. This algorithm employs measurementsbiased towards the unknown state and produces online estimates that are optimalup to logarithmic terms in the number of observed samples.</p>
                <p>Last Updated: 2024-06-26 14:13:50 UTC</p>
                <button class="interpret-button" data-id="2406.18370v1">Interpret</button>
                <div id="interpretation-2406.18370v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Efficient and Accurate Explanation Estimation with Distribution Compression</h3>
                <p>Authors: Hubert BanieckiGiuseppe CasalicchioBernd BischlPrzemyslaw Biecek</p>
                <p><a href="http://arxiv.org/abs/2406.18334v1">Link to paper</a></p>
                <p>Exact computation of various machine learning explanations requires numerousmodel evaluations and in extreme cases becomes impractical. The computationalcost of approximation increases with an ever-increasing size of data and modelparameters. Many heuristics have been proposed to approximate post-hocexplanations efficiently. This paper shows that the standard i.i.d. samplingused in a broad spectrum of algorithms for explanation estimation leads to anapproximation error worthy of improvement. To this end we introduce CompressThen Explain CTE a new paradigm for more efficient and accurate explanationestimation. CTE uses distribution compression through kernel thinning to obtaina data sample that best approximates the marginal distribution. We show thatCTE improves the estimation of removal-based local and global explanations withnegligible computational overhead. It often achieves an on-par explanationapproximation error using 2-3x less samples i.e. requiring 2-3x less modelevaluations. CTE is a simple yet powerful plug-in for any explanation methodthat now relies on i.i.d. sampling.</p>
                <p>Last Updated: 2024-06-26 13:21:24 UTC</p>
                <button class="interpret-button" data-id="2406.18334v1">Interpret</button>
                <div id="interpretation-2406.18334v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation</h3>
                <p>Authors: Ahmed NjifenjouVirgile SucalBassam JabaianFabrice Lefèvre</p>
                <p><a href="http://arxiv.org/abs/2406.18460v1">Link to paper</a></p>
                <p>Recently various methods have been proposed to create open-domainconversational agents with Large Language Models LLMs. These models are ableto answer user queries but in a one-way QA format rather than a trueconversation. Fine-tuning on particular datasets is the usual way to modifytheir style to increase conversational ability but this is expensive andusually only available in a few languages. In this study we explore role-playzero-shot prompting as an efficient and cost-effective solution for open-domainconversation using capable multilingual LLMs Beeching et al. 2023 trainedto obey instructions. We design a prompting system that when combined with aninstruction-following model - here Vicuna Chiang et al. 2023 - producesconversational agents that match and even surpass fine-tuned models in humanevaluation in French in two different tasks.</p>
                <p>Last Updated: 2024-06-26 16:10:53 UTC</p>
                <button class="interpret-button" data-id="2406.18460v1">Interpret</button>
                <div id="interpretation-2406.18460v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>An interactive framework for the evaluation and detection of stereoacuity threshold under ambient lighting</h3>
                <p>Authors: Kritika LohiaRijul Saurabh SoansRohit SaxenaTapan Kumar Gandhi</p>
                <p><a href="http://arxiv.org/abs/2406.18336v1">Link to paper</a></p>
                <p>Objective: Our study aims to provide a novel framework for the continuousevaluation of stereoacuity under ambient lighting conditions using Bayesianinference.  Methods: We applied a combination of psychophysical and expected entropyminimization procedures for the computation of a continuous stereoacuitythreshold. Subsequently we evaluated the effect of ambient lighting duringstereoacuity testing ST by adopting a bisection-matching based adaptive gammacalibration AGC. Participants N187 including visually healthy controlsN51 patients with Intermittent Divergent Squint IDS N45 andcontrols with induced anisometropia IA N91 performed ST with and withoutAGC under two lighting conditions: completely dark 20 cd/m2 and normallylit 130 cd/m2 rooms.  Results: Our framework demonstrated excellent reliability  0.9 and apositive correlation with TNO a clinical stereo test regardless of whetherAGC was conducted. However when AGC is not performed significant differencesFriedman X_r2  28.015 p0.00001 Bland-Altman bias: 30 arc-secwere found in stereoacuity thresholds between dark and light conditions forparticipants with IDS and IA. Controls are unaffected by AGC and yield asimilar stereoacuity threshold under both lighting conditions.  Conclusion: Our study proves that stereoacuity threshold is significantlydeviated particularly in participants with IDS or IA stereo-deficits if ambientlighting is not taken into consideration. Moreover our framework provides aquick approximately 5-10 minutes assessment of stereoacuity threshold and canbe performed within 30 ST and 15 AGC trials.  Significance: Our test is useful in planning treatments and monitoringprognosis for patients with stereo-deficits by accurately assessingstereovision.</p>
                <p>Last Updated: 2024-06-26 13:24:08 UTC</p>
                <button class="interpret-button" data-id="2406.18336v1">Interpret</button>
                <div id="interpretation-2406.18336v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multimodal Reaching-Position Prediction for ADL Support Using Neural Networks</h3>
                <p>Authors: Yutaka TakaseKimitoshi Yamazaki</p>
                <p><a href="http://arxiv.org/abs/2406.18162v1">Link to paper</a></p>
                <p>This study aimed to develop daily living support robots for patients withhemiplegia and the elderly. To support the daily living activities using robotsin ordinary households without imposing physical and mental burdens on usersthe system must detect the actions of the user and move appropriately accordingto their motions.  We propose a reaching-position prediction scheme that targets the motion oflifting the upper arm which is burdensome for patients with hemiplegia and theelderly in daily living activities.  For this motion it is difficult to obtain effective features to create aprediction model in environments where large-scale sensor system installationis not feasible and the motion time is short.  We performed motion-collection experiments revealed the features of thetarget motion and built a prediction model using the multimodal motion featuresand deep learning.  The proposed model achieved an accuracy of 93  macro average and F1-scoreof 0.69 for a 9-class classification prediction at 35 of the motioncompletion.</p>
                <p>Last Updated: 2024-06-26 08:23:13 UTC</p>
                <button class="interpret-button" data-id="2406.18162v1">Interpret</button>
                <div id="interpretation-2406.18162v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>BADGE: BADminton report Generation and Evaluation with LLM</h3>
                <p>Authors: Shang-Hsuan ChiangLin-Wei ChaoKuang-Da WangChih-Chuan WangWen-Chih Peng</p>
                <p><a href="http://arxiv.org/abs/2406.18116v1">Link to paper</a></p>
                <p>Badminton enjoys widespread popularity and reports on matches generallyinclude details such as player names game scores and ball types providingaudiences with a comprehensive view of the games. However writing thesereports can be a time-consuming task. This challenge led us to explore whethera Large Language Model LLM could automate the generation and evaluation ofbadminton reports. We introduce a novel framework named BADGE designed forthis purpose using LLM. Our method consists of two main phases: ReportGeneration and Report Evaluation. Initially badminton-related data isprocessed by the LLM which then generates a detailed report of the match. Wetested different Input Data Types In-Context Learning ICL and LLM findingthat GPT-4 performs best when using CSV data type and the Chain of Thoughtprompting. Following report generation the LLM evaluates and scores thereports to assess their quality. Our comparisons between the scores evaluatedby GPT-4 and human judges show a tendency to prefer GPT-4 generated reports.Since the application of LLM in badminton reporting remains largely unexploredour research serves as a foundational step for future advancements in thisarea. Moreover our method can be extended to other sports games therebyenhancing sports promotion. For more details please refer tohttps://github.com/AndyChiangSH/BADGE.</p>
                <p>Last Updated: 2024-06-26 07:07:52 UTC</p>
                <button class="interpret-button" data-id="2406.18116v1">Interpret</button>
                <div id="interpretation-2406.18116v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Natural Language but Omitted? On the Ineffectiveness of Large Language Models' privacy policy from End-users' Perspective</h3>
                <p>Authors: Shuning ZhangHaobin XingXin YiHewu Li</p>
                <p><a href="http://arxiv.org/abs/2406.18100v1">Link to paper</a></p>
                <p>LLMs driven products were increasingly prevalent in our daily lives With anatural language based interaction style people may potentially leak theirpersonal private information. Thus privacy policy and user agreement played animportant role in regulating and alerting people. However there lacked thework examining the reading of LLMs privacy policy. Thus we conducted thefirst user study to let participants read the privacy policy and user agreementwith two different styles a cursory and detailed style. We found users lackimportant information upon cursory reading and even detailed reading. Besidestheir privacy concerns was not solved even upon detailed reading. We providedfour design implications based on the findings.</p>
                <p>Last Updated: 2024-06-26 06:31:43 UTC</p>
                <button class="interpret-button" data-id="2406.18100v1">Interpret</button>
                <div id="interpretation-2406.18100v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>On Scaling Up 3D Gaussian Splatting Training</h3>
                <p>Authors: Hexu ZhaoHaoyang WengDaohan LuAng LiJinyang LiAurojit PandaSaining Xie</p>
                <p><a href="http://arxiv.org/abs/2406.18533v1">Link to paper</a></p>
                <p>3D Gaussian Splatting 3DGS is increasingly popular for 3D reconstructiondue to its superior visual quality and rendering speed. However 3DGS trainingcurrently occurs on a single GPU limiting its ability to handlehigh-resolution and large-scale 3D reconstruction tasks due to memoryconstraints. We introduce Grendel a distributed system designed to partition3DGS parameters and parallelize computation across multiple GPUs. As eachGaussian affects a small dynamic subset of rendered pixels Grendel employssparse all-to-all communication to transfer the necessary Gaussians to pixelpartitions and performs dynamic load balancing. Unlike existing 3DGS systemsthat train using one camera view image at a time Grendel supports batchedtraining with multiple views. We explore various optimization hyperparameterscaling strategies and find that a simple sqrtbatch size scaling rule ishighly effective. Evaluations using large-scale high-resolution scenes showthat Grendel enhances rendering quality by scaling up 3DGS parameters acrossmultiple GPUs. On the Rubble dataset we achieve a test PSNR of 27.28 bydistributing 40.4 million Gaussians across 16 GPUs compared to a PSNR of 26.28using 11.2 million Gaussians on a single GPU. Grendel is an open-source projectavailable at: https://github.com/nyu-systems/Grendel-GS</p>
                <p>Last Updated: 2024-06-26 17:59:28 UTC</p>
                <button class="interpret-button" data-id="2406.18533v1">Interpret</button>
                <div id="interpretation-2406.18533v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MatchTime: Towards Automatic Soccer Game Commentary Generation</h3>
                <p>Authors: Jiayuan RaoHaoning WuChang LiuYanfeng WangWeidi Xie</p>
                <p><a href="http://arxiv.org/abs/2406.18530v1">Link to paper</a></p>
                <p>Soccer is a globally popular sport with a vast audience in this paper weconsider constructing an automatic soccer game commentary model to improve theaudiences viewing experience. In general we make the following contributions:First observing the prevalent video-text misalignment in existing datasets wemanually annotate timestamps for 49 matches establishing a more robustbenchmark for soccer game commentary generation termed asSN-Caption-test-align Second we propose a multi-modal temporal alignmentpipeline to automatically correct and filter the existing dataset at scalecreating a higher-quality soccer game commentary dataset for training denotedas MatchTime Third based on our curated dataset we train an automaticcommentary generation model named MatchVoice. Extensive experiments andablation studies have demonstrated the effectiveness of our alignment pipelineand training model on the curated datasets achieves state-of-the-artperformance for commentary generation showcasing that better alignment canlead to significant performance improvements in downstream tasks.</p>
                <p>Last Updated: 2024-06-26 17:57:25 UTC</p>
                <button class="interpret-button" data-id="2406.18530v1">Interpret</button>
                <div id="interpretation-2406.18530v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MultiDiff: Consistent Novel View Synthesis from a Single Image</h3>
                <p>Authors: Norman MüllerKatja SchwarzBarbara RoessleLorenzo PorziSamuel Rota BulòMatthias NießnerPeter Kontschieder</p>
                <p><a href="http://arxiv.org/abs/2406.18524v1">Link to paper</a></p>
                <p>We introduce MultiDiff a novel approach for consistent novel view synthesisof scenes from a single RGB image. The task of synthesizing novel views from asingle reference image is highly ill-posed by nature as there exist multipleplausible explanations for unobserved areas. To address this issue weincorporate strong priors in form of monocular depth predictors andvideo-diffusion models. Monocular depth enables us to condition our model onwarped reference images for the target views increasing geometric stability.The video-diffusion prior provides a strong proxy for 3D scenes allowing themodel to learn continuous and pixel-accurate correspondences across generatedimages. In contrast to approaches relying on autoregressive image generationthat are prone to drifts and error accumulation MultiDiff jointly synthesizesa sequence of frames yielding high-quality and multi-view consistent results --even for long-term scene generation with large camera movements while reducinginference time by an order of magnitude. For additional consistency and imagequality improvements we introduce a novel structured noise distribution. Ourexperimental results demonstrate that MultiDiff outperforms state-of-the-artmethods on the challenging real-world datasets RealEstate10K and ScanNet.Finally our model naturally supports multi-view consistent editing without theneed for further tuning.</p>
                <p>Last Updated: 2024-06-26 17:53:51 UTC</p>
                <button class="interpret-button" data-id="2406.18524v1">Interpret</button>
                <div id="interpretation-2406.18524v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation</h3>
                <p>Authors: Shenghai YuanJinfa HuangYongqi XuYaoyang LiuShaofeng ZhangYujun ShiRuijie ZhuXinhua ChengJiebo LuoLi Yuan</p>
                <p><a href="http://arxiv.org/abs/2406.18522v1">Link to paper</a></p>
                <p>We propose a novel text-to-video T2V generation benchmarkChronoMagic-Bench to evaluate the temporal and metamorphic capabilities of theT2V models e.g. Sora and Lumiere in time-lapse video generation. In contrastto existing benchmarks that focus on the visual quality and textual relevanceof generated videos ChronoMagic-Bench focuses on the models ability togenerate time-lapse videos with significant metamorphic amplitude and temporalcoherence. The benchmark probes T2V models for their physics biology andchemistry capabilities in a free-form text query. For these purposesChronoMagic-Bench introduces 1649 prompts and real-world videos as referencescategorized into four major types of time-lapse videos: biologicalhuman-created meteorological and physical phenomena which are furtherdivided into 75 subcategories. This categorization comprehensively evaluatesthe models capacity to handle diverse and complex transformations. Toaccurately align human preference with the benchmark we introduce two newautomatic metrics MTScore and CHScore to evaluate the videos metamorphicattributes and temporal coherence. MTScore measures the metamorphic amplitudereflecting the degree of change over time while CHScore assesses the temporalcoherence ensuring the generated videos maintain logical progression andcontinuity. Based on the ChronoMagic-Bench we conduct comprehensive manualevaluations of ten representative T2V models revealing their strengths andweaknesses across different categories of prompts and providing a thoroughevaluation framework that addresses current gaps in video generation research.Moreover we create a large-scale ChronoMagic-Pro dataset containing 460khigh-quality pairs of 720p time-lapse videos and detailed captions ensuringhigh physical pertinence and large metamorphic amplitude.</p>
                <p>Last Updated: 2024-06-26 17:50:47 UTC</p>
                <button class="interpret-button" data-id="2406.18522v1">Interpret</button>
                <div id="interpretation-2406.18522v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs</h3>
                <p>Authors: Zirui WangMengzhou XiaLuxi HeHoward ChenYitao LiuRichard ZhuKaiqu LiangXindi WuHaotian LiuSadhika MalladiAlexis ChevalierSanjeev AroraDanqi Chen</p>
                <p><a href="http://arxiv.org/abs/2406.18521v1">Link to paper</a></p>
                <p>Chart understanding plays a pivotal role when applying Multimodal LargeLanguage Models MLLMs to real-world tasks such as analyzing scientific papersor financial reports. However existing datasets often focus on oversimplifiedand homogeneous charts with template-based questions leading to anover-optimistic measure of progress. We demonstrate that although open-sourcemodels can appear to outperform strong proprietary models on these benchmarksa simple stress test with slightly different charts or questions candeteriorate performance by up to 34.5. In this work we propose CharXiv acomprehensive evaluation suite involving 2323 natural challenging anddiverse charts from arXiv papers. CharXiv includes two types of questions: 1descriptive questions about examining basic chart elements and 2 reasoningquestions that require synthesizing information across complex visual elementsin the chart. To ensure quality all charts and questions are handpickedcurated and verified by human experts. Our results reveal a substantialpreviously underestimated gap between the reasoning skills of the strongestproprietary model i.e. GPT-4o which achieves 47.1 accuracy and thestrongest open-source model i.e. InternVL Chat V1.5 which achieves 29.2.All models lag far behind human performance of 80.5 underscoring weaknessesin the chart understanding capabilities of existing MLLMs. We hope CharXivfacilitates future research on MLLM chart understanding by providing a morerealistic and faithful measure of progress. Project page and leaderboard:https://charxiv.github.io/</p>
                <p>Last Updated: 2024-06-26 17:50:11 UTC</p>
                <button class="interpret-button" data-id="2406.18521v1">Interpret</button>
                <div id="interpretation-2406.18521v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Symbolic Learning Enables Self-Evolving Agents</h3>
                <p>Authors: Wangchunshu ZhouYixin OuShengwei DingLong LiJialong WuTiannan WangJiamin ChenShuai WangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor Jiang</p>
                <p><a href="http://arxiv.org/abs/2406.18532v1">Link to paper</a></p>
                <p>The AI community has been exploring a pathway to artificial generalintelligence AGI by developing language agents which are complex largelanguage models LLMs pipelines involving both prompting techniques and toolusage methods. While language agents have demonstrated impressive capabilitiesfor many real-world tasks a fundamental limitation of current language agentsresearch is that they are model-centric or engineering-centric. Thats to saythe progress on prompts tools and pipelines of language agents requiressubstantial manual engineering efforts from human experts rather thanautomatically learning from data. We believe the transition from model-centricor engineering-centric to data-centric i.e. the ability of language agentsto autonomously learn and evolve in environments is the key for them topossibly achieve AGI.  In this work we introduce agent symbolic learning a systematic frameworkthat enables language agents to optimize themselves on their own in adata-centric way using symbolic optimizers. Specifically we consider agents assymbolic networks where learnable weights are defined by prompts tools andthe way they are stacked together. Agent symbolic learning is designed tooptimize the symbolic network within language agents by mimicking twofundamental algorithms in connectionist learning: back-propagation and gradientdescent. Instead of dealing with numeric weights agent symbolic learning workswith natural language simulacrums of weights loss and gradients. We conductproof-of-concept experiments on both standard benchmarks and complex real-worldtasks and show that agent symbolic learning enables language agents to updatethemselves after being created and deployed in the wild resulting inself-evolving agents.</p>
                <p>Last Updated: 2024-06-26 17:59:18 UTC</p>
                <button class="interpret-button" data-id="2406.18532v1">Interpret</button>
                <div id="interpretation-2406.18532v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets</h3>
                <p>Authors: Zuxin LiuThai HoangJianguo ZhangMing ZhuTian LanShirley KokaneJuntao TanWeiran YaoZhiwei LiuYihao FengRithesh MurthyLiangwei YangSilvio SavareseJuan Carlos NieblesHuan WangShelby HeineckeCaiming Xiong</p>
                <p><a href="http://arxiv.org/abs/2406.18518v1">Link to paper</a></p>
                <p>The advancement of function-calling agent models requires diverse reliableand high-quality datasets. This paper presents APIGen an automated datageneration pipeline designed to synthesize verifiable high-quality datasets forfunction-calling applications. We leverage APIGen and collect 3673 executableAPIs across 21 different categories to generate diverse function-callingdatasets in a scalable and structured manner. Each data in our dataset isverified through three hierarchical stages: format checking actual functionexecutions and semantic verification ensuring its reliability andcorrectness. We demonstrate that models trained with our curated datasets evenwith only 7B parameters can achieve state-of-the-art performance on theBerkeley Function-Calling Benchmark outperforming multiple GPT-4 models.Moreover our 1B model achieves exceptional performance surpassingGPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60000high-quality entries aiming to advance the field of function-calling agentdomains. The dataset is available on Huggingface:https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and theproject homepage: https://apigen-pipeline.github.io/</p>
                <p>Last Updated: 2024-06-26 17:49:11 UTC</p>
                <button class="interpret-button" data-id="2406.18518v1">Interpret</button>
                <div id="interpretation-2406.18518v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Mental Modeling of Reinforcement Learning Agents by Language Models</h3>
                <p>Authors: Wenhao LuXufeng ZhaoJosua SpisakJae Hee LeeStefan Wermter</p>
                <p><a href="http://arxiv.org/abs/2406.18505v1">Link to paper</a></p>
                <p>Can emergent language models faithfully model the intelligence ofdecision-making agents Though modern language models exhibit already somereasoning ability and theoretically can potentially express any probabledistribution over tokens it remains underexplored how the world knowledgethese pretrained models have memorized can be utilized to comprehend an agentsbehaviour in the physical world. This study empirically examines for the firsttime how well large language models LLMs can build a mental model of agentstermed agent mental modelling by reasoning about an agents behaviour and itseffect on states from agent interaction history. This research may unveil thepotential of leveraging LLMs for elucidating RL agent behaviour addressing akey challenge in eXplainable reinforcement learning XRL. To this end wepropose specific evaluation metrics and test them on selected RL task datasetsof varying complexity reporting findings on agent mental model establishment.Our results disclose that LLMs are not yet capable of fully mental modellingagents through inference alone without further innovations. This work thusprovides new insights into the capabilities and limitations of modern LLMs.</p>
                <p>Last Updated: 2024-06-26 17:14:45 UTC</p>
                <button class="interpret-button" data-id="2406.18505v1">Interpret</button>
                <div id="interpretation-2406.18505v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation</h3>
                <p>Authors: Ahmed NjifenjouVirgile SucalBassam JabaianFabrice Lefèvre</p>
                <p><a href="http://arxiv.org/abs/2406.18460v1">Link to paper</a></p>
                <p>Recently various methods have been proposed to create open-domainconversational agents with Large Language Models LLMs. These models are ableto answer user queries but in a one-way QA format rather than a trueconversation. Fine-tuning on particular datasets is the usual way to modifytheir style to increase conversational ability but this is expensive andusually only available in a few languages. In this study we explore role-playzero-shot prompting as an efficient and cost-effective solution for open-domainconversation using capable multilingual LLMs Beeching et al. 2023 trainedto obey instructions. We design a prompting system that when combined with aninstruction-following model - here Vicuna Chiang et al. 2023 - producesconversational agents that match and even surpass fine-tuned models in humanevaluation in French in two different tasks.</p>
                <p>Last Updated: 2024-06-26 16:10:53 UTC</p>
                <button class="interpret-button" data-id="2406.18460v1">Interpret</button>
                <div id="interpretation-2406.18460v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers</h3>
                <p>Authors: Jonas NgnawéSabyasachi SahooYann PequignotFrédéric PreciosoChristian Gagné</p>
                <p><a href="http://arxiv.org/abs/2406.18451v1">Link to paper</a></p>
                <p>Despite extensive research on adversarial training strategies to improverobustness the decisions of even the most robust deep learning models canstill be quite sensitive to imperceptible perturbations creating serious riskswhen deploying them for high-stakes real-world applications. While detectingsuch cases may be critical evaluating a models vulnerability at aper-instance level using adversarial attacks is computationally too intensiveand unsuitable for real-time deployment scenarios. The input space margin isthe exact score to detect non-robust samples and is intractable for deep neuralnetworks. This paper introduces the concept of margin consistency -- a propertythat links the input space margins and the logit margins in robust models --for efficient detection of vulnerable samples. First we establish that marginconsistency is a necessary and sufficient condition to use a models logitmargin as a score for identifying non-robust samples. Next throughcomprehensive empirical analysis of various robustly trained models on CIFAR10and CIFAR100 datasets we show that they indicate strong margin consistencywith a strong correlation between their input space margins and the logitmargins. Then we show that we can effectively use the logit margin toconfidently detect brittle decisions with such models and accurately estimaterobust accuracy on an arbitrarily large test set by estimating the inputmargins only on a small subset. Finally we address cases where the model isnot sufficiently margin-consistent by learning a pseudo-margin from the featurerepresentation. Our findings highlight the potential of leveraging deeprepresentations to efficiently assess adversarial vulnerability in deploymentscenarios.</p>
                <p>Last Updated: 2024-06-26 16:00:35 UTC</p>
                <button class="interpret-button" data-id="2406.18451v1">Interpret</button>
                <div id="interpretation-2406.18451v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Towards Compositionality in Concept Learning</h3>
                <p>Authors: Adam SteinAaditya NaikYinjun WuMayur NaikEric Wong</p>
                <p><a href="http://arxiv.org/abs/2406.18534v1">Link to paper</a></p>
                <p>Concept-based interpretability methods offer a lens into the internals offoundation models by decomposing their embeddings into high-level concepts.These concept representations are most useful when they are compositionalmeaning that the individual concepts compose to explain the full sample. Weshow that existing unsupervised concept extraction methods find concepts whichare not compositional. To automatically discover compositional conceptrepresentations we identify two salient properties of such representationsand propose Compositional Concept Extraction CCE for finding concepts whichobey these properties. We evaluate CCE on five different datasets over imageand text data. Our evaluation shows that CCE finds more compositional conceptrepresentations than baselines and yields better accuracy on four downstreamclassification tasks. Code and data are available athttps://github.com/adaminsky/compositional_concepts .</p>
                <p>Last Updated: 2024-06-26 17:59:30 UTC</p>
                <button class="interpret-button" data-id="2406.18534v1">Interpret</button>
                <div id="interpretation-2406.18534v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Symbolic Learning Enables Self-Evolving Agents</h3>
                <p>Authors: Wangchunshu ZhouYixin OuShengwei DingLong LiJialong WuTiannan WangJiamin ChenShuai WangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor Jiang</p>
                <p><a href="http://arxiv.org/abs/2406.18532v1">Link to paper</a></p>
                <p>The AI community has been exploring a pathway to artificial generalintelligence AGI by developing language agents which are complex largelanguage models LLMs pipelines involving both prompting techniques and toolusage methods. While language agents have demonstrated impressive capabilitiesfor many real-world tasks a fundamental limitation of current language agentsresearch is that they are model-centric or engineering-centric. Thats to saythe progress on prompts tools and pipelines of language agents requiressubstantial manual engineering efforts from human experts rather thanautomatically learning from data. We believe the transition from model-centricor engineering-centric to data-centric i.e. the ability of language agentsto autonomously learn and evolve in environments is the key for them topossibly achieve AGI.  In this work we introduce agent symbolic learning a systematic frameworkthat enables language agents to optimize themselves on their own in adata-centric way using symbolic optimizers. Specifically we consider agents assymbolic networks where learnable weights are defined by prompts tools andthe way they are stacked together. Agent symbolic learning is designed tooptimize the symbolic network within language agents by mimicking twofundamental algorithms in connectionist learning: back-propagation and gradientdescent. Instead of dealing with numeric weights agent symbolic learning workswith natural language simulacrums of weights loss and gradients. We conductproof-of-concept experiments on both standard benchmarks and complex real-worldtasks and show that agent symbolic learning enables language agents to updatethemselves after being created and deployed in the wild resulting inself-evolving agents.</p>
                <p>Last Updated: 2024-06-26 17:59:18 UTC</p>
                <button class="interpret-button" data-id="2406.18532v1">Interpret</button>
                <div id="interpretation-2406.18532v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation</h3>
                <p>Authors: Christoph LeiterSteffen Eger</p>
                <p><a href="http://arxiv.org/abs/2406.18528v1">Link to paper</a></p>
                <p>Large language models LLMs have revolutionized the field of NLP. Notablytheir in-context learning capabilities also enable their use as evaluationmetrics for natural language generation making them particularly advantageousin low-resource scenarios and time-restricted applications. In this work weintroduce PrExMe a large-scale prompt exploration for metrics where weevaluate more than 720 prompt templates for open-source LLM-based metrics onmachine translation MT and summarization datasets totalling over 6.6Mevaluations. This extensive comparison 1 serves as a benchmark of theperformance of recent open-source LLMs as metrics and 2 explores thestability and variability of different prompting strategies. We discover thaton the one hand there are scenarios for which prompts are stable. Forinstance some LLMs show idiosyncratic preferences and favor to grade generatedtexts with textual labels while others prefer to return numeric scores. On theother hand the stability of prompts and model rankings can be susceptible toseemingly innocuous changes. For example changing the requested output formatfrom 0 to 100 to -1 to 1 can strongly affect the rankings in ourevaluation. Our study contributes to understanding the impact of differentprompting approaches on LLM-based metrics for MT and summarization evaluationhighlighting the most stable prompting patterns and potential limitations.</p>
                <p>Last Updated: 2024-06-26 17:56:29 UTC</p>
                <button class="interpret-button" data-id="2406.18528v1">Interpret</button>
                <div id="interpretation-2406.18528v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation</h3>
                <p>Authors: Shenghai YuanJinfa HuangYongqi XuYaoyang LiuShaofeng ZhangYujun ShiRuijie ZhuXinhua ChengJiebo LuoLi Yuan</p>
                <p><a href="http://arxiv.org/abs/2406.18522v1">Link to paper</a></p>
                <p>We propose a novel text-to-video T2V generation benchmarkChronoMagic-Bench to evaluate the temporal and metamorphic capabilities of theT2V models e.g. Sora and Lumiere in time-lapse video generation. In contrastto existing benchmarks that focus on the visual quality and textual relevanceof generated videos ChronoMagic-Bench focuses on the models ability togenerate time-lapse videos with significant metamorphic amplitude and temporalcoherence. The benchmark probes T2V models for their physics biology andchemistry capabilities in a free-form text query. For these purposesChronoMagic-Bench introduces 1649 prompts and real-world videos as referencescategorized into four major types of time-lapse videos: biologicalhuman-created meteorological and physical phenomena which are furtherdivided into 75 subcategories. This categorization comprehensively evaluatesthe models capacity to handle diverse and complex transformations. Toaccurately align human preference with the benchmark we introduce two newautomatic metrics MTScore and CHScore to evaluate the videos metamorphicattributes and temporal coherence. MTScore measures the metamorphic amplitudereflecting the degree of change over time while CHScore assesses the temporalcoherence ensuring the generated videos maintain logical progression andcontinuity. Based on the ChronoMagic-Bench we conduct comprehensive manualevaluations of ten representative T2V models revealing their strengths andweaknesses across different categories of prompts and providing a thoroughevaluation framework that addresses current gaps in video generation research.Moreover we create a large-scale ChronoMagic-Pro dataset containing 460khigh-quality pairs of 720p time-lapse videos and detailed captions ensuringhigh physical pertinence and large metamorphic amplitude.</p>
                <p>Last Updated: 2024-06-26 17:50:47 UTC</p>
                <button class="interpret-button" data-id="2406.18522v1">Interpret</button>
                <div id="interpretation-2406.18522v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs</h3>
                <p>Authors: Zirui WangMengzhou XiaLuxi HeHoward ChenYitao LiuRichard ZhuKaiqu LiangXindi WuHaotian LiuSadhika MalladiAlexis ChevalierSanjeev AroraDanqi Chen</p>
                <p><a href="http://arxiv.org/abs/2406.18521v1">Link to paper</a></p>
                <p>Chart understanding plays a pivotal role when applying Multimodal LargeLanguage Models MLLMs to real-world tasks such as analyzing scientific papersor financial reports. However existing datasets often focus on oversimplifiedand homogeneous charts with template-based questions leading to anover-optimistic measure of progress. We demonstrate that although open-sourcemodels can appear to outperform strong proprietary models on these benchmarksa simple stress test with slightly different charts or questions candeteriorate performance by up to 34.5. In this work we propose CharXiv acomprehensive evaluation suite involving 2323 natural challenging anddiverse charts from arXiv papers. CharXiv includes two types of questions: 1descriptive questions about examining basic chart elements and 2 reasoningquestions that require synthesizing information across complex visual elementsin the chart. To ensure quality all charts and questions are handpickedcurated and verified by human experts. Our results reveal a substantialpreviously underestimated gap between the reasoning skills of the strongestproprietary model i.e. GPT-4o which achieves 47.1 accuracy and thestrongest open-source model i.e. InternVL Chat V1.5 which achieves 29.2.All models lag far behind human performance of 80.5 underscoring weaknessesin the chart understanding capabilities of existing MLLMs. We hope CharXivfacilitates future research on MLLM chart understanding by providing a morerealistic and faithful measure of progress. Project page and leaderboard:https://charxiv.github.io/</p>
                <p>Last Updated: 2024-06-26 17:50:11 UTC</p>
                <button class="interpret-button" data-id="2406.18521v1">Interpret</button>
                <div id="interpretation-2406.18521v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Towards Compositionality in Concept Learning</h3>
                <p>Authors: Adam SteinAaditya NaikYinjun WuMayur NaikEric Wong</p>
                <p><a href="http://arxiv.org/abs/2406.18534v1">Link to paper</a></p>
                <p>Concept-based interpretability methods offer a lens into the internals offoundation models by decomposing their embeddings into high-level concepts.These concept representations are most useful when they are compositionalmeaning that the individual concepts compose to explain the full sample. Weshow that existing unsupervised concept extraction methods find concepts whichare not compositional. To automatically discover compositional conceptrepresentations we identify two salient properties of such representationsand propose Compositional Concept Extraction CCE for finding concepts whichobey these properties. We evaluate CCE on five different datasets over imageand text data. Our evaluation shows that CCE finds more compositional conceptrepresentations than baselines and yields better accuracy on four downstreamclassification tasks. Code and data are available athttps://github.com/adaminsky/compositional_concepts .</p>
                <p>Last Updated: 2024-06-26 17:59:30 UTC</p>
                <button class="interpret-button" data-id="2406.18534v1">Interpret</button>
                <div id="interpretation-2406.18534v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Symbolic Learning Enables Self-Evolving Agents</h3>
                <p>Authors: Wangchunshu ZhouYixin OuShengwei DingLong LiJialong WuTiannan WangJiamin ChenShuai WangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor Jiang</p>
                <p><a href="http://arxiv.org/abs/2406.18532v1">Link to paper</a></p>
                <p>The AI community has been exploring a pathway to artificial generalintelligence AGI by developing language agents which are complex largelanguage models LLMs pipelines involving both prompting techniques and toolusage methods. While language agents have demonstrated impressive capabilitiesfor many real-world tasks a fundamental limitation of current language agentsresearch is that they are model-centric or engineering-centric. Thats to saythe progress on prompts tools and pipelines of language agents requiressubstantial manual engineering efforts from human experts rather thanautomatically learning from data. We believe the transition from model-centricor engineering-centric to data-centric i.e. the ability of language agentsto autonomously learn and evolve in environments is the key for them topossibly achieve AGI.  In this work we introduce agent symbolic learning a systematic frameworkthat enables language agents to optimize themselves on their own in adata-centric way using symbolic optimizers. Specifically we consider agents assymbolic networks where learnable weights are defined by prompts tools andthe way they are stacked together. Agent symbolic learning is designed tooptimize the symbolic network within language agents by mimicking twofundamental algorithms in connectionist learning: back-propagation and gradientdescent. Instead of dealing with numeric weights agent symbolic learning workswith natural language simulacrums of weights loss and gradients. We conductproof-of-concept experiments on both standard benchmarks and complex real-worldtasks and show that agent symbolic learning enables language agents to updatethemselves after being created and deployed in the wild resulting inself-evolving agents.</p>
                <p>Last Updated: 2024-06-26 17:59:18 UTC</p>
                <button class="interpret-button" data-id="2406.18532v1">Interpret</button>
                <div id="interpretation-2406.18532v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Confident Natural Policy Gradient for Local Planning in $q_π$-realizable Constrained MDPs</h3>
                <p>Authors: Tian TianLin F. YangCsaba Szepesvári</p>
                <p><a href="http://arxiv.org/abs/2406.18529v1">Link to paper</a></p>
                <p>The constrained Markov decision process CMDP framework emerges as animportant reinforcement learning approach for imposing safety or other criticalobjectives while maximizing cumulative reward. However the currentunderstanding of how to learn efficiently in a CMDP environment with apotentially infinite number of states remains under investigation particularlywhen function approximation is applied to the value functions. In this paperwe address the learning problem given linear function approximation withq_pi-realizability where the value functions of all policies are linearlyrepresentable with a known feature map a setting known to be more general andchallenging than other linear settings. Utilizing a local-access model wepropose a novel primal-dual algorithm that after tildeOtextpolydepsilon-3 queries outputs with high probability a policy that strictlysatisfies the constraints while nearly optimizing the value with respect to areward function. Here d is the feature dimension and epsilon  0 is agiven error. The algorithm relies on a carefully crafted off-policy evaluationprocedure to evaluate the policy using historical data which informs policyupdates through policy gradients and conserves samples. To our knowledge thisis the first result achieving polynomial sample complexity for CMDP in theq_pi-realizable setting.</p>
                <p>Last Updated: 2024-06-26 17:57:13 UTC</p>
                <button class="interpret-button" data-id="2406.18529v1">Interpret</button>
                <div id="interpretation-2406.18529v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets</h3>
                <p>Authors: Zuxin LiuThai HoangJianguo ZhangMing ZhuTian LanShirley KokaneJuntao TanWeiran YaoZhiwei LiuYihao FengRithesh MurthyLiangwei YangSilvio SavareseJuan Carlos NieblesHuan WangShelby HeineckeCaiming Xiong</p>
                <p><a href="http://arxiv.org/abs/2406.18518v1">Link to paper</a></p>
                <p>The advancement of function-calling agent models requires diverse reliableand high-quality datasets. This paper presents APIGen an automated datageneration pipeline designed to synthesize verifiable high-quality datasets forfunction-calling applications. We leverage APIGen and collect 3673 executableAPIs across 21 different categories to generate diverse function-callingdatasets in a scalable and structured manner. Each data in our dataset isverified through three hierarchical stages: format checking actual functionexecutions and semantic verification ensuring its reliability andcorrectness. We demonstrate that models trained with our curated datasets evenwith only 7B parameters can achieve state-of-the-art performance on theBerkeley Function-Calling Benchmark outperforming multiple GPT-4 models.Moreover our 1B model achieves exceptional performance surpassingGPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60000high-quality entries aiming to advance the field of function-calling agentdomains. The dataset is available on Huggingface:https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and theproject homepage: https://apigen-pipeline.github.io/</p>
                <p>Last Updated: 2024-06-26 17:49:11 UTC</p>
                <button class="interpret-button" data-id="2406.18518v1">Interpret</button>
                <div id="interpretation-2406.18518v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Mental Modeling of Reinforcement Learning Agents by Language Models</h3>
                <p>Authors: Wenhao LuXufeng ZhaoJosua SpisakJae Hee LeeStefan Wermter</p>
                <p><a href="http://arxiv.org/abs/2406.18505v1">Link to paper</a></p>
                <p>Can emergent language models faithfully model the intelligence ofdecision-making agents Though modern language models exhibit already somereasoning ability and theoretically can potentially express any probabledistribution over tokens it remains underexplored how the world knowledgethese pretrained models have memorized can be utilized to comprehend an agentsbehaviour in the physical world. This study empirically examines for the firsttime how well large language models LLMs can build a mental model of agentstermed agent mental modelling by reasoning about an agents behaviour and itseffect on states from agent interaction history. This research may unveil thepotential of leveraging LLMs for elucidating RL agent behaviour addressing akey challenge in eXplainable reinforcement learning XRL. To this end wepropose specific evaluation metrics and test them on selected RL task datasetsof varying complexity reporting findings on agent mental model establishment.Our results disclose that LLMs are not yet capable of fully mental modellingagents through inference alone without further innovations. This work thusprovides new insights into the capabilities and limitations of modern LLMs.</p>
                <p>Last Updated: 2024-06-26 17:14:45 UTC</p>
                <button class="interpret-button" data-id="2406.18505v1">Interpret</button>
                <div id="interpretation-2406.18505v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Differential error feedback for communication-efficient decentralized learning</h3>
                <p>Authors: Roula NassifStefan VlaskiMarco CarpentieroVincenzo MattaAli H. Sayed</p>
                <p><a href="http://arxiv.org/abs/2406.18418v1">Link to paper</a></p>
                <p>Communication-constrained algorithms for decentralized learning andoptimization rely on local updates coupled with the exchange of compressedsignals. In this context differential quantization is an effective techniqueto mitigate the negative impact of compression by leveraging correlationsbetween successive iterates. In addition the use of error feedback whichconsists of incorporating the compression error into subsequent steps is apowerful mechanism to compensate for the bias caused by the compression. Undererror feedback performance guarantees in the literature have so far focused onalgorithms employing a fusion center or a special class of contractivecompressors that cannot be implemented with a finite number of bits. In thiswork we propose a new decentralized communication-efficient learning approachthat blends differential quantization with error feedback. The approach isspecifically tailored for decentralized learning problems where agents haveindividual risk functions to minimize subject to subspace constraints thatrequire the minimizers across the network to lie in low-dimensional subspaces.This constrained formulation includes consensus or single-task optimization asspecial cases and allows for more general task relatedness models such asmultitask smoothness and coupled optimization. We show that under some generalconditions on the compression noise and for sufficiently small step-sizesmu the resulting communication-efficient strategy is stable both in termsof mean-square error and average bit rate: by reducing mu it is possible tokeep the estimation errors small on the order of mu without increasingindefinitely the bit rate as murightarrow 0. The results establish that inthe small step-size regime and with a finite number of bits it is possible toattain the performance achievable in the absence of compression.</p>
                <p>Last Updated: 2024-06-26 15:11:26 UTC</p>
                <button class="interpret-button" data-id="2406.18418v1">Interpret</button>
                <div id="interpretation-2406.18418v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Building multiscale models with PhysiBoSS, an agent-based modeling tool</h3>
                <p>Authors: Marco RusconeAndrea CheccoliRandy HeilandEmmanuel BarillotPaul MacklinLaurence CalzoneVincent Noël</p>
                <p><a href="http://arxiv.org/abs/2406.18371v1">Link to paper</a></p>
                <p>Multiscale models provide a unique tool for studying complex processes thatstudy events occurring at different scales across space and time. In thecontext of biological systems such models can simulate mechanisms happening atthe intracellular level such as signaling and at the extracellular level wherecells communicate and coordinate with other cells. They aim to understand theimpact of genetic or environmental deregulation observed in complex diseasesdescribe the interplay between a pathological tissue and the immune system andsuggest strategies to revert the diseased phenotypes. The construction of thesemultiscale models remains a very complex task including the choice of thecomponents to consider the level of details of the processes to simulate orthe fitting of the parameters to the data. One additional difficulty is theexpert knowledge needed to program these models in languages such as C orPython which may discourage the participation of non-experts. Simplifying thisprocess through structured description formalisms -- coupled with a graphicalinterface -- is crucial in making modeling more accessible to the broaderscientific community as well as streamlining the process for advanced users.This article introduces three examples of multiscale models which rely on theframework PhysiBoSS an add-on of PhysiCell that includes intracellulardescriptions as continuous time Boolean models to the agent-based approach. Thearticle demonstrates how to easily construct such models relying on PhysiCellStudio the PhysiCell Graphical User Interface. A step-by-step tutorial isprovided as a Supplementary Material and all models are provided at:https://physiboss.github.io/tutorial/.</p>
                <p>Last Updated: 2024-06-26 14:14:57 UTC</p>
                <button class="interpret-button" data-id="2406.18371v1">Interpret</button>
                <div id="interpretation-2406.18371v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Emergence of social hierarchies in a society with two competitive classes</h3>
                <p>Authors: Marc SadurníJosep PerellóMiquel Montero</p>
                <p><a href="http://arxiv.org/abs/2406.18168v1">Link to paper</a></p>
                <p>Agent-based models describing social interactions among individuals can helpto better understand emerging macroscopic patterns in societies. One of thetopics which is worth tackling is the formation of different kinds ofhierarchies that emerge in social spaces such as cities. Here we propose aBonabeau-like model by adding a second class of agents. The fundamentalparticularity of our model is that only a pairwise interaction between agentsof the opposite class is allowed. Agent fitness can thus only change bycompetition among the two classes while the total fitness in the societyremains constant. The main result is that for a broad range of values of themodel parameters the fitness of the agents of each class show a decay in timeexcept for one or very few agents which capture almost all the fitness in thesociety. Numerical simulations also reveal a singular shift from egalitarian tohierarchical society for each class. This behaviour depends on the controlparameter eta playing the role of the inverse of the temperature of thesystem. Results are invariant with regard to the system size contingent solelyon the quantity of agents within each class. Finally a couple of scaling lawsare provided thus showing a data collapse from different model parameters andthey follow a shape which can be related to the presence of a phase transitionin the model.</p>
                <p>Last Updated: 2024-06-26 08:33:08 UTC</p>
                <button class="interpret-button" data-id="2406.18168v1">Interpret</button>
                <div id="interpretation-2406.18168v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Intrinsic Action Tendency Consistency for Cooperative Multi-Agent Reinforcement Learning</h3>
                <p>Authors: Junkai ZhangYifan ZhangXi Sheryl ZhangYifan ZangJian Cheng</p>
                <p><a href="http://arxiv.org/abs/2406.18152v1">Link to paper</a></p>
                <p>Efficient collaboration in the centralized training with decentralizedexecution CTDE paradigm remains a challenge in cooperative multi-agentsystems. We identify divergent action tendencies among agents as a significantobstacle to CTDEs training efficiency requiring a large number of trainingsamples to achieve a unified consensus on agents policies. This divergencestems from the lack of adequate team consensus-related guidance signals duringcredit assignments in CTDE. To address this we propose Intrinsic ActionTendency Consistency a novel approach for cooperative multi-agentreinforcement learning. It integrates intrinsic rewards obtained through anaction model into a reward-additive CTDE RA-CTDE framework. We formulate anaction model that enables surrounding agents to predict the central agentsaction tendency. Leveraging these predictions we compute a cooperativeintrinsic reward that encourages agents to match their actions with theirneighbors predictions. We establish the equivalence between RA-CTDE and CTDEthrough theoretical analyses demonstrating that CTDEs training process can beachieved using agents individual targets. Building on this insight weintroduce a novel method to combine intrinsic rewards and CTDE. Extensiveexperiments on challenging tasks in SMAC and GRF benchmarks showcase theimproved performance of our method.</p>
                <p>Last Updated: 2024-06-26 08:06:29 UTC</p>
                <button class="interpret-button" data-id="2406.18152v1">Interpret</button>
                <div id="interpretation-2406.18152v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Overcooked Generalisation Challenge</h3>
                <p>Authors: Constantin RuhdorferMatteo BortolettoAnna PenzkoferAndreas Bulling</p>
                <p><a href="http://arxiv.org/abs/2406.17949v1">Link to paper</a></p>
                <p>We introduce the Overcooked Generalisation Challenge OGC - the firstbenchmark to study agents zero-shot cooperation abilities when faced withnovel partners and levels in the Overcooked-AI environment. This perspectivestarkly contrasts a large body of previous work that has trained and evaluatedcooperating agents only on the same level failing to capture generalisationabilities required for real-world human-AI cooperation. Our challengeinterfaces with state-of-the-art dual curriculum design DCD methods togenerate auto-curricula for training general agents in Overcooked. It is thefirst cooperative multi-agent environment specially designed for DCD methodsand consequently the first benchmarked with state-of-the-art methods. It isfully GPU-accelerated built on the DCD benchmark suite minimax and freelyavailable under an open-source license:https://git.hcics.simtech.uni-stuttgart.de/public-projects/OGC. We show thatcurrent DCD algorithms struggle to produce useful policies in this novelchallenge even if combined with recent network architectures that weredesigned for scalability and generalisability. The OGC pushes the boundaries ofreal-world human-AI cooperation by enabling the research community to study theimpact of generalisation on cooperating agents.</p>
                <p>Last Updated: 2024-06-25 21:51:43 UTC</p>
                <button class="interpret-button" data-id="2406.17949v1">Interpret</button>
                <div id="interpretation-2406.17949v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-06-27</p>
        </div>
    
        </div>
    </body>
    </html>
    