
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>KNOWNET: Guided Health Information Seeking from LLMs via Knowledge Graph Integration</h3>
                <p>Authors: Youfu YanYu HouYongkang XiaoRui ZhangQianwen Wang</p>
                <p><a href="http://arxiv.org/abs/2407.13598v1">Link to paper</a></p>
                <p>The increasing reliance on Large Language Models LLMs for healthinformation seeking can pose severe risks due to the potential formisinformation and the complexity of these topics. This paper introducesKNOWNET a visualization system that integrates LLMs with Knowledge Graphs KGto provide enhanced accuracy and structured exploration. Specifically forenhanced accuracy KNOWNET extracts triples e.g. entities and theirrelations from LLM outputs and maps them into the validated information andsupported evidence in external KGs. For structured exploration KNOWNETprovides next-step recommendations based on the neighborhood of the currentlyexplored entities in KGs aiming to guide a comprehensive understanding withoutoverlooking critical aspects. To enable reasoning with both the structured datain KGs and the unstructured outputs from LLMs KNOWNET conceptualizes theunderstanding of a subject as the gradual construction of graph visualization.A progressive graph visualization is introduced to monitor past inquiries andbridge the current query with the exploration history and next-steprecommendations. We demonstrate the effectiveness of our system via use casesand expert interviews.</p>
                <p>Last Updated: 2024-07-18 15:37:27 UTC</p>
                <button class="interpret-button" data-id="2407.13598v1">Interpret</button>
                <div id="interpretation-2407.13598v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool Interactions for People with Low Vision</h3>
                <p>Authors: Jaewook LeeAndrew D. TjahjadiJiho KimJunpu YuMinji ParkJiawen ZhangJon E. FroehlichYapeng TianYuhang Zhao</p>
                <p><a href="http://arxiv.org/abs/2407.13515v1">Link to paper</a></p>
                <p>Cooking is a central activity of daily living supporting independence andboth mental and physical health. However prior work has highlighted keybarriers for people with low vision LV to cook particularly around safelyinteracting with cooking tools such as sharp knives or hot pans. Drawing onrecent advancements in computer vision CV and robotics we present CookAR ahead-mounted AR system with real-time object affordance augmentations tosupport safe and efficient interactions with kitchen tools. To design andimplement CookAR we manually collected and annotated the first egocentricdataset of kitchen tool affordances fine-tuned an affordance segmentationmodel and leveraged a stereo camera attached to an AR headset to generate thevisual augmentations. To validate CookAR we conducted a technical performanceevaluation and a three-part qualitative lab study with ten LV participants. Ourtechnical evaluation demonstrates that our fine-tuned model outperforms thebase model on our class-specific dataset while our user study indicates apreference for affordance augmentations over the traditional whole objectaugmentations. Code is available at: https://github.com/makeabilitylab/CookAR</p>
                <p>Last Updated: 2024-07-18 13:46:15 UTC</p>
                <button class="interpret-button" data-id="2407.13515v1">Interpret</button>
                <div id="interpretation-2407.13515v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Empirical Analysis of Sri Lankan Mobile Health Ecosystem: A Precursor to an Effective Stakeholder Engagement</h3>
                <p>Authors: Kenneth ThilakarathnaSachintha PitigalaJayantha FernandoPrimal Wijesekera</p>
                <p><a href="http://arxiv.org/abs/2407.13415v1">Link to paper</a></p>
                <p>Sri Lanka recently passed its first privacy legislation covering a wide rangeof sectors including health. As a precursor for effective stakeholderengagement in the health domain to understand the most effective way toimplement legislation in healthcare we have analyzed 41 popular mobile appsand web portals. We found that 78 of the tested systems have third-partydomains receiving sensitive health data with minimal visibility to theconsumers. We discuss how this will create potential issues in preparing forthe new privacy legislation.</p>
                <p>Last Updated: 2024-07-18 11:38:25 UTC</p>
                <button class="interpret-button" data-id="2407.13415v1">Interpret</button>
                <div id="interpretation-2407.13415v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>DISCOVER: A Data-driven Interactive System for Comprehensive Observation, Visualization, and ExploRation of Human Behaviour</h3>
                <p>Authors: Dominik SchillerTobias HallmenDaksitha Withanage DonElisabeth AndréTobias Baur</p>
                <p><a href="http://arxiv.org/abs/2407.13408v1">Link to paper</a></p>
                <p>Understanding human behavior is a fundamental goal of social sciences yetits analysis presents significant challenges. Conventional methodologiesemployed for the study of behavior characterized by labor-intensive datacollection processes and intricate analyses frequently hinder comprehensiveexploration due to their time and resource demands. In response to thesechallenges computational models have proven to be promising tools that helpresearchers analyze large amounts of data by automatically identifyingimportant behavioral indicators such as social signals. However thewidespread adoption of such state-of-the-art computational models is impeded bytheir inherent complexity and the substantial computational resources necessaryto run them thereby constraining accessibility for researchers withouttechnical expertise and adequate equipment. To address these barriers weintroduce DISCOVER -- a modular and flexible yet user-friendly softwareframework specifically developed to streamline computational-driven dataexploration for human behavior analysis. Our primary objective is todemocratize access to advanced computational methodologies thereby enablingresearchers across disciplines to engage in detailed behavioral analysiswithout the need for extensive technical proficiency. In this paper wedemonstrate the capabilities of DISCOVER using four exemplary data explorationworkflows that build on each other: Interactive Semantic Content ExplorationVisual Inspection Aided Annotation and Multimodal Scene Search. Byillustrating these workflows we aim to emphasize the versatility andaccessibility of DISCOVER as a comprehensive framework and propose a set ofblueprints that can serve as a general starting point for exploratory dataanalysis.</p>
                <p>Last Updated: 2024-07-18 11:28:52 UTC</p>
                <button class="interpret-button" data-id="2407.13408v1">Interpret</button>
                <div id="interpretation-2407.13408v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>How Private is Low-Frequency Speech Audio in the Wild? An Analysis of Verbal Intelligibility by Humans and Machines</h3>
                <p>Authors: Ailin LiuPepijn VunderinkJose Vargas QuirosChirag RamanHayley Hung</p>
                <p><a href="http://arxiv.org/abs/2407.13266v1">Link to paper</a></p>
                <p>Low-frequency audio has been proposed as a promising privacy-preservingmodality to study social dynamics in real-world settings. To this endresearchers have developed wearable devices that can record audio atfrequencies as low as 1250 Hz to mitigate the automatic extraction of theverbal content of speech that may contain private details. This paperinvestigates the validity of this hypothesis examining the degree to whichlow-frequency speech ensures verbal privacy. It includes simulating a potentialprivacy attack in various noise environments. Further it explores thetrade-off between the performance of voice activity detection which isfundamental for understanding social behavior and privacy-preservation. Theevaluation incorporates subjective human intelligibility and automatic speechrecognition performance comprehensively analyzing the delicate balance betweeneffective social behavior analysis and preserving verbal privacy.</p>
                <p>Last Updated: 2024-07-18 08:16:56 UTC</p>
                <button class="interpret-button" data-id="2407.13266v1">Interpret</button>
                <div id="interpretation-2407.13266v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Latent Causal Probing: A Formal Perspective on Probing with Causal Models of Data</h3>
                <p>Authors: Charles Jin</p>
                <p><a href="http://arxiv.org/abs/2407.13765v1">Link to paper</a></p>
                <p>As language models LMs deliver increasing performance on a range of NLPtasks probing classifiers have become an indispensable technique in the effortto better understand their inner workings. A typical setup involves 1defining an auxiliary task consisting of a dataset of text annotated withlabels then 2 supervising small classifiers to predict the labels from therepresentations of a pretrained LM as it processed the dataset. A high probingaccuracy is interpreted as evidence that the LM has learned to perform theauxiliary task as an unsupervised byproduct of its original pretrainingobjective. Despite the widespread usage of probes however the robust designand analysis of probing experiments remains a challenge. We develop a formalperspective on probing using structural causal models SCM. Specificallygiven an SCM which explains the distribution of tokens observed duringtraining we frame the central hypothesis as whether the LM has learned torepresent the latent variables of the SCM. Empirically we extend a recentstudy of LMs in the context of a synthetic grid-world navigation task wherehaving an exact model of the underlying causal structure allows us to drawstrong inferences from the result of probing experiments. Our techniquesprovide robust empirical evidence for the ability of LMs to learn the latentcausal concepts underlying text.</p>
                <p>Last Updated: 2024-07-18 17:59:27 UTC</p>
                <button class="interpret-button" data-id="2407.13765v1">Interpret</button>
                <div id="interpretation-2407.13765v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models</h3>
                <p>Authors: Zhuo ChenJiawei LiuHaotan LiuQikai ChengFan ZhangWei LuXiaozhong Liu</p>
                <p><a href="http://arxiv.org/abs/2407.13757v1">Link to paper</a></p>
                <p>Retrieval-Augmented Generation RAG is applied to solve hallucinationproblems and real-time constraints of large language models but it alsoinduces vulnerabilities against retrieval corruption attacks. Existing researchmainly explores the unreliability of RAG in white-box and closed-domain QAtasks. In this paper we aim to reveal the vulnerabilities ofRetrieval-Enhanced Generative RAG models when faced with black-box attacksfor opinion manipulation. We explore the impact of such attacks on usercognition and decision-making providing new insight to enhance the reliabilityand security of RAG models. We manipulate the ranking results of the retrievalmodel in RAG with instruction and use these results as data to train asurrogate model. By employing adversarial retrieval attack methods to thesurrogate model black-box transfer attacks on RAG are further realized.Experiments conducted on opinion datasets across multiple topics show that theproposed attack strategy can significantly alter the opinion polarity of thecontent generated by RAG. This demonstrates the models vulnerability and moreimportantly reveals the potential negative impact on user cognition anddecision-making making it easier to mislead users into accepting incorrect orbiased information.</p>
                <p>Last Updated: 2024-07-18 17:55:55 UTC</p>
                <button class="interpret-button" data-id="2407.13757v1">Interpret</button>
                <div id="interpretation-2407.13757v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLMs as Function Approximators: Terminology, Taxonomy, and Questions for Evaluation</h3>
                <p>Authors: David Schlangen</p>
                <p><a href="http://arxiv.org/abs/2407.13744v1">Link to paper</a></p>
                <p>Natural Language Processing has moved rather quickly from modelling specifictasks to taking more general pre-trained models and fine-tuning them forspecific tasks to a point where we now have what appear to be inherentlygeneralist models. This paper argues that the resultant loss of clarity on whatthese models model leads to metaphors like artificial general intelligencesthat are not helpful for evaluating their strengths and weaknesses. Theproposal is to see their generality and their potential value in theirability to approximate specialist function based on a natural languagespecification. This framing brings to the fore questions of the quality of theapproximation but beyond that also questions of discoverability stabilityand protectability of these functions. As the paper will show this framinghence brings together in one conceptual framework various aspects ofevaluation both from a practical and a theoretical perspective as well asquestions often relegated to a secondary status such as prompt injection andjailbreaking.</p>
                <p>Last Updated: 2024-07-18 17:49:56 UTC</p>
                <button class="interpret-button" data-id="2407.13744v1">Interpret</button>
                <div id="interpretation-2407.13744v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Scaling Granite Code Models to 128K Context</h3>
                <p>Authors: Matt StalloneVaibhav SaxenaLeonid KarlinskyBridget McGinnTim BulaMayank MishraAdriana Meza SoriaGaoyuan ZhangAditya PrasadYikang ShenSaptha SurendranShanmukha GuttulaHima PatelParameswaran SelvamXuan-Hong DangYan KoyfmanAtin SoodRogerio FerisNirmit DesaiDavid D. CoxRuchir PuriRameswar Panda</p>
                <p><a href="http://arxiv.org/abs/2407.13739v1">Link to paper</a></p>
                <p>This paper introduces long-context Granite code models that support effectivecontext windows of up to 128K tokens. Our solution for scaling context lengthof Granite 3B/8B code models from 2K/4K to 128K consists of a light-weightcontinual pretraining by gradually increasing its RoPE base frequency withrepository-level file packing and length-upsampled long-context data.Additionally we also release instruction-tuned models with long-contextsupport which are derived by further finetuning the long context base models ona mix of permissively licensed short and long-context instruction-responsepairs. While comparing to the original short-context Granite code models ourlong-context models achieve significant improvements on long-context taskswithout any noticeable performance degradation on regular code completionbenchmarks e.g. HumanEval. We release all our long-context Granite codemodels under an Apache 2.0 license for both research and commercial use.</p>
                <p>Last Updated: 2024-07-18 17:46:02 UTC</p>
                <button class="interpret-button" data-id="2407.13739v1">Interpret</button>
                <div id="interpretation-2407.13739v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Baba Is AI: Break the Rules to Beat the Benchmark</h3>
                <p>Authors: Nathan CloosMeagan JensMichelangelo NaimYen-Ling KuoIgnacio CasesAndrei BarbuChristopher J. Cueva</p>
                <p><a href="http://arxiv.org/abs/2407.13729v1">Link to paper</a></p>
                <p>Humans solve problems by following existing rules and procedures and also byleaps of creativity to redefine those rules and objectives. To probe theseabilities we developed a new benchmark based on the game Baba Is You where anagent manipulates both objects in the environment and rules represented bymovable tiles with words written on them to reach a specified goal and win thegame. We test three state-of-the-art multi-modal large language models OpenAIGPT-4o Google Gemini-1.5-Pro and Gemini-1.5-Flash and find that they faildramatically when generalization requires that the rules of the game must bemanipulated and combined.</p>
                <p>Last Updated: 2024-07-18 17:30:48 UTC</p>
                <button class="interpret-button" data-id="2407.13729v1">Interpret</button>
                <div id="interpretation-2407.13729v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Matching-Driven Deep Reinforcement Learning for Energy-Efficient Transmission Parameter Allocation in Multi-Gateway LoRa Networks</h3>
                <p>Authors: Ziqi LinXu ZhangShimin GongLanhua LiZhou SuBo Gu</p>
                <p><a href="http://arxiv.org/abs/2407.13076v1">Link to paper</a></p>
                <p>Long-range LoRa communication technology distinguished by its low powerconsumption and long communication range is widely used in the Internet ofThings. Nevertheless the LoRa MAC layer adopts pure ALOHA for medium accesscontrol which may suffer from severe packet collisions as the network scaleexpands consequently reducing the system energy efficiency EE. To addressthis issue it is critical to carefully allocate transmission parameters suchas the channel CH transmission power TP and spreading factor SF to eachend device ED. Owing to the low duty cycle and sporadic traffic of LoRanetworks evaluating the system EE under various parameter settings proves tobe time-consuming. Consequently we propose an analytical model aimed atcalculating the system EE while fully considering the impact of multiplegateways duty cycling quasi-orthogonal SFs and capture effects. On thisbasis we investigate a joint CH SF and TP allocation problem with theobjective of optimizing the system EE for uplink transmissions. Due to theNP-hard complexity of the problem the optimization problem is decomposed intotwo subproblems: CH assignment and SF/TP assignment. First a matching-basedalgorithm is introduced to address the CH assignment subproblem. Then anattention-based multiagent reinforcement learning technique is employed toaddress the SF/TP assignment subproblem for EDs allocated to the same CH whichreduces the number of learning agents to achieve fast convergence. Thesimulation outcomes indicate that the proposed approach converges quickly undervarious parameter settings and obtains significantly better system EE thanbaseline algorithms.</p>
                <p>Last Updated: 2024-07-18 00:54:26 UTC</p>
                <button class="interpret-button" data-id="2407.13076v1">Interpret</button>
                <div id="interpretation-2407.13076v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Information Compression in Dynamic Games</h3>
                <p>Authors: Dengwang TangVijay SubramanianDemosthenis Teneketzis</p>
                <p><a href="http://arxiv.org/abs/2407.12318v1">Link to paper</a></p>
                <p>One of the reasons why stochastic dynamic games with an underlying dynamicsystem are challenging is since strategic players have access to enormousamount of information which leads to the use of extremely complex strategies atequilibrium. One approach to resolve this challenge is to simplify playersstrategies by identifying appropriate compression of information maps so thatthe players can make decisions solely based on the compressed version ofinformation called the information state. For finite dynamic games withasymmetric information inspired by the notion of information state forsingle-agent control problems we propose two notions of information statesnamely mutually sufficient information MSI and unilaterally sufficientinformation USI. Both these information states are obtained with informationcompression maps independent of the strategy profile. We show that Bayes-NashEquilibria BNE and Sequential Equilibria SE exist when all players useMSI-based strategies. We prove that when all players employ USI-basedstrategies the resulting sets of BNE and SE payoff profiles are the same as thesets of BNE and SE payoff profiles resulting when all players use fullinformation-based strategies. We prove that when all players use USI-basedstrategies the resulting set of weak Perfect Bayesian Equilibrium wPBE payoffprofiles can be a proper subset of all wPBE payoff profiles. We identify MSIand USI in specific models of dynamic games in the literature. We end bypresenting an open problem: Do there exist strategy-dependent informationcompression maps that guarantee the existence of at least one equilibrium ormaintain all equilibria that exist under perfect recall We show by acounterexample that a well-known strategy-dependent information compressionmap used in the literature does not possess any of the properties of MSI orUSI.</p>
                <p>Last Updated: 2024-07-17 05:08:47 UTC</p>
                <button class="interpret-button" data-id="2407.12318v1">Interpret</button>
                <div id="interpretation-2407.12318v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Graph-based Adversarial Imitation Learning Framework for Reliable & Realtime Fleet Scheduling in Urban Air Mobility</h3>
                <p>Authors: Prithvi PoddarSteve PaulSouma Chowdhury</p>
                <p><a href="http://arxiv.org/abs/2407.12113v1">Link to paper</a></p>
                <p>The advent of Urban Air Mobility UAM presents the scope for atransformative shift in the domain of urban transportation. However itswidespread adoption and economic viability depends in part on the ability tooptimally schedule the fleet of aircraft across vertiports in a UAM networkunder uncertainties attributed to airspace congestion changing weatherconditions and varying demands. This paper presents a comprehensiveoptimization formulation of the fleet scheduling problem while alsoidentifying the need for alternate solution approaches since directly solvingthe resulting integer nonlinear programming problem is computationallyprohibitive for daily fleet scheduling. Previous work has shown theeffectiveness of using graph reinforcement learning RL approaches to trainreal-time executable policy models for fleet scheduling. However such policiescan often be brittle on out-of-distribution scenarios or edge cases. Moreovertraining performance also deteriorates as the complexity e.g. number ofconstraints of the problem increases. To address these issues this paperpresents an imitation learning approach where the RL-based policy exploitsexpert demonstrations yielded by solving the exact optimization using a GeneticAlgorithm. The policy model comprises Graph Neural Network GNN based encodersthat embed the space of vertiports and aircraft Transformer networks to encodedemand passenger fare and transport cost profiles and a Multi-head attentionMHA based decoder. Expert demonstrations are used through the GenerativeAdversarial Imitation Learning GAIL algorithm. Interfaced with a UAMsimulation environment involving 8 vertiports and 40 aircrafts in terms of thedaily profits earned reward the new imitative approach achieves better meanperformance and remarkable improvement in the case of unseen worst-casescenarios compared to pure RL results.</p>
                <p>Last Updated: 2024-07-16 18:51:24 UTC</p>
                <button class="interpret-button" data-id="2407.12113v1">Interpret</button>
                <div id="interpretation-2407.12113v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Map of Elections</h3>
                <p>Authors: Stanisław Szufa</p>
                <p><a href="http://arxiv.org/abs/2407.11889v1">Link to paper</a></p>
                <p>Our main contribution is the introduction of the map of elections framework.A map of elections consists of three main elements: 1 a dataset of electionsi.e. collections of ordinal votes over given sets of candidates 2 a wayof measuring similarities between these elections and 3 a representation ofthe elections in the 2D Euclidean space as points so that the more similar twoelections are the closer are their points. In our maps we mostly focus ondatasets of synthetic elections but we also show an example of a map overreal-life ones. To measure similarities we would have preferred to use e.g.the isomorphic swap distance but this is infeasible due to its highcomputational complexity. Hence we propose polynomial-time computablepositionwise distance and use it instead. Regarding the representations in 2DEuclidean space we mostly use the Kamada-Kawai algorithm but we also show twoalternatives.  We develop the necessary theoretical results to form our maps and argueexperimentally that they are accurate and credible. Further we show howcoloring the elections in a map according to various criteria helps inanalyzing results of a number of experiments. In particular we show coloringsaccording to the scores of winning candidates or committees running times ofILP-based winner determination algorithms and approximation ratios achieved byparticular algorithms.</p>
                <p>Last Updated: 2024-07-16 16:18:29 UTC</p>
                <button class="interpret-button" data-id="2407.11889v1">Interpret</button>
                <div id="interpretation-2407.11889v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning to Imitate Spatial Organization in Multi-robot Systems</h3>
                <p>Authors: Ayomide O. AgunloyeSarvapali D. RamchurnMohammad D. Soorati</p>
                <p><a href="http://arxiv.org/abs/2407.11592v1">Link to paper</a></p>
                <p>Understanding collective behavior and how it evolves is important to ensurethat robot swarms can be trusted in a shared environment. One way to understandthe behavior of the swarm is through collective behavior reconstruction usingprior demonstrations. Existing approaches often require access to the swarmcontroller which may not be available. We reconstruct collective behaviors indistinct swarm scenarios involving shared environments without using swarmcontroller information. We achieve this by transforming prior demonstrationsinto features that sufficiently describe multi-agent interactions beforebehavior reconstruction with multi-agent generative adversarial imitationlearning MA-GAIL. We show that our approach outperforms existing algorithmsin all investigated swarm scenarios and can be used to observe and reconstructa swarms behavior for further analysis and testing which might be impracticalor undesirable on the original robot swarm.</p>
                <p>Last Updated: 2024-07-16 10:50:39 UTC</p>
                <button class="interpret-button" data-id="2407.11592v1">Interpret</button>
                <div id="interpretation-2407.11592v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Random Latent Exploration for Deep Reinforcement Learning</h3>
                <p>Authors: Srinath MahankaliZhang-Wei HongAyush SekhariAlexander RakhlinPulkit Agrawal</p>
                <p><a href="http://arxiv.org/abs/2407.13755v1">Link to paper</a></p>
                <p>The ability to efficiently explore high-dimensional state spaces is essentialfor the practical success of deep Reinforcement Learning RL. This paperintroduces a new exploration technique called Random Latent Exploration RLEthat combines the strengths of bonus-based and noise-based two popularapproaches for effective exploration in deep RL exploration strategies. RLEleverages the idea of perturbing rewards by adding structured random rewards tothe original task rewards in certain random states of the environment toencourage the agent to explore the environment during training. RLE isstraightforward to implement and performs well in practice. To demonstrate thepractical effectiveness of RLE we evaluate it on the challenging Atari andIsaacGym benchmarks and show that RLE exhibits higher overall scores across allthe tasks than other approaches.</p>
                <p>Last Updated: 2024-07-18 17:55:22 UTC</p>
                <button class="interpret-button" data-id="2407.13755v1">Interpret</button>
                <div id="interpretation-2407.13755v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-Label Learning with Stronger Consistency Guarantees</h3>
                <p>Authors: Anqi MaoMehryar MohriYutao Zhong</p>
                <p><a href="http://arxiv.org/abs/2407.13746v1">Link to paper</a></p>
                <p>We present a detailed study of surrogate losses and algorithms formulti-label learning supported by H-consistency bounds. We first show thatfor the simplest form of multi-label loss the popular Hamming loss thewell-known consistent binary relevance surrogate suffers from a sub-optimaldependency on the number of labels in terms of H-consistency bounds whenusing smooth losses such as logistic losses. Furthermore this loss functionfails to account for label correlations. To address these drawbacks weintroduce a novel surrogate loss multi-label logistic loss that accounts forlabel correlations and benefits from label-independent H-consistency bounds.We then broaden our analysis to cover a more extensive family of multi-labellosses including all common ones and a new extension defined based onlinear-fractional functions with respect to the confusion matrix. We alsoextend our multi-label logistic losses to more comprehensive multi-labelcomp-sum losses adapting comp-sum losses from standard classification to themulti-label learning. We prove that this family of surrogate losses benefitsfrom H-consistency bounds and thus Bayes-consistency across any generalmulti-label loss. Our work thus proposes a unified surrogate loss frameworkbenefiting from strong consistency guarantees for any multi-label losssignificantly expanding upon previous work which only establishedBayes-consistency and for specific loss functions. Additionally we adaptconstrained losses from standard classification to multi-label constrainedlosses in a similar way which also benefit from H-consistency bounds andthus Bayes-consistency for any multi-label loss. We further describe efficientgradient computation algorithms for minimizing the multi-label logistic loss.</p>
                <p>Last Updated: 2024-07-18 17:51:02 UTC</p>
                <button class="interpret-button" data-id="2407.13746v1">Interpret</button>
                <div id="interpretation-2407.13746v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Optimistic Q-learning for average reward and episodic reinforcement learning</h3>
                <p>Authors: Priyank AgrawalShipra Agrawal</p>
                <p><a href="http://arxiv.org/abs/2407.13743v1">Link to paper</a></p>
                <p>We present an optimistic Q-learning algorithm for regret minimization inaverage reward reinforcement learning under an additional assumption on theunderlying MDP that for all policies the expected time to visit some frequentstate s_0 is finite and upper bounded by H. Our setting strictlygeneralizes the episodic setting and is significantly less restrictive than theassumption of bounded hitting time it for all states made by most previousliterature on model-free algorithms in average reward settings. We demonstratea regret bound of tildeOH5 SsqrtAT where S and A are thenumbers of states and actions and T is the horizon. A key technical noveltyof our work is to introduce an overlineL operator defined as overlineLv  frac1H sum_h1H Lh v where L denotes the Bellman operator. Weshow that under the given assumption the overlineL operator has a strictcontraction in span even in the average reward setting. Our algorithm designthen uses ideas from episodic Q-learning to estimate and apply this operatoriteratively. Therefore we provide a unified view of regret minimization inepisodic and non-episodic settings that may be of independent interest.</p>
                <p>Last Updated: 2024-07-18 17:49:09 UTC</p>
                <button class="interpret-button" data-id="2407.13743v1">Interpret</button>
                <div id="interpretation-2407.13743v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Dynamic Pricing in Securities Lending Market: Application in Revenue Optimization for an Agent Lender Portfolio</h3>
                <p>Authors: Jing XuYung Cheng HsuWilliam Biscarri</p>
                <p><a href="http://arxiv.org/abs/2407.13687v1">Link to paper</a></p>
                <p>Securities lending is an important part of the financial market structurewhere agent lenders help long term institutional investors to lend out theirsecurities to short sellers in exchange for a lending fee. Agent lenders withinthe market seek to optimize revenue by lending out securities at the highestrate possible. Typically this rate is set by hard-coded business rules orstandard supervised machine learning models. These approaches are oftendifficult to scale and are not adaptive to changing market conditions. Unlike atraditional stock exchange with a centralized limit order book the securitieslending market is organized similarly to an e-commerce marketplace where agentlenders and borrowers can transact at any agreed price in a bilateral fashion.This similarity suggests that the use of typical methods for addressing dynamicpricing problems in e-commerce could be effective in the securities lendingmarket. We show that existing contextual bandit frameworks can be successfullyutilized in the securities lending market. Using offline evaluation on realhistorical data we show that the contextual bandit approach can consistentlyoutperform typical approaches by at least 15 in terms of total revenuegenerated.</p>
                <p>Last Updated: 2024-07-18 17:42:37 UTC</p>
                <button class="interpret-button" data-id="2407.13687v1">Interpret</button>
                <div id="interpretation-2407.13687v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review</h3>
                <p>Authors: Masatoshi UeharaYulai ZhaoTommaso BiancalaniSergey Levine</p>
                <p><a href="http://arxiv.org/abs/2407.13734v1">Link to paper</a></p>
                <p>This tutorial provides a comprehensive survey of methods for fine-tuningdiffusion models to optimize downstream reward functions. While diffusionmodels are widely known to provide excellent generative modeling capabilitypractical applications in domains such as biology require generating samplesthat maximize some desired metric e.g. translation efficiency in RNA dockingscore in molecules stability in protein. In these cases the diffusion modelcan be optimized not only to generate realistic samples but also to explicitlymaximize the measure of interest. Such methods are based on concepts fromreinforcement learning RL. We explain the application of various RLalgorithms including PPO differentiable optimization reward-weighted MLEvalue-weighted sampling and path consistency learning tailored specificallyfor fine-tuning diffusion models. We aim to explore fundamental aspects such asthe strengths and limitations of different RL-based fine-tuning algorithmsacross various scenarios the benefits of RL-based fine-tuning compared tonon-RL-based approaches and the formal objectives of RL-based fine-tuningtarget distributions. Additionally we aim to examine their connections withrelated topics such as classifier guidance Gflownets flow-based diffusionmodels path integral control theory and sampling from unnormalizeddistributions such as MCMC. The code of this tutorial is available athttps://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq</p>
                <p>Last Updated: 2024-07-18 17:35:32 UTC</p>
                <button class="interpret-button" data-id="2407.13734v1">Interpret</button>
                <div id="interpretation-2407.13734v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>GroupMamba: Parameter-Efficient and Accurate Group Visual State Space Model</h3>
                <p>Authors: Abdelrahman ShakerSyed Talal WasimSalman KhanJuergen GallFahad Shahbaz Khan</p>
                <p><a href="http://arxiv.org/abs/2407.13772v1">Link to paper</a></p>
                <p>Recent advancements in state-space models SSMs have showcased effectiveperformance in modeling long-range dependencies with subquadratic complexity.However pure SSM-based models still face challenges related to stability andachieving optimal performance on computer vision tasks. Our paper addresses thechallenges of scaling SSM-based models for computer vision particularly theinstability and inefficiency of large model sizes. To address this weintroduce a Modulated Group Mamba layer which divides the input channels intofour groups and applies our proposed SSM-based efficient Visual SingleSelective Scanning VSSS block independently to each group with each VSSSblock scanning in one of the four spatial directions. The Modulated Group Mambalayer also wraps the four VSSS blocks into a channel modulation operator toimprove cross-channel communication. Furthermore we introduce adistillation-based training objective to stabilize the training of largemodels leading to consistent performance gains. Our comprehensive experimentsdemonstrate the merits of the proposed contributions leading to superiorperformance over existing methods for image classification on ImageNet-1Kobject detection instance segmentation on MS-COCO and semantic segmentationon ADE20K. Our tiny variant with 23M parameters achieves state-of-the-artperformance with a classification top-1 accuracy of 83.3 on ImageNet-1K whilebeing 26 efficient in terms of parameters compared to the best existing Mambadesign of same model size. Our code and models are available at:https://github.com/Amshaker/GroupMamba.</p>
                <p>Last Updated: 2024-07-18 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2407.13772v1">Interpret</button>
                <div id="interpretation-2407.13772v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Training-Free Model Merging for Multi-target Domain Adaptation</h3>
                <p>Authors: Wenyi LiHuan-ang GaoMingju GaoBeiwen TianRong ZhiHao Zhao</p>
                <p><a href="http://arxiv.org/abs/2407.13771v1">Link to paper</a></p>
                <p>In this paper we study multi-target domain adaptation of scene understandingmodels. While previous methods achieved commendable results throughinter-domain consistency losses they often assumed unrealistic simultaneousaccess to images from all target domains overlooking constraints such as datatransfer bandwidth limitations and data privacy concerns. Given thesechallenges we pose the question: How to merge models adapted independently ondistinct domains while bypassing the need for direct access to training dataOur solution to this problem involves two components merging model parametersand merging model buffers i.e. normalization layer statistics. For mergingmodel parameters empirical analyses of mode connectivity surprisingly revealthat linear merging suffices when employing the same pretrained backboneweights for adapting separate models. For merging model buffers we model thereal-world distribution with a Gaussian prior and estimate new statistics fromthe buffers of separately trained models. Our method is simple yet effectiveachieving comparable performance with data combination training baselineswhile eliminating the need for accessing training data. Project page:https://air-discover.github.io/ModelMerging</p>
                <p>Last Updated: 2024-07-18 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2407.13771v1">Interpret</button>
                <div id="interpretation-2407.13771v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Addressing Imbalance for Class Incremental Learning in Medical Image Classification</h3>
                <p>Authors: Xuze HaoWenqian NiXuhao JiangWeimin TanBo Yan</p>
                <p><a href="http://arxiv.org/abs/2407.13768v1">Link to paper</a></p>
                <p>Deep convolutional neural networks have made significant breakthroughs inmedical image classification under the assumption that training samples fromall classes are simultaneously available. However in real-world medicalscenarios theres a common need to continuously learn about new diseasesleading to the emerging field of class incremental learning CIL in themedical domain. Typically CIL suffers from catastrophic forgetting whentrained on new classes. This phenomenon is mainly caused by the imbalancebetween old and new classes and it becomes even more challenging withimbalanced medical datasets. In this work we introduce two simple yeteffective plug-in methods to mitigate the adverse effects of the imbalance.First we propose a CIL-balanced classification loss to mitigate the classifierbias toward majority classes via logit adjustment. Second we propose adistribution margin loss that not only alleviates the inter-class overlap inembedding space but also enforces the intra-class compactness. We evaluate theeffectiveness of our method with extensive experiments on three benchmarkdatasets CCH5000 HAM10000 and EyePACS. The results demonstrate that ourapproach outperforms state-of-the-art methods.</p>
                <p>Last Updated: 2024-07-18 17:59:44 UTC</p>
                <button class="interpret-button" data-id="2407.13768v1">Interpret</button>
                <div id="interpretation-2407.13768v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Visual Haystacks: Answering Harder Questions About Sets of Images</h3>
                <p>Authors: Tsung-Han WuGiscard BiambyJerome QuenumRitwik GuptaJoseph E. GonzalezTrevor DarrellDavid M. Chan</p>
                <p><a href="http://arxiv.org/abs/2407.13766v1">Link to paper</a></p>
                <p>Recent advancements in Large Multimodal Models LMMs have made significantprogress in the field of single-image visual question answering. However thesemodels face substantial challenges when tasked with queries that span extensivecollections of images similar to real-world scenarios like searching throughlarge photo albums finding specific information across the internet ormonitoring environmental changes through satellite imagery. This paper exploresthe task of Multi-Image Visual Question Answering MIQA: given a large set ofimages and a natural language query the task is to generate a relevant andgrounded response. We propose a new public benchmark dubbed Visual HaystacksVHs specifically designed to evaluate LMMs capabilities in visualretrieval and reasoning over sets of unrelated images where we performcomprehensive evaluations demonstrating that even robust closed-source modelsstruggle significantly. Towards addressing these shortcomings we introduceMIRAGE Multi-Image Retrieval Augmented Generation a novel retrieval/QAframework tailored for LMMs that confronts the challenges of MIQA with markedefficiency and accuracy improvements over baseline methods. Our evaluationshows that MIRAGE surpasses closed-source GPT-4o models by up to 11 on the VHsbenchmark and offers up to 3.4x improvements in efficiency over text-focusedmulti-stage approaches.</p>
                <p>Last Updated: 2024-07-18 17:59:30 UTC</p>
                <button class="interpret-button" data-id="2407.13766v1">Interpret</button>
                <div id="interpretation-2407.13766v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Shape of Motion: 4D Reconstruction from a Single Video</h3>
                <p>Authors: Qianqian WangVickie YeHang GaoJake AustinZhengqi LiAngjoo Kanazawa</p>
                <p><a href="http://arxiv.org/abs/2407.13764v1">Link to paper</a></p>
                <p>Monocular dynamic reconstruction is a challenging and long-standing visionproblem due to the highly ill-posed nature of the task. Existing approaches arelimited in that they either depend on templates are effective only inquasi-static scenes or fail to model 3D motion explicitly. In this work weintroduce a method capable of reconstructing generic dynamic scenes featuringexplicit full-sequence-long 3D motion from casually captured monocularvideos. We tackle the under-constrained nature of the problem with two keyinsights: First we exploit the low-dimensional structure of 3D motion byrepresenting scene motion with a compact set of SE3 motion bases. Each pointsmotion is expressed as a linear combination of these bases facilitating softdecomposition of the scene into multiple rigidly-moving groups. Second weutilize a comprehensive set of data-driven priors including monocular depthmaps and long-range 2D tracks and devise a method to effectively consolidatethese noisy supervisory signals resulting in a globally consistentrepresentation of the dynamic scene. Experiments show that our method achievesstate-of-the-art performance for both long-range 3D/2D motion estimation andnovel view synthesis on dynamic scenes. Project Page:https://shape-of-motion.github.io/</p>
                <p>Last Updated: 2024-07-18 17:59:08 UTC</p>
                <button class="interpret-button" data-id="2407.13764v1">Interpret</button>
                <div id="interpretation-2407.13764v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Latent Causal Probing: A Formal Perspective on Probing with Causal Models of Data</h3>
                <p>Authors: Charles Jin</p>
                <p><a href="http://arxiv.org/abs/2407.13765v1">Link to paper</a></p>
                <p>As language models LMs deliver increasing performance on a range of NLPtasks probing classifiers have become an indispensable technique in the effortto better understand their inner workings. A typical setup involves 1defining an auxiliary task consisting of a dataset of text annotated withlabels then 2 supervising small classifiers to predict the labels from therepresentations of a pretrained LM as it processed the dataset. A high probingaccuracy is interpreted as evidence that the LM has learned to perform theauxiliary task as an unsupervised byproduct of its original pretrainingobjective. Despite the widespread usage of probes however the robust designand analysis of probing experiments remains a challenge. We develop a formalperspective on probing using structural causal models SCM. Specificallygiven an SCM which explains the distribution of tokens observed duringtraining we frame the central hypothesis as whether the LM has learned torepresent the latent variables of the SCM. Empirically we extend a recentstudy of LMs in the context of a synthetic grid-world navigation task wherehaving an exact model of the underlying causal structure allows us to drawstrong inferences from the result of probing experiments. Our techniquesprovide robust empirical evidence for the ability of LMs to learn the latentcausal concepts underlying text.</p>
                <p>Last Updated: 2024-07-18 17:59:27 UTC</p>
                <button class="interpret-button" data-id="2407.13765v1">Interpret</button>
                <div id="interpretation-2407.13765v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Neural Network Tire Force Modeling for Automated Drifting</h3>
                <p>Authors: Nicholas Drake BroadbentTrey WeberDaiki MoriJ. Christian Gerdes</p>
                <p><a href="http://arxiv.org/abs/2407.13760v1">Link to paper</a></p>
                <p>Automated drifting presents a challenge problem for vehicle controlrequiring models and control algorithms that can precisely handle nonlinearcoupled tire forces at the friction limits. We present a neural networkarchitecture for predicting front tire lateral force as a drop-in replacementfor physics-based approaches. With a full-scale automated vehicle purpose-builtfor the drifting application we deploy these models in a nonlinear modelpredictive controller tuned for tracking a reference drifting trajectory fordirect comparisons of model performance. The neural network tire model exhibitssignificantly improved path tracking performance over the brush tire model incases where front-axle braking force is applied suggesting the neuralnetworks ability to express previously unmodeled latent dynamics in thedrifting condition.</p>
                <p>Last Updated: 2024-07-18 17:58:01 UTC</p>
                <button class="interpret-button" data-id="2407.13760v1">Interpret</button>
                <div id="interpretation-2407.13760v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models</h3>
                <p>Authors: Zhuo ChenJiawei LiuHaotan LiuQikai ChengFan ZhangWei LuXiaozhong Liu</p>
                <p><a href="http://arxiv.org/abs/2407.13757v1">Link to paper</a></p>
                <p>Retrieval-Augmented Generation RAG is applied to solve hallucinationproblems and real-time constraints of large language models but it alsoinduces vulnerabilities against retrieval corruption attacks. Existing researchmainly explores the unreliability of RAG in white-box and closed-domain QAtasks. In this paper we aim to reveal the vulnerabilities ofRetrieval-Enhanced Generative RAG models when faced with black-box attacksfor opinion manipulation. We explore the impact of such attacks on usercognition and decision-making providing new insight to enhance the reliabilityand security of RAG models. We manipulate the ranking results of the retrievalmodel in RAG with instruction and use these results as data to train asurrogate model. By employing adversarial retrieval attack methods to thesurrogate model black-box transfer attacks on RAG are further realized.Experiments conducted on opinion datasets across multiple topics show that theproposed attack strategy can significantly alter the opinion polarity of thecontent generated by RAG. This demonstrates the models vulnerability and moreimportantly reveals the potential negative impact on user cognition anddecision-making making it easier to mislead users into accepting incorrect orbiased information.</p>
                <p>Last Updated: 2024-07-18 17:55:55 UTC</p>
                <button class="interpret-button" data-id="2407.13757v1">Interpret</button>
                <div id="interpretation-2407.13757v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Temporal Representation Learning for Stock Similarities and Its Applications in Investment Management</h3>
                <p>Authors: Yoontae HwangStefan ZohrenYongjae Lee</p>
                <p><a href="http://arxiv.org/abs/2407.13751v1">Link to paper</a></p>
                <p>In the era of rapid globalization and digitalization accurate identificationof similar stocks has become increasingly challenging due to the non-stationarynature of financial markets and the ambiguity in conventional regional andsector classifications. To address these challenges we examine SimStock anovel temporal self-supervised learning framework that combines techniques fromself-supervised learning SSL and temporal domain generalization to learnrobust and informative representations of financial time series data. Theprimary focus of our study is to understand the similarities between stocksfrom a broader perspective considering the complex dynamics of the globalfinancial landscape. We conduct extensive experiments on four real-worlddatasets with thousands of stocks and demonstrate the effectiveness of SimStockin finding similar stocks outperforming existing methods. The practicalutility of SimStock is showcased through its application to various investmentstrategies such as pairs trading index tracking and portfolio optimizationwhere it leads to superior performance compared to conventional methods. Ourfindings empirically examine the potential of data-driven approach to enhanceinvestment decision-making and risk management practices by leveraging thepower of temporal self-supervised learning in the face of the ever-changingglobal financial landscape.</p>
                <p>Last Updated: 2024-07-18 17:54:13 UTC</p>
                <button class="interpret-button" data-id="2407.13751v1">Interpret</button>
                <div id="interpretation-2407.13751v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLMs as Function Approximators: Terminology, Taxonomy, and Questions for Evaluation</h3>
                <p>Authors: David Schlangen</p>
                <p><a href="http://arxiv.org/abs/2407.13744v1">Link to paper</a></p>
                <p>Natural Language Processing has moved rather quickly from modelling specifictasks to taking more general pre-trained models and fine-tuning them forspecific tasks to a point where we now have what appear to be inherentlygeneralist models. This paper argues that the resultant loss of clarity on whatthese models model leads to metaphors like artificial general intelligencesthat are not helpful for evaluating their strengths and weaknesses. Theproposal is to see their generality and their potential value in theirability to approximate specialist function based on a natural languagespecification. This framing brings to the fore questions of the quality of theapproximation but beyond that also questions of discoverability stabilityand protectability of these functions. As the paper will show this framinghence brings together in one conceptual framework various aspects ofevaluation both from a practical and a theoretical perspective as well asquestions often relegated to a secondary status such as prompt injection andjailbreaking.</p>
                <p>Last Updated: 2024-07-18 17:49:56 UTC</p>
                <button class="interpret-button" data-id="2407.13744v1">Interpret</button>
                <div id="interpretation-2407.13744v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Multi-Label Learning with Stronger Consistency Guarantees</h3>
                <p>Authors: Anqi MaoMehryar MohriYutao Zhong</p>
                <p><a href="http://arxiv.org/abs/2407.13746v1">Link to paper</a></p>
                <p>We present a detailed study of surrogate losses and algorithms formulti-label learning supported by H-consistency bounds. We first show thatfor the simplest form of multi-label loss the popular Hamming loss thewell-known consistent binary relevance surrogate suffers from a sub-optimaldependency on the number of labels in terms of H-consistency bounds whenusing smooth losses such as logistic losses. Furthermore this loss functionfails to account for label correlations. To address these drawbacks weintroduce a novel surrogate loss multi-label logistic loss that accounts forlabel correlations and benefits from label-independent H-consistency bounds.We then broaden our analysis to cover a more extensive family of multi-labellosses including all common ones and a new extension defined based onlinear-fractional functions with respect to the confusion matrix. We alsoextend our multi-label logistic losses to more comprehensive multi-labelcomp-sum losses adapting comp-sum losses from standard classification to themulti-label learning. We prove that this family of surrogate losses benefitsfrom H-consistency bounds and thus Bayes-consistency across any generalmulti-label loss. Our work thus proposes a unified surrogate loss frameworkbenefiting from strong consistency guarantees for any multi-label losssignificantly expanding upon previous work which only establishedBayes-consistency and for specific loss functions. Additionally we adaptconstrained losses from standard classification to multi-label constrainedlosses in a similar way which also benefit from H-consistency bounds andthus Bayes-consistency for any multi-label loss. We further describe efficientgradient computation algorithms for minimizing the multi-label logistic loss.</p>
                <p>Last Updated: 2024-07-18 17:51:02 UTC</p>
                <button class="interpret-button" data-id="2407.13746v1">Interpret</button>
                <div id="interpretation-2407.13746v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Optimistic Q-learning for average reward and episodic reinforcement learning</h3>
                <p>Authors: Priyank AgrawalShipra Agrawal</p>
                <p><a href="http://arxiv.org/abs/2407.13743v1">Link to paper</a></p>
                <p>We present an optimistic Q-learning algorithm for regret minimization inaverage reward reinforcement learning under an additional assumption on theunderlying MDP that for all policies the expected time to visit some frequentstate s_0 is finite and upper bounded by H. Our setting strictlygeneralizes the episodic setting and is significantly less restrictive than theassumption of bounded hitting time it for all states made by most previousliterature on model-free algorithms in average reward settings. We demonstratea regret bound of tildeOH5 SsqrtAT where S and A are thenumbers of states and actions and T is the horizon. A key technical noveltyof our work is to introduce an overlineL operator defined as overlineLv  frac1H sum_h1H Lh v where L denotes the Bellman operator. Weshow that under the given assumption the overlineL operator has a strictcontraction in span even in the average reward setting. Our algorithm designthen uses ideas from episodic Q-learning to estimate and apply this operatoriteratively. Therefore we provide a unified view of regret minimization inepisodic and non-episodic settings that may be of independent interest.</p>
                <p>Last Updated: 2024-07-18 17:49:09 UTC</p>
                <button class="interpret-button" data-id="2407.13743v1">Interpret</button>
                <div id="interpretation-2407.13743v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review</h3>
                <p>Authors: Masatoshi UeharaYulai ZhaoTommaso BiancalaniSergey Levine</p>
                <p><a href="http://arxiv.org/abs/2407.13734v1">Link to paper</a></p>
                <p>This tutorial provides a comprehensive survey of methods for fine-tuningdiffusion models to optimize downstream reward functions. While diffusionmodels are widely known to provide excellent generative modeling capabilitypractical applications in domains such as biology require generating samplesthat maximize some desired metric e.g. translation efficiency in RNA dockingscore in molecules stability in protein. In these cases the diffusion modelcan be optimized not only to generate realistic samples but also to explicitlymaximize the measure of interest. Such methods are based on concepts fromreinforcement learning RL. We explain the application of various RLalgorithms including PPO differentiable optimization reward-weighted MLEvalue-weighted sampling and path consistency learning tailored specificallyfor fine-tuning diffusion models. We aim to explore fundamental aspects such asthe strengths and limitations of different RL-based fine-tuning algorithmsacross various scenarios the benefits of RL-based fine-tuning compared tonon-RL-based approaches and the formal objectives of RL-based fine-tuningtarget distributions. Additionally we aim to examine their connections withrelated topics such as classifier guidance Gflownets flow-based diffusionmodels path integral control theory and sampling from unnormalizeddistributions such as MCMC. The code of this tutorial is available athttps://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq</p>
                <p>Last Updated: 2024-07-18 17:35:32 UTC</p>
                <button class="interpret-button" data-id="2407.13734v1">Interpret</button>
                <div id="interpretation-2407.13734v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Realizable $H$-Consistent and Bayes-Consistent Loss Functions for Learning to Defer</h3>
                <p>Authors: Anqi MaoMehryar MohriYutao Zhong</p>
                <p><a href="http://arxiv.org/abs/2407.13732v1">Link to paper</a></p>
                <p>We present a comprehensive study of surrogate loss functions for learning todefer. We introduce a broad family of surrogate losses parameterized by anon-increasing function Psi and establish their realizable H-consistencyunder mild conditions. For cost functions based on classification error wefurther show that these losses admit H-consistency bounds when the hypothesisset is symmetric and complete a property satisfied by common neural networkand linear function hypothesis sets. Our results also resolve an open questionraised in previous work Mozannar et al. 2023 by proving the realizableH-consistency and Bayes-consistency of a specific surrogate loss.Furthermore we identify choices of Psi that lead to H-consistentsurrogate losses for any general cost function thus achievingBayes-consistency realizable H-consistency and H-consistency boundssimultaneously. We also investigate the relationship between H-consistencybounds and realizable H-consistency in learning to defer highlighting keydifferences from standard classification. Finally we empirically evaluate ourproposed surrogate losses and compare them with existing baselines.</p>
                <p>Last Updated: 2024-07-18 17:35:03 UTC</p>
                <button class="interpret-button" data-id="2407.13732v1">Interpret</button>
                <div id="interpretation-2407.13732v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Predictive Low Rank Matrix Learning under Partial Observations: Mixed-Projection ADMM</h3>
                <p>Authors: Dimitris BertsimasNicholas A. G. Johnson</p>
                <p><a href="http://arxiv.org/abs/2407.13731v1">Link to paper</a></p>
                <p>We study the problem of learning a partially observed matrix under the lowrank assumption in the presence of fully observed side information that dependslinearly on the true underlying matrix. This problem consists of an importantgeneralization of the Matrix Completion problem a central problem inStatistics Operations Research and Machine Learning that arises inapplications such as recommendation systems signal processing systemidentification and image denoising. We formalize this problem as anoptimization problem with an objective that balances the strength of the fit ofthe reconstruction to the observed entries with the ability of thereconstruction to be predictive of the side information. We derive amixed-projection reformulation of the resulting optimization problem andpresent a strong semidefinite cone relaxation. We design an efficient scalablealternating direction method of multipliers algorithm that produces highquality feasible solutions to the problem of interest. Our numerical resultsdemonstrate that in the small rank regime k leq 15 our algorithm outputssolutions that achieve on average 79 lower objective value and 90.1lower ell_2 reconstruction error than the solutions returned by theexperiment-wise best performing benchmark method. The runtime of our algorithmis competitive with and often superior to that of the benchmark methods. Ouralgorithm is able to solve problems with n  10000 rows and m  10000columns in less than a minute.</p>
                <p>Last Updated: 2024-07-18 17:33:14 UTC</p>
                <button class="interpret-button" data-id="2407.13731v1">Interpret</button>
                <div id="interpretation-2407.13731v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-07-21</p>
        </div>
    
        </div>
    </body>
    </html>
    