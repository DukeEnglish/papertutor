
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models</h3>
                <p>Authors: Prannay KaulZhizhong LiHao YangYonatan DuklerAshwin SwaminathanC. J. TaylorStefano Soatto</p>
                <p><a href="http://arxiv.org/abs/2405.05256v1">Link to paper</a></p>
                <p>Mitigating hallucinations in large vision-language models LVLMs remains anopen problem. Recent benchmarks do not address hallucinations in open-endedfree-form responses which we term Type I hallucinations. Instead they focuson hallucinations responding to very specific question formats -- typically amultiple-choice response regarding a particular object or attribute -- which weterm Type II hallucinations. Additionally such benchmarks often requireexternal API calls to models which are subject to change. In practice weobserve that a reduction in Type II hallucinations does not lead to a reductionin Type I hallucinations but rather that the two forms of hallucinations areoften anti-correlated. To address this we propose THRONE a novel object-basedautomatic framework for quantitatively evaluating Type I hallucinations in LVLMfree-form outputs. We use public language models LMs to identifyhallucinations in LVLM responses and compute informative metrics. By evaluatinga large selection of recent LVLMs using public datasets we show that animprovement in existing metrics do not lead to a reduction in Type Ihallucinations and that established benchmarks for measuring Type Ihallucinations are incomplete. Finally we provide a simple and effective dataaugmentation method to reduce Type I and Type II hallucinations as a strongbaseline.</p>
                <p>Last Updated: 2024-05-08 17:59:11 UTC</p>
                <button class="interpret-button" data-id="2405.05256v1">Interpret</button>
                <div id="interpretation-2405.05256v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge</h3>
                <p>Authors: Charles KoutchemeNicola DaineseSami SarsaArto HellasJuho LeinonenPaul Denny</p>
                <p><a href="http://arxiv.org/abs/2405.05253v1">Link to paper</a></p>
                <p>Large language models LLMs have shown great potential for the automaticgeneration of feedback in a wide range of computing contexts. However concernshave been voiced around the privacy and ethical implications of sending studentwork to proprietary models. This has sparked considerable interest in the useof open source LLMs in education but the quality of the feedback that suchopen models can produce remains understudied. This is a concern as providingflawed or misleading generated feedback could be detrimental to studentlearning. Inspired by recent work that has utilised very powerful LLMs such asGPT-4 to evaluate the outputs produced by less powerful models we conduct anautomated analysis of the quality of the feedback produced by several opensource models using a dataset from an introductory programming course. Firstwe investigate the viability of employing GPT-4 as an automated evaluator bycomparing its evaluations with those of a human expert. We observe that GPT-4demonstrates a bias toward positively rating feedback while exhibiting moderateagreement with human raters showcasing its potential as a feedback evaluator.Second we explore the quality of feedback generated by several leadingopen-source LLMs by using GPT-4 to evaluate the feedback. We find that somemodels offer competitive performance with popular proprietary LLMs such asChatGPT indicating opportunities for their responsible use in educationalsettings.</p>
                <p>Last Updated: 2024-05-08 17:57:39 UTC</p>
                <button class="interpret-button" data-id="2405.05253v1">Interpret</button>
                <div id="interpretation-2405.05253v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models</h3>
                <p>Authors: Hongjie WangDifan LiuYan KangYijun LiZhe LinNiraj K. JhaYuchen Liu</p>
                <p><a href="http://arxiv.org/abs/2405.05252v1">Link to paper</a></p>
                <p>Diffusion Models DMs have exhibited superior performance in generatinghigh-quality and diverse images. However this exceptional performance comes atthe cost of expensive architectural design particularly due to the attentionmodule heavily used in leading models. Existing works mainly adopt a retrainingprocess to enhance DM efficiency. This is computationally expensive and notvery scalable. To this end we introduce the Attention-driven Training-freeEfficient Diffusion Model AT-EDM framework that leverages attention maps toperform run-time pruning of redundant tokens without the need for anyretraining. Specifically for single-denoising-step pruning we develop a novelranking algorithm Generalized Weighted Page Rank G-WPR to identifyredundant tokens and a similarity-based recovery method to restore tokens forthe convolution operation. In addition we propose a Denoising-Steps-AwarePruning DSAP approach to adjust the pruning budget across different denoisingtimesteps for better generation quality. Extensive evaluations show that AT-EDMperforms favorably against prior art in terms of efficiency e.g. 38.8 FLOPssaving and up to 1.53x speed-up over Stable Diffusion XL while maintainingnearly the same FID and CLIP scores as the full model. Project webpage:https://atedm.github.io.</p>
                <p>Last Updated: 2024-05-08 17:56:47 UTC</p>
                <button class="interpret-button" data-id="2405.05252v1">Interpret</button>
                <div id="interpretation-2405.05252v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLMs with Personalities in Multi-issue Negotiation Games</h3>
                <p>Authors: Sean NohHo-Chun Herbert Chang</p>
                <p><a href="http://arxiv.org/abs/2405.05248v1">Link to paper</a></p>
                <p>Powered by large language models LLMs AI agents have become capable ofmany human tasks. Using the most canonical definitions of the Big Fivepersonality we measure the ability of LLMs to negotiate within agame-theoretical framework as well as methodological challenges to measuringnotions of fairness and risk. Simulations n1500 for both single-issue andmulti-issue negotiation reveal increase in domain complexity with asymmetricissue valuations improve agreement rates but decrease surplus from aggressivenegotiation. Through gradient-boosted regression and Shapley explainers wefind high openness conscientiousness and neuroticism are associated with fairtendencies low agreeableness and low openness are associated with rationaltendencies. Low conscientiousness is associated with high toxicity. Theseresults indicate that LLMs may have built-in guardrails that default to fairbehavior but can be jail broken to exploit agreeable opponents. We alsooffer pragmatic insight in how negotiation bots can be designed and aframework of assessing negotiation behavior based on game theory andcomputational social science.</p>
                <p>Last Updated: 2024-05-08 17:51:53 UTC</p>
                <button class="interpret-button" data-id="2405.05248v1">Interpret</button>
                <div id="interpretation-2405.05248v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge Evaluation Plan</h3>
                <p>Authors: You ZhangYongyi ZangJiatong ShiRyuichi YamamotoJionghao HanYuxun TangTomoki TodaZhiyao Duan</p>
                <p><a href="http://arxiv.org/abs/2405.05244v1">Link to paper</a></p>
                <p>The rapid advancement of AI-generated singing voices which now closely mimicnatural human singing and align seamlessly with musical scores has led toheightened concerns for artists and the music industry. Unlike spoken voicesinging voice presents unique challenges due to its musical nature and thepresence of strong background music making singing voice deepfake detectionSVDD a specialized field requiring focused attention. To promote SVDDresearch we recently proposed the SVDD Challenge the very first researchchallenge focusing on SVDD for lab-controlled and in-the-wild bonafide anddeepfake singing voice recordings. The challenge will be held in conjunctionwith the 2024 IEEE Spoken Language Technology Workshop SLT 2024.</p>
                <p>Last Updated: 2024-05-08 17:40:12 UTC</p>
                <button class="interpret-button" data-id="2405.05244v1">Interpret</button>
                <div id="interpretation-2405.05244v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Is Transductive Learning Equivalent to PAC Learning?</h3>
                <p>Authors: Shaddin DughmiYusuf KalayciGrayson York</p>
                <p><a href="http://arxiv.org/abs/2405.05190v1">Link to paper</a></p>
                <p>Most work in the area of learning theory has focused on designing effectiveProbably Approximately Correct PAC learners. Recently other models oflearning such as transductive error have seen more scrutiny. We move towardshowing that these problems are equivalent by reducing agnostic learning with aPAC guarantee to agnostic learning with a transductive guarantee by adding asmall number of samples to the dataset. We first rederive the result ofAden-Ali et al. arXiv:2304.09167 reducing PAC learning to transductive learningin the realizable setting using simpler techniques and at more generality asbackground for our main positive result. Our agnostic transductive to PACconversion technique extends the aforementioned argument to the agnostic caseshowing that an agnostic transductive learner can be efficiently converted toan agnostic PAC learner. Finally we characterize the performance of theagnostic one inclusion graph algorithm of Asilis et al. arXiv:2309.13692 forbinary classification and show that plugging it into our reduction leads to anagnostic PAC learner that is essentially optimal. Our results imply thattransductive and PAC learning are essentially equivalent for supervisedlearning with pseudometric losses in the realizable setting and for binaryclassification in the agnostic setting. We conjecture this is true moregenerally for the agnostic setting.</p>
                <p>Last Updated: 2024-05-08 16:26:49 UTC</p>
                <button class="interpret-button" data-id="2405.05190v1">Interpret</button>
                <div id="interpretation-2405.05190v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Uncertainty quantification in metric spaces</h3>
                <p>Authors: GÃ¡bor LugosiMarcos Matabuena</p>
                <p><a href="http://arxiv.org/abs/2405.05110v1">Link to paper</a></p>
                <p>This paper introduces a novel uncertainty quantification framework forregression models where the response takes values in a separable metric spaceand the predictors are in a Euclidean space. The proposed algorithms canefficiently handle large datasets and are agnostic to the predictive base modelused. Furthermore the algorithms possess asymptotic consistency guaranteesand in some special homoscedastic cases we provide non-asymptotic guarantees.To illustrate the effectiveness of the proposed uncertainty quantificationframework we use a linear regression model for metric responses known as theglobal Frechet model in various clinical applications related to precisionand digital medicine. The different clinical outcomes analyzed are representedas complex statistical objects including multivariate Euclidean dataLaplacian graphs and probability distributions.</p>
                <p>Last Updated: 2024-05-08 15:06:02 UTC</p>
                <button class="interpret-button" data-id="2405.05110v1">Interpret</button>
                <div id="interpretation-2405.05110v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Biology-inspired joint distribution neurons based on Hierarchical Correlation Reconstruction allowing for multidirectional neural networks</h3>
                <p>Authors: Jarek Duda</p>
                <p><a href="http://arxiv.org/abs/2405.05097v1">Link to paper</a></p>
                <p>Popular artificial neural networks ANN optimize parameters forunidirectional value propagation assuming some guessed parametrization typelike Multi-Layer Perceptron MLP or Kolmogorov-Arnold Network KAN. Incontrast for biological neurons e.g. it is not uncommon for axonalpropagation of action potentials to happen in both directions citeaxon -suggesting they are optimized to continuously operate in multidirectional way.Additionally statistical dependencies a single neuron could model is not justexpected value dependence but entire joint distributions including alsohigher moments. Such agnostic joint distribution neuron would allow formultidirectional propagation of distributions or values e.g. rhoxyz orrhoyzx by substituting to rhoxyz and normalizing. There will bediscussed Hierarchical Correlation Reconstruction HCR for such neuron model:assuming rhoxyzsum_ijk a_ijk f_ix f_jy f_kz typeparametrization of joint distribution with polynomial basis f_i which allowsfor flexible inexpensive processing including nonlinearities direct modelestimation and update trained through standard backpropagation or novel waysfor such structure up to tensor decomposition. Using only pairwiseinput-output dependencies its expected value prediction becomes KAN-likewith trained activation functions as polynomials can be extended by addinghigher order dependencies through included products - in consciousinterpretable way allowing for multidirectional propagation of both values andprobability densities.</p>
                <p>Last Updated: 2024-05-08 14:49:27 UTC</p>
                <button class="interpret-button" data-id="2405.05097v1">Interpret</button>
                <div id="interpretation-2405.05097v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Robust deep learning from weakly dependent data</h3>
                <p>Authors: William KengneModou Wade</p>
                <p><a href="http://arxiv.org/abs/2405.05081v1">Link to paper</a></p>
                <p>Recent developments on deep learning established some theoretical propertiesof deep neural networks estimators. However most of the existing works on thistopic are restricted to bounded loss functions or sub-Gaussian or boundedinput. This paper considers robust deep learning from weakly dependentobservations with unbounded loss function and unbounded input/output. It isonly assumed that the output variable has a finite r order moment with r1. Non asymptotic bounds for the expected excess risk of the deep neuralnetwork estimator are established under strong mixing and psi-weakdependence assumptions on the observations. We derive a relationship betweenthese bounds and r and when the data have moments of any order that isrinfty the convergence rate is close to some well-known results. When thetarget predictor belongs to the class of Holder smooth functions withsufficiently large smoothness index the rate of the expected excess risk forexponentially strongly mixing data is close to or as same as those for obtainedwith i.i.d. samples. Application to robust nonparametric regression and robustnonparametric autoregression are considered. The simulation study for modelswith heavy-tailed errors shows that robust estimators with absolute loss andHuber loss function outperform the least squares method.</p>
                <p>Last Updated: 2024-05-08 14:25:40 UTC</p>
                <button class="interpret-button" data-id="2405.05081v1">Interpret</button>
                <div id="interpretation-2405.05081v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-fidelity Hamiltonian Monte Carlo</h3>
                <p>Authors: Dhruv V. PatelJonghyun LeeMatthew W. FarthingPeter K. KitanidisEric F. Darve</p>
                <p><a href="http://arxiv.org/abs/2405.05033v1">Link to paper</a></p>
                <p>Numerous applications in biology statistics science and engineeringrequire generating samples from high-dimensional probability distributions. Inrecent years the Hamiltonian Monte Carlo HMC method has emerged as astate-of-the-art Markov chain Monte Carlo technique exploiting the shape ofsuch high-dimensional target distributions to efficiently generate samples.Despite its impressive empirical success and increasing popularity itswide-scale adoption remains limited due to the high computational cost ofgradient calculation. Moreover applying this method is impossible when thegradient of the posterior cannot be computed for example with black-boxsimulators. To overcome these challenges we propose a novel two-stageHamiltonian Monte Carlo algorithm with a surrogate model. In thismulti-fidelity algorithm the acceptance probability is computed in the firststage via a standard HMC proposal using an inexpensive differentiable surrogatemodel and if the proposal is accepted the posterior is evaluated in thesecond stage using the high-fidelity HF numerical solver. Splitting thestandard HMC algorithm into these two stages allows for approximating thegradient of the posterior efficiently while producing accurate posteriorsamples by using HF numerical solvers in the second stage. We demonstrate theeffectiveness of this algorithm for a range of problems including linear andnonlinear Bayesian inverse problems with in-silico data and experimental data.The proposed algorithm is shown to seamlessly integrate with variouslow-fidelity and HF models priors and datasets. Remarkably our proposedmethod outperforms the traditional HMC algorithm in both computational andstatistical efficiency by several orders of magnitude all while retaining orimproving the accuracy in computed posterior statistics.</p>
                <p>Last Updated: 2024-05-08 13:03:55 UTC</p>
                <button class="interpret-button" data-id="2405.05033v1">Interpret</button>
                <div id="interpretation-2405.05033v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge</h3>
                <p>Authors: Charles KoutchemeNicola DaineseSami SarsaArto HellasJuho LeinonenPaul Denny</p>
                <p><a href="http://arxiv.org/abs/2405.05253v1">Link to paper</a></p>
                <p>Large language models LLMs have shown great potential for the automaticgeneration of feedback in a wide range of computing contexts. However concernshave been voiced around the privacy and ethical implications of sending studentwork to proprietary models. This has sparked considerable interest in the useof open source LLMs in education but the quality of the feedback that suchopen models can produce remains understudied. This is a concern as providingflawed or misleading generated feedback could be detrimental to studentlearning. Inspired by recent work that has utilised very powerful LLMs such asGPT-4 to evaluate the outputs produced by less powerful models we conduct anautomated analysis of the quality of the feedback produced by several opensource models using a dataset from an introductory programming course. Firstwe investigate the viability of employing GPT-4 as an automated evaluator bycomparing its evaluations with those of a human expert. We observe that GPT-4demonstrates a bias toward positively rating feedback while exhibiting moderateagreement with human raters showcasing its potential as a feedback evaluator.Second we explore the quality of feedback generated by several leadingopen-source LLMs by using GPT-4 to evaluate the feedback. We find that somemodels offer competitive performance with popular proprietary LLMs such asChatGPT indicating opportunities for their responsible use in educationalsettings.</p>
                <p>Last Updated: 2024-05-08 17:57:39 UTC</p>
                <button class="interpret-button" data-id="2405.05253v1">Interpret</button>
                <div id="interpretation-2405.05253v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>You Only Cache Once: Decoder-Decoder Architectures for Language Models</h3>
                <p>Authors: Yutao SunLi DongYi ZhuShaohan HuangWenhui WangShuming MaQuanlu ZhangJianyong WangFuru Wei</p>
                <p><a href="http://arxiv.org/abs/2405.05254v1">Link to paper</a></p>
                <p>We introduce a decoder-decoder architecture YOCO for large language modelswhich only caches key-value pairs once. It consists of two components i.e. across-decoder stacked upon a self-decoder. The self-decoder efficiently encodesglobal key-value KV caches that are reused by the cross-decoder viacross-attention. The overall model behaves like a decoder-only Transformeralthough YOCO only caches once. The design substantially reduces GPU memorydemands yet retains global attention capability. Additionally the computationflow enables prefilling to early exit without changing the final outputthereby significantly speeding up the prefill stage. Experimental resultsdemonstrate that YOCO achieves favorable performance compared to Transformer invarious settings of scaling up model size and number of training tokens. Wealso extend YOCO to 1M context length with near-perfect needle retrievalaccuracy. The profiling results show that YOCO improves inference memoryprefill latency and throughput by orders of magnitude across context lengthsand model sizes. Code is available at https://aka.ms/YOCO.</p>
                <p>Last Updated: 2024-05-08 17:57:39 UTC</p>
                <button class="interpret-button" data-id="2405.05254v1">Interpret</button>
                <div id="interpretation-2405.05254v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLMs with Personalities in Multi-issue Negotiation Games</h3>
                <p>Authors: Sean NohHo-Chun Herbert Chang</p>
                <p><a href="http://arxiv.org/abs/2405.05248v1">Link to paper</a></p>
                <p>Powered by large language models LLMs AI agents have become capable ofmany human tasks. Using the most canonical definitions of the Big Fivepersonality we measure the ability of LLMs to negotiate within agame-theoretical framework as well as methodological challenges to measuringnotions of fairness and risk. Simulations n1500 for both single-issue andmulti-issue negotiation reveal increase in domain complexity with asymmetricissue valuations improve agreement rates but decrease surplus from aggressivenegotiation. Through gradient-boosted regression and Shapley explainers wefind high openness conscientiousness and neuroticism are associated with fairtendencies low agreeableness and low openness are associated with rationaltendencies. Low conscientiousness is associated with high toxicity. Theseresults indicate that LLMs may have built-in guardrails that default to fairbehavior but can be jail broken to exploit agreeable opponents. We alsooffer pragmatic insight in how negotiation bots can be designed and aframework of assessing negotiation behavior based on game theory andcomputational social science.</p>
                <p>Last Updated: 2024-05-08 17:51:53 UTC</p>
                <button class="interpret-button" data-id="2405.05248v1">Interpret</button>
                <div id="interpretation-2405.05248v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CARE-SD: Classifier-based analysis for recognizing and eliminating stigmatizing and doubt marker labels in electronic health records: model development and validation</h3>
                <p>Authors: Drew WalkerAnnie ThorneSudeshna DasJennifer LoveHannah LF CooperMelvin Livingston IIIAbeed Sarker</p>
                <p><a href="http://arxiv.org/abs/2405.05204v1">Link to paper</a></p>
                <p>Objective: To detect and classify features of stigmatizing and biasedlanguage in intensive care electronic health records EHRs using naturallanguage processing techniques. Materials and Methods: We first created alexicon and regular expression lists from literature-driven stem words forlinguistic features of stigmatizing patient labels doubt markers and scarequotes within EHRs. The lexicon was further extended using Word2Vec and GPT3.5 and refined through human evaluation. These lexicons were used to searchfor matches across 18 million sentences from the de-identified MedicalInformation Mart for Intensive Care-III MIMIC-III dataset. For eachlinguistic bias feature 1000 sentence matches were sampled labeled by expertclinical and public health annotators and used to supervised learningclassifiers. Results: Lexicon development from expanded literature stem-wordlists resulted in a doubt marker lexicon containing 58 expressions and astigmatizing labels lexicon containing 127 expressions. Classifiers for doubtmarkers and stigmatizing labels had the highest performance with macroF1-scores of .84 and .79 positive-label recall and precision values rangingfrom .71 to .86 and accuracies aligning closely with human annotator agreement.87. Discussion: This study demonstrated the feasibility of supervisedclassifiers in automatically identifying stigmatizing labels and doubt markersin medical text and identified trends in stigmatizing language use in an EHRsetting. Additional labeled data may help improve lower scare quote modelperformance. Conclusions: Classifiers developed in this study showed high modelperformance and can be applied to identify patterns and target interventions toreduce stigmatizing labels and doubt markers in healthcare systems.</p>
                <p>Last Updated: 2024-05-08 16:40:18 UTC</p>
                <button class="interpret-button" data-id="2405.05204v1">Interpret</button>
                <div id="interpretation-2405.05204v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning</h3>
                <p>Authors: Inderjeet NairLu Wang</p>
                <p><a href="http://arxiv.org/abs/2405.05189v1">Link to paper</a></p>
                <p>We study the task of conducting structured reasoning as generating areasoning graph from natural language input using large language models LLMs.Previous approaches have explored various prompting schemes yet they sufferfrom error propagation due to the autoregressive nature and single-pass-baseddecoding which lack error correction capability. Additionally relying solelyon a single sample may result in the omission of true nodes and edges. Tocounter this we draw inspiration from self-consistency SC which involvessampling a diverse set of reasoning chains and taking the majority vote as thefinal answer. To tackle the substantial challenge of applying SC on generatedgraphs we propose MIDGARD MInimum Description length Guided Aggregation ofReasoning in Directed acyclic graph that leverages Minimum Description LengthMDL-based formulation to identify consistent properties among the differentgraph samples generated by an LLM. This formulation helps reject propertiesthat appear in only a few samples which are likely to be erroneous whileenabling the inclusion of missing elements without compromising precision. Ourmethod demonstrates superior performance than comparisons across variousstructured reasoning tasks including argument structure extractionexplanation graph generation inferring dependency relations among actions foreveryday tasks and semantic graph generation from natural texts.</p>
                <p>Last Updated: 2024-05-08 16:25:42 UTC</p>
                <button class="interpret-button" data-id="2405.05189v1">Interpret</button>
                <div id="interpretation-2405.05189v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>OpenESS: Event-based Semantic Scene Understanding with Open Vocabularies</h3>
                <p>Authors: Lingdong KongYouquan LiuLai Xing NgBenoit R. CottereauWei Tsang Ooi</p>
                <p><a href="http://arxiv.org/abs/2405.05259v1">Link to paper</a></p>
                <p>Event-based semantic segmentation ESS is a fundamental yet challenging taskfor event camera sensing. The difficulties in interpreting and annotating eventdata limit its scalability. While domain adaptation from images to event datacan help to mitigate this issue there exist data representational differencesthat require additional effort to resolve. In this work for the first time wesynergize information from image text and event-data domains and introduceOpenESS to enable scalable ESS in an open-world annotation-efficient manner.We achieve this goal by transferring the semantically rich CLIP knowledge fromimage-text pairs to event streams. To pursue better cross-modality adaptationwe propose a frame-to-event contrastive distillation and a text-to-eventsemantic consistency regularization. Experimental results on popular ESSbenchmarks showed our approach outperforms existing methods. Notably weachieve 53.93 and 43.31 mIoU on DDD17 and DSEC-Semantic without using eitherevent or frame labels.</p>
                <p>Last Updated: 2024-05-08 17:59:58 UTC</p>
                <button class="interpret-button" data-id="2405.05259v1">Interpret</button>
                <div id="interpretation-2405.05259v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving</h3>
                <p>Authors: Lingdong KongXiang XuJiawei RenWenwei ZhangLiang PanKai ChenWei Tsang OoiZiwei Liu</p>
                <p><a href="http://arxiv.org/abs/2405.05258v1">Link to paper</a></p>
                <p>Efficient data utilization is crucial for advancing 3D scene understanding inautonomous driving where reliance on heavily human-annotated LiDAR pointclouds challenges fully supervised methods. Addressing this our study extendsinto semi-supervised learning for LiDAR semantic segmentation leveraging theintrinsic spatial priors of driving scenes and multi-sensor complements toaugment the efficacy of unlabeled datasets. We introduce LaserMix an evolvedframework that integrates laser beam manipulations from disparate LiDAR scansand incorporates LiDAR-camera correspondences to further assist data-efficientlearning. Our framework is tailored to enhance 3D scene consistencyregularization by incorporating multi-modality including 1 multi-modalLaserMix operation for fine-grained cross-sensor interactions 2camera-to-LiDAR feature distillation that enhances LiDAR feature learning and3 language-driven knowledge guidance generating auxiliary supervisions usingopen-vocabulary models. The versatility of LaserMix enables applicationsacross LiDAR representations establishing it as a universally applicablesolution. Our framework is rigorously validated through theoretical analysisand extensive experiments on popular driving perception datasets. Resultsdemonstrate that LaserMix markedly outperforms fully supervised alternativesachieving comparable accuracy with five times fewer annotations andsignificantly improving the supervised-only baselines. This substantialadvancement underscores the potential of semi-supervised approaches in reducingthe reliance on extensive labeled data in LiDAR-based 3D scene understandingsystems.</p>
                <p>Last Updated: 2024-05-08 17:59:53 UTC</p>
                <button class="interpret-button" data-id="2405.05258v1">Interpret</button>
                <div id="interpretation-2405.05258v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models</h3>
                <p>Authors: Prannay KaulZhizhong LiHao YangYonatan DuklerAshwin SwaminathanC. J. TaylorStefano Soatto</p>
                <p><a href="http://arxiv.org/abs/2405.05256v1">Link to paper</a></p>
                <p>Mitigating hallucinations in large vision-language models LVLMs remains anopen problem. Recent benchmarks do not address hallucinations in open-endedfree-form responses which we term Type I hallucinations. Instead they focuson hallucinations responding to very specific question formats -- typically amultiple-choice response regarding a particular object or attribute -- which weterm Type II hallucinations. Additionally such benchmarks often requireexternal API calls to models which are subject to change. In practice weobserve that a reduction in Type II hallucinations does not lead to a reductionin Type I hallucinations but rather that the two forms of hallucinations areoften anti-correlated. To address this we propose THRONE a novel object-basedautomatic framework for quantitatively evaluating Type I hallucinations in LVLMfree-form outputs. We use public language models LMs to identifyhallucinations in LVLM responses and compute informative metrics. By evaluatinga large selection of recent LVLMs using public datasets we show that animprovement in existing metrics do not lead to a reduction in Type Ihallucinations and that established benchmarks for measuring Type Ihallucinations are incomplete. Finally we provide a simple and effective dataaugmentation method to reduce Type I and Type II hallucinations as a strongbaseline.</p>
                <p>Last Updated: 2024-05-08 17:59:11 UTC</p>
                <button class="interpret-button" data-id="2405.05256v1">Interpret</button>
                <div id="interpretation-2405.05256v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models</h3>
                <p>Authors: Hongjie WangDifan LiuYan KangYijun LiZhe LinNiraj K. JhaYuchen Liu</p>
                <p><a href="http://arxiv.org/abs/2405.05252v1">Link to paper</a></p>
                <p>Diffusion Models DMs have exhibited superior performance in generatinghigh-quality and diverse images. However this exceptional performance comes atthe cost of expensive architectural design particularly due to the attentionmodule heavily used in leading models. Existing works mainly adopt a retrainingprocess to enhance DM efficiency. This is computationally expensive and notvery scalable. To this end we introduce the Attention-driven Training-freeEfficient Diffusion Model AT-EDM framework that leverages attention maps toperform run-time pruning of redundant tokens without the need for anyretraining. Specifically for single-denoising-step pruning we develop a novelranking algorithm Generalized Weighted Page Rank G-WPR to identifyredundant tokens and a similarity-based recovery method to restore tokens forthe convolution operation. In addition we propose a Denoising-Steps-AwarePruning DSAP approach to adjust the pruning budget across different denoisingtimesteps for better generation quality. Extensive evaluations show that AT-EDMperforms favorably against prior art in terms of efficiency e.g. 38.8 FLOPssaving and up to 1.53x speed-up over Stable Diffusion XL while maintainingnearly the same FID and CLIP scores as the full model. Project webpage:https://atedm.github.io.</p>
                <p>Last Updated: 2024-05-08 17:56:47 UTC</p>
                <button class="interpret-button" data-id="2405.05252v1">Interpret</button>
                <div id="interpretation-2405.05252v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>BenthicNet: A global compilation of seafloor images for deep learning applications</h3>
                <p>Authors: Scott C. LoweBenjamin MisiukIsaac XuShakhboz AbdulazizovAmit R. BaroiAlex C. BastosMerlin BestVicki FerriniAriell FriedmanDeborah HartOve Hoegh-GuldbergDaniel IerodiaconouJulia Mackin-McLaughlinKathryn MarkeyPedro S. MenandroJacquomo MonkShreya NemaniJohn O'BrienElizabeth OhLuba Y. ReshitnykKatleen RobertChris M. RoelfsemaJessica A. SameotoAlexandre C. G. SchimelJordan A. ThomsonBrittany R. WilsonMelisa C. WongCraig J. BrownThomas Trappenberg</p>
                <p><a href="http://arxiv.org/abs/2405.05241v1">Link to paper</a></p>
                <p>Advances in underwater imaging enable the collection of extensive seafloorimage datasets that are necessary for monitoring important benthic ecosystems.The ability to collect seafloor imagery has outpaced our capacity to analyzeit hindering expedient mobilization of this crucial environmental information.Recent machine learning approaches provide opportunities to increase theefficiency with which seafloor image datasets are analyzed yet large andconsistent datasets necessary to support development of such approaches arescarce. Here we present BenthicNet: a global compilation of seafloor imagerydesigned to support the training and evaluation of large-scale imagerecognition models. An initial set of over 11.4 million images was collectedand curated to represent a diversity of seafloor environments using arepresentative subset of 1.3 million images. These are accompanied by 2.6million annotations translated to the CATAMI scheme which span 190000 of theimages. A large deep learning model was trained on this compilation andpreliminary results suggest it has utility for automating large and small-scaleimage analysis tasks. The compilation and model are made openly available foruse by the scientific community at https://doi.org/10.20383/103.0614.</p>
                <p>Last Updated: 2024-05-08 17:37:57 UTC</p>
                <button class="interpret-button" data-id="2405.05241v1">Interpret</button>
                <div id="interpretation-2405.05241v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving</h3>
                <p>Authors: Lingdong KongXiang XuJiawei RenWenwei ZhangLiang PanKai ChenWei Tsang OoiZiwei Liu</p>
                <p><a href="http://arxiv.org/abs/2405.05258v1">Link to paper</a></p>
                <p>Efficient data utilization is crucial for advancing 3D scene understanding inautonomous driving where reliance on heavily human-annotated LiDAR pointclouds challenges fully supervised methods. Addressing this our study extendsinto semi-supervised learning for LiDAR semantic segmentation leveraging theintrinsic spatial priors of driving scenes and multi-sensor complements toaugment the efficacy of unlabeled datasets. We introduce LaserMix an evolvedframework that integrates laser beam manipulations from disparate LiDAR scansand incorporates LiDAR-camera correspondences to further assist data-efficientlearning. Our framework is tailored to enhance 3D scene consistencyregularization by incorporating multi-modality including 1 multi-modalLaserMix operation for fine-grained cross-sensor interactions 2camera-to-LiDAR feature distillation that enhances LiDAR feature learning and3 language-driven knowledge guidance generating auxiliary supervisions usingopen-vocabulary models. The versatility of LaserMix enables applicationsacross LiDAR representations establishing it as a universally applicablesolution. Our framework is rigorously validated through theoretical analysisand extensive experiments on popular driving perception datasets. Resultsdemonstrate that LaserMix markedly outperforms fully supervised alternativesachieving comparable accuracy with five times fewer annotations andsignificantly improving the supervised-only baselines. This substantialadvancement underscores the potential of semi-supervised approaches in reducingthe reliance on extensive labeled data in LiDAR-based 3D scene understandingsystems.</p>
                <p>Last Updated: 2024-05-08 17:59:53 UTC</p>
                <button class="interpret-button" data-id="2405.05258v1">Interpret</button>
                <div id="interpretation-2405.05258v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models</h3>
                <p>Authors: Prannay KaulZhizhong LiHao YangYonatan DuklerAshwin SwaminathanC. J. TaylorStefano Soatto</p>
                <p><a href="http://arxiv.org/abs/2405.05256v1">Link to paper</a></p>
                <p>Mitigating hallucinations in large vision-language models LVLMs remains anopen problem. Recent benchmarks do not address hallucinations in open-endedfree-form responses which we term Type I hallucinations. Instead they focuson hallucinations responding to very specific question formats -- typically amultiple-choice response regarding a particular object or attribute -- which weterm Type II hallucinations. Additionally such benchmarks often requireexternal API calls to models which are subject to change. In practice weobserve that a reduction in Type II hallucinations does not lead to a reductionin Type I hallucinations but rather that the two forms of hallucinations areoften anti-correlated. To address this we propose THRONE a novel object-basedautomatic framework for quantitatively evaluating Type I hallucinations in LVLMfree-form outputs. We use public language models LMs to identifyhallucinations in LVLM responses and compute informative metrics. By evaluatinga large selection of recent LVLMs using public datasets we show that animprovement in existing metrics do not lead to a reduction in Type Ihallucinations and that established benchmarks for measuring Type Ihallucinations are incomplete. Finally we provide a simple and effective dataaugmentation method to reduce Type I and Type II hallucinations as a strongbaseline.</p>
                <p>Last Updated: 2024-05-08 17:59:11 UTC</p>
                <button class="interpret-button" data-id="2405.05256v1">Interpret</button>
                <div id="interpretation-2405.05256v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Diffusion-HMC: Parameter Inference with Diffusion Model driven Hamiltonian Monte Carlo</h3>
                <p>Authors: Nayantara MudurCarolina Cuesta-LazaroDouglas P. Finkbeiner</p>
                <p><a href="http://arxiv.org/abs/2405.05255v1">Link to paper</a></p>
                <p>Diffusion generative models have excelled at diverse image generation andreconstruction tasks across fields. A less explored avenue is their applicationto discriminative tasks involving regression or classification problems. Thecornerstone of modern cosmology is the ability to generate predictions forobserved astrophysical fields from theory and constrain physical models fromobservations using these predictions. This work uses a single diffusiongenerative model to address these interlinked objectives -- as a surrogatemodel or emulator for cold dark matter density fields conditional on inputcosmological parameters and as a parameter inference model that solves theinverse problem of constraining the cosmological parameters of an input field.The model is able to emulate fields with summary statistics consistent withthose of the simulated target distribution. We then leverage the approximatelikelihood of the diffusion generative model to derive tight constraints oncosmology by using the Hamiltonian Monte Carlo method to sample the posterioron cosmological parameters for a given test image. Finally we demonstrate thatthis parameter inference approach is more robust to the addition of noise thanbaseline parameter inference networks.</p>
                <p>Last Updated: 2024-05-08 17:59:03 UTC</p>
                <button class="interpret-button" data-id="2405.05255v1">Interpret</button>
                <div id="interpretation-2405.05255v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models</h3>
                <p>Authors: Hongjie WangDifan LiuYan KangYijun LiZhe LinNiraj K. JhaYuchen Liu</p>
                <p><a href="http://arxiv.org/abs/2405.05252v1">Link to paper</a></p>
                <p>Diffusion Models DMs have exhibited superior performance in generatinghigh-quality and diverse images. However this exceptional performance comes atthe cost of expensive architectural design particularly due to the attentionmodule heavily used in leading models. Existing works mainly adopt a retrainingprocess to enhance DM efficiency. This is computationally expensive and notvery scalable. To this end we introduce the Attention-driven Training-freeEfficient Diffusion Model AT-EDM framework that leverages attention maps toperform run-time pruning of redundant tokens without the need for anyretraining. Specifically for single-denoising-step pruning we develop a novelranking algorithm Generalized Weighted Page Rank G-WPR to identifyredundant tokens and a similarity-based recovery method to restore tokens forthe convolution operation. In addition we propose a Denoising-Steps-AwarePruning DSAP approach to adjust the pruning budget across different denoisingtimesteps for better generation quality. Extensive evaluations show that AT-EDMperforms favorably against prior art in terms of efficiency e.g. 38.8 FLOPssaving and up to 1.53x speed-up over Stable Diffusion XL while maintainingnearly the same FID and CLIP scores as the full model. Project webpage:https://atedm.github.io.</p>
                <p>Last Updated: 2024-05-08 17:56:47 UTC</p>
                <button class="interpret-button" data-id="2405.05252v1">Interpret</button>
                <div id="interpretation-2405.05252v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Deep learning-based variational autoencoder for classification of quantum and classical states of light</h3>
                <p>Authors: Mahesh BhupatiAbhishek MallAnshuman KumarPankaj K. Jha</p>
                <p><a href="http://arxiv.org/abs/2405.05243v1">Link to paper</a></p>
                <p>Advancements in optical quantum technologies have been enabled by thegeneration manipulation and characterization of light with identificationbased on its photon statistics. However characterizing light and its sourcesthrough single photon measurements often requires efficient detectors andlonger measurement times to obtain high-quality photon statistics. Here weintroduce a deep learning-based variational autoencoder VAE method forclassifying single photon added coherent state SPACS single photon addedthermal state SPACS mixed states between coherent/SPACS and thermal/SPATS oflight. Our semisupervised learning-based VAE efficiently maps the photonstatistics features of light to a lower dimension enabling quasi-instantaneousclassification with low average photon counts. The proposed VAE method isrobust and maintains classification accuracy in the presence of losses inherentin an experiment such as finite collection efficiency non-unity quantumefficiency finite number of detectors etc. Additionally leveraging thetransfer learning capabilities of VAE enables successful classification of dataof any quality using a single trained model. We envision that such a deeplearning methodology will enable better classification of quantum light andlight sources even in the presence of poor detection quality.</p>
                <p>Last Updated: 2024-05-08 17:40:03 UTC</p>
                <button class="interpret-button" data-id="2405.05243v1">Interpret</button>
                <div id="interpretation-2405.05243v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>LLMs with Personalities in Multi-issue Negotiation Games</h3>
                <p>Authors: Sean NohHo-Chun Herbert Chang</p>
                <p><a href="http://arxiv.org/abs/2405.05248v1">Link to paper</a></p>
                <p>Powered by large language models LLMs AI agents have become capable ofmany human tasks. Using the most canonical definitions of the Big Fivepersonality we measure the ability of LLMs to negotiate within agame-theoretical framework as well as methodological challenges to measuringnotions of fairness and risk. Simulations n1500 for both single-issue andmulti-issue negotiation reveal increase in domain complexity with asymmetricissue valuations improve agreement rates but decrease surplus from aggressivenegotiation. Through gradient-boosted regression and Shapley explainers wefind high openness conscientiousness and neuroticism are associated with fairtendencies low agreeableness and low openness are associated with rationaltendencies. Low conscientiousness is associated with high toxicity. Theseresults indicate that LLMs may have built-in guardrails that default to fairbehavior but can be jail broken to exploit agreeable opponents. We alsooffer pragmatic insight in how negotiation bots can be designed and aframework of assessing negotiation behavior based on game theory andcomputational social science.</p>
                <p>Last Updated: 2024-05-08 17:51:53 UTC</p>
                <button class="interpret-button" data-id="2405.05248v1">Interpret</button>
                <div id="interpretation-2405.05248v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fair Voting Outcomes with Impact and Novelty Compromises? Unraveling Biases of Equal Shares in Participatory Budgeting</h3>
                <p>Authors: Sajan MaharjanSrijoni MajumdarEvangelos Pournaras</p>
                <p><a href="http://arxiv.org/abs/2405.05085v1">Link to paper</a></p>
                <p>Participatory budgeting as a paradigm for democratic innovations engagescitizens in the distribution of a public budget to projects which they proposeand vote for implementation. So far voting algorithms have been devised andstudied in social choice literature to elect projects that are popular whileothers prioritize on a proportional representation of voters preferences forinstance equal shares. However the anticipated impact and novelty in thebroader society by the winning projects as selected by different algorithmsremains totally under-explored lacking both a universal theory of impact forvoting and a rigorous framework for impact and novelty assessments. This paperstackles this grand challenge towards new axiomatic foundations for designingeffective and fair voting methods. This is via new and striking insightsderived from a large-scale analysis of biases over 345 real-world votingoutcomes characterized for the first time by a novel portfolio of impact andnovelty metrics. We find strong causal evidence that equal shares comes withimpact loss in several infrastructural projects of different cost levels thathave been so far over-represented. However it also comes with a novel yetover-represented impact gain in welfare education and culture. We discussbroader implications of these results and how impact loss can be mitigated atthe stage of campaign design and project ideation.</p>
                <p>Last Updated: 2024-05-08 14:32:09 UTC</p>
                <button class="interpret-button" data-id="2405.05085v1">Interpret</button>
                <div id="interpretation-2405.05085v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Communication-Efficient Collaborative Perception via Information Filling with Codebook</h3>
                <p>Authors: Yue HuJuntong PengSifei LiuJunhao GeSi LiuSiheng Chen</p>
                <p><a href="http://arxiv.org/abs/2405.04966v1">Link to paper</a></p>
                <p>Collaborative perception empowers each agent to improve its perceptualability through the exchange of perceptual messages with other agents. Itinherently results in a fundamental trade-off between perception ability andcommunication cost. To address this bottleneck issue our core idea is tooptimize the collaborative messages from two key aspects: representation andselection. The proposed codebook-based message representation enables thetransmission of integer codes rather than high-dimensional feature maps. Theproposed information-filling-driven message selection optimizes local messagesto collectively fill each agents information demand preventing informationoverflow among multiple agents. By integrating these two designs we proposeCodeFilling a novel communication-efficient collaborative perception systemwhich significantly advances the perception-communication trade-off and isinclusive to both homogeneous and heterogeneous collaboration settings. Weevaluate CodeFilling in both a real-world dataset DAIR-V2X and a newsimulation dataset OPV2VH. Results show that CodeFilling outperforms previousSOTA Where2comm on DAIR-V2X/OPV2VH with 1333/1206 times lower communicationvolume. Our code is available at https://github.com/PhyllisH/CodeFilling.</p>
                <p>Last Updated: 2024-05-08 11:12:37 UTC</p>
                <button class="interpret-button" data-id="2405.04966v1">Interpret</button>
                <div id="interpretation-2405.04966v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Mitigating Negative Side Effects in Multi-Agent Systems Using Blame Assignment</h3>
                <p>Authors: Pulkit RustagiSandhya Saisubramanian</p>
                <p><a href="http://arxiv.org/abs/2405.04702v1">Link to paper</a></p>
                <p>When agents that are independently trained or designed to complete theirindividual tasks are deployed in a shared environment their joint actions mayproduce negative side effects NSEs. As their training does not account forthe behavior of other agents or their joint action effects on the environmentthe agents have no prior knowledge of the NSEs of their actions. We model theproblem of mitigating NSEs in a cooperative multi-agent system as aLexicographic Decentralized Markov Decision Process with two objectives. Theagents must optimize the completion of their assigned tasks while mitigatingNSEs. We assume independence of transitions and rewards with respect to theagents tasks but the joint NSE penalty creates a form of dependence in thissetting. To improve scalability the joint NSE penalty is decomposed intoindividual penalties for each agent using credit assignment which facilitatesdecentralized policy computation. Our results in simulation on three domainsdemonstrate the effectiveness and scalability of our approach in mitigatingNSEs by updating the policies of a subset of agents in the system.</p>
                <p>Last Updated: 2024-05-07 22:42:04 UTC</p>
                <button class="interpret-button" data-id="2405.04702v1">Interpret</button>
                <div id="interpretation-2405.04702v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Pipe Routing with Topology Control for UAV Networks</h3>
                <p>Authors: Shreyas DevarajuShivam GargAlexander IhlerSunil Kumar</p>
                <p><a href="http://arxiv.org/abs/2405.04678v1">Link to paper</a></p>
                <p>Routing protocols help in transmitting the sensed data from UAVs monitoringthe targets called target UAVs to the BS. However the highly dynamic natureof an autonomous decentralized UAV network leads to frequent route breaks ortraffic disruptions. Traditional routing schemes cannot quickly adapt todynamic UAV networks and/or incur large control overhead and delays. Toestablish stable high-quality routes from target UAVs to the BS we design ahybrid reactive routing scheme called pipe routing that is mobilitycongestion and energy-aware. The pipe routing scheme discovers routeson-demand and proactively switches to alternate high-quality routes within alimited region around the active routes called the pipe when needed reducingthe number of route breaks and increasing data throughput. We then design anovel topology control-based pipe routing scheme to maintain robustconnectivity in the pipe region around the active routes leading to improvedroute stability and increased throughput with minimal impact on the coverageperformance of the UAV network.</p>
                <p>Last Updated: 2024-05-07 21:38:13 UTC</p>
                <button class="interpret-button" data-id="2405.04678v1">Interpret</button>
                <div id="interpretation-2405.04678v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>"Community Guidelines Make this the Best Party on the Internet": An In-Depth Study of Online Platforms' Content Moderation Policies</h3>
                <p>Authors: Brennan SchaffnerArjun Nitin BhagojiSiyuan ChengJacqueline MeiJay L. ShenGrace WangMarshini ChettyNick FeamsterGenevieve LakierChenhao Tan</p>
                <p><a href="http://dx.doi.org/10.1145/3613904.3642333">Link to paper</a></p>
                <p>Moderating user-generated content on online platforms is crucial forbalancing user safety and freedom of speech. Particularly in the United Statesplatforms are not subject to legal constraints prescribing permissible content.Each platform has thus developed bespoke content moderation policies but thereis little work towards a comparative understanding of these policies acrossplatforms and topics. This paper presents the first systematic study of thesepolicies from the 43 largest online platforms hosting user-generated contentfocusing on policies around copyright infringement harmful speech andmisleading content. We build a custom web-scraper to obtain policy text anddevelop a unified annotation scheme to analyze the text for the presence ofcritical components. We find significant structural and compositional variationin policies across topics and platforms with some variation attributable todisparate legal groundings. We lay the groundwork for future studies ofever-evolving content moderation policies and their impact on users.</p>
                <p>Last Updated: 2024-05-08 17:18:37 UTC</p>
                <button class="interpret-button" data-id="2405.05225v1">Interpret</button>
                <div id="interpretation-2405.05225v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Potential and Implications of Generative AI on HCI Education</h3>
                <p>Authors: Ahmed KharrufaIan G Johnson</p>
                <p><a href="http://dx.doi.org/10.1145/3658619.3658627">Link to paper</a></p>
                <p>Generative AI GAI is impacting teaching and learning directly or indirectlyacross a range of subjects and disciplines. As educators we need to understandthe potential and limitations of AI in HCI education and ensure our graduatingHCI students are aware of the potential and limitations of AI in HCI. In thispaper we report on the main pedagogical insights gained from the inclusion ofgenerative AI into a 10 week undergraduate module. We designed the module toencourage student experimentation with GAI models as part of the design briefrequirement and planned practical sessions and discussions. Our insights arebased on replies to a survey sent out to the students after completing themodule. Our key findings for HCI educators report on the use of AI as apersona for developing project ideas and creating resources for design and AIas a mirror for reflecting students understanding of key concepts and ideasand highlighting knowledge gaps. We also discuss potential pitfalls that shouldbe considered and the need to assess students literacies and assumptions ofGAIs as pedagogical tools. Finally we put forward the case for educators totake the opportunities GAI presents as an educational tool and be experimentalcreative and courageous in their practice. We end with a discussion of ourfindings in relation to the TPACK framework in HCI.</p>
                <p>Last Updated: 2024-05-08 15:46:31 UTC</p>
                <button class="interpret-button" data-id="2405.05154v1">Interpret</button>
                <div id="interpretation-2405.05154v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Concerns on Bias in Large Language Models when Creating Synthetic Personae</h3>
                <p>Authors: Helena A. Haxvig</p>
                <p><a href="http://arxiv.org/abs/2405.05080v1">Link to paper</a></p>
                <p>This position paper explores the benefits drawbacks and ethicalconsiderations of incorporating synthetic personae in HCI researchparticularly focusing on the customization challenges beyond the limitations ofcurrent Large Language Models LLMs. These perspectives are derived from theinitial results of a sub-study employing vignettes to showcase the existence ofbias within black-box LLMs and explore methods for manipulating them. The studyaims to establish a foundation for understanding the challenges associated withthese models emphasizing the necessity of thorough testing before utilizingthem to create synthetic personae for HCI research.</p>
                <p>Last Updated: 2024-05-08 14:24:11 UTC</p>
                <button class="interpret-button" data-id="2405.05080v1">Interpret</button>
                <div id="interpretation-2405.05080v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Impact of Tone-Aware Explanations in Recommender Systems</h3>
                <p>Authors: Ayano OkosoKeisuke OtakiSatoshi KoideYukino Baba</p>
                <p><a href="http://arxiv.org/abs/2405.05061v1">Link to paper</a></p>
                <p>In recommender systems the presentation of explanations plays a crucial rolein supporting users decision-making processes. Although numerous existingstudies have focused on the effects transparency or persuasiveness ofexplanation content explanation expression is largely overlooked. Tone suchas formal and humorous is directly linked to expressiveness and is animportant element in human communication. However studies on the impact oftone on explanations within the context of recommender systems areinsufficient. Therefore this study investigates the effect of explanationtones through an online user study from three aspects: perceived effectsdomain differences and user attributes. We create a dataset using a largelanguage model to generate fictional items and explanations with various tonesin the domain of movies hotels and home products. Collected data analysisreveals different perceived effects of tones depending on the domains.Moreover user attributes such as age and personality traits are found toinfluence the impact of tone. This research underscores the critical role oftones in explanations within recommender systems suggesting that attention totone can enhance user experience.</p>
                <p>Last Updated: 2024-05-08 13:55:52 UTC</p>
                <button class="interpret-button" data-id="2405.05061v1">Interpret</button>
                <div id="interpretation-2405.05061v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Overcoming Anchoring Bias: The Potential of AI and XAI-based Decision Support</h3>
                <p>Authors: Felix HaagCarlo StinglKatrin ZerfassKonstantin HopfThorsten Staake</p>
                <p><a href="http://arxiv.org/abs/2405.04972v1">Link to paper</a></p>
                <p>Information systems IS are frequently designed to leverage the negativeeffect of anchoring bias to influence individuals decision-making e.g. bymanipulating purchase decisions. Recent advances in Artificial IntelligenceAI and the explanations of its decisions through explainable AI XAI haveopened new opportunities for mitigating biased decisions. So far the potentialof these technological advances to overcome anchoring bias remains widelyunclear. To this end we conducted two online experiments with a total of N390participants in the context of purchase decisions to examine the impact of AIand XAI-based decision support on anchoring bias. Our results show that AIalone and its combination with XAI help to mitigate the negative effect ofanchoring bias. Ultimately our findings have implications for the design of AIand XAI-based decision support and IS to overcome cognitive biases.</p>
                <p>Last Updated: 2024-05-08 11:25:04 UTC</p>
                <button class="interpret-button" data-id="2405.04972v1">Interpret</button>
                <div id="interpretation-2405.04972v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-05-09</p>
        </div>
    
        </div>
    </body>
    </html>
    