
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI</h3>
                <p>Authors: Lang TongXinyi WangQing Zhao</p>
                <p><a href="http://arxiv.org/abs/2403.06942v1">Link to paper</a></p>
                <p>Purpose This article presents a case for a next-generation grid monitoringand control system leveraging recent advances in generative artificialintelligence AI machine learning and statistical inference. Advancingbeyond earlier generations of wide-area monitoring systems built uponsupervisory control and data acquisition SCADA and synchrophasortechnologies we argue for a monitoring and control framework based on thestreaming of continuous point-on-wave CPOW measurements with AI-powered datacompression and fault detection.  Methods and Results: The architecture of the proposed design originates fromthe Wiener-Kallianpur innovation representation of a random process thattransforms causally a stationary random process into an innovation sequencewith independent and identically distributed random variables. This workpresents a generative AI approach that i learns an innovation autoencoderthat extracts innovation sequence from CPOW time series ii compresses theCPOW streaming data with innovation autoencoder and subband coding and iiidetects unknown faults and novel trends via nonparametric sequential hypothesistesting.  Conclusion: This work argues that conventional monitoring using SCADA andphasor measurement unit PMU technologies is ill-suited for a future grid withdeep penetration of inverter-based renewable generations and distributed energyresources. A monitoring system based on CPOW data streaming and AI dataanalytics should be the basic building blocks for situational awareness of ahighly dynamic future grid.</p>
                <p>Last Updated: 2024-03-11 17:28:46 UTC</p>
                <button class="interpret-button" data-id="2403.06942v1">Interpret</button>
                <div id="interpretation-2403.06942v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Simplicity Bias of Transformers to Learn Low Sensitivity Functions</h3>
                <p>Authors: Bhavya VasudevaDeqing FuTianyi ZhouElliott KauYouqi HuangVatsal Sharan</p>
                <p><a href="http://arxiv.org/abs/2403.06925v1">Link to paper</a></p>
                <p>Transformers achieve state-of-the-art accuracy and robustness across manytasks but an understanding of the inductive biases that they have and howthose biases are different from other neural network architectures remainselusive. Various neural network architectures such as fully connected networkshave been found to have a simplicity bias towards simple functions of the dataone version of this simplicity bias is a spectral bias to learn simplefunctions in the Fourier space. In this work we identify the notion ofsensitivity of the model to random changes in the input as a notion ofsimplicity bias which provides a unified metric to explain the simplicity andspectral bias of transformers across different data modalities. We show thattransformers have lower sensitivity than alternative architectures such asLSTMs MLPs and CNNs across both vision and language tasks. We also show thatlow-sensitivity bias correlates with improved robustness furthermore it canalso be used as an efficient intervention to further improve the robustness oftransformers.</p>
                <p>Last Updated: 2024-03-11 17:12:09 UTC</p>
                <button class="interpret-button" data-id="2403.06925v1">Interpret</button>
                <div id="interpretation-2403.06925v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Benign overfitting in leaky ReLU networks with moderate input dimension</h3>
                <p>Authors: Kedar KarhadkarErin GeorgeMichael MurrayGuido MontúfarDeanna Needell</p>
                <p><a href="http://arxiv.org/abs/2403.06903v1">Link to paper</a></p>
                <p>The problem of benign overfitting asks whether it is possible for a model toperfectly fit noisy training data and still generalize well. We study benignoverfitting in two-layer leaky ReLU networks trained with the hinge loss on abinary classification task. We consider input data which can be decomposed intothe sum of a common signal and a random noise component which lie on subspacesorthogonal to one another. We characterize conditions on the signal to noiseratio SNR of the model parameters giving rise to benign versus non-benign orharmful overfitting: in particular if the SNR is high then benign overfittingoccurs conversely if the SNR is low then harmful overfitting occurs. Weattribute both benign and non-benign overfitting to an approximate marginmaximization property and show that leaky ReLU networks trained on hinge losswith Gradient Descent GD satisfy this property. In contrast to prior work wedo not require near orthogonality conditions on the training data: notably forinput dimension d and training sample size n while prior work showsasymptotically optimal error when d  Omegan2 log n here we requireonly d  Omegaleftn log frac1epsilonright to obtain error withinepsilon of optimal.</p>
                <p>Last Updated: 2024-03-11 16:56:01 UTC</p>
                <button class="interpret-button" data-id="2403.06903v1">Interpret</button>
                <div id="interpretation-2403.06903v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On the Generalization Ability of Unsupervised Pretraining</h3>
                <p>Authors: Yuyang DengJunyuan HongJiayu ZhouMehrdad Mahdavi</p>
                <p><a href="http://arxiv.org/abs/2403.06871v1">Link to paper</a></p>
                <p>Recent advances in unsupervised learning have shown that unsupervisedpre-training followed by fine-tuning can improve model generalization.However a rigorous understanding of how the representation function learned onan unlabeled dataset affects the generalization of the fine-tuned model islacking. Existing theoretical research does not adequately account for theheterogeneity of the distribution and tasks in pre-training and fine-tuningstage. To bridge this gap this paper introduces a novel theoretical frameworkthat illuminates the critical factor influencing the transferability ofknowledge acquired during unsupervised pre-training to the subsequentfine-tuning phase ultimately affecting the generalization capabilities of thefine-tuned model on downstream tasks. We apply our theoretical framework toanalyze generalization bound of two distinct scenarios: Context Encoderpre-training with deep neural networks and Masked Autoencoder pre-training withdeep transformers followed by fine-tuning on a binary classification task.Finally inspired by our findings we propose a novel regularization methodduring pre-training to further enhances the generalization of fine-tuned model.Overall our results contribute to a better understanding of unsupervisedpre-training and fine-tuning paradigm and can shed light on the design of moreeffective pre-training algorithms.</p>
                <p>Last Updated: 2024-03-11 16:23:42 UTC</p>
                <button class="interpret-button" data-id="2403.06871v1">Interpret</button>
                <div id="interpretation-2403.06871v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>In-context Exploration-Exploitation for Reinforcement Learning</h3>
                <p>Authors: Zhenwen DaiFederico TomasiSina Ghiassian</p>
                <p><a href="http://arxiv.org/abs/2403.06826v1">Link to paper</a></p>
                <p>In-context learning is a promising approach for online policy learning ofoffline reinforcement learning RL methods which can be achieved at inferencetime without gradient optimization. However this method is hindered bysignificant computational costs resulting from the gathering of large trainingtrajectory sets and the need to train large Transformer models. We address thischallenge by introducing an In-context Exploration-Exploitation ICEEalgorithm designed to optimize the efficiency of in-context policy learning.Unlike existing models ICEE performs an exploration-exploitation trade-off atinference time within a Transformer model without the need for explicitBayesian inference. Consequently ICEE can solve Bayesian optimization problemsas efficiently as Gaussian process biased methods do but in significantly lesstime. Through experiments in grid world environments we demonstrate that ICEEcan learn to solve new RL tasks using only tens of episodes marking asubstantial improvement over the hundreds of episodes needed by the previousin-context learning method.</p>
                <p>Last Updated: 2024-03-11 15:43:14 UTC</p>
                <button class="interpret-button" data-id="2403.06826v1">Interpret</button>
                <div id="interpretation-2403.06826v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>The pitfalls of next-token prediction</h3>
                <p>Authors: Gregor BachmannVaishnavh Nagarajan</p>
                <p><a href="http://arxiv.org/abs/2403.06963v1">Link to paper</a></p>
                <p>Can a mere next-token predictor faithfully model human intelligence Wecrystallize this intuitive concern which is fragmented in the literature. As astarting point we argue that the two often-conflated phases of next-tokenprediction -- autoregressive inference and teacher-forced training -- must betreated distinctly. The popular criticism that errors can compound duringautoregressive inference crucially assumes that teacher-forcing has learned anaccurate next-token predictor. This assumption sidesteps a more deep-rootedproblem we expose: in certain classes of tasks teacher-forcing can simply failto learn an accurate next-token predictor in the first place. We describe ageneral mechanism of how teacher-forcing can fail and design a minimalplanning task where both the Transformer and the Mamba architecture empiricallyfail in that manner -- remarkably despite the task being straightforward tolearn. We provide preliminary evidence that this failure can be resolved whentraining to predict multiple tokens in advance. We hope this finding can groundfuture debates and inspire explorations beyond the next-token predictionparadigm. We make our code available underhttps://github.com/gregorbachmann/Next-Token-Failures</p>
                <p>Last Updated: 2024-03-11 17:47:30 UTC</p>
                <button class="interpret-button" data-id="2403.06963v1">Interpret</button>
                <div id="interpretation-2403.06963v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data</h3>
                <p>Authors: Jialu LiJaemin ChoYi-Lin SungJaehong YoonMohit Bansal</p>
                <p><a href="http://arxiv.org/abs/2403.06952v1">Link to paper</a></p>
                <p>Recent text-to-image T2I generation models have demonstrated impressivecapabilities in creating images from text descriptions. However these T2Igeneration models often fall short of generating images that precisely matchthe details of the text inputs such as incorrect spatial relationship ormissing objects. In this paper we introduce SELMA: Skill-Specific ExpertLearning and Merging with Auto-Generated Data a novel paradigm to improve thefaithfulness of T2I models by fine-tuning models on automatically generatedmulti-skill image-text datasets with skill-specific expert learning andmerging. First SELMA leverages an LLMs in-context learning capability togenerate multiple datasets of text prompts that can teach different skills andthen generates the images with a T2I model based on the prompts. Next SELMAadapts the T2I model to the new skills by learning multiple single-skill LoRAlow-rank adaptation experts followed by expert merging. Our independentexpert fine-tuning specializes multiple models for different skills and expertmerging helps build a joint multi-skill T2I model that can generate faithfulimages given diverse text prompts while mitigating the knowledge conflict fromdifferent datasets. We empirically demonstrate that SELMA significantlyimproves the semantic alignment and text faithfulness of state-of-the-art T2Idiffusion models on multiple benchmarks 2.1 on TIFA and 6.9 on DSG humanpreference metrics PickScore ImageReward and HPS as well as humanevaluation. Moreover fine-tuning with image-text pairs auto-collected viaSELMA shows comparable performance to fine-tuning with ground truth data.Lastly we show that fine-tuning with images from a weaker T2I model can helpimprove the generation quality of a stronger T2I model suggesting promisingweak-to-strong generalization in T2I models.</p>
                <p>Last Updated: 2024-03-11 17:35:33 UTC</p>
                <button class="interpret-button" data-id="2403.06952v1">Interpret</button>
                <div id="interpretation-2403.06952v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Counterfactual Reasoning with Knowledge Graph Embeddings</h3>
                <p>Authors: Lena ZellingerAndreas StephanBenjamin Roth</p>
                <p><a href="http://arxiv.org/abs/2403.06936v1">Link to paper</a></p>
                <p>Knowledge graph embeddings KGEs were originally developed to infer true butmissing facts in incomplete knowledge repositories. In this paper we linkknowledge graph completion and counterfactual reasoning via our new task CFKGR.We model the original world state as a knowledge graph hypothetical scenariosas edges added to the graph and plausible changes to the graph as inferencesfrom logical rules. We create corresponding benchmark datasets which containdiverse hypothetical scenarios with plausible changes to the original knowledgegraph and facts that should be retained. We develop COULDD a general methodfor adapting existing knowledge graph embeddings given a hypothetical premiseand evaluate it on our benchmark. Our results indicate that KGEs learn patternsin the graph without explicit training. We further observe that KGEs adaptedwith COULDD solidly detect plausible counterfactual changes to the graph thatfollow these patterns. An evaluation on human-annotated data reveals that KGEsadapted with COULDD are mostly unable to recognize changes to the graph that donot follow learned inference rules. In contrast ChatGPT mostly outperformsKGEs in detecting plausible changes to the graph but has poor knowledgeretention. In summary CFKGR connects two previously distinct areas namely KGcompletion and counterfactual reasoning.</p>
                <p>Last Updated: 2024-03-11 17:21:39 UTC</p>
                <button class="interpret-button" data-id="2403.06936v1">Interpret</button>
                <div id="interpretation-2403.06936v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Simplicity Bias of Transformers to Learn Low Sensitivity Functions</h3>
                <p>Authors: Bhavya VasudevaDeqing FuTianyi ZhouElliott KauYouqi HuangVatsal Sharan</p>
                <p><a href="http://arxiv.org/abs/2403.06925v1">Link to paper</a></p>
                <p>Transformers achieve state-of-the-art accuracy and robustness across manytasks but an understanding of the inductive biases that they have and howthose biases are different from other neural network architectures remainselusive. Various neural network architectures such as fully connected networkshave been found to have a simplicity bias towards simple functions of the dataone version of this simplicity bias is a spectral bias to learn simplefunctions in the Fourier space. In this work we identify the notion ofsensitivity of the model to random changes in the input as a notion ofsimplicity bias which provides a unified metric to explain the simplicity andspectral bias of transformers across different data modalities. We show thattransformers have lower sensitivity than alternative architectures such asLSTMs MLPs and CNNs across both vision and language tasks. We also show thatlow-sensitivity bias correlates with improved robustness furthermore it canalso be used as an efficient intervention to further improve the robustness oftransformers.</p>
                <p>Last Updated: 2024-03-11 17:12:09 UTC</p>
                <button class="interpret-button" data-id="2403.06925v1">Interpret</button>
                <div id="interpretation-2403.06925v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning</h3>
                <p>Authors: Yichuan LiXiyao MaSixing LuKyumin LeeXiaohu LiuChenlei Guo</p>
                <p><a href="http://arxiv.org/abs/2403.06914v1">Link to paper</a></p>
                <p>Large Language models LLMs have demonstrated impressive in-context learningICL capabilities where a LLM makes predictions for a given test inputtogether with a few input-output pairs demonstrations. Nevertheless theinclusion of demonstrations leads to a quadratic increase in the computationaloverhead of the self-attention mechanism. Existing solutions attempt to distilllengthy demonstrations into compact vectors. However they often requiretask-specific retraining or compromise LLMs in-context learning performance.To mitigate these challenges we present Meta dEmonstratioN DistillationMEND where a language model learns to distill any lengthy demonstrationsinto vectors without retraining for a new downstream task. We exploit theknowledge distillation to enhance alignment between MEND and LLM achievingboth efficiency and effectiveness simultaneously. MEND is endowed with themeta-knowledge of distilling demonstrations through a two-stage trainingprocess which includes meta-distillation pretraining and fine-tuning.Comprehensive evaluations across seven diverse ICL task partitions usingdecoder-only GPT-2 and encoder-decoder T5 attest to MENDs prowess. It notonly matches but often outperforms the Vanilla ICL as well as otherstate-of-the-art distillation models while significantly reducing thecomputational demands. This innovation promises enhanced scalability andefficiency for the practical deployment of large language models</p>
                <p>Last Updated: 2024-03-11 17:03:04 UTC</p>
                <button class="interpret-button" data-id="2403.06914v1">Interpret</button>
                <div id="interpretation-2403.06914v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling</h3>
                <p>Authors: Wele Gedara Chaminda BandaraVishal M. Patel</p>
                <p><a href="http://arxiv.org/abs/2403.06978v1">Link to paper</a></p>
                <p>In this paper we introduce Attention Prompt Tuning APT - a computationallyefficient variant of prompt tuning for video-based applications such as actionrecognition. Prompt tuning approaches involve injecting a set of learnableprompts along with data tokens during fine-tuning while keeping the backbonefrozen. This approach greatly reduces the number of learnable parameterscompared to full tuning. For image-based downstream tasks normally a couple oflearnable prompts achieve results close to those of full tuning. Howevervideos which contain more complex spatiotemporal information require hundredsof tunable prompts to achieve reasonably good results. This reduces theparameter efficiency observed in images and significantly increases latency andthe number of floating-point operations FLOPs during inference. To tacklethese issues we directly inject the prompts into the keys and values of thenon-local attention mechanism within the transformer block. Additionally weintroduce a novel prompt reparameterization technique to make APT more robustagainst hyperparameter selection. The proposed APT approach greatly reduces thenumber of FLOPs and latency while achieving a significant performance boostover the existing parameter-efficient tuning methods on UCF101 HMDB51 andSSv2 datasets for action recognition. The code and pre-trained models areavailable at https://github.com/wgcban/apt</p>
                <p>Last Updated: 2024-03-11 17:59:41 UTC</p>
                <button class="interpret-button" data-id="2403.06978v1">Interpret</button>
                <div id="interpretation-2403.06978v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>VideoMamba: State Space Model for Efficient Video Understanding</h3>
                <p>Authors: Kunchang LiXinhao LiYi WangYinan HeYali WangLimin WangYu Qiao</p>
                <p><a href="http://arxiv.org/abs/2403.06977v1">Link to paper</a></p>
                <p>Addressing the dual challenges of local redundancy and global dependencies invideo understanding this work innovatively adapts the Mamba to the videodomain. The proposed VideoMamba overcomes the limitations of existing 3Dconvolution neural networks and video transformers. Its linear-complexityoperator enables efficient long-term modeling which is crucial forhigh-resolution long video understanding. Extensive evaluations revealVideoMambas four core abilities: 1 Scalability in the visual domain withoutextensive dataset pretraining thanks to a novel self-distillation technique2 Sensitivity for recognizing short-term actions even with fine-grainedmotion differences 3 Superiority in long-term video understandingshowcasing significant advancements over traditional feature-based models and4 Compatibility with other modalities demonstrating robustness inmulti-modal contexts. Through these distinct advantages VideoMamba sets a newbenchmark for video understanding offering a scalable and efficient solutionfor comprehensive video understanding. All the code and models are available athttps://github.com/OpenGVLab/VideoMamba.</p>
                <p>Last Updated: 2024-03-11 17:59:34 UTC</p>
                <button class="interpret-button" data-id="2403.06977v1">Interpret</button>
                <div id="interpretation-2403.06977v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion</h3>
                <p>Authors: Xuan JuXian LiuXintao WangYuxuan BianYing ShanQiang Xu</p>
                <p><a href="http://arxiv.org/abs/2403.06976v1">Link to paper</a></p>
                <p>Image inpainting the process of restoring corrupted images has seensignificant advancements with the advent of diffusion models DMs. Despitethese advancements current DM adaptations for inpainting which involvemodifications to the sampling strategy or the development ofinpainting-specific DMs frequently suffer from semantic inconsistencies andreduced image quality. Addressing these challenges our work introduces a novelparadigm: the division of masked image features and noisy latent into separatebranches. This division dramatically diminishes the models learning loadfacilitating a nuanced incorporation of essential masked image information in ahierarchical fashion. Herein we present BrushNet a novel plug-and-playdual-branch model engineered to embed pixel-level masked image features intoany pre-trained DM guaranteeing coherent and enhanced image inpaintingoutcomes. Additionally we introduce BrushData and BrushBench to facilitatesegmentation-based inpainting training and performance assessment. Ourextensive experimental analysis demonstrates BrushNets superior performanceover existing models across seven key metrics including image quality maskregion preservation and textual coherence.</p>
                <p>Last Updated: 2024-03-11 17:59:31 UTC</p>
                <button class="interpret-button" data-id="2403.06976v1">Interpret</button>
                <div id="interpretation-2403.06976v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Memory-based Adapters for Online 3D Scene Perception</h3>
                <p>Authors: Xiuwei XuChong XiaZiwei WangLinqing ZhaoYueqi DuanJie ZhouJiwen Lu</p>
                <p><a href="http://arxiv.org/abs/2403.06974v1">Link to paper</a></p>
                <p>In this paper we propose a new framework for online 3D scene perception.Conventional 3D scene perception methods are offline i.e. take an alreadyreconstructed 3D scene geometry as input which is not applicable in roboticapplications where the input data is streaming RGB-D videos rather than acomplete 3D scene reconstructed from pre-collected RGB-D videos. To deal withonline 3D scene perception tasks where data collection and perception should beperformed simultaneously the model should be able to process 3D scenes frameby frame and make use of the temporal information. To this end we propose anadapter-based plug-and-play module for the backbone of 3D scene perceptionmodel which constructs memory to cache and aggregate the extracted RGB-Dfeatures to empower offline models with temporal learning ability.Specifically we propose a queued memory mechanism to cache the supportingpoint cloud and image features. Then we devise aggregation modules whichdirectly perform on the memory and pass temporal information to current frame.We further propose 3D-to-2D adapter to enhance image features with strongglobal context. Our adapters can be easily inserted into mainstream offlinearchitectures of different tasks and significantly boost their performance ononline tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrateour approach achieves leading performance on three 3D scene perception taskscompared with state-of-the-art online methods by simply finetuning existingoffline models without any model and task-specific designs.hrefhttps://xuxw98.github.io/Online3D/Project page.</p>
                <p>Last Updated: 2024-03-11 17:57:41 UTC</p>
                <button class="interpret-button" data-id="2403.06974v1">Interpret</button>
                <div id="interpretation-2403.06974v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Bayesian Diffusion Models for 3D Shape Reconstruction</h3>
                <p>Authors: Haiyang XuYu LeiZeyuan ChenXiang ZhangYue ZhaoYilin WangZhuowen Tu</p>
                <p><a href="http://arxiv.org/abs/2403.06973v1">Link to paper</a></p>
                <p>We present Bayesian Diffusion Models BDM a prediction algorithm thatperforms effective Bayesian inference by tightly coupling the top-down priorinformation with the bottom-up data-driven procedure via joint diffusionprocesses. We show the effectiveness of BDM on the 3D shape reconstructiontask. Compared to prototypical deep learning data-driven approaches trained onpaired supervised data-labels e.g. image-point clouds datasets our BDMbrings in rich prior information from standalone labels e.g. point clouds toimprove the bottom-up 3D reconstruction. As opposed to the standard Bayesianframeworks where explicit prior and likelihood are required for the inferenceBDM performs seamless information fusion via coupled diffusion processes withlearned gradient computation networks. The specialty of our BDM lies in itscapability to engage the active and effective information exchange and fusionof the top-down and bottom-up processes where each itself is a diffusionprocess. We demonstrate state-of-the-art results on both synthetic andreal-world benchmarks for 3D shape reconstruction.</p>
                <p>Last Updated: 2024-03-11 17:55:53 UTC</p>
                <button class="interpret-button" data-id="2403.06973v1">Interpret</button>
                <div id="interpretation-2403.06973v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Generalising Multi-Agent Cooperation through Task-Agnostic Communication</h3>
                <p>Authors: Dulhan JayalathSteven MoradAmanda Prorok</p>
                <p><a href="http://arxiv.org/abs/2403.06750v1">Link to paper</a></p>
                <p>Existing communication methods for multi-agent reinforcement learning MARLin cooperative multi-robot problems are almost exclusively task-specifictraining new communication strategies for each unique task. We address thisinefficiency by introducing a communication strategy applicable to any taskwithin a given environment. We pre-train the communication strategy withouttask-specific reward guidance in a self-supervised manner using a setautoencoder. Our objective is to learn a fixed-size latent Markov state from avariable number of agent observations. Under mild assumptions we prove thatpolicies using our latent representations are guaranteed to converge and upperbound the value error introduced by our Markov state approximation. Our methodenables seamless adaptation to novel tasks without fine-tuning thecommunication strategy gracefully supports scaling to more agents than presentduring training and detects out-of-distribution events in an environment.Empirical results on diverse MARL scenarios validate the effectiveness of ourapproach surpassing task-specific communication strategies in unseen tasks.Our implementation of this work is available athttps://github.com/proroklab/task-agnostic-comms.</p>
                <p>Last Updated: 2024-03-11 14:20:13 UTC</p>
                <button class="interpret-button" data-id="2403.06750v1">Interpret</button>
                <div id="interpretation-2403.06750v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>FashionReGen: LLM-Empowered Fashion Report Generation</h3>
                <p>Authors: Yujuan DingYunshan MaWenqi FanYige YaoTat-Seng ChuaQing Li</p>
                <p><a href="http://arxiv.org/abs/2403.06660v1">Link to paper</a></p>
                <p>Fashion analysis refers to the process of examining and evaluating trendsstyles and elements within the fashion industry to understand and interpretits current state generating fashion reports. It is traditionally performed byfashion professionals based on their expertise and experience which requireshigh labour cost and may also produce biased results for relying heavily on asmall group of people. In this paper to tackle the Fashion Report GenerationFashionReGen task we propose an intelligent Fashion Analyzing and Reportingsystem based the advanced Large Language Models LLMs debbed as GPT-FAR.Specifically it tries to deliver FashionReGen based on effective catwalkanalysis which is equipped with several key procedures namely catwalkunderstanding collective organization and analysis and report generation. Byposing and exploring such an open-ended complex and domain-specific task ofFashionReGen it is able to test the general capability of LLMs in fashiondomain. It also inspires the explorations of more high-level tasks withindustrial significance in other domains. Video illustration and more materialsof GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.</p>
                <p>Last Updated: 2024-03-11 12:29:35 UTC</p>
                <button class="interpret-button" data-id="2403.06660v1">Interpret</button>
                <div id="interpretation-2403.06660v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning</h3>
                <p>Authors: Shuo TangRui YeChenxin XuXiaowen DongSiheng ChenYanfeng Wang</p>
                <p><a href="http://arxiv.org/abs/2403.06535v1">Link to paper</a></p>
                <p>Decentralized and lifelong-adaptive multi-agent collaborative learning aimsto enhance collaboration among multiple agents without a central server witheach agent solving varied tasks over time. To achieve efficient collaborationagents should: i autonomously identify beneficial collaborative relationshipsin a decentralized manner and ii adapt to dynamically changing taskobservations. In this paper we propose DeLAMA a decentralized multi-agentlifelong collaborative learning algorithm with dynamic collaboration graphs. Topromote autonomous collaboration relationship learning we propose adecentralized graph structure learning algorithm eliminating the need forexternal priors. To facilitate adaptation to dynamic tasks we design a memoryunit to capture the agents accumulated learning history and knowledge whilepreserving finite storage consumption. To further augment the systemsexpressive capabilities and computational efficiency we apply algorithmunrolling leveraging the advantages of both mathematical optimization andneural networks. This allows the agents to learn to collaborate through thesupervision of training tasks. Our theoretical analysis verifies thatinter-agent collaboration is communication efficient under a small number ofcommunication rounds. The experimental results verify its ability to facilitatethe discovery of collaboration strategies and adaptation to dynamic learningscenarios achieving a 98.80 reduction in MSE and a 188.87 improvement inclassification accuracy. We expect our work can serve as a foundationaltechnique to facilitate future works towards an intelligent decentralized anddynamic multi-agent system. Code is available athttps://github.com/ShuoTang123/DeLAMA.</p>
                <p>Last Updated: 2024-03-11 09:21:11 UTC</p>
                <button class="interpret-button" data-id="2403.06535v1">Interpret</button>
                <div id="interpretation-2403.06535v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Scalable Distributed Optimization Despite Byzantine Adversaries</h3>
                <p>Authors: Kananart KuwaranancharoenLei XinShreyas Sundaram</p>
                <p><a href="http://arxiv.org/abs/2403.06502v1">Link to paper</a></p>
                <p>The problem of distributed optimization requires a group of networked agentsto compute a parameter that minimizes the average of their local costfunctions. While there are a variety of distributed optimization algorithmsthat can solve this problem they are typically vulnerable to Byzantineagents that do not follow the algorithm. Recent attempts to address this issuefocus on single dimensional functions or assume certain statistical propertiesof the functions at the agents. In this paper we provide two resilientscalable distributed optimization algorithms for multi-dimensional functions.Our schemes involve two filters 1 a distance-based filter and 2 a min-maxfilter which each remove neighborhood states that are extreme definedprecisely in our algorithms at each iteration. We show that these algorithmscan mitigate the impact of up to F unknown Byzantine agents in theneighborhood of each regular agent. In particular we show that if the networktopology satisfies certain conditions all of the regular agents states areguaranteed to converge to a bounded region that contains the minimizer of theaverage of the regular agents functions.</p>
                <p>Last Updated: 2024-03-11 08:23:37 UTC</p>
                <button class="interpret-button" data-id="2403.06502v1">Interpret</button>
                <div id="interpretation-2403.06502v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The Geometry of Cyclical Social Trends</h3>
                <p>Authors: Bernard ChazelleKritkorn KarntikoonJakob Nogler</p>
                <p><a href="http://arxiv.org/abs/2403.06376v1">Link to paper</a></p>
                <p>We investigate the emergence of periodic behavior in opinion dynamics and itsunderlying geometry. For this we use a bounded-confidence model withcontrarian agents in a convolution social network. This means that agents adapttheir opinions by interacting with their neighbors in a time-varying socialnetwork. Being contrarian the agents are kept from reaching consensus. This isthe key feature that allows the emergence of cyclical trends. We show that thesystems either converge to nonconsensual equilibrium or are attracted toperiodic or quasi-periodic orbits. We bound the dimension of the attractors andthe period of cyclical trends. We exhibit instances where each orbit is denseand uniformly distributed within its attractor. We also investigate the case ofrandomly changing social networks.</p>
                <p>Last Updated: 2024-03-11 02:00:15 UTC</p>
                <button class="interpret-button" data-id="2403.06376v1">Interpret</button>
                <div id="interpretation-2403.06376v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Bayesian Diffusion Models for 3D Shape Reconstruction</h3>
                <p>Authors: Haiyang XuYu LeiZeyuan ChenXiang ZhangYue ZhaoYilin WangZhuowen Tu</p>
                <p><a href="http://arxiv.org/abs/2403.06973v1">Link to paper</a></p>
                <p>We present Bayesian Diffusion Models BDM a prediction algorithm thatperforms effective Bayesian inference by tightly coupling the top-down priorinformation with the bottom-up data-driven procedure via joint diffusionprocesses. We show the effectiveness of BDM on the 3D shape reconstructiontask. Compared to prototypical deep learning data-driven approaches trained onpaired supervised data-labels e.g. image-point clouds datasets our BDMbrings in rich prior information from standalone labels e.g. point clouds toimprove the bottom-up 3D reconstruction. As opposed to the standard Bayesianframeworks where explicit prior and likelihood are required for the inferenceBDM performs seamless information fusion via coupled diffusion processes withlearned gradient computation networks. The specialty of our BDM lies in itscapability to engage the active and effective information exchange and fusionof the top-down and bottom-up processes where each itself is a diffusionprocess. We demonstrate state-of-the-art results on both synthetic andreal-world benchmarks for 3D shape reconstruction.</p>
                <p>Last Updated: 2024-03-11 17:55:53 UTC</p>
                <button class="interpret-button" data-id="2403.06973v1">Interpret</button>
                <div id="interpretation-2403.06973v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A representation-learning game for classes of prediction tasks</h3>
                <p>Authors: Neria UzanNir Weinberger</p>
                <p><a href="http://arxiv.org/abs/2403.06971v1">Link to paper</a></p>
                <p>We propose a game-based formulation for learning dimensionality-reducingrepresentations of feature vectors when only a prior knowledge on futureprediction tasks is available. In this game the first player chooses arepresentation and then the second player adversarially chooses a predictiontask from a given class representing the prior knowledge. The first playeraims is to minimize and the second player to maximize the regret: The minimalprediction loss using the representation compared to the same loss using theoriginal features. For the canonical setting in which the representation theresponse to predict and the predictors are all linear functions and under themean squared error loss function we derive the theoretically optimalrepresentation in pure strategies which shows the effectiveness of the priorknowledge and the optimal regret in mixed strategies which shows theusefulness of randomizing the representation. For general representations andloss functions we propose an efficient algorithm to optimize a randomizedrepresentation. The algorithm only requires the gradients of the loss functionand is based on incrementally adding a representation rule to a mixture of suchrules.</p>
                <p>Last Updated: 2024-03-11 17:54:42 UTC</p>
                <button class="interpret-button" data-id="2403.06971v1">Interpret</button>
                <div id="interpretation-2403.06971v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts</h3>
                <p>Authors: Onur CelikAleksandar TaranovicGerhard Neumann</p>
                <p><a href="http://arxiv.org/abs/2403.06966v1">Link to paper</a></p>
                <p>Reinforcement learning RL is a powerful approach for acquiring agood-performing policy. However learning diverse skills is challenging in RLdue to the commonly used Gaussian policy parameterization. We proposetextbfDiverse textbfSkill textbfLearning Di-SkilL an RL method forlearning diverse skills using Mixture of Experts where each expert formalizesa skill as a contextual motion primitive. Di-SkilL optimizes each expert andits associate context distribution to a maximum entropy objective thatincentivizes learning diverse skills in similar contexts. The per-expertcontext distribution enables automatic curricula learning allowing each expertto focus on its best-performing sub-region of the context space. To overcomehard discontinuities and multi-modalities without any prior knowledge of theenvironments unknown context probability space we leverage energy-basedmodels to represent the per-expert context distributions and demonstrate how wecan efficiently train them using the standard policy gradient objective. Weshow on challenging robot simulation tasks that Di-SkilL can learn diverse andperformant skills.</p>
                <p>Last Updated: 2024-03-11 17:49:18 UTC</p>
                <button class="interpret-button" data-id="2403.06966v1">Interpret</button>
                <div id="interpretation-2403.06966v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The pitfalls of next-token prediction</h3>
                <p>Authors: Gregor BachmannVaishnavh Nagarajan</p>
                <p><a href="http://arxiv.org/abs/2403.06963v1">Link to paper</a></p>
                <p>Can a mere next-token predictor faithfully model human intelligence Wecrystallize this intuitive concern which is fragmented in the literature. As astarting point we argue that the two often-conflated phases of next-tokenprediction -- autoregressive inference and teacher-forced training -- must betreated distinctly. The popular criticism that errors can compound duringautoregressive inference crucially assumes that teacher-forcing has learned anaccurate next-token predictor. This assumption sidesteps a more deep-rootedproblem we expose: in certain classes of tasks teacher-forcing can simply failto learn an accurate next-token predictor in the first place. We describe ageneral mechanism of how teacher-forcing can fail and design a minimalplanning task where both the Transformer and the Mamba architecture empiricallyfail in that manner -- remarkably despite the task being straightforward tolearn. We provide preliminary evidence that this failure can be resolved whentraining to predict multiple tokens in advance. We hope this finding can groundfuture debates and inspire explorations beyond the next-token predictionparadigm. We make our code available underhttps://github.com/gregorbachmann/Next-Token-Failures</p>
                <p>Last Updated: 2024-03-11 17:47:30 UTC</p>
                <button class="interpret-button" data-id="2403.06963v1">Interpret</button>
                <div id="interpretation-2403.06963v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic Perovskites</h3>
                <p>Authors: Nima KarimitariWilliam J. BaldwinEvan W. MullerZachary J. L. BareW. Joshua KennedyGábor CsányiChristopher Sutton</p>
                <p><a href="http://arxiv.org/abs/2403.06955v1">Link to paper</a></p>
                <p>Low dimensional hybrid organic-inorganic perovskites HOIPs represent apromising class of electronically active materials for both light absorptionand emission. The design space of HOIPs is extremely large since a diversespace of organic cations can be combined with different inorganic frameworks.This immense design space allows for tunable electronic and mechanicalproperties but also necessitates the development of new tools for in silicohigh throughput analysis of candidate structures. In this work we present anaccurate efficient transferable and widely applicable machine learninginteratomic potential MLIP for predicting the structure of new 2D HOIPs.Using the MACE architecture an MLIP is trained on 86 diverse experimentallyreported HOIP structures. The model is tested on 73 unseen perovskitecompositions and achieves chemical accuracy with respect to the referenceelectronic structure method. Our model is then combined with a simple randomstructure search algorithm to predict the structure of hypothetical HOIPs givenonly the proposed composition. Success is demonstrated by correctly andreliably recovering the crystal structure of a set of experimentally known 2Dperovskites. Such a random structure search is impossible with ab initiomethods due to the associated computational cost but is relatively inexpensivewith the MACE potential. Finally the procedure is used to predict thestructure formed by a new organic cation with no previously known correspondingperovskite. Laboratory synthesis of the new hybrid perovskite confirms theaccuracy of our prediction. This capability applied at scale enablesefficient screening of thousands of combinations of organic cations andinorganic layers.</p>
                <p>Last Updated: 2024-03-11 17:39:08 UTC</p>
                <button class="interpret-button" data-id="2403.06955v1">Interpret</button>
                <div id="interpretation-2403.06955v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>MRL Parsing Without Tears: The Case of Hebrew</h3>
                <p>Authors: Shaltiel ShmidmanAvi ShmidmanMoshe KoppelReut Tsarfaty</p>
                <p><a href="http://arxiv.org/abs/2403.06970v1">Link to paper</a></p>
                <p>Syntactic parsing remains a critical tool for relation extraction andinformation extraction especially in resource-scarce languages where LLMs arelacking. Yet in morphologically rich languages MRLs where parsers need toidentify multiple lexical units in each token existing systems suffer inlatency and setup complexity. Some use a pipeline to peel away the layers:first segmentation then morphology tagging and then syntax parsing howevererrors in earlier layers are then propagated forward. Others use a jointarchitecture to evaluate all permutations at once while this improvesaccuracy it is notoriously slow. In contrast and taking Hebrew as a testcase we present a new flipped pipeline: decisions are made directly on thewhole-token units by expert classifiers each one dedicated to one specifictask. The classifiers are independent of one another and only at the end do wesynthesize their predictions. This blazingly fast approach sets a new SOTA inHebrew POS tagging and dependency parsing while also reaching near-SOTAperformance on other Hebrew NLP tasks. Because our architecture does not relyon any language-specific resources it can serve as a model to develop similarparsers for other MRLs.</p>
                <p>Last Updated: 2024-03-11 17:54:33 UTC</p>
                <button class="interpret-button" data-id="2403.06970v1">Interpret</button>
                <div id="interpretation-2403.06970v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena</h3>
                <p>Authors: Leonie WeissweilerAbdullatif KöksalHinrich Schütze</p>
                <p><a href="http://arxiv.org/abs/2403.06965v1">Link to paper</a></p>
                <p>Argument Structure Constructions ASCs are one of the most well-studiedconstruction groups providing a unique opportunity to demonstrate theusefulness of Construction Grammar CxG. For example the caused-motionconstruction CMC She sneezed the foam off her cappuccino demonstratesthat constructions must carry meaning otherwise the fact that sneeze inthis context causes movement cannot be explained. We form the hypothesis thatthis remains challenging even for state-of-the-art Large Language ModelsLLMs for which we devise a test based on substituting the verb with aprototypical motion verb. To be able to perform this test at statisticallysignificant scale in the absence of adequate CxG corpora we develop a novelpipeline of NLP-assisted collection of linguistically annotated text. We showhow dependency parsing and GPT-3.5 can be used to significantly reduceannotation cost and thus enable the annotation of rare phenomena at scale. Wethen evaluate GPT Gemini Llama2 and Mistral models for their understanding ofthe CMC using the newly collected corpus. We find that all models struggle withunderstanding the motion component that the CMC adds to a sentence.</p>
                <p>Last Updated: 2024-03-11 17:47:47 UTC</p>
                <button class="interpret-button" data-id="2403.06965v1">Interpret</button>
                <div id="interpretation-2403.06965v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The pitfalls of next-token prediction</h3>
                <p>Authors: Gregor BachmannVaishnavh Nagarajan</p>
                <p><a href="http://arxiv.org/abs/2403.06963v1">Link to paper</a></p>
                <p>Can a mere next-token predictor faithfully model human intelligence Wecrystallize this intuitive concern which is fragmented in the literature. As astarting point we argue that the two often-conflated phases of next-tokenprediction -- autoregressive inference and teacher-forced training -- must betreated distinctly. The popular criticism that errors can compound duringautoregressive inference crucially assumes that teacher-forcing has learned anaccurate next-token predictor. This assumption sidesteps a more deep-rootedproblem we expose: in certain classes of tasks teacher-forcing can simply failto learn an accurate next-token predictor in the first place. We describe ageneral mechanism of how teacher-forcing can fail and design a minimalplanning task where both the Transformer and the Mamba architecture empiricallyfail in that manner -- remarkably despite the task being straightforward tolearn. We provide preliminary evidence that this failure can be resolved whentraining to predict multiple tokens in advance. We hope this finding can groundfuture debates and inspire explorations beyond the next-token predictionparadigm. We make our code available underhttps://github.com/gregorbachmann/Next-Token-Failures</p>
                <p>Last Updated: 2024-03-11 17:47:30 UTC</p>
                <button class="interpret-button" data-id="2403.06963v1">Interpret</button>
                <div id="interpretation-2403.06963v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data</h3>
                <p>Authors: Jialu LiJaemin ChoYi-Lin SungJaehong YoonMohit Bansal</p>
                <p><a href="http://arxiv.org/abs/2403.06952v1">Link to paper</a></p>
                <p>Recent text-to-image T2I generation models have demonstrated impressivecapabilities in creating images from text descriptions. However these T2Igeneration models often fall short of generating images that precisely matchthe details of the text inputs such as incorrect spatial relationship ormissing objects. In this paper we introduce SELMA: Skill-Specific ExpertLearning and Merging with Auto-Generated Data a novel paradigm to improve thefaithfulness of T2I models by fine-tuning models on automatically generatedmulti-skill image-text datasets with skill-specific expert learning andmerging. First SELMA leverages an LLMs in-context learning capability togenerate multiple datasets of text prompts that can teach different skills andthen generates the images with a T2I model based on the prompts. Next SELMAadapts the T2I model to the new skills by learning multiple single-skill LoRAlow-rank adaptation experts followed by expert merging. Our independentexpert fine-tuning specializes multiple models for different skills and expertmerging helps build a joint multi-skill T2I model that can generate faithfulimages given diverse text prompts while mitigating the knowledge conflict fromdifferent datasets. We empirically demonstrate that SELMA significantlyimproves the semantic alignment and text faithfulness of state-of-the-art T2Idiffusion models on multiple benchmarks 2.1 on TIFA and 6.9 on DSG humanpreference metrics PickScore ImageReward and HPS as well as humanevaluation. Moreover fine-tuning with image-text pairs auto-collected viaSELMA shows comparable performance to fine-tuning with ground truth data.Lastly we show that fine-tuning with images from a weaker T2I model can helpimprove the generation quality of a stronger T2I model suggesting promisingweak-to-strong generalization in T2I models.</p>
                <p>Last Updated: 2024-03-11 17:35:33 UTC</p>
                <button class="interpret-button" data-id="2403.06952v1">Interpret</button>
                <div id="interpretation-2403.06952v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Materials science in the era of large language models: a perspective</h3>
                <p>Authors: Ge LeiRonan DochertySamuel J. Cooper</p>
                <p><a href="http://arxiv.org/abs/2403.06949v1">Link to paper</a></p>
                <p>Large Language Models LLMs have garnered considerable interest due to theirimpressive natural language capabilities which in conjunction with variousemergent properties make them versatile tools in workflows ranging from complexcode generation to heuristic finding for combinatorial problems. In this paperwe offer a perspective on their applicability to materials science researcharguing their ability to handle ambiguous requirements across a range of tasksand disciplines mean they could be a powerful tool to aid researchers. Wequalitatively examine basic LLM theory connecting it to relevant propertiesand techniques in the literature before providing two case studies thatdemonstrate their use in task automation and knowledge extraction at-scale. Attheir current stage of development we argue LLMs should be viewed less asoracles of novel insight and more as tireless workers that can accelerate andunify exploration across domains. It is our hope that this paper canfamiliarise material science researchers with the concepts needed to leveragethese tools in their own research.</p>
                <p>Last Updated: 2024-03-11 17:34:25 UTC</p>
                <button class="interpret-button" data-id="2403.06949v1">Interpret</button>
                <div id="interpretation-2403.06949v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>AI as a Child of Mother Earth: Regrounding Human-AI Interaction in Ecological Thinking</h3>
                <p>Authors: Chunchen XuXiao Ge</p>
                <p><a href="http://dx.doi.org/10.1145/3613905.3644065">Link to paper</a></p>
                <p>The anthropocentric cultural idea that humans are active agents exertingcontrol over their environments has been largely normalized and inscribed inpractices policies and products of contemporary industrialized societies.This view underlies a human-ecology relationship based on resource andknowledge extraction. To create a more sustainable and equitable future it isessential to consider alternative cultural ideas rooted in ecological thinking.This perspective underscores the interconnectedness between humans andmore-than-human worlds. We propose a path to reshape the human-ecologyrelationship by advocating for alternative human-AI interactions. In thispaper we undertake a critical comparison between anthropocentrism andecological thinking using storytelling to illustrate various human-AIinteractions that embody ecological thinking. We also delineate a set of designprinciples aimed at guiding AI developments toward fostering a more caringhuman-ecology relationship.</p>
                <p>Last Updated: 2024-03-11 17:30:22 UTC</p>
                <button class="interpret-button" data-id="2403.06943v1">Interpret</button>
                <div id="interpretation-2403.06943v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Data Cubes in Hand: A Design Space of Tangible Cubes for Visualizing 3D Spatio-Temporal Data in Mixed Reality</h3>
                <p>Authors: Shuqi HeHaonan YaoLuyan JiangKaiwen LiNan XiangYue LiHai-Ning LiangLingyun Yu</p>
                <p><a href="http://dx.doi.org/10.1145/3613904.3642740">Link to paper</a></p>
                <p>Tangible interfaces in mixed reality MR environments allow for intuitivedata interactions. Tangible cubes with their rich interaction affordanceshigh maneuverability and stable structure are particularly well-suited forexploring multi-dimensional data types. However the design potential of thesecubes is underexplored. This study introduces a design space for tangible cubesin MR focusing on interaction space visualization space sizes andmultiplicity. Using spatio-temporal data we explored the interactionaffordances of these cubes in a workshop N24. We identified uniqueinteractions like rotating tapping and stacking which are linked toaugmented reality AR visualization commands. Integrating user-identifiedinteractions we created a design space for tangible-cube interactions andvisualization. A prototype visualizing global health spending with small cubeswas developed and evaluated supporting both individual and combined cubemanipulation. This research enhances our grasp of tangible interaction in MRoffering insights for future design and application in diverse data contexts.</p>
                <p>Last Updated: 2024-03-11 16:47:39 UTC</p>
                <button class="interpret-button" data-id="2403.06891v1">Interpret</button>
                <div id="interpretation-2403.06891v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Transparent AI Disclosure Obligations: Who, What, When, Where, Why, How</h3>
                <p>Authors: Abdallah El AliKarthikeya Puttur VenkatrajSophie MorosoliLaurens NaudtsNatali HelbergerPablo Cesar</p>
                <p><a href="http://dx.doi.org/10.1145/3613905.3650750">Link to paper</a></p>
                <p>Advances in Generative Artificial Intelligence AI are resulting inAI-generated media output that is nearly indistinguishable from human-createdcontent. This can drastically impact users and the media sector especiallygiven global risks of misinformation. While the currently discussed European AIAct aims at addressing these risks through Article 52s AI transparencyobligations its interpretation and implications remain unclear. In this earlywork we adopt a participatory AI approach to derive key questions based onArticle 52s disclosure obligations. We ran two workshops with researchersdesigners and engineers across disciplines N16 where participantsdeconstructed Article 52s relevant clauses using the 5W1H framework. Wecontribute a set of 149 questions clustered into five themes and 18 sub-themes.We believe these can not only help inform future legal developments andinterpretations of Article 52 but also provide a starting point forHuman-Computer Interaction research to re-examine disclosure transparencyfrom a human-centered AI lens.</p>
                <p>Last Updated: 2024-03-11 15:40:36 UTC</p>
                <button class="interpret-button" data-id="2403.06823v1">Interpret</button>
                <div id="interpretation-2403.06823v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Born to Run, Programmed to Play: Mapping the Extended Reality Exergames Landscape</h3>
                <p>Authors: Sukran KaraosmanogluSebastian CmentowskiLennart E. NackeFrank Steinicke</p>
                <p><a href="http://dx.doi.org/10.1145/3613904.3642124">Link to paper</a></p>
                <p>Many people struggle to exercise regularly raising the risk of serioushealth-related issues. Extended reality XR exergames address these hurdles bycombining physical exercises with enjoyable immersive gameplay. While agrowing body of research explores XR exergames no previous review hasstructured this rapidly expanding research landscape. We conducted a scopingreview of the current state of XR exergame research to i provide a structuredoverview ii highlight trends and iii uncover knowledge gaps. Afteridentifying 1318 papers in human-computer interaction and medical databases weultimately included 186 papers in our analysis. We provide a quantitative andqualitative summary of XR exergame research showing current trends andpotential future considerations. Finally we provide a taxonomy of XR exergamesto help future design and methodological investigation and reporting.</p>
                <p>Last Updated: 2024-03-11 14:43:56 UTC</p>
                <button class="interpret-button" data-id="2403.06776v1">Interpret</button>
                <div id="interpretation-2403.06776v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>HILL: A Hallucination Identifier for Large Language Models</h3>
                <p>Authors: Florian LeiserSven EckhardtValentin LeutheMerlin KnaebleAlexander MaedcheGerhard SchwabeAli Sunyaev</p>
                <p><a href="http://arxiv.org/abs/2403.06710v1">Link to paper</a></p>
                <p>Large language models LLMs are prone to hallucinations i.e. nonsensicalunfaithful and undesirable text. Users tend to overrely on LLMs andcorresponding hallucinations which can lead to misinterpretations and errors.To tackle the problem of overreliance we propose HILL the HallucinationIdentifier for Large Language Models. First we identified design features forHILL with a Wizard of Oz approach with nine participants. Subsequently weimplemented HILL based on the identified design features and evaluated HILLsinterface design by surveying 17 participants. Further we investigated HILLsfunctionality to identify hallucinations based on an existingquestion-answering dataset and five user interviews. We find that HILL cancorrectly identify and highlight hallucinations in LLM responses which enablesusers to handle LLM responses with more caution. With that we propose aneasy-to-implement adaptation to existing LLMs and demonstrate the relevance ofuser-centered designs of AI artifacts.</p>
                <p>Last Updated: 2024-03-11 13:36:00 UTC</p>
                <button class="interpret-button" data-id="2403.06710v1">Interpret</button>
                <div id="interpretation-2403.06710v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-03-12</p>
        </div>
    
        </div>
    </body>
    </html>
    