
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Aligning LLMs with Individual Preferences via Interaction</h3>
                <p>Authors: Shujin WuMay FungCheng QianJeonghwan KimDilek Hakkani-TurHeng Ji</p>
                <p><a href="http://arxiv.org/abs/2410.03642v1">Link to paper</a></p>
                <p>As large language models LLMs demonstrate increasingly advancedcapabilities aligning their behaviors with human values and preferencesbecomes crucial for their wide adoption. While previous research focuses ongeneral alignment to principles such as helpfulness harmlessness and honestythe need to account for individual and diverse preferences has been largelyoverlooked potentially undermining customized human experiences. To addressthis gap we train LLMs that can interact to align essentially cultivatingthe meta-skill of LLMs to implicitly infer the unspoken personalizedpreferences of the current user through multi-turn conversations and thendynamically align their following behaviors and responses to these inferredpreferences. Our approach involves establishing a diverse pool of 3310distinct user personas by initially creating seed examples which are thenexpanded through iterative self-generation and filtering. Guided by distinctuser personas we leverage multi-LLM collaboration to develop a multi-turnpreference dataset containing 3K multi-turn conversations in tree structures.Finally we apply supervised fine-tuning and reinforcement learning to enhanceLLMs using this dataset. For evaluation we establish the ALOE ALign WithCustOmized PrEferences benchmark consisting of 100 carefully selectedexamples and well-designed metrics to measure the customized alignmentperformance during conversations. Experimental results demonstrate theeffectiveness of our method in enabling dynamic personalized alignment viainteraction.</p>
                <p>Last Updated: 2024-10-04 17:48:29 UTC</p>
                <button class="interpret-button" data-id="2410.03642v1">Interpret</button>
                <div id="interpretation-2410.03642v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation</h3>
                <p>Authors: Jonathan CookTim RocktäschelJakob FoersterDennis AumillerAlex Wang</p>
                <p><a href="http://arxiv.org/abs/2410.03608v1">Link to paper</a></p>
                <p>Given the widespread adoption and usage of Large Language Models LLMs itis crucial to have flexible and interpretable evaluations of theirinstruction-following ability. Preference judgments between model outputs havebecome the de facto evaluation standard despite distilling complexmulti-faceted preferences into a single ranking. Furthermore as humanannotation is slow and costly LLMs are increasingly used to make thesejudgments at the expense of reliability and interpretability. In this work wepropose TICK Targeted Instruct-evaluation with ChecKlists a fully automatedinterpretable evaluation protocol that structures evaluations withLLM-generated instruction-specific checklists. We first show that given aninstruction LLMs can reliably produce high-quality tailored evaluationchecklists that decompose the instruction into a series of YES/NO questions.Each question asks whether a candidate response meets a specific requirement ofthe instruction. We demonstrate that using TICK leads to a significant increase46.4 to 52.2 in the frequency of exact agreements between LLM judgementsand human preferences as compared to having an LLM directly score an output.We then show that STICK Self-TICK can be used to improve generation qualityacross multiple benchmarks via self-refinement and Best-of-N selection. STICKself-refinement on LiveBench reasoning tasks leads to an absolute gain of7.8 whilst Best-of-N selection with STICK attains 6.3 absoluteimprovement on the real-world instruction dataset WildBench. In light of thisstructured multi-faceted self-improvement is shown to be a promising way tofurther advance LLM capabilities. Finally by providing LLM-generatedchecklists to human evaluators tasked with directly scoring LLM responses toWildBench instructions we notably increase inter-annotator agreement 0.194to 0.256.</p>
                <p>Last Updated: 2024-10-04 17:09:08 UTC</p>
                <button class="interpret-button" data-id="2410.03608v1">Interpret</button>
                <div id="interpretation-2410.03608v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Generative AI in the Software Engineering Domain: Tensions of Occupational Identity and Patterns of Identity Protection</h3>
                <p>Authors: Anuschka SchmittKrzysztof Z. GajosOsnat Mokryn</p>
                <p><a href="http://arxiv.org/abs/2410.03571v1">Link to paper</a></p>
                <p>The adoption of generative Artificial Intelligence GAI in organizationalsettings calls into question workers roles and relatedly the implicationsfor their long-term skill development and domain expertise. In our qualitativestudy in the software engineering domain we build on the theoretical lenses ofoccupational identity and self-determination theory to understand how and whysoftware engineers make sense of GAI for their work. We find that engineerssense-making is contingent on domain expertise as juniors and seniors felttheir needs for competence autonomy and relatedness to be differentlyimpacted by GAI. We shed light on the importance of the individuals role inpreserving tacit domain knowledge as engineers engaged in sense-making thatprotected their occupational identity. We illustrate how organizations play anactive role in shaping workers sense-making process and propose designguidelines on how organizations and system designers can facilitate the impactof technological change on workers occupational identity.</p>
                <p>Last Updated: 2024-10-04 16:20:39 UTC</p>
                <button class="interpret-button" data-id="2410.03571v1">Interpret</button>
                <div id="interpretation-2410.03571v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Artificial Human Lecturers: Initial Findings From Asia's First AI Lecturers in Class to Promote Innovation in Education</h3>
                <p>Authors: Ching Christie PangYawei ZhaoZhizhuo YinJia SunReza Hadi MogaviPan Hui</p>
                <p><a href="http://arxiv.org/abs/2410.03525v1">Link to paper</a></p>
                <p>In recent years artificial intelligence AI has become increasinglyintegrated into education reshaping traditional learning environments. Despitethis there has been limited investigation into fully operational artificialhuman lecturers. To the best of our knowledge our paper presents the worldsfirst study examining their deployment in a real-world educational setting.Specifically we investigate the use of digital teachers AI-powered virtuallecturers in a postgraduate course at the Hong Kong University of Science andTechnology HKUST. Our study explores how features such as appearancenon-verbal cues voice and verbal expression impact students learningexperiences. Findings suggest that students highly value naturalnessauthenticity and interactivity in digital teachers highlighting areas forimprovement such as increased responsiveness personalized avatars andintegration with larger learning platforms. We conclude that digital teachershave significant potential to enhance education by providing a more flexibleengaging personalized and accessible learning experience for students.</p>
                <p>Last Updated: 2024-10-04 15:45:41 UTC</p>
                <button class="interpret-button" data-id="2410.03525v1">Interpret</button>
                <div id="interpretation-2410.03525v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>How Toxicity Classifiers and Large Language Models Respond to Ableism</h3>
                <p>Authors: Mahika PhutaneAnanya SeelamAditya Vashistha</p>
                <p><a href="http://arxiv.org/abs/2410.03448v1">Link to paper</a></p>
                <p>People with disabilities PwD regularly encounter ableist hate andmicroaggressions online. While online platforms use machine learning models tomoderate online harm there is little research investigating how these modelsinteract with ableism. In this paper we curated a dataset of 100 social mediacomments targeted towards PwD and recruited 160 participants to rate andexplain how toxic and ableist these comments were. We then promptedstate-of-the art toxicity classifiers TCs and large language models LLMs torate and explain the harm. Our analysis revealed that TCs and LLMs ratedtoxicity significantly lower than PwD but LLMs rated ableism generally on parwith PwD. However ableism explanations by LLMs overlooked emotional harm andlacked specificity and acknowledgement of context important facets of PwDexplanations. Going forward we discuss challenges in designingdisability-aware toxicity classifiers and advocate for the shift from ableismdetection to ableism interpretation and explanation.</p>
                <p>Last Updated: 2024-10-04 14:09:12 UTC</p>
                <button class="interpret-button" data-id="2410.03448v1">Interpret</button>
                <div id="interpretation-2410.03448v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>System 2 reasoning capabilities are nigh</h3>
                <p>Authors: Scott C. Lowe</p>
                <p><a href="http://arxiv.org/abs/2410.03662v1">Link to paper</a></p>
                <p>In recent years machine learning models have made strides towards human-likereasoning capabilities from several directions. In this work we review thecurrent state of the literature and describe the remaining steps to achieve aneural model which can perform System 2 reasoning analogous to a human. Weargue that if current models are insufficient to be classed as performingreasoning there remains very little additional progress needed to attain thatgoal.</p>
                <p>Last Updated: 2024-10-04 17:59:36 UTC</p>
                <button class="interpret-button" data-id="2410.03662v1">Interpret</button>
                <div id="interpretation-2410.03662v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RAFT: Realistic Attacks to Fool Text Detectors</h3>
                <p>Authors: James WangRan LiJunfeng YangChengzhi Mao</p>
                <p><a href="http://arxiv.org/abs/2410.03658v1">Link to paper</a></p>
                <p>Large language models LLMs have exhibited remarkable fluency across varioustasks. However their unethical applications such as disseminatingdisinformation have become a growing concern. Although recent works haveproposed a number of LLM detection methods their robustness and reliabilityremain unclear. In this paper we present RAFT: a grammar error-free black-boxattack against existing LLM detectors. In contrast to previous attacks forlanguage models our method exploits the transferability of LLM embeddings atthe word-level while preserving the original text quality. We leverage anauxiliary embedding to greedily select candidate words to perturb against thetarget detector. Experiments reveal that our attack effectively compromises alldetectors in the study across various domains by up to 99 and aretransferable across source models. Manual human evaluation studies show ourattacks are realistic and indistinguishable from original human-written text.We also show that examples generated by RAFT can be used to train adversariallyrobust detectors. Our work shows that current LLM detectors are notadversarially robust underscoring the urgent need for more resilient detectionmechanisms.</p>
                <p>Last Updated: 2024-10-04 17:59:00 UTC</p>
                <button class="interpret-button" data-id="2410.03658v1">Interpret</button>
                <div id="interpretation-2410.03658v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Geometric Representation Condition Improves Equivariant Molecule Generation</h3>
                <p>Authors: Zian LiCai ZhouXiyuan WangXingang PengMuhan Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.03655v1">Link to paper</a></p>
                <p>Recent advancements in molecular generative models have demonstratedsubstantial potential in accelerating scientific discovery particularly indrug design. However these models often face challenges in generatinghigh-quality molecules especially in conditional scenarios where specificmolecular properties must be satisfied. In this work we introduce GeoRCG ageneral framework to enhance the performance of molecular generative models byintegrating geometric representation conditions. We decompose the moleculegeneration process into two stages: first generating an informative geometricrepresentation second generating a molecule conditioned on therepresentation. Compared to directly generating a molecule the relativelyeasy-to-generate representation in the first-stage guides the second-stagegeneration to reach a high-quality molecule in a more goal-oriented and muchfaster way. Leveraging EDM as the base generator we observe significantquality improvements in unconditional molecule generation on the widely-usedQM9 and GEOM-DRUG datasets. More notably in the challenging conditionalmolecular generation task our framework achieves an average 31 performanceimprovement over state-of-the-art approaches highlighting the superiority ofconditioning on semantically rich geometric representations over conditioningon individual property values as in previous approaches. Furthermore we showthat with such representation guidance the number of diffusion steps can bereduced to as small as 100 while maintaining superior generation quality thanthat achieved with 1000 steps thereby significantly accelerating thegeneration process.</p>
                <p>Last Updated: 2024-10-04 17:57:35 UTC</p>
                <button class="interpret-button" data-id="2410.03655v1">Interpret</button>
                <div id="interpretation-2410.03655v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Learning Humanoid Locomotion over Challenging Terrain</h3>
                <p>Authors: Ilija RadosavovicSarthak KamatTrevor DarrellJitendra Malik</p>
                <p><a href="http://arxiv.org/abs/2410.03654v1">Link to paper</a></p>
                <p>Humanoid robots can in principle use their legs to go almost anywhere.Developing controllers capable of traversing diverse terrains however remainsa considerable challenge. Classical controllers are hard to generalize broadlywhile the learning-based methods have primarily focused on gentle terrains.Here we present a learning-based approach for blind humanoid locomotioncapable of traversing challenging natural and man-made terrain. Our method usesa transformer model to predict the next action based on the history ofproprioceptive observations and actions. The model is first pre-trained on adataset of flat-ground trajectories with sequence modeling and then fine-tunedon uneven terrain using reinforcement learning. We evaluate our model on a realhumanoid robot across a variety of terrains including rough deformable andsloped surfaces. The model demonstrates robust performance in-contextadaptation and emergent terrain representations. In real-world case studiesour humanoid robot successfully traversed over 4 miles of hiking trails inBerkeley and climbed some of the steepest streets in San Francisco.</p>
                <p>Last Updated: 2024-10-04 17:57:09 UTC</p>
                <button class="interpret-button" data-id="2410.03654v1">Interpret</button>
                <div id="interpretation-2410.03654v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Minimax-optimal trust-aware multi-armed bandits</h3>
                <p>Authors: Changxiao CaiJiacheng Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.03651v1">Link to paper</a></p>
                <p>Multi-armed bandit MAB algorithms have achieved significant success insequential decision-making applications under the premise that humansperfectly implement the recommended policy. However existing methods oftenoverlook the crucial factor of human trust in learning algorithms. When trustis lacking humans may deviate from the recommended policy leading toundesired learning performance. Motivated by this gap we study the trust-awareMAB problem by integrating a dynamic trust model into the standard MABframework. Specifically it assumes that the recommended and actuallyimplemented policy differs depending on human trust which in turn evolves withthe quality of the recommended policy. We establish the minimax regret in thepresence of the trust issue and demonstrate the suboptimality of vanilla MABalgorithms such as the upper confidence bound UCB algorithm. To overcome thislimitation we introduce a novel two-stage trust-aware procedure that provablyattains near-optimal statistical guarantees. A simulation study is conducted toillustrate the benefits of our proposed algorithm when dealing with the trustissue.</p>
                <p>Last Updated: 2024-10-04 17:55:31 UTC</p>
                <button class="interpret-button" data-id="2410.03651v1">Interpret</button>
                <div id="interpretation-2410.03651v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Estimating Body and Hand Motion in an Ego-sensed World</h3>
                <p>Authors: Brent YiVickie YeMaya ZhengLea MüllerGeorgios PavlakosYi MaJitendra MalikAngjoo Kanazawa</p>
                <p><a href="http://arxiv.org/abs/2410.03665v1">Link to paper</a></p>
                <p>We present EgoAllo a system for human motion estimation from a head-mounteddevice. Using only egocentric SLAM poses and images EgoAllo guides samplingfrom a conditional diffusion model to estimate 3D body pose height and handparameters that capture the wearers actions in the allocentric coordinateframe of the scene. To achieve this our key insight is in representation: wepropose spatial and temporal invariance criteria for improving modelperformance from which we derive a head motion conditioning parameterizationthat improves estimation by up to 18. We also show how the bodies estimated byour system can improve the hands: the resulting kinematic and temporalconstraints result in over 40 lower hand estimation errors compared to noisymonocular estimates. Project page: https://egoallo.github.io/</p>
                <p>Last Updated: 2024-10-04 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2410.03665v1">Interpret</button>
                <div id="interpretation-2410.03665v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models</h3>
                <p>Authors: Zhuochun LiYuelyu JiRui MengDaqing He</p>
                <p><a href="http://arxiv.org/abs/2410.03663v1">Link to paper</a></p>
                <p>Large language models LLMs have exhibited complex reasoning abilities bygenerating question rationales and demonstrated exceptional performance innatural language processing NLP tasks. However these reasoning capabilitiesgenerally emerge in models with tens of billions of parameters creatingsignificant computational challenges for real-world deployment. Recent researchhas concentrated on improving open-source smaller models through knowledgedistillation KD from commercial LLMs. Nevertheless most of these studiesrely solely on the responses from one single LLM as the gold rationale fortraining. In this paper we introduce a novel Mistake-Aware Peer-ReviewDistillation MAPD approach: 1 Instead of merely obtaining gold rationalesfrom teachers our method asks teachers to identify and explain the studentsmistakes providing customized instruction learning data. 2 We design asimulated peer-review process between teacher LLMs which selects only thegenerated rationales above the acceptance threshold. This reduces the chance ofteachers guessing correctly with flawed rationale improving instructional dataquality. Comprehensive experiments and analysis on mathematical commonsenseand logical reasoning tasks demonstrate the effectiveness of our method.</p>
                <p>Last Updated: 2024-10-04 17:59:41 UTC</p>
                <button class="interpret-button" data-id="2410.03663v1">Interpret</button>
                <div id="interpretation-2410.03663v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>System 2 reasoning capabilities are nigh</h3>
                <p>Authors: Scott C. Lowe</p>
                <p><a href="http://arxiv.org/abs/2410.03662v1">Link to paper</a></p>
                <p>In recent years machine learning models have made strides towards human-likereasoning capabilities from several directions. In this work we review thecurrent state of the literature and describe the remaining steps to achieve aneural model which can perform System 2 reasoning analogous to a human. Weargue that if current models are insufficient to be classed as performingreasoning there remains very little additional progress needed to attain thatgoal.</p>
                <p>Last Updated: 2024-10-04 17:59:36 UTC</p>
                <button class="interpret-button" data-id="2410.03662v1">Interpret</button>
                <div id="interpretation-2410.03662v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Geometric Representation Condition Improves Equivariant Molecule Generation</h3>
                <p>Authors: Zian LiCai ZhouXiyuan WangXingang PengMuhan Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.03655v1">Link to paper</a></p>
                <p>Recent advancements in molecular generative models have demonstratedsubstantial potential in accelerating scientific discovery particularly indrug design. However these models often face challenges in generatinghigh-quality molecules especially in conditional scenarios where specificmolecular properties must be satisfied. In this work we introduce GeoRCG ageneral framework to enhance the performance of molecular generative models byintegrating geometric representation conditions. We decompose the moleculegeneration process into two stages: first generating an informative geometricrepresentation second generating a molecule conditioned on therepresentation. Compared to directly generating a molecule the relativelyeasy-to-generate representation in the first-stage guides the second-stagegeneration to reach a high-quality molecule in a more goal-oriented and muchfaster way. Leveraging EDM as the base generator we observe significantquality improvements in unconditional molecule generation on the widely-usedQM9 and GEOM-DRUG datasets. More notably in the challenging conditionalmolecular generation task our framework achieves an average 31 performanceimprovement over state-of-the-art approaches highlighting the superiority ofconditioning on semantically rich geometric representations over conditioningon individual property values as in previous approaches. Furthermore we showthat with such representation guidance the number of diffusion steps can bereduced to as small as 100 while maintaining superior generation quality thanthat achieved with 1000 steps thereby significantly accelerating thegeneration process.</p>
                <p>Last Updated: 2024-10-04 17:57:35 UTC</p>
                <button class="interpret-button" data-id="2410.03655v1">Interpret</button>
                <div id="interpretation-2410.03655v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs</h3>
                <p>Authors: Pu HuaMinghuan LiuAnnabella MacalusoYunfeng LinWeinan ZhangHuazhe XuLirui Wang</p>
                <p><a href="http://arxiv.org/abs/2410.03645v1">Link to paper</a></p>
                <p>Robotic simulation today remains challenging to scale up due to the humanefforts required to create diverse simulation tasks and scenes.Simulation-trained policies also face scalability issues as many sim-to-realmethods focus on a single task. To address these challenges this work proposesGenSim2 a scalable framework that leverages coding LLMs with multi-modal andreasoning capabilities for complex and realistic simulation task creationincluding long-horizon tasks with articulated objects. To automaticallygenerate demonstration data for these tasks at scale we propose planning andRL solvers that generalize within object categories. The pipeline can generatedata for up to 100 articulated tasks with 200 objects and reduce the requiredhuman efforts. To utilize such data we propose an effective multi-tasklanguage-conditioned policy architecture dubbed proprioceptive point-cloudtransformer PPT that learns from the generated demonstrations and exhibitsstrong sim-to-real zero-shot transfer. Combining the proposed pipeline and thepolicy architecture we show a promising usage of GenSim2 that the generateddata can be used for zero-shot transfer or co-train with real-world collecteddata which enhances the policy performance by 20 compared with trainingexclusively on limited real data.</p>
                <p>Last Updated: 2024-10-04 17:51:33 UTC</p>
                <button class="interpret-button" data-id="2410.03645v1">Interpret</button>
                <div id="interpretation-2410.03645v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models</h3>
                <p>Authors: Zhuochun LiYuelyu JiRui MengDaqing He</p>
                <p><a href="http://arxiv.org/abs/2410.03663v1">Link to paper</a></p>
                <p>Large language models LLMs have exhibited complex reasoning abilities bygenerating question rationales and demonstrated exceptional performance innatural language processing NLP tasks. However these reasoning capabilitiesgenerally emerge in models with tens of billions of parameters creatingsignificant computational challenges for real-world deployment. Recent researchhas concentrated on improving open-source smaller models through knowledgedistillation KD from commercial LLMs. Nevertheless most of these studiesrely solely on the responses from one single LLM as the gold rationale fortraining. In this paper we introduce a novel Mistake-Aware Peer-ReviewDistillation MAPD approach: 1 Instead of merely obtaining gold rationalesfrom teachers our method asks teachers to identify and explain the studentsmistakes providing customized instruction learning data. 2 We design asimulated peer-review process between teacher LLMs which selects only thegenerated rationales above the acceptance threshold. This reduces the chance ofteachers guessing correctly with flawed rationale improving instructional dataquality. Comprehensive experiments and analysis on mathematical commonsenseand logical reasoning tasks demonstrate the effectiveness of our method.</p>
                <p>Last Updated: 2024-10-04 17:59:41 UTC</p>
                <button class="interpret-button" data-id="2410.03663v1">Interpret</button>
                <div id="interpretation-2410.03663v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models</h3>
                <p>Authors: Tinghui ZhuQin LiuFei WangZhengzhong TuMuhao Chen</p>
                <p><a href="http://arxiv.org/abs/2410.03659v1">Link to paper</a></p>
                <p>Large Vision-Language Models LVLMs have demonstrated impressivecapabilities for capturing and reasoning over multimodal inputs. However thesemodels are prone to parametric knowledge conflicts which arise frominconsistencies of represented knowledge between their vision and languagecomponents. In this paper we formally define the problem oftextbfcross-modality parametric knowledge conflict and present asystematic approach to detect interpret and mitigate them. We introduce apipeline that identifies conflicts between visual and textual answers showinga persistently high conflict rate across modalities in recent LVLMs regardlessof the model size. We further investigate how these conflicts interfere withthe inference process and propose a contrastive metric to discern theconflicting samples from the others. Building on these insights we develop anovel dynamic contrastive decoding method that removes undesirable logitsinferred from the less confident modality components based on answerconfidence. For models that do not provide logits we also introduce twoprompt-based strategies to mitigate the conflicts. Our methods achievepromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.Specifically using LLaVA-34B our proposed dynamic contrastive decodingimproves an average accuracy of 2.24.</p>
                <p>Last Updated: 2024-10-04 17:59:28 UTC</p>
                <button class="interpret-button" data-id="2410.03659v1">Interpret</button>
                <div id="interpretation-2410.03659v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RAFT: Realistic Attacks to Fool Text Detectors</h3>
                <p>Authors: James WangRan LiJunfeng YangChengzhi Mao</p>
                <p><a href="http://arxiv.org/abs/2410.03658v1">Link to paper</a></p>
                <p>Large language models LLMs have exhibited remarkable fluency across varioustasks. However their unethical applications such as disseminatingdisinformation have become a growing concern. Although recent works haveproposed a number of LLM detection methods their robustness and reliabilityremain unclear. In this paper we present RAFT: a grammar error-free black-boxattack against existing LLM detectors. In contrast to previous attacks forlanguage models our method exploits the transferability of LLM embeddings atthe word-level while preserving the original text quality. We leverage anauxiliary embedding to greedily select candidate words to perturb against thetarget detector. Experiments reveal that our attack effectively compromises alldetectors in the study across various domains by up to 99 and aretransferable across source models. Manual human evaluation studies show ourattacks are realistic and indistinguishable from original human-written text.We also show that examples generated by RAFT can be used to train adversariallyrobust detectors. Our work shows that current LLM detectors are notadversarially robust underscoring the urgent need for more resilient detectionmechanisms.</p>
                <p>Last Updated: 2024-10-04 17:59:00 UTC</p>
                <button class="interpret-button" data-id="2410.03658v1">Interpret</button>
                <div id="interpretation-2410.03658v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Aligning LLMs with Individual Preferences via Interaction</h3>
                <p>Authors: Shujin WuMay FungCheng QianJeonghwan KimDilek Hakkani-TurHeng Ji</p>
                <p><a href="http://arxiv.org/abs/2410.03642v1">Link to paper</a></p>
                <p>As large language models LLMs demonstrate increasingly advancedcapabilities aligning their behaviors with human values and preferencesbecomes crucial for their wide adoption. While previous research focuses ongeneral alignment to principles such as helpfulness harmlessness and honestythe need to account for individual and diverse preferences has been largelyoverlooked potentially undermining customized human experiences. To addressthis gap we train LLMs that can interact to align essentially cultivatingthe meta-skill of LLMs to implicitly infer the unspoken personalizedpreferences of the current user through multi-turn conversations and thendynamically align their following behaviors and responses to these inferredpreferences. Our approach involves establishing a diverse pool of 3310distinct user personas by initially creating seed examples which are thenexpanded through iterative self-generation and filtering. Guided by distinctuser personas we leverage multi-LLM collaboration to develop a multi-turnpreference dataset containing 3K multi-turn conversations in tree structures.Finally we apply supervised fine-tuning and reinforcement learning to enhanceLLMs using this dataset. For evaluation we establish the ALOE ALign WithCustOmized PrEferences benchmark consisting of 100 carefully selectedexamples and well-designed metrics to measure the customized alignmentperformance during conversations. Experimental results demonstrate theeffectiveness of our method in enabling dynamic personalized alignment viainteraction.</p>
                <p>Last Updated: 2024-10-04 17:48:29 UTC</p>
                <button class="interpret-button" data-id="2410.03642v1">Interpret</button>
                <div id="interpretation-2410.03642v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>What Matters for Model Merging at Scale?</h3>
                <p>Authors: Prateek YadavTu VuJonathan LaiAlexandra ChronopoulouManaal FaruquiMohit BansalTsendsuren Munkhdalai</p>
                <p><a href="http://arxiv.org/abs/2410.03617v1">Link to paper</a></p>
                <p>Model merging aims to combine multiple expert models into a more capablesingle model offering benefits such as reduced storage and serving costsimproved generalization and support for decentralized model development.Despite its promise previous studies have primarily focused on merging a fewsmall models. This leaves many unanswered questions about the effect of scalingmodel size and how it interplays with other key factors -- like the base modelquality and number of expert models --  to affect the merged modelsperformance. This work systematically evaluates the utility of model merging atscale examining the impact of these different factors. We experiment withmerging fully fine-tuned models using 4 popular merging methods -- AveragingTaskArithmetic Dare and TIES -- across model sizes ranging from 1B-64Bparameters and merging up to 8 different expert models. We evaluate the mergedmodels on both held-in tasks i.e. the experts training tasks and zero-shotgeneralization to unseen held-out tasks. Our experiments provide several newinsights about model merging at scale and the interplay between differentfactors. First we find that merging is more effective when experts are createdfrom strong base models i.e. models with good zero-shot performance. Secondlarger models facilitate easier merging. Third merging consistently improvesgeneralization capabilities. Notably when merging 8 large expert models themerged models often generalize better compared to the multitask trained models.Fourth we can better merge more expert models when working with larger models.Fifth different merging methods behave very similarly at larger scales.Overall our findings shed light on some interesting properties of modelmerging while also highlighting some limitations. We hope that this study willserve as a reference point on large-scale merging for upcoming research.</p>
                <p>Last Updated: 2024-10-04 17:17:19 UTC</p>
                <button class="interpret-button" data-id="2410.03617v1">Interpret</button>
                <div id="interpretation-2410.03617v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Distributed Networked Multi-task Learning</h3>
                <p>Authors: Lingzhou HongAlfredo Garcia</p>
                <p><a href="http://arxiv.org/abs/2410.03403v1">Link to paper</a></p>
                <p>We consider a distributed multi-task learning scheme that accounts formultiple linear model estimation tasks with heterogeneous and/or correlateddata streams. We assume that nodes can be partitioned into groups correspondingto different learning tasks and communicate according to a directed networktopology. Each node estimates a linear model asynchronously and is subject tolocal within-group regularization and global across groups regularizationterms targeting noise reduction and generalization performance improvementrespectively. We provide a finite-time characterization of convergence of theestimators and task relation and illustrate the schemes general applicabilityin two examples: random field temperature estimation and modeling studentperformance from different academic districts.</p>
                <p>Last Updated: 2024-10-04 13:10:31 UTC</p>
                <button class="interpret-button" data-id="2410.03403v1">Interpret</button>
                <div id="interpretation-2410.03403v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-Robot Motion Planning with Diffusion Models</h3>
                <p>Authors: Yorai ShaoulItamar MishaniShivam VatsJiaoyang LiMaxim Likhachev</p>
                <p><a href="http://arxiv.org/abs/2410.03072v1">Link to paper</a></p>
                <p>Diffusion models have recently been successfully applied to a wide range ofrobotics applications for learning complex multi-modal behaviors from data.However prior works have mostly been confined to single-robot and small-scaleenvironments due to the high sample complexity of learning multi-robotdiffusion models. In this paper we propose a method for generatingcollision-free multi-robot trajectories that conform to underlying datadistributions while using only single-robot data. Our algorithm Multi-robotMulti-model planning Diffusion MMD does so by combining learned diffusionmodels with classical search-based techniques -- generating data-driven motionsunder collision constraints. Scaling further we show how to compose multiplediffusion models to plan in large environments where a single diffusion modelfails to generalize well. We demonstrate the effectiveness of our approach inplanning for dozens of robots in a variety of simulated scenarios motivated bylogistics environments. View video demonstrations in our supplementarymaterial and our code at: https://github.com/yoraish/mmd.</p>
                <p>Last Updated: 2024-10-04 01:31:13 UTC</p>
                <button class="interpret-button" data-id="2410.03072v1">Interpret</button>
                <div id="interpretation-2410.03072v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML</h3>
                <p>Authors: Patara TriratWonyong JeongSung Ju Hwang</p>
                <p><a href="http://arxiv.org/abs/2410.02958v1">Link to paper</a></p>
                <p>Automated machine learning AutoML accelerates AI development by automatingtasks in the development pipeline such as optimal model search andhyperparameter tuning. Existing AutoML systems often require technicalexpertise to set up complex tools which is in general time-consuming andrequires a large amount of human effort. Therefore recent works have startedexploiting large language models LLM to lessen such burden and increase theusability of AutoML frameworks via a natural language interface allowingnon-expert users to build their data-driven solutions. These methods howeverare usually designed only for a particular process in the AI developmentpipeline and do not efficiently use the inherent capacity of the LLMs. Thispaper proposes AutoML-Agent a novel multi-agent framework tailored forfull-pipeline AutoML i.e. from data retrieval to model deployment.AutoML-Agent takes users task descriptions facilitates collaboration betweenspecialized LLM agents and delivers deployment-ready models. Unlike existingwork instead of devising a single plan we introduce a retrieval-augmentedplanning strategy to enhance exploration to search for more optimal plans. Wealso decompose each plan into sub-tasks e.g. data preprocessing and neuralnetwork design each of which is solved by a specialized agent we build viaprompting executing in parallel making the search process more efficient.Moreover we propose a multi-stage verification to verify executed results andguide the code generation LLM in implementing successful solutions. Extensiveexperiments on seven downstream tasks using fourteen datasets show thatAutoML-Agent achieves a higher success rate in automating the full AutoMLprocess yielding systems with good performance throughout the diverse domains.</p>
                <p>Last Updated: 2024-10-03 20:01:09 UTC</p>
                <button class="interpret-button" data-id="2410.02958v1">Interpret</button>
                <div id="interpretation-2410.02958v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Grounded Answers for Multi-agent Decision-making Problem through Generative World Model</h3>
                <p>Authors: Zeyang LiuXinrui YangShiguang SunLong QianLipeng WanXingyu ChenXuguang Lan</p>
                <p><a href="http://arxiv.org/abs/2410.02664v1">Link to paper</a></p>
                <p>Recent progress in generative models has stimulated significant innovationsin many fields such as image generation and chatbots. Despite their successthese models often produce sketchy and misleading solutions for complexmulti-agent decision-making problems because they miss the trial-and-errorexperience and reasoning as humans. To address this limitation we explore aparadigm that integrates a language-guided simulator into the multi-agentreinforcement learning pipeline to enhance the generated answer. The simulatoris a world model that separately learns dynamics and reward where the dynamicsmodel comprises an image tokenizer as well as a causal transformer to generateinteraction transitions autoregressively and the reward model is abidirectional transformer learned by maximizing the likelihood of trajectoriesin the expert demonstrations under language guidance. Given an image of thecurrent state and the task description we use the world model to train thejoint policy and produce the image sequence as the answer by running theconverged policy on the dynamics model. The empirical results demonstrate thatthis framework can improve the answers for multi-agent decision-making problemsby showing superior performance on the training and unseen tasks of theStarCraft Multi-Agent Challenge benchmark. In particular it can generateconsistent interaction sequences and explainable reward functions atinteraction states opening the path for training generative models of thefuture.</p>
                <p>Last Updated: 2024-10-03 16:49:59 UTC</p>
                <button class="interpret-button" data-id="2410.02664v1">Interpret</button>
                <div id="interpretation-2410.02664v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Agents' Room: Narrative Generation through Multi-step Collaboration</h3>
                <p>Authors: Fantine HuotReinald Kim AmplayoJennimaria PalomakiAlice Shoshana JakobovitsElizabeth ClarkMirella Lapata</p>
                <p><a href="http://arxiv.org/abs/2410.02603v1">Link to paper</a></p>
                <p>Writing compelling fiction is a multifaceted process combining elements suchas crafting a plot developing interesting characters and using evocativelanguage. While large language models LLMs show promise for story writingthey currently rely heavily on intricate prompting which limits their use. Wepropose Agents Room a generation framework inspired by narrative theory thatdecomposes narrative writing into subtasks tackled by specialized agents. Toillustrate our method we introduce Tell Me A Story a high-quality dataset ofcomplex writing prompts and human-written stories and a novel evaluationframework designed specifically for assessing long narratives. We show thatAgents Room generates stories that are preferred by expert evaluators overthose produced by baseline systems by leveraging collaboration andspecialization to decompose the complex story writing task into tractablecomponents. We provide extensive analysis with automated and human-basedmetrics of the generated output.</p>
                <p>Last Updated: 2024-10-03 15:44:42 UTC</p>
                <button class="interpret-button" data-id="2410.02603v1">Interpret</button>
                <div id="interpretation-2410.02603v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Minimax-optimal trust-aware multi-armed bandits</h3>
                <p>Authors: Changxiao CaiJiacheng Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.03651v1">Link to paper</a></p>
                <p>Multi-armed bandit MAB algorithms have achieved significant success insequential decision-making applications under the premise that humansperfectly implement the recommended policy. However existing methods oftenoverlook the crucial factor of human trust in learning algorithms. When trustis lacking humans may deviate from the recommended policy leading toundesired learning performance. Motivated by this gap we study the trust-awareMAB problem by integrating a dynamic trust model into the standard MABframework. Specifically it assumes that the recommended and actuallyimplemented policy differs depending on human trust which in turn evolves withthe quality of the recommended policy. We establish the minimax regret in thepresence of the trust issue and demonstrate the suboptimality of vanilla MABalgorithms such as the upper confidence bound UCB algorithm. To overcome thislimitation we introduce a novel two-stage trust-aware procedure that provablyattains near-optimal statistical guarantees. A simulation study is conducted toillustrate the benefits of our proposed algorithm when dealing with the trustissue.</p>
                <p>Last Updated: 2024-10-04 17:55:31 UTC</p>
                <button class="interpret-button" data-id="2410.03651v1">Interpret</button>
                <div id="interpretation-2410.03651v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework</h3>
                <p>Authors: Yinuo RenHaoxuan ChenGrant M. RotskoffLexing Ying</p>
                <p><a href="http://arxiv.org/abs/2410.03601v1">Link to paper</a></p>
                <p>Discrete diffusion models have gained increasing attention for their abilityto model complex distributions with tractable sampling and inference. Howeverthe error analysis for discrete diffusion models remains less well-understood.In this work we propose a comprehensive framework for the error analysis ofdiscrete diffusion models based on Levy-type stochastic integrals. Bygeneralizing the Poisson random measure to that with a time-independent andstate-dependent intensity we rigorously establish a stochastic integralformulation of discrete diffusion models and provide the corresponding changeof measure theorems that are intriguingly analogous to Ito integrals andGirsanovs theorem for their continuous counterparts. Our framework unifies andstrengthens the current theoretical results on discrete diffusion models andobtains the first error bound for the tau-leaping scheme in KL divergence.With error sources clearly identified our analysis gives new insight into themathematical properties of discrete diffusion models and offers guidance forthe design of efficient and accurate algorithms for real-world discretediffusion model applications.</p>
                <p>Last Updated: 2024-10-04 16:59:29 UTC</p>
                <button class="interpret-button" data-id="2410.03601v1">Interpret</button>
                <div id="interpretation-2410.03601v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Nonstationary Sparse Spectral Permanental Process</h3>
                <p>Authors: Zicheng SunYixuan ZhangZenan LingXuhui FanFeng Zhou</p>
                <p><a href="http://arxiv.org/abs/2410.03581v1">Link to paper</a></p>
                <p>Existing permanental processes often impose constraints on kernel types orstationarity limiting the models expressiveness. To overcome theselimitations we propose a novel approach utilizing the sparse spectralrepresentation of nonstationary kernels. This technique relaxes the constraintson kernel types and stationarity allowing for more flexible modeling whilereducing computational complexity to the linear level. Additionally weintroduce a deep kernel variant by hierarchically stacking multiple spectralfeature mappings further enhancing the models expressiveness to capturecomplex patterns in data. Experimental results on both synthetic and real-worlddatasets demonstrate the effectiveness of our approach particularly inscenarios with pronounced data nonstationarity. Additionally ablation studiesare conducted to provide insights into the impact of various hyperparameters onmodel performance.</p>
                <p>Last Updated: 2024-10-04 16:40:56 UTC</p>
                <button class="interpret-button" data-id="2410.03581v1">Interpret</button>
                <div id="interpretation-2410.03581v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>On the Hardness of Learning One Hidden Layer Neural Networks</h3>
                <p>Authors: Shuchen LiIlias ZadikManolis Zampetakis</p>
                <p><a href="http://arxiv.org/abs/2410.03477v1">Link to paper</a></p>
                <p>In this work we consider the problem of learning one hidden layer ReLUneural networks with inputs from mathbbRd. We show that this learningproblem is hard under standard cryptographic assumptions even when: 1 thesize of the neural network is polynomial in d 2 its input distribution isa standard Gaussian and 3 the noise is Gaussian and polynomially small ind. Our hardness result is based on the hardness of the Continuous Learningwith Errors CLWE problem and in particular is based on the largely believedworst-case hardness of approximately solving the shortest vector problem up toa multiplicative polynomial factor.</p>
                <p>Last Updated: 2024-10-04 14:48:13 UTC</p>
                <button class="interpret-button" data-id="2410.03477v1">Interpret</button>
                <div id="interpretation-2410.03477v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Linear Transformer Topological Masking with Graph Random Features</h3>
                <p>Authors: Isaac ReidKumar Avinava DubeyDeepali JainWill WhitneyAmr AhmedJoshua AinslieAlex BewleyMithun JacobAranyak MehtaDavid RendlemanConnor SchenckRichard E. TurnerRené WagnerAdrian WellerKrzysztof Choromanski</p>
                <p><a href="http://arxiv.org/abs/2410.03462v1">Link to paper</a></p>
                <p>When training transformers on graph-structured data incorporatinginformation about the underlying topology is crucial for good performance.Topological masking a type of relative position encoding achieves this byupweighting or downweighting attention depending on the relationship betweenthe query and keys in a graph. In this paper we propose to parameterisetopological masks as a learnable function of a weighted adjacency matrix -- anovel flexible approach which incorporates a strong structural inductive bias.By approximating this mask with graph random features for which we prove thefirst known concentration bounds we show how this can be made fullycompatible with linear attention preserving mathcalON time and spacecomplexity with respect to the number of input tokens. The fastest previousalternative was mathcalON log N and only suitable for specific graphs.Our efficient masking algorithms provide strong performance gains for tasks onimage and point cloud data including with 30k nodes.</p>
                <p>Last Updated: 2024-10-04 14:24:06 UTC</p>
                <button class="interpret-button" data-id="2410.03462v1">Interpret</button>
                <div id="interpretation-2410.03462v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Estimating Body and Hand Motion in an Ego-sensed World</h3>
                <p>Authors: Brent YiVickie YeMaya ZhengLea MüllerGeorgios PavlakosYi MaJitendra MalikAngjoo Kanazawa</p>
                <p><a href="http://arxiv.org/abs/2410.03665v1">Link to paper</a></p>
                <p>We present EgoAllo a system for human motion estimation from a head-mounteddevice. Using only egocentric SLAM poses and images EgoAllo guides samplingfrom a conditional diffusion model to estimate 3D body pose height and handparameters that capture the wearers actions in the allocentric coordinateframe of the scene. To achieve this our key insight is in representation: wepropose spatial and temporal invariance criteria for improving modelperformance from which we derive a head motion conditioning parameterizationthat improves estimation by up to 18. We also show how the bodies estimated byour system can improve the hands: the resulting kinematic and temporalconstraints result in over 40 lower hand estimation errors compared to noisymonocular estimates. Project page: https://egoallo.github.io/</p>
                <p>Last Updated: 2024-10-04 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2410.03665v1">Interpret</button>
                <div id="interpretation-2410.03665v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models</h3>
                <p>Authors: Tinghui ZhuQin LiuFei WangZhengzhong TuMuhao Chen</p>
                <p><a href="http://arxiv.org/abs/2410.03659v1">Link to paper</a></p>
                <p>Large Vision-Language Models LVLMs have demonstrated impressivecapabilities for capturing and reasoning over multimodal inputs. However thesemodels are prone to parametric knowledge conflicts which arise frominconsistencies of represented knowledge between their vision and languagecomponents. In this paper we formally define the problem oftextbfcross-modality parametric knowledge conflict and present asystematic approach to detect interpret and mitigate them. We introduce apipeline that identifies conflicts between visual and textual answers showinga persistently high conflict rate across modalities in recent LVLMs regardlessof the model size. We further investigate how these conflicts interfere withthe inference process and propose a contrastive metric to discern theconflicting samples from the others. Building on these insights we develop anovel dynamic contrastive decoding method that removes undesirable logitsinferred from the less confident modality components based on answerconfidence. For models that do not provide logits we also introduce twoprompt-based strategies to mitigate the conflicts. Our methods achievepromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.Specifically using LLaVA-34B our proposed dynamic contrastive decodingimproves an average accuracy of 2.24.</p>
                <p>Last Updated: 2024-10-04 17:59:28 UTC</p>
                <button class="interpret-button" data-id="2410.03659v1">Interpret</button>
                <div id="interpretation-2410.03659v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs</h3>
                <p>Authors: Pu HuaMinghuan LiuAnnabella MacalusoYunfeng LinWeinan ZhangHuazhe XuLirui Wang</p>
                <p><a href="http://arxiv.org/abs/2410.03645v1">Link to paper</a></p>
                <p>Robotic simulation today remains challenging to scale up due to the humanefforts required to create diverse simulation tasks and scenes.Simulation-trained policies also face scalability issues as many sim-to-realmethods focus on a single task. To address these challenges this work proposesGenSim2 a scalable framework that leverages coding LLMs with multi-modal andreasoning capabilities for complex and realistic simulation task creationincluding long-horizon tasks with articulated objects. To automaticallygenerate demonstration data for these tasks at scale we propose planning andRL solvers that generalize within object categories. The pipeline can generatedata for up to 100 articulated tasks with 200 objects and reduce the requiredhuman efforts. To utilize such data we propose an effective multi-tasklanguage-conditioned policy architecture dubbed proprioceptive point-cloudtransformer PPT that learns from the generated demonstrations and exhibitsstrong sim-to-real zero-shot transfer. Combining the proposed pipeline and thepolicy architecture we show a promising usage of GenSim2 that the generateddata can be used for zero-shot transfer or co-train with real-world collecteddata which enhances the policy performance by 20 compared with trainingexclusively on limited real data.</p>
                <p>Last Updated: 2024-10-04 17:51:33 UTC</p>
                <button class="interpret-button" data-id="2410.03645v1">Interpret</button>
                <div id="interpretation-2410.03645v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need</h3>
                <p>Authors: Xianlong WangMinghui LiWei LiuHangtao ZhangShengshan HuYechao ZhangZiqi ZhouHai Jin</p>
                <p><a href="http://arxiv.org/abs/2410.03644v1">Link to paper</a></p>
                <p>Traditional unlearnable strategies have been proposed to prevent unauthorizedusers from training on the 2D image data. With more 3D point cloud datacontaining sensitivity information unauthorized usage of this new type datahas also become a serious concern. To address this we propose the firstintegral unlearnable framework for 3D point clouds including two processes: iwe propose an unlearnable data protection scheme involving a class-wisesetting established by a category-adaptive allocation strategy andmulti-transformations assigned to samples ii we propose a data restorationscheme that utilizes class-wise inverse matrix transformation thus enablingauthorized-only training for unlearnable data. This restoration process is apractical issue overlooked in most existing unlearnable literature ie evenauthorized users struggle to gain knowledge from 3D unlearnable data. Boththeoretical and empirical results including 6 datasets 16 models and 2tasks demonstrate the effectiveness of our proposed unlearnable framework. Ourcode is available at urlhttps://github.com/CGCL-codes/UnlearnablePC</p>
                <p>Last Updated: 2024-10-04 17:49:32 UTC</p>
                <button class="interpret-button" data-id="2410.03644v1">Interpret</button>
                <div id="interpretation-2410.03644v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>HyperCMR: Enhanced Multi-Contrast CMR Reconstruction with Eagle Loss</h3>
                <p>Authors: Ruru XuCaner ÖzerIlkay Oksuz</p>
                <p><a href="http://arxiv.org/abs/2410.03624v1">Link to paper</a></p>
                <p>Accelerating image acquisition for cardiac magnetic resonance imaging CMRIis a critical task. CMRxRecon2024 challenge aims to set the state of the artfor multi-contrast CMR reconstruction. This paper presents HyperCMR a novelframework designed to accelerate the reconstruction of multi-contrast cardiacmagnetic resonance CMR images. HyperCMR enhances the existing PromptMR modelby incorporating advanced loss functions notably the innovative Eagle Losswhich is specifically designed to recover missing high-frequency information inundersampled k-space. Extensive experiments conducted on the CMRxRecon2024challenge dataset demonstrate that HyperCMR consistently outperforms thebaseline across multiple evaluation metrics achieving superior SSIM and PSNRscores.</p>
                <p>Last Updated: 2024-10-04 17:29:38 UTC</p>
                <button class="interpret-button" data-id="2410.03624v1">Interpret</button>
                <div id="interpretation-2410.03624v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-10-08</p>
        </div>
    
        </div>
    </body>
    </html>
    