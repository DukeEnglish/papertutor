
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Notes on Applicability of GPT-4 to Document Understanding</h3>
                <p>Authors: Łukasz Borchmann</p>
                <p><a href="http://arxiv.org/abs/2405.18433v1">Link to paper</a></p>
                <p>We perform a missing reproducible evaluation of all publicly available GPT-4family models concerning the Document Understanding field where it isfrequently required to comprehend text spacial arrangement and visual clues inaddition to textual semantics. Benchmark results indicate that though it ishard to achieve satisfactory results with text-only models GPT-4 Vision Turboperforms well when one provides both text recognized by an external OCR engineand document images on the input. Evaluation is followed by analyses thatsuggest possible contamination of textual GPT-4 models and indicate thesignificant performance drop for lengthy documents.</p>
                <p>Last Updated: 2024-05-28 17:59:53 UTC</p>
                <button class="interpret-button" data-id="2405.18433v1">Interpret</button>
                <div id="interpretation-2405.18433v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Why are Visually-Grounded Language Models Bad at Image Classification?</h3>
                <p>Authors: Yuhui ZhangAlyssa UnellXiaohan WangDhruba GhoshYuchang SuLudwig SchmidtSerena Yeung-Levy</p>
                <p><a href="http://arxiv.org/abs/2405.18415v1">Link to paper</a></p>
                <p>Image classification is one of the most fundamental capabilities of machinevision intelligence. In this work we revisit the image classification taskusing visually-grounded language models VLMs such as GPT-4V and LLaVA. Wefind that existing proprietary and public VLMs despite often using CLIP as avision encoder and having many more parameters significantly underperform CLIPon standard image classification benchmarks like ImageNet. To understand thereason we explore several hypotheses concerning the inference algorithmstraining objectives and data processing in VLMs. Our analysis reveals that theprimary cause is data-related: critical information for image classification isencoded in the VLMs latent space but can only be effectively decoded withenough training data. Specifically there is a strong correlation between thefrequency of class exposure during VLM training and instruction-tuning and theVLMs performance in those classes when trained with sufficient data VLMs canmatch the accuracy of state-of-the-art classification models. Based on thesefindings we enhance a VLM by integrating classification-focused datasets intoits training and demonstrate that the enhanced classification performance ofthe VLM transfers to its general capabilities resulting in an improvement of11.8 on the newly collected ImageWikiQA dataset.</p>
                <p>Last Updated: 2024-05-28 17:57:06 UTC</p>
                <button class="interpret-button" data-id="2405.18415v1">Interpret</button>
                <div id="interpretation-2405.18415v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Don't Forget to Connect! Improving RAG with Graph-based Reranking</h3>
                <p>Authors: Jialin DongBahare FatemiBryan PerozziLin F. YangAnton Tsitsulin</p>
                <p><a href="http://arxiv.org/abs/2405.18414v1">Link to paper</a></p>
                <p>Retrieval Augmented Generation RAG has greatly improved the performance ofLarge Language Model LLM responses by grounding generation with context fromexisting documents. These systems work well when documents are clearly relevantto a question context. But what about when a document has partial informationor less obvious connections to the context And how should we reason aboutconnections between documents In this work we seek to answer these two corequestions about RAG generation. We introduce G-RAG a reranker based on graphneural networks GNNs between the retriever and reader in RAG. Our methodcombines both connections between documents and semantic information viaAbstract Meaning Representation graphs to provide a context-informed rankerfor RAG. G-RAG outperforms state-of-the-art approaches while having smallercomputational footprint. Additionally we assess the performance of PaLM 2 as areranker and find it to significantly underperform G-RAG. This resultemphasizes the importance of reranking for RAG even when using Large LanguageModels.</p>
                <p>Last Updated: 2024-05-28 17:56:46 UTC</p>
                <button class="interpret-button" data-id="2405.18414v1">Interpret</button>
                <div id="interpretation-2405.18414v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RACCooN: Remove, Add, and Change Video Content with Auto-Generated Narratives</h3>
                <p>Authors: Jaehong YoonShoubin YuMohit Bansal</p>
                <p><a href="http://arxiv.org/abs/2405.18406v1">Link to paper</a></p>
                <p>Recent video generative models primarily rely on carefully written textprompts for specific tasks like inpainting or style editing. They requirelabor-intensive textual descriptions for input videos hindering theirflexibility to adapt personal/raw videos to user specifications. This paperproposes RACCooN a versatile and user-friendly video-to-paragraph-to-videogenerative framework that supports multiple video editing capabilities such asremoval addition and modification through a unified pipeline. RACCooNconsists of two principal stages: Video-to-Paragraph V2P andParagraph-to-Video P2V. In the V2P stage we automatically describe videoscenes in well-structured natural language capturing both the holistic contextand focused object details. Subsequently in the P2V stage users canoptionally refine these descriptions to guide the video diffusion modelenabling various modifications to the input video such as removing changingsubjects and/or adding new objects. The proposed approach stands out fromother methods through several significant contributions: 1 RACCooN suggests amulti-granular spatiotemporal pooling strategy to generate well-structuredvideo descriptions capturing both the broad context and object details withoutrequiring complex human annotations simplifying precise video content editingbased on text for users. 2 Our video generative model incorporatesauto-generated narratives or instructions to enhance the quality and accuracyof the generated content. It supports the addition of video objectsinpainting and attribute modification within a unified framework surpassingexisting video editing and inpainting benchmarks. The proposed frameworkdemonstrates impressive versatile capabilities in video-to-paragraphgeneration video content editing and can be incorporated into other SoTAvideo generative models for further enhancement.</p>
                <p>Last Updated: 2024-05-28 17:46:36 UTC</p>
                <button class="interpret-button" data-id="2405.18406v1">Interpret</button>
                <div id="interpretation-2405.18406v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass</h3>
                <p>Authors: Ethan ShenAlan FanSarah M PrattJae Sung ParkMatthew WallingfordSham M. KakadeAri HoltzmanRanjay KrishnaAli FarhadiAditya Kusupati</p>
                <p><a href="http://arxiv.org/abs/2405.18400v1">Link to paper</a></p>
                <p>Many applications today provide users with multiple auto-complete drafts asthey type including GitHubs code completion Gmails smart compose andApples messaging auto-suggestions. Under the hood language models supportthis by running an autoregressive inference pass to provide a draft.Consequently providing k drafts to the user requires running an expensivelanguage model k times. To alleviate the computation cost of running kinference passes we propose Superposed Decoding a new decoding algorithm thatgenerates k drafts at the computation cost of one autoregressive inferencepass. We achieve this by feeding a superposition of the k most recent tokenembeddings from the drafts as input to the next decoding step of the languagemodel. At every inference step we combine the k drafts with the top-ktokens to get k2 new drafts and cache the k most likely options using ann-gram interpolation with minimal compute overhead to filter out incoherentgenerations. Our experiments show that k drafts from Superposed Decoding areat least as coherent and factual as Nucleus Sampling and Greedy Decodingrespectively while being at least 2.44times faster for kge3. In acompute-normalized setting user evaluations demonstrably favor text generatedby Superposed Decoding over Nucleus Sampling. Code and more examplesopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.</p>
                <p>Last Updated: 2024-05-28 17:40:48 UTC</p>
                <button class="interpret-button" data-id="2405.18400v1">Interpret</button>
                <div id="interpretation-2405.18400v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets</h3>
                <p>Authors: Khen CohenNoam LeviYaron Oz</p>
                <p><a href="http://arxiv.org/abs/2405.18427v1">Link to paper</a></p>
                <p>We derive closed-form expressions for the Bayes optimal decision boundariesin binary classification of high dimensional overlapping Gaussian mixture modelGMM data and show how they depend on the eigenstructure of the classcovariances for particularly interesting structured data. We empiricallydemonstrate through experiments on synthetic GMMs inspired by real-world datathat deep neural networks trained for classification learn predictors whichapproximate the derived optimal classifiers. We further extend our study tonetworks trained on authentic data observing that decision thresholdscorrelate with the covariance eigenvectors rather than the eigenvaluesmirroring our GMM analysis. This provides theoretical insights regarding neuralnetworks ability to perform probabilistic inference and distill statisticalpatterns from intricate distributions.</p>
                <p>Last Updated: 2024-05-28 17:59:31 UTC</p>
                <button class="interpret-button" data-id="2405.18427v1">Interpret</button>
                <div id="interpretation-2405.18427v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Tensor Methods in High Dimensional Data Analysis: Opportunities and Challenges</h3>
                <p>Authors: Arnab AuddyDong XiaMing Yuan</p>
                <p><a href="http://arxiv.org/abs/2405.18412v1">Link to paper</a></p>
                <p>Large amount of multidimensional data represented by multiway arrays ortensors are prevalent in modern applications across various fields such aschemometrics genomics physics psychology and signal processing. Thestructural complexity of such data provides vast new opportunities for modelingand analysis but efficiently extracting information content from them bothstatistically and computationally presents unique and fundamental challenges.Addressing these challenges requires an interdisciplinary approach that bringstogether tools and insights from statistics optimization and numerical linearalgebra among other fields. Despite these hurdles significant progress hasbeen made in the last decade. This review seeks to examine some of the keyadvancements and identify common threads among them under eight differentstatistical settings.</p>
                <p>Last Updated: 2024-05-28 17:54:03 UTC</p>
                <button class="interpret-button" data-id="2405.18412v1">Interpret</button>
                <div id="interpretation-2405.18412v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Explicit Formulae to Interchangeably use Hyperplanes and Hyperballs using Inversive Geometry</h3>
                <p>Authors: Erik ThordsenErich Schubert</p>
                <p><a href="http://arxiv.org/abs/2405.18401v1">Link to paper</a></p>
                <p>Many algorithms require discriminative boundaries such as separatinghyperplanes or hyperballs or are specifically designed to work on sphericaldata. By applying inversive geometry we show that the two discriminativeboundaries can be used interchangeably and that general Euclidean data can betransformed into spherical data whenever a change in point distances isacceptable. We provide explicit formulae to embed general Euclidean data intospherical data and to unembed it back. We further show a duality betweenhyperspherical caps i.e. the volume created by a separating hyperplane onspherical data and hyperballs and provide explicit formulae to map between thetwo. We further provide equations to translate inner products and Euclideandistances between the two spaces to avoid explicit embedding and unembedding.We also provide a method to enforce projections of the general Euclidean spaceonto hemi-hyperspheres and propose an intrinsic dimensionality based method toobtain all-purpose parameters. To show the usefulness of thecap-ball-duality we discuss example applications in machine learning andvector similarity search.</p>
                <p>Last Updated: 2024-05-28 17:43:16 UTC</p>
                <button class="interpret-button" data-id="2405.18401v1">Interpret</button>
                <div id="interpretation-2405.18401v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Note on the Prediction-Powered Bootstrap</h3>
                <p>Authors: Tijana Zrnic</p>
                <p><a href="http://arxiv.org/abs/2405.18379v1">Link to paper</a></p>
                <p>We introduce PPBoot: a bootstrap-based method for prediction-poweredinference. PPBoot is applicable to arbitrary estimation problems and is verysimple to implement essentially only requiring one application of thebootstrap. Through a series of examples we demonstrate that PPBoot oftenperforms nearly identically to and sometimes better than the earlier PPImethod based on asymptotic normalityunicodex2013when the latter isapplicableunicodex2013without requiring any asymptotic characterizations.Given its versatility PPBoot could simplify and expand the scope ofapplication of prediction-powered inference to problems where central limittheorems are hard to prove.</p>
                <p>Last Updated: 2024-05-28 17:22:15 UTC</p>
                <button class="interpret-button" data-id="2405.18379v1">Interpret</button>
                <div id="interpretation-2405.18379v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Hessian-Aware Stochastic Differential Equation for Modelling SGD</h3>
                <p>Authors: Xiang LiZebang ShenLiang ZhangNiao He</p>
                <p><a href="http://arxiv.org/abs/2405.18373v1">Link to paper</a></p>
                <p>Continuous-time approximation of Stochastic Gradient Descent SGD is acrucial tool to study its escaping behaviors from stationary points. Howeverexisting stochastic differential equation SDE models fail to fully capturethese behaviors even for simple quadratic objectives. Built on a novelstochastic backward error analysis framework we derive the Hessian-AwareStochastic Modified Equation HA-SME an SDE that incorporates Hessianinformation of the objective function into both its drift and diffusion terms.Our analysis shows that HA-SME matches the order-best approximation errorguarantee among existing SDE models in the literature while achieving asignificantly reduced dependence on the smoothness parameter of the objective.Further for quadratic objectives under mild conditions HA-SME is proved tobe the first SDE model that recovers exactly the SGD dynamics in thedistributional sense. Consequently when the local landscape near a stationarypoint can be approximated by quadratics HA-SME is expected to accuratelypredict the local escaping behaviors of SGD.</p>
                <p>Last Updated: 2024-05-28 17:11:34 UTC</p>
                <button class="interpret-button" data-id="2405.18373v1">Interpret</button>
                <div id="interpretation-2405.18373v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention</h3>
                <p>Authors: Lianghui ZhuZilong HuangBencheng LiaoJun Hao LiewHanshu YanJiashi FengXinggang Wang</p>
                <p><a href="http://arxiv.org/abs/2405.18428v1">Link to paper</a></p>
                <p>Diffusion models with large-scale pre-training have achieved significantsuccess in the field of visual content generation particularly exemplified byDiffusion Transformers DiT. However DiT models have faced challenges withscalability and quadratic complexity efficiency. In this paper we aim toleverage the long sequence modeling capability of Gated Linear Attention GLATransformers expanding its applicability to diffusion models. We introduceDiffusion Gated Linear Attention Transformers DiG a simple adoptablesolution with minimal parameter overhead following the DiT design butoffering superior efficiency and effectiveness. In addition to betterperformance than DiT DiG-S/2 exhibits 2.5times higher training speed thanDiT-S/2 and saves 75.7 GPU memory at a resolution of 1792 times 1792.Moreover we analyze the scalability of DiG across a variety of computationalcomplexity. DiG models with increased depth/width or augmentation of inputtokens consistently exhibit decreasing FID. We further compare DiG with othersubquadratic-time diffusion models. With the same model size DiG-XL/2 is4.2times faster than the recent Mamba-based diffusion model at a 1024resolution and is 1.8times faster than DiT with CUDA-optimizedFlashAttention-2 under the 2048 resolution. All these results demonstrate itssuperior efficiency among the latest diffusion models. Code is released athttps://github.com/hustvl/DiG.</p>
                <p>Last Updated: 2024-05-28 17:59:33 UTC</p>
                <button class="interpret-button" data-id="2405.18428v1">Interpret</button>
                <div id="interpretation-2405.18428v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GFlow: Recovering 4D World from Monocular Video</h3>
                <p>Authors: Shizun WangXingyi YangQiuhong ShenZhenxiang JiangXinchao Wang</p>
                <p><a href="http://arxiv.org/abs/2405.18426v1">Link to paper</a></p>
                <p>Reconstructing 4D scenes from video inputs is a crucial yet challenging task.Conventional methods usually rely on the assumptions of multi-view videoinputs known camera parameters or static scenes all of which are typicallyabsent under in-the-wild scenarios. In this paper we relax all theseconstraints and tackle a highly ambitious but practical task which we termedas AnyV4D: we assume only one monocular video is available without any cameraparameters as input and we aim to recover the dynamic 4D world alongside thecamera poses. To this end we introduce GFlow a new framework that utilizesonly 2D priors depth and optical flow to lift a video 3D to a 4D explicitrepresentation entailing a flow of Gaussian splatting through space and time.GFlow first clusters the scene into still and moving parts then applies asequential optimization process that optimizes camera poses and the dynamics of3D Gaussian points based on 2D priors and scene clustering ensuring fidelityamong neighboring points and smooth movement across frames. Since dynamicscenes always introduce new content we also propose a new pixel-wisedensification strategy for Gaussian points to integrate new visual content.Moreover GFlow transcends the boundaries of mere 4D reconstruction it alsoenables tracking of any points across frames without the need for priortraining and segments moving objects from the scene in an unsupervised way.Additionally the camera poses of each frame can be derived from GFlowallowing for rendering novel views of a video scene through changing camerapose. By employing the explicit representation we may readily conductscene-level or object-level editing as desired underscoring its versatilityand power. Visit our project website at: https://littlepure2333.github.io/GFlow</p>
                <p>Last Updated: 2024-05-28 17:59:22 UTC</p>
                <button class="interpret-button" data-id="2405.18426v1">Interpret</button>
                <div id="interpretation-2405.18426v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention</h3>
                <p>Authors: Bencheng LiaoXinggang WangLianghui ZhuQian ZhangChang Huang</p>
                <p><a href="http://arxiv.org/abs/2405.18425v1">Link to paper</a></p>
                <p>Recently linear complexity sequence modeling networks have achieved modelingcapabilities similar to Vision Transformers on a variety of computer visiontasks while using fewer FLOPs and less memory. However their advantage interms of actual runtime speed is not significant. To address this issue weintroduce Gated Linear Attention GLA for vision leveraging its superiorhardware-awareness and efficiency. We propose direction-wise gating to capture1D global context through bidirectional modeling and a 2D gating localityinjection to adaptively inject 2D local details into 1D global context. Ourhardware-aware implementation further merges forward and backward scanning intoa single kernel enhancing parallelism and reducing memory cost and latency.The proposed model name offers a favorable trade-off in accuracyparameters and FLOPs on ImageNet and downstream tasks outperforming popularTransformer and CNN-based models. Notably name-S matches DeiT-Bs accuracywhile using only 27 of the parameters and 20 of the FLOPs running2times faster on 224times224 images. At 1024times1024 resolutionname-T uses 5.2times fewer FLOPs saves 90 GPU memory runs 4.8timesfaster and achieves 20.7 higher top-1 accuracy than DeiT-T. These resultsposition name as an efficient and scalable solution for visualrepresentation learning. Code is available aturlhttps://github.com/hustvl/ViG.</p>
                <p>Last Updated: 2024-05-28 17:59:21 UTC</p>
                <button class="interpret-button" data-id="2405.18425v1">Interpret</button>
                <div id="interpretation-2405.18425v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting</h3>
                <p>Authors: Qihang ZhangYinghao XuChaoyang WangHsin-Ying LeeGordon WetzsteinBolei ZhouCeyuan Yang</p>
                <p><a href="http://arxiv.org/abs/2405.18424v1">Link to paper</a></p>
                <p>Scene image editing is crucial for entertainment photography andadvertising design. Existing methods solely focus on either 2D individualobject or 3D global scene editing. This results in a lack of a unified approachto effectively control and manipulate scenes at the 3D level with differentlevels of granularity. In this work we propose 3DitScene a novel and unifiedscene editing framework leveraging language-guided disentangled GaussianSplatting that enables seamless editing from 2D to 3D allowing precise controlover scene composition and individual objects. We first incorporate 3DGaussians that are refined through generative priors and optimizationtechniques. Language features from CLIP then introduce semantics into 3Dgeometry for object disentanglement. With the disentangled Gaussians 3DitSceneallows for manipulation at both the global and individual levelsrevolutionizing creative expression and empowering control over scenes andobjects. Experimental results demonstrate the effectiveness and versatility of3DitScene in scene image editing. Code and online demo can be found at ourproject homepage: https://zqh0253.github.io/3DitScene/.</p>
                <p>Last Updated: 2024-05-28 17:59:01 UTC</p>
                <button class="interpret-button" data-id="2405.18424v1">Interpret</button>
                <div id="interpretation-2405.18424v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Hierarchical World Models as Visual Whole-Body Humanoid Controllers</h3>
                <p>Authors: Nicklas HansenJyothir S VVlad SobalYann LeCunXiaolong WangHao Su</p>
                <p><a href="http://arxiv.org/abs/2405.18418v1">Link to paper</a></p>
                <p>Whole-body control for humanoids is challenging due to the high-dimensionalnature of the problem coupled with the inherent instability of a bipedalmorphology. Learning from visual observations further exacerbates thisdifficulty. In this work we explore highly data-driven approaches to visualwhole-body humanoid control based on reinforcement learning without anysimplifying assumptions reward design or skill primitives. Specifically wepropose a hierarchical world model in which a high-level agent generatescommands based on visual observations for a low-level agent to execute both ofwhich are trained with rewards. Our approach produces highly performant controlpolicies in 8 tasks with a simulated 56-DoF humanoid while synthesizingmotions that are broadly preferred by humans. Code and videos:https://nicklashansen.com/rlpuppeteer</p>
                <p>Last Updated: 2024-05-28 17:57:23 UTC</p>
                <button class="interpret-button" data-id="2405.18418v1">Interpret</button>
                <div id="interpretation-2405.18418v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Brain Tumor Segmentation (BraTS) Challenge 2024: Meningioma Radiotherapy Planning Automated Segmentation</h3>
                <p>Authors: Dominic LaBellaKatherine SchumacherMichael MixKevin LeuShan McBurney-LinPierre NedelecJavier Villanueva-MeyerJonathan ShapeyTom VercauterenKazumi ChiaOmar Al-SalihiJustin LeuLia HalaszYury VelichkoChunhao WangJohn KirkpatrickScott FloydZachary J. ReitmanTrey MullikinUlas BagciSean SachdevJona A. Hattangadi-GluthTyler SeibertNikdokht FaridConnor PuettMatthew W. PeaseKevin ShiueSyed Muhammad AnwarShahriar FaghaniMuhammad Ammar HaiderPranav WarmanJake AlbrechtAndrás JakabMana MoassefiVerena ChungAlejandro AristizabalAlexandros KarargyrisHasan KassemSarthak PatiMicah ShellerChristina HuangAaron ColeySiddharth GhantaAlex SchneiderConrad SharpRachit SalujaFlorian KoflerPhilipp LohmannPhillipp VollmuthLouis GagnonMaruf AdewoleHongwei Bran LiAnahita Fathi KazerooniNourel Hoda TahonUdunna AnazodoAhmed W. MoawadBjoern MenzeMarius George LinguraruMariam AboianBenedikt WiestlerUjjwal BaidGian-Marco ConteAndreas M. T. RauscheckerAyman NadaAly H. AbayazeedRaymond HuangMaria Correia de VerdierJeffrey D. RudieSpyridon BakasEvan Calabrese</p>
                <p><a href="http://arxiv.org/abs/2405.18383v1">Link to paper</a></p>
                <p>The 2024 Brain Tumor Segmentation Meningioma Radiotherapy BraTS-MEN-RTchallenge aims to advance automated segmentation algorithms using the largestknown multi-institutional dataset of radiotherapy planning brain MRIs withexpert-annotated target labels for patients with intact or post-operativemeningioma that underwent either conventional external beam radiotherapy orstereotactic radiosurgery. Each case includes a defaced 3D post-contrastT1-weighted radiotherapy planning MRI in its native acquisition spaceaccompanied by a single-label target volume representing the gross tumorvolume GTV and any at-risk post-operative site. Target volume annotationsadhere to established radiotherapy planning protocols ensuring consistencyacross cases and institutions. For pre-operative meningiomas the target volumeencompasses the entire GTV and associated nodular dural tail while forpost-operative cases it includes at-risk resection cavity margins asdetermined by the treating institution. Case annotations were reviewed andapproved by expert neuroradiologists and radiation oncologists. Participatingteams will develop containerize and evaluate automated segmentation modelsusing this comprehensive dataset. Model performance will be assessed using thelesion-wise Dice Similarity Coefficient and the 95 Hausdorff distance. Thetop-performing teams will be recognized at the Medical Image Computing andComputer Assisted Intervention Conference in October 2024. BraTS-MEN-RT isexpected to significantly advance automated radiotherapy planning by enablingprecise tumor segmentation and facilitating tailored treatment ultimatelyimproving patient outcomes.</p>
                <p>Last Updated: 2024-05-28 17:25:43 UTC</p>
                <button class="interpret-button" data-id="2405.18383v1">Interpret</button>
                <div id="interpretation-2405.18383v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Hostile Counterspeech Drives Users From Hate Subreddits</h3>
                <p>Authors: Daniel HickeyMatheus SchmitzDaniel M. T. FesslerPaul E. SmaldinoKristina LermanGoran MurićKeith Burghardt</p>
                <p><a href="http://arxiv.org/abs/2405.18374v1">Link to paper</a></p>
                <p>Counterspeech -- speech that opposes hate speech -- has gained significantattention recently as a strategy to reduce hate on social media. While previousstudies suggest that counterspeech can somewhat reduce hate speech little isknown about its effects on participation in online hate communities nor whichcounterspeech tactics reduce harmful behavior. We begin to address these gapsby identifying 25 large hate communities subreddits within Reddit andanalyzing the effect of counterspeech on newcomers within these communities. Wefirst construct a new public dataset of carefully annotated counterspeech andnon-counterspeech comments within these subreddits. We use this dataset totrain a state-of-the-art counterspeech detection model. Next we use matchingto evaluate the causal effects of hostile and non-hostile counterspeech on theengagement of newcomers in hate subreddits. We find that while non-hostilecounterspeech is ineffective at keeping users from fully disengaging from thesehate subreddits a single hostile counterspeech comment substantially reducesboth future likelihood of engagement. While offering nuance to theunderstanding of counterspeech efficacy these results a leave unanswered thequestion of whether hostile counterspeech dissuades newcomers fromparticipation in online hate writ large or merely drives them intoless-moderated and more extreme hate communities and b raises ethicalconsiderations about hostile counterspeech which is both comparatively commonand might exacerbate rather than mitigate the net level of antagonism insociety. These findings underscore the importance of future work to improvecounterspeech tactics and minimize unintended harm.</p>
                <p>Last Updated: 2024-05-28 17:12:41 UTC</p>
                <button class="interpret-button" data-id="2405.18374v1">Interpret</button>
                <div id="interpretation-2405.18374v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>The CoExplorer Technology Probe: A Generative AI-Powered Adaptive Interface to Support Intentionality in Planning and Running Video Meetings</h3>
                <p>Authors: Gun WooParkPayod PandaLev TankelevitchSean Rintel</p>
                <p><a href="http://arxiv.org/abs/2405.18239v1">Link to paper</a></p>
                <p>Effective meetings are effortful but traditional videoconferencing systemsoffer little support for reducing this effort across the meeting lifecycle.Generative AI GenAI has the potential to radically redefine meetings byaugmenting intentional meeting behaviors. CoExplorer our novel adaptivemeeting prototype preemptively generates likely phases that meetings wouldundergo tools that allow capturing attendees thoughts before the meeting andfor each phase window layouts and appropriate applications and files. UsingCoExplorer as a technology probe in a guided walkthrough we studied itspotential in a sample of participants from a global technology company. Ourfindings suggest that GenAI has the potential to help meetings stay on trackand reduce workload although concerns were raised about users agency trustand possible disruption to traditional meeting norms. We discuss these concernsand their design implications for the development of GenAI meeting technology.</p>
                <p>Last Updated: 2024-05-28 14:48:19 UTC</p>
                <button class="interpret-button" data-id="2405.18239v1">Interpret</button>
                <div id="interpretation-2405.18239v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>An Open-Source Reproducible Chess Robot for Human-Robot Interaction Research</h3>
                <p>Authors: Renchi ZhangJoost de WinterDimitra DodouHarleigh SeyffertYke Bauke Eisma</p>
                <p><a href="http://arxiv.org/abs/2405.18170v1">Link to paper</a></p>
                <p>Recent advancements in AI have sped up the evolution of versatile robotdesigns. Chess provides a standardized environment that allows for theevaluation of the influence of robot behaviors on human behavior. This articlepresents an open-source chess robot for human-robot interaction HRI researchspecifically focusing on verbal and non-verbal interactions. OpenChessRobotrecognizes chess pieces using computer vision executes moves and interactswith the human player using voice and robotic gestures. We detail the softwaredesign provide quantitative evaluations of the robots efficacy and offer aguide for its reproducibility. The code and are accessible on GitHub:https://github.com/renchizhhhh/OpenChessRobot</p>
                <p>Last Updated: 2024-05-28 13:30:53 UTC</p>
                <button class="interpret-button" data-id="2405.18170v1">Interpret</button>
                <div id="interpretation-2405.18170v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Fatigue and mental underload further pronounced in L3 conditionally automated driving: Results from an EEG experiment on a test track</h3>
                <p>Authors: Nikol FigalováHans Joachim BiegMichael SchulzJürgen PichenMartin BaumannLewis ChuangOlga Pollatos</p>
                <p><a href="http://dx.doi.org/10.1145/3581754.3584133">Link to paper</a></p>
                <p>Drivers role changes with increasing automation from the primary driver to asystem supervisor. This study investigates how supervising an SAE L2 and L3automated vehicle AV affects drivers mental workload and sleepiness comparedto manual driving. Using an AV prototype on a test track the oscillatory brainactivity of 23 adult participants was recorded during L2 L3 and manualdriving. Results showed decreased mental workload and increased sleepiness inL3 drives compared to L2 and manual drives indicated by self-report scales andchanges in the frontal alpha and theta power spectral density. These findingssuggest that fatigue and mental underload are significant issues in L3 drivingand should be considered when designing future AV interfaces.</p>
                <p>Last Updated: 2024-05-28 12:23:24 UTC</p>
                <button class="interpret-button" data-id="2405.18114v1">Interpret</button>
                <div id="interpretation-2405.18114v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention</h3>
                <p>Authors: Lianghui ZhuZilong HuangBencheng LiaoJun Hao LiewHanshu YanJiashi FengXinggang Wang</p>
                <p><a href="http://arxiv.org/abs/2405.18428v1">Link to paper</a></p>
                <p>Diffusion models with large-scale pre-training have achieved significantsuccess in the field of visual content generation particularly exemplified byDiffusion Transformers DiT. However DiT models have faced challenges withscalability and quadratic complexity efficiency. In this paper we aim toleverage the long sequence modeling capability of Gated Linear Attention GLATransformers expanding its applicability to diffusion models. We introduceDiffusion Gated Linear Attention Transformers DiG a simple adoptablesolution with minimal parameter overhead following the DiT design butoffering superior efficiency and effectiveness. In addition to betterperformance than DiT DiG-S/2 exhibits 2.5times higher training speed thanDiT-S/2 and saves 75.7 GPU memory at a resolution of 1792 times 1792.Moreover we analyze the scalability of DiG across a variety of computationalcomplexity. DiG models with increased depth/width or augmentation of inputtokens consistently exhibit decreasing FID. We further compare DiG with othersubquadratic-time diffusion models. With the same model size DiG-XL/2 is4.2times faster than the recent Mamba-based diffusion model at a 1024resolution and is 1.8times faster than DiT with CUDA-optimizedFlashAttention-2 under the 2048 resolution. All these results demonstrate itssuperior efficiency among the latest diffusion models. Code is released athttps://github.com/hustvl/DiG.</p>
                <p>Last Updated: 2024-05-28 17:59:33 UTC</p>
                <button class="interpret-button" data-id="2405.18428v1">Interpret</button>
                <div id="interpretation-2405.18428v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets</h3>
                <p>Authors: Khen CohenNoam LeviYaron Oz</p>
                <p><a href="http://arxiv.org/abs/2405.18427v1">Link to paper</a></p>
                <p>We derive closed-form expressions for the Bayes optimal decision boundariesin binary classification of high dimensional overlapping Gaussian mixture modelGMM data and show how they depend on the eigenstructure of the classcovariances for particularly interesting structured data. We empiricallydemonstrate through experiments on synthetic GMMs inspired by real-world datathat deep neural networks trained for classification learn predictors whichapproximate the derived optimal classifiers. We further extend our study tonetworks trained on authentic data observing that decision thresholdscorrelate with the covariance eigenvectors rather than the eigenvaluesmirroring our GMM analysis. This provides theoretical insights regarding neuralnetworks ability to perform probabilistic inference and distill statisticalpatterns from intricate distributions.</p>
                <p>Last Updated: 2024-05-28 17:59:31 UTC</p>
                <button class="interpret-button" data-id="2405.18427v1">Interpret</button>
                <div id="interpretation-2405.18427v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>GFlow: Recovering 4D World from Monocular Video</h3>
                <p>Authors: Shizun WangXingyi YangQiuhong ShenZhenxiang JiangXinchao Wang</p>
                <p><a href="http://arxiv.org/abs/2405.18426v1">Link to paper</a></p>
                <p>Reconstructing 4D scenes from video inputs is a crucial yet challenging task.Conventional methods usually rely on the assumptions of multi-view videoinputs known camera parameters or static scenes all of which are typicallyabsent under in-the-wild scenarios. In this paper we relax all theseconstraints and tackle a highly ambitious but practical task which we termedas AnyV4D: we assume only one monocular video is available without any cameraparameters as input and we aim to recover the dynamic 4D world alongside thecamera poses. To this end we introduce GFlow a new framework that utilizesonly 2D priors depth and optical flow to lift a video 3D to a 4D explicitrepresentation entailing a flow of Gaussian splatting through space and time.GFlow first clusters the scene into still and moving parts then applies asequential optimization process that optimizes camera poses and the dynamics of3D Gaussian points based on 2D priors and scene clustering ensuring fidelityamong neighboring points and smooth movement across frames. Since dynamicscenes always introduce new content we also propose a new pixel-wisedensification strategy for Gaussian points to integrate new visual content.Moreover GFlow transcends the boundaries of mere 4D reconstruction it alsoenables tracking of any points across frames without the need for priortraining and segments moving objects from the scene in an unsupervised way.Additionally the camera poses of each frame can be derived from GFlowallowing for rendering novel views of a video scene through changing camerapose. By employing the explicit representation we may readily conductscene-level or object-level editing as desired underscoring its versatilityand power. Visit our project website at: https://littlepure2333.github.io/GFlow</p>
                <p>Last Updated: 2024-05-28 17:59:22 UTC</p>
                <button class="interpret-button" data-id="2405.18426v1">Interpret</button>
                <div id="interpretation-2405.18426v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention</h3>
                <p>Authors: Bencheng LiaoXinggang WangLianghui ZhuQian ZhangChang Huang</p>
                <p><a href="http://arxiv.org/abs/2405.18425v1">Link to paper</a></p>
                <p>Recently linear complexity sequence modeling networks have achieved modelingcapabilities similar to Vision Transformers on a variety of computer visiontasks while using fewer FLOPs and less memory. However their advantage interms of actual runtime speed is not significant. To address this issue weintroduce Gated Linear Attention GLA for vision leveraging its superiorhardware-awareness and efficiency. We propose direction-wise gating to capture1D global context through bidirectional modeling and a 2D gating localityinjection to adaptively inject 2D local details into 1D global context. Ourhardware-aware implementation further merges forward and backward scanning intoa single kernel enhancing parallelism and reducing memory cost and latency.The proposed model name offers a favorable trade-off in accuracyparameters and FLOPs on ImageNet and downstream tasks outperforming popularTransformer and CNN-based models. Notably name-S matches DeiT-Bs accuracywhile using only 27 of the parameters and 20 of the FLOPs running2times faster on 224times224 images. At 1024times1024 resolutionname-T uses 5.2times fewer FLOPs saves 90 GPU memory runs 4.8timesfaster and achieves 20.7 higher top-1 accuracy than DeiT-T. These resultsposition name as an efficient and scalable solution for visualrepresentation learning. Code is available aturlhttps://github.com/hustvl/ViG.</p>
                <p>Last Updated: 2024-05-28 17:59:21 UTC</p>
                <button class="interpret-button" data-id="2405.18425v1">Interpret</button>
                <div id="interpretation-2405.18425v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Why are Visually-Grounded Language Models Bad at Image Classification?</h3>
                <p>Authors: Yuhui ZhangAlyssa UnellXiaohan WangDhruba GhoshYuchang SuLudwig SchmidtSerena Yeung-Levy</p>
                <p><a href="http://arxiv.org/abs/2405.18415v1">Link to paper</a></p>
                <p>Image classification is one of the most fundamental capabilities of machinevision intelligence. In this work we revisit the image classification taskusing visually-grounded language models VLMs such as GPT-4V and LLaVA. Wefind that existing proprietary and public VLMs despite often using CLIP as avision encoder and having many more parameters significantly underperform CLIPon standard image classification benchmarks like ImageNet. To understand thereason we explore several hypotheses concerning the inference algorithmstraining objectives and data processing in VLMs. Our analysis reveals that theprimary cause is data-related: critical information for image classification isencoded in the VLMs latent space but can only be effectively decoded withenough training data. Specifically there is a strong correlation between thefrequency of class exposure during VLM training and instruction-tuning and theVLMs performance in those classes when trained with sufficient data VLMs canmatch the accuracy of state-of-the-art classification models. Based on thesefindings we enhance a VLM by integrating classification-focused datasets intoits training and demonstrate that the enhanced classification performance ofthe VLM transfers to its general capabilities resulting in an improvement of11.8 on the newly collected ImageWikiQA dataset.</p>
                <p>Last Updated: 2024-05-28 17:57:06 UTC</p>
                <button class="interpret-button" data-id="2405.18415v1">Interpret</button>
                <div id="interpretation-2405.18415v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Mutation-Bias Learning in Games</h3>
                <p>Authors: Johann BauerSheldon WestEduardo AlonsoMark Broom</p>
                <p><a href="http://arxiv.org/abs/2405.18190v1">Link to paper</a></p>
                <p>We present two variants of a multi-agent reinforcement learning algorithmbased on evolutionary game theoretic considerations. The intentional simplicityof one variant enables us to prove results on its relationship to a system ofordinary differential equations of replicator-mutator dynamics type allowingus to present proofs on the algorithms convergence conditions in varioussettings via its ODE counterpart. The more complicated variant enablescomparisons to Q-learning based algorithms. We compare both variantsexperimentally to WoLF-PHC and frequency-adjusted Q-learning on a range ofsettings illustrating cases of increasing dimensionality where our variantspreserve convergence in contrast to more complicated algorithms. Theavailability of analytic results provides a degree of transferability ofresults as compared to purely empirical case studies illustrating the generalutility of a dynamical systems perspective on multi-agent reinforcementlearning when addressing questions of convergence and reliable generalisation.</p>
                <p>Last Updated: 2024-05-28 14:02:44 UTC</p>
                <button class="interpret-button" data-id="2405.18190v1">Interpret</button>
                <div id="interpretation-2405.18190v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Individual Contributions as Intrinsic Exploration Scaffolds for Multi-agent Reinforcement Learning</h3>
                <p>Authors: Xinran LiZifan LiuShibo ChenJun Zhang</p>
                <p><a href="http://arxiv.org/abs/2405.18110v1">Link to paper</a></p>
                <p>In multi-agent reinforcement learning MARL effective exploration iscritical especially in sparse reward environments. Although introducing globalintrinsic rewards can foster exploration in such settings it often complicatescredit assignment among agents. To address this difficulty we proposeIndividual Contributions as intrinsic Exploration Scaffolds ICES a novelapproach to motivate exploration by assessing each agents contribution from aglobal view. In particular ICES constructs exploration scaffolds with Bayesiansurprise leveraging global transition information during centralized training.These scaffolds used only in training help to guide individual agents towardsactions that significantly impact the global latent state transitions.Additionally ICES separates exploration policies from exploitation policiesenabling the former to utilize privileged global information during training.Extensive experiments on cooperative benchmark tasks with sparse rewardsincluding Google Research Football GRF and StarCraft Multi-agent ChallengeSMAC demonstrate that ICES exhibits superior exploration capabilitiescompared with baselines. The code is publicly available athttps://github.com/LXXXXR/ICES.</p>
                <p>Last Updated: 2024-05-28 12:18:19 UTC</p>
                <button class="interpret-button" data-id="2405.18110v1">Interpret</button>
                <div id="interpretation-2405.18110v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins</h3>
                <p>Authors: Yuchen XiaDaniel DittlerNasser JazdiHaonan ChenMichael Weyrich</p>
                <p><a href="http://arxiv.org/abs/2405.18092v1">Link to paper</a></p>
                <p>This paper presents a novel design of a multi-agent system framework thatapplies a large language model LLM to automate the parametrization of processsimulations in digital twins. We propose a multi-agent framework that includesfour types of agents: observation reasoning decision and summarization. Byenabling dynamic interaction between LLM agents and simulation model thedeveloped system can automatically explore the parametrization of thesimulation and use heuristic reasoning to determine a set of parameters tocontrol the simulation to achieve an objective. The proposed approach enhancesthe simulation model by infusing it with heuristics from LLM and enablesautonomous search for feasible parametrization to solve a user task.Furthermore the system has the potential to increase user-friendliness andreduce the cognitive load on human users by assisting in complexdecision-making processes. The effectiveness and functionality of the systemare demonstrated through a case study and the visualized demos are availableat a GitHub Repository: https://github.com/YuchenXia/LLMDrivenSimulation</p>
                <p>Last Updated: 2024-05-28 11:59:40 UTC</p>
                <button class="interpret-button" data-id="2405.18092v1">Interpret</button>
                <div id="interpretation-2405.18092v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Network Diffusion -- Framework to Simulate Spreading Processes in Complex Networks</h3>
                <p>Authors: Michał CzubaMateusz NurekDamian SerwataYu-Xuan QiuMingshan JiaKatarzyna MusialRadosław MichalskiPiotr Bródka</p>
                <p><a href="http://dx.doi.org/10.26599/BDMA.2024.9020010">Link to paper</a></p>
                <p>With the advancement of computational network science its research scope hassignificantly expanded beyond static graphs to encompass more complexstructures. The introduction of streaming temporal multilayer andhypernetwork approaches has brought new possibilities and imposed additionalrequirements. For instance by utilising these advancements one can modelstructures such as social networks in a much more refined manner which isparticularly relevant in simulations of the spreading processes. Unfortunatelythe pace of advancement is often too rapid for existing computational packagesto keep up with the functionality updates. This results in a significantproliferation of tools used by researchers and consequently a lack of auniversally accepted technological stack that would standardise experimentalmethods as seen e.g. in machine learning. This article addresses that issueby presenting an extended version of the Network Diffusion library. First asurvey of the existing approaches and toolkits for simulating spreadingphenomena is shown and then an overview of the framework functionalities.Finally we report four case studies conducted with the package to demonstrateits usefulness: the impact of sanitary measures on the spread of COVID-19 thecomparison of information diffusion on two temporal network models and theeffectiveness of seed selection methods in the task of influence maximisationin multilayer networks. We conclude the paper with a critical assessment of thelibrary and the outline of still awaiting challenges to standardise researchenvironments in computational network science.</p>
                <p>Last Updated: 2024-05-28 11:46:18 UTC</p>
                <button class="interpret-button" data-id="2405.18085v1">Interpret</button>
                <div id="interpretation-2405.18085v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Cognitive Insights and Stable Coalition Matching for Fostering Multi-Agent Cooperation</h3>
                <p>Authors: Jiaqi ShaoTianjun YuanTao LinXuanyu CaoBing Luo</p>
                <p><a href="http://arxiv.org/abs/2405.18044v1">Link to paper</a></p>
                <p>Cognitive abilities such as Theory of Mind ToM play a vital role infacilitating cooperation in human social interactions. However our studyreveals that agents with higher ToM abilities may not necessarily exhibitbetter cooperative behavior compared to those with lower ToM abilities. Toaddress this challenge we propose a novel matching coalition mechanism thatleverages the strengths of agents with different ToM levels by explicitlyconsidering belief alignment and specialized abilities when forming coalitions.Our proposed matching algorithm seeks to find stable coalitions that maximizethe potential for cooperative behavior and ensure long-term viability. Byincorporating cognitive insights into the design of multi-agent systems ourwork demonstrates the potential of leveraging ToM to create more sophisticatedand human-like coordination strategies that foster cooperation and improveoverall system performance.</p>
                <p>Last Updated: 2024-05-28 10:59:33 UTC</p>
                <button class="interpret-button" data-id="2405.18044v1">Interpret</button>
                <div id="interpretation-2405.18044v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>On the Origin of Llamas: Model Tree Heritage Recovery</h3>
                <p>Authors: Eliahu HorwitzAsaf ShulYedid Hoshen</p>
                <p><a href="http://arxiv.org/abs/2405.18432v1">Link to paper</a></p>
                <p>The rapid growth of neural network models shared on the internet has mademodel weights an important data modality. However this information isunderutilized as the weights are uninterpretable and publicly available modelsare disorganized. Inspired by Darwins tree of life we define the Model Treewhich describes the origin of models i.e. the parent model that was used tofine-tune the target model. Similarly to the natural world the tree structureis unknown. In this paper we introduce the task of Model Tree HeritageRecovery MoTHer Recovery for discovering Model Trees in the ever-growinguniverse of neural networks. Our hypothesis is that model weights encode thisinformation the challenge is to decode the underlying tree structure given theweights. Beyond the immediate application of model authorship attributionMoTHer recovery holds exciting long-term applications akin to indexing theinternet by search engines. Practically for each pair of models this taskrequires: i determining if they are related and ii establishing thedirection of the relationship. We find that certain distributional propertiesof the weights evolve monotonically during training which enables us toclassify the relationship between two given models. MoTHer recoveryreconstructs entire model hierarchies represented by a directed tree where aparent model gives rise to multiple child models through additional training.Our approach successfully reconstructs complex Model Trees as well as thestructure of in-the-wild model families such as Llama 2 and Stable Diffusion.</p>
                <p>Last Updated: 2024-05-28 17:59:51 UTC</p>
                <button class="interpret-button" data-id="2405.18432v1">Interpret</button>
                <div id="interpretation-2405.18432v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets</h3>
                <p>Authors: Khen CohenNoam LeviYaron Oz</p>
                <p><a href="http://arxiv.org/abs/2405.18427v1">Link to paper</a></p>
                <p>We derive closed-form expressions for the Bayes optimal decision boundariesin binary classification of high dimensional overlapping Gaussian mixture modelGMM data and show how they depend on the eigenstructure of the classcovariances for particularly interesting structured data. We empiricallydemonstrate through experiments on synthetic GMMs inspired by real-world datathat deep neural networks trained for classification learn predictors whichapproximate the derived optimal classifiers. We further extend our study tonetworks trained on authentic data observing that decision thresholdscorrelate with the covariance eigenvectors rather than the eigenvaluesmirroring our GMM analysis. This provides theoretical insights regarding neuralnetworks ability to perform probabilistic inference and distill statisticalpatterns from intricate distributions.</p>
                <p>Last Updated: 2024-05-28 17:59:31 UTC</p>
                <button class="interpret-button" data-id="2405.18427v1">Interpret</button>
                <div id="interpretation-2405.18427v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Hierarchical World Models as Visual Whole-Body Humanoid Controllers</h3>
                <p>Authors: Nicklas HansenJyothir S VVlad SobalYann LeCunXiaolong WangHao Su</p>
                <p><a href="http://arxiv.org/abs/2405.18418v1">Link to paper</a></p>
                <p>Whole-body control for humanoids is challenging due to the high-dimensionalnature of the problem coupled with the inherent instability of a bipedalmorphology. Learning from visual observations further exacerbates thisdifficulty. In this work we explore highly data-driven approaches to visualwhole-body humanoid control based on reinforcement learning without anysimplifying assumptions reward design or skill primitives. Specifically wepropose a hierarchical world model in which a high-level agent generatescommands based on visual observations for a low-level agent to execute both ofwhich are trained with rewards. Our approach produces highly performant controlpolicies in 8 tasks with a simulated 56-DoF humanoid while synthesizingmotions that are broadly preferred by humans. Code and videos:https://nicklashansen.com/rlpuppeteer</p>
                <p>Last Updated: 2024-05-28 17:57:23 UTC</p>
                <button class="interpret-button" data-id="2405.18418v1">Interpret</button>
                <div id="interpretation-2405.18418v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Why are Visually-Grounded Language Models Bad at Image Classification?</h3>
                <p>Authors: Yuhui ZhangAlyssa UnellXiaohan WangDhruba GhoshYuchang SuLudwig SchmidtSerena Yeung-Levy</p>
                <p><a href="http://arxiv.org/abs/2405.18415v1">Link to paper</a></p>
                <p>Image classification is one of the most fundamental capabilities of machinevision intelligence. In this work we revisit the image classification taskusing visually-grounded language models VLMs such as GPT-4V and LLaVA. Wefind that existing proprietary and public VLMs despite often using CLIP as avision encoder and having many more parameters significantly underperform CLIPon standard image classification benchmarks like ImageNet. To understand thereason we explore several hypotheses concerning the inference algorithmstraining objectives and data processing in VLMs. Our analysis reveals that theprimary cause is data-related: critical information for image classification isencoded in the VLMs latent space but can only be effectively decoded withenough training data. Specifically there is a strong correlation between thefrequency of class exposure during VLM training and instruction-tuning and theVLMs performance in those classes when trained with sufficient data VLMs canmatch the accuracy of state-of-the-art classification models. Based on thesefindings we enhance a VLM by integrating classification-focused datasets intoits training and demonstrate that the enhanced classification performance ofthe VLM transfers to its general capabilities resulting in an improvement of11.8 on the newly collected ImageWikiQA dataset.</p>
                <p>Last Updated: 2024-05-28 17:57:06 UTC</p>
                <button class="interpret-button" data-id="2405.18415v1">Interpret</button>
                <div id="interpretation-2405.18415v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Don't Forget to Connect! Improving RAG with Graph-based Reranking</h3>
                <p>Authors: Jialin DongBahare FatemiBryan PerozziLin F. YangAnton Tsitsulin</p>
                <p><a href="http://arxiv.org/abs/2405.18414v1">Link to paper</a></p>
                <p>Retrieval Augmented Generation RAG has greatly improved the performance ofLarge Language Model LLM responses by grounding generation with context fromexisting documents. These systems work well when documents are clearly relevantto a question context. But what about when a document has partial informationor less obvious connections to the context And how should we reason aboutconnections between documents In this work we seek to answer these two corequestions about RAG generation. We introduce G-RAG a reranker based on graphneural networks GNNs between the retriever and reader in RAG. Our methodcombines both connections between documents and semantic information viaAbstract Meaning Representation graphs to provide a context-informed rankerfor RAG. G-RAG outperforms state-of-the-art approaches while having smallercomputational footprint. Additionally we assess the performance of PaLM 2 as areranker and find it to significantly underperform G-RAG. This resultemphasizes the importance of reranking for RAG even when using Large LanguageModels.</p>
                <p>Last Updated: 2024-05-28 17:56:46 UTC</p>
                <button class="interpret-button" data-id="2405.18414v1">Interpret</button>
                <div id="interpretation-2405.18414v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-05-29</p>
        </div>
    
        </div>
    </body>
    </html>
    