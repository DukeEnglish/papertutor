
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>CiteClick: A Browser Extension for Real-Time Scholar Citation Tracking</h3>
                <p>Authors: Nishat Raihan</p>
                <p><a href="http://arxiv.org/abs/2410.16211v1">Link to paper</a></p>
                <p>This technical report presents CiteClick a browser extension designed tomonitor and track Google Scholar citation counts for multiple researchers inreal-time. We discuss the motivation behind the tool its key featuresimplementation details and potential impact on the academic community. Thereport covers installation procedures usage guidelines and customizationoptions concluding with a discussion on future work and potentialimprovements. By automating the process of citation tracking CiteClick aims toenhance research evaluation processes and facilitate more informeddecision-making in academic contexts.</p>
                <p>Last Updated: 2024-10-21 17:11:31 UTC</p>
                <button class="interpret-button" data-id="2410.16211v1">Interpret</button>
                <div id="interpretation-2410.16211v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Musinger: Communication of Music over a Distance with Wearable Haptic Display and Touch Sensitive Surface</h3>
                <p>Authors: Miguel Altamirano CabreraMuhammad Haris KhanAli AlabbasLuis MorenoIssatay TokmurziyevDzmitry Tsetserukou</p>
                <p><a href="http://arxiv.org/abs/2410.16202v1">Link to paper</a></p>
                <p>This study explores the integration of auditory and tactile experiences inmusical haptics focusing on enhancing sensory dimensions of music throughtouch. Addressing the gap in translating auditory signals to meaningful tactilefeedback our research introduces a novel method involving a touch-sensitiverecorder and a wearable haptic display that captures musical interactions viaforce sensors and converts these into tactile sensations. Previous studies haveshown the potential of haptic feedback to enhance musical expressivity yetchallenges remain in conveying complex musical nuances. Our method aims toexpand music accessibility for individuals with hearing impairments and deependigital musical interactions. Experimental results reveal high accuracy 98without noise 93 with white noise in melody recognition through tactilefeedback demonstrating effective transmission and perception of musicalinformation. The findings highlight the potential of haptic technology tobridge sensory gaps offering significant implications for music therapyeducation and remote musical collaboration advancing the field of musicalhaptics and multi-sensory technology applications.</p>
                <p>Last Updated: 2024-10-21 17:04:28 UTC</p>
                <button class="interpret-button" data-id="2410.16202v1">Interpret</button>
                <div id="interpretation-2410.16202v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Privacy as Social Norm: Systematically Reducing Dysfunctional Privacy Concerns on Social Media</h3>
                <p>Authors: JaeWon KimSoobin ChoRobert WolfeJishnu Hari NairAlexis Hiniker</p>
                <p><a href="http://arxiv.org/abs/2410.16137v1">Link to paper</a></p>
                <p>Privacy is essential to fully enjoying the benefits of social media. Whilefear around privacy risks can sometimes motivate privacy management thenegative impact of such fear particularly when it is perceived asunaddressable i.e. dysfunctional fear can significantly harm teenwell-being. In a co-design study with 136 participants aged 13-18 we exploredhow teens can protect their privacy without experiencing heightened fear. Weidentified seven different sources of dysfunctional fear such as fear of ahostile environment and fear of overstepping privacy norms. We alsoevaluated ten designs co-created with teen participants that address thesefears. Our findings suggest that social media platforms can mitigatedysfunctional fear without compromising privacy by creating a culture whereprivacy protection is the norm through default privacy-protective features.However we also found that even the most effective privacy features are notlikely to be adopted unless they balance the multifaceted and diverse needs ofteens. Individual teens have different needs -- for example public and privateaccount users have different needs -- and teens often want to enjoy thebenefits they get from slightly reducing privacy and widening their socialreach. Given these considerations augmenting default privacy features byallowing them to be toggled on and off will allow individual users to choosetheir own balance while still maintaining a privacy-focused norm.</p>
                <p>Last Updated: 2024-10-21 16:03:18 UTC</p>
                <button class="interpret-button" data-id="2410.16137v1">Interpret</button>
                <div id="interpretation-2410.16137v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Virtual Reality Games: Extending Unity Learn Games to VR</h3>
                <p>Authors: Ryan P. McMahanNayan N. ChawlaChristian S. CassellChristopher Peerapon Lee</p>
                <p><a href="http://arxiv.org/abs/2410.16061v1">Link to paper</a></p>
                <p>Research involving virtual reality VR has dramatically increased since theintroduction of consumer VR systems. In turn research on VR games has gainedpopularity within several fields. However most VR games are closed sourcewhich limits research opportunities. Some VR games are open source but most ofthem are either very basic or too complex to be easily used in research. Inthis paper we present two source-available VR games developed from freelyavailable Unity Learn games: a kart racing game and a 3D adventure game. Ourhope is that other researchers find them easy to use for VR studies as UnityTechnologies developed the games for beginners and has provided tutorials onusing them.</p>
                <p>Last Updated: 2024-10-21 14:39:44 UTC</p>
                <button class="interpret-button" data-id="2410.16061v1">Interpret</button>
                <div id="interpretation-2410.16061v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with Fine-tuning of Voice Activity Projection</h3>
                <p>Authors: Koji InoueDivesh LalaGabriel SkantzeTatsuya Kawahara</p>
                <p><a href="http://arxiv.org/abs/2410.15929v1">Link to paper</a></p>
                <p>In human conversations short backchannel utterances such as yeah and ohplay a crucial role in facilitating smooth and engaging dialogue. Thesebackchannels signal attentiveness and understanding without interrupting thespeaker making their accurate prediction essential for creating more naturalconversational agents. This paper proposes a novel method for real-timecontinuous backchannel prediction using a fine-tuned Voice Activity ProjectionVAP model. While existing approaches have relied on turn-based orartificially balanced datasets our approach predicts both the timing and typeof backchannels in a continuous and frame-wise manner on unbalanced real-worlddatasets. We first pre-train the VAP model on a general dialogue corpus tocapture conversational dynamics and then fine-tune it on a specialized datasetfocused on backchannel behavior. Experimental results demonstrate that ourmodel outperforms baseline methods in both timing and type prediction tasksachieving robust performance in real-time environments. This research offers apromising step toward more responsive and human-like dialogue systems withimplications for interactive spoken dialogue applications such as virtualassistants and robots.</p>
                <p>Last Updated: 2024-10-21 11:57:56 UTC</p>
                <button class="interpret-button" data-id="2410.15929v1">Interpret</button>
                <div id="interpretation-2410.15929v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in Communicative Multi-Agent Systems</h3>
                <p>Authors: Yihuan MaoYipeng KangPeilun LiNing ZhangWei XuChongjie Zhang</p>
                <p><a href="http://arxiv.org/abs/2410.16237v1">Link to paper</a></p>
                <p>As large language model LLM agents increasingly integrate into ourinfrastructure their robust coordination and message synchronization becomevital. The Byzantine Generals Problem BGP is a critical model forconstructing resilient multi-agent systems MAS under adversarial attacks. Itdescribes a scenario where malicious agents with unknown identities exist inthe system-situations that in our context could result from LLM agentshallucinations or external attacks. In BGP the objective of the entire systemis to reach a consensus on the action to be taken. Traditional BGP requiresglobal consensus among all agents however in practical scenarios globalconsensus is not always necessary and can even be inefficient. Therefore thereis a pressing need to explore a refined version of BGP that aligns with thelocal coordination patterns observed in MAS. We refer to this refined versionas Imperfect BGP IBGP in our research aiming to address this discrepancy. Totackle this issue we propose a framework that leverages consensus protocolswithin general MAS settings providing provable resilience againstcommunication attacks and adaptability to changing environments as validatedby empirical results. Additionally we present a case study in a sensor networkenvironment to illustrate the practical application of our protocol.</p>
                <p>Last Updated: 2024-10-21 17:41:42 UTC</p>
                <button class="interpret-button" data-id="2410.16237v1">Interpret</button>
                <div id="interpretation-2410.16237v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>LASER: Script Execution by Autonomous Agents for On-demand Traffic Simulation</h3>
                <p>Authors: Hao GaoJingyue WangWenyang FangJingwei XuYunpeng HuangTaolue ChenXiaoxing Ma</p>
                <p><a href="http://arxiv.org/abs/2410.16197v2">Link to paper</a></p>
                <p>Autonomous Driving Systems ADS require diverse and safety-critical trafficscenarios for effective training and testing but the existing data generationmethods struggle to provide flexibility and scalability. We propose LASER anovel frame-work that leverage large language models LLMs to conduct trafficsimulations based on natural language inputs. The framework operates in twostages: it first generates scripts from user-provided descriptions and thenexecutes them using autonomous agents in real time. Validated in the CARLAsimulator LASER successfully generates complex on-demand driving scenariossignificantly improving ADS training and testing data generation.</p>
                <p>Last Updated: 2024-10-22 07:14:11 UTC</p>
                <button class="interpret-button" data-id="2410.16197v2">Interpret</button>
                <div id="interpretation-2410.16197v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Spiking Neural Networks as a Controller for Emergent Swarm Agents</h3>
                <p>Authors: Kevin ZhuConnor MattsonShay SnyderRicardo VegaDaniel S. BrownMaryam ParsaCameron Nowzari</p>
                <p><a href="http://arxiv.org/abs/2410.16175v1">Link to paper</a></p>
                <p>Drones which can swarm and loiter in a certain area cost hundreds of dollarsbut mosquitos can do the same and are essentially worthless. To control swarmsof low-cost robots researchers may end up spending countless hoursbrainstorming robot configurations and policies to organically createbehaviors which do not need expensive sensors and perception. Existing researchexplores the possible emergent behaviors in swarms of robots with only a binarysensor and a simple but hand-picked controller structure. Even agents in thishighly limited sensing actuation and computational capability class canexhibit relatively complex global behaviors such as aggregation milling anddispersal but finding the local interaction rules that enable more collectivebehaviors remains a significant challenge. This paper investigates thefeasibility of training spiking neural networks to find those local interactionrules that result in particular emergent behaviors. In this paper we focus onsimulating a specific milling behavior already known to be producible usingvery simple binary sensing and acting agents. To do this we use evolutionaryalgorithms to evolve not only the parameters the weights biases and delaysof a spiking neural network but also its structure. To create a baseline wealso show an evolutionary search strategy over the parameters for the incumbenthand-picked binary controller structure. Our simulations show that spikingneural networks can be evolved in binary sensing agents to form a mill.</p>
                <p>Last Updated: 2024-10-21 16:41:35 UTC</p>
                <button class="interpret-button" data-id="2410.16175v1">Interpret</button>
                <div id="interpretation-2410.16175v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Natural GaLore: Accelerating GaLore for memory-efficient LLM Training and Fine-tuning</h3>
                <p>Authors: Arijit Das</p>
                <p><a href="http://arxiv.org/abs/2410.16029v1">Link to paper</a></p>
                <p>Training LLMs presents significant memory challenges due to growing size ofdata weights and optimizer states. Techniques such as data and modelparallelism gradient checkpointing and offloading strategies address thisissue but are often infeasible due to hardware constraints. To mitigate memoryusage alternative methods like Parameter-Efficient-Fine-Tuning PEFT andGaLore approximate weights or optimizer states. PEFT methods such as LoRAhave gained popularity for fine-tuning LLMs though they require a full-rankwarm start. In contrast GaLore allows full-parameter learning while being morememory-efficient. This work introduces Natural GaLore a simple drop inreplacement for AdamW which efficiently applies the inverse Empirical FisherInformation Matrix to low-rank gradients using Woodburys Identity. Wedemonstrate that incorporating second-order information speeds up optimizationsignificantly especially when the iteration budget is limited. Empiricalpretraining on 60M 130M 350M and 1.1B parameter Llama models on C4 datademonstrate significantly lower perplexity over GaLore without additionalmemory overhead. By fine-tuning RoBERTa on the GLUE benchmark using NaturalGaLore we demonstrate significant reduction in gap 86.05 vs 86.28 forfull-finetuning. Furthermore fine-tuning the TinyLlama 1.1B model for functioncalling using the TinyAgent framework shows that Natural GaLore achieving83.09 accuracy on the TinyAgent dataset significantly outperforms 16-bit LoRAat 80.06 and even surpasses GPT4-Turbo by 4 all while using 30 less memory.  All code to reproduce the results are available at:https://github.com/selfsupervised-ai/Natural-GaLore.git</p>
                <p>Last Updated: 2024-10-21 14:05:06 UTC</p>
                <button class="interpret-button" data-id="2410.16029v1">Interpret</button>
                <div id="interpretation-2410.16029v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Analyzing Closed-loop Training Techniques for Realistic Traffic Agent Models in Autonomous Highway Driving Simulations</h3>
                <p>Authors: Matthias BitzerReinis CimursBenjamin CoorsJohannes GothSebastian ZieschePhilipp GeigerMaximilian Naumann</p>
                <p><a href="http://arxiv.org/abs/2410.15987v1">Link to paper</a></p>
                <p>Simulation plays a crucial role in the rapid development and safe deploymentof autonomous vehicles. Realistic traffic agent models are indispensable forbridging the gap between simulation and the real world. Many existingapproaches for imitating human behavior are based on learning fromdemonstration. However these approaches are often constrained by focusing onindividual training strategies. Therefore to foster a broader understanding ofrealistic traffic agent modeling in this paper we provide an extensivecomparative analysis of different training principles with a focus onclosed-loop methods for highway driving simulation. We experimentally comparei open-loop vs. closed-loop multi-agent training ii adversarial vs.deterministic supervised training iii the impact of reinforcement lossesand iv the impact of training alongside log-replayed agents to identifysuitable training techniques for realistic agent modeling. Furthermore weidentify promising combinations of different closed-loop training methods.</p>
                <p>Last Updated: 2024-10-21 13:16:58 UTC</p>
                <button class="interpret-button" data-id="2410.15987v1">Interpret</button>
                <div id="interpretation-2410.15987v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Implicit Regularization for Tubal Tensor Factorizations via Gradient Descent</h3>
                <p>Authors: Santhosh KarnikAnna VeselovskaMark IwenFelix Krahmer</p>
                <p><a href="http://arxiv.org/abs/2410.16247v1">Link to paper</a></p>
                <p>We provide a rigorous analysis of implicit regularization in anoverparametrized tensor factorization problem beyond the lazy training regime.For matrix factorization problems this phenomenon has been studied in a numberof works. A particular challenge has been to design universal initializationstrategies which provably lead to implicit regularization in gradient-descentmethods. At the same time it has been argued by Cohen et. al. 2016 that moregeneral classes of neural networks can be captured by considering tensorfactorizations. However in the tensor case implicit regularization has onlybeen rigorously established for gradient flow or in the lazy training regime.In this paper we prove the first tensor result of its kind for gradientdescent rather than gradient flow. We focus on the tubal tensor product and theassociated notion of low tubal rank encouraged by the relevance of this modelfor image data. We establish that gradient descent in an overparametrizedtensor factorization model with a small random initialization exhibits animplicit bias towards solutions of low tubal rank. Our theoretical findings areillustrated in an extensive set of numerical simulations show-casing thedynamics predicted by our theory as well as the crucial role of using a smallrandom initialization.</p>
                <p>Last Updated: 2024-10-21 17:52:01 UTC</p>
                <button class="interpret-button" data-id="2410.16247v1">Interpret</button>
                <div id="interpretation-2410.16247v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Asymmetries in Financial Spillovers</h3>
                <p>Authors: Florian HuberKarin KlieberMassimiliano MarcellinoLuca OnoranteMichael Pfarrhofer</p>
                <p><a href="http://arxiv.org/abs/2410.16214v1">Link to paper</a></p>
                <p>This paper analyzes nonlinearities in the international transmission offinancial shocks originating in the US. To do so we develop a flexiblenonlinear multi-country model. Our framework is capable of producingasymmetries in the responses to financial shocks for shock size and sign andover time. We show that international reactions to US-based financial shocksare asymmetric along these dimensions. Particularly we find that adverseshocks trigger stronger declines in output inflation and stock markets thanbenign shocks. Further we investigate time variation in the estimated dynamiceffects and characterize the responsiveness of three major central banks tofinancial shocks.</p>
                <p>Last Updated: 2024-10-21 17:14:58 UTC</p>
                <button class="interpret-button" data-id="2410.16214v1">Interpret</button>
                <div id="interpretation-2410.16214v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Theoretical Limitations of Ensembles in the Age of Overparameterization</h3>
                <p>Authors: Niclas DernJohn P. CunninghamGeoff Pleiss</p>
                <p><a href="http://arxiv.org/abs/2410.16201v1">Link to paper</a></p>
                <p>Classic tree-based ensembles generalize better than any single decision tree.In contrast recent empirical studies find that modern ensembles ofoverparameterized neural networks may not provide any inherent generalizationadvantage over single but larger neural networks. This paper clarifies howmodern overparameterized ensembles differ from their classic underparameterizedcounterparts using ensembles of random feature RF regressors as a basis fordeveloping theory. In contrast to the underparameterized regime whereensembling typically induces regularization and increases generalization weprove that infinite ensembles of overparameterized RF regressors becomepointwise equivalent to single infinite-width RF regressors. Thisequivalence which is exact for ridgeless models and approximate for smallridge penalties implies that overparameterized ensembles and single largemodels exhibit nearly identical generalization. As a consequence we cancharacterize the predictive variance amongst ensemble members and demonstratethat it quantifies the expected effects of increasing capacity rather thancapturing any conventional notion of uncertainty. Our results challenge commonassumptions about the advantages of ensembles in overparameterized settingsprompting a reconsideration of how well intuitions from underparameterizedensembles transfer to deep ensembles and the overparameterized regime.</p>
                <p>Last Updated: 2024-10-21 17:03:20 UTC</p>
                <button class="interpret-button" data-id="2410.16201v1">Interpret</button>
                <div id="interpretation-2410.16201v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Trust-Region Method for Graphical Stein Variational Inference</h3>
                <p>Authors: Liam PavlovicDavid M. Rosen</p>
                <p><a href="http://arxiv.org/abs/2410.16195v1">Link to paper</a></p>
                <p>Stein variational inference SVI is a sample-based approximate Bayesianinference technique that generates a sample set by jointly optimizing thesamples locations to minimize an information-theoretic measure of discrepancywith the target probability distribution. SVI thus provides a fast andsignificantly more sample-efficient approach to Bayesian inference thantraditional random-sampling-based alternatives. However the optimizationtechniques employed in existing SVI methods struggle to address problems inwhich the target distribution is high-dimensional poorly-conditioned ornon-convex which severely limits the range of their practical applicability.In this paper we propose a novel trust-region optimization approach for SVIthat successfully addresses each of these challenges. Our method builds uponprior work in SVI by leveraging conditional independences in the targetdistribution to achieve high-dimensional scaling and second-order informationto address poor conditioning while additionally providing an effectiveadaptive step control procedure which is essential for ensuring convergence onchallenging non-convex optimization problems. Experimental results show ourmethod achieves superior numerical performance both in convergence rate andsample accuracy and scales better in high-dimensional distributions thanprevious SVI techniques.</p>
                <p>Last Updated: 2024-10-21 16:59:01 UTC</p>
                <button class="interpret-button" data-id="2410.16195v1">Interpret</button>
                <div id="interpretation-2410.16195v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Theoretical Insights into Line Graph Transformation on Graph Learning</h3>
                <p>Authors: Fan YangXingyue Huang</p>
                <p><a href="http://arxiv.org/abs/2410.16138v1">Link to paper</a></p>
                <p>Line graph transformation has been widely studied in graph theory where eachnode in a line graph corresponds to an edge in the original graph. This hasinspired a series of graph neural networks GNNs applied to transformed linegraphs which have proven effective in various graph representation learningtasks. However there is limited theoretical study on how line graphtransformation affects the expressivity of GNN models. In this study we focuson two types of graphs known to be challenging to the Weisfeiler-Leman WLtests: Cai-Furer-Immerman CFI graphs and strongly regular graphs and showthat applying line graph transformation helps exclude these challenging graphproperties thus potentially assist WL tests in distinguishing these graphs. Weempirically validate our findings by conducting a series of experiments thatcompare the accuracy and efficiency of graph isomorphism tests and GNNs on bothline-transformed and original graphs across these graph structure types.</p>
                <p>Last Updated: 2024-10-21 16:04:50 UTC</p>
                <button class="interpret-button" data-id="2410.16138v1">Interpret</button>
                <div id="interpretation-2410.16138v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors</h3>
                <p>Authors: Chin-Yang LinChung-Ho WuChang-Han YehShih-Han YenCheng SunYu-Lun Liu</p>
                <p><a href="http://arxiv.org/abs/2410.16271v1">Link to paper</a></p>
                <p>Neural Radiance Fields NeRF face significant challenges in few-shotscenarios primarily due to overfitting and long training times forhigh-fidelity rendering. Existing methods such as FreeNeRF and SparseNeRF usefrequency regularization or pre-trained priors but struggle with complexscheduling and bias. We introduce FrugalNeRF a novel few-shot NeRF frameworkthat leverages weight-sharing voxels across multiple scales to efficientlyrepresent scene details. Our key contribution is a cross-scale geometricadaptation scheme that selects pseudo ground truth depth based on reprojectionerrors across scales. This guides training without relying on externallylearned priors enabling full utilization of the training data. It can alsointegrate pre-trained priors enhancing quality without slowing convergence.Experiments on LLFF DTU and RealEstate-10K show that FrugalNeRF outperformsother few-shot NeRF methods while significantly reducing training time makingit a practical solution for efficient and accurate 3D scene reconstruction.</p>
                <p>Last Updated: 2024-10-21 17:59:53 UTC</p>
                <button class="interpret-button" data-id="2410.16271v1">Interpret</button>
                <div id="interpretation-2410.16271v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MvDrag3D: Drag-based Creative 3D Editing via Multi-view Generation-Reconstruction Priors</h3>
                <p>Authors: Honghua ChenYushi LanYongwei ChenYifan ZhouXingang Pan</p>
                <p><a href="http://arxiv.org/abs/2410.16272v1">Link to paper</a></p>
                <p>Drag-based editing has become popular in 2D content creation driven by thecapabilities of image generative models. However extending this technique to3D remains a challenge. Existing 3D drag-based editing methods whetheremploying explicit spatial transformations or relying on implicit latentoptimization within limited-capacity 3D generative models fall short inhandling significant topology changes or generating new textures across diverseobject categories. To overcome these limitations we introduce MVDrag3D anovel framework for more flexible and creative drag-based 3D editing thatleverages multi-view generation and reconstruction priors. At the core of ourapproach is the usage of a multi-view diffusion model as a strong generativeprior to perform consistent drag editing over multiple rendered views which isfollowed by a reconstruction model that reconstructs 3D Gaussians of the editedobject. While the initial 3D Gaussians may suffer from misalignment betweendifferent views we address this via view-specific deformation networks thatadjust the position of Gaussians to be well aligned. In addition we propose amulti-view score function that distills generative priors from multiple viewsto further enhance the view consistency and visual quality. Extensiveexperiments demonstrate that MVDrag3D provides a precise generative andflexible solution for 3D drag-based editing supporting more versatile editingeffects across various object categories and 3D representations.</p>
                <p>Last Updated: 2024-10-21 17:59:53 UTC</p>
                <button class="interpret-button" data-id="2410.16272v1">Interpret</button>
                <div id="interpretation-2410.16272v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree</h3>
                <p>Authors: Shuangrui DingRui QianXiaoyi DongPan ZhangYuhang ZangYuhang CaoYuwei GuoDahua LinJiaqi Wang</p>
                <p><a href="http://arxiv.org/abs/2410.16268v1">Link to paper</a></p>
                <p>The Segment Anything Model 2 SAM 2 has emerged as a powerful foundationmodel for object segmentation in both images and videos paving the way forvarious downstream video applications. The crucial design of SAM 2 for videosegmentation is its memory module which prompts object-aware memories fromprevious frames for current frame prediction. However its greedy-selectionmemory design suffers from the error accumulation problem where an erroredor missed mask will cascade and influence the segmentation of the subsequentframes which limits the performance of SAM 2 toward complex long-term videos.To this end we introduce SAM2Long an improved training-free video objectsegmentation strategy which considers the segmentation uncertainty within eachframe and chooses the video-level optimal results from multiple segmentationpathways in a constrained tree search manner. In practice we maintain a fixednumber of segmentation pathways throughout the video. For each frame multiplemasks are proposed based on the existing pathways creating various candidatebranches. We then select the same fixed number of branches with highercumulative scores as the new pathways for the next frame. After processing thefinal frame the pathway with the highest cumulative score is chosen as thefinal segmentation result. Benefiting from its heuristic search designSAM2Long is robust toward occlusions and object reappearances and caneffectively segment and track objects for complex long-term videos. NotablySAM2Long achieves an average improvement of 3.0 points across all 24head-to-head comparisons with gains of up to 5.3 points in JF on long-termvideo object segmentation benchmarks such as SA-V and LVOS. The code isreleased at https://github.com/Mark12Ding/SAM2Long.</p>
                <p>Last Updated: 2024-10-21 17:59:19 UTC</p>
                <button class="interpret-button" data-id="2410.16268v1">Interpret</button>
                <div id="interpretation-2410.16268v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs</h3>
                <p>Authors: Michael S. RyooHonglu ZhouShrikant KendreCan QinLe XueManli ShuSilvio SavareseRan XuCaiming XiongJuan Carlos Niebles</p>
                <p><a href="http://arxiv.org/abs/2410.16267v1">Link to paper</a></p>
                <p>We present xGen-MM-Vid BLIP-3-Video: a multimodal language model forvideos particularly designed to efficiently capture temporal information overmultiple frames. BLIP-3-Video takes advantage of the temporal encoder inaddition to the conventional visual tokenizer which maps a sequence of tokensover multiple frames into a compact set of visual tokens. This enablesBLIP3-Video to use much fewer visual tokens than its competing models e.g. 32vs. 4608 tokens. We explore different types of temporal encoders includinglearnable spatio-temporal pooling as well as sequential models like TokenTuring Machines. We experimentally confirm that BLIP-3-Video obtains videoquestion-answering accuracies comparable to much larger state-of-the-art modelse.g. 34B while being much smaller i.e. 4B and more efficient by usingfewer visual tokens. The project website is athttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html</p>
                <p>Last Updated: 2024-10-21 17:59:11 UTC</p>
                <button class="interpret-button" data-id="2410.16267v1">Interpret</button>
                <div id="interpretation-2410.16267v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors</h3>
                <p>Authors: Xi LiuChaoyi ZhouSiyu Huang</p>
                <p><a href="http://arxiv.org/abs/2410.16266v1">Link to paper</a></p>
                <p>Novel-view synthesis aims to generate novel views of a scene from multipleinput images or videos and recent advancements like 3D Gaussian splatting3DGS have achieved notable success in producing photorealistic renderingswith efficient pipelines. However generating high-quality novel views underchallenging settings such as sparse input views remains difficult due toinsufficient information in under-sampled areas often resulting in noticeableartifacts. This paper presents 3DGS-Enhancer a novel pipeline for enhancingthe representation quality of 3DGS representations. We leverage 2D videodiffusion priors to address the challenging 3D view consistency problemreformulating it as achieving temporal consistency within a video generationprocess. 3DGS-Enhancer restores view-consistent latent features of renderednovel views and integrates them with the input views through a spatial-temporaldecoder. The enhanced views are then used to fine-tune the initial 3DGS modelsignificantly improving its rendering performance. Extensive experiments onlarge-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yieldssuperior reconstruction performance and high-fidelity rendering resultscompared to state-of-the-art methods. The project webpage ishttps://xiliu8006.github.io/3DGS-Enhancer-project .</p>
                <p>Last Updated: 2024-10-21 17:59:09 UTC</p>
                <button class="interpret-button" data-id="2410.16266v1">Interpret</button>
                <div id="interpretation-2410.16266v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Reflection-Bench: probing AI intelligence with reflection</h3>
                <p>Authors: Lingyu LiYixu WangHaiquan ZhaoShuqi KongYan TengChunbo LiYingchun Wang</p>
                <p><a href="http://arxiv.org/abs/2410.16270v1">Link to paper</a></p>
                <p>The ability to adapt beliefs or behaviors in response to unexpected outcomesreflection is fundamental to intelligent systems interaction with the world.From a cognitive science perspective this serves as a core principle ofintelligence applicable to both human and AI systems. To address the debate onthe intelligence of large language models LLMs we propose Reflection-Bencha comprehensive benchmark comprising 7 tasks spanning core cognitive functionscrucial for reflection including perception memory belief updatingdecision-making prediction counterfactual thinking and meta-reflection. Weevaluate the performances of 13 prominent LLMs such as OpenAI o1 GPT-4 Claude3.5 Sonnet etc. The results indicate that current LLMs still lack satisfactoryreflection ability. We discuss the underlying causes of these results andsuggest potential avenues for future research. In conclusion Reflection-Benchoffers both evaluation tools and inspiration for developing AI capable ofreliably interacting with the environment. Our data and code are available athttps://github.com/YabYum/ReflectionBench.</p>
                <p>Last Updated: 2024-10-21 17:59:50 UTC</p>
                <button class="interpret-button" data-id="2410.16270v1">Interpret</button>
                <div id="interpretation-2410.16270v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs</h3>
                <p>Authors: Michael S. RyooHonglu ZhouShrikant KendreCan QinLe XueManli ShuSilvio SavareseRan XuCaiming XiongJuan Carlos Niebles</p>
                <p><a href="http://arxiv.org/abs/2410.16267v1">Link to paper</a></p>
                <p>We present xGen-MM-Vid BLIP-3-Video: a multimodal language model forvideos particularly designed to efficiently capture temporal information overmultiple frames. BLIP-3-Video takes advantage of the temporal encoder inaddition to the conventional visual tokenizer which maps a sequence of tokensover multiple frames into a compact set of visual tokens. This enablesBLIP3-Video to use much fewer visual tokens than its competing models e.g. 32vs. 4608 tokens. We explore different types of temporal encoders includinglearnable spatio-temporal pooling as well as sequential models like TokenTuring Machines. We experimentally confirm that BLIP-3-Video obtains videoquestion-answering accuracies comparable to much larger state-of-the-art modelse.g. 34B while being much smaller i.e. 4B and more efficient by usingfewer visual tokens. The project website is athttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html</p>
                <p>Last Updated: 2024-10-21 17:59:11 UTC</p>
                <button class="interpret-button" data-id="2410.16267v1">Interpret</button>
                <div id="interpretation-2410.16267v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors</h3>
                <p>Authors: Xi LiuChaoyi ZhouSiyu Huang</p>
                <p><a href="http://arxiv.org/abs/2410.16266v1">Link to paper</a></p>
                <p>Novel-view synthesis aims to generate novel views of a scene from multipleinput images or videos and recent advancements like 3D Gaussian splatting3DGS have achieved notable success in producing photorealistic renderingswith efficient pipelines. However generating high-quality novel views underchallenging settings such as sparse input views remains difficult due toinsufficient information in under-sampled areas often resulting in noticeableartifacts. This paper presents 3DGS-Enhancer a novel pipeline for enhancingthe representation quality of 3DGS representations. We leverage 2D videodiffusion priors to address the challenging 3D view consistency problemreformulating it as achieving temporal consistency within a video generationprocess. 3DGS-Enhancer restores view-consistent latent features of renderednovel views and integrates them with the input views through a spatial-temporaldecoder. The enhanced views are then used to fine-tune the initial 3DGS modelsignificantly improving its rendering performance. Extensive experiments onlarge-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yieldssuperior reconstruction performance and high-fidelity rendering resultscompared to state-of-the-art methods. The project webpage ishttps://xiliu8006.github.io/3DGS-Enhancer-project .</p>
                <p>Last Updated: 2024-10-21 17:59:09 UTC</p>
                <button class="interpret-button" data-id="2410.16266v1">Interpret</button>
                <div id="interpretation-2410.16266v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution</h3>
                <p>Authors: Maosong CaoAlexander LamHaodong DuanHongwei LiuSongyang ZhangKai Chen</p>
                <p><a href="http://arxiv.org/abs/2410.16256v1">Link to paper</a></p>
                <p>Efficient and accurate evaluation is crucial for the continuous improvementof large language models LLMs. Among various assessment methods subjectiveevaluation has garnered significant attention due to its superior alignmentwith real-world usage scenarios and human preferences. However human-basedevaluations are costly and lack reproducibility making precise automatedevaluators judgers vital in this process. In this report we introducetextbfCompassJudger-1 the first open-source textbfall-in-one judge LLM.CompassJudger-1 is a general-purpose LLM that demonstrates remarkableversatility. It is capable of: 1. Performing unitary scoring and two-modelcomparisons as a reward model 2. Conducting evaluations according to specifiedformats 3. Generating critiques 4. Executing diverse tasks like a generalLLM. To assess the evaluation capabilities of different judge models under aunified setting we have also established textbfJudgerBench a new benchmarkthat encompasses various subjective evaluation tasks and covers a wide range oftopics. CompassJudger-1 offers a comprehensive solution for various evaluationtasks while maintaining the flexibility to adapt to diverse requirements. BothCompassJudger and JudgerBench are released and available to the researchcommunity athttps://github.com/open-compass/CompassJudger. We believe that byopen-sourcing these tools we can foster collaboration and accelerate progressin LLM evaluation methodologies.</p>
                <p>Last Updated: 2024-10-21 17:56:51 UTC</p>
                <button class="interpret-button" data-id="2410.16256v1">Interpret</button>
                <div id="interpretation-2410.16256v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays, ECGs, and Diagnostic Report</h3>
                <p>Authors: Samrajya ThapaKoushik HowladerSubhankar BhattacharjeeWei le</p>
                <p><a href="http://arxiv.org/abs/2410.16239v1">Link to paper</a></p>
                <p>In this paper we introduce a novel Multi-Modal Contrastive Pre-trainingFramework that synergistically combines X-rays electrocardiograms ECGs andradiology/cardiology reports. Our approach leverages transformers to encodethese diverse modalities into a unified representation space aiming to enhancediagnostic accuracy and facilitate comprehensive patient assessments. Weutilize LoRA-Peft to significantly reduce trainable parameters in the LLM andincorporate recent linear attention dropping strategy in the VisionTransformerViT for smoother attention. Furthermore we provide novelmultimodal attention explanations and retrieval for our model. To the best ofour knowledge we are the first to propose an integrated model that combinesX-ray ECG and Radiology/Cardiology Report with this approach. By utilizingcontrastive loss MoRE effectively aligns modality-specific features into acoherent embedding which supports various downstream tasks such as zero-shotclassification and multimodal retrieval. Employing our proposed methodology weachieve state-of-the-art SOTA on the Mimic-IV CheXpert Edema Severity andPtbXl downstream datasets surpassing existing multimodal approaches. Ourproposed framework shows significant improvements in capturing intricateinter-modal relationships and its robustness in medical diagnosis thatestablishes a framework for future research in multimodal learning in thehealthcare sector.</p>
                <p>Last Updated: 2024-10-21 17:42:41 UTC</p>
                <button class="interpret-button" data-id="2410.16239v1">Interpret</button>
                <div id="interpretation-2410.16239v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs</h3>
                <p>Authors: Michael S. RyooHonglu ZhouShrikant KendreCan QinLe XueManli ShuSilvio SavareseRan XuCaiming XiongJuan Carlos Niebles</p>
                <p><a href="http://arxiv.org/abs/2410.16267v1">Link to paper</a></p>
                <p>We present xGen-MM-Vid BLIP-3-Video: a multimodal language model forvideos particularly designed to efficiently capture temporal information overmultiple frames. BLIP-3-Video takes advantage of the temporal encoder inaddition to the conventional visual tokenizer which maps a sequence of tokensover multiple frames into a compact set of visual tokens. This enablesBLIP3-Video to use much fewer visual tokens than its competing models e.g. 32vs. 4608 tokens. We explore different types of temporal encoders includinglearnable spatio-temporal pooling as well as sequential models like TokenTuring Machines. We experimentally confirm that BLIP-3-Video obtains videoquestion-answering accuracies comparable to much larger state-of-the-art modelse.g. 34B while being much smaller i.e. 4B and more efficient by usingfewer visual tokens. The project website is athttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html</p>
                <p>Last Updated: 2024-10-21 17:59:11 UTC</p>
                <button class="interpret-button" data-id="2410.16267v1">Interpret</button>
                <div id="interpretation-2410.16267v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution</h3>
                <p>Authors: Maosong CaoAlexander LamHaodong DuanHongwei LiuSongyang ZhangKai Chen</p>
                <p><a href="http://arxiv.org/abs/2410.16256v1">Link to paper</a></p>
                <p>Efficient and accurate evaluation is crucial for the continuous improvementof large language models LLMs. Among various assessment methods subjectiveevaluation has garnered significant attention due to its superior alignmentwith real-world usage scenarios and human preferences. However human-basedevaluations are costly and lack reproducibility making precise automatedevaluators judgers vital in this process. In this report we introducetextbfCompassJudger-1 the first open-source textbfall-in-one judge LLM.CompassJudger-1 is a general-purpose LLM that demonstrates remarkableversatility. It is capable of: 1. Performing unitary scoring and two-modelcomparisons as a reward model 2. Conducting evaluations according to specifiedformats 3. Generating critiques 4. Executing diverse tasks like a generalLLM. To assess the evaluation capabilities of different judge models under aunified setting we have also established textbfJudgerBench a new benchmarkthat encompasses various subjective evaluation tasks and covers a wide range oftopics. CompassJudger-1 offers a comprehensive solution for various evaluationtasks while maintaining the flexibility to adapt to diverse requirements. BothCompassJudger and JudgerBench are released and available to the researchcommunity athttps://github.com/open-compass/CompassJudger. We believe that byopen-sourcing these tools we can foster collaboration and accelerate progressin LLM evaluation methodologies.</p>
                <p>Last Updated: 2024-10-21 17:56:51 UTC</p>
                <button class="interpret-button" data-id="2410.16256v1">Interpret</button>
                <div id="interpretation-2410.16256v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Can Knowledge Editing Really Correct Hallucinations?</h3>
                <p>Authors: Baixiang HuangCanyu ChenXiongxiao XuAli PayaniKai Shu</p>
                <p><a href="http://arxiv.org/abs/2410.16251v1">Link to paper</a></p>
                <p>Large Language Models LLMs suffer from hallucinations referring to thenon-factual information in generated content despite their superior capacitiesacross tasks. Meanwhile knowledge editing has been developed as a new popularparadigm to correct the erroneous factual knowledge encoded in LLMs with theadvantage of avoiding retraining from scratch. However one common issue ofexisting evaluation datasets for knowledge editing is that they do not ensureLLMs actually generate hallucinated answers to the evaluation questions beforeediting. When LLMs are evaluated on such datasets after being edited bydifferent techniques it is hard to directly adopt the performance to assessthe effectiveness of different knowledge editing methods in correctinghallucinations. Thus the fundamental question remains insufficientlyvalidated: Can knowledge editing really correct hallucinations in LLMs Weproposed HalluEditBench to holistically benchmark knowledge editing methods incorrecting real-world hallucinations. First we rigorously construct a massivehallucination dataset with 9 domains 26 topics and more than 6000hallucinations. Then we assess the performance of knowledge editing methods ina holistic way on five dimensions including Efficacy GeneralizationPortability Locality and Robustness. Through HalluEditBench we have providednew insights into the potentials and limitations of different knowledge editingmethods in correcting hallucinations which could inspire future improvementsand facilitate the progress in the field of knowledge editing.</p>
                <p>Last Updated: 2024-10-21 17:55:54 UTC</p>
                <button class="interpret-button" data-id="2410.16251v1">Interpret</button>
                <div id="interpretation-2410.16251v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Analyzing Context Contributions in LLM-based Machine Translation</h3>
                <p>Authors: Emmanouil ZaranisNuno M. GuerreiroAndr F. T. Martins</p>
                <p><a href="http://arxiv.org/abs/2410.16246v1">Link to paper</a></p>
                <p>Large language models LLMs have achieved state-of-the-art performance inmachine translation MT and demonstrated the ability to leverage in-contextlearning through few-shot examples. However the mechanisms by which LLMs usedifferent parts of the input context remain largely unexplored. In this workwe provide a comprehensive analysis of context utilization in MT studying howLLMs use various context parts such as few-shot examples and the source textwhen generating translations. We highlight several key findings: 1 the sourcepart of few-shot examples appears to contribute more than its correspondingtargets irrespective of translation direction 2 finetuning LLMs withparallel data alters the contribution patterns of different context parts and3 there is a positional bias where earlier few-shot examples have highercontributions to the translated sequence. Finally we demonstrate thatinspecting anomalous context contributions can potentially uncover pathologicaltranslations such as hallucinations. Our findings shed light on the internalworkings of LLM-based MT which go beyond those known for standardencoder-decoder MT models.</p>
                <p>Last Updated: 2024-10-21 17:51:41 UTC</p>
                <button class="interpret-button" data-id="2410.16246v1">Interpret</button>
                <div id="interpretation-2410.16246v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>ToW: Thoughts of Words Improve Reasoning in Large Language Models</h3>
                <p>Authors: Zhikun XuMing ShenJacob DineenZhaonan LiXiao YeShijie LuAswin RRVChitta BaralBen Zhou</p>
                <p><a href="http://arxiv.org/abs/2410.16235v1">Link to paper</a></p>
                <p>We introduce thoughts of words ToW a novel training-time data-augmentationmethod for next-word prediction. ToW views next-word prediction as a corereasoning task and injects fine-grained thoughts explaining what the next wordshould be and how it is related to the previous contexts in pre-training texts.Our formulation addresses two fundamental drawbacks of existing next-wordprediction learning schemes: they induce factual hallucination and areinefficient for models to learn the implicit reasoning processes in raw texts.While there are many ways to acquire such thoughts of words we explore thefirst step of acquiring ToW annotations through distilling from larger models.After continual pre-training with only 70K ToW annotations we effectivelyimprove models reasoning performances by 7 to 9 on average and reduce modelhallucination by up to 10. At the same time ToW is entirely agnostic to tasksand applications introducing no additional biases on labels or semantics.</p>
                <p>Last Updated: 2024-10-21 17:41:11 UTC</p>
                <button class="interpret-button" data-id="2410.16235v1">Interpret</button>
                <div id="interpretation-2410.16235v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs</h3>
                <p>Authors: Michael S. RyooHonglu ZhouShrikant KendreCan QinLe XueManli ShuSilvio SavareseRan XuCaiming XiongJuan Carlos Niebles</p>
                <p><a href="http://arxiv.org/abs/2410.16267v1">Link to paper</a></p>
                <p>We present xGen-MM-Vid BLIP-3-Video: a multimodal language model forvideos particularly designed to efficiently capture temporal information overmultiple frames. BLIP-3-Video takes advantage of the temporal encoder inaddition to the conventional visual tokenizer which maps a sequence of tokensover multiple frames into a compact set of visual tokens. This enablesBLIP3-Video to use much fewer visual tokens than its competing models e.g. 32vs. 4608 tokens. We explore different types of temporal encoders includinglearnable spatio-temporal pooling as well as sequential models like TokenTuring Machines. We experimentally confirm that BLIP-3-Video obtains videoquestion-answering accuracies comparable to much larger state-of-the-art modelse.g. 34B while being much smaller i.e. 4B and more efficient by usingfewer visual tokens. The project website is athttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html</p>
                <p>Last Updated: 2024-10-21 17:59:11 UTC</p>
                <button class="interpret-button" data-id="2410.16267v1">Interpret</button>
                <div id="interpretation-2410.16267v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Revisiting Deep Feature Reconstruction for Logical and Structural Industrial Anomaly Detection</h3>
                <p>Authors: Sukanya PatraSouhaib Ben Taieb</p>
                <p><a href="http://arxiv.org/abs/2410.16255v1">Link to paper</a></p>
                <p>Industrial anomaly detection is crucial for quality control and predictivemaintenance but it presents challenges due to limited training data diverseanomaly types and external factors that alter object appearances. Existingmethods commonly detect structural anomalies such as dents and scratches byleveraging multi-scale features from image patches extracted through deeppre-trained networks. However significant memory and computational demandsoften limit their practical application. Additionally detecting logicalanomalies-such as images with missing or excess elements-requires anunderstanding of spatial relationships that traditional patch-based methodsfail to capture. In this work we address these limitations by focusing on DeepFeature Reconstruction DFR a memory- and compute-efficient approach fordetecting structural anomalies. We further enhance DFR into a unifiedframework called ULSAD which is capable of detecting both structural andlogical anomalies. Specifically we refine the DFR training objective toimprove performance in structural anomaly detection while introducing anattention-based loss mechanism using a global autoencoder-like network tohandle logical anomaly detection. Our empirical evaluation across fivebenchmark datasets demonstrates the performance of ULSAD in detecting andlocalizing both structural and logical anomalies outperforming eightstate-of-the-art methods. An extensive ablation study further highlights thecontribution of each component to the overall performance improvement. Our codeis available at https://github.com/sukanyapatra1997/ULSAD-2024.git</p>
                <p>Last Updated: 2024-10-21 17:56:47 UTC</p>
                <button class="interpret-button" data-id="2410.16255v1">Interpret</button>
                <div id="interpretation-2410.16255v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Distribution Learning with Valid Outputs Beyond the Worst-Case</h3>
                <p>Authors: Nick RittlerKamalika Chaudhuri</p>
                <p><a href="http://arxiv.org/abs/2410.16253v1">Link to paper</a></p>
                <p>Generative models at times produce invalid outputs such as images withgeneration artifacts and unnatural sounds. Validity-constrained distributionlearning attempts to address this problem by requiring that the learneddistribution have a provably small fraction of its mass in invalid parts ofspace -- something which standard loss minimization does not always ensure. Tothis end a learner in this model can guide the learning via validityqueries which allow it to ascertain the validity of individual examples.Prior work on this problem takes a worst-case stance showing that properlearning requires an exponential number of validity queries and demonstratingan improper algorithm which -- while generating guarantees in a wide-range ofsettings -- makes an atypical polynomial number of validity queries. In thiswork we take a first step towards characterizing regimes where guaranteeingvalidity is easier than in the worst-case. We show that when the datadistribution lies in the model class and the log-loss is minimized the numberof samples required to ensure validity has a weak dependence on the validityrequirement. Additionally we show that when the validity region belongs to aVC-class a limited number of validity queries are often sufficient.</p>
                <p>Last Updated: 2024-10-21 17:56:09 UTC</p>
                <button class="interpret-button" data-id="2410.16253v1">Interpret</button>
                <div id="interpretation-2410.16253v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Implicit Regularization for Tubal Tensor Factorizations via Gradient Descent</h3>
                <p>Authors: Santhosh KarnikAnna VeselovskaMark IwenFelix Krahmer</p>
                <p><a href="http://arxiv.org/abs/2410.16247v1">Link to paper</a></p>
                <p>We provide a rigorous analysis of implicit regularization in anoverparametrized tensor factorization problem beyond the lazy training regime.For matrix factorization problems this phenomenon has been studied in a numberof works. A particular challenge has been to design universal initializationstrategies which provably lead to implicit regularization in gradient-descentmethods. At the same time it has been argued by Cohen et. al. 2016 that moregeneral classes of neural networks can be captured by considering tensorfactorizations. However in the tensor case implicit regularization has onlybeen rigorously established for gradient flow or in the lazy training regime.In this paper we prove the first tensor result of its kind for gradientdescent rather than gradient flow. We focus on the tubal tensor product and theassociated notion of low tubal rank encouraged by the relevance of this modelfor image data. We establish that gradient descent in an overparametrizedtensor factorization model with a small random initialization exhibits animplicit bias towards solutions of low tubal rank. Our theoretical findings areillustrated in an extensive set of numerical simulations show-casing thedynamics predicted by our theory as well as the crucial role of using a smallrandom initialization.</p>
                <p>Last Updated: 2024-10-21 17:52:01 UTC</p>
                <button class="interpret-button" data-id="2410.16247v1">Interpret</button>
                <div id="interpretation-2410.16247v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays, ECGs, and Diagnostic Report</h3>
                <p>Authors: Samrajya ThapaKoushik HowladerSubhankar BhattacharjeeWei le</p>
                <p><a href="http://arxiv.org/abs/2410.16239v1">Link to paper</a></p>
                <p>In this paper we introduce a novel Multi-Modal Contrastive Pre-trainingFramework that synergistically combines X-rays electrocardiograms ECGs andradiology/cardiology reports. Our approach leverages transformers to encodethese diverse modalities into a unified representation space aiming to enhancediagnostic accuracy and facilitate comprehensive patient assessments. Weutilize LoRA-Peft to significantly reduce trainable parameters in the LLM andincorporate recent linear attention dropping strategy in the VisionTransformerViT for smoother attention. Furthermore we provide novelmultimodal attention explanations and retrieval for our model. To the best ofour knowledge we are the first to propose an integrated model that combinesX-ray ECG and Radiology/Cardiology Report with this approach. By utilizingcontrastive loss MoRE effectively aligns modality-specific features into acoherent embedding which supports various downstream tasks such as zero-shotclassification and multimodal retrieval. Employing our proposed methodology weachieve state-of-the-art SOTA on the Mimic-IV CheXpert Edema Severity andPtbXl downstream datasets surpassing existing multimodal approaches. Ourproposed framework shows significant improvements in capturing intricateinter-modal relationships and its robustness in medical diagnosis thatestablishes a framework for future research in multimodal learning in thehealthcare sector.</p>
                <p>Last Updated: 2024-10-21 17:42:41 UTC</p>
                <button class="interpret-button" data-id="2410.16239v1">Interpret</button>
                <div id="interpretation-2410.16239v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-10-23</p>
        </div>
    
        </div>
    </body>
    </html>
    