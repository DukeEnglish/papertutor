
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks</h3>
                <p>Authors: Murtaza DalalTarun ChiruvoluDevendra ChaplotRuslan Salakhutdinov</p>
                <p><a href="http://arxiv.org/abs/2405.01534v1">Link to paper</a></p>
                <p>Large Language Models LLMs have been shown to be capable of performinghigh-level planning for long-horizon robotics tasks yet existing methodsrequire access to a pre-defined skill library e.g. picking placing pullingpushing navigating. However LLM planning does not address how to design orlearn those behaviors which remains challenging particularly in long-horizonsettings. Furthermore for many tasks of interest the robot needs to be ableto adjust its behavior in a fine-grained manner requiring the agent to becapable of modifying low-level control actions. Can we instead use theinternet-scale knowledge from LLMs for high-level policies guidingreinforcement learning RL policies to efficiently solve robotic control tasksonline without requiring a pre-determined set of skills In this paper wepropose Plan-Seq-Learn PSL: a modular approach that uses motion planning tobridge the gap between abstract language and learned low-level control forsolving long-horizon robotics tasks from scratch. We demonstrate that PSLachieves state-of-the-art results on over 25 challenging robotics tasks with upto 10 stages. PSL solves long-horizon tasks from raw visual input spanning fourbenchmarks at success rates of over 85 out-performing language-basedclassical and end-to-end approaches. Video results and code athttps://mihdalal.github.io/planseqlearn/</p>
                <p>Last Updated: 2024-05-02 17:59:31 UTC</p>
                <button class="interpret-button" data-id="2405.01534v1">Interpret</button>
                <div id="interpretation-2405.01534v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models</h3>
                <p>Authors: Nishad SinghiJae Myung KimKarsten RothZeynep Akata</p>
                <p><a href="http://arxiv.org/abs/2405.01531v1">Link to paper</a></p>
                <p>Concept Bottleneck Models CBMs ground image classification onhuman-understandable concepts to allow for interpretable model decisions.Crucially the CBM design inherently allows for human interventions in whichexpert users are given the ability to modify potentially misaligned conceptchoices to influence the decision behavior of the model in an interpretablefashion. However existing approaches often require numerous humaninterventions per image to achieve strong performances posing practicalchallenges in scenarios where obtaining human feedback is expensive. In thispaper we find that this is noticeably driven by an independent treatment ofconcepts during intervention wherein a change of one concept does notinfluence the use of other ones in the models final decision. To address thisissue we introduce a trainable concept intervention realignment module whichleverages concept relations to realign concept assignments post-intervention.Across standard real-world benchmarks we find that concept realignment cansignificantly improve intervention efficacy significantly reducing the numberof interventions needed to reach a target classification performance or conceptprediction accuracy. In addition it easily integrates into existingconcept-based architectures without requiring changes to the models themselves.This reduced cost of human-model collaboration is crucial to enhancing thefeasibility of CBMs in resource-constrained environments.</p>
                <p>Last Updated: 2024-05-02 17:59:01 UTC</p>
                <button class="interpret-button" data-id="2405.01531v1">Interpret</button>
                <div id="interpretation-2405.01531v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>FLAME: Factuality-Aware Alignment for Large Language Models</h3>
                <p>Authors: Sheng-Chieh LinLuyu GaoBarlas OguzWenhan XiongJimmy LinWen-tau YihXilun Chen</p>
                <p><a href="http://arxiv.org/abs/2405.01525v1">Link to paper</a></p>
                <p>Alignment is a standard procedure to fine-tune pre-trained large languagemodels LLMs to follow natural language instructions and serve as helpful AIassistants. We have observed however that the conventional alignment processfails to enhance the factual accuracy of LLMs and often leads to thegeneration of more false facts i.e. hallucination. In this paper we studyhow to make the LLM alignment process more factual by first identifyingfactors that lead to hallucination in both alignment steps: supervisedfine-tuning SFT and reinforcement learning RL. In particular we find thattraining the LLM on new knowledge or unfamiliar texts can encouragehallucination. This makes SFT less factual as it trains on human labeled datathat may be novel to the LLM. Furthermore reward functions used in standard RLcan also encourage hallucination because it guides the LLM to provide morehelpful responses on a diverse set of instructions often preferring longer andmore detailed responses. Based on these observations we proposefactuality-aware alignment comprised of factuality-aware SFT andfactuality-aware RL through direct preference optimization. Experiments showthat our proposed factuality-aware alignment guides LLMs to output more factualresponses while maintaining instruction-following capability.</p>
                <p>Last Updated: 2024-05-02 17:54:54 UTC</p>
                <button class="interpret-button" data-id="2405.01525v1">Interpret</button>
                <div id="interpretation-2405.01525v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A separability-based approach to quantifying generalization: which layer is best?</h3>
                <p>Authors: Luciano DyballaEvan GerritzSteven W. Zucker</p>
                <p><a href="http://arxiv.org/abs/2405.01524v1">Link to paper</a></p>
                <p>Generalization to unseen data remains poorly understood for deep learningclassification and foundation models. How can one assess the ability ofnetworks to adapt to new or extended versions of their input space in thespirit of few-shot learning out-of-distribution generalization and domainadaptation Which layers of a network are likely to generalize best We providea new method for evaluating the capacity of networks to represent a sampleddomain regardless of whether the network has been trained on all classes inthe domain. Our approach is the following: after fine-tuning state-of-the-artpre-trained models for visual classification on a particular domain we assesstheir performance on data from related but distinct variations in that domain.Generalization power is quantified as a function of the latent embeddings ofunseen data from intermediate layers for both unsupervised and supervisedsettings. Working throughout all stages of the network we find that i highclassification accuracy does not imply high generalizability and ii deeperlayers in a model do not always generalize the best which has implications forpruning. Since the trends observed across datasets are largely consistent weconclude that our approach reveals a function of the intrinsic capacity ofthe different layers of a model to generalize.</p>
                <p>Last Updated: 2024-05-02 17:54:35 UTC</p>
                <button class="interpret-button" data-id="2405.01524v1">Interpret</button>
                <div id="interpretation-2405.01524v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Analyzing the Role of Semantic Representations in the Era of Large Language Models</h3>
                <p>Authors: Zhijing JinYuen ChenFernando GonzalezJiarui LiuJiayi ZhangJulian MichaelBernhard SchölkopfMona Diab</p>
                <p><a href="http://arxiv.org/abs/2405.01502v1">Link to paper</a></p>
                <p>Traditionally natural language processing NLP models often use a rich setof features created by linguistic expertise such as semantic representations.However in the era of large language models LLMs more and more tasks areturned into generic end-to-end sequence generation problems. In this paper weinvestigate the question: what is the role of semantic representations in theera of LLMs Specifically we investigate the effect of Abstract MeaningRepresentation AMR across five diverse NLP tasks. We propose an AMR-drivenchain-of-thought prompting method which we call AMRCoT and find that itgenerally hurts performance more than it helps. To investigate what AMR mayhave to offer on these tasks we conduct a series of analysis experiments. Wefind that it is difficult to predict which input examples AMR may help or hurton but errors tend to arise with multi-word expressions named entities andin the final inference step where the LLM must connect its reasoning over theAMR to its prediction. We recommend focusing on these areas for future work insemantic representations for LLMs. Our code:https://github.com/causalNLP/amr_llm.</p>
                <p>Last Updated: 2024-05-02 17:32:59 UTC</p>
                <button class="interpret-button" data-id="2405.01502v1">Interpret</button>
                <div id="interpretation-2405.01502v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Accelerating Convergence in Bayesian Few-Shot Classification</h3>
                <p>Authors: Tianjun KeHaoqun CaoFeng Zhou</p>
                <p><a href="http://arxiv.org/abs/2405.01507v1">Link to paper</a></p>
                <p>Bayesian few-shot classification has been a focal point in the field offew-shot learning. This paper seamlessly integrates mirror descent-basedvariational inference into Gaussian process-based few-shot classificationaddressing the challenge of non-conjugate inference. By leveragingnon-Euclidean geometry mirror descent achieves accelerated convergence byproviding the steepest descent direction along the corresponding manifold. Italso exhibits the parameterization invariance property concerning thevariational distribution. Experimental results demonstrate competitiveclassification accuracy improved uncertainty quantification and fasterconvergence compared to baseline models. Additionally we investigate theimpact of hyperparameters and components. Code is publicly available athttps://github.com/keanson/MD-BSFC.</p>
                <p>Last Updated: 2024-05-02 17:37:39 UTC</p>
                <button class="interpret-button" data-id="2405.01507v1">Interpret</button>
                <div id="interpretation-2405.01507v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Digital Twin Generators for Disease Modeling</h3>
                <p>Authors: Nameyeh AlamJake BasilicoDaniele BertoliniSatish Casie ChettyHeather D'AngeloRyan DouglasCharles K. FisherFranklin FullerMelissa GomesRishabh GuptaAlex LangAnton LoukianovRachel Mak-McCullyCary MurrayHanalei PhamSusanna QiaoElena Ryapolova-WebbAaron SmithDimitri TheoharatosAnil TolwaniEric W. TramelAnna VidovszkyJudy ViduyaJonathan R. Walsh</p>
                <p><a href="http://arxiv.org/abs/2405.01488v1">Link to paper</a></p>
                <p>A patients digital twin is a computational model that describes theevolution of their health over time. Digital twins have the potential torevolutionize medicine by enabling individual-level computer simulations ofhuman health which can be used to conduct more efficient clinical trials or torecommend personalized treatment options. Due to the overwhelming complexity ofhuman biology machine learning approaches that leverage large datasets ofhistorical patients longitudinal health records to generate patients digitaltwins are more tractable than potential mechanistic models. In this manuscriptwe describe a neural network architecture that can learn conditional generativemodels of clinical trajectories which we call Digital Twin Generators DTGsthat can create digital twins of individual patients. We show that the sameneural network architecture can be trained to generate accurate digital twinsfor patients across 13 different indications simply by changing the trainingset and tuning hyperparameters. By introducing a general purpose architecturewe aim to unlock the ability to scale machine learning approaches to largerdatasets and across more indications so that a digital twin could be createdfor any patient in the world.</p>
                <p>Last Updated: 2024-05-02 17:23:04 UTC</p>
                <button class="interpret-button" data-id="2405.01488v1">Interpret</button>
                <div id="interpretation-2405.01488v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Designing Algorithmic Recommendations to Achieve Human-AI Complementarity</h3>
                <p>Authors: Bryce McLaughlinJann Spiess</p>
                <p><a href="http://arxiv.org/abs/2405.01484v1">Link to paper</a></p>
                <p>Algorithms frequently assist rather than replace human decision-makers.However the design and analysis of algorithms often focus on predictingoutcomes and do not explicitly model their effect on human decisions. Thisdiscrepancy between the design and role of algorithmic assistants becomes ofparticular concern in light of empirical evidence that suggests thatalgorithmic assistants again and again fail to improve human decisions. In thisarticle we formalize the design of recommendation algorithms that assist humandecision-makers without making restrictive ex-ante assumptions about howrecommendations affect decisions. We formulate an algorithmic-design problemthat leverages the potential-outcomes framework from causal inference to modelthe effect of recommendations on a human decision-makers binary treatmentchoice. Within this model we introduce a monotonicity assumption that leads toan intuitive classification of human responses to the algorithm. Under thismonotonicity assumption we can express the humans response to algorithmicrecommendations in terms of their compliance with the algorithm and thedecision they would take if the algorithm sends no recommendation. We showcasethe utility of our framework using an online experiment that simulates a hiringtask. We argue that our approach explains the relative performance of differentrecommendation algorithms in the experiment and can help design solutions thatrealize human-AI complementarity.</p>
                <p>Last Updated: 2024-05-02 17:15:30 UTC</p>
                <button class="interpret-button" data-id="2405.01484v1">Interpret</button>
                <div id="interpretation-2405.01484v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>In-and-Out: Algorithmic Diffusion for Sampling Convex Bodies</h3>
                <p>Authors: Yunbum KookSantosh S. VempalaMatthew S. Zhang</p>
                <p><a href="http://arxiv.org/abs/2405.01425v1">Link to paper</a></p>
                <p>We present a new random walk for uniformly sampling high-dimensional convexbodies. It achieves state-of-the-art runtime complexity with strongerguarantees on the output than previously known namely in Renyi divergencewhich implies TV mathcalW_2 KL chi2. The proof departs from knownapproaches for polytime algorithms for the problem -- we utilize a stochasticdiffusion perspective to show contraction to the target distribution with therate of convergence determined by functional isoperimetric constants of thestationary density.</p>
                <p>Last Updated: 2024-05-02 16:15:46 UTC</p>
                <button class="interpret-button" data-id="2405.01425v1">Interpret</button>
                <div id="interpretation-2405.01425v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Random Pareto front surfaces</h3>
                <p>Authors: Ben TuNikolas KantasRobert M. LeeBehrang Shafei</p>
                <p><a href="http://arxiv.org/abs/2405.01404v1">Link to paper</a></p>
                <p>The Pareto front of a set of vectors is the subset which is comprised solelyof all of the best trade-off points. By interpolating this subset we obtainthe optimal trade-off surface. In this work we prove a very useful resultwhich states that all Pareto front surfaces can be explicitly parametrisedusing polar coordinates. In particular our polar parametrisation result tellsus that we can fully characterise any Pareto front surface using the lengthfunction which is a scalar-valued function that returns the projected lengthalong any positive radial direction. Consequently by exploiting thisrepresentation we show how it is possible to generalise many useful conceptsfrom linear algebra probability and statistics and decision theory tofunction over the space of Pareto front surfaces. Notably we focus ourattention on the stochastic setting where the Pareto front surface itself is astochastic process. Among other things we showcase how it is possible todefine and estimate many statistical quantities of interest such as theexpectation covariance and quantile of any Pareto front surface distribution.As a motivating example we investigate how these statistics can be used withina design of experiments setting where the goal is to both infer and use thePareto front surface distribution in order to make effective decisions. Besidesthis we also illustrate how these Pareto front ideas can be used within thecontext of extreme value theory. Finally as a numerical example we appliedsome of our new methodology on a real-world air pollution data set.</p>
                <p>Last Updated: 2024-05-02 15:54:46 UTC</p>
                <button class="interpret-button" data-id="2405.01404v1">Interpret</button>
                <div id="interpretation-2405.01404v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</h3>
                <p>Authors: Seungone KimJuyoung SukShayne LongpreBill Yuchen LinJamin ShinSean WelleckGraham NeubigMoontae LeeKyungjae LeeMinjoon Seo</p>
                <p><a href="http://arxiv.org/abs/2405.01535v1">Link to paper</a></p>
                <p>Proprietary LMs such as GPT-4 are often employed to assess the quality ofresponses from various LMs. However concerns including transparencycontrollability and affordability strongly motivate the development ofopen-source LMs specialized in evaluations. On the other hand existing openevaluator LMs exhibit critical shortcomings: 1 they issue scores thatsignificantly diverge from those assigned by humans and 2 they lack theflexibility to perform both direct assessment and pairwise ranking the twomost prevalent forms of assessment. Additionally they do not possess theability to evaluate based on custom evaluation criteria focusing instead ongeneral attributes like helpfulness and harmlessness. To address these issueswe introduce Prometheus 2 a more powerful evaluator LM than its predecessorthat closely mirrors human and GPT-4 judgements. Moreover it is capable ofprocessing both direct assessment and pair-wise ranking formats grouped with auser-defined evaluation criteria. On four direct assessment benchmarks and fourpairwise ranking benchmarks Prometheus 2 scores the highest correlation andagreement with humans and proprietary LM judges among all tested open evaluatorLMs. Our models code and data are all publicly available athttps://github.com/prometheus-eval/prometheus-eval.</p>
                <p>Last Updated: 2024-05-02 17:59:35 UTC</p>
                <button class="interpret-button" data-id="2405.01535v1">Interpret</button>
                <div id="interpretation-2405.01535v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>FLAME: Factuality-Aware Alignment for Large Language Models</h3>
                <p>Authors: Sheng-Chieh LinLuyu GaoBarlas OguzWenhan XiongJimmy LinWen-tau YihXilun Chen</p>
                <p><a href="http://arxiv.org/abs/2405.01525v1">Link to paper</a></p>
                <p>Alignment is a standard procedure to fine-tune pre-trained large languagemodels LLMs to follow natural language instructions and serve as helpful AIassistants. We have observed however that the conventional alignment processfails to enhance the factual accuracy of LLMs and often leads to thegeneration of more false facts i.e. hallucination. In this paper we studyhow to make the LLM alignment process more factual by first identifyingfactors that lead to hallucination in both alignment steps: supervisedfine-tuning SFT and reinforcement learning RL. In particular we find thattraining the LLM on new knowledge or unfamiliar texts can encouragehallucination. This makes SFT less factual as it trains on human labeled datathat may be novel to the LLM. Furthermore reward functions used in standard RLcan also encourage hallucination because it guides the LLM to provide morehelpful responses on a diverse set of instructions often preferring longer andmore detailed responses. Based on these observations we proposefactuality-aware alignment comprised of factuality-aware SFT andfactuality-aware RL through direct preference optimization. Experiments showthat our proposed factuality-aware alignment guides LLMs to output more factualresponses while maintaining instruction-following capability.</p>
                <p>Last Updated: 2024-05-02 17:54:54 UTC</p>
                <button class="interpret-button" data-id="2405.01525v1">Interpret</button>
                <div id="interpretation-2405.01525v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>D2PO: Discriminator-Guided DPO with Response Evaluation Models</h3>
                <p>Authors: Prasann SinghalNathan LambertScott NiekumTanya GoyalGreg Durrett</p>
                <p><a href="http://arxiv.org/abs/2405.01511v1">Link to paper</a></p>
                <p>Varied approaches for aligning language models have been proposed includingsupervised fine-tuning RLHF and direct optimization methods such as DPO.Although DPO has rapidly gained popularity due to its straightforward trainingprocess and competitive results there is an open question of whether thereremain practical advantages of using a discriminator like a reward model toevaluate responses. We propose D2PO discriminator-guided DPO an approach forthe online setting where preferences are being collected throughout learning.As we collect gold preferences we use these not only to train our policy butto train a discriminative response evaluation model to silver-label even moresynthetic data for policy training. We explore this approach across a set ofdiverse tasks including a realistic chat setting we find that our approachleads to higher-quality outputs compared to DPO with the same data budget andgreater efficiency in terms of preference data requirements. Furthermore weshow conditions under which silver labeling is most helpful: it is mosteffective when training the policy with DPO outperforming traditional PPO andbenefits from maintaining a separate discriminator from the policy model.</p>
                <p>Last Updated: 2024-05-02 17:44:41 UTC</p>
                <button class="interpret-button" data-id="2405.01511v1">Interpret</button>
                <div id="interpretation-2405.01511v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Analyzing the Role of Semantic Representations in the Era of Large Language Models</h3>
                <p>Authors: Zhijing JinYuen ChenFernando GonzalezJiarui LiuJiayi ZhangJulian MichaelBernhard SchölkopfMona Diab</p>
                <p><a href="http://arxiv.org/abs/2405.01502v1">Link to paper</a></p>
                <p>Traditionally natural language processing NLP models often use a rich setof features created by linguistic expertise such as semantic representations.However in the era of large language models LLMs more and more tasks areturned into generic end-to-end sequence generation problems. In this paper weinvestigate the question: what is the role of semantic representations in theera of LLMs Specifically we investigate the effect of Abstract MeaningRepresentation AMR across five diverse NLP tasks. We propose an AMR-drivenchain-of-thought prompting method which we call AMRCoT and find that itgenerally hurts performance more than it helps. To investigate what AMR mayhave to offer on these tasks we conduct a series of analysis experiments. Wefind that it is difficult to predict which input examples AMR may help or hurton but errors tend to arise with multi-word expressions named entities andin the final inference step where the LLM must connect its reasoning over theAMR to its prediction. We recommend focusing on these areas for future work insemantic representations for LLMs. Our code:https://github.com/causalNLP/amr_llm.</p>
                <p>Last Updated: 2024-05-02 17:32:59 UTC</p>
                <button class="interpret-button" data-id="2405.01502v1">Interpret</button>
                <div id="interpretation-2405.01502v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Controllable Text Generation in the Instruction-Tuning Era</h3>
                <p>Authors: Dhananjay AshokBarnabas Poczos</p>
                <p><a href="http://arxiv.org/abs/2405.01490v1">Link to paper</a></p>
                <p>While most research on controllable text generation has focused on steeringbase Language Models the emerging instruction-tuning and prompting paradigmoffers an alternate approach to controllability. We compile and releaseConGenBench a testbed of 17 different controllable generation tasks using asubset of it to benchmark the performance of 9 different baselines and methodson Instruction-tuned Language Models. To our surprise we find thatprompting-based approaches outperform controllable text generation methods onmost datasets and tasks highlighting a need for research on controllable textgeneration with Instruction-tuned Language Models in specific. Prompt-basedapproaches match human performance on most stylistic tasks while lagging onstructural tasks foregrounding a need to study more varied constraints andmore challenging stylistic tasks. To facilitate such research we provide analgorithm that uses only a task dataset and a Large Language Model within-context capabilities to automatically generate a constraint dataset. Thismethod eliminates the fields dependence on pre-curated constraint datasetshence vastly expanding the range of constraints that can be studied in thefuture.</p>
                <p>Last Updated: 2024-05-02 17:24:30 UTC</p>
                <button class="interpret-button" data-id="2405.01490v1">Interpret</button>
                <div id="interpretation-2405.01490v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>Multi-Space Alignments Towards Universal LiDAR Segmentation</h3>
                <p>Authors: Youquan LiuLingdong KongXiaoyang WuRunnan ChenXin LiLiang PanZiwei LiuYuexin Ma</p>
                <p><a href="http://arxiv.org/abs/2405.01538v1">Link to paper</a></p>
                <p>A unified and versatile LiDAR segmentation model with strong robustness andgeneralizability is desirable for safe autonomous driving perception. This workpresents M3Net a one-of-a-kind framework for fulfilling multi-taskmulti-dataset multi-modality LiDAR segmentation in a universal manner usingjust a single set of parameters. To better exploit data volume and diversitywe first combine large-scale driving datasets acquired by different types ofsensors from diverse scenes and then conduct alignments in three spaces namelydata feature and label spaces during the training. As a result M3Net iscapable of taming heterogeneous data for training state-of-the-art LiDARsegmentation models. Extensive experiments on twelve LiDAR segmentationdatasets verify our effectiveness. Notably using a shared set of parametersM3Net achieves 75.1 83.1 and 72.4 mIoU scores respectively on theofficial benchmarks of SemanticKITTI nuScenes and Waymo Open.</p>
                <p>Last Updated: 2024-05-02 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2405.01538v1">Interpret</button>
                <div id="interpretation-2405.01538v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Customizing Text-to-Image Models with a Single Image Pair</h3>
                <p>Authors: Maxwell JonesSheng-Yu WangNupur KumariDavid BauJun-Yan Zhu</p>
                <p><a href="http://arxiv.org/abs/2405.01536v1">Link to paper</a></p>
                <p>Art reinterpretation is the practice of creating a variation of a referencework making a paired artwork that exhibits a distinct artistic style. We askif such an image pair can be used to customize a generative model to capturethe demonstrated stylistic difference. We propose Pair Customization a newcustomization method that learns stylistic difference from a single image pairand then applies the acquired style to the generation process. Unlike existingmethods that learn to mimic a single concept from a collection of images ourmethod captures the stylistic difference between paired images. This allows usto apply a stylistic change without overfitting to the specific image contentin the examples. To address this new task we employ a joint optimizationmethod that explicitly separates the style and content into distinct LoRAweight spaces. We optimize these style and content weights to reproduce thestyle and content images while encouraging their orthogonality. Duringinference we modify the diffusion process via a new style guidance based onour learned weights. Both qualitative and quantitative experiments show thatour method can effectively learn style while avoiding overfitting to imagecontent highlighting the potential of modeling such stylistic differences froma single image pair.</p>
                <p>Last Updated: 2024-05-02 17:59:52 UTC</p>
                <button class="interpret-button" data-id="2405.01536v1">Interpret</button>
                <div id="interpretation-2405.01536v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks</h3>
                <p>Authors: Murtaza DalalTarun ChiruvoluDevendra ChaplotRuslan Salakhutdinov</p>
                <p><a href="http://arxiv.org/abs/2405.01534v1">Link to paper</a></p>
                <p>Large Language Models LLMs have been shown to be capable of performinghigh-level planning for long-horizon robotics tasks yet existing methodsrequire access to a pre-defined skill library e.g. picking placing pullingpushing navigating. However LLM planning does not address how to design orlearn those behaviors which remains challenging particularly in long-horizonsettings. Furthermore for many tasks of interest the robot needs to be ableto adjust its behavior in a fine-grained manner requiring the agent to becapable of modifying low-level control actions. Can we instead use theinternet-scale knowledge from LLMs for high-level policies guidingreinforcement learning RL policies to efficiently solve robotic control tasksonline without requiring a pre-determined set of skills In this paper wepropose Plan-Seq-Learn PSL: a modular approach that uses motion planning tobridge the gap between abstract language and learned low-level control forsolving long-horizon robotics tasks from scratch. We demonstrate that PSLachieves state-of-the-art results on over 25 challenging robotics tasks with upto 10 stages. PSL solves long-horizon tasks from raw visual input spanning fourbenchmarks at success rates of over 85 out-performing language-basedclassical and end-to-end approaches. Video results and code athttps://mihdalal.github.io/planseqlearn/</p>
                <p>Last Updated: 2024-05-02 17:59:31 UTC</p>
                <button class="interpret-button" data-id="2405.01534v1">Interpret</button>
                <div id="interpretation-2405.01534v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning</h3>
                <p>Authors: Shihao WangZhiding YuXiaohui JiangShiyi LanMin ShiNadine ChangJan KautzYing LiJose M. Alvarez</p>
                <p><a href="http://arxiv.org/abs/2405.01533v1">Link to paper</a></p>
                <p>The advances in multimodal large language models MLLMs have led to growinginterests in LLM-based autonomous driving agents to leverage their strongreasoning capabilities. However capitalizing on MLLMs strong reasoningcapabilities for improved planning behavior is challenging since planningrequires full 3D situational awareness beyond 2D reasoning. To address thischallenge our work proposes a holistic framework for strong alignment betweenagent models and 3D driving tasks. Our framework starts with a novel 3D MLLMarchitecture that uses sparse queries to lift and compress visualrepresentations into 3D before feeding them into an LLM. This query-basedrepresentation allows us to jointly encode dynamic objects and static mapelements e.g. traffic lanes providing a condensed world model forperception-action alignment in 3D. We further propose OmniDrive-nuScenes a newvisual question-answering dataset challenging the true 3D situational awarenessof a model with comprehensive visual question-answering VQA tasks includingscene description traffic regulation 3D grounding counterfactual reasoningdecision making and planning. Extensive studies show the effectiveness of theproposed architecture as well as the importance of the VQA tasks for reasoningand planning in complex 3D scenes.</p>
                <p>Last Updated: 2024-05-02 17:59:24 UTC</p>
                <button class="interpret-button" data-id="2405.01533v1">Interpret</button>
                <div id="interpretation-2405.01533v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models</h3>
                <p>Authors: Nishad SinghiJae Myung KimKarsten RothZeynep Akata</p>
                <p><a href="http://arxiv.org/abs/2405.01531v1">Link to paper</a></p>
                <p>Concept Bottleneck Models CBMs ground image classification onhuman-understandable concepts to allow for interpretable model decisions.Crucially the CBM design inherently allows for human interventions in whichexpert users are given the ability to modify potentially misaligned conceptchoices to influence the decision behavior of the model in an interpretablefashion. However existing approaches often require numerous humaninterventions per image to achieve strong performances posing practicalchallenges in scenarios where obtaining human feedback is expensive. In thispaper we find that this is noticeably driven by an independent treatment ofconcepts during intervention wherein a change of one concept does notinfluence the use of other ones in the models final decision. To address thisissue we introduce a trainable concept intervention realignment module whichleverages concept relations to realign concept assignments post-intervention.Across standard real-world benchmarks we find that concept realignment cansignificantly improve intervention efficacy significantly reducing the numberof interventions needed to reach a target classification performance or conceptprediction accuracy. In addition it easily integrates into existingconcept-based architectures without requiring changes to the models themselves.This reduced cost of human-model collaboration is crucial to enhancing thefeasibility of CBMs in resource-constrained environments.</p>
                <p>Last Updated: 2024-05-02 17:59:01 UTC</p>
                <button class="interpret-button" data-id="2405.01531v1">Interpret</button>
                <div id="interpretation-2405.01531v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Multi-Space Alignments Towards Universal LiDAR Segmentation</h3>
                <p>Authors: Youquan LiuLingdong KongXiaoyang WuRunnan ChenXin LiLiang PanZiwei LiuYuexin Ma</p>
                <p><a href="http://arxiv.org/abs/2405.01538v1">Link to paper</a></p>
                <p>A unified and versatile LiDAR segmentation model with strong robustness andgeneralizability is desirable for safe autonomous driving perception. This workpresents M3Net a one-of-a-kind framework for fulfilling multi-taskmulti-dataset multi-modality LiDAR segmentation in a universal manner usingjust a single set of parameters. To better exploit data volume and diversitywe first combine large-scale driving datasets acquired by different types ofsensors from diverse scenes and then conduct alignments in three spaces namelydata feature and label spaces during the training. As a result M3Net iscapable of taming heterogeneous data for training state-of-the-art LiDARsegmentation models. Extensive experiments on twelve LiDAR segmentationdatasets verify our effectiveness. Notably using a shared set of parametersM3Net achieves 75.1 83.1 and 72.4 mIoU scores respectively on theofficial benchmarks of SemanticKITTI nuScenes and Waymo Open.</p>
                <p>Last Updated: 2024-05-02 17:59:57 UTC</p>
                <button class="interpret-button" data-id="2405.01538v1">Interpret</button>
                <div id="interpretation-2405.01538v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Customizing Text-to-Image Models with a Single Image Pair</h3>
                <p>Authors: Maxwell JonesSheng-Yu WangNupur KumariDavid BauJun-Yan Zhu</p>
                <p><a href="http://arxiv.org/abs/2405.01536v1">Link to paper</a></p>
                <p>Art reinterpretation is the practice of creating a variation of a referencework making a paired artwork that exhibits a distinct artistic style. We askif such an image pair can be used to customize a generative model to capturethe demonstrated stylistic difference. We propose Pair Customization a newcustomization method that learns stylistic difference from a single image pairand then applies the acquired style to the generation process. Unlike existingmethods that learn to mimic a single concept from a collection of images ourmethod captures the stylistic difference between paired images. This allows usto apply a stylistic change without overfitting to the specific image contentin the examples. To address this new task we employ a joint optimizationmethod that explicitly separates the style and content into distinct LoRAweight spaces. We optimize these style and content weights to reproduce thestyle and content images while encouraging their orthogonality. Duringinference we modify the diffusion process via a new style guidance based onour learned weights. Both qualitative and quantitative experiments show thatour method can effectively learn style while avoiding overfitting to imagecontent highlighting the potential of modeling such stylistic differences froma single image pair.</p>
                <p>Last Updated: 2024-05-02 17:59:52 UTC</p>
                <button class="interpret-button" data-id="2405.01536v1">Interpret</button>
                <div id="interpretation-2405.01536v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks</h3>
                <p>Authors: Murtaza DalalTarun ChiruvoluDevendra ChaplotRuslan Salakhutdinov</p>
                <p><a href="http://arxiv.org/abs/2405.01534v1">Link to paper</a></p>
                <p>Large Language Models LLMs have been shown to be capable of performinghigh-level planning for long-horizon robotics tasks yet existing methodsrequire access to a pre-defined skill library e.g. picking placing pullingpushing navigating. However LLM planning does not address how to design orlearn those behaviors which remains challenging particularly in long-horizonsettings. Furthermore for many tasks of interest the robot needs to be ableto adjust its behavior in a fine-grained manner requiring the agent to becapable of modifying low-level control actions. Can we instead use theinternet-scale knowledge from LLMs for high-level policies guidingreinforcement learning RL policies to efficiently solve robotic control tasksonline without requiring a pre-determined set of skills In this paper wepropose Plan-Seq-Learn PSL: a modular approach that uses motion planning tobridge the gap between abstract language and learned low-level control forsolving long-horizon robotics tasks from scratch. We demonstrate that PSLachieves state-of-the-art results on over 25 challenging robotics tasks with upto 10 stages. PSL solves long-horizon tasks from raw visual input spanning fourbenchmarks at success rates of over 85 out-performing language-basedclassical and end-to-end approaches. Video results and code athttps://mihdalal.github.io/planseqlearn/</p>
                <p>Last Updated: 2024-05-02 17:59:31 UTC</p>
                <button class="interpret-button" data-id="2405.01534v1">Interpret</button>
                <div id="interpretation-2405.01534v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models</h3>
                <p>Authors: Nishad SinghiJae Myung KimKarsten RothZeynep Akata</p>
                <p><a href="http://arxiv.org/abs/2405.01531v1">Link to paper</a></p>
                <p>Concept Bottleneck Models CBMs ground image classification onhuman-understandable concepts to allow for interpretable model decisions.Crucially the CBM design inherently allows for human interventions in whichexpert users are given the ability to modify potentially misaligned conceptchoices to influence the decision behavior of the model in an interpretablefashion. However existing approaches often require numerous humaninterventions per image to achieve strong performances posing practicalchallenges in scenarios where obtaining human feedback is expensive. In thispaper we find that this is noticeably driven by an independent treatment ofconcepts during intervention wherein a change of one concept does notinfluence the use of other ones in the models final decision. To address thisissue we introduce a trainable concept intervention realignment module whichleverages concept relations to realign concept assignments post-intervention.Across standard real-world benchmarks we find that concept realignment cansignificantly improve intervention efficacy significantly reducing the numberof interventions needed to reach a target classification performance or conceptprediction accuracy. In addition it easily integrates into existingconcept-based architectures without requiring changes to the models themselves.This reduced cost of human-model collaboration is crucial to enhancing thefeasibility of CBMs in resource-constrained environments.</p>
                <p>Last Updated: 2024-05-02 17:59:01 UTC</p>
                <button class="interpret-button" data-id="2405.01531v1">Interpret</button>
                <div id="interpretation-2405.01531v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A separability-based approach to quantifying generalization: which layer is best?</h3>
                <p>Authors: Luciano DyballaEvan GerritzSteven W. Zucker</p>
                <p><a href="http://arxiv.org/abs/2405.01524v1">Link to paper</a></p>
                <p>Generalization to unseen data remains poorly understood for deep learningclassification and foundation models. How can one assess the ability ofnetworks to adapt to new or extended versions of their input space in thespirit of few-shot learning out-of-distribution generalization and domainadaptation Which layers of a network are likely to generalize best We providea new method for evaluating the capacity of networks to represent a sampleddomain regardless of whether the network has been trained on all classes inthe domain. Our approach is the following: after fine-tuning state-of-the-artpre-trained models for visual classification on a particular domain we assesstheir performance on data from related but distinct variations in that domain.Generalization power is quantified as a function of the latent embeddings ofunseen data from intermediate layers for both unsupervised and supervisedsettings. Working throughout all stages of the network we find that i highclassification accuracy does not imply high generalizability and ii deeperlayers in a model do not always generalize the best which has implications forpruning. Since the trends observed across datasets are largely consistent weconclude that our approach reveals a function of the intrinsic capacity ofthe different layers of a model to generalize.</p>
                <p>Last Updated: 2024-05-02 17:54:35 UTC</p>
                <button class="interpret-button" data-id="2405.01524v1">Interpret</button>
                <div id="interpretation-2405.01524v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Causal Influence in Federated Edge Inference</h3>
                <p>Authors: Mert KayaalpYunus InanVisa KoivunenAli H. Sayed</p>
                <p><a href="http://arxiv.org/abs/2405.01260v1">Link to paper</a></p>
                <p>In this paper we consider a setting where heterogeneous agents withconnectivity are performing inference using unlabeled streaming data. Observeddata are only partially informative about the target variable of interest. Inorder to overcome the uncertainty agents cooperate with each other byexchanging their local inferences with and through a fusion center. To evaluatehow each agent influences the overall decision we adopt a causal framework inorder to distinguish the actual influence of agents from mere correlationswithin the decision-making process. Various scenarios reflecting differentagent participation patterns and fusion center policies are investigated. Wederive expressions to quantify the causal impact of each agent on the jointdecision which could be beneficial for anticipating and addressing atypicalscenarios such as adversarial attacks or system malfunctions. We validate ourtheoretical results with numerical simulations and a real-world application ofmulti-camera crowd counting.</p>
                <p>Last Updated: 2024-05-02 13:06:50 UTC</p>
                <button class="interpret-button" data-id="2405.01260v1">Interpret</button>
                <div id="interpretation-2405.01260v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot Applications</h3>
                <p>Authors: Jan BlumenkampSteven MoradJennifer GielisAmanda Prorok</p>
                <p><a href="http://arxiv.org/abs/2405.01107v1">Link to paper</a></p>
                <p>Spatial understanding from vision is crucial for robots operating inunstructured environments. In the real world spatial understanding is often anill-posed problem. There are a number of powerful classical methods thataccurately regress relative pose however these approaches often lack theability to leverage data-derived priors to resolve ambiguities. In multi-robotsystems these challenges are exacerbated by the need for accurate and frequentposition estimates of cooperating agents. To this end we propose CoViS-Net acooperative multi-robot visual spatial foundation model that learns spatialpriors from data. Unlike prior work evaluated primarily on offline datasets wedesign our model specifically for online evaluation and real-world deploymenton cooperative robots. Our model is completely decentralized platformagnostic executable in real-time using onboard compute and does not requireexisting network infrastructure. In this work we focus on relative poseestimation and local Birds Eye View BEV prediction tasks. Unlike classicalapproaches we show that our model can accurately predict relative poseswithout requiring camera overlap and predict BEVs of regions not visible tothe ego-agent. We demonstrate our model on a multi-robot formation control taskoutside the confines of the laboratory.</p>
                <p>Last Updated: 2024-05-02 09:14:41 UTC</p>
                <button class="interpret-button" data-id="2405.01107v1">Interpret</button>
                <div id="interpretation-2405.01107v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Computing Threshold Circuits with Bimolecular Void Reactions in Step Chemical Reaction Networks</h3>
                <p>Authors: Rachel AndersonBin FuAiden MassieGourab MukhopadhyayAdrian SalinasRobert SchwellerEvan TomaiTim Wylie</p>
                <p><a href="http://arxiv.org/abs/2405.00940v1">Link to paper</a></p>
                <p>Step Chemical Reaction Networks step CRNs are an augmentation of theChemical Reaction Network CRN model where additional species may beintroduced to the system in a sequence of steps. We study step CRN systemsusing a weak subset of reaction rules emphvoid rules in which molecularspecies can only be deleted. We demonstrate that step CRNs with only void rulesof size 20 can simulate threshold formulas TFs under linear resources.These limited systems can also simulate threshold emphcircuits TCs bymodifying the volume of the system to be exponential. We then prove a matchingexponential lower bound on the required volume for simulating thresholdcircuits in a step CRN with 20-size rules under a restrictedemphgate-wise simulation thus showing our construction is optimal forsimulating circuits in this way.</p>
                <p>Last Updated: 2024-05-02 01:55:48 UTC</p>
                <button class="interpret-button" data-id="2405.00940v1">Interpret</button>
                <div id="interpretation-2405.00940v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Verification of Population Protocols with Unordered Data</h3>
                <p>Authors: Steffen van BergeremRoland GuttenbergSandra KieferCorto MascleNicolas WaldburgerChana Weil-Kennedy</p>
                <p><a href="http://arxiv.org/abs/2405.00921v1">Link to paper</a></p>
                <p>Population protocols are a well-studied model of distributed computation inwhich a group of anonymous finite-state agents communicates via pairwiseinteractions. Together they decide whether their initial configuration thatis the initial distribution of agents in the states satisfies a property. Asan extension in order to express properties of multisets over an infinite datadomain Blondin and Ladouceur ICALP23 introduced population protocols withunordered data PPUD. In PPUD each agent carries a fixed data value and theinteractions between agents depend on whether their data are equal or not.Blondin and Ladouceur also identified the interesting subclass of immediateobservation PPUD IOPPUD where in every transition one of the two agentsremains passive and does not move and they characterised its expressive power.  We study the decidability and complexity of formally verifying theseprotocols. The main verification problem for population protocols iswell-specification that is checking whether the given PPUD computes somefunction. We show that well-specification is undecidable in general. Bycontrast for IOPPUD we exhibit a large yet natural class of problems whichincludes well-specification among other classic problems and establish thatthese problems are in EXPSPACE. We also provide a lower complexity boundnamely coNEXPTIME-hardness.</p>
                <p>Last Updated: 2024-05-02 00:32:02 UTC</p>
                <button class="interpret-button" data-id="2405.00921v1">Interpret</button>
                <div id="interpretation-2405.00921v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure</h3>
                <p>Authors: Zhicheng ZhangYancheng LiangYi WuFei Fang</p>
                <p><a href="http://arxiv.org/abs/2405.00902v1">Link to paper</a></p>
                <p>Multi-agent reinforcement learning MARL algorithms often struggle to findstrategies close to Pareto optimal Nash Equilibrium owing largely to the lackof efficient exploration. The problem is exacerbated in sparse-reward settingscaused by the larger variance exhibited in policy learning. This paperintroduces MESA a novel meta-exploration method for cooperative multi-agentlearning. It learns to explore by first identifying the agents high-rewardingjoint state-action subspace from training tasks and then learning a set ofdiverse exploration policies to cover the subspace. These trained explorationpolicies can be integrated with any off-policy MARL algorithm for test-timetasks. We first showcase MESAs advantage in a multi-step matrix game.Furthermore experiments show that with learned exploration policies MESAachieves significantly better performance in sparse-reward tasks in severalmulti-agent particle environments and multi-agent MuJoCo environments andexhibits the ability to generalize to more challenging tasks at test time.</p>
                <p>Last Updated: 2024-05-01 23:19:48 UTC</p>
                <button class="interpret-button" data-id="2405.00902v1">Interpret</button>
                <div id="interpretation-2405.00902v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Supporting Business Document Workflows via Collection-Centric Information Foraging with Large Language Models</h3>
                <p>Authors: Raymond FokNedim LipkaTong SunAlexa Siu</p>
                <p><a href="http://arxiv.org/abs/2405.01501v1">Link to paper</a></p>
                <p>Knowledge workers often need to extract and analyze information from acollection of documents to solve complex information tasks in the workplacee.g. hiring managers reviewing resumes or analysts assessing risk incontracts. However foraging for relevant information can become tedious andrepetitive over many documents and criteria of interest. We introduce Marco amixed-initiative workspace supporting sensemaking over diverse businessdocument collections. Through collection-centric assistance Marco reduces thecognitive costs of extracting and structuring information allowing users toprioritize comparative synthesis and decision making processes. Usersinteractively communicate their information needs to an AI assistant usingnatural language and compose schemas that provide an overview of a documentcollection. Findings from a usability study n16 demonstrate that when usingMarco users complete sensemaking tasks 16 more quickly with less effort andwithout diminishing accuracy. A design probe with seven domain expertsidentifies how Marco can benefit various real-world workflows.</p>
                <p>Last Updated: 2024-05-02 17:32:38 UTC</p>
                <button class="interpret-button" data-id="2405.01501v1">Interpret</button>
                <div id="interpretation-2405.01501v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Designing Algorithmic Recommendations to Achieve Human-AI Complementarity</h3>
                <p>Authors: Bryce McLaughlinJann Spiess</p>
                <p><a href="http://arxiv.org/abs/2405.01484v1">Link to paper</a></p>
                <p>Algorithms frequently assist rather than replace human decision-makers.However the design and analysis of algorithms often focus on predictingoutcomes and do not explicitly model their effect on human decisions. Thisdiscrepancy between the design and role of algorithmic assistants becomes ofparticular concern in light of empirical evidence that suggests thatalgorithmic assistants again and again fail to improve human decisions. In thisarticle we formalize the design of recommendation algorithms that assist humandecision-makers without making restrictive ex-ante assumptions about howrecommendations affect decisions. We formulate an algorithmic-design problemthat leverages the potential-outcomes framework from causal inference to modelthe effect of recommendations on a human decision-makers binary treatmentchoice. Within this model we introduce a monotonicity assumption that leads toan intuitive classification of human responses to the algorithm. Under thismonotonicity assumption we can express the humans response to algorithmicrecommendations in terms of their compliance with the algorithm and thedecision they would take if the algorithm sends no recommendation. We showcasethe utility of our framework using an online experiment that simulates a hiringtask. We argue that our approach explains the relative performance of differentrecommendation algorithms in the experiment and can help design solutions thatrealize human-AI complementarity.</p>
                <p>Last Updated: 2024-05-02 17:15:30 UTC</p>
                <button class="interpret-button" data-id="2405.01484v1">Interpret</button>
                <div id="interpretation-2405.01484v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Student Reflections on Self-Initiated GenAI Use in HCI Education</h3>
                <p>Authors: Hauke SandhausMaria Teresa ParreiraWendy Ju</p>
                <p><a href="http://arxiv.org/abs/2405.01467v1">Link to paper</a></p>
                <p>This study explores students self-initiated use of Generative ArtificialIntelligence GenAI tools in an interactive systems design class. Through 12group interviews students revealed the dual nature of GenAI in 1 stimulatingcreativity and 2 speeding up design iterations alongside concerns over itspotential to cause shallow learning and reliance. GenAIs benefits werepronounced in the execution phase of design aiding rapid prototyping andideation while its use in initial insight generation posed risks to depth andreflective practice. This reflection highlights the complex role of GenAI inHuman-Computer Interaction education emphasizing the need for balancedintegration to leverage its advantages without compromising fundamentallearning outcomes.</p>
                <p>Last Updated: 2024-05-02 16:58:17 UTC</p>
                <button class="interpret-button" data-id="2405.01467v1">Interpret</button>
                <div id="interpretation-2405.01467v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Human-Robot Interaction Conversational User Enjoyment Scale (HRI CUES)</h3>
                <p>Authors: Bahar IrfanJura MiniotaSofia ThunbergErik LagerstedtSanna KuoppamäkiGabriel SkantzeAndré Pereira</p>
                <p><a href="http://arxiv.org/abs/2405.01354v1">Link to paper</a></p>
                <p>Understanding user enjoyment is crucial in human-robot interaction HRI asit can impact interaction quality and influence user acceptance and long-termengagement with robots particularly in the context of conversations withsocial robots. However current assessment methods rely solely on self-reportedquestionnaires failing to capture interaction dynamics. This work introducesthe Human-Robot Interaction Conversational User Enjoyment Scale HRI CUES anovel scale for assessing user enjoyment from an external perspective duringconversations with a robot. Developed through rigorous evaluations anddiscussions of three annotators with relevant expertise the scale provides astructured framework for assessing enjoyment in each conversation exchangeturn alongside overall interaction levels. It aims to complementself-reported enjoyment from users and holds the potential for autonomouslyidentifying user enjoyment in real-time HRI. The scale was validated on 25older adults open-domain dialogue with a companion robot that was powered by alarge language model for conversations corresponding to 174 minutes of datashowing moderate to good alignment. Additionally the study offers insightsinto understanding the nuances and challenges of assessing user enjoyment inrobot interactions and provides guidelines on applying the scale to otherdomains.</p>
                <p>Last Updated: 2024-05-02 15:01:43 UTC</p>
                <button class="interpret-button" data-id="2405.01354v1">Interpret</button>
                <div id="interpretation-2405.01354v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Quantifying Spatial Domain Explanations in BCI using Earth Mover's Distance</h3>
                <p>Authors: Param RajpuraHubert CecottiYogesh Kumar Meena</p>
                <p><a href="http://arxiv.org/abs/2405.01277v1">Link to paper</a></p>
                <p>Brain-computer interface BCI systems facilitate unique communicationbetween humans and computers benefiting severely disabled individuals. Despitedecades of research BCIs are not fully integrated into clinical and commercialsettings. Its crucial to assess and explain BCI performance offering clearexplanations for potential users to avoid frustration when it doesnt work asexpected. This work investigates the efficacy of different deep learning andRiemannian geometry-based classification models in the context of motor imageryMI based BCI using electroencephalography EEG. We then propose an optimaltransport theory-based approach using earth movers distance EMD to quantifythe comparison of the feature relevance map with the domain knowledge ofneuroscience. For this we utilized explainable AI XAI techniques forgenerating feature relevance in the spatial domain to identify importantchannels for model outcomes. Three state-of-the-art models are implemented - 1Riemannian geometry-based classifier 2 EEGNet and 3 EEG Conformer and theobserved trend in the models accuracy across different architectures on thedataset correlates with the proposed feature relevance metrics. The models withdiverse architectures perform significantly better when trained on channelsrelevant to motor imagery than data-driven channel selection. This work focusesattention on the necessity for interpretability and incorporating metricsbeyond accuracy underscores the value of combining domain knowledge andquantifying model interpretations with data-driven approaches in creatingreliable and robust Brain-Computer Interfaces BCIs.</p>
                <p>Last Updated: 2024-05-02 13:35:15 UTC</p>
                <button class="interpret-button" data-id="2405.01277v1">Interpret</button>
                <div id="interpretation-2405.01277v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-05-05</p>
        </div>
    
        </div>
    </body>
    </html>
    