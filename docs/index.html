
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Research Papers</title>
        <link rel="stylesheet" href="style.css">
        <script src="script.js"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    </head>
    <body>
        <div id="sidebar">
            <h3>Papers by Category:</h3>
            <ul id="categories">
    
                <li><a href="#cs.MA">cs.MA</a></li>
            
                <li><a href="#cs.AI">cs.AI</a></li>
            
                <li><a href="#stat.ML">stat.ML</a></li>
            
                <li><a href="#cs.CL">cs.CL</a></li>
            
                <li><a href="#cs.HC">cs.HC</a></li>
            
                <li><a href="#cs.LG">cs.LG</a></li>
            
                <li><a href="#cs.CV">cs.CV</a></li>
            
            </ul>
            <li>
                    <a href="https://github.com/DukeEnglish/papertutor" target="_blank" rel="noopener noreferrer">
                        <span class="fab fa-github"></span> GitHub
                    </a>
            </li>
        </div>
        <div id="content">
    
    <section id="cs.MA">
        <h2>cs.MA</h2>
        <ul>
    
            <li>
                <h3>Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning</h3>
                <p>Authors: Zeyang LiuLipeng WanXinrui YangZhuoran ChenXingyu ChenXuguang Lan</p>
                <p><a href="http://arxiv.org/abs/2402.17978v1">Link to paper</a></p>
                <p>Effective exploration is crucial to discovering optimal strategies formulti-agent reinforcement learning MARL in complex coordination tasks.Existing methods mainly utilize intrinsic rewards to enable committedexploration or use role-based learning for decomposing joint action spacesinstead of directly conducting a collective search in the entireaction-observation space. However they often face challenges obtainingspecific joint action sequences to reach successful states in long-horizontasks. To address this limitation we propose Imagine Initialize and ExploreIIE a novel method that offers a promising solution for efficientmulti-agent exploration in complex scenarios. IIE employs a transformer modelto imagine how the agents reach a critical state that can influence eachothers transition functions. Then we initialize the environment at this stateusing a simulator before the exploration phase. We formulate the imagination asa sequence modeling problem where the states observations prompts actionsand rewards are predicted autoregressively. The prompt consists oftimestep-to-go return-to-go influence value and one-shot demonstrationspecifying the desired state and trajectory as well as guiding the actiongeneration. By initializing agents at the critical states IIE significantlyincreases the likelihood of discovering potentially important under-exploredregions. Despite its simplicity empirical results demonstrate that our methodoutperforms multi-agent exploration baselines on the StarCraft Multi-AgentChallenge SMAC and SMACv2 environments. Particularly IIE shows improvedperformance in the sparse-reward SMAC tasks and produces more effectivecurricula over the initialized states than other generative methods such asCVAE-GAN and diffusion models.</p>
                <p>Last Updated: 2024-02-28 01:45:01 UTC</p>
                <button class="interpret-button" data-id="2402.17978v1">Interpret</button>
                <div id="interpretation-2402.17978v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Heterogeneous Agent Model of Mortgage Servicing: An Income-based Relief Analysis</h3>
                <p>Authors: Deepeka GargBenjamin Patrick EvansLeo ArdonAnnapoorani Lakshmi NarayananJared VannUdari MadhushaniMakada Henry-NickieSumitra Ganesh</p>
                <p><a href="http://arxiv.org/abs/2402.17932v2">Link to paper</a></p>
                <p>Mortgages account for the largest portion of household debt in the UnitedStates totaling around 12 trillion nationwide. In times of financialhardship alleviating mortgage burdens is essential for supporting affectedhouseholds. The mortgage servicing industry plays a vital role in offering thisassistance yet there has been limited research modelling the complexrelationship between households and servicers. To bridge this gap we developedan agent-based model that explores household behavior and the effectiveness ofrelief measures during financial distress. Our model represents households asadaptive learning agents with realistic financial attributes. These householdsexperience exogenous income shocks which may influence their ability to makemortgage payments. Mortgage servicers provide relief options to thesehouseholds who then choose the most suitable relief based on their uniquefinancial circumstances and individual preferences. We analyze the impact ofvarious external shocks and the success of different mortgage relief strategieson specific borrower subgroups. Through this analysis we show that our modelcan not only replicate real-world mortgage studies but also act as a tool forconducting a broad range of what-if scenario analyses. Our approach offersfine-grained insights that can inform the development of more effective andinclusive mortgage relief solutions.</p>
                <p>Last Updated: 2024-02-29 14:21:05 UTC</p>
                <button class="interpret-button" data-id="2402.17932v2">Interpret</button>
                <div id="interpretation-2402.17932v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Independent Learning in Constrained Markov Potential Games</h3>
                <p>Authors: Philip JordanAnas BarakatNiao He</p>
                <p><a href="http://arxiv.org/abs/2402.17885v1">Link to paper</a></p>
                <p>Constrained Markov games offer a formal mathematical framework for modelingmulti-agent reinforcement learning problems where the behavior of the agents issubject to constraints. In this work we focus on the recently introduced classof constrained Markov Potential Games. While centralized algorithms have beenproposed for solving such constrained games the design of convergingindependent learning algorithms tailored for the constrained setting remains anopen question. We propose an independent policy gradient algorithm for learningapproximate constrained Nash equilibria: Each agent observes their own actionsand rewards along with a shared state. Inspired by the optimizationliterature our algorithm performs proximal-point-like updates augmented with aregularized constraint set. Each proximal step is solved inexactly using astochastic switching gradient algorithm. Notably our algorithm can beimplemented independently without a centralized coordination mechanismrequiring turn-based agent updates. Under some technical constraintqualification conditions we establish convergence guarantees towardsconstrained approximate Nash equilibria. We perform simulations to illustrateour results.</p>
                <p>Last Updated: 2024-02-27 20:57:35 UTC</p>
                <button class="interpret-button" data-id="2402.17885v1">Interpret</button>
                <div id="interpretation-2402.17885v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>A Multi-Agent Model for Opinion Evolution under Cognitive Biases</h3>
                <p>Authors: Mário S. AlvimArtur Gaspar da SilvaSophia KnightFrank Valencia</p>
                <p><a href="http://arxiv.org/abs/2402.17615v1">Link to paper</a></p>
                <p>We generalize the DeGroot model for opinion dynamics to better capturerealistic social scenarios. We introduce a model where each agent has their ownindividual cognitive biases. Society is represented as a directed graph whoseedges indicate how much agents influence one another. Biases are represented asthe functions in the square region -112 and categorized into foursub-regions based on the potential reactions they may elicit in an agent duringinstances of opinion disagreement. Under the assumption that each bias of everyagent is a continuous function within the region of receptive but resistantreactions mathbfR we show that the society converges to a consensus ifthe graph is strongly connected. Under the same assumption we also establishthat the entire society converges to a unanimous opinion if and only if thesource components of the graph-namely strongly connected components with noexternal influence-converge to that opinion. We illustrate that convergence isnot guaranteed for strongly connected graphs when biases are eitherdiscontinuous functions in mathbfR or not included in mathbfR. Weshowcase our model through a series of examples and simulations offeringinsights into how opinions form in social networks under cognitive biases.</p>
                <p>Last Updated: 2024-02-27 15:44:12 UTC</p>
                <button class="interpret-button" data-id="2402.17615v1">Interpret</button>
                <div id="interpretation-2402.17615v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas</h3>
                <p>Authors: Hao GuoChunjiang MuYang ChenChen ShenShuyue HuZhen Wang</p>
                <p><a href="http://arxiv.org/abs/2402.17270v1">Link to paper</a></p>
                <p>The study of cooperation within social dilemmas has long been a fundamentaltopic across various disciplines including computer science and socialscience. Recent advancements in Artificial Intelligence AI have significantlyreshaped this field offering fresh insights into understanding and enhancingcooperation. This survey examines three key areas at the intersection of AI andcooperation in social dilemmas. First focusing on multi-agent cooperation wereview the intrinsic and external motivations that support cooperation amongrational agents and the methods employed to develop effective strategiesagainst diverse opponents. Second looking into human-agent cooperation wediscuss the current AI algorithms for cooperating with humans and the humanbiases towards AI agents. Third we review the emergent field of leveraging AIagents to enhance cooperation among humans. We conclude by discussing futureresearch avenues such as using large language models establishing unifiedtheoretical frameworks revisiting existing theories of human cooperation andexploring multiple real-world applications.</p>
                <p>Last Updated: 2024-02-27 07:31:30 UTC</p>
                <button class="interpret-button" data-id="2402.17270v1">Interpret</button>
                <div id="interpretation-2402.17270v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.AI">
        <h2>cs.AI</h2>
        <ul>
    
            <li>
                <h3>Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards</h3>
                <p>Authors: Haoxiang WangYong LinWei XiongRui YangShizhe DiaoShuang QiuHan ZhaoTong Zhang</p>
                <p><a href="http://arxiv.org/abs/2402.18571v2">Link to paper</a></p>
                <p>Fine-grained control over large language models LLMs remains a significantchallenge hindering their adaptability to diverse user needs. WhileReinforcement Learning from Human Feedback RLHF shows promise in aligningLLMs its reliance on scalar rewards often limits its ability to capturediverse user preferences in real-world applications. To address thislimitation we introduce the Directional Preference Alignment DPA framework.Unlike the scalar-reward RLHF DPA incorporates multi-objective reward modelingto represent diverse preference profiles. Additionally DPA models userpreferences as directions i.e. unit vectors in the reward space to achieveuser-dependent preference control. Our method involves training amulti-objective reward model and then fine-tuning the LLM with apreference-conditioned variant of Rejection Sampling Finetuning RSF an RLHFmethod adopted by Llama 2. This method enjoys a better performance trade-offacross various reward objectives. In comparison with the scalar-reward RLHFDPA offers users intuitive control over LLM generation: they can arithmeticallyspecify their desired trade-offs e.g. more helpfulness with less verbosity.We also validate the effectiveness of DPA with real-world alignment experimentson Mistral-7B. Our method provides straightforward arithmetic control over thetrade-off between helpfulness and verbosity while maintaining competitiveperformance with strong baselines such as Direct Preference Optimization DPO.</p>
                <p>Last Updated: 2024-02-29 04:33:29 UTC</p>
                <button class="interpret-button" data-id="2402.18571v2">Interpret</button>
                <div id="interpretation-2402.18571v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Approaching Human-Level Forecasting with Language Models</h3>
                <p>Authors: Danny HalawiFred ZhangChen Yueh-HanJacob Steinhardt</p>
                <p><a href="http://arxiv.org/abs/2402.18563v1">Link to paper</a></p>
                <p>Forecasting future events is important for policy and decision making. Inthis work we study whether language models LMs can forecast at the level ofcompetitive human forecasters. Towards this goal we develop aretrieval-augmented LM system designed to automatically search for relevantinformation generate forecasts and aggregate predictions. To facilitate ourstudy we collect a large dataset of questions from competitive forecastingplatforms. Under a test set published after the knowledge cut-offs of our LMswe evaluate the end-to-end performance of our system against the aggregates ofhuman forecasts. On average the system nears the crowd aggregate ofcompetitive forecasters and in some settings surpasses it. Our work suggeststhat using LMs to forecast the future could provide accurate predictions atscale and help to inform institutional decision making.</p>
                <p>Last Updated: 2024-02-28 18:54:18 UTC</p>
                <button class="interpret-button" data-id="2402.18563v1">Interpret</button>
                <div id="interpretation-2402.18563v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates</h3>
                <p>Authors: Kaifeng LyuHaoyu ZhaoXinran GuDingli YuAnirudh GoyalSanjeev Arora</p>
                <p><a href="http://arxiv.org/abs/2402.18540v1">Link to paper</a></p>
                <p>Public LLMs such as the Llama 2-Chat have driven huge activity in LLMresearch. These models underwent alignment training and were considered safe.Recently Qi et al. 2023 reported that even benign fine-tuning e.g. onseemingly safe datasets can give rise to unsafe behaviors in the models. Thecurrent paper is about methods and best practices to mitigate such loss ofalignment. Through extensive experiments on several chat models Metas Llama2-Chat Mistral AIs Mistral 7B Instruct v0.2 and OpenAIs GPT-3.5 Turbothis paper uncovers that the prompt templates used during fine-tuning andinference play a crucial role in preserving safety alignment and proposes thePure Tuning Safe Testing PTST principle -- fine-tune models without asafety prompt but include it at test time. Fine-tuning experiments on GSM8KChatDoctor and OpenOrca show that PTST significantly reduces the rise ofunsafe behaviors and even almost eliminates them in some cases.</p>
                <p>Last Updated: 2024-02-28 18:23:49 UTC</p>
                <button class="interpret-button" data-id="2402.18540v1">Interpret</button>
                <div id="interpretation-2402.18540v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Language Models Represent Beliefs of Self and Others</h3>
                <p>Authors: Wentao ZhuZhining ZhangYizhou Wang</p>
                <p><a href="http://arxiv.org/abs/2402.18496v2">Link to paper</a></p>
                <p>Understanding and attributing mental states known as Theory of Mind ToMemerges as a fundamental capability for human social reasoning. While LargeLanguage Models LLMs appear to possess certain ToM abilities the mechanismsunderlying these capabilities remain elusive. In this study we discover thatit is possible to linearly decode the belief status from the perspectives ofvarious agents through neural activations of language models indicating theexistence of internal representations of self and others beliefs. Bymanipulating these representations we observe dramatic changes in the modelsToM performance underscoring their pivotal role in the social reasoningprocess. Additionally our findings extend to diverse social reasoning tasksthat involve different causal inference patterns suggesting the potentialgeneralizability of these representations.</p>
                <p>Last Updated: 2024-02-29 13:22:17 UTC</p>
                <button class="interpret-button" data-id="2402.18496v2">Interpret</button>
                <div id="interpretation-2402.18496v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay</h3>
                <p>Authors: Mahya RamezaniJose Luis Sanchez-Lopez</p>
                <p><a href="http://arxiv.org/abs/2402.18487v1">Link to paper</a></p>
                <p>The integration of Unmanned Aerial Vehicles UAVs into Search and RescueSAR missions presents a promising avenue for enhancing operational efficiencyand effectiveness. However the success of these missions is not solelydependent on the technical capabilities of the drones but also on theiracceptance and interaction with humans on the ground. This paper explores theeffect of human-centric factor in UAV trajectory planning for SAR missions. Weintroduce a novel approach based on the reinforcement learning augmented withAnalytic Hierarchy Process and novel similarity-based experience replay tooptimize UAV trajectories balancing operational objectives with human comfortand safety considerations. Additionally through a comprehensive survey weinvestigate the impact of gender cues and anthropomorphism in UAV design onpublic acceptance and trust revealing significant implications for droneinteraction strategies in SAR. Our contributions include 1 a reinforcementlearning framework for UAV trajectory planning that dynamically integratesmulti-objective considerations 2 an analysis of human perceptions towardsgendered and anthropomorphized drones in SAR contexts and 3 the applicationof similarity-based experience replay for enhanced learning efficiency incomplex SAR scenarios. The findings offer valuable insights into designing UAVsystems that are not only technically proficient but also aligned withhuman-centric values.</p>
                <p>Last Updated: 2024-02-28 17:10:22 UTC</p>
                <button class="interpret-button" data-id="2402.18487v1">Interpret</button>
                <div id="interpretation-2402.18487v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="stat.ML">
        <h2>stat.ML</h2>
        <ul>
    
            <li>
                <h3>Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards</h3>
                <p>Authors: Haoxiang WangYong LinWei XiongRui YangShizhe DiaoShuang QiuHan ZhaoTong Zhang</p>
                <p><a href="http://arxiv.org/abs/2402.18571v2">Link to paper</a></p>
                <p>Fine-grained control over large language models LLMs remains a significantchallenge hindering their adaptability to diverse user needs. WhileReinforcement Learning from Human Feedback RLHF shows promise in aligningLLMs its reliance on scalar rewards often limits its ability to capturediverse user preferences in real-world applications. To address thislimitation we introduce the Directional Preference Alignment DPA framework.Unlike the scalar-reward RLHF DPA incorporates multi-objective reward modelingto represent diverse preference profiles. Additionally DPA models userpreferences as directions i.e. unit vectors in the reward space to achieveuser-dependent preference control. Our method involves training amulti-objective reward model and then fine-tuning the LLM with apreference-conditioned variant of Rejection Sampling Finetuning RSF an RLHFmethod adopted by Llama 2. This method enjoys a better performance trade-offacross various reward objectives. In comparison with the scalar-reward RLHFDPA offers users intuitive control over LLM generation: they can arithmeticallyspecify their desired trade-offs e.g. more helpfulness with less verbosity.We also validate the effectiveness of DPA with real-world alignment experimentson Mistral-7B. Our method provides straightforward arithmetic control over thetrade-off between helpfulness and verbosity while maintaining competitiveperformance with strong baselines such as Direct Preference Optimization DPO.</p>
                <p>Last Updated: 2024-02-29 04:33:29 UTC</p>
                <button class="interpret-button" data-id="2402.18571v2">Interpret</button>
                <div id="interpretation-2402.18571v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Implicit Bias of Next-Token Prediction</h3>
                <p>Authors: Christos Thrampoulidis</p>
                <p><a href="http://arxiv.org/abs/2402.18551v1">Link to paper</a></p>
                <p>Next-token prediction NTP the go-to training paradigm in training largelanguage models involves predicting the next token in a sequence. Departingfrom traditional one-hot classification in NTP multiple tokens with varyingfrequencies follow each given context. This work frames NTP training ascross-entropy minimization over distinct contexts each associated with asparse empirical probability vector across a finite vocabulary. It thenaddresses the following question: do gradient-based optimizers exhibit a biastowards solutions with specific structure as the NTP training loss reaches itslower bound entropy Specifically for linear NTP models trained usinggradient descent GD we make the following contributions: Firstly wedetermine NTP-separability conditions on the data under which GD can attainits lower bound. We also demonstrate that these conditions hold underoverparameterization. Secondly we establish that the parameters of GDprojected onto an appropriate data subspace converge to the unique solution ofa system of linear equations which requires the logits difference ofin-support tokens to be equal to the log-ratio of their respectiveprobabilities. Meanwhile on the orthogonal subspace the parameters divergeand converge in the direction of the solution of a max-margin quadraticprogram minimizing the Euclidean norm of parameters satisfying theNTP-separability conditions. Akin to prior research on implicit bias ofone-hot classification our work opens exciting avenues for future researchthat can lead to better understanding optimization generalization androbustness principles of models trained with NTP.</p>
                <p>Last Updated: 2024-02-28 18:34:53 UTC</p>
                <button class="interpret-button" data-id="2402.18551v1">Interpret</button>
                <div id="interpretation-2402.18551v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval</h3>
                <p>Authors: Kaiyue WenXingyu DangKaifeng Lyu</p>
                <p><a href="http://arxiv.org/abs/2402.18510v2">Link to paper</a></p>
                <p>This paper investigates the gap in representation powers of Recurrent NeuralNetworks RNNs and Transformers in the context of solving algorithmicproblems. We focus on understanding whether RNNs known for their memoryefficiency in handling long sequences can match the performance ofTransformers particularly when enhanced with Chain-of-Thought CoT prompting.Our theoretical analysis reveals that CoT improves RNNs but is insufficient toclose the gap with Transformers. A key bottleneck lies in the inability of RNNsto perfectly retrieve information from the context even with CoT: for severaltasks that explicitly or implicitly require this capability such asassociative recall and determining if a graph is a tree we prove that RNNs arenot expressive enough to solve the tasks while Transformers can solve them withease. Conversely we prove that adopting techniques to enhance the in-contextretrieval capability of RNNs including Retrieval-Augmented Generation RAGand adding a single Transformer layer can elevate RNNs to be capable ofsolving all polynomial-time solvable problems with CoT hence closing therepresentation gap with Transformers.</p>
                <p>Last Updated: 2024-02-29 07:06:10 UTC</p>
                <button class="interpret-button" data-id="2402.18510v2">Interpret</button>
                <div id="interpretation-2402.18510v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes</h3>
                <p>Authors: Georg MantenCecilia CasoloEmilio FerrucciSøren Wengel MogensenCristopher SalviNiki Kilbertus</p>
                <p><a href="http://arxiv.org/abs/2402.18477v1">Link to paper</a></p>
                <p>Inferring the causal structure underlying stochastic dynamical systems fromobservational data holds great promise in domains ranging from science andhealth to finance. Such processes can often be accurately modeled viastochastic differential equations SDEs which naturally imply causalrelationships via which variables enter the differential of which othervariables. In this paper we develop a kernel-based test of conditionalindependence CI on path-space -- solutions to SDEs -- by leveraging recentadvances in signature kernels. We demonstrate strictly superior performance ofour proposed CI test compared to existing approaches on path-space. Then wedevelop constraint-based causal discovery algorithms for acyclic stochasticdynamical systems allowing for loops that leverage temporal information torecover the entire directed graph. Assuming faithfulness and a CI oracle ouralgorithm is sound and complete. We empirically verify that our developed CItest in conjunction with the causal discovery algorithm reliably outperformsbaselines across a range of settings.</p>
                <p>Last Updated: 2024-02-28 16:58:31 UTC</p>
                <button class="interpret-button" data-id="2402.18477v1">Interpret</button>
                <div id="interpretation-2402.18477v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Unveiling the Potential of Robustness in Evaluating Causal Inference Models</h3>
                <p>Authors: Yiyan HuangCheuk Hang LeungSiyi WangYijun LiQi Wu</p>
                <p><a href="http://arxiv.org/abs/2402.18392v1">Link to paper</a></p>
                <p>The growing demand for personalized decision-making has led to a surge ofinterest in estimating the Conditional Average Treatment Effect CATE. Theintersection of machine learning and causal inference has yielded variouseffective CATE estimators. However deploying these estimators in practice isoften hindered by the absence of counterfactual labels making it challengingto select the desirable CATE estimator using conventional model selectionprocedures like cross-validation. Existing approaches for CATE estimatorselection such as plug-in and pseudo-outcome metrics face two inherentchallenges. Firstly they are required to determine the metric form and theunderlying machine learning models for fitting nuisance parameters or plug-inlearners. Secondly they lack a specific focus on selecting a robust estimator.To address these challenges this paper introduces a novel approach theDistributionally Robust Metric DRM for CATE estimator selection. Theproposed DRM not only eliminates the need to fit additional models but alsoexcels at selecting a robust CATE estimator. Experimental studies demonstratethe efficacy of the DRM method showcasing its consistent effectiveness inidentifying superior estimators while mitigating the risk of selecting inferiorones.</p>
                <p>Last Updated: 2024-02-28 15:12:24 UTC</p>
                <button class="interpret-button" data-id="2402.18392v1">Interpret</button>
                <div id="interpretation-2402.18392v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CL">
        <h2>cs.CL</h2>
        <ul>
    
            <li>
                <h3>Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards</h3>
                <p>Authors: Haoxiang WangYong LinWei XiongRui YangShizhe DiaoShuang QiuHan ZhaoTong Zhang</p>
                <p><a href="http://arxiv.org/abs/2402.18571v2">Link to paper</a></p>
                <p>Fine-grained control over large language models LLMs remains a significantchallenge hindering their adaptability to diverse user needs. WhileReinforcement Learning from Human Feedback RLHF shows promise in aligningLLMs its reliance on scalar rewards often limits its ability to capturediverse user preferences in real-world applications. To address thislimitation we introduce the Directional Preference Alignment DPA framework.Unlike the scalar-reward RLHF DPA incorporates multi-objective reward modelingto represent diverse preference profiles. Additionally DPA models userpreferences as directions i.e. unit vectors in the reward space to achieveuser-dependent preference control. Our method involves training amulti-objective reward model and then fine-tuning the LLM with apreference-conditioned variant of Rejection Sampling Finetuning RSF an RLHFmethod adopted by Llama 2. This method enjoys a better performance trade-offacross various reward objectives. In comparison with the scalar-reward RLHFDPA offers users intuitive control over LLM generation: they can arithmeticallyspecify their desired trade-offs e.g. more helpfulness with less verbosity.We also validate the effectiveness of DPA with real-world alignment experimentson Mistral-7B. Our method provides straightforward arithmetic control over thetrade-off between helpfulness and verbosity while maintaining competitiveperformance with strong baselines such as Direct Preference Optimization DPO.</p>
                <p>Last Updated: 2024-02-29 04:33:29 UTC</p>
                <button class="interpret-button" data-id="2402.18571v2">Interpret</button>
                <div id="interpretation-2402.18571v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Approaching Human-Level Forecasting with Language Models</h3>
                <p>Authors: Danny HalawiFred ZhangChen Yueh-HanJacob Steinhardt</p>
                <p><a href="http://arxiv.org/abs/2402.18563v1">Link to paper</a></p>
                <p>Forecasting future events is important for policy and decision making. Inthis work we study whether language models LMs can forecast at the level ofcompetitive human forecasters. Towards this goal we develop aretrieval-augmented LM system designed to automatically search for relevantinformation generate forecasts and aggregate predictions. To facilitate ourstudy we collect a large dataset of questions from competitive forecastingplatforms. Under a test set published after the knowledge cut-offs of our LMswe evaluate the end-to-end performance of our system against the aggregates ofhuman forecasts. On average the system nears the crowd aggregate ofcompetitive forecasters and in some settings surpasses it. Our work suggeststhat using LMs to forecast the future could provide accurate predictions atscale and help to inform institutional decision making.</p>
                <p>Last Updated: 2024-02-28 18:54:18 UTC</p>
                <button class="interpret-button" data-id="2402.18563v1">Interpret</button>
                <div id="interpretation-2402.18563v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Implicit Bias of Next-Token Prediction</h3>
                <p>Authors: Christos Thrampoulidis</p>
                <p><a href="http://arxiv.org/abs/2402.18551v1">Link to paper</a></p>
                <p>Next-token prediction NTP the go-to training paradigm in training largelanguage models involves predicting the next token in a sequence. Departingfrom traditional one-hot classification in NTP multiple tokens with varyingfrequencies follow each given context. This work frames NTP training ascross-entropy minimization over distinct contexts each associated with asparse empirical probability vector across a finite vocabulary. It thenaddresses the following question: do gradient-based optimizers exhibit a biastowards solutions with specific structure as the NTP training loss reaches itslower bound entropy Specifically for linear NTP models trained usinggradient descent GD we make the following contributions: Firstly wedetermine NTP-separability conditions on the data under which GD can attainits lower bound. We also demonstrate that these conditions hold underoverparameterization. Secondly we establish that the parameters of GDprojected onto an appropriate data subspace converge to the unique solution ofa system of linear equations which requires the logits difference ofin-support tokens to be equal to the log-ratio of their respectiveprobabilities. Meanwhile on the orthogonal subspace the parameters divergeand converge in the direction of the solution of a max-margin quadraticprogram minimizing the Euclidean norm of parameters satisfying theNTP-separability conditions. Akin to prior research on implicit bias ofone-hot classification our work opens exciting avenues for future researchthat can lead to better understanding optimization generalization androbustness principles of models trained with NTP.</p>
                <p>Last Updated: 2024-02-28 18:34:53 UTC</p>
                <button class="interpret-button" data-id="2402.18551v1">Interpret</button>
                <div id="interpretation-2402.18551v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates</h3>
                <p>Authors: Kaifeng LyuHaoyu ZhaoXinran GuDingli YuAnirudh GoyalSanjeev Arora</p>
                <p><a href="http://arxiv.org/abs/2402.18540v1">Link to paper</a></p>
                <p>Public LLMs such as the Llama 2-Chat have driven huge activity in LLMresearch. These models underwent alignment training and were considered safe.Recently Qi et al. 2023 reported that even benign fine-tuning e.g. onseemingly safe datasets can give rise to unsafe behaviors in the models. Thecurrent paper is about methods and best practices to mitigate such loss ofalignment. Through extensive experiments on several chat models Metas Llama2-Chat Mistral AIs Mistral 7B Instruct v0.2 and OpenAIs GPT-3.5 Turbothis paper uncovers that the prompt templates used during fine-tuning andinference play a crucial role in preserving safety alignment and proposes thePure Tuning Safe Testing PTST principle -- fine-tune models without asafety prompt but include it at test time. Fine-tuning experiments on GSM8KChatDoctor and OpenOrca show that PTST significantly reduces the rise ofunsafe behaviors and even almost eliminates them in some cases.</p>
                <p>Last Updated: 2024-02-28 18:23:49 UTC</p>
                <button class="interpret-button" data-id="2402.18540v1">Interpret</button>
                <div id="interpretation-2402.18540v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval</h3>
                <p>Authors: Kaiyue WenXingyu DangKaifeng Lyu</p>
                <p><a href="http://arxiv.org/abs/2402.18510v2">Link to paper</a></p>
                <p>This paper investigates the gap in representation powers of Recurrent NeuralNetworks RNNs and Transformers in the context of solving algorithmicproblems. We focus on understanding whether RNNs known for their memoryefficiency in handling long sequences can match the performance ofTransformers particularly when enhanced with Chain-of-Thought CoT prompting.Our theoretical analysis reveals that CoT improves RNNs but is insufficient toclose the gap with Transformers. A key bottleneck lies in the inability of RNNsto perfectly retrieve information from the context even with CoT: for severaltasks that explicitly or implicitly require this capability such asassociative recall and determining if a graph is a tree we prove that RNNs arenot expressive enough to solve the tasks while Transformers can solve them withease. Conversely we prove that adopting techniques to enhance the in-contextretrieval capability of RNNs including Retrieval-Augmented Generation RAGand adding a single Transformer layer can elevate RNNs to be capable ofsolving all polynomial-time solvable problems with CoT hence closing therepresentation gap with Transformers.</p>
                <p>Last Updated: 2024-02-29 07:06:10 UTC</p>
                <button class="interpret-button" data-id="2402.18510v2">Interpret</button>
                <div id="interpretation-2402.18510v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.HC">
        <h2>cs.HC</h2>
        <ul>
    
            <li>
                <h3>Mental Models of Meeting Goals: Supporting Intentionality in Meeting Technologies</h3>
                <p>Authors: Ava Elizabeth ScottLev TankelevitchSean Rintel</p>
                <p><a href="http://arxiv.org/abs/2402.18526v1">Link to paper</a></p>
                <p>Ineffective meetings due to unclear goals are major obstacles toproductivity yet support for intentionality is surprisingly scant in ourmeeting and allied workflow technologies. To design for intentionality we needto understand workers attitudes and practices around goals. We interviewed 21employees of a global technology company and identified contrasting mentalmodels of meeting goals: meetings as a means to an end and meetings as an endin themselves. We explore how these mental models impact how meeting goalsarise goal prioritization obstacles to considering goals and how lack ofalignment around goals may create tension between organizers and attendees. Wehighlight the challenges in balancing preparation constraining scope andclear outcomes with the need for intentional adaptability and discovery inmeetings. Our findings have implications for designing systems which increaseeffectiveness in meetings by catalyzing intentionality and reducing tension inthe organisation of meetings.</p>
                <p>Last Updated: 2024-02-28 18:06:45 UTC</p>
                <button class="interpret-button" data-id="2402.18526v1">Interpret</button>
                <div id="interpretation-2402.18526v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration</h3>
                <p>Authors: Crystal QianJames Wexler</p>
                <p><a href="http://arxiv.org/abs/2402.18498v1">Link to paper</a></p>
                <p>Although recent developments in generative AI have greatly enhanced thecapabilities of conversational agents such as Googles Bard or OpenAIsChatGPT its unclear whether the usage of these agents aids users acrossvarious contexts. To better understand how access to conversational AI affectsproductivity and trust we conducted a mixed-methods task-based user studyobserving 76 software engineers N76 as they completed a programming examwith and without access to Bard. Effects on performance efficiencysatisfaction and trust vary depending on user expertise question typeopen-ended solve questions vs. definitive search questions andmeasurement type demonstrated vs. self-reported. Our findings includeevidence of automation complacency increased reliance on the AI over thecourse of the task and increased performance for novices on solve-typequestions when using the AI. We discuss common behaviors designrecommendations and impact considerations to improve collaborations withconversational AI.</p>
                <p>Last Updated: 2024-02-28 17:26:45 UTC</p>
                <button class="interpret-button" data-id="2402.18498v1">Interpret</button>
                <div id="interpretation-2402.18498v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay</h3>
                <p>Authors: Mahya RamezaniJose Luis Sanchez-Lopez</p>
                <p><a href="http://arxiv.org/abs/2402.18487v1">Link to paper</a></p>
                <p>The integration of Unmanned Aerial Vehicles UAVs into Search and RescueSAR missions presents a promising avenue for enhancing operational efficiencyand effectiveness. However the success of these missions is not solelydependent on the technical capabilities of the drones but also on theiracceptance and interaction with humans on the ground. This paper explores theeffect of human-centric factor in UAV trajectory planning for SAR missions. Weintroduce a novel approach based on the reinforcement learning augmented withAnalytic Hierarchy Process and novel similarity-based experience replay tooptimize UAV trajectories balancing operational objectives with human comfortand safety considerations. Additionally through a comprehensive survey weinvestigate the impact of gender cues and anthropomorphism in UAV design onpublic acceptance and trust revealing significant implications for droneinteraction strategies in SAR. Our contributions include 1 a reinforcementlearning framework for UAV trajectory planning that dynamically integratesmulti-objective considerations 2 an analysis of human perceptions towardsgendered and anthropomorphized drones in SAR contexts and 3 the applicationof similarity-based experience replay for enhanced learning efficiency incomplex SAR scenarios. The findings offer valuable insights into designing UAVsystems that are not only technically proficient but also aligned withhuman-centric values.</p>
                <p>Last Updated: 2024-02-28 17:10:22 UTC</p>
                <button class="interpret-button" data-id="2402.18487v1">Interpret</button>
                <div id="interpretation-2402.18487v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Robotising Psychometrics: Validating Wellbeing Assessment Tools in Child-Robot Interactions</h3>
                <p>Authors: Nida Itrat AbbasiGuy LabanTamsin FordPeter B JonesHatice Gunes</p>
                <p><a href="http://arxiv.org/abs/2402.18325v1">Link to paper</a></p>
                <p>The interdisciplinary nature of Child-Robot Interaction CRI fostersincorporating measures and methodologies from many established domains.However when employing CRI approaches to sensitive avenues of health andwellbeing caution is critical in adapting metrics to retain their safetystandards and ensure accurate utilisation. In this work we conducted asecondary analysis to previous empirical work investigating the reliabilityand construct validity of established psychological questionnaires such as theShort Moods and Feelings Questionnaire SMFQ and three subscales generalisedanxiety panic and low mood of the Revised Child Anxiety and Depression ScaleRCADS within a CRI setting for the assessment of mental wellbeing. Throughconfirmatory principal component analysis we have observed that these measuresare reliable and valid in the context of CRI. Furthermore our analysisrevealed that scales communicated by a robot demonstrated a better fit thanwhen self-reported underscoring the efficiency and effectiveness ofrobot-mediated psychological assessments in these settings. Nevertheless wehave also observed variations in item contributions to the main factorsuggesting potential areas of examination and revision e.g. relating tophysiological changes inactivity and cognitive demands when used in CRI.Findings from this work highlight the importance of verifying the reliabilityand validity of standardised metrics and assessment tools when employed in CRIsettings thus aiming to avoid any misinterpretations and misrepresentations.</p>
                <p>Last Updated: 2024-02-28 13:42:12 UTC</p>
                <button class="interpret-button" data-id="2402.18325v1">Interpret</button>
                <div id="interpretation-2402.18325v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Affective State Detection using fNIRs and Machine Learning</h3>
                <p>Authors: Ritam Ghosh</p>
                <p><a href="http://arxiv.org/abs/2402.18241v1">Link to paper</a></p>
                <p>Affective states regulate our day to day to function and has a tremendouseffect on mental and physical health. Detection of affective states is ofutmost importance for mental health monitoring smart entertainment selectionand dynamic workload management. In this paper we discussed relevantliterature on affective state detection using physiology data the benefits andlimitations of different sensors and methods used for collecting physiologydata and our rationale for selecting functional near-infrared spectroscopy. Wepresent the design of an experiment involving nine subjects to evoke theaffective states of meditation amusement and cognitive load and the results ofthe attempt to classify using machine learning. A mean accuracy of 83.04 wasachieved in three class classification with an individual model 84.39accuracy was achieved for a group model and 60.57 accuracy was achieved forsubject independent model using leave one out cross validation. It was foundthat prediction accuracy for cognitive load was higher evoked using a pen andpaper task than the other two classes evoked using computer bases tasks. Toverify that this discrepancy was not due to motor skills involved in the penand paper task a second experiment was conducted using four participants andthe results of that experiment has also been presented in the paper.</p>
                <p>Last Updated: 2024-02-28 11:12:47 UTC</p>
                <button class="interpret-button" data-id="2402.18241v1">Interpret</button>
                <div id="interpretation-2402.18241v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.LG">
        <h2>cs.LG</h2>
        <ul>
    
            <li>
                <h3>Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards</h3>
                <p>Authors: Haoxiang WangYong LinWei XiongRui YangShizhe DiaoShuang QiuHan ZhaoTong Zhang</p>
                <p><a href="http://arxiv.org/abs/2402.18571v2">Link to paper</a></p>
                <p>Fine-grained control over large language models LLMs remains a significantchallenge hindering their adaptability to diverse user needs. WhileReinforcement Learning from Human Feedback RLHF shows promise in aligningLLMs its reliance on scalar rewards often limits its ability to capturediverse user preferences in real-world applications. To address thislimitation we introduce the Directional Preference Alignment DPA framework.Unlike the scalar-reward RLHF DPA incorporates multi-objective reward modelingto represent diverse preference profiles. Additionally DPA models userpreferences as directions i.e. unit vectors in the reward space to achieveuser-dependent preference control. Our method involves training amulti-objective reward model and then fine-tuning the LLM with apreference-conditioned variant of Rejection Sampling Finetuning RSF an RLHFmethod adopted by Llama 2. This method enjoys a better performance trade-offacross various reward objectives. In comparison with the scalar-reward RLHFDPA offers users intuitive control over LLM generation: they can arithmeticallyspecify their desired trade-offs e.g. more helpfulness with less verbosity.We also validate the effectiveness of DPA with real-world alignment experimentson Mistral-7B. Our method provides straightforward arithmetic control over thetrade-off between helpfulness and verbosity while maintaining competitiveperformance with strong baselines such as Direct Preference Optimization DPO.</p>
                <p>Last Updated: 2024-02-29 04:33:29 UTC</p>
                <button class="interpret-button" data-id="2402.18571v2">Interpret</button>
                <div id="interpretation-2402.18571v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Diffusion Language Models Are Versatile Protein Learners</h3>
                <p>Authors: Xinyou WangZaixiang ZhengFei YeDongyu XueShujian HuangQuanquan Gu</p>
                <p><a href="http://arxiv.org/abs/2402.18567v1">Link to paper</a></p>
                <p>This paper introduces diffusion protein language model DPLM a versatileprotein language model that demonstrates strong generative and predictivecapabilities for protein sequences. We first pre-train scalable DPLMs fromevolutionary-scale protein sequences within a generative self-superviseddiscrete diffusion probabilistic framework which generalizes language modelingfor proteins in a principled way. After pre-training DPLM exhibits the abilityto generate structurally plausible novel and diverse protein sequences forunconditional generation. We further demonstrate the proposed diffusiongenerative pre-training makes DPLM possess a better understanding of proteinsmaking it a superior representation learner which can be fine-tuned forvarious predictive tasks comparing favorably to ESM2 Lin et al. 2022.Moreover DPLM can be tailored for various needs which showcases its prowessof conditional generation in several ways: 1 conditioning on partial peptidesequences e.g. generating scaffolds for functional motifs with high successrate 2 incorporating other modalities as conditioner e.g.structure-conditioned generation for inverse folding and 3 steering sequencegeneration towards desired properties e.g. satisfying specified secondarystructures through a plug-and-play classifier guidance.</p>
                <p>Last Updated: 2024-02-28 18:57:56 UTC</p>
                <button class="interpret-button" data-id="2402.18567v1">Interpret</button>
                <div id="interpretation-2402.18567v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Approaching Human-Level Forecasting with Language Models</h3>
                <p>Authors: Danny HalawiFred ZhangChen Yueh-HanJacob Steinhardt</p>
                <p><a href="http://arxiv.org/abs/2402.18563v1">Link to paper</a></p>
                <p>Forecasting future events is important for policy and decision making. Inthis work we study whether language models LMs can forecast at the level ofcompetitive human forecasters. Towards this goal we develop aretrieval-augmented LM system designed to automatically search for relevantinformation generate forecasts and aggregate predictions. To facilitate ourstudy we collect a large dataset of questions from competitive forecastingplatforms. Under a test set published after the knowledge cut-offs of our LMswe evaluate the end-to-end performance of our system against the aggregates ofhuman forecasts. On average the system nears the crowd aggregate ofcompetitive forecasters and in some settings surpasses it. Our work suggeststhat using LMs to forecast the future could provide accurate predictions atscale and help to inform institutional decision making.</p>
                <p>Last Updated: 2024-02-28 18:54:18 UTC</p>
                <button class="interpret-button" data-id="2402.18563v1">Interpret</button>
                <div id="interpretation-2402.18563v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Implicit Bias of Next-Token Prediction</h3>
                <p>Authors: Christos Thrampoulidis</p>
                <p><a href="http://arxiv.org/abs/2402.18551v1">Link to paper</a></p>
                <p>Next-token prediction NTP the go-to training paradigm in training largelanguage models involves predicting the next token in a sequence. Departingfrom traditional one-hot classification in NTP multiple tokens with varyingfrequencies follow each given context. This work frames NTP training ascross-entropy minimization over distinct contexts each associated with asparse empirical probability vector across a finite vocabulary. It thenaddresses the following question: do gradient-based optimizers exhibit a biastowards solutions with specific structure as the NTP training loss reaches itslower bound entropy Specifically for linear NTP models trained usinggradient descent GD we make the following contributions: Firstly wedetermine NTP-separability conditions on the data under which GD can attainits lower bound. We also demonstrate that these conditions hold underoverparameterization. Secondly we establish that the parameters of GDprojected onto an appropriate data subspace converge to the unique solution ofa system of linear equations which requires the logits difference ofin-support tokens to be equal to the log-ratio of their respectiveprobabilities. Meanwhile on the orthogonal subspace the parameters divergeand converge in the direction of the solution of a max-margin quadraticprogram minimizing the Euclidean norm of parameters satisfying theNTP-separability conditions. Akin to prior research on implicit bias ofone-hot classification our work opens exciting avenues for future researchthat can lead to better understanding optimization generalization androbustness principles of models trained with NTP.</p>
                <p>Last Updated: 2024-02-28 18:34:53 UTC</p>
                <button class="interpret-button" data-id="2402.18551v1">Interpret</button>
                <div id="interpretation-2402.18551v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces</h3>
                <p>Authors: Geeling ChauYujin AnAhamed Raffey IqbalSoon-Jo ChungYisong YueSabera Talukder</p>
                <p><a href="http://arxiv.org/abs/2402.18546v2">Link to paper</a></p>
                <p>A major goal in neuroscience is to discover neural data representations thatgeneralize. This goal is challenged by variability along recording sessionse.g. environment subjects e.g. varying neural structures and sensorse.g. sensor noise among others. Recent work has begun to addressgeneralization across sessions and subjects but few study robustness to sensorfailure which is highly prevalent in neuroscience experiments. In order toaddress these generalizability dimensions we first collect our ownelectroencephalography dataset with numerous sessions subjects and sensorsthen study two time series models: EEGNet Lawhern et al. 2018 and TOTEMTalukder et al. 2024. EEGNet is a widely used convolutional neural networkwhile TOTEM is a discrete time series tokenizer and transformer model. We findthat TOTEM outperforms or matches EEGNet across all generalizability cases.Finally through analysis of TOTEMs latent codebook we observe thattokenization enables generalization</p>
                <p>Last Updated: 2024-02-29 18:35:58 UTC</p>
                <button class="interpret-button" data-id="2402.18546v2">Interpret</button>
                <div id="interpretation-2402.18546v2" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
    <section id="cs.CV">
        <h2>cs.CV</h2>
        <ul>
    
            <li>
                <h3>UniMODE: Unified Monocular 3D Object Detection</h3>
                <p>Authors: Zhuoling LiXiaogang XuSerNam LimHengshuang Zhao</p>
                <p><a href="http://arxiv.org/abs/2402.18573v1">Link to paper</a></p>
                <p>Realizing unified monocular 3D object detection including both indoor andoutdoor scenes holds great importance in applications like robot navigation.However involving various scenarios of data to train models poses challengesdue to their significantly different characteristics e.g. diverse geometryproperties and heterogeneous domain distributions. To address these challengeswe build a detector based on the birds-eye-view BEV detection paradigmwhere the explicit feature projection is beneficial to addressing the geometrylearning ambiguity when employing multiple scenarios of data to traindetectors. Then we split the classical BEV detection architecture into twostages and propose an uneven BEV grid design to handle the convergenceinstability caused by the aforementioned challenges. Moreover we develop asparse BEV feature projection strategy to reduce computational cost and aunified domain alignment method to handle heterogeneous domains. Combiningthese techniques a unified detector UniMODE is derived which surpasses theprevious state-of-the-art on the challenging Omni3D dataset a large-scaledataset including both indoor and outdoor scenes by 4.9 AP_3D revealing thefirst successful generalization of a BEV detector to unified 3D objectdetection.</p>
                <p>Last Updated: 2024-02-28 18:59:31 UTC</p>
                <button class="interpret-button" data-id="2402.18573v1">Interpret</button>
                <div id="interpretation-2402.18573v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Selection of appropriate multispectral camera exposure settings and radiometric calibration methods for applications in phenotyping and precision agriculture</h3>
                <p>Authors: Vaishali SwaminathanJ. Alex ThomassonRobert G. HardinNithya Rajan</p>
                <p><a href="http://arxiv.org/abs/2402.18553v1">Link to paper</a></p>
                <p>Radiometric accuracy of data is crucial in quantitative precisionagriculture to produce reliable and repeatable data for modeling and decisionmaking. The effect of exposure time and gain settings on the radiometricaccuracy of multispectral images was not explored enough. The goal of thisstudy was to determine if having a fixed exposure FE time during imageacquisition improved radiometric accuracy of images compared to the defaultauto-exposure AE settings. This involved quantifying the errors fromauto-exposure and determining ideal exposure values within which radiometricmean absolute percentage error MAPE were minimal  5. The results showedthat FE orthomosaic was closer to ground-truth higher R2 and lower MAPE thanAE orthomosaic. An ideal exposure range was determined for capturing canopy andsoil objects without loss of information from under-exposure or saturationfrom over-exposure. A simulation of errors from AE showed that MAPE  5 forthe blue green red and NIR bands and  7 for the red edge band for exposuresettings within the determined ideal ranges and increased exponentially beyondthe ideal exposure upper limit. Further prediction of total plant nitrogenuptake g/plant using vegetation indices VIs from two different growingseasons were closer to the ground truth mostly R2  0.40 and MAPE  12 to14 p  0.05 when FE was used compared to the prediction from AE imagesmostly R2  0.13 MAPE  15 to 18 p  0.05.</p>
                <p>Last Updated: 2024-02-28 18:35:59 UTC</p>
                <button class="interpret-button" data-id="2402.18553v1">Interpret</button>
                <div id="interpretation-2402.18553v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Gradient Reweighting: Towards Imbalanced Class-Incremental Learning</h3>
                <p>Authors: Jiangpeng HeFengqing Zhu</p>
                <p><a href="http://arxiv.org/abs/2402.18528v1">Link to paper</a></p>
                <p>Class-Incremental Learning CIL trains a model to continually recognize newclasses from non-stationary data while retaining learned knowledge. A majorchallenge of CIL arises when applying to real-world data characterized bynon-uniform distribution which introduces a dual imbalance problem involvingi disparities between stored exemplars of old tasks and new class datainter-phase imbalance and ii severe class imbalances within eachindividual task intra-phase imbalance. We show that this dual imbalance issuecauses skewed gradient updates with biased weights in FC layers thus inducingover/under-fitting and catastrophic forgetting in CIL. Our method addresses itby reweighting the gradients towards balanced optimization and unbiasedclassifier learning. Additionally we observe imbalanced forgetting whereparadoxically the instance-rich classes suffer higher performance degradationduring CIL due to a larger amount of training data becoming unavailable insubsequent learning phases. To tackle this we further introduce adistribution-aware knowledge distillation loss to mitigate forgetting byaligning output logits proportionally with the distribution of lost trainingdata. We validate our method on CIFAR-100 ImageNetSubset and Food101 acrossvarious evaluation protocols and demonstrate consistent improvements comparedto existing works showing great potential to apply CIL in real-world scenarioswith enhanced robustness and effectiveness.</p>
                <p>Last Updated: 2024-02-28 18:08:03 UTC</p>
                <button class="interpret-button" data-id="2402.18528v1">Interpret</button>
                <div id="interpretation-2402.18528v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep Structures</h3>
                <p>Authors: Andrei CozmaLandon HarrisHairong QiPing JiWenpeng GuoSong Yuan</p>
                <p><a href="http://arxiv.org/abs/2402.18527v1">Link to paper</a></p>
                <p>This paper introduces a robust approach for automated defect detection intire X-ray images by harnessing traditional feature extraction methods such asLocal Binary Pattern LBP and Gray Level Co-Occurrence Matrix GLCM featuresas well as Fourier and Wavelet-based features complemented by advanced machinelearning techniques. Recognizing the challenges inherent in the complexpatterns and textures of tire X-ray images the study emphasizes thesignificance of feature engineering to enhance the performance of defectdetection systems. By meticulously integrating combinations of these featureswith a Random Forest RF classifier and comparing them against advanced modelslike YOLOv8 the research not only benchmarks the performance of traditionalfeatures in defect detection but also explores the synergy between classicaland modern approaches. The experimental results demonstrate that thesetraditional features when fine-tuned and combined with machine learningmodels can significantly improve the accuracy and reliability of tire defectdetection aiming to set a new standard in automated quality assurance in tiremanufacturing.</p>
                <p>Last Updated: 2024-02-28 18:07:47 UTC</p>
                <button class="interpret-button" data-id="2402.18527v1">Interpret</button>
                <div id="interpretation-2402.18527v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
            <li>
                <h3>Multimodal Learning To Improve Cardiac Late Mechanical Activation Detection From Cine MR Images</h3>
                <p>Authors: Jiarui XingNian WuKenneth BilchickFrederick EpsteinMiaomiao Zhang</p>
                <p><a href="http://arxiv.org/abs/2402.18507v1">Link to paper</a></p>
                <p>This paper presents a multimodal deep learning framework that utilizesadvanced image techniques to improve the performance of clinical analysisheavily dependent on routinely acquired standard images. More specifically wedevelop a joint learning network that for the first time leverages the accuracyand reproducibility of myocardial strains obtained from Displacement Encodingwith Stimulated Echo DENSE to guide the analysis of cine cardiac magneticresonance CMR imaging in late mechanical activation LMA detection. An imageregistration network is utilized to acquire the knowledge of cardiac motionsan important feature estimator of strain values from standard cine CMRs. Ourframework consists of two major components: i a DENSE-supervised strainnetwork leveraging latent motion features learned from a registration networkto predict myocardial strains and ii a LMA network taking advantage of thepredicted strain for effective LMA detection. Experimental results show thatour proposed work substantially improves the performance of strain analysis andLMA detection from cine CMR images aligning more closely with the achievementsof DENSE.</p>
                <p>Last Updated: 2024-02-28 17:34:58 UTC</p>
                <button class="interpret-button" data-id="2402.18507v1">Interpret</button>
                <div id="interpretation-2402.18507v1" class="interpretation" style="display:none;">
                    <p>Interpretation: <br></p>
                </div>
            </li>
        
        </ul>
    </section>
    
        <div id="last-updated">
            <p>Updated Time: 2024-03-01</p>
        </div>
    
        </div>
    </body>
    </html>
    